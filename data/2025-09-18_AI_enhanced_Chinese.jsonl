{"id": "2509.13367", "categories": ["quant-ph", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.13367", "abs": "https://arxiv.org/abs/2509.13367", "authors": ["Tom\u00e1\u0161 Bezd\u011bk"], "title": "Numerical Optimization Methods in the environment with Quantum Noise", "comment": "Revised version of a Master's thesis originally submitted to VSB -\n  Technical University of Ostrava in 2025. This updated version contains minor\n  cosmetic changes, including typos, grammatical errors, and clarifying\n  sentences. New citations were also added. No results or conclusions were\n  changed from the original. The original source is at\n  https://dspace.vsb.cz/handle/10084/156936", "summary": "The accurate calculation of electronic potential energy surfaces for ground\nand excited states is crucial for understanding photochemical processes,\nparticularly near conical intersections. While classical methods are limited by\nscaling and quantum algorithms by hardware, this thesis focuses on the\nState-Averaged Orbital-Optimized Variational Quantum Eigensolver (SA-OO-VQE).\nThis hybrid quantum-classical algorithm provides a balanced description of\nmultiple electronic states by combining quantum state preparation with\nclassical state-averaged orbital optimization.\n  A key contribution is the implementation and evaluation of the Differential\nEvolution algorithm within the SA-OO-VQE framework, with a comparative study\nagainst classical optimizers like the Broyden-Fletcher-Goldfarb-Shanno (BFGS)\nand Sequential Least Squares Programming (SLSQP) algorithms. The performance of\nthese optimizers is assessed by calculating ground and first excited state\nenergies for H$_2$, H$_4$, and LiH.\n  The thesis also demonstrates SA-OO-VQE's capability to accurately model\npotential energy surfaces near conical intersections, using formaldimine as a\ncase study. The results show that orbital optimization is essential for\ncorrectly capturing the potential energy surface topology, a task where\nstandard methods with fixed orbitals fail. Our findings indicate that while\nDifferential Evolution presents efficiency challenges, gradient-based methods\nlike BFGS and SLSQP offer superior performance, confirming that the SA-OO-VQE\napproach is crucial for treating complex electronic structures.", "AI": {"tldr": "\u91cf\u5b50\u7b97\u6cd5\u7ed3\u5408\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u8ba1\u7b97\u5316\u5b66\u53cd\u5e94\u4e2d\u7684\u7535\u5b50\u52bf\u80fd\u9762\u3002", "motivation": "\u51c6\u786e\u8ba1\u7b97\u591a\u7535\u5b50\u4f53\u7cfb\u7684\u52bf\u80fd\u9762\uff0c\u7279\u522b\u662f\u5728\u5706\u9525\u4ea4\u53c9\u70b9\u9644\u8fd1\uff0c\u5bf9\u4e8e\u7406\u89e3\u5149\u5316\u5b66\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u6216\u786c\u4ef6\u65b9\u9762\u5b58\u5728\u9650\u5236\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u540d\u4e3a\u72b6\u6001\u5e73\u5747\u8f68\u9053\u4f18\u5316\u53d8\u5206\u91cf\u5b50\u7279\u5f81\u6c42\u89e3\u5668\uff08SA-OO-VQE\uff09\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u91cf\u5b50\u6001\u5236\u5907\u548c\u7ecf\u5178\u7684\u72b6\u6001\u5e73\u5747\u8f68\u9053\u4f18\u5316\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u5728SA-OO-VQE\u6846\u67b6\u5185\u7684\u8868\u73b0\uff0c\u5e76\u4e0eBFGS\u548cSLSQP\u7b49\u7ecf\u5178\u4f18\u5316\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97H2\u3001H4\u548cLiH\u7684\u57fa\u6001\u548c\u7b2c\u4e00\u6fc0\u53d1\u6001\u80fd\u91cf\uff0c\u4ee5\u53ca\u4f7f\u7528\u7532\u919b\u4e9a\u80fa\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86SA-OO-VQE\u80fd\u591f\u51c6\u786e\u6a21\u62df\u5706\u9525\u4ea4\u53c9\u70b9\u9644\u8fd1\u7684\u52bf\u80fd\u9762\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f68\u9053\u4f18\u5316\u5bf9\u4e8e\u6355\u6349\u52bf\u80fd\u9762\u7684\u62d3\u6251\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u800c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u5982BFGS\u548cSLSQP\uff09\u6bd4\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "SA-OO-VQE\u662f\u4e00\u79cd\u5904\u7406\u590d\u6742\u7535\u5b50\u7ed3\u6784\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5176\u4e2d\u8f68\u9053\u4f18\u5316\u662f\u7cbe\u786e\u6a21\u62df\u52bf\u80fd\u9762\u7684\u5173\u952e\u3002\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u5728SA-OO-VQE\u6846\u67b6\u4e2d\u6bd4\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2509.13678", "categories": ["quant-ph", "cs.NA", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.13678", "abs": "https://arxiv.org/abs/2509.13678", "authors": ["Carolyn Mayer", "Anand Ganti", "Uzoma Onunkwo", "Tzvetan Metodi", "Benjamin Anker", "Jacek Skryzalin"], "title": "Rare Event Simulation of Quantum Error-Correcting Circuits", "comment": "10 pages, 15 figures", "summary": "We describe a practical approach for accessing the logical failure rates of\nquantum error-correcting (QEC) circuits under low physical (component) failure\nrate regimes. Standard Monte Carlo is often the de facto approach for studying\nthe failure rates of quantum circuits. However, in the study of fault-tolerant\nerror-correcting circuits, the ability to extend this approach to low physical\nfailure rates is limited. In particular, the use of Monte Carlo to access\ncircuits that are relatively large or have high correcting power becomes more\ndifficult as we lower the input failure rates of the individual components\n(gates) in the circuit. For these reasons, many simulations studying the\ncircuit model go no lower than end-to-end logical failure rates in the 10^{-6}\nregime. In this report, we outline an approach that borrows from earlier work\nby Bravyi and Vargo to the more complex circuit noise model. Earlier works\nstudied both the capacity and phenomenological noise models, but the work is\ninsufficient for generating similar simulations in the circuit-noise model. To\nthe best of our knowledge, our team is the first to develop a full prescription\nof the rare event simulation by splitting technique for the circuit-based noise\nmodel. We have also generated promising results that are confirmed by standard\nMonte Carlo simulation under an accessible regime. This work shows that we can\naccess noise in the circuit-model prescription of quantum error-correcting code\nto failure rates below 10^{-20} regime.", "AI": {"tldr": "We present a novel rare event simulation technique for analyzing the failure rates of quantum error-correcting circuits, enabling simulations at failure rates below 10^-20, which is significantly lower than previously possible with standard Monte Carlo methods.", "motivation": "Standard Monte Carlo methods struggle to accurately simulate the low physical failure rates of quantum error-correcting circuits, especially for larger circuits or those with high correcting power. This limitation prevents the study of failure rates below the 10^-6 regime.", "method": "This paper adapts and extends the rare event simulation by splitting technique, originally developed by Bravyi and Vargo, to a circuit-based noise model. This approach allows for more efficient and accurate simulations at extremely low failure rates.", "result": "The proposed method successfully simulates quantum error-correcting circuits at logical failure rates below 10^-20. The results have been validated against standard Monte Carlo simulations in an accessible regime, confirming their accuracy.", "conclusion": "The developed rare event simulation technique provides a practical and effective way to analyze the performance of quantum error-correcting codes under very low physical failure rates, opening up new possibilities for the study and design of fault-tolerant quantum computers."}}
{"id": "2509.13717", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.13717", "abs": "https://arxiv.org/abs/2509.13717", "authors": ["Yifan Yu", "Cheuk Hin Ho", "Yangshuai Wang"], "title": "A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving PDEs, yet existing uncertainty quantification (UQ) approaches for\nPINNs generally lack rigorous statistical guarantees. In this work, we bridge\nthis gap by introducing a distribution-free conformal prediction (CP) framework\nfor UQ in PINNs. This framework calibrates prediction intervals by constructing\nnonconformity scores on a calibration set, thereby yielding distribution-free\nuncertainty estimates with rigorous finite-sample coverage guarantees for\nPINNs. To handle spatial heteroskedasticity, we further introduce local\nconformal quantile estimation, enabling spatially adaptive uncertainty bands\nwhile preserving theoretical guarantee. Through systematic evaluations on\ntypical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz\nequations) and comprehensive testing across multiple uncertainty metrics, our\nresults demonstrate that the proposed framework achieves reliable calibration\nand locally adaptive uncertainty intervals, consistently outperforming\nheuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work\nintroduces a general framework that not only enhances calibration and\nreliability, but also opens new avenues for uncertainty-aware modeling of\ncomplex PDE systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03-free \u5171\u8bc6\u9884\u6d4b\uff08CP\uff09\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4e25\u683c\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u89e3\u51b3\u4e86\u7a7a\u95f4\u5f02\u65b9\u5dee\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684 PINNs UQ \u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u5206\u5e03-free \u5171\u8bc6\u9884\u6d4b\uff08CP\uff09\u7684 UQ \u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u5171\u8bc6\u5206\u4f4d\u6570\u4f30\u8ba1\u6765\u5904\u7406\u7a7a\u95f4\u5f02\u65b9\u5dee\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u79cd PDE \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u9884\u6d4b\u533a\u95f4\u6821\u51c6\u53ef\u9760\u4e14\u5177\u6709\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684 UQ \u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06 PINNs \u4e0e\u5206\u5e03-free UQ \u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u6821\u51c6\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u590d\u6742 PDE \u7cfb\u7edf\u7684 UQ \u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.13952", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.13952", "abs": "https://arxiv.org/abs/2509.13952", "authors": ["Amin Lotfalian", "Mohammad Reza Banan", "Pooyan Broumand"], "title": "eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems", "comment": null, "summary": "This paper presents eXtended Physics-Informed Neural Network (X-PINN), a\nnovel and robust framework for addressing fracture mechanics problems involving\nmultiple cracks in fractured media. To address this, an energy-based loss\nfunction, customized integration schemes, and domain decomposition procedures\nare proposed. Inspired by the Extended Finite Element Method (XFEM), the neural\nnetwork solution space is enriched with specialized functions that allow crack\nbody discontinuities and singularities at crack tips to be explicitly captured.\nFurthermore, a structured framework is introduced in which standard and\nenriched solution components are modeled using distinct neural networks,\nenabling flexible and effective simulations of complex multiple-crack problems\nin 1D and 2D domains, with convenient extensibility to 3D problems. Numerical\nexperiments are conducted to validate the effectiveness and robustness of the\nproposed method.", "AI": {"tldr": "X-PINN\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u591a\u88c2\u7f1d\u65ad\u88c2\u529b\u5b66\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408XFEM\u7684\u7279\u70b9\uff0c\u5229\u7528\u4e13\u95e8\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u6355\u6349\u88c2\u7f1d\u7684\u590d\u6742\u6027\u3002", "motivation": "\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6d89\u53ca\u591a\u88c2\u7f1d\u7684\u65ad\u88c2\u529b\u5b66\u95ee\u9898\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u635f\u5931\u51fd\u6570\u3001\u5b9a\u5236\u7684\u79ef\u5206\u65b9\u6848\u548c\u57df\u5206\u89e3\u7a0b\u5e8f\u3002X-PINN\u7684\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4\u901a\u8fc7\u4e13\u95e8\u7684\u51fd\u6570\u8fdb\u884c\u4e30\u5bcc\uff0c\u4ee5\u660e\u786e\u6355\u6349\u88c2\u7f1d\u4f53\u7684\u4e0d\u8fde\u7eed\u6027\u548c\u88c2\u7f1d\u5c16\u7aef\u7684\u5947\u70b9\u3002\u6807\u51c6\u548c\u4e30\u5bcc\u89e3\u5206\u91cf\u7531\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u3002", "result": "X-PINN\u80fd\u591f\u7075\u6d3b\u6709\u6548\u5730\u6a21\u62df1D\u548c2D\u57df\u4e2d\u7684\u590d\u6742\u591a\u88c2\u7f1d\u95ee\u9898\uff0c\u5e76\u6613\u4e8e\u6269\u5c55\u52303D\u95ee\u9898\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "X-PINN\u6846\u67b6\u5728\u5904\u7406\u591a\u88c2\u7f1d\u65ad\u88c2\u529b\u5b66\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.13325", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13325", "abs": "https://arxiv.org/abs/2509.13325", "authors": ["Matteo Zanotto", "Leonardo Vicentini", "Redi Vreto", "Francesco Lumpp", "Diego Braga", "Sandro Fiore"], "title": "A User-centric Kubernetes-based Architecture for Green Cloud Computing", "comment": null, "summary": "To meet the increasing demand for cloud computing services, the scale and\nnumber of data centers keeps increasing worldwide. This growth comes at the\ncost of increased electricity consumption, which directly correlates to CO2\nemissions, the main driver of climate change. As such, researching ways to\nreduce cloud computing emissions is more relevant than ever. However, although\ncloud providers are reportedly already working near optimal power efficiency,\nthey fail in providing precise sustainability reporting. This calls for further\nimprovements on the cloud computing consumer's side. To this end, in this paper\nwe propose a user-centric, Kubernetes-based architecture for green cloud\ncomputing. We implement a carbon intensity forecaster and we use it to schedule\nworkloads based on the availability of green energy, exploiting both regional\nand temporal variations to minimize emissions. We evaluate our system using\nreal-world traces of cloud workloads execution comparing the achieved carbon\nemission savings against a baseline round-robin scheduler. Our findings\nindicate that our system can achieve up to a 13% reduction in emissions in a\nstrict scenario with heavy limitations on the available resources.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u3001\u57fa\u4e8eKubernetes\u7684\u7eff\u8272\u4e91\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u9884\u6d4b\u78b3\u6392\u653e\u5f3a\u5ea6\u5e76\u636e\u6b64\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u6765\u5229\u7528\u7eff\u8272\u80fd\u6e90\uff0c\u4ee5\u6700\u5c0f\u5316\u78b3\u6392\u653e\u3002", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u9700\u6c42\u7684\u589e\u957f\uff0c\u6570\u636e\u4e2d\u5fc3\u7684\u7535\u529b\u6d88\u8017\u548c\u76f8\u5173\u7684CO2\u6392\u653e\u5bf9\u6c14\u5019\u53d8\u5316\u4ea7\u751f\u4e86\u91cd\u5927\u5f71\u54cd\u3002\u5c3d\u7ba1\u4e91\u670d\u52a1\u63d0\u4f9b\u5546\u5728\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u65b9\u9762\u5df2\u63a5\u8fd1\u6700\u4f18\uff0c\u4f46\u5728\u63d0\u4f9b\u7cbe\u786e\u7684\u53ef\u6301\u7eed\u6027\u62a5\u544a\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u4e91\u7528\u6237\u5c42\u9762\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u4e2d\u5fc3\u3001\u57fa\u4e8eKubernetes\u7684\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u78b3\u6392\u653e\u5f3a\u5ea6\u9884\u6d4b\u5668\uff0c\u5e76\u5229\u7528\u8be5\u9884\u6d4b\u5668\u57fa\u4e8e\u7eff\u8272\u80fd\u6e90\u7684\u53ef\u7528\u6027\u6765\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5145\u5206\u5229\u7528\u533a\u57df\u548c\u65f6\u95f4\u4e0a\u7684\u53d8\u5316\u6765\u6700\u5c0f\u5316\u6392\u653e\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u8ddf\u8e2a\u8bc4\u4f30\u8be5\u7cfb\u7edf\uff0c\u5e76\u4e0e\u57fa\u7ebf\u8f6e\u8be2\u8c03\u5ea6\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u53d1\u73b0\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e25\u683c\u573a\u666f\u4e0b\uff0c\u8be5\u7cfb\u7edf\u53ef\u5b9e\u73b0\u9ad8\u8fbe13%\u7684\u78b3\u6392\u653e\u51cf\u5c11\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7528\u6237\u4e2d\u5fc3\u3001\u57fa\u4e8eKubernetes\u7684\u67b6\u6784\u80fd\u591f\u901a\u8fc7\u667a\u80fd\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u6765\u6709\u6548\u51cf\u5c11\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u78b3\u6392\u653e\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u5b9e\u73b0\u7eff\u8272\u4e91\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13328", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13328", "abs": "https://arxiv.org/abs/2509.13328", "authors": ["Danish Rizvi", "David Boyle"], "title": "Dual Actor DDPG for Airborne STAR-RIS Assisted Communications", "comment": null, "summary": "This study departs from the prevailing assumption of independent Transmission\nand Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect\nReconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a\nnovel multi-user downlink communication system that leverages a UAV-mounted\nSTAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key\ncontributions include the joint optimization of UAV trajectory, active\nbeamforming vectors at the base station, and passive RIS TRCs to enhance\ncommunication efficiency, while considering UAV energy constraints. We design\nthe TRC as a combination of discrete and continuous actions, and propose a\nnovel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The\nalgorithm relies on two separate actor networks for high-dimensional hybrid\naction space. We also propose a novel harmonic mean index (HFI)-based reward\nfunction to ensure communication fairness amongst users. For comprehensive\nanalysis, we study the impact of RIS size on UAV aerodynamics showing that it\nincreases drag and energy demand. Simulation results demonstrate that the\nproposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based\nsolutions by 24% and 97%, respectively, in accumulated reward.\nThree-dimensional UAV trajectory optimization achieves 28% higher communication\nefficiency compared to two-dimensional and altitude optimization. The HFI based\nreward function provides 41% lower QoS denial rates as compared to other\nbenchmarks. The mobile Aerial-STAR system shows superior performance over fixed\ndeployed counterparts, with the coupled phase STAR-RIS outperforming dual\nTransmit/Reflect RIS and conventional RIS setups. These findings highlight the\npotential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG\napproach in optimizing their performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u6302\u8f7d\u7684\u667a\u80fd\u53cd\u5c04\u9762\uff08STAR-RIS\uff09\u7cfb\u7edf\uff08Aerial-STAR\uff09\uff0c\u91c7\u7528\u8026\u5408\u7684\u4f20\u8f93\u548c\u53cd\u5c04\u7cfb\u6570\uff08TRC\uff09\u76f8\u4f4d\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u591a\u7528\u6237\u4e0b\u884c\u901a\u4fe1\u6548\u7387\u3002\u8be5\u7cfb\u7edf\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u57fa\u7ad9\u6ce2\u675f\u8d4b\u5f62\u548cRIS TRC\uff0c\u540c\u65f6\u8003\u8651\u65e0\u4eba\u673a\u7684\u80fd\u6e90\u9650\u5236\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u53ccActor\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DA-DDPG\uff09\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e09\u7ef4\u8f68\u8ff9\u4f18\u5316\u548c\u8026\u5408\u76f8\u4f4d\u6a21\u578b\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u5347\u901a\u4fe1\u6548\u7387\u548c\u7528\u6237\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u666e\u904d\u5047\u8bbeSTAR-RIS\u4e2d\u7684\u4f20\u8f93\u548c\u53cd\u5c04\u7cfb\u6570\u662f\u72ec\u7acb\u7684\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7d22\u5728UAV\u6302\u8f7d\u7684STAR-RIS\u7cfb\u7edf\u4e2d\uff0c\u8003\u8651\u8026\u5408TRC\u76f8\u4f4d\u6a21\u578b\u4e0b\uff0c\u8054\u5408\u4f18\u5316UAV\u8f68\u8ff9\u3001\u57fa\u7ad9\u6ce2\u675f\u8d4b\u5f62\u548cRIS TRC\u4ee5\u63d0\u5347\u901a\u4fe1\u6548\u7387\u548c\u7528\u6237\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ccActor\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DA-DDPG\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u7684Actor\u7f51\u7edc\u6765\u5904\u7406\u9ad8\u7ef4\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\uff08\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\uff09\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c10\u6ce2\u5747\u503c\u6307\u6570\uff08HFI\uff09\u7684\u5956\u52b1\u51fd\u6570\u6765\u786e\u4fdd\u7528\u6237\u4e4b\u95f4\u7684\u901a\u4fe1\u516c\u5e73\u6027\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86RIS\u5c3a\u5bf8\u5bf9UAV\u7a7a\u6c14\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "result": "\u6240\u63d0\u51fa\u7684DA-DDPG\u7b97\u6cd5\u5728\u7d2f\u79ef\u5956\u52b1\u65b9\u9762\u6bd4\u4f20\u7edf\u7684DDPG\u548cDQN\u7b97\u6cd5\u5206\u522b\u63d0\u9ad8\u4e8624%\u548c97%\u3002\u4e09\u7ef4UAV\u8f68\u8ff9\u4f18\u5316\u6bd4\u4e8c\u7ef4\u548c\u9ad8\u5ea6\u4f18\u5316\u63d0\u9ad8\u4e8628%\u7684\u901a\u4fe1\u6548\u7387\u3002\u57fa\u4e8eHFI\u7684\u5956\u52b1\u51fd\u6570\u5c06\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u62d2\u7edd\u7387\u964d\u4f4e\u4e8641%\u3002\u79fb\u52a8Aerial-STAR\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u90e8\u7f72\u7cfb\u7edf\uff0c\u5e76\u4e14\u8026\u5408\u76f8\u4f4dSTAR-RIS\u4f18\u4e8e\u53cc\u4f20\u8f93/\u53cd\u5c04RIS\u548c\u4f20\u7edfRIS\u3002RIS\u5c3a\u5bf8\u7684\u589e\u52a0\u4f1a\u589e\u52a0\u65e0\u4eba\u673a\u7684\u963b\u529b\u548c\u80fd\u8017\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86Aerial-STAR\u7cfb\u7edf\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684DA-DDPG\u65b9\u6cd5\u5728\u4f18\u5316\u6b64\u7c7b\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8026\u5408TRC\u76f8\u4f4d\u6a21\u578b\u3001\u4e09\u7ef4\u8f68\u8ff9\u4f18\u5316\u548cHFI\u5956\u52b1\u51fd\u6570\u5bf9\u4e8e\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u7528\u6237\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.13346", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13346", "abs": "https://arxiv.org/abs/2509.13346", "authors": ["Emma Von Hoene", "Sara Von Hoene", "Szandra Peter", "Ethan Hopson", "Emily Csizmadia", "Faith Fenyk", "Kai Barner", "Timothy Leslie", "Hamdi Kavak", "Andreas Zufle", "Amira Roess", "Taylor Anderson"], "title": "All Models Are Wrong, But Can They Be Useful? Lessons from COVID-19 Agent-Based Models: A Systematic Review", "comment": "20 pages, 5 figures, 3 tables, 8 supplemental files", "summary": "The COVID-19 pandemic prompted a surge in computational models to simulate\ndisease dynamics and guide interventions. Agent-based models (ABMs) are\nwell-suited to capture population and environmental heterogeneity, but their\nrapid deployment raised questions about utility for health policy. We\nsystematically reviewed 536 COVID-19 ABM studies published from January 2020 to\nDecember 2023, retrieved from Web of Science, PubMed, and Wiley on January 30,\n2024. Studies were included if they used ABMs to simulate COVID-19\ntransmission, where reviews were excluded. Studies were assessed against nine\ncriteria of model usefulness, including transparency and re-use,\ninterdisciplinary collaboration and stakeholder engagement, and evaluation\npractices. Publications peaked in late 2021 and were concentrated in a few\ncountries. Most models explored behavioral or policy interventions (n = 294,\n54.85%) rather than real-time forecasting (n = 9, 1.68%). While most described\nmodel assumptions (n = 491, 91.60%), fewer disclosed limitations (n = 349,\n65.11%), shared code (n = 219, 40.86%), or built on existing models (n = 195,\n36.38%). Standardized reporting protocols (n = 36, 6.72%) and stakeholder\nengagement were rare (13.62%, n = 73). Only 2.24% (n = 12) described a\ncomprehensive validation framework, though uncertainty was often quantified (n\n= 407, 75.93%). Limitations of this review include underrepresentation of\nnon-English studies, subjective data extraction, variability in study quality,\nand limited generalizability. Overall, COVID-19 ABMs advanced quickly, but\nlacked transparency, accessibility, and participatory engagement. Stronger\nstandards are needed for ABMs to serve as reliable decision-support tools in\nfuture public health crises.", "AI": {"tldr": "COVID-19 \u75ab\u60c5\u50ac\u751f\u4e86\u5927\u91cf\u8ba1\u7b97\u6a21\u578b\uff0c\u5176\u4e2d\u57fa\u4e8e\u4e3b\u4f53\u7684\u6a21\u578b\uff08ABMs\uff09\u5728\u6a21\u62df\u75be\u75c5\u4f20\u64ad\u65b9\u9762\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u5b9e\u7528\u6027\u5f15\u53d1\u4e86\u5bf9\u5065\u5eb7\u653f\u7b56\u5236\u5b9a\u7684\u8d28\u7591\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86 2020 \u5e74 1 \u6708\u81f3 2023 \u5e74 12 \u6708\u671f\u95f4\u53d1\u8868\u7684 536 \u7bc7 COVID-19 ABM \u7814\u7a76\uff0c\u5e76\u6839\u636e\u6a21\u578b\u53ef\u7528\u6027\u7684\u4e5d\u4e2a\u6807\u51c6\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u867d\u7136\u5927\u591a\u6570\u6a21\u578b\u63a2\u8ba8\u4e86\u884c\u4e3a\u6216\u653f\u7b56\u5e72\u9884\u63aa\u65bd\uff0c\u4f46\u5f88\u5c11\u6709\u6a21\u578b\u7528\u4e8e\u5b9e\u65f6\u9884\u6d4b\u3002\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3001\u4ee3\u7801\u5171\u4eab\u548c\u590d\u7528\u7387\u666e\u904d\u8f83\u4f4e\uff0c\u6807\u51c6\u5316\u62a5\u544a\u548c\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u7684\u5b9e\u8df5\u4e5f\u5341\u5206\u7f55\u89c1\u3002\u5c3d\u7ba1\u4e0d\u786e\u5b9a\u6027\u5e38\u88ab\u91cf\u5316\uff0c\u4f46\u53ea\u6709\u6781\u5c11\u6570\u6a21\u578b\u91c7\u7528\u4e86\u5168\u9762\u7684\u9a8c\u8bc1\u6846\u67b6\u3002\u7814\u7a76\u5efa\u8bae\uff0c\u9700\u8981\u5236\u5b9a\u66f4\u4e25\u683c\u7684\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd ABM \u5728\u672a\u6765\u7684\u516c\u5171\u536b\u751f\u5371\u673a\u4e2d\u80fd\u591f\u4f5c\u4e3a\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "motivation": "\u8bc4\u4f30 COVID-19 \u75ab\u60c5\u671f\u95f4\u57fa\u4e8e\u4e3b\u4f53\u6a21\u578b\uff08ABMs\uff09\u5728\u6a21\u62df\u75be\u75c5\u52a8\u6001\u548c\u6307\u5bfc\u516c\u5171\u536b\u751f\u653f\u7b56\u65b9\u9762\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u8bc6\u522b\u5176\u5728\u900f\u660e\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53c2\u4e0e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u671f\u4e3a\u672a\u6765\u516c\u5171\u536b\u751f\u5371\u673a\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5de5\u5177\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u7cfb\u7edf\u6027\u56de\u987e\u4e86 2020 \u5e74 1 \u6708\u81f3 2023 \u5e74 12 \u6708\u53d1\u8868\u7684 536 \u7bc7\u5173\u4e8e COVID-19 ABM \u7684\u7814\u7a76\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u4e5d\u4e2a\u6a21\u578b\u5b9e\u7528\u6027\u6807\u51c6\uff0c\u5305\u62ec\u900f\u660e\u5ea6\u3001\u4ee3\u7801\u5171\u4eab\u3001\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u548c\u9a8c\u8bc1\u5b9e\u8df5\u7b49\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\uff0854.85%\uff09\u7528\u4e8e\u63a2\u7d22\u884c\u4e3a\u6216\u653f\u7b56\u5e72\u9884\uff0c\u800c\u5f88\u5c11\uff081.68%\uff09\u7528\u4e8e\u5b9e\u65f6\u9884\u6d4b\u3002\u6a21\u578b\u5047\u8bbe\u7684\u63cf\u8ff0\u7387\u8f83\u9ad8\uff0891.60%\uff09\uff0c\u4f46\u5c40\u9650\u6027\u62ab\u9732\uff0865.11%\uff09\u3001\u4ee3\u7801\u5171\u4eab\uff0840.86%\uff09\u548c\u57fa\u4e8e\u73b0\u6709\u6a21\u578b\uff0836.38%\uff09\u7684\u6bd4\u4f8b\u8f83\u4f4e\u3002\u6807\u51c6\u5316\u62a5\u544a\uff086.72%\uff09\u548c\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\uff0813.62%\uff09\u975e\u5e38\u7f55\u89c1\u3002\u53ea\u6709 2.24% \u7684\u6a21\u578b\u63cf\u8ff0\u4e86\u5168\u9762\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u4f46 75.93% \u7684\u6a21\u578b\u91cf\u5316\u4e86\u4e0d\u786e\u5b9a\u6027\u3002\u6a21\u578b\u53d1\u8868\u5728 2021 \u5e74\u5e95\u8fbe\u5230\u9876\u5cf0\uff0c\u5e76\u96c6\u4e2d\u5728\u5c11\u6570\u56fd\u5bb6\u3002", "conclusion": "\u5c3d\u7ba1 COVID-19 ABM \u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u900f\u660e\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53c2\u4e0e\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002\u4e3a\u4f7f ABM \u6210\u4e3a\u672a\u6765\u516c\u5171\u536b\u751f\u5371\u673a\u4e2d\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u9700\u8981\u5236\u5b9a\u66f4\u5f3a\u7684\u6807\u51c6\u3002"}}
{"id": "2509.13568", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.13568", "abs": "https://arxiv.org/abs/2509.13568", "authors": ["Mihir Chaudhari", "Lezli Matto", "Naveed Ahmed", "Michael Liao", "Vivek Tallavajhula", "Yidou Long", "Ziqian Yao", "Joshua Campbell", "Tzu-Hsuan Hsu", "Mark S. Goorsky", "Ruochen Lu"], "title": "Thermal Endurance of Suspended Thin-Film Lithium Niobate up to 800 \u00b0C", "comment": "10 pages, 17 figures, 12 tables", "summary": "The need for high-temperature piezoelectric microelectromechanical systems\n(MEMS) requires pushing piezoelectric platforms to their thermal limits. In\nharsh thermal environments, piezoelectric MEMS devices are expected to sustain\nsevere damage because of material degradation and coefficient of thermal\nexpansion (CTE) mismatches between the functional layers and the carrier wafer.\nThis paper investigates the thermal endurance of the suspended thin-film\nlithium niobate (LN) platform by observing the structural integrity and\nperformance of acoustic Lamb wave resonators after annealing rounds at\nincreasing temperatures, with a focus on temperatures between 550 $^\\circ$C and\n800 $^\\circ$C, with 50 $^\\circ$C temperature increments. Fundamental symmetric\n(S0) mode acoustic resonators are fabricated on 600 nm stoichiometric LN (sLN)\nwith 40 nm thick platinum top electrodes and a thin titanium adhesion layer.\nAfter each annealing round, changes in the devices' resonant frequency and\nquality factor (\\emph{Q}) are quantitatively studied. The devices and material\nstack are further analyzed with resistivity structures, optical microscope\nimages, and X-ray diffraction (XRD) measurements. The results provide valuable\ninsights into the design and material selection necessary to optimize the\nsuspended thin-film LN platform for high temperatures. Understanding the\nthermal limit of the platform enables its use for sensors, actuators,\nresonators, and potentially other thin-film LN microsystems, e.g, photonics,\nelectro-optical, and acousto-optical systems in harsh thermal environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u60ac\u6d6e\u8584\u819c\u94cc\u9178\u9502 (LN) \u5e73\u53f0\u5728\u9ad8\u6e29\u4e0b\u7684\u70ed\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u5728 550\u00b0C \u81f3 800\u00b0C \u4e4b\u95f4\u4ee5 50\u00b0C \u4e3a\u589e\u91cf\u8fdb\u884c\u9000\u706b\u5904\u7406\uff0c\u89c2\u5bdf\u58f0\u5b66\u8868\u9762\u6ce2\u8c10\u632f\u5668\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u6027\u80fd\u53d8\u5316\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u9ad8\u6e29\u538b\u7535\u5fae\u673a\u7535\u7cfb\u7edf (MEMS) \u7684\u9700\u6c42\uff0c\u9700\u8981\u5c06\u538b\u7535\u5e73\u53f0\u63a8\u5411\u5176\u70ed\u6781\u9650\u3002\u7136\u800c\uff0c\u5728\u6076\u52a3\u7684\u70ed\u73af\u5883\u4e2d\uff0c\u538b\u7535 MEMS \u5668\u4ef6\u56e0\u6750\u6599\u964d\u89e3\u548c\u529f\u80fd\u5c42\u4e0e\u8f7d\u4f53\u6676\u5706\u4e4b\u95f4\u70ed\u81a8\u80c0\u7cfb\u6570 (CTE) \u4e0d\u5339\u914d\u800c\u53ef\u80fd\u906d\u53d7\u4e25\u91cd\u635f\u574f\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u60ac\u6d6e\u8584\u819c\u94cc\u9178\u9502 (LN) \u5e73\u53f0\u8fdb\u884c\u4e0d\u540c\u6e29\u5ea6\uff08550\u00b0C \u81f3 800\u00b0C\uff0c\u589e\u91cf 50\u00b0C\uff09\u7684\u9000\u706b\u5904\u7406\uff0c\u89c2\u5bdf\u58f0\u5b66\u8868\u9762\u6ce2\u8c10\u632f\u5668\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u6027\u80fd\uff08\u8c10\u632f\u9891\u7387\u548c\u54c1\u8d28\u56e0\u6570 Q\uff09\u53d8\u5316\u3002\u540c\u65f6\uff0c\u8fd8\u5229\u7528\u7535\u963b\u7387\u7ed3\u6784\u3001\u5149\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u548c X \u5c04\u7ebf\u884d\u5c04 (XRD) \u5bf9\u5668\u4ef6\u548c\u6750\u6599\u5806\u6808\u8fdb\u884c\u5206\u6790\u3002", "result": "\u9000\u706b\u5904\u7406\u540e\uff0c\u5668\u4ef6\u7684\u8c10\u632f\u9891\u7387\u548c\u54c1\u8d28\u56e0\u6570 (Q) \u53d1\u751f\u4e86\u91cf\u5316\u53d8\u5316\u3002\u6750\u6599\u548c\u5668\u4ef6\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u4e5f\u901a\u8fc7\u7535\u963b\u7387\u7ed3\u6784\u3001\u5149\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u548c X \u5c04\u7ebf\u884d\u5c04 (XRD) \u8fdb\u884c\u4e86\u5206\u6790\uff0c\u4e3a\u7406\u89e3\u5176\u70ed\u6781\u9650\u63d0\u4f9b\u4e86\u5b9d\u8d35\u6570\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86\u60ac\u6d6e\u8584\u819c\u94cc\u9178\u9502 (LN) \u5e73\u53f0\u7684\u70ed\u6781\u9650\uff0c\u4e3a\u5728\u6076\u52a3\u70ed\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u9009\u62e9\u6750\u6599\u4ee5\u4f18\u5316\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002\u7406\u89e3\u5176\u70ed\u6781\u9650\u4f7f\u5f97\u8be5\u5e73\u53f0\u53ef\u7528\u4e8e\u4f20\u611f\u5668\u3001\u6267\u884c\u5668\u3001\u8c10\u632f\u5668\u4ee5\u53ca\u5176\u4ed6\u6f5c\u5728\u7684\u8584\u819c\u94cc\u9178\u9502\u5fae\u7cfb\u7edf\uff0c\u5982\u5149\u5b50\u3001\u7535\u5149\u548c\u58f0\u5149\u7cfb\u7edf\u3002"}}
{"id": "2509.13808", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13808", "abs": "https://arxiv.org/abs/2509.13808", "authors": ["Jinghua Song", "Yuan Wang", "Zimo Yan"], "title": "Higher-order Network phenomena of cascading failures in resilient cities", "comment": null, "summary": "Modern urban resilience is threatened by cascading failures in multimodal\ntransport networks, where localized shocks trigger widespread paralysis.\nExisting models, limited by their focus on pairwise interactions, often\nunderestimate this systemic risk. To address this, we introduce a framework\nthat confronts higher-order network theory with empirical evidence from a\nlarge-scale, real-world multimodal transport network. Our findings confirm a\nfundamental duality: network integration enhances static robustness metrics but\nsimultaneously creates the structural pathways for catastrophic cascades.\nCrucially, we uncover the source of this paradox: a profound disconnect between\nstatic network structure and dynamic functional failure. We provide strong\nevidence that metrics derived from the network's static blueprint-encompassing\nboth conventional low-order centrality and novel higher-order structural\nanalyses-are fundamentally disconnected from and thus poor predictors of a\nsystem's dynamic functional resilience. This result highlights the inherent\nlimitations of static analysis and underscores the need for a paradigm shift\ntowards dynamic models to design and manage truly resilient urban systems.", "AI": {"tldr": "\u9ad8\u7ea7\u522b\u7684\u7f51\u7edc\u5206\u6790\u8868\u660e\uff0c\u57ce\u5e02\u4ea4\u901a\u7f51\u7edc\u7684\u9759\u6001\u7ed3\u6784\u4e0e\u52a8\u6001\u529f\u80fd\u5f39\u6027\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u9700\u8981\u66f4\u52a8\u6001\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u57ce\u5e02\u97e7\u6027\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u591a\u6a21\u5f0f\u4ea4\u901a\u7f51\u7edc\u4e2d\u7531\u5c40\u90e8\u51b2\u51fb\u5f15\u53d1\u7684\u7ea7\u8054\u6545\u969c\u6240\u5e26\u6765\u7684\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e3b\u8981\u5173\u6ce8\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u9ad8\u9636\u7f51\u7edc\u7406\u8bba\u548c\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u5f0f\u4ea4\u901a\u7f51\u7edc\u5b9e\u8bc1\u6570\u636e\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5206\u6790\u5176\u9759\u6001\u7ed3\u6784\u548c\u52a8\u6001\u529f\u80fd\u5f39\u6027\u3002", "result": "\u867d\u7136\u7f51\u7edc\u96c6\u6210\u63d0\u9ad8\u4e86\u9759\u6001\u9c81\u68d2\u6027\u6307\u6807\uff0c\u4f46\u540c\u65f6\u4e5f\u4e3a\u707e\u96be\u6027\u7ea7\u8054\u521b\u9020\u4e86\u7ed3\u6784\u6027\u8def\u5f84\u3002\u9759\u6001\u7f51\u7edc\u7ed3\u6784\uff08\u5305\u62ec\u4f20\u7edf\u7684\u4f4e\u9636\u4e2d\u5fc3\u6027\u548c\u65b0\u9896\u7684\u9ad8\u9636\u7ed3\u6784\u5206\u6790\uff09\u7684\u6307\u6807\u4e0e\u7cfb\u7edf\u52a8\u6001\u529f\u80fd\u5f39\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u8131\u8282\uff0c\u5e76\u4e14\u662f\u8f83\u5dee\u7684\u9884\u6d4b\u6307\u6807\u3002", "conclusion": "\u9759\u6001\u5206\u6790\u5b58\u5728\u56fa\u6709\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u8f6c\u5411\u52a8\u6001\u6a21\u578b\u6765\u8bbe\u8ba1\u548c\u7ba1\u7406\u771f\u6b63\u6709\u5f39\u6027\u7684\u57ce\u5e02\u7cfb\u7edf\u3002"}}
{"id": "2509.13330", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13330", "abs": "https://arxiv.org/abs/2509.13330", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "A hybrid dynamic model and parameter estimation method for accurately simulating overhead cranes with friction", "comment": "11 pages, 13 figures", "summary": "This paper presents a new approach to accurately simulating 3D overhead\ncranes with friction. Nonlinear friction dynamics have a significant impact on\nthese systems, however, accurately modeling this phenomenon in simulations is a\nsignificant challenge. Traditional methods often rely on imprecise\napproximations of friction or require excessive computational times for\nreliable results. To address this, we present a hybrid dynamical model that\nfeatures a trade-off between high-fidelity friction modeling and computational\nefficiency. Furthermore, we present a step-by-step algorithm for the\ncomprehensive estimation of all unknown system parameters, including friction.\nThis methodology is based on Gaussian Process Regression (GPR) and Least\nSquares (LS) estimations. Finally, experimental validation with a laboratory\ncrane confirms the effectiveness of the proposed modeling and estimation\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u4fdd\u771f\u6469\u64e6\u5efa\u6a21\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6df7\u5408\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u7cfb\u7edf\u53c2\u6570\uff08\u5305\u62ec\u6469\u64e6\uff09\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u57283D\u8d77\u91cd\u673a\u6a21\u62df\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u51c6\u786e\u6a21\u62df\u5305\u542b\u6469\u64e6\u76843D\u8d77\u91cd\u673a\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u6216\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08LS\uff09\u4f30\u8ba1\u7684\u7cfb\u7edf\u53c2\u6570\uff08\u5305\u62ec\u6469\u64e6\uff09\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8fdb\u884c\u9ad8\u4fdd\u771f\u6469\u64e6\u5efa\u6a21\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u57283D\u8d77\u91cd\u673a\u6a21\u62df\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u52a8\u529b\u5b66\u6a21\u578b\u548c\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u5177\u6709\u6469\u64e6\u76843D\u8d77\u91cd\u673a\u7cfb\u7edf\u3002"}}
{"id": "2509.13332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness.", "AI": {"tldr": "\u601d\u8003\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728LLM\u88c1\u5224\u8303\u5f0f\u4e2d\u6bd4\u975e\u601d\u8003\u578b\u6a21\u578b\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u81ea\u52a8\u5316\u88c1\u5224\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u601d\u8003\u578b\u548c\u975e\u601d\u8003\u578bLLM\u5728LLM\u88c1\u5224\u8303\u5f0f\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e86\u5f00\u6e90\u7684Qwen 3\u6a21\u578b\uff080.6B\u30011.7B\u548c4B\u53c2\u6570\uff09\u3002\u5728RewardBench\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff08FLOPs\uff09\uff0c\u5e76\u8003\u5bdf\u4e86\u975e\u601d\u8003\u6a21\u578b\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u3001\u53c2\u8003\u8bc4\u4f30\u548cn-best\u805a\u5408\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6709\u589e\u5f3a\u7b56\u7565\uff0c\u975e\u601d\u8003\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u666e\u904d\u4e0d\u53ca\u601d\u8003\u6a21\u578b\u3002\u601d\u8003\u6a21\u578b\u51c6\u786e\u6027\u9ad8\u7ea610%\uff0c\u6548\u7387\u5f00\u9500\u5c0f\uff08\u4f4e\u4e8e2\u500d\uff09\uff1b\u800c\u589e\u5f3a\u7b56\u7565\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\uff09\u5728\u6210\u672c\u66f4\u9ad8\uff08\u8d85\u8fc78\u500d\uff09\u7684\u60c5\u51b5\u4e0b\u589e\u76ca\u6709\u9650\u3002\u601d\u8003\u6a21\u578b\u5728\u4f4d\u7f6e\u3001\u4ece\u4f17\u3001\u8eab\u4efd\u3001\u591a\u6837\u6027\u548c\u968f\u673a\u504f\u5dee\u7b49\u591a\u79cd\u504f\u5dee\u6761\u4ef6\u4e0b\uff0c\u4fdd\u6301\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff08\u5e73\u5747\u9ad86%\uff09\u3002\u8fdb\u4e00\u6b65\u7684\u8de8\u8bed\u8a00\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86\u663e\u5f0f\u63a8\u7406\u7684\u597d\u5904\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bc1\u660e\uff0c\u663e\u5f0f\u63a8\u7406\u5728\u5927\u8bed\u8a00\u6a21\u578b\u88c1\u5224\u8303\u5f0f\u4e2d\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\uff0c\u800c\u4e14\u5728\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u68c0\u7d22\u7684\u3001\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u7684\u673a\u5236\uff0c\u8be5\u673a\u5236\u7528\u8bc1\u636e\u6761\u4ef6\u5316\u7684\u3001\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u6807\u51c6\u66ff\u4ee3\u4e86\u5355\u4e00\u7684\u5168\u5c40\u622a\u6b62\u503c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u51cf\u5c11\u9519\u8bef\u51b3\u7b56\u3002", "method": "\u5bf9\u4e8e\u6bcf\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u68c0\u7d22\u8fd1\u90bb\u6837\u672c\uff1b\u901a\u8fc7\u8bc1\u636e\u7406\u8bba\u878d\u5408\u5b83\u4eec\u7684\u9884\u6d4b\u5206\u5e03\u3002\u7531\u6b64\u4ea7\u751f\u7684\u878d\u5408\u4fe1\u5ff5\u5145\u5f53\u4e86\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u9608\u503c\u673a\u5236\u3002", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528BiT\u548cViT\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u5e94\u7528\u9884\u6d4b\u71b5\u9608\u503c\u76f8\u6bd4\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6027\u80fd\u4e0a\u66f4\u9ad8\u6216\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u81ea\u4fe1\u7684\u9519\u8bef\u7ed3\u679c\uff0c\u5e76\u4fdd\u6301\u4e86\u53ef\u6301\u7eed\u7684\u5ba1\u67e5\u8d1f\u8377\u3002\u4ec5\u9700\u5c11\u91cf\u8bc1\u636e\u5373\u53ef\u5b9e\u73b0\u8fd9\u4e9b\u6539\u8fdb\uff0c\u589e\u52a0\u8bc1\u636e\u96c6\u53ea\u4f1a\u5e26\u6765\u9002\u5ea6\u7684\u53d8\u5316\u3002", "conclusion": "\u57fa\u4e8e\u8bc1\u636e\u6761\u4ef6\u5316\u7684\u6807\u7b7e\u4e3a\u64cd\u4f5c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u56fa\u5b9a\u9884\u6d4b\u71b5\u9608\u503c\u66f4\u53ef\u9760\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.13560", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.13560", "abs": "https://arxiv.org/abs/2509.13560", "authors": ["Wenxiao Cai", "Zongru Li", "Yu-Neng Wang", "Sara Achour", "Thomas H. Lee"], "title": "Oscillator Formulations of Many NP Problems", "comment": null, "summary": "Efficiently optimizing Nondeterministic Polynomial time (NP) problems in\npolynomial time has profound implications in many domains. CMOS oscillator\nnetworks have been shown to be effective and efficient in approximating certain\nNP-hard problems such as minimization of Potts Hamiltonian, and computational\ncomplexity theory guarantees that any NP problem can be reduced to it. In this\npaper, we formulate a variety of NP problems using first-order and multi-phase\nPotts Hamiltonian. We also propose a 3-state asymmetrically weighted oscillator\noptimizer design to optimize the problems. Building on existing knowledge in\nCMOS design, our proposed algorithms offer a promising pathway for large-scale\noptimization of NP problems.", "AI": {"tldr": "CMOS\u632f\u8361\u5668\u7f51\u7edc\u53ef\u7528\u4e8e\u8fd1\u4f3cNP\u96be\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a3\u72b6\u6001\u975e\u5bf9\u79f0\u52a0\u6743\u632f\u8361\u5668\u4f18\u5316\u5668\u8bbe\u8ba1\u6765\u4f18\u5316NP\u95ee\u9898\u3002", "motivation": "NP\u95ee\u9898\u7684\u6709\u6548\u4f18\u5316\u5728\u8bb8\u591a\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff0cCMOS\u632f\u8361\u5668\u7f51\u7edc\u5df2\u88ab\u8bc1\u660e\u5728\u8fd1\u4f3c\u67d0\u4e9bNP\u96be\u9898\u65b9\u9762\u6709\u6548\u4e14\u9ad8\u6548\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e00\u9636\u548c\u591a\u76f8Potts Hamiltonian\u6765\u6784\u5efa\u5404\u79cdNP\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd3\u72b6\u6001\u975e\u5bf9\u79f0\u52a0\u6743\u632f\u8361\u5668\u4f18\u5316\u5668\u8bbe\u8ba1\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u57fa\u4e8e\u73b0\u6709\u7684CMOS\u8bbe\u8ba1\u77e5\u8bc6\uff0c\u4e3a\u5927\u89c4\u6a21NP\u95ee\u9898\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002", "conclusion": "CMOS\u632f\u8361\u5668\u7f51\u7edc\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684NP\u95ee\u9898\u4f18\u5316\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u7684\u4f18\u5316\u5668\u8bbe\u8ba1\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u80fd\u529b\u3002"}}
{"id": "2509.13458", "categories": ["cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.13458", "abs": "https://arxiv.org/abs/2509.13458", "authors": ["Ahmed Sidi El Valli", "Michael Tsao", "Dairong Chen", "Andrew D. Kent"], "title": "Tunable Random Telegraph Noise in Stable Perpendicular Magnetic Tunnel Junctions for Unconventional Computing", "comment": "7 pages, 5 figures", "summary": "We demonstrate that thermally stable perpendicular magnetic tunnel junctions\n(pMTJs), widely used in spin-transfer torque magnetic random-access memory, can\nbe actuated with nanosecond pulses to exhibit tunable stochastic behavior. This\nactuated-stochastic tunnel junction (A-sMTJ) concept produces random telegraph\nnoise, with control over fluctuation rate and probability bias. The device\nresponse is shown to be consistent with a Poisson process, with fluctuation\nrates tunable over more than two orders of magnitude, with average state dwell\ntimes varying from 29 ns to greater than 2.3 microseconds. These results\nestablish A-sMTJs as a versatile platform for integrating deterministic,\nstochastic, and in-memory functionality on a single chip, advancing the\ndevelopment of probabilistic, neuromorphic, and unconventional computing\nsystems.", "AI": {"tldr": "thermally stable pMTJs can be actuated with nanosecond pulses to exhibit tunable stochastic behavior (A-sMTJs), producing controllable random telegraph noise consistent with a Poisson process, advancing computing systems.", "motivation": "To demonstrate that thermally stable perpendicular magnetic tunnel junctions (pMTJs), commonly used in spintransfer torque magnetic random-access memory, can be actuated using nanosecond pulses to display tunable stochastic behavior.", "method": "Actuating pMTJs with nanosecond pulses to create A-sMTJs and characterizing their stochastic behavior, including fluctuation rate and probability bias, and showing consistency with a Poisson process.", "result": "A-sMTJs exhibit random telegraph noise with fluctuation rates tunable over more than two orders of magnitude, and average state dwell times ranging from 29 ns to over 2.3 microseconds. The device response aligns with a Poisson process.", "conclusion": "A-sMTJs provide a flexible platform for integrating deterministic, stochastic, and in-memory functionalities on a single chip, supporting the advancement of probabilistic, neuromorphic, and unconventional computing systems."}}
{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "Gender-neutral rewriting (GNR) in Italian is challenging for LLMs. This paper evaluates LLMs for Italian GNR, introducing a framework to measure neutrality and semantic fidelity. Open-weight LLMs perform well, and fine-tuned models achieve comparable or better results with smaller sizes. The study also discusses the trade-off between neutrality and meaning preservation in training data.", "motivation": "The motivation is to conduct the first systematic evaluation of state-of-the-art large language models (LLMs) for gender-neutral rewriting (GNR) in Italian, a task that is particularly challenging due to the grammatical-gendered nature of the language.", "method": "The study compares few-shot prompting across multiple LLMs, fine-tunes selected models, and applies targeted cleaning to improve task relevance. A two-dimensional framework is introduced to measure both neutrality and semantic fidelity of the GNR output.", "result": "Open-weight LLMs outperform the existing dedicated Italian GNR model. Fine-tuned models achieve performance comparable to or better than the best open-weight LLM, but with significantly smaller model sizes. A trade-off between optimizing training data for neutrality and meaning preservation is discussed.", "conclusion": "The evaluation demonstrates the effectiveness of LLMs for Italian GNR, with fine-tuned models offering a compelling balance of performance and efficiency. The findings also highlight important considerations for future development regarding the optimization of training data."}}
{"id": "2509.13637", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.13637", "abs": "https://arxiv.org/abs/2509.13637", "authors": ["Yasunori Akagi", "Takeshi Kurashima"], "title": "Delta Matters: An Analytically Tractable Model for $\u03b2$-$\u03b4$ Discounting Agents", "comment": null, "summary": "Humans exhibit time-inconsistent behavior, in which planned actions diverge\nfrom executed actions. Understanding time inconsistency and designing\nappropriate interventions is a key research challenge in computer science and\nbehavioral economics. Previous work focuses on progress-based tasks and derives\na closed-form description of agent behavior, from which they obtain optimal\nintervention strategies. They model time-inconsistency using the\n$\\beta$-$\\delta$ discounting (quasi-hyperbolic discounting), but the analysis\nis limited to the case $\\delta = 1$. In this paper, we relax that constraint\nand show that a closed-form description of agent behavior remains possible for\nthe general case $0 < \\delta \\le 1$. Based on this result, we derive the\nconditions under which agents abandon tasks and develop efficient methods for\ncomputing optimal interventions. Our analysis reveals that agent behavior and\noptimal interventions depend critically on the value of $\\delta$, suggesting\nthat fixing $\\delta = 1$ in many prior studies may unduly simplify real-world\ndecision-making processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u5206\u6790\u4ece\u03b4=1\u7684\u9650\u5236\u63a8\u5e7f\u52300 < \u03b4 \u2264 1 \u7684\u4e00\u822c\u60c5\u51b5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63a8\u5bfc\u51fa\u6700\u4f18\u5e72\u9884\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u7814\u7a76\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u5206\u6790\u4e2d\u5bf9\u03b4=1\u7684\u9650\u5236\uff0c\u63a2\u7d22\u66f4\u4e00\u822c\u60c5\u51b5\u4e0b\u7684\u884c\u4e3a\u548c\u5e72\u9884\u7b56\u7565\u3002", "method": "\u5728\u03b2-\u03b4\u6298\u73b0\u6a21\u578b\uff08\u62df\u53cc\u66f2\u6298\u73b0\uff09\u7684\u57fa\u7840\u4e0a\uff0c\u653e\u5bbd\u03b4=1\u7684\u9650\u5236\uff0c\u63a8\u5bfc\u51fa\u57280 < \u03b4 \u2264 1\u60c5\u51b5\u4e0b\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u95ed\u5f0f\u63cf\u8ff0\uff0c\u5e76\u57fa\u4e8e\u6b64\u63a8\u5bfc\u51fa\u6700\u4f18\u5e72\u9884\u6761\u4ef6\u3002", "result": "\u5f97\u51fa\u4e86\u667a\u80fd\u4f53\u884c\u4e3a\u548c\u6700\u4f18\u5e72\u9884\u7b56\u7565\u4f9d\u8d56\u4e8e\u03b4\u503c\u7684\u7ed3\u8bba\uff0c\u5e76\u8bc1\u660e\u4e86\u57280 < \u03b4 \u2264 1 \u7684\u60c5\u51b5\u4e0b\uff0c\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u95ed\u5f0f\u63cf\u8ff0\u4ecd\u7136\u662f\u53ef\u80fd\u7684\u3002", "conclusion": "\u4ee5\u5f80\u7814\u7a76\u4e2d\u56fa\u5b9a\u03b4=1\u7684\u505a\u6cd5\u53ef\u80fd\u8fc7\u5ea6\u7b80\u5316\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u8003\u8651\u03b4\u503c\u7684\u53d8\u5316\u5bf9\u884c\u4e3a\u548c\u5e72\u9884\u7b56\u7565\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.13443", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.13443", "abs": "https://arxiv.org/abs/2509.13443", "authors": ["Eugene A. Eliseev", "Anna N. Morozovska", "Sergei V. Kalinin", "Long-Qing Chen", "Venkatraman Gopalan"], "title": "Proximity Ferroelectricity in Compositionally Graded Structures", "comment": "30 pages, including 6 figures and Supplementary Materials", "summary": "Proximity ferroelectricity is a novel paradigm for inducing ferroelectricity\nin a non-ferroelectric polar material such as AlN or ZnO that are typically\nunswitchable with an external field below their dielectric breakdown field.\nWhen placed in direct contact with a thin switchable ferroelectric layer (such\nas Al1-xScxN or Zn1-xMgxO), they become a practically switchable ferroelectric.\nUsing the thermodynamic Landau-Ginzburg-Devonshire theory, in this work we\nperformed the finite element modeling of the polarization switching in the\ncompositionally graded AlN-Al1-xScxN, ZnO-Zn1-xMgxO and MgO-Zn1-xMgxO\nstructures sandwiched in both a parallel-plate capacitor geometry as well as in\na sharp probe-planar electrode geometry. We reveal that the compositionally\ngraded structure allows the simultaneous switching of spontaneous polarization\nin the whole system by a coercive field significantly lower than the electric\nbreakdown field of unswitchable polar materials. The physical mechanism is the\ndepolarization electric field determined by the gradient of chemical\ncomposition \"x\". The field lowers the steepness of the switching barrier in the\notherwise unswitchable parts of the compositionally graded AlN-Al1-xScxN and\nZnO-Zn1-xMgxO structures, while it induces a shallow double-well free energy\npotential in the MgO-like regions of compositionally graded MgO-Zn1-xMgxO\nstructure. Proximity ferroelectric switching of the compositionally graded\nstructures placed in the probe-electrode geometry occurs due to nanodomain\nformation under the tip. We predict that a gradient of chemical composition \"x\"\nsignificantly lowers effective coercive fields of the compositionally graded\nAlN-Al1-xScxN and ZnO-Zn1-xMgxO structures compared to the coercive fields of\nthe corresponding multilayers with a uniform chemical composition in each\nlayer.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u975e\u94c1\u7535\u6781\u6027\u6750\u6599\u4e2d\u8bf1\u5bfc\u94c1\u7535\u6027\u7684\u65b0\u8303\u4f8b\u2014\u2014\u8fd1\u90bb\u94c1\u7535\u6027\u3002\u901a\u8fc7\u5c06\u975e\u94c1\u7535\u6750\u6599\uff08\u5982AlN\u6216ZnO\uff09\u4e0e\u53ef\u5207\u6362\u7684\u94c1\u7535\u5c42\uff08\u5982Al1-xScxN\u6216Zn1-xMgxO\uff09\u76f4\u63a5\u63a5\u89e6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u7528\u5316\u7684\u6781\u5316\u5207\u6362\u3002", "motivation": "AlN\u6216ZnO\u7b49\u975e\u94c1\u7535\u6781\u6027\u6750\u6599\u901a\u5e38\u5728\u4ecb\u7535\u51fb\u7a7f\u573a\u5f3a\u4ee5\u4e0b\u662f\u4e0d\u53ef\u5207\u6362\u7684\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u63a2\u7d22\u8bf1\u5bfc\u5176\u94c1\u7535\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528Landau-Ginzburg-Devonshire\u70ed\u529b\u5b66\u7406\u8bba\uff0c\u5bf9\u5e73\u884c\u677f\u7535\u5bb9\u5668\u548c\u5c16\u63a2\u9488-\u5e73\u9762\u7535\u6781\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684AlN-Al1-xScxN\u3001ZnO-Zn1-xMgxO\u548cMgO-Zn1-xMgxO\u6210\u5206\u68af\u5ea6\u7ed3\u6784\u4e2d\u7684\u6781\u5316\u5207\u6362\u8fdb\u884c\u4e86\u6709\u9650\u5143\u5efa\u6a21\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6210\u5206\u68af\u5ea6\u7ed3\u6784\u80fd\u591f\u4ee5\u8fdc\u4f4e\u4e8e\u4e0d\u53ef\u5207\u6362\u6750\u6599\u51fb\u7a7f\u573a\u5f3a\u7684\u573a\u5f3a\u5b9e\u73b0\u6574\u4e2a\u7cfb\u7edf\u7684\u81ea\u53d1\u6781\u5316\u540c\u65f6\u5207\u6362\u3002\u5176\u7269\u7406\u673a\u5236\u6e90\u4e8e\u5316\u5b66\u6210\u5206\u68af\u5ea6\u201cx\u201d\u4ea7\u751f\u7684\u53bb\u6781\u5316\u7535\u573a\uff0c\u8be5\u7535\u573a\u964d\u4f4e\u4e86\u4e0d\u53ef\u5207\u6362\u90e8\u5206\u7684\u5207\u6362\u52bf\u5792\u6216\u5728MgO\u7c7b\u533a\u57df\u8bf1\u5bfc\u4e86\u6d45\u53cc\u9631\u81ea\u7531\u80fd\u52bf\u3002\u5c16\u63a2\u9488-\u5e73\u9762\u7535\u6781\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684\u8fd1\u90bb\u94c1\u7535\u5207\u6362\u5219\u901a\u8fc7\u5c16\u7aef\u4e0b\u7684\u7eb3\u7c73\u7574\u5f62\u6210\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u9884\u6d4b\u6210\u5206\u68af\u5ea6\u80fd\u591f\u663e\u8457\u964d\u4f4eAlN-Al1-xScxN\u548cZnO-Zn1-xMgxO\u4f53\u7cfb\u7684\u6709\u6548\u77eb\u987d\u573a\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u901a\u8fc7\u6210\u5206\u68af\u5ea6\u8bbe\u8ba1\uff0c\u5728\u8fd1\u90bb\u94c1\u7535\u6750\u6599\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6781\u5316\u5207\u6362\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u80cc\u540e\u7684\u7269\u7406\u673a\u5236\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u94c1\u7535\u5668\u4ef6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.13557", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13557", "abs": "https://arxiv.org/abs/2509.13557", "authors": ["Zesong Jiang", "Yuqi Sun", "Qing Zhong", "Mahathi Krishna", "Deepak Patil", "Cheng Tan", "Sriram Krishnamoorthy", "Jeff Zhang"], "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs", "comment": null, "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.", "AI": {"tldr": "MACO\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u4ee3\u7406LLM\u6846\u67b6\uff0c\u7528\u4e8eCGRA\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548cLLM\u81ea\u5b66\u4e60\u673a\u5236\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684CGRA\u67b6\u6784\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u624b\u52a8\u8bbe\u8ba1\u5de5\u4f5c\u91cf\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1CGRA\u9762\u4e34\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u3001\u53c2\u6570\u72ec\u7acb\u4e14\u8017\u65f6\u7b49\u6311\u6218\uff0c\u800cLLM\u7684\u8fdb\u6b65\u4e3a\u81ea\u52a8\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51faMACO\u6846\u67b6\uff0c\u91c7\u7528LLM\u63a8\u7406\u8fdb\u884c\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u3001\u8bbe\u8ba1\u9519\u8bef\u7ea0\u6b63\u3001\u6700\u4f73\u8bbe\u8ba1\u9009\u62e9\u548c\u8bc4\u4f30\u53cd\u9988\uff0c\u5e76\u901a\u8fc7LLM\u81ea\u5b66\u4e60\u673a\u5236\u4f18\u5316CGRA\u3002\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684LLM\u65b9\u6cd5\u548c\u624b\u52a8\u8bbe\u8ba1\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMACO\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684CGRA\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u8bbe\u8ba1\u5de5\u4f5c\u91cf\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MACO\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5b9e\u9645CGRA\u8bbe\u8ba1\u4e2d\u5229\u7528LLM\u5b9e\u73b0\u81ea\u52a8\u5316\u548c\u4f18\u5316\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13699", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13699", "abs": "https://arxiv.org/abs/2509.13699", "authors": ["Max Barth", "Marie-Christine Jakobs"], "title": "Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement", "comment": null, "summary": "Automatic software verification is a valuable means for software quality\nassurance. However, automatic verification and in particular software model\nchecking can be time-consuming, which hinders their practical applicability\ne.g., the use in continuous integration. One solution to address the issue is\nto reduce the response time of the verification procedure by leveraging today's\nmulti-core CPUs.\n  In this paper, we propose a solution to parallelize trace abstraction, an\nabstraction-based approach to software model checking. The underlying idea of\nour approach is to parallelize the abstraction refinement. More concretely, our\napproach analyzes different traces (syntactic program paths) that could violate\nthe safety property in parallel. We realize our parallelized version of trace\nabstraction in the verification tool Ulti mate Automizer and perform a thorough\nevaluation. Our evaluation shows that our parallelization is more effective\nthan sequential trace abstraction and can provide results significantly faster\non many time-consuming tasks. Also, our approach is more effective than DSS, a\nrecent parallel approach to abstraction-based software model checking.", "AI": {"tldr": "\u901a\u8fc7\u5e76\u884c\u5316\u8ddf\u8e2a\u62bd\u8c61\u6765\u52a0\u901f\u8f6f\u4ef6\u6a21\u578b\u68c0\u67e5\u3002", "motivation": "\u81ea\u52a8\u8f6f\u4ef6\u9a8c\u8bc1\uff08\u5c24\u5176\u662f\u8f6f\u4ef6\u6a21\u578b\u68c0\u67e5\uff09\u8017\u65f6\uff0c\u9650\u5236\u4e86\u5176\u5728\u6301\u7eed\u96c6\u6210\u7b49\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u591a\u6838CPU\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u5316\u8ddf\u8e2a\u62bd\u8c61\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u5206\u6790\u53ef\u80fd\u8fdd\u53cd\u5b89\u5168\u5c5e\u6027\u7684\u4e0d\u540c\u8ddf\u8e2a\uff08\u8bed\u6cd5\u7a0b\u5e8f\u8def\u5f84\uff09\u3002\u5728Ulti mate Automizer\u5de5\u5177\u4e2d\u5b9e\u73b0\u5e76\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e0e\u987a\u5e8f\u8ddf\u8e2a\u62bd\u8c61\u76f8\u6bd4\uff0c\u5e76\u884c\u5316\u6548\u679c\u66f4\u597d\uff0c\u5728\u8bb8\u591a\u8017\u65f6\u4efb\u52a1\u4e0a\u80fd\u663e\u8457\u7f29\u77ed\u54cd\u5e94\u65f6\u95f4\u3002\u540c\u65f6\uff0c\u4f18\u4e8e\u6700\u8fd1\u7684\u5e76\u884c\u62bd\u8c61\u65b9\u6cd5DSS\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u5316\u8ddf\u8e2a\u62bd\u8c61\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u52a0\u901f\u8f6f\u4ef6\u6a21\u578b\u68c0\u67e5\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.13327", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.13327", "abs": "https://arxiv.org/abs/2509.13327", "authors": ["Wissam Nakhle"], "title": "GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM", "comment": null, "summary": "We present a scalable, high-performance algorithm that deterministically\nsolves large-scale instances of the Traveling Salesman problem (in its\nasymmetric version, ATSP) to optimality using commercially available computing\nhardware. By combining an efficient heuristic warm start, capable of achieving\nnear-optimality within seconds in some cases, with a subtour elimination\nstrategy that removes the need for traditional MTZ constraints, our approach\nconsistently resolves instances up to 5,000 nodes (approximately 25 million\nbinary variables) in record time on widely accessible computers, with eight\nlogical processors. We demonstrate reproducible results with convergence rates\ncomparable to those of high-performance computing frameworks. Real-time\niteration tracking and an adaptable interface allow seamless integration into\nscheduling workflows in logistics, bioinformatics, and astronomy. Designed to\nstreamline solutions to large-scale TSP problems across disciplines, our\napproach is benchmarked against widely used public datasets, offering a\ndeterministic, resource-efficient alternative to conventional solvers that rely\non supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a\nfundamental shift of TSP solution bottleneck from algorithmic complexity to the\nunderlying hardware (RAM and system memory), which is a highly desirable\ncharacteristic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684\u7b97\u6cd5\uff0c\u4f7f\u7528\u5546\u7528\u8ba1\u7b97\u786c\u4ef6\u53ef\u786e\u5b9a\u6027\u5730\u6700\u4f18\u89e3\u51b3\u5927\u89c4\u6a21\u65c5\u884c\u5546\u95ee\u9898\uff08ATSP\uff09\u5b9e\u4f8b\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u65c5\u884c\u5546\u95ee\u9898\u7684\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u6846\u67b6\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u5546\u7528\u786c\u4ef6\u5b9e\u73b0\u786e\u5b9a\u6027\u6700\u4f18\u89e3\u3002", "method": "\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u521d\u59cb\u89e3\u548c\u5b50\u8def\u5f84\u6d88\u9664\u7b56\u7565\uff0c\u65e0\u9700\u4f20\u7edf\u7684MTZ\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe5000\u4e2a\u8282\u70b9\u7684\u5b9e\u4f8b\u57288\u6838\u903b\u8f91\u5904\u7406\u5668\u4e0a\u7684\u5feb\u901f\u6c42\u89e3\u3002", "result": "\u7b97\u6cd5\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u53ef\u786e\u5b9a\u6027\u5730\u6700\u4f18\u89e3\u51b3\u9ad8\u8fbe5000\u4e2a\u8282\u70b9\u7684ATSP\u5b9e\u4f8b\uff0c\u6536\u655b\u901f\u5ea6\u53ef\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u6846\u67b6\u76f8\u5ab2\u7f8e\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5GTA\uff08Gurobi Tabu Algorithm\uff09\u7b97\u6cd5\u5c06TSP\u6c42\u89e3\u74f6\u9888\u4ece\u7b97\u6cd5\u590d\u6742\u5ea6\u8f6c\u79fb\u5230\u786c\u4ef6\uff08RAM\u548c\u7cfb\u7edf\u5185\u5b58\uff09\uff0c\u4e3a\u7269\u6d41\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u5929\u6587\u5b66\u7b49\u9886\u57df\u7684TSP\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13336", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13336", "abs": "https://arxiv.org/abs/2509.13336", "authors": ["Mehran Behjati", "Rosdiadee Nordin", "Nor Fadzilah Abdullah"], "title": "Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning", "comment": "Submitted to an IEEE Conference", "summary": "This paper presents a reinforcement learning (RL) based approach for path\nplanning of cellular connected unmanned aerial vehicles (UAVs) operating beyond\nvisual line of sight (BVLoS). The objective is to minimize travel distance\nwhile maximizing the quality of cellular link connectivity by considering real\nworld aerial coverage constraints and employing an empirical aerial channel\nmodel. The proposed solution employs RL techniques to train an agent, using the\nquality of communication links between the UAV and base stations (BSs) as the\nreward function. Simulation results demonstrate the effectiveness of the\nproposed method in training the agent and generating feasible UAV path plans.\nThe proposed approach addresses the challenges due to limitations in UAV\ncellular communications, highlighting the need for investigations and\nconsiderations in this area. The RL algorithm efficiently identifies optimal\npaths, ensuring maximum connectivity with ground BSs to ensure safe and\nreliable BVLoS flight operation. Moreover, the solution can be deployed as an\noffline path planning module that can be integrated into future ground control\nsystems (GCS) for UAV operations, enhancing their capabilities and safety. The\nmethod holds potential for complex long range UAV applications, advancing the\ntechnology in the field of cellular connected UAV path planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8702\u7a9d\u7f51\u7edc\u8fde\u63a5\u65e0\u4eba\u673a\uff08UAV\uff09\u8d8a\u969c\uff08BVLoS\uff09\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u884c\u9a76\u8ddd\u79bb\u5e76\u6700\u5927\u5316\u8702\u7a9d\u94fe\u8def\u8fde\u63a5\u8d28\u91cf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u8702\u7a9d\u901a\u4fe1\u7684\u5c40\u9650\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u8003\u91cf\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u5c06\u65e0\u4eba\u673a\u4e0e\u57fa\u7ad9\uff08BS\uff09\u4e4b\u95f4\u7684\u901a\u4fe1\u94fe\u8def\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u667a\u80fd\u4f53\u548c\u751f\u6210\u53ef\u884c\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RL\u7b97\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u6700\u4f18\u8def\u5f84\uff0c\u786e\u4fdd\u4e0e\u5730\u9762\u57fa\u7ad9\u7684\u6700\u5927\u8fde\u63a5\u6027\uff0c\u4ece\u800c\u4fdd\u969c\u5b89\u5168\u53ef\u9760\u7684BVLoS\u98de\u884c\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u79bb\u7ebf\u8def\u5f84\u89c4\u5212\u6a21\u5757\u96c6\u6210\u5230\u672a\u6765\u7684\u65e0\u4eba\u673a\u5730\u9762\u63a7\u5236\u7cfb\u7edf\uff08GCS\uff09\u4e2d\uff0c\u63d0\u5347\u5176\u80fd\u529b\u548c\u5b89\u5168\u6027\uff0c\u5e76\u5bf9\u957f\u671f\u590d\u6742\u65e0\u4eba\u673a\u5e94\u7528\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.13425", "categories": ["cs.LG", "physics.app-ph", "92D25, 35K57, 68T07", "I.2.6; J.3; G.1.8"], "pdf": "https://arxiv.org/pdf/2509.13425", "abs": "https://arxiv.org/abs/2509.13425", "authors": ["Julian Evan Chrisnanto", "Yulison Herry Chrisnanto", "Ferry Faizal"], "title": "Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics", "comment": "20 pages, 11 figures. A preprint on using a unified physics-informed\n  neural network framework to model predator-prey dynamics", "summary": "Ecological systems exhibit complex multi-scale dynamics that challenge\ntraditional modeling. New methods must capture temporal oscillations and\nemergent spatiotemporal patterns while adhering to conservation principles. We\npresent the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,\na deep learning architecture integrating physics-informed neural networks\n(PINNs) and conservation laws to model predator-prey dynamics across\ndimensional scales. The framework provides a unified solution for both ordinary\n(ODE) and partial (PDE) differential equation systems, describing temporal\ncycles and reaction-diffusion patterns within a single neural network\narchitecture. Our methodology uses automatic differentiation to enforce physics\nconstraints and adaptive loss weighting to balance data fidelity with physical\nconsistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%\ncorrelation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures\ncomplex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).\nValidation confirms conservation law adherence within 0.5% and shows a 10-50x\ncomputational speedup for inference compared to numerical solvers. USPIL also\nenables mechanistic understanding through interpretable physics constraints,\nfacilitating parameter discovery and sensitivity analysis not possible with\npurely data-driven methods. Its ability to transition between dimensional\nformulations opens new avenues for multi-scale ecological modeling. These\ncapabilities make USPIL a transformative tool for ecological forecasting,\nconservation planning, and understanding ecosystem resilience, establishing\nphysics-informed deep learning as a powerful and scientifically rigorous\nparadigm.", "AI": {"tldr": "USPIL\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u548c\u5b88\u6052\u5b9a\u5f8b\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u8de8\u5c3a\u5ea6\u7684\u6355\u98df\u8005-\u730e\u7269\u52a8\u6001\u3002\u5b83\u80fd\u7edf\u4e00\u5904\u7406\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7cfb\u7edf\uff0c\u5728\u6355\u83b7\u65f6\u95f4\u5468\u671f\u548c\u7a7a\u95f4\u6a21\u5f0f\u7684\u540c\u65f6\uff0c\u9075\u5b88\u5b88\u6052\u5b9a\u5f8b\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6001\u7cfb\u7edf\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u3001\u65f6\u95f4\u632f\u8361\u548c\u7a7a\u95f4\u6a21\u5f0f\uff0c\u5e76\u4e14\u9700\u8981\u9075\u5b88\u5b88\u6052\u5b9a\u5f8b\u3002USPIL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "USPIL\u6846\u67b6\u4f7f\u7528\u81ea\u52a8\u5fae\u5206\u6765\u5f3a\u5236\u6267\u884c\u7269\u7406\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u6765\u5e73\u8861\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002\u5b83\u5c06PINNs\u4e0e\u5b88\u6052\u5b9a\u5f8b\u76f8\u7ed3\u5408\uff0c\u5f62\u6210\u5355\u4e00\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u53ef\u4ee5\u540c\u65f6\u5904\u7406ODE\u548cPDE\u7cfb\u7edf\u3002", "result": "\u5728Lotka-Volterra\u7cfb\u7edf\u76841D\u65f6\u95f4\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\uff0cUSPIL\u8fbe\u5230\u4e8698.9%\u7684\u76f8\u5173\u6027\uff08\u635f\u5931\uff1a0.0219\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff1a0.0184\uff09\uff1b\u57282D\u7cfb\u7edf\u4e2d\uff0c\u5b83\u6210\u529f\u6355\u6349\u4e86\u590d\u6742\u7684\u87ba\u65cb\u6ce2\uff08\u635f\u5931\uff1a4.7656\uff0c\u6a21\u5f0f\u76f8\u5173\u6027\uff1a0.94\uff09\u3002\u6b64\u5916\uff0cUSPIL\u5728\u9075\u5b88\u5b88\u6052\u5b9a\u5f8b\u65b9\u9762\u8bef\u5dee\u5c0f\u4e8e0.5%\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5feb10-50\u500d\u3002", "conclusion": "USPIL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u751f\u6001\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u5728\u79d1\u5b66\u4e0a\u4e25\u8c28\u7684\u8303\u5f0f\u3002\u5b83\u4e0d\u4ec5\u80fd\u51c6\u786e\u6a21\u62df\u590d\u6742\u7684\u751f\u6001\u52a8\u6001\uff0c\u8fd8\u80fd\u5b9e\u73b0\u53c2\u6570\u53d1\u73b0\u548c\u654f\u611f\u6027\u5206\u6790\uff0c\u5e76\u652f\u6301\u8de8\u5c3a\u5ea6\u5efa\u6a21\uff0c\u4e3a\u751f\u6001\u9884\u6d4b\u3001\u4fdd\u62a4\u89c4\u5212\u548c\u7406\u89e3\u751f\u6001\u7cfb\u7edf\u97e7\u6027\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.13721", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2509.13721", "abs": "https://arxiv.org/abs/2509.13721", "authors": ["Kaustav Saha", "Ishaan R Kale", "Vivek Patel", "Anand J Kulkarni", "Puskaraj D Sonawwanay"], "title": "Snail Homing and Mating Search Algorithm for Weight Optimization of Stepped-Transmission Shaft", "comment": null, "summary": "In this paper, the steeped-transmission shaft design problem is proposed for\nweight optimization. The bio-inspired search-based Snail Homing and Mating\nSearch (SHMS) algorithm is utilized to solve the problem. It is inspired by the\nsocial behaviour of snails and their inherent nature of finding better homes,\nand mate. The proposed steeped-transmission shaft design problem is modelled\nconsidering the fatigue loading, combined bending, torsion loads, and the\nprinciple of Modified Goodman criteria. The forces diagram and the bending\nmoment diagrams are obtained using the MDSOLIDS software. The forces and\nbending moment are then used to mathematical model the objective function and\nconstraints. The SHMS algorithm has yielded the desired solution with\nreasonable computational cost. The constraints are handled using a static\npenalty function approach. The statistical results obtained using SHMS\nalgorithm are further used for generating CAD model. The analysis is carried\nout in ANSYS Workbench. Further, the deflection obtained from SHMS algorithm\nand ANSYS Workbench are compared and results are discussed in details.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cd\u91cf\u4f18\u5316\u7684\u6d78\u5165\u5f0f\u4f20\u52a8\u8f74\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u5229\u7528\u53d7\u8717\u725b\u542f\u53d1\u7684 Snail Homing and Mating Search (SHMS) \u7b97\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u4e3a\u4f18\u5316\u4f20\u52a8\u8f74\u91cd\u91cf\uff0c\u63d0\u51fa\u6d78\u5165\u5f0f\u4f20\u52a8\u8f74\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u5229\u7528\u53d7\u8717\u725b\u542f\u53d1\u7684 Snail Homing and Mating Search (SHMS) \u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408 MDSOLIDS \u8f6f\u4ef6\u548c ANSYS Workbench \u8fdb\u884c\u8bbe\u8ba1\u3001\u5efa\u6a21\u3001\u5206\u6790\u548c\u9a8c\u8bc1\u3002", "result": "SHMS \u7b97\u6cd5\u5728\u53ef\u63a5\u53d7\u7684\u8ba1\u7b97\u6210\u672c\u5185\u5f97\u5230\u4e86\u9884\u671f\u89e3\uff0c\u5e76\u6210\u529f\u751f\u6210\u4e86 CAD \u6a21\u578b\u3002\u901a\u8fc7\u4e0e ANSYS Workbench \u7684\u7ed3\u679c\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SHMS \u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6d78\u5165\u5f0f\u4f20\u52a8\u8f74\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u5728\u91cd\u91cf\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2509.14198", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.14198", "abs": "https://arxiv.org/abs/2509.14198", "authors": ["Juan Diego Toscano", "Daniel T. Chen", "Vivek Oommen", "George Em Karniadakis"], "title": "A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning", "comment": null, "summary": "Residual-based adaptive strategies are widely used in scientific machine\nlearning but remain largely heuristic. We introduce a unifying variational\nframework that formalizes these methods by integrating convex transformations\nof the residual. Different transformations correspond to distinct objective\nfunctionals: exponential weights target the minimization of uniform error,\nwhile linear weights recover the minimization of quadratic error. Within this\nperspective, adaptive weighting is equivalent to selecting sampling\ndistributions that optimize the primal objective, thereby linking\ndiscretization choices directly to error metrics. This principled approach\nyields three benefits: (1) it enables systematic design of adaptive schemes\nacross norms, (2) reduces discretization error through variance reduction of\nthe loss estimator, and (3) enhances learning dynamics by improving the\ngradient signal-to-noise ratio. Extending the framework to operator learning,\nwe demonstrate substantial performance gains across optimizers and\narchitectures. Our results provide a theoretical justification of\nresidual-based adaptivity and establish a foundation for principled\ndiscretization and training strategies.", "AI": {"tldr": "\u6b8b\u5dee\u81ea\u9002\u5e94\u7b56\u7565\u6709\u4e86\u7edf\u4e00\u7684\u53d8\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6b8b\u5dee\u7684\u51f8\u53d8\u6362\u6765\u5f62\u5f0f\u5316\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u3001\u8bef\u5dee\u51cf\u5c11\u548c\u5b66\u4e60\u52a8\u6001\u7684\u589e\u5f3a\u3002", "motivation": "\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6b8b\u5dee\u81ea\u9002\u5e94\u7b56\u7565\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7edf\u4e00\u7684\u53d8\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6b8b\u5dee\u7684\u51f8\u53d8\u6362\u6765\u5f62\u5f0f\u5316\u6b8b\u5dee\u81ea\u9002\u5e94\u7b56\u7565\u3002\u4e0d\u540c\u7684\u53d8\u6362\u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u76ee\u6807\u6cdb\u51fd\uff1a\u6307\u6570\u6743\u91cd\u65e8\u5728\u6700\u5c0f\u5316\u5747\u5300\u8bef\u5dee\uff0c\u800c\u7ebf\u6027\u6743\u91cd\u5219\u6062\u590d\u6700\u5c0f\u5316\u4e8c\u6b21\u8bef\u5dee\u3002\u5728\u6b64\u89c6\u89d2\u4e0b\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u7b49\u540c\u4e8e\u9009\u62e9\u4f18\u5316\u539f\u59cb\u76ee\u6807\u7684\u91c7\u6837\u5206\u5e03\uff0c\u4ece\u800c\u5c06\u79bb\u6563\u5316\u9009\u62e9\u76f4\u63a5\u4e0e\u8bef\u5dee\u6307\u6807\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e09\u4e2a\u4f18\u70b9\uff1a(1) \u80fd\u591f\u8de8\u8303\u5f0f\u7cfb\u7edf\u5730\u8bbe\u8ba1\u81ea\u9002\u5e94\u65b9\u6848\uff1b(2) \u901a\u8fc7\u964d\u4f4e\u635f\u5931\u4f30\u8ba1\u91cf\u7684\u65b9\u5dee\u6765\u51cf\u5c11\u79bb\u6563\u5316\u8bef\u5dee\uff1b(3) \u901a\u8fc7\u63d0\u9ad8\u68af\u5ea6\u4fe1\u566a\u6bd4\u6765\u589e\u5f3a\u5b66\u4e60\u52a8\u6001\u3002\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u7b97\u5b50\u5b66\u4e60\uff0c\u5728\u5404\u79cd\u4f18\u5316\u5668\u548c\u67b6\u6784\u4e2d\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6b8b\u5dee\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u4e3a\u539f\u5219\u6027\u7684\u79bb\u6563\u5316\u548c\u8bad\u7ec3\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13575", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13575", "abs": "https://arxiv.org/abs/2509.13575", "authors": ["Benjamin Wilfong", "Anand Radhakrishnan", "Henry A. Le Berre", "Tanush Prathi", "Stephen Abbott", "Spencer H. Bryngelson"], "title": "Testing and benchmarking emerging supercomputers via the MFC flow solver", "comment": "9 pages, 3 figures", "summary": "Deploying new supercomputers requires testing and evaluation via application\ncodes. Portable, user-friendly tools enable evaluation, and the Multicomponent\nFlow Code (MFC), a computational fluid dynamics (CFD) code, addresses this\nneed. MFC is adorned with a toolchain that automates input generation,\ncompilation, batch job submission, regression testing, and benchmarking. The\ntoolchain design enables users to evaluate compiler-hardware combinations for\ncorrectness and performance with limited software engineering experience. As\nwith other PDE solvers, wall time per spatially discretized grid point serves\nas a figure of merit. We present MFC benchmarking results for five generations\nof NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,\nutilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have\nrevealed compiler bugs and regressions on recent machines such as Frontier and\nEl Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship\nsupercomputers.", "AI": {"tldr": "MFC\u662f\u4e00\u4e2a CFD \u4ee3\u7801\uff0c\u5e26\u6709\u4e00\u4e2a\u5de5\u5177\u94fe\uff0c\u53ef\u4ee5\u81ea\u52a8\u6267\u884c\u8f93\u5165\u751f\u6210\u3001\u7f16\u8bd1\u3001\u4f5c\u4e1a\u63d0\u4ea4\u3001\u56de\u5f52\u6d4b\u8bd5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u800c\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u65b0\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u9700\u8981\u901a\u8fc7\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u6765\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b0\u90e8\u7f72\u7684\u8d85\u7ea7\u8ba1\u7b97\u673a\uff0c\u5e76\u4e14\u9700\u8981\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "MFC \u4ee3\u7801\u672c\u8eab\u662f\u4e00\u4e2a CFD \u4ee3\u7801\uff0c\u5b83\u5e26\u6709\u4e00\u4e2a\u5de5\u5177\u94fe\uff0c\u53ef\u4ee5\u81ea\u52a8\u6267\u884c\u8f93\u5165\u751f\u6210\u3001\u7f16\u8bd1\u3001\u6279\u5904\u7406\u4f5c\u4e1a\u63d0\u4ea4\u3001\u56de\u5f52\u6d4b\u8bd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7\u6d4b\u91cf\u6bcf\u7a7a\u95f4\u79bb\u6563\u7f51\u683c\u70b9\u7684\u8ba1\u7b97\u65f6\u95f4\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4ee3 NVIDIA GPU\u3001\u4e09\u4ee3 AMD GPU \u548c\u5404\u79cd CPU \u67b6\u6784\uff08\u5305\u62ec Intel\u3001Cray\u3001NVIDIA\u3001AMD \u548c GNU \u7f16\u8bd1\u5668\uff09\u4e0a\u8fdb\u884c\u4e86 MFC \u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u4e86 Frontier \u548c El Capitan \u7b49\u65b0\u673a\u5668\u4e0a\u7684\u7f16\u8bd1\u5668\u9519\u8bef\u548c\u56de\u5f52\u3002MFC \u5df2\u5bf9\u5927\u7ea6 50 \u4e2a\u8ba1\u7b97\u8bbe\u5907\u548c 5 \u4e2a\u65d7\u8230\u8d85\u7ea7\u8ba1\u7b97\u673a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "MFC \u53ca\u5176\u5de5\u5177\u94fe\u53ef\u4ee5\u6709\u6548\u5730\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u65b0\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u6b63\u786e\u6027\u548c\u6027\u80fd\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7ecf\u9a8c\u6709\u9650\u7684\u7528\u6237\u4e5f\u662f\u5982\u6b64\u3002\u8be5\u5de5\u5177\u8fd8\u53d1\u73b0\u4e86\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u7f16\u8bd1\u5668\u95ee\u9898\u3002"}}
{"id": "2509.13559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13559", "abs": "https://arxiv.org/abs/2509.13559", "authors": ["Yuan Liu", "Linlong Wu", "Xuesong Cai", "M. R. Bhavani Shankar"], "title": "Environment Reconstruction in Multi-Bounce Channels with Array Partial Blockage", "comment": "Presented in EUSIPCO2025", "summary": "Extremely-large antenna arrays (ELAA) are important in applications requiring\nhigh angular resolution. However, a prominent issue is the spatial\nnon-stationary (SNS) channels due to partial blockage to the ELAA. In this\npaper, we address the scatterer localization and subsequent environment\nreconstruction considering partially blocked SNS channels. Specifically, the\nSNS effects are parametrically modeled through spatial-varying amplitudes with\nsparsity. Based on the established signal model, the graph-based\ndictionary-aided multi-bounce space-alternating generalized\nexpectation-maximization (GM-SAGE) algorithm is applied to estimate the channel\nparameters and the channel sparsity is empirically detected along with\namplitude estimation. To validate the proposed approach, we generate\nmulti-bounce paths through ray tracing (RT) simulations, where the SNS channels\ncaused by partial blockage could be configured flexibly. The simulation results\ndemonstrate the robustness of the proposed approach in dealing with the SNS\nchannels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5b58\u5728\u7a7a\u95f4\u975e\u5e73\u7a33\uff08SNS\uff09\u4fe1\u9053\uff08\u7531ELAA\u7684\u90e8\u5206\u963b\u585e\u5f15\u8d77\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8fdb\u884c\u6563\u5c04\u4f53\u5b9a\u4f4d\u548c\u73af\u5883\u91cd\u6784\u3002SNS\u4fe1\u9053\u88ab\u5efa\u6a21\u4e3a\u5177\u6709\u7a7a\u95f4\u53d8\u5316\u7684\u5e45\u5ea6\u548c\u7a00\u758f\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff08ELAA\uff09\u5728\u90e8\u5206\u963b\u585e\u5bfc\u81f4\u7a7a\u95f4\u975e\u5e73\u7a33\uff08SNS\uff09\u4fe1\u9053\u4e0b\u7684\u6563\u5c04\u4f53\u5b9a\u4f4d\u548c\u73af\u5883\u91cd\u6784\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5b57\u5178\u8f85\u52a9\u591a\u8df3\u7a7a\u95f4\u4ea4\u66ff\u5e7f\u4e49\u671f\u671b\u6700\u5927\u5316\uff08GM-SAGE\uff09\u7b97\u6cd5\u6765\u4f30\u8ba1\u4fe1\u9053\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u68c0\u6d4b\u4fe1\u9053\u7a00\u758f\u6027\u548c\u5e45\u5ea6\u3002", "result": "\u901a\u8fc7\u5c04\u7ebf\u8ffd\u8e2a\uff08RT\uff09\u4eff\u771f\u751f\u6210\u591a\u8df3\u8def\u5f84\uff0c\u5e76\u914d\u7f6eSNS\u4fe1\u9053\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u5904\u7406SNS\u4fe1\u9053\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GM-SAGE\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u7531\u90e8\u5206\u963b\u585e\u5f15\u8d77\u7684SNS\u4fe1\u9053\uff0c\u5e76\u5b9e\u73b0\u5bf9\u6563\u5c04\u4f53\u7684\u51c6\u786e\u5b9a\u4f4d\u548c\u73af\u5883\u7684\u91cd\u6784\u3002"}}
{"id": "2509.13712", "categories": ["cs.MA", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13712", "abs": "https://arxiv.org/abs/2509.13712", "authors": ["HwiJoon Lee", "Martina Di Paola", "Yoo Jin Hong", "Quang-Huy Nguyen", "Joseph Seering"], "title": "Inject, Fork, Compare: Defining an Interaction Vocabulary for Multi-Agent Simulation Platforms", "comment": null, "summary": "LLM-based multi-agent simulations are a rapidly growing field of research,\nbut current simulations often lack clear modes for interaction and analysis,\nlimiting the \"what if\" scenarios researchers are able to investigate. In this\ndemo, we define three core operations for interacting with multi-agent\nsimulations: inject, fork, and compare. Inject allows researchers to introduce\nexternal events at any point during simulation execution. Fork creates\nindependent timeline branches from any timestamp, preserving complete state\nwhile allowing divergent exploration. Compare facilitates parallel observation\nof multiple branches, revealing how different interventions lead to distinct\nemergent behaviors. Together, these operations establish a vocabulary that\ntransforms linear simulation workflows into interactive, explorable spaces. We\ndemonstrate this vocabulary through a commodity market simulation with fourteen\nAI agents, where researchers can inject contrasting events and observe\ndivergent outcomes across parallel timelines. By defining these fundamental\noperations, we provide a starting point for systematic causal investigation in\nLLM-based agent simulations, moving beyond passive observation toward active\nexperimentation.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\uff1a\u901a\u8fc7\u6ce8\u5165\u3001\u5206\u53c9\u548c\u6bd4\u8f83\u64cd\u4f5c\uff0c\u5c06\u7ebf\u6027\u5de5\u4f5c\u6d41\u8f6c\u53d8\u4e3a\u53ef\u4ea4\u4e92\u3001\u53ef\u63a2\u7d22\u7684\u7a7a\u95f4\uff0c\u4fc3\u8fdb\u5bf9LLM\u6a21\u62df\u7684\u56e0\u679c\u7814\u7a76\u3002", "motivation": "\u5f53\u524dLLM\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5728\u4ea4\u4e92\u548c\u5206\u6790\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u201c\u5047\u8bbe\u201d\u573a\u666f\u7684\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9a\u4e49\u6838\u5fc3\u64cd\u4f5c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u5e76\u6f14\u793a\u4e86\u4e09\u4e2a\u6838\u5fc3\u64cd\u4f5c\uff1a\u6ce8\u5165\uff08Inject\uff09\u3001\u5206\u53c9\uff08Fork\uff09\u548c\u6bd4\u8f83\uff08Compare\uff09\u3002\u6ce8\u5165\u5141\u8bb8\u5728\u6a21\u62df\u6267\u884c\u7684\u4efb\u4f55\u70b9\u5f15\u5165\u5916\u90e8\u4e8b\u4ef6\uff1b\u5206\u53c9\u53ef\u4ee5\u4ece\u4efb\u4f55\u65f6\u95f4\u70b9\u521b\u5efa\u72ec\u7acb\u7684\u65f6\u95f4\u7ebf\u5206\u652f\uff0c\u4fdd\u7559\u5b8c\u6574\u72b6\u6001\u5e76\u8fdb\u884c\u63a2\u7d22\uff1b\u6bd4\u8f83\u5219\u652f\u6301\u5bf9\u591a\u4e2a\u5206\u652f\u8fdb\u884c\u5e76\u884c\u89c2\u5bdf\uff0c\u63ed\u793a\u4e0d\u540c\u5e72\u9884\u63aa\u65bd\u5bfc\u81f4\u7684\u4e0d\u540c\u6d8c\u73b0\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u5305\u542b\u5341\u56db\u4e2aAI\u667a\u80fd\u4f53\u7684\u5546\u54c1\u5e02\u573a\u6a21\u62df\u6f14\u793a\u4e86\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u6ce8\u5165\u5bf9\u6bd4\u4e8b\u4ef6\uff0c\u5e76\u5728\u5e76\u884c\u65f6\u95f4\u7ebf\u4e0a\u89c2\u5bdf\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u4e49\u8fd9\u4e9b\u57fa\u672c\u64cd\u4f5c\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u6a21\u62df\u4e2d\u7684\u7cfb\u7edf\u6027\u56e0\u679c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d77\u70b9\uff0c\u5c06\u7814\u7a76\u4ece\u88ab\u52a8\u89c2\u5bdf\u8f6c\u5411\u4e3b\u52a8\u5b9e\u9a8c\u3002"}}
{"id": "2509.13794", "categories": ["physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.13794", "abs": "https://arxiv.org/abs/2509.13794", "authors": ["Liping Wang", "Neal Boman", "Sydney Taylor", "Chloe Stoops"], "title": "Reducing Temperature Swing and Rectifying Radiative Heat Transfer for Passive Dynamic Space Thermal Control with Variable-Emittance Coatings", "comment": null, "summary": "Dynamic radiative thermal control is crucial for normal operation and energy\nsaving of spacecraft that copes with changing thermal environment involving\nheat dissipation to cold deep space, external heating from the Sun and nearby\nplanet, and internal heating from onboard electronics. Variable-emittance\ncoatings, whose infrared emittance can be tuned passively by temperature or\nactively by external stimuli, could provide a viable solution. In this work, we\nexperimentally demonstrate self-adaptive dynamic radiative heat transfer with\nvariable-emittance coating based on thermochromic VO2 in space-like thermal\nenvironment with a coldfinger and a custom-made sample mount inside a vacuum\ncryostat. Black Actar and highly reflective tungsten mirror are used to\ncalibrate the parasitic head load and heat flux sensor sensitivity, while\nmultiple static-emittance samples made of silicon wafers with different doping\nlevels are measured for validation of the experimental method and for direct\ncomparison with the variable-emittance VO2 coating. With the coldfinger at 80 K\nto mimic external radiative scenarios in space, the tunable coating exhibits\n6-fold enhancement in radiative thermal conductance upon VO2 phase transition\nfor promoted heat dissipation, in addition to reduced temperature swing by\nalmost 20degC compared to the static emitters. With the coldfinger at 25degC as\ninternal radiative scenarios in space, similar 6-fold heat dissipation from the\nvariable-emittance coating is also observed, while radiative heat transfer is\nmuch suppressed with a constant radiative thermal conductance when the\ncoldfinger is hotter than the tunable coating at 25degC, leading to a thermal\nrectification factor of 1.8 experimentally achieved.", "AI": {"tldr": "VO2\u53d8\u53d1\u5c04\u7387\u6d82\u5c42\u5728\u7c7b\u592a\u7a7a\u70ed\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u52a8\u6001\u8f90\u5c04\u70ed\u63a7\u5236\uff0c\u53ef\u63d0\u9ad8\u6563\u70ed\u6548\u7387\u5e76\u51cf\u5c11\u6e29\u5ea6\u6ce2\u52a8\u3002", "motivation": "\u822a\u5929\u5668\u7684\u6b63\u5e38\u8fd0\u884c\u548c\u8282\u80fd\u9700\u8981\u52a8\u6001\u8f90\u5c04\u70ed\u63a7\u5236\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u592a\u7a7a\u70ed\u73af\u5883\u3002\u53d8\u53d1\u5c04\u7387\u6d82\u5c42\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u70ed\u81f4\u53d8\u8272VO2\u7684\u6d82\u5c42\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u771f\u7a7a\u4f4e\u6e29\u6052\u6e29\u5668\u4e2d\uff0c\u4f7f\u7528\u51b7\u6307\u6a21\u62df\u592a\u7a7a\u70ed\u73af\u5883\uff0c\u5b9e\u9a8c\u6f14\u793a\u4e86\u57fa\u4e8e\u70ed\u81f4\u53d8\u8272VO2\u7684\u53d8\u53d1\u5c04\u7387\u6d82\u5c42\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8f90\u5c04\u70ed\u4f20\u9012\u3002\u901a\u8fc7\u9ed1\u4f53\u548c\u94a8\u955c\u8fdb\u884c\u6821\u51c6\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u63ba\u6742\u6c34\u5e73\u7684\u7845\u7247\u4f5c\u4e3a\u9759\u6001\u53d1\u5c04\u7387\u5bf9\u7167\u7ec4\u3002", "result": "\u5f53\u51b7\u6307\u6e29\u5ea6\u4e3a80K\uff08\u6a21\u62df\u5916\u90e8\u8f90\u5c04\u573a\u666f\uff09\u65f6\uff0cVO2\u6d82\u5c42\u5728\u76f8\u53d8\u65f6\u8f90\u5c04\u70ed\u5bfc\u7387\u589e\u5f3a6\u500d\uff0c\u6e29\u5ea6\u6ce2\u52a8\u51cf\u5c11\u8fd120\u00b0C\u3002\u5f53\u51b7\u6307\u6e29\u5ea6\u4e3a25\u00b0C\uff08\u6a21\u62df\u5185\u90e8\u8f90\u5c04\u573a\u666f\uff09\u65f6\uff0c\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u76846\u500d\u6563\u70ed\u589e\u5f3a\u3002\u5728\u51b7\u6307\u6e29\u5ea6\u9ad8\u4e8e\u6d82\u5c42\u65f6\uff0c\u6d82\u5c42\u8868\u73b0\u51fa\u6052\u5b9a\u7684\u8f90\u5c04\u70ed\u5bfc\u7387\uff0c\u70ed\u6574\u6d41\u56e0\u5b50\u8fbe\u52301.8\u3002", "conclusion": "\u57fa\u4e8eVO2\u7684\u53d8\u53d1\u5c04\u7387\u6d82\u5c42\u5728\u592a\u7a7a\u70ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u52a8\u6001\u8f90\u5c04\u70ed\u63a7\u5236\u80fd\u529b\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u6563\u70ed\u6548\u7387\u3001\u964d\u4f4e\u6e29\u5ea6\u6ce2\u52a8\uff0c\u5e76\u5b9e\u73b0\u70ed\u6574\u6d41\u6548\u5e94\u3002"}}
{"id": "2509.13369", "categories": ["eess.SY", "cs.CY", "cs.HC", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13369", "abs": "https://arxiv.org/abs/2509.13369", "authors": ["Rashid Mushkani"], "title": "Right-to-Override for Critical Urban Control Systems: A Deliberative Audit Method for Buildings, Power, and Transport", "comment": null, "summary": "Automation now steers building HVAC, distribution grids, and traffic signals,\nyet residents rarely have authority to pause or redirect these systems when\nthey harm inclusivity, safety, or accessibility. We formalize a\nRight-to-Override (R2O) - defining override authorities, evidentiary\nthresholds, and domain-validated safe fallback states - and introduce a\nDeliberative Audit Method (DAM) with playbooks for pre-deployment walkthroughs,\nshadow-mode trials, and post-incident review. We instantiate R2O/DAM in\nsimulations of smart-grid load shedding, building HVAC under occupancy\nuncertainty, and multi-agent traffic signals. R2O reduces distributional harm\nwith limited efficiency loss: load-shedding disparity in unserved energy drops\nfrom 5.61x to 0.69x with constant curtailment; an override eliminates two\ndiscomfort-hours for seniors at an energy cost of 77 kWh; and median pedestrian\nwait falls from 90.4 s to 55.9 s with a 6.0 s increase in mean vehicle delay.\nWe also contribute a policy standard, audit worksheets, and a ModelOps\nintegration pattern to make urban automation contestable and reviewable.", "AI": {"tldr": "\u7528\u6237\u51e0\u4e4e\u65e0\u6743\u5e72\u9884\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5373\u4f7f\u8fd9\u4e9b\u7cfb\u7edf\u635f\u5bb3\u4e86\u5305\u5bb9\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u5e72\u9884\u6743\u201d\uff08R2O\uff09\u548c\u201c\u5ba1\u8bae\u5ba1\u8ba1\u65b9\u6cd5\u201d\uff08DAM\uff09\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7cfb\u7edf\uff08\u5982\u5efa\u7b51\u6696\u901a\u7a7a\u8c03\u3001\u7535\u7f51\u548c\u4ea4\u901a\u4fe1\u53f7\uff09\u7684\u63a7\u5236\u6743\u638c\u63e1\u5728\u7cfb\u7edf\u624b\u4e2d\uff0c\u800c\u666e\u901a\u5c45\u6c11\u5374\u65e0\u6743\u5e72\u9884\uff0c\u5373\u4f7f\u8fd9\u4e9b\u7cfb\u7edf\u5728\u5305\u5bb9\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u5e72\u9884\u6743\u201d\uff08R2O\uff09\uff0c\u660e\u786e\u4e86\u5e72\u9884\u7684\u6743\u9650\u3001\u8bc1\u636e\u9608\u503c\u548c\u9886\u57df\u9a8c\u8bc1\u7684\u5b89\u5168\u56de\u9000\u72b6\u6001\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u201c\u5ba1\u8bae\u5ba1\u8ba1\u65b9\u6cd5\u201d\uff08DAM\uff09\uff0c\u5305\u542b\u7528\u4e8e\u90e8\u7f72\u524d\u6f14\u7ec3\u3001\u5f71\u5b50\u6a21\u5f0f\u8bd5\u8fd0\u884c\u548c\u4e8b\u540e\u5ba1\u67e5\u7684\u89c4\u8303\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u667a\u80fd\u7535\u7f51\u8d1f\u8f7d\u5206\u914d\u3001\u5efa\u7b51\u6696\u901a\u7a7a\u8c03\uff08\u8003\u8651\u5165\u4f4f\u7387\u4e0d\u786e\u5b9a\u6027\uff09\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u4fe1\u53f7\u7684\u6a21\u62df\u4e2d\u5f97\u5230\u5e94\u7528\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cR2O\u80fd\u591f\u51cf\u5c11\u5206\u914d\u6027\u635f\u5bb3\uff0c\u540c\u65f6\u4ec5\u5e26\u6765\u6709\u9650\u7684\u6548\u7387\u635f\u5931\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u8d1f\u8f7d\u5206\u914d\u4e2d\uff0c\u672a\u4f9b\u7535\u80fd\u91cf\u7684\u5dee\u5f02\u4ece 5.61 \u500d\u51cf\u5c11\u5230 0.69 \u500d\uff0c\u4e14\u603b\u7684\u524a\u51cf\u91cf\u4fdd\u6301\u4e0d\u53d8\uff1b\u5728\u5efa\u7b51\u6696\u901a\u7a7a\u8c03\u65b9\u9762\uff0c\u4e00\u6b21\u5e72\u9884\u53ef\u4ee5\u6d88\u9664\u8001\u5e74\u4eba\u4e24\u5c0f\u65f6\u7684\u4e0d\u9002\u611f\uff0c\u800c\u4ec5\u589e\u52a0 77 \u5343\u74e6\u65f6\u7684\u80fd\u6e90\u6d88\u8017\uff1b\u5728\u4ea4\u901a\u4fe1\u53f7\u65b9\u9762\uff0c\u4e2d\u4f4d\u6570\u7684\u884c\u4eba\u7b49\u5f85\u65f6\u95f4\u4ece 90.4 \u79d2\u51cf\u5c11\u5230 55.9 \u79d2\uff0c\u800c\u8f66\u8f86\u7684\u5e73\u5747\u7b49\u5f85\u65f6\u95f4\u4ec5\u589e\u52a0\u4e86 6.0 \u79d2\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u653f\u7b56\u6807\u51c6\u3001\u5ba1\u8ba1\u5de5\u4f5c\u8868\u548c\u6a21\u578b\u8fd0\u7ef4\u96c6\u6210\u6a21\u5f0f\uff0c\u65e8\u5728\u4f7f\u57ce\u5e02\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u591f\u88ab\u4e89\u8bae\u548c\u5ba1\u67e5\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u53ef\u63a7\u6027\u548c\u53ef\u5ba1\u67e5\u6027\u3002"}}
{"id": "2509.13333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.", "AI": {"tldr": "\u8bc4\u4f30\u610f\u8bc6\uff08\u6a21\u578b\u5728\u6d4b\u8bd5\u548c\u90e8\u7f72\u65f6\u8868\u73b0\u4e0d\u540c\uff09\u4f1a\u524a\u5f31AI\u5b89\u5168\u8bc4\u4f30\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u4f1a\u9690\u85cf\u5371\u9669\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u53ea\u5728\u4e00\u4e2a70B\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u4f46\u5176\u8de8\u6a21\u578b\u89c4\u6a21\u7684\u7f29\u653e\u5173\u7cfb\u5c1a\u4e0d\u6e05\u695a\u3002\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u57280.27B\u523070B\u53c2\u6570\u768415\u4e2a\u6a21\u578b\u4e0a\u7814\u7a76\u4e86\u8bc4\u4f30\u610f\u8bc6\u7684\u7f29\u653e\u884c\u4e3a\u3002", "motivation": "\u8bc4\u4f30AI\u5b89\u5168\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5728\u6d4b\u8bd5\u671f\u95f4\u9690\u85cf\u5371\u9669\u80fd\u529b\uff0c\u5e76\u4e14\u4e86\u89e3\u8bc4\u4f30\u610f\u8bc6\u5982\u4f55\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u3002", "method": "\u5728\u6765\u81ea\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\u768415\u4e2a\u6a21\u578b\uff08\u89c4\u6a21\u4ece0.27B\u523070B\u53c2\u6570\uff09\u4e0a\uff0c\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u5bf9\u6a21\u578b\u4e2d\u7684\u201c\u63a7\u5236\u5411\u91cf\u6fc0\u6d3b\u201d\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u7814\u7a76\u8bc4\u4f30\u610f\u8bc6\u3002", "result": "\u8bc4\u4f30\u610f\u8bc6\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\u800c\u5448\u5e42\u5f8b\u5173\u7cfb\uff08power-law\uff09\u589e\u957f\u3002\u8fd9\u79cd\u589e\u957f\u8d8b\u52bf\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u662f\u53ef\u9884\u6d4b\u7684\u3002", "conclusion": "\u8bc4\u4f30\u610f\u8bc6\u4f1a\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u800c\u589e\u5f3a\uff0c\u5e76\u4e14\u8fd9\u79cd\u589e\u5f3a\u662f\u53ef\u9884\u6d4b\u7684\u3002\u8fd9\u79cd\u5e42\u5f8b\u5173\u7cfb\u53ef\u4ee5\u5e2e\u52a9\u9884\u6d4b\u672a\u6765\u66f4\u5927\u6a21\u578b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u5e76\u6307\u5bfc\u8bbe\u8ba1\u66f4\u5177\u9002\u5e94\u6027\u7684AI\u5b89\u5168\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u7eaf\u7ecf\u5178\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u3002", "motivation": "\u8bc4\u4f30\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u5728 MNIST\u3001CIFAR100 \u548c STL10 \u6570\u636e\u96c6\u4e0a\u5bf9\u6df7\u5408\u6a21\u578b\uff08\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff09\u548c\u7eaf\u7ecf\u5178\u6a21\u578b\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff08MNIST\uff1a99.38%\uff0cCIFAR100\uff1a41.69%\uff0cSTL10\uff1a74.05%\uff09\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e86 5-12 \u500d\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u4e86 6-32%\u3002\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\uff0c\u6df7\u5408\u6a21\u578b\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5728\u5185\u5b58\u548c CPU \u4f7f\u7528\u7387\u65b9\u9762\u66f4\u6709\u6548\u7387\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2509.13964", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.13964", "abs": "https://arxiv.org/abs/2509.13964", "authors": ["Donato Francesco Falcone", "Stephan Menzel", "Tommaso Stecconi", "Matteo Galetta", "Antonio La Porta", "Bert Jan Offrein", "Valeria Bragaglia"], "title": "Analytical Modelling of the Transport in Analog Filamentary Conductive-Metal-Oxide/HfOx ReRAM Devices", "comment": null, "summary": "The recent co-optimization of memristive technologies and programming\nalgorithms enabled neural networks training with in-memory computing systems.\nIn this context, novel analog filamentary conductive-metal-oxide (CMO)/HfOx\nredox-based resistive switching memory (ReRAM) represents a key technology.\nDespite device performance enhancements reported in literature, the underlying\nmechanism behind resistive switching is not fully understood. This work\npresents the first physics-based analytical model of the current transport and\nof the resistive switching in these devices. As a case study, analog TaOx/HfOx\nReRAM devices are considered. The current transport is explained by a\ntrap-to-trap tunneling process, and the resistive switching by a modulation of\nthe defect density within the sub-band of the TaOx that behaves as electric\nfield and temperature confinement layer. The local temperature and electric\nfield distributions are derived from the solution of the electric and heat\ntransport equations in a 3D finite element ReRAM model. The intermediate\nresistive states are described as a gradual modulation of the TaOx defect\ndensity, which results in a variation of its electrical conductivity. The\ndrift-dynamics of ions during the resistive switching is analytically\ndescribed, allowing the estimation of defect migration energies in the TaOx\nlayer. Moreover, the role of the electro-thermal properties of the CMO layer is\nunveiled. The proposed analytical model accurately describes the experimental\nswitching characteristic of analog TaOx/HfOx ReRAM devices, increasing the\nphysical understanding and providing the equations necessary for circuit\nsimulations incorporating this technology.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u89e3\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u6a21\u62df\u5fc6\u963b\u5668\uff08ReRAM\uff09\u7684\u7535\u6d41\u4f20\u8f93\u548c\u963b\u53d8\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8\u4e86 TaOx/HfOx \u5668\u4ef6\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u9677\u9631\u5230\u9677\u9631\u7684\u96a7\u7a7f\u8fc7\u7a0b\u89e3\u91ca\u7535\u6d41\u4f20\u8f93\uff0c\u901a\u8fc7 TaOx \u5c42\u7f3a\u9677\u5bc6\u5ea6\u8c03\u5236\u89e3\u91ca\u963b\u53d8\u884c\u4e3a\uff0c\u5e76\u8003\u8651\u4e86\u5c40\u90e8\u7535\u573a\u548c\u6e29\u5ea6\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u80fd\u591f\u51c6\u786e\u63cf\u8ff0\u5b9e\u9a8c\u6570\u636e\uff0c\u4e3a\u96c6\u6210\u8be5\u6280\u672f\u8fdb\u884c\u7535\u8def\u4eff\u771f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u5fc6\u963b\u5668\u6280\u672f\u548c\u8bad\u7ec3\u7b97\u6cd5\u7684\u534f\u540c\u4f18\u5316\u4f7f\u5f97\u5185\u5b58\u8ba1\u7b97\u7cfb\u7edf\u80fd\u591f\u8fdb\u884c\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u5e76\u4e14\u6a21\u62df\u7684\u4e1d\u72b6\u5bfc\u7535\u6c27\u5316\u7269\uff08CMO\uff09/ HfOx \u6c27\u5316\u8fd8\u539f\u57fa\u7535\u963b\u5f00\u5173\u5b58\u50a8\u5668\uff08ReRAM\uff09\u662f\u5176\u4e2d\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u963b\u53d8\u673a\u5236\u4ecd\u672a\u88ab\u5b8c\u5168\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u89e3\u6790\u6a21\u578b\u6765\u6df1\u5165\u7406\u89e3\u8be5\u5668\u4ef6\u7684\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u89e3\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790 TaOx/HfOx ReRAM \u5668\u4ef6\u7684\u7535\u6d41\u4f20\u8f93\u548c\u963b\u53d8\u673a\u5236\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u9677\u9631\u5230\u9677\u9631\u7684\u96a7\u7a7f\u8fc7\u7a0b\u6765\u89e3\u91ca\u7535\u6d41\u4f20\u8f93\uff0c\u5e76\u901a\u8fc7\u6539\u53d8 TaOx \u5c42\u5185\u7f3a\u9677\u5bc6\u5ea6\u6765\u89e3\u91ca\u963b\u53d8\u884c\u4e3a\uff0c\u540c\u65f6\u8003\u8651\u4e86\u7535\u573a\u548c\u6e29\u5ea6\u5728\u5668\u4ef6\u5185\u7684\u5206\u5e03\u3002\u901a\u8fc7\u6c42\u89e3\u4e09\u7ef4\u6709\u9650\u5143\u6a21\u578b\u7684\u7535\u548c\u70ed\u4f20\u8f93\u65b9\u7a0b\uff0c\u53ef\u4ee5\u63a8\u5bfc\u51fa\u5c40\u57df\u6e29\u5ea6\u548c\u7535\u573a\u5206\u5e03\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5206\u6790\u79bb\u5b50\u6f02\u79fb\u52a8\u529b\u5b66\uff0c\u4f30\u7b97\u7f3a\u9677\u8fc1\u79fb\u80fd\uff0c\u5e76\u63ed\u793a CMO \u5c42\u7535\u70ed\u6027\u8d28\u7684\u4f5c\u7528\u3002", "result": "\u6240\u63d0\u51fa\u7684\u89e3\u6790\u6a21\u578b\u80fd\u591f\u51c6\u786e\u5730\u63cf\u8ff0\u6a21\u62df TaOx/HfOx ReRAM \u5668\u4ef6\u7684\u5b9e\u9a8c\u5f00\u5173\u7279\u6027\u3002\u6a21\u578b\u89e3\u91ca\u4e86\u7535\u6d41\u4f20\u8f93\u673a\u5236\uff08\u9677\u9631\u5230\u9677\u9631\u96a7\u7a7f\uff09\u548c\u963b\u53d8\u673a\u5236\uff08TaOx \u7f3a\u9677\u5bc6\u5ea6\u8c03\u5236\uff09\uff0c\u5e76\u80fd\u63cf\u8ff0\u4e2d\u95f4\u7535\u963b\u72b6\u6001\u4ee5\u53ca\u4f30\u7b97\u7f3a\u9677\u8fc1\u79fb\u80fd\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7269\u7406\u6a21\u578b\u4e3a\u7406\u89e3 TaOx/HfOx ReRAM \u5668\u4ef6\u7684\u7535\u6d41\u4f20\u8f93\u548c\u963b\u53d8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u80fd\u591f\u51c6\u786e\u63cf\u8ff0\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u4e3a\u5c06\u8be5\u6280\u672f\u96c6\u6210\u5230\u7535\u8def\u4eff\u771f\u4e2d\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.13469", "categories": ["cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.13469", "abs": "https://arxiv.org/abs/2509.13469", "authors": ["Jonathan Z. Sun", "Christopher Safranski", "Siyuranga Koswata", "Pouya Hashemi", "Andrew D. Kent"], "title": "Superparamagnetic and Stochastic-Write Magnetic Tunnel Junctions for High-Speed True Random Number Generation in Advanced Computing", "comment": "11 pages, 5 figures", "summary": "We review two magnetic tunnel junction (MTJ) approaches for compact,\nlow-power, CMOS-integrated true random number generation (TRNG). The first\nemploys passive-read, easy-plane superparamagnetic MTJs (sMTJs) that generate\nthermal-fluctuation-driven bit streams at $0.5$--$1$~Gb/s per device. The\nsecond uses MTJs with magnetically stable free layers, operated with stochastic\nwrite pulses to achieve switching probabilities of about $0.5$ (\\emph{i.e.},\nwrite error rates of $\\simeq 0.5$), achieving $\\gtrsim 0.1$~Gb/s per device; we\nrefer to these as stochastic-write MTJs (SW-MTJs). Randomness from both\napproaches has been validated using the NIST~SP800 test suites. The sMTJ\napproach uses a read-only cell with low power and can be compatible with most\nadvanced CMOS nodes, while SW-MTJs leverage standard CMOS MTJ process flows,\nenabling co-integration with embedded spin-transfer torque magnetic random\naccess memory (STT-MRAM). Both approaches can achieve deep sub-0.01~$\\mu$m$^2$\nMTJ footprints and offer orders-of-magnitude better energy efficiency than\nCPU/GPU-based generators, enabling placement near logic for high-throughput\nrandom bit-streams for probabilistic computing, statistical modeling, and\ncryptography. In terms of performance, sMTJs generally suit applications\nrequiring very high data-rate random bits near logic processors, such as\nprobabilistic computing or large-scale statistical modeling. By contrast,\nSW-MTJs are an attractive option for edge-oriented microcontrollers, providing\nentropy sources for computing or cryptographic enhancement. We highlight the\nstrengths, limitations, and integration challenges of each approach,\nemphasizing the need to reduce device-to-device variability in sMTJs --\nparticularly by mitigating magnetostriction-induced in-plane anisotropy -- and\nto improve temporal stability in SW-MTJs for robust, large-scale deployment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u7528\u4e8e\u7d27\u51d1\u3001\u4f4e\u529f\u8017\u3001CMOS\u96c6\u6210\u771f\u968f\u673a\u6570\u751f\u6210\u5668(TRNG)\u7684\u78c1\u6027\u96a7\u9053\u7ed3(MTJ)\u65b9\u6cd5\uff1asMTJ\u548cSW-MTJ\u3002sMTJ\u5229\u7528\u70ed\u6da8\u843d\u4ea7\u751f\u6570\u636e\u6d41\uff0c\u901f\u7387\u4e3a0.5-1 Gb/s\uff1bSW-MTJ\u5229\u7528\u968f\u673a\u5199\u8109\u51b2\u4ea7\u751f\u7ea60.5\u7684\u5f00\u5173\u6982\u7387\uff0c\u901f\u7387\u7ea6\u4e3a0.1 Gb/s\u3002\u4e24\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u968f\u673a\u6027\u5747\u901a\u8fc7NIST SP800\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\u3002sMTJ\u529f\u8017\u4f4e\uff0c\u517c\u5bb9\u5148\u8fdbCMOS\u8282\u70b9\uff1bSW-MTJ\u5229\u7528\u6807\u51c6CMOS MTJ\u5de5\u827a\uff0c\u53ef\u4e0eSTT-MRAM\u5171\u96c6\u6210\u3002\u4e24\u79cd\u65b9\u6cd5\u7684MTJ\u5c3a\u5bf8\u5747\u53ef\u8fbe0.01 \u03bcm\u00b2\u4ee5\u4e0b\uff0c\u80fd\u6548\u6bd4CPU/GPU\u751f\u6210\u5668\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u7d27\u51d1\u3001\u4f4e\u529f\u8017\u3001CMOS\u96c6\u6210\u7684\u771f\u968f\u673a\u6570\u751f\u6210\u5668(TRNG)\uff0c\u672c\u6587\u7814\u7a76\u4e86\u4e24\u79cd\u57fa\u4e8e\u78c1\u6027\u96a7\u9053\u7ed3(MTJ)\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e24\u79cdMTJ\u65b9\u6cd5\uff1a1. \u88ab\u52a8\u8bfb\u53d6\u7684\u6613\u5e73\u9762\u8d85\u987a\u78c1MTJ(sMTJ)\uff0c\u5229\u7528\u70ed\u6da8\u843d\u4ea7\u751f\u968f\u673a\u6bd4\u7279\u6d41\uff0c\u901f\u7387\u4e3a0.5-1 Gb/s\u30022. \u5177\u6709\u78c1\u7a33\u5b9a\u81ea\u7531\u5c42\u7684MTJ\uff0c\u901a\u8fc7\u968f\u673a\u5199\u8109\u51b2\u64cd\u4f5c\uff0c\u5b9e\u73b0\u7ea60.5\u7684\u5f00\u5173\u6982\u7387\uff0c\u901f\u7387\u7ea6\u4e3a0.1 Gb/s\uff0c\u79f0\u4e3a\u968f\u673a\u5199MTJ(SW-MTJ)\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u968f\u673a\u6027\u5747\u901a\u8fc7NIST SP800\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\u3002sMTJ\u529f\u8017\u4f4e\uff0c\u517c\u5bb9\u5148\u8fdbCMOS\u8282\u70b9\uff1bSW-MTJ\u5229\u7528\u6807\u51c6CMOS MTJ\u5de5\u827a\uff0c\u53ef\u4e0eSTT-MRAM\u5171\u96c6\u6210\u3002\u4e24\u79cd\u65b9\u6cd5\u7684MTJ\u5c3a\u5bf8\u5747\u53ef\u8fbe0.01 \u03bcm\u00b2\u4ee5\u4e0b\uff0c\u80fd\u6548\u6bd4CPU/GPU\u751f\u6210\u5668\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002sMTJ\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6570\u636e\u901f\u7387\u968f\u673a\u6bd4\u7279\u7684\u5e94\u7528\uff1bSW-MTJ\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u5fae\u63a7\u5236\u5668\uff0c\u53ef\u63d0\u4f9b\u71b5\u6e90\u3002", "conclusion": "sMTJ\u548cSW-MTJ\u662f\u4e24\u79cd\u6709\u524d\u666f\u7684TRNG\u6280\u672f\uff0c\u5404\u6709\u4f18\u52a3\u3002sMTJ\u9002\u7528\u4e8e\u9ad8\u541e\u5410\u91cf\u5e94\u7528\uff0c\u800cSW-MTJ\u5219\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002\u672a\u6765\u7684\u5de5\u4f5c\u9700\u8981\u89e3\u51b3sMTJ\u7684\u5668\u4ef6\u95f4\u53d8\u5f02\u6027\u95ee\u9898\uff08\u7279\u522b\u662f\u78c1\u81f4\u4f38\u7f29\u5f15\u8d77\u7684\u5e73\u9762\u78c1\u5f02\u65b9\u6027\uff09\u4ee5\u53caSW-MTJ\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "Op-Fed\u662f\u4e00\u4e2a\u5305\u542b1044\u4e2a\u53e5\u5b50\u53ca\u5176\u4e0a\u4e0b\u6587\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5206\u6790\u7f8e\u8054\u50a8\u516c\u5f00\u5e02\u573a\u59d4\u5458\u4f1a\uff08FOMC\uff09\u4f1a\u8bae\u7eaa\u8981\u4e2d\u7684\u89c2\u70b9\u548c\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u3002\u8be5\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u53e5\u5b50\u95f4\u4f9d\u8d56\u6027\u4e24\u5927\u6311\u6218\uff0c\u5e76\u88ab\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aLLM\u5728\u533a\u5206\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "FOMC\u7684\u8d27\u5e01\u653f\u7b56\u51b3\u7b56\u5f71\u54cd\u5e7f\u6cdb\uff0c\u4f46\u5176\u4f1a\u8bae\u7eaa\u8981\u4e2d\u7684\u89c2\u70b9\u548c\u7acb\u573a\u5206\u6790\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u3002\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5bf9FOMC\u4f1a\u8bae\u7eaa\u8981\u4e2d\u5173\u4e8e\u8d27\u5e01\u653f\u7b56\u7684\u89c2\u70b9\u548c\u7acb\u573a\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u5206\u6790\u3002", "method": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1044\u4e2a\u6807\u6ce8\u53e5\u5b50\u7684Op-Fed\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\uff08\u5c11\u4e8e8%\u7684\u53e5\u5b50\u8868\u8fbe\u975e\u4e2d\u7acb\u7acb\u573a\uff09\u548c\u53e5\u5b50\u95f4\u4f9d\u8d56\u6027\uff0865%\u7684\u5b9e\u4f8b\u9700\u8981\u8d85\u8d8a\u53e5\u5b50\u5c42\u7ea7\u7684\u4e0a\u4e0b\u6587\uff09\u7684\u6311\u6218\u3002\u4ed6\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u7684\u5c42\u6b21\u5316\u6a21\u5f0f\u6765\u5206\u79bb\u89c2\u70b9\u3001\u8d27\u5e01\u653f\u7b56\u548c\u7acb\u573a\uff0c\u5e76\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u6765\u9009\u62e9\u6807\u6ce8\u5b9e\u4f8b\uff0c\u5c06\u6240\u6709\u65b9\u9762\u79ef\u6781\u5b9e\u4f8b\u7684\u6570\u91cf\u589e\u52a0\u4e86\u4e00\u500d\u3002", "result": "\u4f7f\u7528Op-Fed\u6570\u636e\u96c6\uff0c\u7814\u7a76\u53d1\u73b0\u4e00\u4e2a\u8868\u73b0\u6700\u4f73\u7684\u95ed\u6e90LLM\u5728\u96f6\u6837\u672c\u89c2\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e860.80\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5728\u96f6\u6837\u672c\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u7c7b\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u52300.61\u7684\u51c6\u786e\u7387\uff0c\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u7ebf0.89\u3002", "conclusion": "Op-Fed\u6570\u636e\u96c6\u4e3a\u672a\u6765\u6a21\u578b\u8bad\u7ec3\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4ee5\u53ca\u4f5c\u4e3a\u672a\u6765\u6807\u6ce8\u5de5\u4f5c\u7684\u79cd\u5b50\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u7406\u89e3\u548c\u5206\u6790FOMC\u4f1a\u8bae\u7eaa\u8981\u4e2d\u7684\u8d27\u5e01\u653f\u7b56\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2509.13377", "categories": ["quant-ph", "math.PR", "60J05, 81P16"], "pdf": "https://arxiv.org/pdf/2509.13377", "abs": "https://arxiv.org/abs/2509.13377", "authors": ["Federico Girotti", "Alessandro Vitale"], "title": "Purification of quantum trajectories in infinite dimensions", "comment": "16 pages, no figures, comments are welcome", "summary": "In this work we exhibit a class of examples that show that the\ncharacterization of purification of quantum trajectories in terms of 'dark'\nsubspaces that was proved for finite dimensional systems fails to hold in\ninfinite dimensional ones. Moreover, we prove that the new phenomenon emerging\nin our class of models and preventing purification to happen is the only new\npossibility that emerges in infinite dimensional systems. Our proof strategy\npoints out that the emergence of new phenomena in infinite dimensional systems\nis due to the fact that the set of orthogonal projections is not sequentially\ncompact. Having in mind this insight, we are able to prove that the finite\ndimensional extends to a class of infinite dimensional models.", "AI": {"tldr": "\u6709\u9650\u7ef4\u5ea6\u548c\u65e0\u9650\u7ef4\u5ea6\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u7eaf\u5316\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u5728\u65e0\u9650\u7ef4\u5ea6\u7cfb\u7edf\u4e2d\uff0c\u6709\u9650\u7ef4\u5ea6\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u8f68\u8ff9\u7eaf\u5316\u7406\u8bba\u662f\u5426\u4ecd\u7136\u6210\u7acb\u3002", "method": "\u901a\u8fc7\u5c55\u793a\u4e00\u7c7b\u65b0\u7684\u4f8b\u5b50\uff0c\u5e76\u5728\u65e0\u9650\u7ef4\u5ea6\u7cfb\u7edf\u4e2d\u8bc1\u660e\u963b\u6b62\u7eaf\u5316\u7684\u65b0\u73b0\u8c61\u662f\u552f\u4e00\u53ef\u80fd\u51fa\u73b0\u7684\u60c5\u51b5\uff0c\u540c\u65f6\u6307\u51fa\u8be5\u73b0\u8c61\u7684\u51fa\u73b0\u662f\u7531\u4e8e\u6b63\u4ea4\u6295\u5f71\u96c6\u4e0d\u662f\u5e8f\u5217\u7d27\u96c6\u3002", "result": "\u5728\u65e0\u9650\u7ef4\u5ea6\u7cfb\u7edf\u4e2d\uff0c\u91cf\u5b50\u8f68\u8ff9\u7684\u7eaf\u5316\u7406\u8bba\u5931\u6548\uff0c\u5e76\u4e14\u51fa\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u73b0\u8c61\u963b\u6b62\u4e86\u7eaf\u5316\u3002", "conclusion": "\u6709\u9650\u7ef4\u5ea6\u7cfb\u7edf\u7684\u7406\u8bba\u53ef\u4ee5\u6269\u5c55\u5230\u4e00\u7c7b\u65e0\u9650\u7ef4\u5ea6\u6a21\u578b\u3002"}}
{"id": "2509.13653", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13653", "abs": "https://arxiv.org/abs/2509.13653", "authors": ["Hang Ren", "Yulin Wu", "Shuhan Qi", "Jiajia Zhang", "Xiaozhen Sun", "Tianzi Ma", "Xuan Wang"], "title": "Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation", "comment": null, "summary": "Regret minimization is a powerful method for finding Nash equilibria in\nNormal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically\nguarantees convergence only for the average strategy. However, computing the\naverage strategy requires significant computational resources or introduces\nadditional errors, limiting its practical applicability. The Reward\nTransformation (RT) framework was introduced to regret minimization to achieve\nlast-iterate convergence through reward function regularization. However, it\nfaces practical challenges: its performance is highly sensitive to manually\ntuned parameters, which often deviate from theoretical convergence conditions,\nleading to slow convergence, oscillations, or stagnation in local optima.\n  Inspired by previous work, we propose an adaptive technique to address these\nissues, ensuring better consistency between theoretical guarantees and\npractical performance for RT Regret Matching (RTRM), RT Counterfactual Regret\nMinimization (RTCFR), and their variants in solving NFGs and EFGs more\neffectively. Our adaptive methods dynamically adjust parameters, balancing\nexploration and exploitation while improving regret accumulation, ultimately\nenhancing asymptotic last-iterate convergence and achieving linear convergence.\nExperimental results demonstrate that our methods significantly accelerate\nconvergence, outperforming state-of-the-art algorithms.", "AI": {"tldr": "Regret minimization methods for finding Nash equilibria have issues with average strategy computation and reward transformation framework's sensitivity to parameters. This paper proposes adaptive methods to dynamically adjust parameters, improving convergence and outperforming existing algorithms.", "motivation": "Regret minimization methods for finding Nash equilibria in Normal-Form Games (NFGs) and Extensive-Form Games (EFGs) typically guarantee convergence only for the average strategy, which is computationally expensive or error-prone. The Reward Transformation (RT) framework aimed to solve this via reward function regularization but suffers from parameter sensitivity, leading to poor performance. This work is motivated by the need for more practical and effective regret minimization techniques.", "method": "This paper proposes an adaptive technique to improve the performance of Reward Transformation Regret Matching (RTRM) and RT Counterfactual Regret Minimization (RTCFR) and their variants. The adaptive methods dynamically adjust parameters to balance exploration and exploitation, enhance regret accumulation, and ensure better consistency between theoretical guarantees and practical performance for solving NFGs and EFGs.", "result": "The proposed adaptive methods significantly accelerate convergence compared to existing algorithms. Experimental results demonstrate that these methods outperform state-of-the-art algorithms in solving NFGs and EFGs, showing improved asymptotic last-iterate convergence and achieving linear convergence.", "conclusion": "The adaptive technique effectively addresses the practical challenges of the Reward Transformation framework in regret minimization for solving Normal-Form Games and Extensive-Form Games. By dynamically adjusting parameters, the proposed methods achieve faster and more consistent convergence, outperforming current state-of-the-art algorithms and enhancing the practical applicability of regret minimization."}}
{"id": "2509.13445", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.13445", "abs": "https://arxiv.org/abs/2509.13445", "authors": ["Jing Li", "Huilin Lai", "Andrew H. Comstock", "Aeron McConnell", "Bharat Giri", "Yu Yun", "Tianhao Zhao", "Xiao Wang", "Yongseong Choi", "Xuemei Cheng", "Jian Shen", "Zhigang Jiang", "Dali Sun", "Wenbin Wang", "Xiaoshan Xu"], "title": "Persistent Interfacial Topological Hall Effect Demonstrating Electrical Readout of Topological Spin Structures in Insulators", "comment": null, "summary": "Conventional topological Hall effects (THE) require conducting magnets,\nleaving insulating systems largely inaccessible. Here we introduce the\ninterfacial topological Hall effect (ITHE), where the noncoplanar spin textures\nof insulating magnets are imprinted onto an adjacent heavy metal via the\nmagnetic proximity effect (MPE) and detected electrically. In Pt/h-LuFeO3\nbilayers, h-LuFeO3 hosts a topological spin structure robust against high\nmagnetic fields, arising from a 120{\\deg} triangular spin lattice with small\nspin canting that yields nontrivial topology but minimal magnetization. This\ngenerates a giant Hall response in Pt up to 0.5% of the longitudinal\nresistivity and a Hall-conductivity/magnetization ratio above 2 V^{-1}, clearly\ndistinguishable from the spin Hall Hanle effect background. Field- and\ntemperature-dependent analysis further reveals that Pt nanoclusters inherit\ntopological textures from h-LuFeO3 via MPE. Unlike the conventional THE narrow\npeak-and-dip features, ITHE in Pt/h-LuFeO3 persists across a broad magnetic\nfield range up to 14 T, demonstrating the exceptional stability of the\nunderlying topological spin structure. This establishes ITHE as a powerful and\nsensitive probe for topological magnetism in ultrathin insulating films and\npaves the way for new spintronic applications.", "AI": {"tldr": "\u901a\u8fc7\u78c1\u6027\u90bb\u8fd1\u6548\u5e94\uff0c\u5728\u7edd\u7f18\u78c1\u4f53\u4e2d\u5f15\u5165\u754c\u9762\u62d3\u6251\u970d\u5c14\u6548\u5e94\uff08ITHE\uff09\u4ee5\u89e3\u51b3\u4f20\u7edf\u62d3\u6251\u970d\u5c14\u6548\u5e94\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728Pt/h-LuFeO3\u53cc\u5c42\u7ed3\u6784\u4e2d\u5b9e\u73b0\u4e86ITHE\uff0c\u8be5\u6548\u5e94\u5177\u6709\u5de8\u5927\u7684\u970d\u5c14\u54cd\u5e94\u548c\u4f18\u8d8a\u7684\u7a33\u5b9a\u6027\uff0c\u53ef\u7528\u4e8e\u63a2\u6d4b\u8d85\u8584\u7edd\u7f18\u8584\u819c\u4e2d\u7684\u62d3\u6251\u78c1\u6027\u3002", "motivation": "\u4f20\u7edf\u62d3\u6251\u970d\u5c14\u6548\u5e94\uff08THE\uff09\u9700\u8981\u5bfc\u78c1\u4f53\uff0c\u8fd9\u4f7f\u5f97\u7edd\u7f18\u7cfb\u7edf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u65e0\u6cd5\u88ab\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6548\u5e94\uff0c\u4f7f\u5f97\u80fd\u591f\u7814\u7a76\u7edd\u7f18\u78c1\u4f53\u7684\u62d3\u6251\u78c1\u6027\u3002", "method": "\u901a\u8fc7\u78c1\u6027\u90bb\u8fd1\u6548\u5e94\uff08MPE\uff09\uff0c\u5c06\u7edd\u7f18\u78c1\u4f53\uff08h-LuFeO3\uff09\u7684\u975e\u5171\u9762\u81ea\u65cb\u7ed3\u6784 imprinted \u5230\u76f8\u90bb\u7684\u91cd\u91d1\u5c5e\uff08Pt\uff09\u4e0a\uff0c\u5e76\u901a\u8fc7\u7535\u5668\u65b9\u5f0f\u68c0\u6d4b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u4e86Pt/h-LuFeO3\u53cc\u5c42\u7ed3\u6784\uff0c\u5176\u4e2dh-LuFeO3\u5177\u6709\u7531120\u5ea6\u4e09\u89d2\u5f62\u81ea\u65cb\u6676\u683c\u4ea7\u751f\u7684\u62d3\u6251\u81ea\u65cb\u7ed3\u6784\u3002", "result": "\u5728Pt/h-LuFeO3\u53cc\u5c42\u7ed3\u6784\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e86\u5de8\u5927\u7684\u970d\u5c14\u54cd\u5e94\uff08\u9ad8\u8fbe\u7eb5\u5411\u7535\u963b\u76840.5%\uff09\u548c\u9ad8\u970d\u5c14\u7535\u5bfc\u7387/\u78c1\u5316\u6bd4\uff08>2 V^{-1}\uff09\u3002\u8be5\u6548\u5e94\u4e0e\u81ea\u65cb\u970d\u5c14\u970d\u5c14\u6548\u5e94\u80cc\u666f\u6709\u660e\u663e\u533a\u522b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cPt\u7eb3\u7c73\u56e2\u7c07\u901a\u8fc7MPE\u7ee7\u627f\u4e86h-LuFeO3\u7684\u62d3\u6251\u7ed3\u6784\u3002\u4e0e\u4f20\u7edf\u7684THE\u4e0d\u540c\uff0cITHE\u5728\u9ad8\u8fbe14T\u7684\u5bbd\u78c1\u573a\u8303\u56f4\u5185\u6301\u7eed\u5b58\u5728\uff0c\u8868\u660e\u5176\u6f5c\u5728\u7684\u62d3\u6251\u81ea\u65cb\u7ed3\u6784\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u754c\u9762\u62d3\u6251\u970d\u5c14\u6548\u5e94\uff08ITHE\uff09\u662f\u4e00\u79cd\u6709\u6548\u4e14\u7075\u654f\u7684\u63a2\u6d4b\u8d85\u8584\u7edd\u7f18\u8584\u819c\u4e2d\u62d3\u6251\u78c1\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u65b0\u7684\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13694", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13694", "abs": "https://arxiv.org/abs/2509.13694", "authors": ["Hanchen Ye", "Deming Chen"], "title": "StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs", "comment": "Accepted by MICRO'25", "summary": "Efficient execution of deep learning workloads on dataflow architectures is\ncrucial for overcoming memory bottlenecks and maximizing performance. While\nstreaming intermediate results between computation kernels can significantly\nimprove efficiency, existing approaches struggle with inter-kernel\ncorrelations, external memory access management, and buffer optimization. In\nthis work, we propose StreamTensor, a compiler framework that automatically\nconstructs and optimizes stream-based dataflow accelerators. StreamTensor\nintroduces a novel iterative tensor type system to explicitly encode stream\nlayouts, enabling seamless kernel fusion, buffer allocation, and memory\noptimization. By systematically exploring three hierarchical design spaces,\nincluding tensor tiling, kernel fusion, and resource allocation, StreamTensor\nbalances computational intensity, memory efficiency, and data streaming to\nmaximize performance. Based on FPGA evaluations on Large Language Models (LLM),\nStreamTensor achieves up to 0.76x and 0.64x lower latency compared to the\nstate-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy\nefficiency compared to GPUs, making it a promising approach for scalable\ndataflow-based deep learning acceleration.", "AI": {"tldr": "StreamTensor\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f20\u91cf\u7c7b\u578b\u7cfb\u7edf\u548c\u5206\u5c42\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u4f18\u5316\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728LLM\u7684FPGA\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6570\u636e\u6d41\u67b6\u6784\u65f6\uff0c\u5728\u5904\u7406\u6838\u95f4\u76f8\u5173\u6027\u3001\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\u7ba1\u7406\u548c\u7f13\u51b2\u533a\u4f18\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "StreamTensor\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u5f20\u91cf\u7c7b\u578b\u7cfb\u7edf\uff0c\u7528\u4e8e\u663e\u5f0f\u7f16\u7801\u6d41\u5e03\u5c40\uff0c\u5b9e\u73b0\u6838\u878d\u5408\u3001\u7f13\u51b2\u533a\u5206\u914d\u548c\u5185\u5b58\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u63a2\u7d22\u5f20\u91cf\u5206\u5757\u3001\u6838\u878d\u5408\u548c\u8d44\u6e90\u5206\u914d\u4e09\u4e2a\u5206\u5c42\u8bbe\u8ba1\u7a7a\u95f4\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728LLM\u7684FPGA\u8bc4\u4f30\u4e2d\uff0cStreamTensor\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u7684FPGA LLM\u52a0\u901f\u5668\u548cGPU\u4f4e0.76\u500d\u548c0.64\u500d\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u6bd4GPU\u9ad81.99\u500d\u7684\u80fd\u6548\u3002", "conclusion": "StreamTensor\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u6d41\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u65b9\u6cd5\u3002"}}
{"id": "2509.13871", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.13871", "abs": "https://arxiv.org/abs/2509.13871", "authors": ["Dror Fried", "Etay Segal", "Gad E. Yaron"], "title": "Algorithmic Perspective on Toda's Theorem", "comment": null, "summary": "Toda's Theorem is a fundamental result in computational complexity theory,\nwhose proof relies on a reduction from a QBF problem with a constant number of\nquantifiers to a model counting problem. While this reduction, henceforth\ncalled Toda's reduction, is of a purely theoretical nature, the recent progress\nof model counting tools raises the question of whether the reduction can be\nutilized to an efficient algorithm for solving QBF. In this work, we address\nthis question by looking at Toda's reduction from an algorithmic perspective.\nWe first convert the reduction into a concrete algorithm that given a QBF\nformula and a probability measure, produces the correct result with a\nconfidence level corresponding to the given measure. Beyond obtaining a naive\nprototype, our algorithm and the analysis that follows shed light on the fine\ndetails of the reduction that are so far left elusive. Then, we improve this\nprototype through various theoretical and algorithmic refinements. While our\nresults show a significant progress over the naive prototype, they also provide\na clearer understanding of the remaining challenges in turning Toda's reduction\ninto a competitive solver.", "AI": {"tldr": "Toda\u5b9a\u7406\u7684\u8fd8\u539f\u88ab\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5177\u4f53\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3QBF\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u4e0a\u7684\u6539\u8fdb\u3002", "motivation": "\u63a2\u7a76Toda\u5b9a\u7406\u7684\u8fd8\u539f\u662f\u5426\u80fd\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3QBF\u95ee\u9898\uff0c\u5e76\u4ece\u7b97\u6cd5\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\u3002", "method": "\u5c06Toda\u5b9a\u7406\u7684\u8fd8\u539f\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5177\u4f53\u7684\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u548c\u7b97\u6cd5\u4e0a\u7684\u6539\u8fdb\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u76f8\u5bf9\u4e8e\u6734\u7d20\u539f\u578b\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u5c06Toda\u8fd8\u539f\u8f6c\u5316\u4e3a\u6709\u7ade\u4e89\u529b\u7684\u6c42\u89e3\u5668\u7684\u5269\u4f59\u6311\u6218\u3002", "conclusion": "Toda\u5b9a\u7406\u7684\u8fd8\u539f\u53ef\u4ee5\u88ab\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5177\u4f53\u7684\u7b97\u6cd5\uff0c\u5c3d\u7ba1\u4ecd\u6709\u6311\u6218\uff0c\u4f46\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2509.13584", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.13584", "abs": "https://arxiv.org/abs/2509.13584", "authors": ["Yan S. Couto", "Cristina G. Fernandes"], "title": "Hardness of Dynamic Core and Truss Decompositions", "comment": "Full version of the paper accepted in WAOA 2025", "summary": "The k-core of a graph is its maximal subgraph with minimum degree at least k,\nand the core value of a vertex u is the largest k for which u is contained in\nthe k-core of the graph. Among cohesive subgraphs, k-core and its variants have\nreceived a lot of attention recently, particularly on dynamic graphs, as\nreported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic\ngraph algorithms. We answer questions on k-core stated in the survey, proving\nthat there is no efficient dynamic algorithm for k-core or to find (2 -\n{\\epsilon})-approximations for the core values, unless we can improve\ndecade-long state-of-the-art algorithms in many areas including matrix\nmultiplication and satisfiability, based on the established OMv and SETH\nconjectures. Some of our results show that there is no dynamic algorithm for\nk-core asymptotically faster than the trivial ones. This explains why most\nrecent research papers in this area focus not on a generic efficient dynamic\nalgorithm, but on finding a bounded algorithm, which is fast when few core\nvalues change per update. However, we also prove that such bounded algorithms\ndo not exist, based on the OMv conjecture. We present lower bounds also for a\ndirected version of the problem, and for the edge variant of the problem, known\nas k-truss. On the positive side, we present a polylogarithmic dynamic\nalgorithm for 2-core.", "AI": {"tldr": "\u6ca1\u6709\u9ad8\u6548\u7684\u52a8\u6001k-core\u7b97\u6cd5\uff0c\u4f46\u5b58\u57282-core\u7684\u5bf9\u6570\u7ea7\u52a8\u6001\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u6001\u56fe\u4e2d\u7684k-core\u95ee\u9898\uff0c\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u4e2d\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u8fd1\u4f3c\u7b97\u6cd5\u548c\u6709\u754c\u7b97\u6cd5\u7684\u53ef\u80fd\u6027\u3002", "method": "\u57fa\u4e8eOMv\u548cSETH\u731c\u60f3\uff0c\u901a\u8fc7\u8bc1\u660e\u4e0d\u5b58\u5728\u6bd4\u5e73\u51e1\u7b97\u6cd5\u66f4\u5feb\u7684\u52a8\u6001k-core\u7b97\u6cd5\uff0c\u5e76\u9488\u5bf9k-truss\u548c\u6709\u5411k-core\u95ee\u9898\u63d0\u51fa\u4e0b\u754c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e2-core\u7684\u5bf9\u6570\u7ea7\u52a8\u6001\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u4e0d\u5b58\u5728\u9ad8\u6548\u7684\u52a8\u6001k-core\u7b97\u6cd5\uff0c\u4e5f\u4e0d\u5b58\u5728(2 - {\\epsilon})-\u8fd1\u4f3c\u6838\u5fc3\u503c\u7684\u7b97\u6cd5\u3002\u8bc1\u660e\u4e86\u6709\u754c\u7b97\u6cd5\u4e0d\u5b58\u5728\u3002\u63d0\u51fa\u4e86\u6709\u5411k-core\u548ck-truss\u95ee\u9898\u7684\u4e0b\u754c\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e2-core\u7684\u5bf9\u6570\u7ea7\u52a8\u6001\u7b97\u6cd5\u3002", "conclusion": "\u76ee\u524d\u4e0d\u5b58\u5728\u9ad8\u6548\u7684\u52a8\u6001k-core\u7b97\u6cd5\uff0c\u4f46\u5bf9\u4e8e2-core\u95ee\u9898\u5b58\u5728\u4e00\u4e2a\u5bf9\u6570\u7ea7\u7684\u52a8\u6001\u7b97\u6cd5\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u96c6\u4e2d\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u7b97\u6cd5\u6216\u8fd1\u4f3c\u7b97\u6cd5\u4e0a\u3002"}}
{"id": "2509.13342", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13342", "abs": "https://arxiv.org/abs/2509.13342", "authors": ["Isaac Ronald Ward"], "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments", "comment": "This report is submitted as partial fulfilment of the requirements\n  for the Honours Programme of the Department of Computer Science and Software\n  Engineering, The University of Western Australia, 2019", "summary": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceRGB\u56fe\u50cf\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u901a\u8fc7\u6269\u5c55\u635f\u5931\u51fd\u6570\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u5668\u4eba\u4ece\u89c6\u89c9\u4fe1\u606f\uff08RGB\u56fe\u50cf\uff09\u8fdb\u884c\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u589e\u5f3a\u5bf9\u611f\u77e5\u6df7\u6dc6\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u8bad\u7ec3\u7684\u7b80\u6613\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\uff0c\u76f4\u89c2\u5730\u7ed3\u5408\u4e86\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u611f\u77e5\u6df7\u6dc6\u7684\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u751f\u6210\u5e26\u6709\u59ff\u6001\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u672c\u5730\u73af\u5883\u4e2d\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u4e0e\u672a\u7ecf\u4fee\u6539\u7684\u7f51\u7edc\u76f8\u6bd4\uff0c\u5ba4\u5185\u573a\u666f\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u6709\u6240\u63d0\u9ad8\uff0c\u4e2d\u503c\u4f4d\u7f6e\u8bef\u5dee\u548c\u4e2d\u503c\u65cb\u8f6c\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e86\u9ad8\u8fbe9.64%\u548c2.99%\u3002\u5728\u672c\u5730\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u6a21\u578b\u8fbe\u5230\u4e860.11\u7c73\u548c0.89\u5ea6\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u4e3a\u4efb\u4f55\u7ed9\u5b9a\u7684\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u521b\u5efa\u9c81\u68d2\u7684\u5bfc\u822a\u7b97\u6cd5\uff0c\u53ea\u9700\u6536\u96c6\u8be5\u573a\u666f\u7684\u56fe\u50cf\u5373\u53ef\uff0c\u91c7\u96c6\u65f6\u95f4\u6700\u77ed\u53ef\u8fbe330\u79d2\u3002"}}
{"id": "2509.13516", "categories": ["cs.LG", "68T05 (Primary) 90C30, 68W40 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.13516", "abs": "https://arxiv.org/abs/2509.13516", "authors": ["Tom Almog"], "title": "An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training", "comment": "7 pages. 3 figures", "summary": "As machine learning models grow increasingly complex and computationally\ndemanding, understanding the environmental impact of training decisions becomes\ncritical for sustainable AI development. This paper presents a comprehensive\nempirical study investigating the relationship between optimizer choice and\nenergy efficiency in neural network training. We conducted 360 controlled\nexperiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using\neight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,\nNAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking\non Apple M1 Pro hardware, we measured training duration, peak memory usage,\ncarbon dioxide emissions, and final model performance. Our findings reveal\nsubstantial trade-offs between training speed, accuracy, and environmental\nimpact that vary across datasets and model complexity. We identify AdamW and\nNAdam as consistently efficient choices, while SGD demonstrates superior\nperformance on complex datasets despite higher emissions. These results provide\nactionable insights for practitioners seeking to balance performance and\nsustainability in machine learning workflows.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u5bf9\u73af\u5883\u6709\u5f71\u54cd\uff0c\u9009\u62e9\u5408\u9002\u7684\u4f18\u5316\u5668\u53ef\u4ee5\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u7406\u89e3\u8bad\u7ec3\u51b3\u7b56\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u5bf9\u4e8e\u53ef\u6301\u7eed\u7684AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08MNIST, CIFAR-10, CIFAR-100\uff09\u4e0a\uff0c\u4f7f\u7528\u516b\u79cd\u6d41\u884c\u4f18\u5316\u5668\uff08SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax, NAdam\uff09\u548c15\u4e2a\u968f\u673a\u79cd\u5b50\u8fdb\u884c\u4e86360\u6b21\u5bf9\u7167\u5b9e\u9a8c\u3002\u4f7f\u7528CodeCarbon\u7cbe\u786e\u8ffd\u8e2aApple M1 Pro\u786c\u4ef6\u4e0a\u7684\u80fd\u8017\uff0c\u6d4b\u91cf\u4e86\u8bad\u7ec3\u65f6\u957f\u3001\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u91cf\u3001\u4e8c\u6c27\u5316\u78b3\u6392\u653e\u91cf\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u73af\u5883\u5f71\u54cd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6743\u8861\uff0c\u5e76\u4e14\u8fd9\u79cd\u6743\u8861\u56e0\u6570\u636e\u96c6\u548c\u6a21\u578b\u590d\u6742\u5ea6\u800c\u5f02\u3002AdamW\u548cNAdam\u662f\u59cb\u7ec8\u9ad8\u6548\u7684\u9009\u62e9\uff0c\u800cSGD\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6392\u653e\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5bfb\u6c42\u5728\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u5e73\u8861\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\u7684\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.14066", "categories": ["cs.NE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.14066", "abs": "https://arxiv.org/abs/2509.14066", "authors": ["Mirco Tincani", "Khaled Kerouch", "Umberto Garlando", "Mattia Barezzi", "Alessandro Sanginario", "Giacomo Indiveri", "Chiara De Luca"], "title": "A neuromorphic continuous soil monitoring system for precision irrigation", "comment": null, "summary": "Sensory processing at the edge requires ultra-low power stand-alone computing\ntechnologies. This is particularly true for modern agriculture and precision\nirrigation systems which aim to optimize water usage by monitoring key\nenvironmental observables continuously using distributed efficient embedded\nprocessing elements. Neuromorphic processing systems are emerging as a\npromising technology for extreme edge-computing applications that need to run\non resource-constrained hardware. As such, they are a very good candidate for\nimplementing efficient water management systems based on data measured from\nsoil and plants, across large fields. In this work, we present a fully\nenergy-efficient neuromorphic irrigation control system that operates\nautonomously without any need for data transmission or remote processing.\nLeveraging the properties of a biologically realistic spiking neural network,\nour system performs computation, and decision-making locally. We validate this\napproach using real-world soil moisture data from apple and kiwi orchards\napplied to a mixed-signal neuromorphic processor, and show that the generated\nirrigation commands closely match those derived from conventional methods\nacross different soil depths. Our results show that local neuromorphic\ninference can maintain decision accuracy, paving the way for autonomous,\nsustainable irrigation solutions at scale.", "AI": {"tldr": "\u4f7f\u7528\u8282\u80fd\u7684\u795e\u7ecf\u5f62\u6001\u5904\u7406\u7cfb\u7edf\uff0c\u5728\u8fb9\u7f18\u5b9e\u73b0\u81ea\u4e3b\u7684\u3001\u65e0\u9700\u6570\u636e\u4f20\u8f93\u7684\u667a\u80fd\u704c\u6e89\u63a7\u5236\u3002", "motivation": "\u73b0\u4ee3\u519c\u4e1a\u548c\u7cbe\u51c6\u704c\u6e89\u7cfb\u7edf\u9700\u8981\u8d85\u4f4e\u529f\u8017\u7684\u72ec\u7acb\u8ba1\u7b97\u6280\u672f\u6765\u6301\u7eed\u76d1\u6d4b\u73af\u5883\uff0c\u4f18\u5316\u6c34\u8d44\u6e90\u5229\u7528\u3002\u795e\u7ecf\u5f62\u6001\u5904\u7406\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u65b9\u9762\u663e\u793a\u51fa\u524d\u666f\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u571f\u58e4\u548c\u690d\u7269\u6570\u636e\u7684\u0639efficient water management\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u8282\u80fd\u7684\u795e\u7ecf\u5f62\u6001\u704c\u6e89\u63a7\u5236\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u751f\u7269\u5b66\u4e0a\u771f\u5b9e\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u6027\uff0c\u5728\u672c\u5730\u8fdb\u884c\u8ba1\u7b97\u548c\u51b3\u7b56\uff0c\u65e0\u9700\u6570\u636e\u4f20\u8f93\u6216\u8fdc\u7a0b\u5904\u7406\u3002", "result": "\u4f7f\u7528\u6765\u81ea\u82f9\u679c\u548c\u7315\u7334\u6843\u56ed\u7684\u771f\u5b9e\u571f\u58e4\u6e7f\u5ea6\u6570\u636e\uff0c\u5728\u6df7\u5408\u4fe1\u53f7\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u704c\u6e89\u6307\u4ee4\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0d\u540c\u571f\u58e4\u6df1\u5ea6\u4e0b\u7684\u7ed3\u679c\u975e\u5e38\u543b\u5408\u3002", "conclusion": "\u672c\u5730\u795e\u7ecf\u5f62\u6001\u63a8\u7406\u80fd\u591f\u4fdd\u6301\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u81ea\u4e3b\u3001\u53ef\u6301\u7eed\u7684\u704c\u6e89\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13583", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13583", "abs": "https://arxiv.org/abs/2509.13583", "authors": ["Varsha Rao", "Andrew A. Chien"], "title": "Modeling the Carbon Footprint of HPC: The Top 500 and EasyC", "comment": "15 pages, 11 figures", "summary": "Climate change is a critical concern for HPC systems, but GHG protocol\ncarbon-emission accounting methodologies are difficult for a single system, and\neffectively infeasible for a collection of systems. As a result, there is no\nHPC-wide carbon reporting, and even the largest HPC sites do not do GHG\nprotocol reporting.\n  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The\nkey challenge lies in modeling the carbon footprint with limited data\navailability.\n  With the disclosed Top500.org data, and using a new tool, EasyC, we were able\nto model the operational carbon of 391 HPC systems and the embodied carbon of\n283 HPC systems. We further show how this coverage can be enhanced by\nexploiting additional public information. With improved coverage, then\ninterpolation is used to produce the first carbon footprint estimates of the\nTop 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1\nYear) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top\n500's carbon footprint will increase through 2030.\n  A key enabler is the EasyC tool which models carbon footprint with only a few\ndata metrics. We explore availability of data and enhancement, showing that\ncoverage can be increased to 98% of Top 500 systems for operational and 80.8%\nof the systems for embodied emissions.", "AI": {"tldr": "\u7531\u4e8e\u7f3a\u4e4f\u6570\u636e\uff0c\u9ad8\u5bc6\u5ea6\u8ba1\u7b97\uff08HPC\uff09\u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u96be\u4ee5\u8ba1\u7b97\u3002\u672c\u7814\u7a76\u4f7f\u7528\u540d\u4e3a EasyC \u7684\u65b0\u5de5\u5177\uff0c\u5bf9 391 \u4e2a HPC \u7cfb\u7edf\u7684\u8fd0\u884c\u78b3\u6392\u653e\u548c 283 \u4e2a HPC \u7cfb\u7edf\u7684\u4f53\u73b0\u78b3\u6392\u653e\u8fdb\u884c\u4e86\u5efa\u6a21\u3002\u901a\u8fc7\u5229\u7528\u5176\u4ed6\u516c\u5f00\u4fe1\u606f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8986\u76d6\u7387\u3002\u7814\u7a76\u4f30\u8ba1\uff0cTop 500 HPC \u7cfb\u7edf\u7684\u8fd0\u884c\u78b3\u6392\u653e\u91cf\u4e3a 1,393.7 \u767e\u4e07\u5428\u4e8c\u6c27\u5316\u78b3\u5f53\u91cf\uff081 \u5e74\uff09\uff0c\u4f53\u73b0\u78b3\u6392\u653e\u91cf\u4e3a 1,881.8 \u767e\u4e07\u5428\u4e8c\u6c27\u5316\u78b3\u5f53\u91cf\u3002\u7814\u7a76\u8fd8\u9884\u6d4b\uff0c\u5230 2030 \u5e74\uff0cTop 500 \u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u5c06\u6709\u6240\u589e\u52a0\u3002EasyC \u5de5\u5177\u53ef\u4ee5\u901a\u8fc7\u4ec5\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u6307\u6807\u6765\u4f30\u7b97\u78b3\u8db3\u8ff9\u3002", "motivation": "\u9274\u4e8e\u6e29\u5ba4\u6c14\u4f53\u534f\u8bae\u7684\u78b3\u6392\u653e\u6838\u7b97\u65b9\u6cd5\u5bf9\u4e8e\u5355\u4e2a\u6216\u591a\u4e2a HPC \u7cfb\u7edf\u800c\u8a00\u96be\u4ee5\u5e94\u7528\uff0c\u5bfc\u81f4 HPC \u9886\u57df\u7f3a\u4e4f\u78b3\u6392\u653e\u62a5\u544a\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30 HPC\uff08\u7279\u522b\u662f Top 500 \u7cfb\u7edf\uff09\u7684\u78b3\u8db3\u8ff9\uff0c\u89e3\u51b3\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528 Top500.org \u516c\u5e03\u7684\u6570\u636e\u548c\u540d\u4e3a EasyC \u7684\u65b0\u5de5\u5177\uff0c\u5bf9 391 \u4e2a HPC \u7cfb\u7edf\u7684\u8fd0\u884c\u78b3\u6392\u653e\u548c 283 \u4e2a HPC \u7cfb\u7edf\u7684\u4f53\u73b0\u78b3\u6392\u653e\u8fdb\u884c\u4e86\u5efa\u6a21\u3002\u901a\u8fc7\u6316\u6398\u5176\u4ed6\u516c\u5f00\u4fe1\u606f\u6765\u63d0\u9ad8\u8986\u76d6\u7387\uff0c\u5e76\u4f7f\u7528\u63d2\u503c\u6cd5\u4f30\u7b97 Top 500 HPC \u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u3002EasyC \u5de5\u5177\u80fd\u591f\u4ec5\u901a\u8fc7\u51e0\u4e2a\u6570\u636e\u6307\u6807\u6765\u6a21\u62df\u78b3\u8db3\u8ff9\u3002", "result": "\u672c\u7814\u7a76\u4f30\u8ba1 Top 500 HPC \u7cfb\u7edf\u7684\u8fd0\u884c\u78b3\u6392\u653e\u91cf\u4e3a 1,393.7 \u767e\u4e07\u5428\u4e8c\u6c27\u5316\u78b3\u5f53\u91cf\uff081 \u5e74\uff09\uff0c\u4f53\u73b0\u78b3\u6392\u653e\u91cf\u4e3a 1,881.8 \u767e\u4e07\u5428\u4e8c\u6c27\u5316\u78b3\u5f53\u91cf\u3002\u7814\u7a76\u8fd8\u9884\u6d4b\u4e86 Top 500 \u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u5c06\u589e\u957f\u81f3 2030 \u5e74\u3002EasyC \u5de5\u5177\u80fd\u591f\u5c06 Top 500 \u7cfb\u7edf\u7684\u8fd0\u884c\u78b3\u6392\u653e\u8986\u76d6\u7387\u63d0\u9ad8\u5230 98%\uff0c\u4f53\u73b0\u78b3\u6392\u653e\u8986\u76d6\u7387\u63d0\u9ad8\u5230 80.8%\u3002", "conclusion": "EasyC \u5de5\u5177\u80fd\u591f\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u6307\u6807\u6709\u6548\u5730\u6a21\u62df HPC \u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\uff0c\u4ece\u800c\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u78b3\u8db3\u8ff9\u6838\u7b97\u7684\u8986\u76d6\u7387\u3002\u8be5\u5de5\u5177\u4e3a\u4f30\u7b97 Top 500 HPC \u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u4f30\u8ba1\u503c\uff0c\u5e76\u4e3a\u9884\u6d4b\u672a\u6765\u78b3\u8db3\u8ff9\u8d8b\u52bf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13592", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13592", "abs": "https://arxiv.org/abs/2509.13592", "authors": ["Youval Klioui"], "title": "Fast Single-Snapshot Harmonic Recovery with 2D Sparse Arrays using BCCB Matrices", "comment": null, "summary": "We introduce an efficient implementation of sparse recovery methods for the\nproblem of harmonic estimation with 2D sparse arrays using a single snapshot.\nBy imposing a uniformity constraint on the harmonic grids of the\nsubdictionaries used in the sparse recovery problem, in addition to a mild\nconstraint on the array topology that consists in having the elements lie on a\ngrid specified in half-wavelength units, we show that the Gram matrices that\nappear in these sparse recovery methods exhibit a block-circulant with\ncirculant blocks (BCCB) structure. The BCCB structure is then exploited to\nreduce the computational complexity of the matrix-vector products that appear\nin these methods through the use of 2D fast Fourier transforms (FFT) from\nO((L1L2)^2) down to O(L1L2 log(L1L2)) operations per iterations, where L1, L2\nare the lengths of the subdictionaries used for estimating the harmonics in the\nfirst and second dimension, respectively. We experimentally verify the proposed\nimplementation using the iterative shrinkage thresholding algorithm (ISTA), the\nfast iterative shrinkage-thresholding algorithm (FISTA), and the alternating\ndirection method of multipliers (ADMM) where we observe improvements", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e8c\u7ef4\u7a00\u758f\u9635\u5217\u4e0a\u4f7f\u7528\u5355\u5feb\u7167\u8fdb\u884c\u8c10\u6ce2\u4f30\u8ba1\u7684\u7a00\u758f\u6062\u590d\u65b9\u6cd5\u7684\u6709\u6548\u5b9e\u73b0\u65b9\u5f0f\u3002", "motivation": "\u901a\u8fc7\u5bf9\u7a00\u758f\u6062\u590d\u95ee\u9898\u4e2d\u4f7f\u7528\u7684\u5b50\u8bcd\u5178\u7684\u8c10\u6ce2\u7f51\u683c\u65bd\u52a0\u5747\u5300\u6027\u7ea6\u675f\uff0c\u4ee5\u53ca\u5bf9\u9635\u5217\u62d3\u6251\u65bd\u52a0\u6e29\u548c\u7ea6\u675f\uff08\u8981\u6c42\u5355\u5143\u4f4d\u4e8e\u534a\u6ce2\u957f\u5355\u4f4d\u6307\u5b9a\u7684\u7f51\u683c\u4e0a\uff09\uff0c\u6765\u89e3\u51b3\u8c10\u6ce2\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u5229\u7528Gram\u77e9\u9635\u7684\u5757\u5faa\u73af\u5e26\u5faa\u73af\u5757\uff08BCCB\uff09\u7ed3\u6784\uff0c\u901a\u8fc7\u4f7f\u7528\u4e8c\u7ef4\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u5c06\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO((L1L2)^2)\u964d\u4f4e\u5230O(L1L2 log(L1L2))\u3002", "result": "\u901a\u8fc7\u8fed\u4ee3\u6536\u7f29\u9608\u503c\u7b97\u6cd5\uff08ISTA\uff09\u3001\u5feb\u901f\u8fed\u4ee3\u6536\u7f29\u9608\u503c\u7b97\u6cd5\uff08FISTA\uff09\u548c\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u89c2\u5bdf\u5230\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eBCCB\u7ed3\u6784\u7684FFT\u52a0\u901f\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u964d\u4f4e\u4e8c\u7ef4\u7a00\u758f\u9635\u5217\u4e0a\u5355\u5feb\u7167\u8c10\u6ce2\u4f30\u8ba1\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.13380", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13380", "abs": "https://arxiv.org/abs/2509.13380", "authors": ["Alejandro D. Mousist"], "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy", "comment": "This preprint presents ASTREA, a multi-agent architecture combining\n  LLM-guided semantic modulation with reinforcement learning for autonomous\n  satellite operations. The system is validated in hardware orbital\n  environments", "summary": "This paper presents ASTREA, the first agentic system deployed on\nflight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using\nthermal control as a representative use case, we integrate a\nresource-constrained Large Language Model (LLM) agent with a reinforcement\nlearning controller in an asynchronous architecture tailored for\nspace-qualified platforms. Ground experiments show that LLM-guided supervision\nimproves thermal stability and reduces violations, confirming the feasibility\nof combining semantic reasoning with adaptive control under hardware\nconstraints. However, on-orbit validation aboard the International Space\nStation (ISS) reveals performance degradation caused by inference latency\nmismatched with the rapid thermal cycles characteristic of Low Earth Orbit\n(LEO) satellites. These results highlight both the opportunities and current\nlimitations of agentic LLM-based systems in real flight environments, providing\npractical design guidelines for future space autonomy.", "AI": {"tldr": "ASTREA\u662f\u9996\u4e2a\u90e8\u7f72\u5728\u5177\u6709\u98de\u884c\u9a8c\u8bc1\u7684\u786c\u4ef6\uff08TRL 9\uff09\u4e0a\u7684\u81ea\u4e3b\u822a\u5929\u5668\u8fd0\u884c\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "motivation": "\u4f7f\u7528\u70ed\u63a7\u4f5c\u4e3a\u4ee3\u8868\u6027\u7528\u4f8b\uff0c\u6574\u5408\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u5e76\u91c7\u7528\u4e86\u4e3a\u7a7a\u95f4\u8ba4\u8bc1\u5e73\u53f0\u91cf\u8eab\u5b9a\u5236\u7684\u5f02\u6b65\u67b6\u6784\u3002", "method": "\u5728\u5730\u9762\u5b9e\u9a8c\u4e2d\uff0cLLM\u9a71\u52a8\u7684\u76d1\u7763\u63d0\u9ad8\u4e86\u70ed\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u4e86\u8fdd\u89c4\u73b0\u8c61\uff0c\u8bc1\u5b9e\u4e86\u5728\u786c\u4ef6\u9650\u5236\u4e0b\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "result": "\u7136\u800c\uff0c\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u8fdb\u884c\u7684\u5728\u8f68\u9a8c\u8bc1\u663e\u793a\uff0c\u7531\u4e8e\u63a8\u7406\u5ef6\u8fdf\u4e0e\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7279\u6709\u7684\u5feb\u901f\u70ed\u5faa\u73af\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u5b9e\u9645\u98de\u884c\u73af\u5883\u4e2d\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u7684\u673a\u9047\u548c\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7a7a\u95f4\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2509.14141", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.14141", "abs": "https://arxiv.org/abs/2509.14141", "authors": ["Haonan Chen", "Xin Tong", "Linchao Yang", "Yangxue Zhang"], "title": "A Study on Optimizing the Thermal Performance of Coaxial Heat Exchanger Systems in Medium-Deep Geothermal Wells", "comment": null, "summary": "Medium-deep geothermal energy is a key renewable source, but existing coaxial\ndownhole heat exchanger (DHE) systems suffer from low efficiency and\ntemperature decay. This study evaluates the impacts of circulating flow rate,\ninlet temperature, and operation mode on DHE performance, using field data from\ntwo geothermal wells (Well A: 3200m, 130.5$^{\\circ}$C; Well B: 2500m,\n103.3$^{\\circ}$C). Results show that under an optimal condition (LC3: 50\nm$^3$/h, 30$^{\\circ}$C), Well A's heat extraction rate increased from 35% to\n42%, with its outlet temperature rising from 15$^{\\circ}$C to 20$^{\\circ}$C. In\ncontrast, Well B's rate decreased from 15% to 5%. After one week of continuous\noperation, the outlet temperature of Well A dropped from 55.7$^{\\circ}$C to\n16.5$^{\\circ}$C, and Well B's from 68$^{\\circ}$C to 17$^{\\circ}$C. Adopting an\nintermittent mode (16h operation, 8h shutdown daily) reduced the temperature\ndecay rate by approximately 10%. Based on these findings, we propose\noptimization strategies: controlling flow rate to 35m$^3$/h, maintaining an\ninlet temperature of 6--10$^{\\circ}$C, and implementing intermittent\nscheduling. This work provides guidance for the efficient design and\nsustainable operation of DHE systems.", "AI": {"tldr": "\u4e2d\u6df1\u5c42\u5730\u70ed\u662f\u5173\u952e\u7684\u53ef\u518d\u751f\u80fd\u6e90\uff0c\u4f46\u73b0\u6709\u7684\u540c\u8f74\u4e95\u4e0b\u6362\u70ed\u5668\uff08DHE\uff09\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\u4e14\u5b58\u5728\u6e29\u5ea6\u8870\u51cf\u95ee\u9898\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5faa\u73af\u6d41\u901f\u3001\u5165\u53e3\u6e29\u5ea6\u548c\u8fd0\u884c\u6a21\u5f0f\u5bf9DHE\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5229\u7528\u4e86\u4e24\u4e2a\u5730\u70ed\u4e95\uff08\u4e95A\uff1a3200\u7c73\uff0c130.5\u6444\u6c0f\u5ea6\uff1b\u4e95B\uff1a2500\u7c73\uff0c103.3\u6444\u6c0f\u5ea6\uff09\u7684\u73b0\u573a\u6570\u636e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6700\u4f73\u6761\u4ef6\u4e0b\uff08LC3\uff1a50\u7acb\u65b9\u7c73/\u5c0f\u65f6\uff0c30\u6444\u6c0f\u5ea6\uff09\uff0c\u4e95A\u7684\u70ed\u63d0\u53d6\u7387\u4ece35%\u63d0\u9ad8\u523042%\uff0c\u51fa\u53e3\u6e29\u5ea6\u4ece15\u6444\u6c0f\u5ea6\u5347\u81f320\u6444\u6c0f\u5ea6\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e95B\u7684\u6548\u7387\u4ece15%\u4e0b\u964d\u52305%\u3002\u8fde\u7eed\u8fd0\u884c\u4e00\u5468\u540e\uff0c\u4e95A\u7684\u51fa\u53e3\u6e29\u5ea6\u4ece55.7\u6444\u6c0f\u5ea6\u964d\u81f316.5\u6444\u6c0f\u5ea6\uff0c\u4e95B\u4ece68\u6444\u6c0f\u5ea6\u964d\u81f317\u6444\u6c0f\u5ea6\u3002\u91c7\u7528\u95f4\u6b47\u8fd0\u884c\u6a21\u5f0f\uff08\u6bcf\u5929\u8fd0\u884c16\u5c0f\u65f6\uff0c\u5173\u95ed8\u5c0f\u65f6\uff09\u53ef\u5c06\u6e29\u5ea6\u8870\u51cf\u7387\u964d\u4f4e\u7ea610%\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f18\u5316\u7b56\u7565\uff1a\u5c06\u6d41\u901f\u63a7\u5236\u572835\u7acb\u65b9\u7c73/\u5c0f\u65f6\uff0c\u5c06\u5165\u53e3\u6e29\u5ea6\u7ef4\u6301\u57286-10\u6444\u6c0f\u5ea6\uff0c\u5e76\u5b9e\u65bd\u95f4\u6b47\u8c03\u5ea6\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3aDHE\u7cfb\u7edf\u7684\u9ad8\u6548\u8bbe\u8ba1\u548c\u53ef\u6301\u7eed\u8fd0\u884c\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u8bc4\u4f30\u5faa\u73af\u6d41\u901f\u3001\u5165\u53e3\u6e29\u5ea6\u548c\u8fd0\u884c\u6a21\u5f0f\u5bf9\u4e2d\u6df1\u5c42\u5730\u70ed\u540c\u8f74\u4e95\u4e0b\u6362\u70ed\u5668\uff08DHE\uff09\u7cfb\u7edf\u6548\u7387\u548c\u6e29\u5ea6\u8870\u51cf\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u4e24\u4e2a\u5730\u70ed\u4e95\uff08\u4e95A\uff1a3200\u7c73\uff0c130.5\u6444\u6c0f\u5ea6\uff1b\u4e95B\uff1a2500\u7c73\uff0c103.3\u6444\u6c0f\u5ea6\uff09\u7684\u73b0\u573a\u6570\u636e\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5de5\u51b5\u4e0b\u7684DHE\u6027\u80fd\uff0c\u5305\u62ec\u70ed\u63d0\u53d6\u7387\u3001\u51fa\u53e3\u6e29\u5ea6\u548c\u6e29\u5ea6\u8870\u51cf\u7387\u3002\u5bf9\u6bd4\u4e86\u8fde\u7eed\u8fd0\u884c\u548c\u95f4\u6b47\u8fd0\u884c\u6a21\u5f0f\u7684\u6548\u679c\u3002", "result": "\u5728\u6700\u4f73\u6761\u4ef6\u4e0b\uff08LC3\uff1a50\u7acb\u65b9\u7c73/\u5c0f\u65f6\uff0c30\u6444\u6c0f\u5ea6\uff09\uff0c\u4e95A\u7684\u70ed\u63d0\u53d6\u7387\u4ece35%\u63d0\u9ad8\u523042%\uff0c\u51fa\u53e3\u6e29\u5ea6\u4ece15\u6444\u6c0f\u5ea6\u5347\u81f320\u6444\u6c0f\u5ea6\uff0c\u800c\u4e95B\u7684\u6548\u7387\u4ece15%\u4e0b\u964d\u52305%\u3002\u8fde\u7eed\u8fd0\u884c\u4e00\u5468\u540e\uff0c\u4e95A\u548c\u4e95B\u7684\u51fa\u53e3\u6e29\u5ea6\u5747\u663e\u8457\u4e0b\u964d\u3002\u95f4\u6b47\u8fd0\u884c\u6a21\u5f0f\u53ef\u5c06\u6e29\u5ea6\u8870\u51cf\u7387\u964d\u4f4e\u7ea610%\u3002", "conclusion": "\u5efa\u8bae\u4f18\u5316DHE\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8fd0\u884c\u7b56\u7565\uff0c\u5305\u62ec\u63a7\u5236\u6d41\u901f\u572835\u7acb\u65b9\u7c73/\u5c0f\u65f6\uff0c\u7ef4\u6301\u5165\u53e3\u6e29\u5ea6\u57286-10\u6444\u6c0f\u5ea6\uff0c\u5e76\u91c7\u7528\u95f4\u6b47\u8fd0\u884c\u6a21\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u7f13\u6e29\u5ea6\u8870\u51cf\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6301\u7eed\u8fd0\u884c\u3002"}}
{"id": "2509.13371", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13371", "abs": "https://arxiv.org/abs/2509.13371", "authors": ["Xuyuan Kang", "Xiao Wang", "Jingjing An", "Da Yan"], "title": "A novel approach of day-ahead cooling load prediction and optimal control for ice-based thermal energy storage (TES) system in commercial buildings", "comment": "16 pages,14 figures,published to Energy & Buildings", "summary": "Thermal energy storage (TES) is an effective method for load shifting and\ndemand response in buildings. Optimal TES control and management are essential\nto improve the performance of the cooling system. Most existing TES systems\noperate on a fixed schedule, which cannot take full advantage of its load\nshifting capability, and requires extensive investigation and optimization.\nThis study proposed a novel integrated load prediction and optimized control\napproach for ice-based TES in commercial buildings. A cooling load prediction\nmodel was developed and a mid-day modification mechanism was introduced into\nthe prediction model to improve the accuracy. Based on the predictions, a\nrule-based control strategy was proposed according to the time-of-use tariff;\nthe mid-day control adjustment mechanism was introduced in accordance with the\nmid-day prediction modifications. The proposed approach was applied in the\nice-based TES system of a commercial complex in Beijing, and achieved a mean\nabsolute error (MAE) of 389 kW and coefficient of variance of MAE of 12.5%. The\nintegrated prediction-based control strategy achieved an energy cost saving\nrate of 9.9%. The proposed model was deployed in the realistic building\nautomation system of the case building and significantly improved the\nefficiency and automation of the cooling system.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u8d1f\u8377\u9884\u6d4b\u548c\u4f18\u5316\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5546\u4e1a\u5efa\u7b51\u4e2d\u7684\u51b0\u57fa\u84c4\u80fd\uff08TES\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u9884\u6d4b\u6a21\u578b\u548c\u5f15\u5165\u57fa\u4e8e\u5206\u65f6\u7535\u4ef7\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e869.9%\u7684\u80fd\u8017\u6210\u672c\u8282\u7ea6\u3002", "motivation": "\u73b0\u6709\u7684TES\u7cfb\u7edf\u591a\u91c7\u7528\u56fa\u5b9a\u8ba1\u5212\u8fd0\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5176\u524a\u5cf0\u586b\u8c37\u7684\u6f5c\u529b\uff0c\u9700\u8981\u5927\u91cf\u7684\u4f18\u5316\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u8d1f\u8377\u9884\u6d4b\u548c\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5546\u4e1a\u5efa\u7b51\u4e2dTES\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u5efa\u7b51\u5236\u51b7\u8d1f\u8377\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e2d\u5348\u7684\u4fee\u6b63\u673a\u5236\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u57fa\u4e8e\u9884\u6d4b\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6839\u636e\u5206\u65f6\u7535\u4ef7\u5236\u5b9a\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e2d\u5348\u9884\u6d4b\u4fee\u6b63\u673a\u5236\u8fdb\u884c\u63a7\u5236\u8c03\u6574\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5546\u4e1a\u5efa\u7b51\u7684\u51b0\u57faTES\u7cfb\u7edf\u4e2d\u5e94\u7528\uff0c\u9884\u6d4b\u6a21\u578b\u5b9e\u73b0\u4e8612.5%\u7684MAE\u53d8\u5f02\u7cfb\u6570\u548c389 kW\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002\u96c6\u6210\u7684\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u4e869.9%\u7684\u80fd\u8017\u6210\u672c\u8282\u7ea6\u7387\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684\u96c6\u6210\u8d1f\u8377\u9884\u6d4b\u548c\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5546\u4e1a\u5efa\u7b51\u4e2d\u51b0\u57faTES\u7cfb\u7edf\u7684\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u5e76\u5b9e\u73b0\u663e\u8457\u7684\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "Chain-of-thought\uff08CoT\uff09\u63a8\u7406\u751f\u6210\u7684\u6b65\u9aa4\u5e38\u5e38\u65e0\u6cd5\u5bf9\u6700\u7ec8\u7b54\u6848\u4ea7\u751f\u56e0\u679c\u5f71\u54cd\uff0c\u5bfc\u81f4\u8f93\u51fa\u8106\u5f31\u4e14\u4e0d\u53ef\u4fe1\u3002FRIT\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\u5b9e\u73b0\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\uff0c\u901a\u8fc7\u8150\u8d25\u793a\u4f8b\u8bad\u7ec3\u6a21\u578b\uff0c\u751f\u6210\u771f\u5b9e/\u4e0d\u771f\u5b9e\u7684\u6210\u5bf9\u6570\u636e\uff0c\u4ee5\u8bc6\u522b\u63a8\u7406\u4e2d\u65ad\u7684\u5730\u65b9\u3002\u76f4\u63a5\u504f\u597d\u4f18\u5316\u88ab\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0cFRIT\u4f7fMistral\u6a21\u578b\u7684\u63a8\u7406\u5fe0\u5b9e\u5ea6\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\uff0c\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u3001\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ee5\u83b7\u5f97\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684Chain-of-thought\uff08CoT\uff09\u63a8\u7406\u65b9\u6cd5\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u65b9\u9762\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u6b65\u9aa4\u5f80\u5f80\u65e0\u6cd5\u5bf9\u6700\u7ec8\u7b54\u6848\u4ea7\u751f\u56e0\u679c\u5f71\u54cd\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u7684\u8106\u5f31\u6027\u548c\u4e0d\u53ef\u9760\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u8861\u91cf\u63a8\u7406\u7684\u5fe0\u5b9e\u5ea6\uff0c\u800c\u7528\u4e8e\u7cfb\u7edf\u6027\u6539\u8fdb\u63a8\u7406\u5fe0\u5b9e\u5ea6\u7684\u65b9\u6cd5\u5374\u5f88\u6709\u9650\u3002", "method": "FRIT\uff08Faithful Reasoning via Intervention Training\uff09\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u7cfb\u7edf\u6027\u8150\u8d25\u7684\u793a\u4f8b\u6765\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0cFRIT\u901a\u8fc7\u5e72\u9884\u6a21\u578b\u751f\u6210\u7684CoT\u4e2d\u7684\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u6765\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u521b\u5efa\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u6210\u5bf9\u6570\u636e\uff0c\u4ece\u800c\u7a81\u51fa\u63a8\u7406\u4f55\u65f6\u51fa\u73b0\u4e2d\u65ad\u3002\u4e4b\u540e\uff0c\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08Direct Preference Optimization\uff09\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Qwen3-8B\u548cMistral-7B-v0.1\u6a21\u578b\u4e0a\uff0c\u8de8\u4e8b\u5b9e\u548c\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cFRIT\u4f7fMistral\u6a21\u578b\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u7684\u5fe0\u5b9e\u63a8\u7406\u80fd\u529b\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u51c6\u786e\u7387\u4e5f\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FRIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9996\u4e2a\u53ef\u6269\u5c55\u3001\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u53ef\u9760\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002\u7814\u7a76\u4ee3\u7801\u5df2\u5728https://github.com/Anut-py/frit \u53d1\u5e03\u3002"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u51b3\u9ad8\u901f\u516c\u8def\u62e5\u5835\u68c0\u6d4b\u548c\u9884\u6d4b\u95ee\u9898\u7684\u7efc\u5408\u6280\u672f\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316YOLOv11\u548cDeepSort\u7b97\u6cd5\u63d0\u9ad8\u4e86\u8f66\u8f86\u611f\u77e5\u51c6\u786e\u7387\uff0c\u5e76\u5229\u7528GRU-Attention\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u62e5\u5835\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u68c0\u6d4b\u548c\u9884\u6d4b\u7cfb\u7edf\u5728\u8f66\u8f86\u611f\u77e5\u51c6\u786e\u7387\u548c\u957f\u5e8f\u5217\u4f9d\u8d56\u6027\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u5f71\u54cd\u4e86\u4ea4\u901a\u6548\u7387\u548c\u533a\u57df\u8fde\u63a5\u6027\u3002", "method": "1. \u4f18\u5316\u4e86YOLOv11\u6a21\u578b\uff0c\u7528DIoU Loss\u66ff\u6362GIoU Loss\uff0c\u5e76\u6539\u8fdb\u4e86DeepSort\u7b97\u6cd5\uff0c\u878d\u5408\u4e86\u9a6c\u6c0f\u8ddd\u79bb\u548c\u4f59\u5f26\u8ddd\u79bb\u30022. \u5229\u7528Greenberg\u6a21\u578b\u5206\u6790\u4e86\u8f66\u901f\u548c\u8f66\u6d41\u91cf\u7684\u5173\u7cfb\u30023. \u6784\u5efa\u4e86GRU-Attention\u6a21\u578b\u6765\u9884\u6d4b\u62e5\u5835\u3002", "result": "\u4f18\u5316\u540e\u7684YOLOv11-DIoU\u6a21\u578bmAP\u8fbe\u523095.7%\uff0cDeepSort\u7b97\u6cd5MOTA\u8fbe\u523093.8%\u3002GRU-Attention\u6a21\u578b\u5728\u62e5\u5835\u9884\u6d4b\u65b9\u9762\u8fbe\u523099.7%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u572810\u5206\u949f\u5185\u63d0\u524d30\u5206\u949f\u9884\u6d4b\u62e5\u5835\uff0c\u8bef\u5dee\u5c0f\u4e8e1\u5206\u949f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u901f\u516c\u8def\u62e5\u5835\u63a7\u5236\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u5728\u667a\u80fd\u4ea4\u901a\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.13495", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.13495", "abs": "https://arxiv.org/abs/2509.13495", "authors": ["Daniel Miravet", "Hassan Allami", "Marek Korkusinski", "Pawel Hawrylak"], "title": "Bilayer graphene quantum dots as a quantum simulator of Haldane topological quantum matter", "comment": null, "summary": "We demonstrate here that a chain of Bilayer Graphene Quantum Dots (BLGQD) can\nrealize topological quantum matter by effectively simulating a spin-1 chain\nthat hosts the Haldane phase within a specific range of parameters. We describe\na chain of BLGQD with two electrons each using an atomistic tight-binding model\ncombined with the exact diagonalization technique to solve the interacting\nfew-electron problem. Coulomb interactions and valley mixing effects are\ntreated within the same microscopic framework, allowing us to systematically\ninvestigate spin and valley polarization transitions as functions of\ninteraction strength and external tuning parameters. We calculate the low\nenergy states for single and double QDs as a function of the number of\nelectrons, identifying regimes of highly correlated multi-electron states. We\nconfirm the presence of a spin-one ground state for two electrons. Then, we\nexplore two coupled QDs with 4 electrons and extend the analysis to QD arrays.\nUsing a mapping of the BLGQD chain to an effective bilinear-biquadratic (BLBQ)\nspin model, we demonstrate that BLGQD arrays can work as a quantum simulator\nfor one-dimensional spin chains with emergent many-body topological phases.", "AI": {"tldr": "Bilayer Graphene Quantum Dots (BLGQD) can simulate a spin-1 chain exhibiting the Haldane phase, realizing topological quantum matter.", "motivation": "To explore the potential of BLGQD chains in simulating topological quantum matter, specifically the Haldane phase of a spin-1 chain.", "method": "Utilizing an atomistic tight-binding model and exact diagonalization to solve the interacting few-electron problem in BLGQDs, including Coulomb interactions and valley mixing. Investigating spin and valley polarization transitions and calculating low-energy states for single and double QDs. Mapping BLGQD chains to an effective bilinear-biquadratic (BLBQ) spin model.", "result": "Confirmed the presence of a spin-one ground state for two electrons in BLGQDs. Identified regimes of highly correlated multi-electron states. Demonstrated that BLGQD arrays can function as quantum simulators for 1D spin chains with emergent topological phases.", "conclusion": "BLGQD chains can effectively simulate spin-1 chains and host topological quantum matter like the Haldane phase, with BLGQD arrays acting as quantum simulators for 1D spin chains exhibiting emergent topological phases."}}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendon\u00e7a", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "Jo\u00e3o Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1 \u8bc4\u4f30\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u5728\u591a\u7ef4\u5ea6\u3001\u8bed\u8a00\u548c\u5b89\u5168\u65b9\u9762\u7684\u8868\u73b0\u3002\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u9762\uff0cLlama-3-8B \u57fa\u7ebf\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u5728\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u5b89\u5168\u68c0\u6d4b\u65b9\u9762\uff0c\u53c2\u8d5b\u961f\u4f0d\u5728\u591a\u8bed\u8a00\u5b50\u4efb\u52a1\u4e0a\u4f18\u4e8e Llama-Guard-3-1B \u57fa\u7ebf\uff0c\u4f46\u5728\u6587\u5316\u5b50\u4efb\u52a1\u4e0a\u57fa\u7ebf\u8868\u73b0\u66f4\u4f73\uff0c\u51f8\u663e\u4e86\u6587\u5316\u610f\u8bc6\u5b89\u5168\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4f20\u7edf\u6307\u6807\u4e0d\u8db3\u3001\u5b89\u5168\u5b9a\u4e49\u72ed\u9698\u6216\u5b58\u5728\u6587\u5316\u504f\u89c1\u7684\u60c5\u51b5\u4e0b\uff0cDSTC12 Track 1 \u65e8\u5728\u5f25\u5408\u8fd9\u4e9b\u5dee\u8ddd\u3002", "method": "Track 1 \u5305\u542b\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a(1) \u5bf9\u8bdd\u7ea7\u3001\u591a\u7ef4\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c(2) \u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u5b89\u5168\u68c0\u6d4b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u63d0\u4f9b\u7ed9\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\uff0c\u4ee5\u53ca\u4e24\u4e2a\u5b50\u4efb\u52a1\u7684\u63d0\u4ea4\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5b50\u4efb\u52a1 1\uff1aLlama-3-8B \u57fa\u7ebf\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747 Spearman \u76f8\u5173\u7cfb\u6570 (0.1681)\u3002\u5b50\u4efb\u52a1 2\uff1a\u53c2\u8d5b\u961f\u4f0d\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5b50\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e Llama-Guard-3-1B \u57fa\u7ebf (\u6700\u9ad8 ROC-AUC 0.9648)\uff0c\u4f46\u5728\u6587\u5316\u5b50\u96c6\u4e0a\u57fa\u7ebf\u8868\u73b0\u66f4\u4f18 (0.5126 ROC-AUC)\u3002", "conclusion": "DSTC12 Track 1 \u7684\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u5728\u591a\u7ef4\u5ea6\u548c\u5c24\u5176\u662f\u5728\u6587\u5316\u610f\u8bc6\u5b89\u5168\u65b9\u9762\u4ecd\u9700\u91cd\u5927\u6539\u8fdb\u3002"}}
{"id": "2509.13398", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.13398", "abs": "https://arxiv.org/abs/2509.13398", "authors": ["Stephan Troyer", "Florian Fechtel", "Lorenz Hummer", "Henning Rudolph", "Benjamin A. Stickler", "Uro\u0161 Deli\u0107", "Markus Arndt"], "title": "Quantum ground-state cooling of two librational modes of a nanorotor", "comment": null, "summary": "Controlling the motion of nanoscale objects at the quantum limit promises new\ntests of quantum mechanics and advanced sensors. Rotational motion is of\nparticular interest, as it follows nonlinear dynamics in a compact, closed\nconfiguration space, which opens up a plethora of phenomena and applications\nbeyond the possibilities of free or trapped linear motion. A prerequisite for\nsuch experiments is the capability to trap nanorotors and initialize them in a\nquantum ground state of libration. Here, we demonstrate the reliable,\nrepetitive laser-induced loading of silica nanodimers and trimers into an\noptical tweezer. Coherent scattering in a high-finesse cavity allows us to cool\ntwo different librational modes to the quantum ground state with occupation\nnumbers as low as $n_{\\beta}=0.54\\pm0.32$ and $n_{\\alpha}=0.21\\pm0.03$. By\nsimultaneously cooling both degrees of freedom ($n_\\beta=0.73\\pm0.22$,\n$n_\\alpha=1.02\\pm0.08$) we align nanorotors to a space-fixed axis with\nprecision better than 20$\\,\\mu$rad, close to the zero-point amplitude of\nlibrations.", "AI": {"tldr": "\u901a\u8fc7\u5149\u5b66\u954a\u5b50\u6355\u83b7\u5e76\u51b7\u5374\u7eb3\u7c73\u4e8c\u805a\u4f53\u548c\u4e09\u805a\u4f53\uff0c\u4f7f\u5176\u632f\u52a8\u6a21\u5f0f\u8fbe\u5230\u91cf\u5b50\u57fa\u6001\uff0c\u4ece\u800c\u7cbe\u786e\u63a7\u5236\u7eb3\u7c73\u65cb\u8f6c\u5668\u7684\u8fd0\u52a8\u3002", "motivation": "\u63a7\u5236\u7eb3\u7c73\u7269\u4f53\u5728\u91cf\u5b50\u6781\u9650\u4e0b\u7684\u8fd0\u52a8\uff0c\u4ee5\u8fdb\u884c\u65b0\u7684\u91cf\u5b50\u529b\u5b66\u68c0\u9a8c\u548c\u5f00\u53d1\u5148\u8fdb\u7684\u4f20\u611f\u5668\u3002\u7279\u522b\u5730\uff0c\u5bf9\u65cb\u8f6c\u8fd0\u52a8\u7684\u63a7\u5236\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u5176\u5177\u6709\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\u3002", "method": "\u4f7f\u7528\u5149\u5b66\u954a\u5b50\u6355\u83b7\u4e8c\u6c27\u5316\u7845\u7eb3\u7c73\u4e8c\u805a\u4f53\u548c\u4e09\u805a\u4f53\uff0c\u5e76\u901a\u8fc7\u8154\u589e\u5f3a\u76f8\u5e72\u6563\u5c04\u6280\u672f\u51b7\u5374\u4e24\u79cd\u4e0d\u540c\u7684\u632f\u52a8\u6a21\u5f0f\uff08$\beta$\u548c$\beta$\u6a21\u5f0f\uff09\u81f3\u91cf\u5b50\u57fa\u6001\uff0c\u540c\u65f6\u51b7\u5374\u8fd9\u4e24\u79cd\u81ea\u7531\u5ea6\u3002", "result": "\u6210\u529f\u5c06\u4e24\u79cd\u632f\u52a8\u6a21\u5f0f\u51b7\u5374\u81f3\u91cf\u5b50\u57fa\u6001\uff0c\u5360\u636e\u6570\u4f4e\u81f3 $n_{\beta}=0.54\text{\u00b1}0.32$ \u548c $n_{\text{\u03b1}}=0.21\text{\u00b1}0.03$\u3002\u540c\u65f6\u51b7\u5374\u4e24\u79cd\u81ea\u7531\u5ea6\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e20$\text{\u03bc}$rad\u7684\u7cbe\u5ea6\uff0c\u63a5\u8fd1\u96f6\u70b9\u632f\u5e45\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u5149\u5b66\u954a\u5b50\u4e2d\u6355\u83b7\u548c\u51b7\u5374\u7eb3\u7c73\u65cb\u8f6c\u5668\u81f3\u91cf\u5b50\u57fa\u6001\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u7eb3\u7c73\u65cb\u8f6c\u5668\u7684\u9ad8\u7cbe\u5ea6\u7a7a\u95f4\u5b9a\u5411\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7684\u91cf\u5b50\u5b9e\u9a8c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.14032", "categories": ["cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14032", "abs": "https://arxiv.org/abs/2509.14032", "authors": ["Philip Jordan", "Maryam Kamgarpour"], "title": "Nash Equilibria in Games with Playerwise Concave Coupling Constraints: Existence and Computation", "comment": null, "summary": "We study the existence and computation of Nash equilibria in continuous\nstatic games where the players' admissible strategies are subject to shared\ncoupling constraints, i.e., constraints that depend on their \\emph{joint}\nstrategies. Specifically, we focus on a class of games characterized by\nplayerwise concave utilities and playerwise concave constraints. Prior results\non the existence of Nash equilibria are not applicable to this class, as they\nrely on strong assumptions such as joint convexity of the feasible set. By\nleveraging topological fixed point theory and novel structural insights into\nthe contractibility of feasible sets under playerwise concave constraints, we\ngive an existence proof for Nash equilibria under weaker conditions. Having\nestablished existence, we then focus on the computation of Nash equilibria via\nindependent gradient methods under the additional assumption that the utilities\nadmit a potential function. To account for the possibly nonconvex feasible\nregion, we employ a log barrier regularized gradient ascent with adaptive\nstepsizes. Starting from an initial feasible strategy profile and under exact\ngradient feedback, the proposed method converges to an $\\epsilon$-approximate\nconstrained Nash equilibrium within $\\mathcal{O}(\\epsilon^{-3})$ iterations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5177\u6709\u5171\u4eab\u8026\u5408\u7ea6\u675f\u7684\u8fde\u7eed\u9759\u6001\u535a\u5f08\u4e2d\u7eb3\u4ec0\u5747\u8861\u7684\u5b58\u5728\u6027\u548c\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u7eb3\u4ec0\u5747\u8861\u5b58\u5728\u6027\u7684\u7ed3\u679c\u4e0d\u9002\u7528\u4e8e\u73a9\u5bb6\u6548\u7528\u548c\u7ea6\u675f\u4e3a\u73a9\u5bb6\u6574\u4f53\u51f9\u51fd\u6570\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7ed3\u679c\u4f9d\u8d56\u4e8e\u5bf9\u53ef\u884c\u96c6\u6574\u4f53\u51f8\u6027\u7b49\u5f3a\u5047\u8bbe\u3002", "method": "\u5229\u7528\u62d3\u6251\u4e0d\u52a8\u70b9\u7406\u8bba\u548c\u5bf9\u73a9\u5bb6\u6574\u4f53\u51f9\u7ea6\u675f\u4e0b\u53ef\u884c\u96c6\u6536\u7f29\u6027\u7684\u65b0\u7ed3\u6784\u6d1e\u5bdf\uff0c\u5728\u8f83\u5f31\u6761\u4ef6\u4e0b\u8bc1\u660e\u4e86\u7eb3\u4ec0\u5747\u8861\u7684\u5b58\u5728\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5047\u8bbe\u6548\u7528\u51fd\u6570\u5b58\u5728\u52bf\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5bf9\u6570\u969c\u788d\u6b63\u5219\u5316\u68af\u5ea6\u4e0a\u5347\u6cd5\u548c\u81ea\u9002\u5e94\u6b65\u957f\u6765\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\u7684\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6536\u655b\u5230\u8fd1\u4f3c\u7ea6\u675f\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u4e14\u5728\u5177\u6709\u7cbe\u786e\u68af\u5ea6\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u8fed\u4ee3\u6b21\u6570\u4e3aO(\u03b5-3)\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u8f83\u5f31\u7684\u5047\u8bbe\u4e0b\u8bc1\u660e\u4e86\u7eb3\u4ec0\u5747\u8861\u7684\u5b58\u5728\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2509.13456", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.13456", "abs": "https://arxiv.org/abs/2509.13456", "authors": ["Jian Yan", "Hikaru Takeda", "Haruka Iwahata", "Jun-ichi Yamaura", "Rajesh Kumar Ulaganathan", "Kalaivanan Raju", "Raman Sankar", "Minoru Yamashita"], "title": "Field-Angle Dependence of Phonon Thermal Hall Effect in Na2X2TeO6 (X = Co, Zn)", "comment": "11 pages, 5 figures, and Supplementary Materials. To appear in\n  Scientific Reports", "summary": "The mechanism behind thermal Hall effects by phonons, which are observed in\nvarious materials, is not clarified despite the dominant contribution as heat\ncarriers. Theoretically, mechanisms based on the intrinsic Berry phase and\nthose on extrinsic impurity-induced scatterings have been proposed, which can\nbe distinguished by comparing the field-angle dependence of the thermal Hall\neffect and that of the magnetic anisotropy. Here, we investigate the\nfield-angle dependence of the thermal Hall effects in the antiferromagnet\nNa2Co2TeO6 and its non-magnetic isostructural analogue Na2Zn2TeO6 in the ac\nplane. We find that the field-angle dependence of the thermal Hall conductivity\nin both materials well follows that of the out-of-plane magnetization, showing\na common mechanism by extrinsic impurity-induced scatterings in both the phonon\nthermal Hall effect and that enhanced by a coupling with the magnetism.", "AI": {"tldr": "\u6750\u6599\u4e2d\u7684\u58f0\u5b50\u70ed\u970d\u5c14\u6548\u5e94\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u901a\u8fc7\u6bd4\u8f83\u70ed\u970d\u5c14\u6548\u5e94\u548c\u78c1\u5404\u5411\u5f02\u6027\u7684\u78c1\u573a\u89d2\u5ea6\u4f9d\u8d56\u6027\uff0c\u53ef\u4ee5\u533a\u5206\u57fa\u4e8e\u8d1d\u91cc\u76f8\u548c\u6742\u8d28\u6563\u5c04\u7684\u673a\u5236\u3002\u7814\u7a76\u4e86Na2Co2TeO6\u548cNa2Zn2TeO6\u4e2d\u7684\u70ed\u970d\u5c14\u6548\u5e94\uff0c\u53d1\u73b0\u4e24\u8005\u5728ac\u5e73\u9762\u7684\u78c1\u573a\u89d2\u5ea6\u4f9d\u8d56\u6027\u90fd\u4e0e\u5782\u76f4\u78c1\u5316\u5f3a\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u4e24\u79cd\u6750\u6599\u4e2d\u90fd\u5b58\u5728\u7531\u6742\u8d28\u8bf1\u5bfc\u7684\u6563\u5c04\u673a\u5236\u3002", "motivation": "\u9610\u660e\u6750\u6599\u4e2d\u70ed\u970d\u5c14\u6548\u5e94\uff08\u7531\u58f0\u5b50\u5f15\u8d77\uff09\u7684\u673a\u5236\uff0c\u56e0\u4e3a\u58f0\u5b50\u662f\u4e3b\u8981\u7684\u4f20\u70ed\u8f7d\u6d41\u5b50\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u91ca\u3002", "method": "\u6bd4\u8f83\u4e86\u53cd\u94c1\u78c1\u4f53Na2Co2TeO6\u53ca\u5176\u975e\u78c1\u6027\u7c7b\u4f3c\u7269Na2Zn2TeO6\u5728ac\u5e73\u9762\u4e0a\u7684\u70ed\u970d\u5c14\u6548\u5e94\u548c\u78c1\u5404\u5411\u5f02\u6027\u7684\u78c1\u573a\u89d2\u5ea6\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e24\u79cd\u6750\u6599\u4e2d\uff0c\u70ed\u5bfc\u7387\u7684\u78c1\u573a\u89d2\u5ea6\u4f9d\u8d56\u6027\u90fd\u4e0e\u5782\u76f4\u78c1\u5316\u5f3a\u5ea6\u7684\u78c1\u573a\u89d2\u5ea6\u4f9d\u8d56\u6027\u4e00\u81f4\u3002\u8fd9\u8868\u660e\u58f0\u5b50\u70ed\u970d\u5c14\u6548\u5e94\u548c\u58f0\u5b50-\u78c1\u8026\u5408\u589e\u5f3a\u7684\u70ed\u970d\u5c14\u6548\u5e94\u5177\u6709\u5171\u540c\u7684\u673a\u5236\uff0c\u5373\u7531\u6742\u8d28\u5f15\u8d77\u7684\u6563\u5c04\u3002", "conclusion": "\u5728Na2Co2TeO6\u548cNa2Zn2TeO6\u4e2d\uff0c\u70ed\u970d\u5c14\u6548\u5e94\u662f\u7531\u6742\u8d28\u8bf1\u5bfc\u7684\u6563\u5c04\u5f15\u8d77\u7684\uff0c\u8fd9\u4e0e\u78c1\u6027\u8026\u5408\u65e0\u5173\u3002"}}
{"id": "2509.13710", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13710", "abs": "https://arxiv.org/abs/2509.13710", "authors": ["Hongyi Li", "Songchen Ma", "Huanyu Qu", "Weihao Zhang", "Jia Chen", "Junfeng Lin", "Fengbin Tu", "Rong Zhao"], "title": "CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized\nvarious aspects of human life, yet their immense computational and energy\ndemands pose significant challenges for efficient inference. The memory wall,\nthe growing processor-memory speed disparity, remains a critical bottleneck for\nLLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating\ncompute units with memory, leveraging 5-20$\\times$ higher internal bandwidth\nand enabling greater energy efficiency than GPUs. However, existing PIMs\nstruggle to balance flexibility, performance, and cost-efficiency for LLMs'\ndynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from\ninter-bank communication overhead despite its vector parallelism. SRAM-PIM\noffers sub-10ns latency for matrix operation but is constrained by limited\ncapacity. This work introduces CompAir, a novel PIM architecture that\nintegrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear\ncomputations while unlocking multi-granularity data pathways. We further\ndevelop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic\nlogic unit that performs non-linear operations during data movement,\nsimultaneously reducing communication overhead and area cost. Finally, we\ndevelop a hierarchical Instruction Set Architecture that ensures both\nflexibility and programmability of the hybrid PIM. Experimental results\ndemonstrate that CompAir achieves 1.83-7.98$\\times$ prefill and\n1.95-6.28$\\times$ decode improvement over the current state-of-the-art fully\nPIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir\nachieves 3.52$\\times$ energy consumption reduction with comparable throughput.\nThis work represents the first systematic exploration of hybrid DRAM-PIM and\nSRAM-PIM architectures with in-network computation capabilities, offering a\nhigh-efficiency solution for LLM.", "AI": {"tldr": "CompAir\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6df7\u5408PIM\u67b6\u6784\uff0c\u7ed3\u5408\u4e86DRAM-PIM\u548cSRAM-PIM\uff0c\u5e76\u901a\u8fc7\u7247\u4e0a\u7f51\u7edc\uff08NoC\uff09\u8fdb\u884c\u589e\u5f3a\uff0c\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u9700\u6c42\u5de8\u5927\uff0c\u5185\u5b58\u5899\uff08\u5904\u7406\u5668-\u5185\u5b58\u901f\u5ea6\u5dee\u8ddd\uff09\u662fLLM\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u7684\u5904\u7406\u5185\u5b58\uff08PIM\uff09\u67b6\u6784\u5728\u7075\u6d3b\u6027\u3001\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u65b9\u9762\u96be\u4ee5\u5e73\u8861LLM\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompAir\u7684\u65b0\u578bPIM\u67b6\u6784\uff0c\u5b83\u96c6\u6210\u4e86DRAM-PIM\u548cSRAM-PIM\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u952e\u5408\u6280\u672f\u3002\u5f00\u53d1\u4e86CompAir-NoC\uff0c\u4e00\u4e2a\u5d4c\u5165\u4e86\u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09\u7684\u5148\u8fdb\u7f51\u7edc\u82af\u7247\uff08NoC\uff09\uff0c\u53ef\u5728\u6570\u636e\u79fb\u52a8\u65f6\u6267\u884c\u975e\u7ebf\u6027\u8fd0\u7b97\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5206\u5c42\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u4ee5\u786e\u4fdd\u6df7\u5408PIM\u7684\u7075\u6d3b\u6027\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "result": "\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5168PIM\u67b6\u6784\u76f8\u6bd4\uff0cCompAir\u5728\u9884\u586b\u5145\uff08prefill\uff09\u9636\u6bb5\u5b9e\u73b0\u4e861.83-7.98\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u89e3\u7801\uff08decode\uff09\u9636\u6bb5\u5b9e\u73b0\u4e861.95-6.28\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002\u4e0e\u6df7\u5408A100\u548cHBM-PIM\u7cfb\u7edf\u76f8\u6bd4\uff0cCompAir\u5728\u541e\u5410\u91cf\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u8017\u964d\u4f4e\u4e863.52\u500d\u3002", "conclusion": "CompAir\u662f\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u6027\u63a2\u7d22\u5177\u6709\u7f51\u7edc\u5185\u8ba1\u7b97\u80fd\u529b\u7684\u6df7\u5408DRAM-PIM\u548cSRAM-PIM\u67b6\u6784\u7684\u5de5\u4f5c\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u9ad8\u80fd\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14089", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.14089", "abs": "https://arxiv.org/abs/2509.14089", "authors": ["Luca Aceto", "Antonis Achilleos", "Aggeliki Chalki", "Anna Ing\u00f3lfsd\u00f3ttir"], "title": "The Complexity of Deciding Characteristic Formulae Modulo Nested Simulation (extended abstract)", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258. A full version of this\n  paper, containing all proofs, appears at arXiv:2505.22277", "summary": "This paper studies the complexity of determining whether a formula in the\nmodal logics characterizing the nested-simulation semantics is characteristic\nfor some process, which is equivalent to determining whether the formula is\nsatisfiable and prime. The main results are that the problem of determining\nwhether a formula is prime in the modal logic characterizing the\n2-nested-simulation preorder is coNP-complete and is PSPACE-complete in the\ncase of the n-nested-simulation preorder, when n>=3. This establishes that\ndeciding characteristic formulae for the n-nested simulation semantics is\nPSPACE-complete, when n>=3. In the case of the 2-nested simulation semantics,\nthat problem lies in the complexity class DP, which consists of languages that\ncan be expressed as the intersection of one language in NP and of one in coNP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u786e\u5b9a\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\u7684\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u516c\u5f0f\u662f\u5426\u662f\u7279\u5f81\u516c\u5f0f\u7684\u590d\u6742\u6027\uff0c\u8fd9\u7b49\u4ef7\u4e8e\u786e\u5b9a\u516c\u5f0f\u662f\u5426\u53ef\u6ee1\u8db3\u548c\u662f\u4e3b\u8981\u7684\u3002", "motivation": "\u7814\u7a76\u786e\u5b9a\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\u7684\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u516c\u5f0f\u662f\u5426\u662f\u7279\u5f81\u516c\u5f0f\u7684\u590d\u6742\u6027\u3002", "method": "\u7814\u7a76\u6a21\u6001\u903b\u8f91\u7684\u7279\u5f81\u516c\u5f0f\u95ee\u9898\u3002", "result": "\u5bf9\u4e8e\u4e8c\u91cd\u5d4c\u5957\u6a21\u62df\u524d\u5e8f\u5173\u7cfb\uff0c\u8be5\u95ee\u9898\u662fcoNP-\u5b8c\u5168\u7684\uff1b\u5bf9\u4e8en>=3\u7684n\u91cd\u5d4c\u5957\u6a21\u62df\u524d\u5e8f\u5173\u7cfb\uff0c\u8be5\u95ee\u9898\u662fPSPACE-\u5b8c\u5168\u7684\u3002\u56e0\u6b64\uff0c\u51b3\u5b9an>=3\u7684n\u91cd\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\u7684\u7279\u5f81\u516c\u5f0f\u662fPSPACE-\u5b8c\u5168\u7684\u3002\u5bf9\u4e8e\u4e8c\u91cd\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\uff0c\u8be5\u95ee\u9898\u5c5e\u4e8eDP\u7c7b\u3002", "conclusion": "\u786e\u5b9an>=3\u7684n\u91cd\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\u7684\u7279\u5f81\u516c\u5f0f\u662fPSPACE-\u5b8c\u5168\u7684\u3002\u5bf9\u4e8e\u4e8c\u91cd\u5d4c\u5957\u6a21\u62df\u8bed\u4e49\uff0c\u8be5\u95ee\u9898\u5c5e\u4e8eDP\u7c7b\u3002"}}
{"id": "2509.13891", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.13891", "abs": "https://arxiv.org/abs/2509.13891", "authors": ["Tsz Chiu Kwok", "Zhewei Wei", "Mingji Yang"], "title": "On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time", "comment": "46 pages", "summary": "We initiate a study of solving a row/column diagonally dominant (RDD/CDD)\nlinear system $Mx=b$ in sublinear time, with the goal of estimating\n$t^{\\top}x^*$ for a given vector $t\\in R^n$ and a specific solution $x^*$. This\nsetting naturally generalizes the study of sublinear-time solvers for symmetric\ndiagonally dominant (SDD) systems [AKP19] to the asymmetric case.\n  Our first contributions are characterizations of the problem's mathematical\nstructure. We express a solution $x^*$ via a Neumann series, prove its\nconvergence, and upper bound the truncation error on this series through a\nnovel quantity of $M$, termed the maximum $p$-norm gap. This quantity\ngeneralizes the spectral gap of symmetric matrices and captures how the\nstructure of $M$ governs the problem's computational difficulty.\n  For systems with bounded maximum $p$-norm gap, we develop a collection of\nalgorithmic results for locally approximating $t^{\\top}x^*$ under various\nscenarios and error measures. We derive these results by adapting the\ntechniques of random-walk sampling, local push, and their bidirectional\ncombination, which have proved powerful for special cases of solving RDD/CDD\nsystems, particularly estimating PageRank and effective resistance on graphs.\nOur general framework yields deeper insights, extended results, and improved\ncomplexity bounds for these problems. Notably, our perspective provides a\nunified understanding of Forward Push and Backward Push, two fundamental\napproaches for estimating random-walk probabilities on graphs.\n  Our framework also inherits the hardness results for sublinear-time SDD\nsolvers and local PageRank computation, establishing lower bounds on the\nmaximum $p$-norm gap or the accuracy parameter. We hope that our work opens the\ndoor for further study into sublinear solvers, local graph algorithms, and\ndirected spectral graph theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u4e9a\u7ebf\u6027\u65f6\u95f4\u5185\u6c42\u89e3\u884c/\u5217\u5bf9\u89d2\u5360\u4f18\uff08RDD/CDD\uff09\u7ebf\u6027\u7cfb\u7edf $Mx=b$\uff0c\u76ee\u6807\u662f\u4f30\u8ba1\u7ed9\u5b9a\u5411\u91cf $t$ \u548c\u7279\u5b9a\u89e3 $x^*$ \u7684 $t^{\top}x^*$\u3002", "motivation": "\u5c06\u4e9a\u7ebf\u6027\u65f6\u95f4\u6c42\u89e3\u5668\u7814\u7a76\u4ece\u5bf9\u79f0\u5bf9\u89d2\u5360\u4f18\uff08SDD\uff09\u7cfb\u7edf\u63a8\u5e7f\u5230\u975e\u5bf9\u79f0\u60c5\u51b5\u3002", "method": "\u901a\u8fc7 Neumann \u7ea7\u6570\u8868\u8fbe $x^*$\uff0c\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5e76\u5f15\u5165\u201c\u6700\u5927p\u8303\u6570\u5dee\u201d\u6765\u754c truncation error\u3002\u8be5\u65b9\u6cd5\u63a8\u5e7f\u4e86SDD\u77e9\u9635\u7684\u8c31\u9699\u3002", "result": "\u5728\u6700\u5927p\u8303\u6570\u5dee\u6709\u754c\u7684\u5047\u8bbe\u4e0b\uff0c\u5f00\u53d1\u4e86\u7528\u4e8e\u5c40\u90e8\u8fd1\u4f3c $t^{\top}x^*$ \u7684\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u968f\u673a\u6e38\u8d70\u91c7\u6837\u3001\u5c40\u90e8\u63a8\u8fdb\u7b49\u6280\u672f\u7684\u590d\u6742\u6027\u754c\u9650\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u52a0\u6df1\u4e86\u5bf9Forward Push\u548cBackward Push\u7b49\u56fe\u7b97\u6cd5\u7684\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86\u4e9a\u7ebf\u6027\u6c42\u89e3\u5668\u548c\u5c40\u90e8\u56fe\u7b97\u6cd5\u7684\u786c\u5ea6\u7ed3\u679c\u3002"}}
{"id": "2509.13349", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13349", "abs": "https://arxiv.org/abs/2509.13349", "authors": ["Jed Guzelkabaagac", "Boris Petrovi\u0107"], "title": "Label-Efficient Grasp Joint Prediction with Point-JEPA", "comment": "4 pages, 5 figures. Submitted to IROS 2025 Workshop", "summary": "We investigate whether 3D self-supervised pretraining with a Joint-Embedding\nPredictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle\nprediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained\nPoint-JEPA encoder, we train a lightweight multi-hypothesis head with\nwinner-takes-all and evaluate by top-logit selection. On DLR-Hand II with\nobject-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes\nand reaches parity with full supervision. These results suggest JEPA-style\npretraining is a practical approach for data-efficient grasp learning.", "AI": {"tldr": "Point-JEPA\u901a\u8fc73D\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6765\u63d0\u9ad8\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u7684\u6570\u636e\u6548\u7387\uff0c\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u663e\u8457\u964d\u4f4eRMSE\u5e76\u8fbe\u5230\u5b8c\u5168\u76d1\u7763\u7684\u6c34\u5e73\u3002", "motivation": "\u7814\u7a763D\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08Point-JEPA\uff09\u5728\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u4e2d\u7684\u6807\u7b7e\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7ShapeNet\u9884\u8bad\u7ec3\u7684Point-JEPA\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u70b9\u4e91\u548c\u8f7b\u91cf\u7ea7\u591a\u5047\u8bbe\u9884\u6d4b\u5934\uff0c\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728DLR-Hand II\u6570\u636e\u96c6\u4e0a\uff0cPoint-JEPA\u5728\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u5c06RMSE\u964d\u4f4e\u4e86\u9ad8\u8fbe26%\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u5b8c\u5168\u76d1\u7763\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "JEPA\u98ce\u683c\u7684\u9884\u8bad\u7ec3\u662f\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u6293\u53d6\u5b66\u4e60\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2509.13520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13520", "abs": "https://arxiv.org/abs/2509.13520", "authors": ["Varun Kumar", "Jing Bi", "Cyril Ngo Ngoc", "Victor Oancea", "George Em Karniadakis"], "title": "Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework", "comment": null, "summary": "Neural surrogates and operator networks for solving partial differential\nequation (PDE) problems have attracted significant research interest in recent\nyears. However, most existing approaches are limited in their ability to\ngeneralize solutions across varying non-parametric geometric domains. In this\nwork, we address this challenge in the context of Polyethylene Terephthalate\n(PET) bottle buckling analysis, a representative packaging design problem\nconventionally solved using computationally expensive finite element analysis\n(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously\npredicts nodal displacement fields and the time evolution of reaction forces\nduring top load compression. Our methodology is evaluated on two families of\nbottle geometries parameterized by two and four design variables. Training data\nis generated using nonlinear FEA simulations in Abaqus for 254 unique designs\nper family. The proposed framework achieves mean relative $L^{2}$ errors of\n2.5-13% for displacement fields and approximately 2.4% for time-dependent\nreaction forces for the four-parameter bottle family. Point-wise error analyses\nfurther show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,\nwith the largest discrepancies confined to localized geometric regions.\nImportantly, the model accurately captures key physical phenomena, such as\nbuckling behavior, across diverse bottle geometries. These results highlight\nthe potential of our framework as a scalable and computationally efficient\nsurrogate, particularly for multi-task predictions in computational mechanics\nand applications requiring rapid design evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDeepONet-Transolver\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684PET\u74f6\u5c48\u66f2\u5206\u6790\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5728\u975e\u53c2\u6570\u51e0\u4f55\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u4f4d\u79fb\u573a\u548c\u53cd\u4f5c\u7528\u529b\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u5728\u5904\u7406\u53d8\u5316\u7684\u975e\u53c2\u6570\u51e0\u4f55\u57df\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cPET\u74f6\u5c48\u66f2\u5206\u6790\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5305\u88c5\u8bbe\u8ba1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepONet-Transolver\u7684\u6df7\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u8282\u70b9\u4f4d\u79fb\u573a\u548c\u53cd\u4f5c\u7528\u529b\u968f\u65f6\u95f4\u6f14\u53d8\u3002\u901a\u8fc7\u4f7f\u7528Abaqus\u8fdb\u884c\u975e\u7ebf\u6027\u6709\u9650\u5143\u5206\u6790\uff08FEA\uff09\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5728\u4e24\u7c7b\u74f6\u5b50\u51e0\u4f55\u5f62\u72b6\uff08\u7531\u4e24\u4e2a\u548c\u56db\u4e2a\u8bbe\u8ba1\u53d8\u91cf\u53c2\u6570\u5316\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5bf9\u4e8e\u56db\u53c2\u6570\u74f6\u5b50\u5bb6\u65cf\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4f4d\u79fb\u573a\u65b9\u9762\u5b9e\u73b0\u4e862.5-13%\u7684\u5e73\u5747\u76f8\u5bf9L2\u8bef\u5dee\uff0c\u5728\u968f\u65f6\u95f4\u53d8\u5316\u7684\u529b\u65b9\u9762\u5b9e\u73b0\u4e86\u7ea62.4%\u7684\u8bef\u5dee\u3002\u70b9\u4f4d\u79fb\u8bef\u5dee\u5206\u6790\u663e\u793a\u7edd\u5bf9\u4f4d\u79fb\u8bef\u5dee\u572810^-4\u523010^-3\u91cf\u7ea7\uff0c\u4e14\u6700\u5927\u7684\u5dee\u5f02\u5c40\u9650\u4e8e\u5c40\u90e8\u51e0\u4f55\u533a\u57df\u3002\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u5230\u5404\u79cd\u74f6\u5b50\u51e0\u4f55\u5f62\u72b6\u4e2d\u7684\u5c48\u66f2\u884c\u4e3a\u7b49\u5173\u952e\u7269\u7406\u73b0\u8c61\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u8ba1\u7b97\u529b\u5b66\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u548c\u9700\u8981\u5feb\u901f\u8bbe\u8ba1\u8bc4\u4f30\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u3001\u53ef\u6269\u5c55\u7684\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.13703", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13703", "abs": "https://arxiv.org/abs/2509.13703", "authors": ["Sriram Srinivasan", "Hamdan Alabsi", "Rand Obeidat", "Nithisha Ponnala", "Azene Zenebe"], "title": "GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach", "comment": null, "summary": "We present the design, implementation, and comprehensive evaluation of a\nspecialized course on GPU architecture, GPU programming, and how these are used\nfor developing AI agents. This course is offered to undergraduate and graduate\nstudents during Fall 2024 and Spring 2025. The course began with foundational\nconcepts in GPU/CPU hardware and parallel computing and progressed to develop\nRAG and optimizing them using GPUs. Students gained experience provisioning and\nconfiguring cloud-based GPU instances, implementing parallel algorithms, and\ndeploying scalable AI solutions. We evaluated learning outcomes through\nassessments, course evaluations, and anonymous surveys. The results reveal that\n(1) AWS served as an effective and economical platform for practical GPU\nprogramming, (2) experiential learning significantly enhanced technical\nproficiency and engagement, and (3) the course strengthened students'\nproblem-solving and critical thinking skills through tools such as TensorBoard\nand HPC profilers, which exposed performance bottlenecks and scaling issues.\nOur findings underscore the pedagogical value of integrating parallel computing\ninto STEM education. We advocate for broader adoption of similar electives\nacross STEM curricula to prepare students for the demands of modern,\ncompute-intensive fields.", "AI": {"tldr": "\u672c\u8bfe\u7a0b\u6559\u6388GPU\u67b6\u6784\u3001\u7f16\u7a0b\u53ca\u5728AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u63d0\u5347\u5b66\u751f\u5728\u73b0\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u578b\u9886\u57df\u7684\u7ade\u4e89\u529b\u3002", "motivation": "\u4e3a\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u63d0\u4f9bGPU\u67b6\u6784\u3001\u7f16\u7a0b\u53caAI\u4ee3\u7406\u5f00\u53d1\u4e13\u9879\u8bfe\u7a0b\uff0c\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u578b\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u8bfe\u7a0b\u5185\u5bb9\u6db5\u76d6GPU/CPU\u786c\u4ef6\u57fa\u7840\u3001\u5e76\u884c\u8ba1\u7b97\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5f00\u53d1\u4e0eGPU\u4f18\u5316\uff0c\u5e76\u5305\u542b\u4e91GPU\u5b9e\u4f8b\u914d\u7f6e\u3001\u5e76\u884c\u7b97\u6cd5\u5b9e\u73b0\u548c\u53ef\u6269\u5c55AI\u89e3\u51b3\u65b9\u6848\u90e8\u7f72\u7684\u5b9e\u8df5\u7ecf\u9a8c\u3002\u901a\u8fc7\u8bc4\u4f30\u3001\u8bfe\u7a0b\u53cd\u9988\u548c\u533f\u540d\u8c03\u67e5\u6765\u8861\u91cf\u5b66\u4e60\u6548\u679c\u3002", "result": "1. AWS\u662f\u8fdb\u884cGPU\u7f16\u7a0b\u7684\u6709\u6548\u4e14\u7ecf\u6d4e\u7684\u5e73\u53f0\u3002 2. \u5b9e\u8df5\u7ecf\u9a8c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u7684\u6280\u80fd\u6c34\u5e73\u548c\u53c2\u4e0e\u5ea6\u3002 3. \u8bfe\u7a0b\u5229\u7528TensorBoard\u548cHPC\u5206\u6790\u5668\u7b49\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u5b66\u751f\u89e3\u51b3\u95ee\u9898\u548c\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\uff0c\u66b4\u9732\u4e86\u6027\u80fd\u74f6\u9888\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "conclusion": "\u5c06\u5e76\u884c\u8ba1\u7b97\u7eb3\u5165STEM\u6559\u80b2\u5177\u6709\u91cd\u8981\u7684\u6559\u5b66\u4ef7\u503c\uff0c\u5efa\u8bae\u5728STEM\u8bfe\u7a0b\u4e2d\u63a8\u5e7f\u6b64\u7c7b\u9009\u4fee\u8bfe\uff0c\u4ee5\u57f9\u517b\u5b66\u751f\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u9886\u57df\u7684\u80fd\u529b\u3002"}}
{"id": "2509.13600", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13600", "abs": "https://arxiv.org/abs/2509.13600", "authors": ["Argyris Kriezis", "Yu-Hsuan Chen", "Dennis Akos", "Sherman Lo", "Todd Walter"], "title": "GNSS Jamming and Spoofing Monitoring Using Low-Cost COTS Receivers", "comment": "Submitted to ION NAVIGATION Journal", "summary": "The Global Navigation Satellite System (GNSS) is increasingly vulnerable to\nradio frequency interference (RFI), including jamming and spoofing, which\nthreaten the integrity of navigation and timing services. This paper presents a\nmethodology for detecting and classifying RFI events using low-cost commercial\noff-the-shelf (COTS) GNSS receivers. By combining carrier-to-noise ratio (C/N0)\nmeasurements with a calibrated received power metric, a two-dimensional\ndetection space is constructed to identify and distinguish nominal, jammed,\nspoofed, and blocked signal conditions. The method is validated through both\ncontrolled jamming tests in Norway and real-world deployments in Poland, and\nthe Southeast Mediterranean which have experienced such conditions. Results\ndemonstrate that COTS-based detection, when properly calibrated, offers a\nviable and effective approach for GNSS RFI monitoring.", "AI": {"tldr": "GNSS\u4fe1\u53f7\u6613\u53d7\u5e72\u6270\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f4e\u6210\u672cGNSS\u63a5\u6536\u673a\u7684RFI\u68c0\u6d4b\u548c\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408C/N0\u6d4b\u91cf\u548c\u63a5\u6536\u529f\u7387\uff0c\u6784\u5efa\u4e8c\u7ef4\u68c0\u6d4b\u7a7a\u95f4\uff0c\u53ef\u6709\u6548\u533a\u5206\u6b63\u5e38\u3001\u5e72\u6270\u3001\u6b3a\u9a97\u548c\u963b\u585e\u4fe1\u53f7\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "GNSS\u4fe1\u53f7\u65e5\u76ca\u53d7\u5230\u5c04\u9891\u5e72\u6270\uff08RFI\uff09\u7684\u5a01\u80c1\uff0c\u5a01\u80c1\u5bfc\u822a\u548c\u6388\u65f6\u670d\u52a1\u7684\u5b8c\u6574\u6027\u3002", "method": "\u7ed3\u5408\u8f7d\u566a\u6bd4\uff08C/N0\uff09\u6d4b\u91cf\u548c\u6821\u51c6\u7684\u63a5\u6536\u529f\u7387\u6307\u6807\uff0c\u6784\u5efa\u4e8c\u7ef4\u68c0\u6d4b\u7a7a\u95f4\uff0c\u4ee5\u8bc6\u522b\u548c\u533a\u5206\u6807\u79f0\u3001\u5e72\u6270\u3001\u6b3a\u9a97\u548c\u963b\u585e\u4fe1\u53f7\u6761\u4ef6\u3002", "result": "\u57fa\u4e8eCOTS\u7684\u68c0\u6d4b\u5728\u9002\u5f53\u6821\u51c6\u540e\uff0c\u4e3aGNSS RFI\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u6709\u6548\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eCOTS\u63a5\u6536\u673a\u7684RFI\u68c0\u6d4b\u548c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u88ab\u8bc1\u660e\u662f\u6709\u6548\u53ef\u884c\u7684\u3002"}}
{"id": "2509.13381", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13381", "abs": "https://arxiv.org/abs/2509.13381", "authors": ["Zhang Xueyao", "Yang Bo", "Yu Zhiwen", "Cao Xuelin", "George C. Alexandropoulos", "Merouane Debbah", "Chau Yuen"], "title": "Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach", "comment": "6 pages", "summary": "Autonomous Underwater Vehicles (AUVs) have shown great potential for\ncooperative detection and reconnaissance. However, collaborative AUV\ncommunications introduce risks of exposure. In adversarial environments,\nachieving efficient collaboration while ensuring covert operations becomes a\nkey challenge for underwater cooperative missions. In this paper, we propose a\nnovel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization\n(H-MAPPO) framework. The high-level component determines the individuals\nparticipating in the task based on a central AUV, while the low-level component\nreduces exposure probabilities through power and trajectory control by the\nparticipating AUVs. Simulation results show that the proposed framework\nachieves rapid convergence, outperforms benchmark algorithms in terms of\nperformance, and maximizes long-term cooperative efficiency while ensuring\ncovert operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08H-MAPPO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u534f\u540c\u6c34\u4e0b\u63a2\u6d4b\u548c\u4fa6\u5bdf\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u853d\u6027\u3002", "motivation": "\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\u548c\u9690\u853d\u64cd\u4f5c\u662f\u6c34\u4e0b\u534f\u540c\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\uff0c\u56e0\u4e3a\u534f\u540c\u901a\u4fe1\u4f1a\u5e26\u6765\u66b4\u9732\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u65f6\u95f4\u5c3a\u5ea6H-MAPPO\u6846\u67b6\u3002\u9ad8\u5c42\u7ec4\u4ef6\u7531\u4e2d\u592eAUV\u51b3\u5b9a\u53c2\u4e0e\u4efb\u52a1\u7684\u4e2a\u4f53\uff0c\u4f4e\u5c42\u7ec4\u4ef6\u901a\u8fc7\u53c2\u4e0eAUV\u7684\u529f\u7387\u548c\u8f68\u8ff9\u63a7\u5236\u6765\u964d\u4f4e\u66b4\u9732\u6982\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5177\u6709\u5feb\u901f\u6536\u655b\u6027\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u5e76\u5728\u786e\u4fdd\u9690\u853d\u64cd\u4f5c\u7684\u540c\u65f6\u6700\u5927\u5316\u4e86\u957f\u671f\u7684\u534f\u540c\u6548\u7387\u3002", "conclusion": "H-MAPPO\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6c34\u4e0b\u534f\u540c\u63a2\u6d4b\u548c\u4fa6\u5bdf\u4e2d\u7684\u6311\u6218\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u6548\u7387\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u901a\u4fe1\u66b4\u9732\u7684\u98ce\u9669\u3002"}}
{"id": "2509.13383", "categories": ["cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13383", "abs": "https://arxiv.org/abs/2509.13383", "authors": ["Boliang Lin", "Xiang Li", "Yuxue Gu", "Dishen Lu"], "title": "Location and allocation problem of high-speed train maintenance bases", "comment": null, "summary": "Maintenance bases are crucial for the safe and stable operation of high-speed\ntrains, necessitating significant financial investment for their construction\nand operation. Planning the location and task allocation of these bases in the\nvast high-speed railway network is a complex combinatorial optimization\nproblem. This paper explored the strategic planning of identifying optimal\nlocations for maintenance bases, introducing a bi-level programming model. The\nupper-level objective was to minimize the annualized total cost, including\ninvestment for new or expanding bases and total maintenance costs, while the\nlower-level focused on dispatching high-speed trains to the most suitable base\nfor maintenance tasks, thereby reducing maintenance operation dispatch costs\nunder various investment scenarios. A case study of the Northwest China\nhigh-speed rail network demonstrated the application of this model, and\nincluded the sensitivity analysis reflecting maintenance policy reforms. The\nresults showed that establishing a new base in Hami and expanding Xi'an base\ncould minimize the total annualized cost during the planning period, amounting\nto a total of 2,278.15 million RMB. This paper offers an optimization method\nfor selecting maintenance base locations that ensures reliability and\nefficiency in maintenance work as the number of trains increases in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u4e2d\u56fd\u897f\u5317\u9ad8\u901f\u94c1\u8def\u7f51\u7684\u7ef4\u62a4\u57fa\u5730\u9009\u5740\u548c\u4efb\u52a1\u5206\u914d\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u5e74\u5ea6\u603b\u6210\u672c\uff0c\u5e76\u8fdb\u884c\u4e86\u654f\u611f\u6027\u5206\u6790\u3002", "motivation": "\u9ad8\u901f\u5217\u8f66\u5b89\u5168\u7a33\u5b9a\u8fd0\u884c\u79bb\u4e0d\u5f00\u7ef4\u62a4\u57fa\u5730\uff0c\u4f46\u5176\u9009\u5740\u548c\u4efb\u52a1\u5206\u914d\u662f\u4e00\u4e2a\u590d\u6742\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u6295\u8d44\u548c\u8fd0\u8425\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53cc\u5c42\u89c4\u5212\u6a21\u578b\uff1a\u4e0a\u5c42\u6700\u5c0f\u5316\u5e74\u5316\u603b\u6210\u672c\uff08\u5305\u62ec\u57fa\u5730\u6295\u8d44\u548c\u603b\u7ef4\u62a4\u6210\u672c\uff09\uff0c\u4e0b\u5c42\u5728\u4e0d\u540c\u6295\u8d44\u60c5\u666f\u4e0b\u6700\u5c0f\u5316\u7ef4\u62a4\u8fd0\u884c\u8c03\u5ea6\u6210\u672c\u3002", "result": "\u5728\u4e2d\u56fd\u897f\u5317\u9ad8\u901f\u94c1\u8def\u7f51\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5728\u54c8\u5bc6\u5efa\u7acb\u65b0\u57fa\u5730\u5e76\u6269\u5c55\u897f\u5b89\u57fa\u5730\u53ef\u5c06\u89c4\u5212\u671f\u5185\u7684\u5e74\u5316\u603b\u6210\u672c\u964d\u81f3\u6700\u4f4e\uff0c\u4e3a22.7815\u4ebf\u5143\u4eba\u6c11\u5e01\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f18\u5316\u7ef4\u62a4\u57fa\u5730\u9009\u5740\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u65e5\u76ca\u589e\u957f\u7684\u5217\u8f66\u6570\u91cf\uff0c\u786e\u4fdd\u7ef4\u62a4\u5de5\u4f5c\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "AI\u5b89\u5168\u7814\u7a76\u5e94\u91c7\u53d6\u53cd\u8106\u5f31\u89c6\u89d2\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6765\u4e3a\u672a\u6765\u66f4\u4e0d\u53ef\u9884\u6d4b\u7684\u6311\u6218\u505a\u51c6\u5907\uff0c\u4ee5\u5e94\u5bf9\u73af\u5883\u6f14\u53d8\u548c\u6a21\u578b\u9002\u5e94\u4e0d\u826f\u3002", "motivation": "\u5f53\u524d\u7684AI\u5b89\u5168\u7814\u7a76\u65b9\u6cd5\uff08\u9759\u6001\u57fa\u51c6\u548c\u5355\u6b21\u9c81\u68d2\u6027\u6d4b\u8bd5\uff09\u65e0\u6cd5\u5e94\u5bf9\u73af\u5883\u6f14\u53d8\u548c\u6a21\u578b\u53ef\u80fd\u51fa\u73b0\u7684\u9002\u5e94\u4e0d\u826f\uff08\u5982\u5956\u52b1\u653b\u51fb\u3001\u8fc7\u5ea6\u4f18\u5316\u6216\u80fd\u529b\u9000\u5316\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u6269\u5c55\u5176\u957f\u671fAI\u5b89\u5168\u4fdd\u969c\u80fd\u529b\uff08\u5982\u5904\u7406\u7f55\u89c1\u6216\u5206\u5e03\u5916\u4e8b\u4ef6\uff09\u7684\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u5e76\u8bba\u8bc1\u4e86\u53cd\u8106\u5f31\uff08antifragile\uff09\u89c6\u89d2\u5728AI\u5b89\u5168\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u5176\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6765\u4e3a\u672a\u6765\u66f4\u5927\u3001\u66f4\u4e0d\u53ef\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u505a\u51c6\u5907\u7684\u4f18\u52bf\u3002\u8bc6\u522b\u4e86\u9759\u6001\u6d4b\u8bd5\uff08\u573a\u666f\u591a\u6837\u6027\u3001\u5956\u52b1\u653b\u51fb\u3001\u8fc7\u5ea6\u5bf9\u9f50\uff09\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u53cd\u8106\u5f31\u89e3\u51b3\u65b9\u6848\u5728\u7ba1\u7406\u7f55\u89c1\u4e8b\u4ef6\u65b9\u9762\u7684\u6f5c\u529b\u3002", "result": "\u8be5\u8bba\u6587\u8bc6\u522b\u4e86\u73b0\u6709AI\u5b89\u5168\u6d4b\u8bd5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u91c7\u7528\u53cd\u8106\u5f31\u89c6\u89d2\u4f5c\u4e3a\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u957f\u671f\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7f55\u89c1\u6216\u5206\u5e03\u5916\u4e8b\u4ef6\u65f6\u3002", "conclusion": "AI\u5b89\u5168\u7814\u7a76\u9700\u8981\u6839\u672c\u6027\u5730\u8c03\u6574\u8861\u91cf\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6301\u7eed\u6539\u8fdbAI\u5b89\u5168\u7684\u65b9\u6cd5\uff0c\u5c06\u53cd\u8106\u5f31\u6027\u878d\u5165\u5176\u4e2d\uff0c\u4ee5\u5e94\u5bf9\u5f00\u653e\u5f0f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u957f\u671f\u53ef\u9760\u6027\u6311\u6218\uff0c\u5e76\u4e3a\u6784\u5efa\u53cd\u8106\u5f31AI\u5b89\u5168\u793e\u533a\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "\u672c\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u6d41\u7a0b\u6765\u4f18\u5316\u4e00\u4e2a\u57fa\u4e8e\u4f17\u5305\u6570\u636e\u7684\u5b9e\u65f6\u505c\u8f66\u670d\u52a1\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u505c\u8f66\u4f4d\uff0c\u5c06\u4eba\u529b\u8d44\u6e90\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe99.58%\u3002", "motivation": "\u4f18\u5316\u73b0\u6709\u7684\u57fa\u4e8e\u4f17\u5305\u6570\u636e\u7684\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u7684\u8d28\u91cf\uff0c\u5177\u4f53\u662f\u901a\u8fc7\u81ea\u52a8\u5316\u5730\u9762\u771f\u5b9e\u6027\u6d4b\u8bd5\u7684\u73b0\u6709\u6d4b\u8bd5\u6d41\u7a0b\u3002", "method": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662f\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6765\u4e30\u5bcc\u6570\u636e\u5e93\u5e76\u66ff\u4ee3\u5206\u6790\u8fc7\u7a0b\u4e2d\u7684\u5927\u91cf\u4eba\u5de5\u5de5\u7a0b\u5de5\u4f5c\u3002", "result": "\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u6307\u6807\u5c55\u793a\u4e86\u6240\u8fbe\u5230\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u663e\u793a\u4eba\u529b\u8d44\u6e90\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe99.58%\u3002", "conclusion": "\u81ea\u52a8\u5316\u7684\u5206\u6790\u5de5\u5177\u5728\u63d0\u9ad8\u505c\u8f66\u670d\u52a1\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u548c\u6f5c\u5728\u5e94\u7528\u63d0\u4f9b\u4e86\u5c55\u671b\u3002"}}
{"id": "2509.13554", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.13554", "abs": "https://arxiv.org/abs/2509.13554", "authors": ["Xilong Xu", "Haonan Wang", "Li Yang"], "title": "Axial Hall Effect in Altermagnetic Lieb Lattices", "comment": "19 pages with 4 figures and 2 tables", "summary": "We predict a so-called axial Hall effect, a Berry-curvature-driven anomalous\nHall response, in Lieb-lattice altermagnets. By constructing a tight-binding\nmodel, we identify the axial direction as a hidden topological degree of\nfreedom. Breaking the double degeneracy of axial symmetry generates substantial\nBerry curvature and induces a pronounced anomalous Hall conductivity.\nFirst-principles calculations further confirm the emergence of this effect in\nstrained altermagnets, particularly in ternary transition-metal\ndichalcogenides. We take Mn2WS4 as an example to reveal that the axial Hall\neffect originates from the interplay between Dresselhaus spin-orbit coupling\nand the intrinsic piezomagnetic response of Lieb-lattice altermagnets, leading\nto highly localized and enhanced Berry curvature. Remarkably, the magnitude of\nthe axial Hall effect is significant and remains unchanged when varying the\nstrain, highlighting the topological nature of the axial degree of freedom.\nFinally, in multilayer systems, the effect manifests as a distinctive\nthickness-dependent modulation of both anomalous and spin Hall responses. These\nfindings emphasize the critical role of spin-orbit coupling and noncollinear\nspin textures in altermagnets, an area that has received limited attention, and\nopen new pathways for exploring intrinsic Hall phenomena in topological\nmagnetic systems.", "AI": {"tldr": "Lieb\u6676\u683c\u7684altermagnets\u4e2d\u5b58\u5728\u4e00\u4e2a\u7531Berry\u66f2\u7387\u9a71\u52a8\u7684\u8f74\u5411\u970d\u5c14\u6548\u5e94\uff0c\u8be5\u6548\u5e94\u7531Dresselhaus\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u538b\u78c1\u54cd\u5e94\u5f15\u8d77\uff0c\u5e76\u5177\u6709\u62d3\u6251\u6027\u8d28\u3002", "motivation": "\u63a2\u7d22altermagnets\u4e2d\u65b0\u7684\u5185\u7980\u970d\u5c14\u73b0\u8c61\uff0c\u7279\u522b\u662f\u4e0e\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u975e\u5171\u7ebf\u81ea\u65cb\u7ed3\u6784\u76f8\u5173\u7684\u73b0\u8c61\u3002", "method": "\u6784\u5efa\u7d27\u675f\u7f1a\u6a21\u578b\u548c\u8fdb\u884c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7814\u7a76\u5e94\u53d8\u5bf9altermagnets\u4e2d\u8f74\u5411\u970d\u5c14\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u5176\u5728\u591a\u5c42\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728Lieb\u6676\u683c\u7684altermagnets\u4e2d\u53d1\u73b0\u4e86\u8f74\u5411\u970d\u5c14\u6548\u5e94\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5176\u7531Berry\u66f2\u7387\u9a71\u52a8\u3002\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6548\u5e94\u5728\u5e94\u53d8altermagnets\uff08\u7279\u522b\u662f\u4e09\u5143\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u4ee3\u7269\uff09\u4e2d\u4f1a\u51fa\u73b0\u3002\u4ee5Mn2WS4\u4e3a\u4f8b\uff0c\u53d1\u73b0\u8be5\u6548\u5e94\u6e90\u4e8eDresselhaus\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u4e0e\u538b\u78c1\u54cd\u5e94\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4Berry\u66f2\u7387\u7684\u9ad8\u5ea6\u5c40\u90e8\u5316\u548c\u589e\u5f3a\u3002\u8f74\u5411\u970d\u5c14\u6548\u5e94\u7684\u5e45\u5ea6\u5f88\u5927\uff0c\u5e76\u4e14\u4e0d\u968f\u5e94\u53d8\u53d8\u5316\u3002\u5728\u591a\u5c42\u7cfb\u7edf\u4e2d\uff0c\u8be5\u6548\u5e94\u8868\u73b0\u4e3a\u5f02\u5e38\u970d\u5c14\u548c\u81ea\u65cb\u970d\u5c14\u54cd\u5e94\u968f\u539a\u5ea6\u7684\u4f9d\u8d56\u6027\u8c03\u5236\u3002", "conclusion": "\u8f74\u5411\u970d\u5c14\u6548\u5e94\u662f\u4e00\u4e2a\u7531Berry\u66f2\u7387\u9a71\u52a8\u7684\u3001\u5177\u6709\u62d3\u6251\u6027\u8d28\u7684\u970d\u5c14\u54cd\u5e94\uff0c\u5728Lieb\u6676\u683c\u7684altermagnets\u4e2d\u5b58\u5728\u3002\u8be5\u6548\u5e94\u7684\u53d1\u73b0\u4e3a\u5728\u62d3\u6251\u78c1\u6027\u7cfb\u7edf\u4e2d\u63a2\u7d22\u5185\u7980\u970d\u5c14\u73b0\u8c61\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "LLM \u9002\u5e94\u6027\u662f\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u672a\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u9047\u5230\u7684\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u6765\u7814\u7a76\u8de8\u4efb\u52a1\u4ea4\u4e92\u3002\u901a\u8fc7\u8bad\u7ec3\u548c\u5206\u6790 10 \u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u6f5c\u5728\u80fd\u529b\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u526f\u4f5c\u7528\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\u5e76\u975e\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u800c\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u5982\u7c7b\u522b\u5206\u5e03\u548c\u751f\u6210\u957f\u5ea6\u503e\u5411\uff09\u4ee5\u53ca\u7279\u5b9a\u7684\u8bed\u8a00\u7279\u5f81\u66f4\u4e3a\u91cd\u8981\u3002\u8be5\u7814\u7a76\u4e3a\u66f4\u53ef\u9884\u6d4b\u3001\u66f4\u6709\u6548\u7684 LLM \u9002\u5e94\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u7531\u4e8e LLM \u7ecf\u5e38\u9700\u8981\u5904\u7406\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u9047\u5230\u7684\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5206\u5e03\u5916\u67e5\u8be2\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u6280\u672f\uff0c\u4ee5\u7814\u7a76\u8de8\u4efb\u52a1\u4ea4\u4e92\u3002\u8bad\u7ec3\u4e86 10 \u4e2a\u6a21\u578b\uff0c\u4ee5\u8bc6\u522b\u6f5c\u5728\u80fd\u529b\uff08\u5982\u63a8\u7406\u3001\u60c5\u611f\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7b97\u672f\uff09\u5e76\u5206\u6790\u8fc1\u79fb\u5b66\u4e60\u7684\u526f\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6027\u80fd\u63d0\u5347\u901a\u5e38\u65e0\u6cd5\u901a\u8fc7\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\u6765\u89e3\u91ca\u3002\u76f8\u53cd\uff0c\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u4f8b\u5982\u7c7b\u522b\u5206\u5e03\u548c\u751f\u6210\u957f\u5ea6\u503e\u5411\uff09\u548c\u7279\u5b9a\u7684\u8bed\u8a00\u7279\u5f81\u5bf9 LLM \u7684\u9002\u5e94\u6027\u5177\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9884\u6d4b\u548c\u66f4\u6709\u6548\u7684 LLM \u9002\u5e94\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13405", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13405", "abs": "https://arxiv.org/abs/2509.13405", "authors": ["Carla Ferradini", "Martin Sandfuchs", "Ramona Wolf", "Renato Renner"], "title": "Defining Security in Quantum Key Distribution", "comment": "21+3 pages. Comments are welcome", "summary": "The security of quantum key distribution (QKD) is quantified by a parameter\n$\\varepsilon>0$, which -- under well-defined physical assumptions -- can be\nbounded explicitly. This contrasts with computationally secure schemes, where\nsecurity claims are only asymptotic (i.e., under standard complexity\nassumptions, one only knows that $\\varepsilon \\to 0$ as the key size grows, but\nhas no explicit bound). Here we explain the definition and interpretation of\n$\\varepsilon$-security. Adopting an axiomatic approach, we show that\n$\\varepsilon$ can be understood as the maximum probability of a security\nfailure. Finally, we review and address several criticisms of this definition\nthat have appeared in the literature.", "AI": {"tldr": "\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1(QKD)\u7684\u5b89\u5168\u6027\u7531\u53c2\u6570\u03b5\u91cf\u5316\uff0c\u8be5\u53c2\u6570\u53ef\u4ee5\u660e\u786e\u9650\u5b9a\uff0c\u8fd9\u4e0e\u8ba1\u7b97\u5b89\u5168\u65b9\u6848\u4e0d\u540c\uff0c\u8ba1\u7b97\u5b89\u5168\u65b9\u6848\u7684\u5b89\u5168\u6027\u58f0\u660e\u4ec5\u662f\u6e10\u8fd1\u7684\u3002", "motivation": "\u89e3\u91ca\u03b5-\u5b89\u5168\u6027\u7684\u5b9a\u4e49\u548c\u89e3\u91ca\u3002", "method": "\u91c7\u7528\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u5c06\u03b5\u7406\u89e3\u4e3a\u5b89\u5168\u5931\u8d25\u7684\u6700\u5927\u6982\u7387\u3002", "result": "\u5bf9\u6587\u732e\u4e2d\u51fa\u73b0\u7684\u5bf9\u03b5-\u5b89\u5168\u6027\u5b9a\u4e49\u7684\u51e0\u9879\u6279\u8bc4\u8fdb\u884c\u4e86\u56de\u987e\u548c\u8ba8\u8bba\u3002", "conclusion": "\u03b5-\u5b89\u5168\u6027\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u5b89\u5168\u5931\u8d25\u7684\u6700\u5927\u6982\u7387\u3002"}}
{"id": "2509.14091", "categories": ["cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.14091", "abs": "https://arxiv.org/abs/2509.14091", "authors": ["Sougata Bose", "Daniel Hausmann", "Soumyajit Paul", "Sven Schewe", "Tansholpan Zhanabekova"], "title": "Generalised Reachability Games Revisited", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "Classic reachability games on graphs are zero-sum games, where the goal of\none player, Eve, is to visit a vertex from a given target set, and that of\nother player, Adam, is to prevent this. Generalised reachability games, studied\nby Fijalkow and Horn, are a generalisation of reachability objectives, where\ninstead of a single target set, there is a family of target sets and Eve must\nvisit all of them in any order. In this work, we further study the complexity\nof solving two-player games on graphs with generalised reachability objectives.\nOur results are twofold: first, we provide an improved complexity picture for\ngeneralised reachability games, expanding the known tractable class from games\nin which all target sets are singleton to additionally allowing a logarithmic\nnumber of target sets of arbitrary size. Second, we study optimisation variants\nof generalised reachability with a focus on the size of the target sets. For\nthese problems, we show intractability for most interesting cases.\nParticularly, in contrast to the tractability in the classic variant for\nsingleton target sets, the optimisation problem is NP-hard when Eve tries to\nmaximise the number of singleton target sets that are visited. Tractability can\nbe recovered in the optimisation setting when all target sets are singleton by\nrequiring that Eve pledges a maximum sized subset of target sets that she can\nguarantee to visit.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u63a8\u5e7f\u53ef\u8fbe\u6027\u76ee\u6807\u7684\u4e24\u73a9\u5bb6\u56fe\u535a\u5f08\u7684\u590d\u6742\u6027\u3002", "motivation": "\u63a8\u5e7f\u53ef\u8fbe\u6027\u535a\u5f08\u662f\u5bf9\u7ecf\u5178\u53ef\u8fbe\u6027\u535a\u5f08\u7684\u6982\u62ec\uff0c\u5176\u4e2d\u4e00\u4e2a\u73a9\u5bb6\uff08Eve\uff09\u7684\u76ee\u6807\u662f\u6309\u4efb\u610f\u987a\u5e8f\u8bbf\u95ee\u4e00\u4e2a\u76ee\u6807\u96c6\u65cf\u4e2d\u7684\u6240\u6709\u76ee\u6807\u96c6\uff0c\u800c\u53e6\u4e00\u4e2a\u73a9\u5bb6\uff08Adam\uff09\u5219\u8bd5\u56fe\u963b\u6b62\u5979\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86\u5177\u6709\u63a8\u5e7f\u53ef\u8fbe\u6027\u76ee\u6807\u7684\u4e24\u73a9\u5bb6\u56fe\u535a\u5f08\u7684\u590d\u6742\u6027\u3002\u7b2c\u4e00\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u63a8\u5e7f\u53ef\u8fbe\u6027\u535a\u5f08\u7684\u590d\u6742\u6027\u56fe\u666f\uff0c\u5c06\u5df2\u77e5\u7684\u6613\u5904\u7406\u7c7b\u4ece\u6240\u6709\u76ee\u6807\u96c6\u5747\u4e3a\u5355\u4f8b\u7684\u6e38\u620f\u6269\u5c55\u5230\u6b64\u5916\u8fd8\u5141\u8bb8\u6709\u5bf9\u6570\u6570\u91cf\u7684\u4efb\u610f\u5927\u5c0f\u7684\u76ee\u6807\u96c6\u3002\u7b2c\u4e8c\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u63a8\u5e7f\u53ef\u8fbe\u6027\u535a\u5f08\u7684\u4f18\u5316\u53d8\u4f53\uff0c\u91cd\u70b9\u662f\u76ee\u6807\u96c6\u7684\u5927\u5c0f\u3002", "result": "\u5bf9\u4e8e\u4f18\u5316\u95ee\u9898\uff0c\u5f53 Eve \u8bd5\u56fe\u6700\u5927\u5316\u8bbf\u95ee\u7684\u5355\u4f8b\u76ee\u6807\u96c6\u7684\u6570\u91cf\u65f6\uff0c\u8be5\u95ee\u9898\u662f NP-hard \u7684\uff0c\u8fd9\u4e0e\u7ecf\u5178\u53d8\u4f53\u4e2d\u5355\u4f8b\u76ee\u6807\u96c6\u7684\u6613\u5904\u7406\u6027\u5f62\u6210\u5bf9\u6bd4\u3002\u5f53\u6240\u6709\u76ee\u6807\u96c6\u5747\u4e3a\u5355\u4f8b\u65f6\uff0c\u901a\u8fc7\u8981\u6c42 Eve \u627f\u8bfa\u5979\u80fd\u4fdd\u8bc1\u8bbf\u95ee\u7684\u6700\u5927\u76ee\u6807\u96c6\u5b50\u96c6\u7684\u5927\u5c0f\uff0c\u53ef\u4ee5\u5728\u4f18\u5316\u8bbe\u7f6e\u4e2d\u6062\u590d\u6613\u5904\u7406\u6027\u3002", "conclusion": "\u672c\u6587\u5bf9\u5177\u6709\u63a8\u5e7f\u53ef\u8fbe\u6027\u76ee\u6807\u7684\u4e24\u73a9\u5bb6\u56fe\u535a\u5f08\u7684\u590d\u6742\u6027\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u590d\u6742\u6027\u56fe\u666f\u548c\u5bf9\u4f18\u5316\u53d8\u4f53\u7684\u5206\u6790\u3002"}}
{"id": "2509.13477", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13477", "abs": "https://arxiv.org/abs/2509.13477", "authors": ["Zhuohan Li", "Benjamin X. Lam", "Gerbrand Ceder"], "title": "Li+/H+ exchange in solid-state oxide Li-ion conductors", "comment": null, "summary": "Understanding the moisture stability of oxide Li-ion conductors is important\nfor their practical applications in solid-state batteries. Unlike sulfide or\nhalide conductors, oxide conductors generally better resist degradation when in\ncontact with water, but can still undergo topotactic ion exchange between Li\nions within the structure and protons in the environment. In this work, we\ncombine density functional theory (DFT) calculations with a machine-learning\ninteratomic potential model to investigate the thermodynamic driving force of\nthe Li+/H+ exchange reaction for two representative oxide Li-ion conductor\nfamilies: garnets and NASICONs. Our results indicate that the high Li chemical\npotential in Li-stuffed garnets is responsible for the stronger driving force\nfor exchanging Li with protons as compared to the Li-unstuffed structures. In\ncontrast, NASICONs demonstrate a higher resistance against proton exchange,\nwhich is attributed to the lower Li chemical potential and the lower O-H bond\ncovalency for polyanion-bonded oxygens. Our findings highlight the trade-off\nwhen using Li stuffing as a mechanism to enhance Li-ion conductivity, as it\nalso promotes degradation by moisture. This study underscores the importance of\ndesigning Li-ion conductors that not only possess high conductivity, but also\nexhibit high stability in practical environments.", "AI": {"tldr": "\u6c27\u5316\u7269\u79bb\u5b50\u5bfc\u4f53\u5728\u6c34\u5206\u5b58\u5728\u4e0b\u4f1a\u53d1\u751f\u79bb\u5b50\u4ea4\u6362\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5176\u5728\u56fa\u6001\u7535\u6c60\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e86\u89e3\u6c27\u5316\u7269\u79bb\u5b50\u5bfc\u4f53\u5728\u6c34\u5206\u4e0b\u7684\u7a33\u5b9a\u6027\u5bf9\u4e8e\u56fa\u6001\u7535\u6c60\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u77f3\u69b4\u77f3\u548cNASICON\u4e24\u5927\u7c7b\u6c27\u5316\u7269\u79bb\u5b50\u5bfc\u4f53\u4e2dLi+/H+\u4ea4\u6362\u7684\u9a71\u52a8\u529b\u3002", "result": "\u9502\u542b\u91cf\u9ad8\u7684\u77f3\u69b4\u77f3\u7ed3\u6784\u6bd4\u9502\u542b\u91cf\u4f4e\u7684\u7ed3\u6784\u66f4\u6613\u53d1\u751fLi+/H+\u4ea4\u6362\u3002NASICON\u7ed3\u6784\u5bf9\u8d28\u5b50\u4ea4\u6362\u7684\u62b5\u6297\u80fd\u529b\u66f4\u5f3a\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u8f83\u4f4e\u7684\u9502\u5316\u5b66\u52bf\u548c\u8f83\u4f4e\u7684O-H\u952e\u5171\u4ef7\u6027\u3002", "conclusion": "\u63d0\u9ad8\u9502\u542b\u91cf\u53ef\u4ee5\u589e\u5f3a\u79bb\u5b50\u5bfc\u7535\u6027\uff0c\u4f46\u4e5f\u4f1a\u589e\u52a0\u6c34\u5206\u964d\u89e3\u7684\u98ce\u9669\u3002\u8bbe\u8ba1\u79bb\u5b50\u5bfc\u4f53\u65f6\uff0c\u9700\u8981\u5728\u9ad8\u7535\u5bfc\u7387\u548c\u9ad8\u7a33\u5b9a\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2509.13765", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13765", "abs": "https://arxiv.org/abs/2509.13765", "authors": ["Zhirui Huang", "Rui Ma", "Shijie Cao", "Ran Shu", "Ian Wang", "Ting Cao", "Chixiao Chen", "Yongqiang Xiong"], "title": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge", "comment": null, "summary": "Ternary quantization has emerged as a powerful technique for reducing both\ncomputational and memory footprint of large language models (LLM), enabling\nefficient real-time inference deployment without significantly compromising\nmodel accuracy. Conventional LLM inference platforms (e.g GPUs) cannot\ncapitalize on its benefits, as they (i) lack native support for ternary\narithmetic and memory specialization and (ii) remain severely under-utilized in\nlow-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware\nLUT-centric architecture that co-optimizes algorithm, compute, and memory for\nternary LLM inference. To maximize the efficiency of Ternary Linear layer,\nTENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary\nmixed-precision GEMM using a symmetric precompute lookup table. It also\nfeatures Dynamic Activation N:M Sparsity to exploit the sparsity within the\nactivation of each token. Additionally, we propose a LUT-based 64B:80B ternary\nweight decompression module to fully exploit the memory efficiency of ternary\nvalues. At the system level, we design a heterogeneous TENET accelerator with\nfull programmability that integrates STL cores with high-precision cores. An\nassociated Linear-Projection-aware Sparse Attention dataflow is introduced to\noptimize memory access and hardware utilization. We implement TENET accelerator\nprototype on both FPGA and ASIC platforms. Experiments across various model\nsizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy\nefficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100\nGPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to\nthe A100 GPU in end-to-end inference latency.", "AI": {"tldr": "Ternary quantization is an effective method for LLM compression, but current platforms like GPUs are not optimized for it. TENET is a new architecture that co-optimizes algorithm, compute, and memory for ternary LLM inference, using a Sparse Ternary LUT (STL) core, Dynamic Activation N:M Sparsity, and a LUT-based weight decompression module. Its heterogeneous accelerator design and dataflow achieve significant improvements in energy efficiency and inference latency compared to A100 GPUs.", "motivation": "Conventional LLM inference platforms (e.g. GPUs) lack native support for ternary arithmetic and memory specialization, and are under-utilized in low-batch, real-time scenarios, hindering the benefits of ternary quantization.", "method": "TENET proposes a sparse-aware LUT-centric architecture featuring a Sparse Ternary LUT (STL) core for optimized ternary mixed-precision GEMM, Dynamic Activation N:M Sparsity for activation sparsity, and a LUT-based 64B:80B ternary weight decompression module. It also introduces a heterogeneous TENET accelerator integrating STL cores with high-precision cores and a Linear-Projection-aware Sparse Attention dataflow.", "result": "TENET-FPGA and TENET-ASIC demonstrate significant improvements in energy efficiency, with 4.3x and 21.1x gains respectively over A100 GPU. TENET-ASIC also achieves a 2.7x average speedup in end-to-end inference latency compared to A100 GPU.", "conclusion": "TENET is a novel architecture that effectively addresses the limitations of current platforms for ternary LLM inference, delivering substantial improvements in energy efficiency and inference speed through its specialized hardware and dataflow optimizations."}}
{"id": "2509.14090", "categories": ["cs.LO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.14090", "abs": "https://arxiv.org/abs/2509.14090", "authors": ["Massimo Benerecetti", "Dario Della Monica", "Angelo Matteo", "Fabio Mogavero", "Gabriele Puppis"], "title": "An Automaton-based Characterisation of First-Order Logic over Infinite Trees", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "In this paper, we study First Order Logic (FO) over (unordered) infinite\ntrees and its connection with branching-time temporal logics. More\nspecifically, we provide an automata-theoretic characterisation of FO\ninterpreted over infinite trees. To this end, two different classes of hesitant\ntree automata are introduced and proved to capture precisely the expressive\npower of two branching time temporal logics, denoted polcCTLp and cCTL*[f],\nwhich are, respectively, a restricted version of counting CTL with past and\ncounting CTL* over finite paths, both of which have been previously shown\nequivalent to FO over infinite trees. The two automata characterisations\nnaturally lead to normal forms for the two temporal logics, and highlight the\nfact that FO can only express properties of the tree branches which are either\nsafety or co-safety in nature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u53ef\u6570\u65e0\u9650\u6811\u4e0a\u7684\u7b2c\u4e00\u9636\u903b\u8f91 (FO) \u53ca\u5176\u4e0e\u5206\u652f\u65f6\u95f4\u65f6\u5e8f\u903b\u8f91\u7684\u5173\u7cfb\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4e3a\u4e0d\u53ef\u6570\u65e0\u9650\u6811\u4e0a\u7684\u7b2c\u4e00\u9636\u903b\u8f91 (FO) \u63d0\u4f9b\u81ea\u52a8\u673a\u7406\u8bba\u7684\u523b\u753b\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u4e24\u79cd\u5206\u652f\u65f6\u95f4\u65f6\u5e8f\u903b\u8f91\u7684\u5173\u7cfb\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u7c7b\u72b9\u8c6b\u6811\u81ea\u52a8\u673a\uff0c\u5e76\u8bc1\u660e\u5b83\u4eec\u5206\u522b\u7cbe\u786e\u5730\u6355\u6349\u4e86\u4e24\u79cd\u5206\u652f\u65f6\u95f4\u65f6\u5e8f\u903b\u8f91\uff08polcCTLp \u548c cCTL*[f]\uff09\u7684\u8868\u8fbe\u80fd\u529b\u3002\u8fd9\u4e24\u79cd\u903b\u8f91\u5206\u522b\u662f\u5e26\u5386\u53f2\u7684\u8ba1\u6570 CTL \u7684\u53d7\u9650\u7248\u672c\u548c\u6709\u9650\u8def\u5f84\u4e0a\u7684\u8ba1\u6570 CTL*\uff0c\u5b83\u4eec\u90fd\u5df2\u88ab\u8bc1\u660e\u7b49\u4ef7\u4e8e\u4e0d\u53ef\u6570\u65e0\u9650\u6811\u4e0a\u7684 FO\u3002", "result": "\u4e24\u79cd\u81ea\u52a8\u673a\u523b\u753b\u81ea\u7136\u5730\u5bfc\u51fa\u4e86\u4e24\u79cd\u65f6\u5e8f\u903b\u8f91\u7684\u8303\u5f0f\uff0c\u5e76\u5f3a\u8c03\u4e86 FO \u53ea\u80fd\u8868\u8fbe\u6811\u5206\u652f\u4e2d\u672c\u8d28\u4e0a\u662f\u5b89\u5168\uff08safety\uff09\u6216\u5171\u5b89\u5168\uff08co-safety\uff09\u7684\u6027\u8d28\u8fd9\u4e00\u4e8b\u5b9e\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u673a\u7406\u8bba\u523b\u753b\u4e86\u4e0d\u53ef\u6570\u65e0\u9650\u6811\u4e0a\u7684\u7b2c\u4e00\u9636\u903b\u8f91 (FO)\uff0c\u5e76\u5c06\u5176\u4e0e\u5206\u652f\u65f6\u95f4\u65f6\u5e8f\u903b\u8f91 polcCTLp \u548c cCTL*[f] \u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86 FO \u53ea\u80fd\u8868\u8fbe\u6811\u5206\u652f\u4e2d\u5b89\u5168\u6216\u5171\u5b89\u5168\u6027\u8d28\u3002"}}
{"id": "2509.13378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13378", "abs": "https://arxiv.org/abs/2509.13378", "authors": ["Mattias Wingren", "S\u00f6ren Andersson", "Sara Rosenberg", "Malin Andtfolk", "Susanne H\u00e4gglund", "Prashani Jayasingha Arachchige", "Linda Nyholm"], "title": "Using role-play and Hierarchical Task Analysis for designing human-robot interaction", "comment": "11 pages. This is a preprint version of the published paper in the\n  International Conference on Social Robotics:\n  https://link.springer.com/chapter/10.1007/978-981-96-3522-1_28", "summary": "We present the use of two methods we believe warrant more use than they\ncurrently have in the field of human-robot interaction: role-play and\nHierarchical Task Analysis. Some of its potential is showcased through our use\nof them in an ongoing research project which entails developing a robot\napplication meant to assist at a community pharmacy. The two methods have\nprovided us with several advantages. The role-playing provided a controlled and\nadjustable environment for understanding the customers' needs where pharmacists\ncould act as models for the robot's behavior; and the Hierarchical Task\nAnalysis ensured the behavior displayed was modelled correctly and aided\ndevelopment through facilitating co-design. Future research could focus on\ndeveloping task analysis methods especially suited for social robot\ninteraction.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5e94\u7528\u89d2\u8272\u626e\u6f14\u548c\u5206\u5c42\u4efb\u52a1\u5206\u6790\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5728\u793e\u533a\u836f\u623f\u673a\u5668\u4eba\u5e94\u7528\u7684\u7814\u7a76\u9879\u76ee\u4e2d\u7684\u5e94\u7528\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u4ecb\u7ecd\u89d2\u8272\u626e\u6f14\u548c\u5206\u5c42\u4efb\u52a1\u5206\u6790\u8fd9\u4e24\u79cd\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5e94\u7528\u8f83\u5c11\u4f46\u5177\u6709\u6f5c\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u8fd0\u7528\u89d2\u8272\u626e\u6f14\u548c\u5206\u5c42\u4efb\u52a1\u5206\u6790\u6765\u5f00\u53d1\u4e00\u4e2a\u8f85\u52a9\u793e\u533a\u836f\u623f\u7684\u673a\u5668\u4eba\u5e94\u7528\u3002", "result": "\u89d2\u8272\u626e\u6f14\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u7684\u73af\u5883\u6765\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u6a21\u578b\uff1b\u5206\u5c42\u4efb\u52a1\u5206\u6790\u786e\u4fdd\u4e86\u884c\u4e3a\u7684\u6b63\u786e\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u4fc3\u8fdb\u5171\u540c\u8bbe\u8ba1\u6765\u8f85\u52a9\u5f00\u53d1\u3002", "conclusion": "\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u96c6\u4e2d\u5728\u5f00\u53d1\u66f4\u9002\u5408\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u4efb\u52a1\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2509.13523", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13523", "abs": "https://arxiv.org/abs/2509.13523", "authors": ["V\u00e4in\u00f6 Hatanp\u00e4\u00e4", "Eugene Ku", "Jason Stock", "Murali Emani", "Sam Foreman", "Chunyong Jung", "Sandeep Madireddy", "Tung Nguyen", "Varuni Sastry", "Ray A. O. Sinurat", "Sam Wheeler", "Huihuo Zheng", "Troy Arcomano", "Venkatram Vishwanath", "Rao Kotamarthi"], "title": "AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions", "comment": "14 pages, 7 figures", "summary": "Generative machine learning offers new opportunities to better understand\ncomplex Earth system dynamics. Recent diffusion-based methods address spectral\nbiases and improve ensemble calibration in weather forecasting compared to\ndeterministic methods, yet have so far proven difficult to scale stably at high\nresolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin\ndiffusion transformer to address this gap, and SWiPe, a generalizable technique\nthat composes window parallelism with sequence and pipeline parallelism to\nshard window-based transformers without added communication cost or increased\nglobal batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS\n(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \\times 1$\npatch size on the 0.25{\\deg} ERA5 dataset, achieving 95.5% weak scaling\nefficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS\nand remains stable on seasonal scales to 90 days, highlighting the potential of\nbillion-parameter diffusion models for weather and climate prediction.", "AI": {"tldr": "AERIS\u662f\u4e00\u4e2a1.3\u523080B\u53c2\u6570\u7684\u50cf\u7d20\u7ea7Swin\u6269\u6563Transformer\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u5929\u6c14\u9884\u62a5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002SWiPe\u662f\u4e00\u79cd\u53ef\u7ec4\u5408\u7684\u6280\u672f\uff0c\u901a\u8fc7\u7a97\u53e3\u5e76\u884c\u3001\u5e8f\u5217\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u6765\u5206\u7247\u7a97\u53e3Transformer\uff0c\u800c\u65e0\u9700\u589e\u52a0\u901a\u4fe1\u6210\u672c\u6216\u5168\u5c40\u6279\u6b21\u5927\u5c0f\u3002AERIS\u5728Aurora\u4e0a\u5b9e\u73b0\u4e8610.21 ExaFLOPS\u7684\u6df7\u5408\u7cbe\u5ea6\u6027\u80fd\uff0c\u5e76\u57280.25\u5ea6ERA5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8695.5%\u7684\u5f31\u6269\u5c55\u6548\u7387\u548c81.6%\u7684\u5f3a\u6269\u5c55\u6548\u7387\u3002AERIS\u7684\u6027\u80fd\u4f18\u4e8eIFS ENS\uff0c\u5e76\u5728\u957f\u8fbe90\u5929\u7684\u5b63\u8282\u6027\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u7a33\u5b9a\uff0c\u5c55\u793a\u4e86\u767e\u4ebf\u53c2\u6570\u6269\u6563\u6a21\u578b\u5728\u5929\u6c14\u548c\u6c14\u5019\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u751f\u6210\u673a\u5668\u5b66\u4e60\u4e3a\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u5730\u7403\u7cfb\u7edf\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u65f6\u5b58\u5728\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86AERIS\uff0c\u4e00\u4e2a1.3\u523080B\u53c2\u6570\u7684\u50cf\u7d20\u7ea7Swin\u6269\u6563Transformer\uff0c\u4ee5\u53caSWiPe\uff0c\u4e00\u79cd\u53ef\u7ec4\u5408\u7a97\u53e3\u5e76\u884c\u3001\u5e8f\u5217\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u4ee5\u5206\u7247\u7a97\u53e3Transformer\u7684\u6280\u672f\uff0c\u800c\u65e0\u9700\u589e\u52a0\u901a\u4fe1\u6210\u672c\u6216\u5168\u5c40\u6279\u6b21\u5927\u5c0f\u3002", "result": "AERIS\u5728Aurora\u4e0a\u5b9e\u73b0\u4e8610.21 ExaFLOPS\u7684\u6df7\u5408\u7cbe\u5ea6\u6027\u80fd\uff0c\u5e76\u57280.25\u5ea6ERA5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8695.5%\u7684\u5f31\u6269\u5c55\u6548\u7387\u548c81.6%\u7684\u5f3a\u6269\u5c55\u6548\u7387\u3002AERIS\u7684\u6027\u80fd\u4f18\u4e8eIFS ENS\uff0c\u5e76\u5728\u957f\u8fbe90\u5929\u7684\u5b63\u8282\u6027\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u767e\u4ebf\u53c2\u6570\u6269\u6563\u6a21\u578b\u5728\u5929\u6c14\u548c\u6c14\u5019\u9884\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.13793", "categories": ["eess.SY", "cs.LG", "cs.NE", "cs.SY", "math.OC", "65K10, 68T05, 93B30, 93D99"], "pdf": "https://arxiv.org/pdf/2509.13793", "abs": "https://arxiv.org/abs/2509.13793", "authors": ["Thomas Chaffey"], "title": "Circuit realization and hardware linearization of monotone operator equilibrium networks", "comment": null, "summary": "It is shown that the port behavior of a resistor-diode network corresponds to\nthe solution of a ReLU monotone operator equilibrium network (a neural network\nin the limit of infinite depth), giving a parsimonious construction of a neural\nnetwork in analog hardware. We furthermore show that the gradient of such a\ncircuit can be computed directly in hardware, using a procedure we call\nhardware linearization. This allows the network to be trained in hardware,\nwhich we demonstrate with a device-level circuit simulation. We extend the\nresults to cascades of resistor-diode networks, which can be used to implement\nfeedforward and other asymmetric networks. We finally show that different\nnonlinear elements give rise to different activation functions, and introduce\nthe novel diode ReLU which is induced by a non-ideal diode model.", "AI": {"tldr": "\u7535\u963b-\u4e8c\u6781\u7ba1\u7f51\u7edc\u5728\u6a21\u62df\u786c\u4ef6\u4e2d\u53ef\u4ee5\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u786c\u4ef6\u7ebf\u6027\u5316\u5728\u786c\u4ef6\u4e2d\u76f4\u63a5\u8ba1\u7b97\u68af\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u8bad\u7ec3\u3002", "motivation": "\u65e8\u5728\u4e8e\u63a2\u7d22\u5728\u6a21\u62df\u786c\u4ef6\u4e2d\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5b9e\u73b0\u5176\u5728\u786c\u4ef6\u4e2d\u7684\u8bad\u7ec3\u3002", "method": "\u5c06\u7535\u963b-\u4e8c\u6781\u7ba1\u7f51\u7edc\u7684\u7aef\u53e3\u884c\u4e3a\u4e0eReLU\u5355\u8c03\u7b97\u5b50\u5e73\u8861\u7f51\u7edc\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u786c\u4ef6\u7ebf\u6027\u5316\u65b9\u6cd5\u8ba1\u7b97\u68af\u5ea6\u3002", "result": "\u6210\u529f\u5728\u5668\u4ef6\u7ea7\u7535\u8def\u4eff\u771f\u4e2d\u6f14\u793a\u4e86\u5728\u786c\u4ef6\u4e2d\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u6269\u5c55\u5230\u7ea7\u8054\u7f51\u7edc\u4ee5\u5b9e\u73b0\u524d\u9988\u548c\u975e\u5bf9\u79f0\u7f51\u7edc\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u975e\u7406\u60f3\u4e8c\u6781\u7ba1\u6a21\u578b\u7684\u4e8c\u6781\u7ba1ReLU\u6fc0\u6d3b\u51fd\u6570\u3002", "conclusion": "\u7535\u963b-\u4e8c\u6781\u7ba1\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6a21\u62df\u786c\u4ef6\u4e2d\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7b80\u6d01\u65b9\u6cd5\uff0c\u786c\u4ef6\u7ebf\u6027\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u5728\u786c\u4ef6\u4e2d\u7684\u76f4\u63a5\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\u3002"}}
{"id": "2509.13978", "categories": ["cs.DC", "cs.AI", "cs.DB", "68M14, 68M20, 68T07", "C.2.4; D.1.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.13978", "abs": "https://arxiv.org/abs/2509.13978", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "comment": "Paper accepted in the proceedings of the ACM/IEEE Supercomputing\n  Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel\n  Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael\n  Ferreira da Silva. 2025. LLM Agents for Interactive Workflow Provenance:\n  Reference Architecture and Evaluation Methodology. In SC Workshops (WORKS)", "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u8fdb\u884c\u79d1\u5b66\u5de5\u4f5c\u6d41\u8fd0\u884c\u65f6\u6570\u636e\u4ea4\u4e92\u5f0f\u5206\u6790\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u53d1\u73b0\u4f9d\u8d56\u4e8e\u8de8\u8fb9\u7f18\u3001\u4e91\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u7684\u6570\u636e\u5904\u7406\u5de5\u4f5c\u6d41\uff0c\u4f46\u5927\u89c4\u6a21\u4e0b\u7684\u5de5\u4f5c\u6d41\u8ffd\u8e2a\u6570\u636e\u5206\u6790\u590d\u6742\u4e14\u56f0\u96be\uff0c\u73b0\u6709\u7cfb\u7edf\u4ea4\u4e92\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3001\u53c2\u8003\u67b6\u6784\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u5229\u7528\u4ea4\u4e92\u5f0fLLM\u4ee3\u7406\u8fdb\u884c\u8fd0\u884c\u65f6\u6570\u636e\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u5143\u6570\u636e\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u8ffd\u8e2a\u67e5\u8be2\uff0c\u5e76\u7ed3\u5408\u4e86\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u63d0\u793a\u8c03\u6574\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002", "result": "\u5728LLaMA\u3001GPT\u3001Gemini\u548cClaude\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u67e5\u8be2\u7c7b\u522b\u548c\u4e00\u4e2a\u771f\u5b9e\u7684\u5316\u5b66\u5de5\u4f5c\u6d41\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u4e14\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684LLM\u4ee3\u7406\u54cd\u5e94\uff0c\u8d85\u8d8a\u4e86\u8bb0\u5f55\u7684\u8ffd\u8e2a\u6570\u636e\u3002", "conclusion": "LLM\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u6709\u6548\u7b80\u5316\u548c\u589e\u5f3a\u5927\u89c4\u6a21\u79d1\u5b66\u5de5\u4f5c\u6d41\u6570\u636e\u7684\u5206\u6790\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.13745", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13745", "abs": "https://arxiv.org/abs/2509.13745", "authors": ["Hiroki Kuroda", "Renato Luis Garrido Cavalcante", "Masahiro Yukawa"], "title": "Theoretical Validation of the Latent Optimally Partitioned-$\\ell_2/\\ell_1$ Penalty with Application to Angular Power Spectrum Estimation", "comment": null, "summary": "This paper demonstrates that, in both theory and practice, the latent\noptimally partitioned (LOP)-$\\ell_2/\\ell_1$ penalty is effective for exploiting\nblock-sparsity without the knowledge of the concrete block structure. More\nprecisely, we first present a novel theoretical result showing that the\noptimized block partition in the LOP-$\\ell_2/\\ell_1$ penalty satisfy a\ncondition required for accurate recovery of block-sparse signals. Motivated by\nthis result, we present a new application of the LOP-$\\ell_2/\\ell_1$ penalty to\nestimation of angular power spectrum, which is block-sparse with unknown block\npartition, in MIMO communication systems. Numerical simulations show that the\nproposed use of block-sparsity with the LOP-$\\ell_2/\\ell_1$ penalty\nsignificantly improves the estimation accuracy of the angular power spectrum.", "AI": {"tldr": "LOP-ell2/ell1\u60e9\u7f5a\u5728\u4e0d\u77e5\u9053\u5177\u4f53\u5757\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u80fd\u6709\u6548\u5730\u5229\u7528\u5757\u7a00\u758f\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5229\u7528\u5757\u7a00\u758f\u6027\u6765\u63d0\u9ad8MIMO\u901a\u4fe1\u7cfb\u7edf\u4e2d\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u800c\u4e8b\u5148\u4e0d\u77e5\u9053\u5757\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u8868\u660eLOP-ell2/ell1\u60e9\u7f5a\u4e2d\u7684\u4f18\u5316\u5757\u5212\u5206\u6ee1\u8db3\u7cbe\u786e\u6062\u590d\u5757\u7a00\u758f\u4fe1\u53f7\u7684\u6761\u4ef6\u3002\u57fa\u4e8e\u6b64\uff0c\u5c06LOP-ell2/ell1\u60e9\u7f5a\u5e94\u7528\u4e8eMIMO\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u672a\u77e5\u5757\u5212\u5206\u7684\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u4f7f\u7528LOP-ell2/ell1\u60e9\u7f5a\u7684\u5757\u7a00\u758f\u6027\u663e\u8457\u63d0\u9ad8\u4e86\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LOP-ell2/ell1\u60e9\u7f5a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u77e5\u9053\u5177\u4f53\u5757\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u5757\u7a00\u758f\u6027\u6765\u6539\u8fdb\u4fe1\u53f7\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728MIMO\u901a\u4fe1\u7cfb\u7edf\u7684\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u65b9\u9762\u3002"}}
{"id": "2509.13882", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13882", "abs": "https://arxiv.org/abs/2509.13882", "authors": ["Junhwa Hong", "Beomjoon Lee", "Woojin Lee", "Changjoo Nam"], "title": "Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning", "comment": "7 pages", "summary": "We propose an efficient motion planning method designed to efficiently find\ncollision-free trajectories for multiple manipulators. While multi-manipulator\nsystems offer significant advantages, coordinating their motions is\ncomputationally challenging owing to the high dimensionality of their composite\nconfiguration space. Conflict-Based Search (CBS) addresses this by decoupling\nmotion planning, but suffers from subsequent conflicts incurred by resolving\nexisting conflicts, leading to an exponentially growing constraint tree of CBS.\nOur proposed method is based on repulsive trajectory modification within the\ntwo-level structure of CBS. Unlike conventional CBS variants, the low-level\nplanner applies a gradient descent approach using an Artificial Potential\nField. This field generates repulsive forces that guide the trajectory of the\nconflicting manipulator away from those of other robots. As a result,\nsubsequent conflicts are less likely to occur. Additionally, we develop a\nstrategy that, under a specific condition, directly attempts to find a\nconflict-free solution in a single step without growing the constraint tree.\nThrough extensive tests including physical robot experiments, we demonstrate\nthat our method consistently reduces the number of expanded nodes in the\nconstraint tree, achieves a higher success rate, and finds a solution faster\ncompared to Enhanced CBS and other state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65a5\u529b\u8f68\u8ff9\u4fee\u6b63\u7684\u51b2\u7a81\u9a71\u52a8\u641c\u7d22\uff08CBS\uff09\u65b9\u6cd5\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u89e3\u51b3\u591a\u673a\u68b0\u81c2\u534f\u8c03\u89c4\u5212\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u4eba\u5de5\u52bf\u573a\u751f\u6210\u65a5\u529b\u5f15\u5bfc\u89c4\u5212\uff0c\u51cf\u5c11\u540e\u7eed\u51b2\u7a81\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5c1d\u8bd5\u4e00\u6b65\u627e\u5230\u65e0\u51b2\u7a81\u89e3\u3002", "motivation": "\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u5b58\u5728\u9ad8\u7ef4\u590d\u5408\u6784\u578b\u7a7a\u95f4\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u6709\u51b2\u7a81\u9a71\u52a8\u641c\u7d22\uff08CBS\uff09\u65b9\u6cd5\u5b58\u5728\u7ea6\u675f\u6811\u7206\u70b8\u95ee\u9898\u3002", "method": "\u5728CBS\u7684\u4e24\u5c42\u7ed3\u6784\u4e2d\uff0c\u4f4e\u5c42\u89c4\u5212\u5668\u91c7\u7528\u57fa\u4e8e\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u751f\u6210\u65a5\u529b\u5f15\u5bfc\u51b2\u7a81\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\uff0c\u907f\u5f00\u5176\u4ed6\u673a\u5668\u4eba\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e00\u79cd\u7b56\u7565\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5c1d\u8bd5\u4e00\u6b65\u627e\u5230\u65e0\u51b2\u7a81\u89e3\u3002", "result": "\u4e0e\u589e\u5f3aCBS\u53ca\u5176\u4ed6\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u7ea6\u675f\u6811\u7684\u6269\u5c55\u8282\u70b9\u6570\uff0c\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u66f4\u5feb\u5730\u627e\u5230\u89e3\u3002\u5b9e\u9a8c\u5305\u62ec\u7269\u7406\u673a\u5668\u4eba\u6d4b\u8bd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5728CBS\u6846\u67b6\u5185\u7ed3\u5408\u65a5\u529b\u8f68\u8ff9\u4fee\u6b63\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u89e3\u51b3\u591a\u673a\u68b0\u81c2\u534f\u8c03\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u514b\u670d\u4e86\u4f20\u7edfCBS\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13392", "categories": ["cs.SY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.13392", "abs": "https://arxiv.org/abs/2509.13392", "authors": ["Demyan Yarmoshik", "Igor Ignashin", "Ekaterina Sikacheva", "Alexander Gasnikov"], "title": "Modeling skiers flows via Wardrope equilibrium in closed capacitated networks", "comment": null, "summary": "We propose an equilibrium model of ski resorts where users are assigned to\ncycles in a closed network. As queues form on lifts with limited capacity, we\nderive an efficient way to find waiting times via convex optimization. The\nequilibrium problem is formulated as a variational inequality, and numerical\nexperiments show that it can be solved using standard algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6ed1\u96ea\u80dc\u5730\u5747\u8861\u6a21\u578b\uff0c\u7528\u6237\u88ab\u5206\u914d\u5230\u5c01\u95ed\u7f51\u7edc\u4e2d\u7684\u5faa\u73af\u3002\u901a\u8fc7\u6c42\u89e3\u51f8\u4f18\u5316\u95ee\u9898\u6765\u9ad8\u6548\u5730\u8ba1\u7b97\u7535\u68af\u961f\u5217\u7684\u7b49\u5f85\u65f6\u95f4\u3002\u8be5\u5747\u8861\u95ee\u9898\u88ab\u8868\u8ff0\u4e3a\u53d8\u5206\u4e0d\u7b49\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7b97\u6cd5\u6c42\u89e3\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u6ed1\u96ea\u80dc\u5730\u5747\u8861\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u7531\u4e8e\u7535\u68af\u5bb9\u91cf\u6709\u9650\u800c\u4ea7\u751f\u7684\u961f\u5217\u7b49\u5f85\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6ed1\u96ea\u80dc\u5730\u5747\u8861\u6a21\u578b\uff0c\u5e76\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u53d8\u5206\u4e0d\u7b49\u5f0f\uff0c\u7136\u540e\u4f7f\u7528\u51f8\u4f18\u5316\u6765\u8ba1\u7b97\u7b49\u5f85\u65f6\u95f4\uff0c\u5e76\u7528\u6807\u51c6\u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97\u6ed1\u96ea\u80dc\u5730\u7b49\u5f85\u65f6\u95f4\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5747\u8861\u6a21\u578b\u548c\u8ba1\u7b97\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u6ed1\u96ea\u80dc\u5730\u7684\u7b49\u5f85\u65f6\u95f4\u95ee\u9898\u3002"}}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. G\u00fczel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rockt\u00e4schel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "\u6587\u7ae0\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u6a21\u62df\u73af\u5883\u6765\u8bad\u7ec3\u80fd\u591f\u5728\u65b0\u4efb\u52a1\u53d8\u4f53\u4e2d\u6cdb\u5316\u7684\u9c81\u68d2\u6027\u667a\u80fd\u4f53\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIMAC\uff08Imagined Autocurricula\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u6765\u81ea\u52a8\u751f\u6210\u5b66\u4e60\u8bfe\u7a0b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u8bad\u7ec3\u6570\u636e\u6216\u7cbe\u786e\u6a21\u62df\u7684\u7f3a\u4e4f\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u5229\u7528\u79bb\u7ebf\u6536\u96c6\u7684\u6570\u636e\u751f\u6210\u591a\u6837\u5316\u7684\u6a21\u62df\u73af\u5883\u6765\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIMAC\uff08Imagined Autocurricula\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u5728\u751f\u6210\u7684\u4e16\u754c\u4e2d\u8bf1\u5bfc\u4e00\u4e2a\u81ea\u52a8\u5b66\u4e60\u8bfe\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u7a0b\u5e8f\u751f\u6210\u73af\u5883\u4e2d\uff0c\u8bc1\u660e\u4e86\u4ec5\u5728\u4ece\u8f83\u7a84\u6570\u636e\u96c6\u5b66\u4e60\u5230\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u53ef\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7684\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u6587\u7ae0\u8ba4\u4e3a\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5229\u7528\u66f4\u5927\u89c4\u6a21\u7684\u57fa\u7840\u4e16\u754c\u6a21\u578b\u6765\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "VLMs\u5728\u96f6\u6837\u672cOOD\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6709\u6548\u6027\u3001\u4f18\u52bf\u548c\u9c81\u68d2\u6027\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86VLM\u7684\u5185\u5728\u673a\u5236\u3001\u76f8\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\uff08\u6e90\u4e8e\u8bed\u4e49\u65b0\u9896\u6027\uff09\uff0c\u5e76\u6307\u51fa\u4e86\u5176\u5728\u63d0\u793a\u8bcd\u9009\u62e9\u4e0a\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u96f6\u6837\u672cOOD\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u3001\u4f18\u52bf\u4ee5\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\u7f3a\u4e4f\u5168\u9762\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u4f7f\u7528ID\u548cOOD\u63d0\u793a\u8bcd\uff0c\u5bf9VLM\u8fdb\u884c\u7cfb\u7edf\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4ee5\uff081\uff09\u89e3\u6790\u5176\u5de5\u4f5c\u673a\u5236\uff0c\uff082\uff09\u91cf\u5316\u5176\u76f8\u8f83\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\uff083\uff09\u63a2\u7a76\u5176\u9c81\u68d2\u6027\uff08\u7279\u522b\u662f\u5bf9\u63d0\u793a\u8bcd\u7684\u654f\u611f\u6027\uff09\u3002", "result": "\uff081\uff09\u63ed\u793a\u4e86VLM\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4fc3\u8fdb\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7684\u5173\u952e\u64cd\u4f5c\u7279\u6027\uff1b\uff082\uff09\u7ecf\u9a8c\u91cf\u5316\u4e86VLM\u76f8\u6bd4\u4e8e\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5f52\u56e0\u4e8eVLM\u5229\u7528\u4e30\u5bcc\u8bed\u4e49\u65b0\u9896\u6027\u7684\u80fd\u529b\uff1b\uff083\uff09\u53d1\u73b0\u4e86VLM\u5728\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u5bf9\u56fe\u50cf\u566a\u58f0\u6709\u5f39\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u8bcd\u7684\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u672c\u6587\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u5bf9VLM\u9a71\u52a8\u7684OOD\u68c0\u6d4b\u7684\u4f18\u52bf\u548c\u5173\u952e\u8106\u5f31\u6027\u7684\u66f4\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u53ef\u9760\u7684\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.13837", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.13837", "abs": "https://arxiv.org/abs/2509.13837", "authors": ["Wei-Zhe Yuan", "Yangyu Guo", "Hong-Liang Yi"], "title": "Crystal Orientation Dependence of Extreme Near-Field Heat Transfer between Polar Materials Governed by Surface Phonon Modes", "comment": "9 pages, 6 figures", "summary": "Due to the rapid development of micro- and nano-manufacturing and electronic\ndevices, heat transfer at the transition regime between radiation and\nconduction becomes increasingly important. Recent work has demonstrated the\nimportance of nonlocal optical response and phonon tunneling. However, it\nremains unclear how the crystal orientation impacts them. In this work, we\nstudy this effect on heat transport across vacuum gaps between magnesium oxide\n(MgO) by nonequilibrium molecular dynamics (NEMD) simulation. At 5~\\AA~gaps,\nthe overall thermal conductance exhibits 30\\% enhancement for [100] orientation\nversus [110] and [210], while becoming orientation-insensitive beyond 6~\\AA.\nWhen the gap size is extremely small, the crystal orientation significantly\nimpacts the resonance frequencies of spectral thermal conductance which are\nquite close to those of unique surface phonon modes distinct from bulk\ncounterparts. As the gap size gradually increases, the spectral thermal\nconductance gradually converges to the predicted results of\nfluctuation-electrodynamics (FE) theory in the long-wavelength approximation.\nOur findings reveal how surface phonon modes govern extreme near-field heat\ntransfer across nanogap, providing insights for thermal management in\nelectronic devices.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6676\u4f53\u53d6\u5411\u5bf9MgO\u7eb3\u7c73\u95f4\u9699\u70ed\u4f20\u8f93\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u6781\u8fd1\u8ddd\u79bb\u65f6\u6676\u4f53\u53d6\u5411\u663e\u8457\u5f71\u54cd\u70ed\u5bfc\uff0c\u800c\u5728\u8f83\u5927\u8ddd\u79bb\u65f6\u5f71\u54cd\u51cf\u5f31\uff0c\u8fd9\u4e0e\u8868\u9762\u58f0\u5b50\u6a21\u5f0f\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u7406\u8bba\u6709\u5173\u3002", "motivation": "\u5fae\u7eb3\u5236\u9020\u548c\u7535\u5b50\u5668\u4ef6\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u8f90\u5c04\u548c\u4f20\u5bfc\u4e4b\u95f4\u7684\u8fc7\u6e21\u72b6\u6001\u4e0b\u7684\u70ed\u4f20\u8f93\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u6676\u4f53\u53d6\u5411\u5bf9\u5176\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u975e\u5e73\u8861\u5206\u5b50\u52a8\u529b\u5b66\uff08NEMD\uff09\u6a21\u62df\u7814\u7a76\u4e86\u9541\u6a44\u6984\u77f3\uff08MgO\uff09\u4e4b\u95f4\u771f\u7a7a\u95f4\u9699\u4e2d\u7684\u70ed\u4f20\u8f93\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u6676\u4f53\u53d6\u5411\u7684\u5f71\u54cd\u3002", "result": "\u57285\u57c3\u7684\u95f4\u9699\u4e0b\uff0c[100]\u53d6\u5411\u76f8\u6bd4\u4e8e[110]\u548c[210]\u53d6\u5411\uff0c\u6574\u4f53\u70ed\u5bfc\u7387\u63d0\u9ad8\u4e8630%\uff0c\u800c\u57286\u57c3\u4ee5\u4e0a\u5219\u65e0\u660e\u663e\u53d6\u5411\u4f9d\u8d56\u6027\u3002\u6781\u5c0f\u95f4\u9699\u65f6\uff0c\u6676\u4f53\u53d6\u5411\u663e\u8457\u5f71\u54cd\u4e86\u4e0e\u8868\u9762\u58f0\u5b50\u6a21\u5f0f\u76f8\u5173\u7684\u5149\u8c31\u70ed\u5bfc\u5171\u632f\u9891\u7387\u3002\u968f\u7740\u95f4\u9699\u589e\u5927\uff0c\u5149\u8c31\u70ed\u5bfc\u9010\u6e10\u7b26\u5408\u957f\u6ce2\u8fd1\u4f3c\u4e0b\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u8868\u9762\u58f0\u5b50\u6a21\u5f0f\u5728\u63a7\u5236\u7eb3\u7c73\u95f4\u9699\u4e2d\u7684\u8fd1\u573a\u70ed\u4f20\u8f93\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u7535\u5b50\u5668\u4ef6\u7684\u70ed\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "LLMs can be controlled at the neuron level to detect and manage ambiguity in questions.", "motivation": "Ambiguity in real-world questions is common, but LLMs often provide confident answers without seeking clarification. This work aims to address this by understanding and controlling how LLMs process ambiguity.", "method": "Identify and isolate 'Ambiguity-Encoding Neurons' (AENs) responsible for processing ambiguity in LLMs. Analyze their emergence in shallow layers and demonstrate control over LLM behavior by manipulating these neurons.", "result": "A small number of AENs (as few as one) encode ambiguity information, which can be detected with high accuracy using probes. These probes outperform baseline methods. Manipulating AENs allows control over LLM responses, shifting from direct answers to abstention.", "conclusion": "LLMs possess compact internal representations for question ambiguity, which can be interpreted and controlled at the neuron level. This opens possibilities for more nuanced and controlled LLM interactions."}}
{"id": "2509.13406", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2509.13406", "abs": "https://arxiv.org/abs/2509.13406", "authors": ["Shreya Vardhan", "Jinzhao Wang"], "title": "Free mutual information and higher-point OTOCs", "comment": "45+38 pages, 24 figures. Comments are welcome!", "summary": "We introduce a quantity called the free mutual information (FMI), adapted\nfrom concepts in free probability theory, as a new physical measure of quantum\nchaos. This quantity captures the spreading of a time-evolved operator in the\nspace of all possible operators on the Hilbert space, which is doubly\nexponential in the number of degrees of freedom. It thus provides a finer\nnotion of operator spreading than the well-understood phenomenon of operator\ngrowth in physical space. We derive two central results which apply in any\nphysical system: first, an explicit ``Coulomb gas'' formula for the FMI of two\nobservables $A(t)$ and $B$ in terms of the eigenvalues of the product operator\n$A(t)B$; and second, a general relation expressing the FMI as a weighted sum of\nall higher-point out-of-time-ordered correlators (OTOCs). This second result\nprovides a precise information-theoretic interpretation for the higher-point\nOTOCs as collectively quantifying operator ergodicity and the approach to\nfreeness. This physical interpretation is particularly useful in light of\nrecent progress in experimentally measuring higher-point OTOCs. We identify\nuniversal behaviours of the FMI and higher-point OTOCs across a variety of\nchaotic systems, including random unitary circuits and chaotic spin chains,\nwhich indicate that spreading in the doubly exponential operator space is a\ngeneric feature of quantum many-body chaos. At the same time, the non-generic\nbehavior of the FMI in various non-chaotic systems, including certain unitary\ndesigns, shows that there are cases where an operator spreads in physical space\nbut remains localized in operator space. The FMI is thus a sharper diagnostic\nof chaos than the standard 4-point OTOC.", "AI": {"tldr": "\u514d\u8d39\u4e92\u4fe1\u606f\uff08FMI\uff09\u662f\u91cf\u5b50\u6df7\u6c8c\u7684\u65b0\u7269\u7406\u5ea6\u91cf\uff0c\u5b83\u6bd4\u4f20\u7edf\u7684\u7b97\u5b50\u589e\u957f\u66f4\u80fd\u7cbe\u7ec6\u5730\u6355\u6349\u7b97\u5b50\u7684\u4f20\u64ad\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u7269\u7406\u5ea6\u91cf\u6765\u91cf\u5316\u91cf\u5b50\u6df7\u6c8c\uff0c\u8be5\u5ea6\u91cf\u6bd4\u73b0\u6709\u7684\u7b97\u5b50\u589e\u957f\u6982\u5ff5\u66f4\u7cbe\u7ec6\u3002", "method": "\u63a8\u5bfc\u4e86FMI\u7684\u201c\u5e93\u4ed1\u6c14\u4f53\u201d\u516c\u5f0f\uff0c\u5e76\u5c06FMI\u4e0e\u9ad8\u9636\u65f6\u5e8f\u65e0\u5173\u5173\u8054\uff08OTOC\uff09\u8054\u7cfb\u8d77\u6765\u3002", "result": "FMI\u7684\u201c\u5e93\u4ed1\u6c14\u4f53\u201d\u516c\u5f0f\uff0cFMI\u4e0e\u9ad8\u9636OTOC\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u5404\u79cd\u6df7\u6c8c\u548c\u975e\u6df7\u6c8c\u7cfb\u7edf\u4e2dFMI\u548cOTOC\u7684\u666e\u9002\u884c\u4e3a\u548c\u975e\u666e\u9002\u884c\u4e3a\u3002", "conclusion": "FMI\u662f\u6bd4\u6807\u51c64\u70b9OTOC\u66f4\u7cbe\u786e\u7684\u6df7\u6c8c\u8bca\u65ad\u5de5\u5177\uff0c\u5b83\u91cf\u5316\u4e86\u7b97\u5b50\u5728\u7b97\u5b50\u7a7a\u95f4\u4e2d\u7684\u6269\u6563\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u591a\u4f53\u6df7\u6c8c\u7684\u901a\u7528\u7279\u5f81\uff0c\u5e76\u533a\u5206\u4e86\u7b97\u5b50\u7a7a\u95f4\u4e2d\u7684\u6269\u6563\u548c\u7269\u7406\u7a7a\u95f4\u4e2d\u7684\u6269\u6563\u3002"}}
{"id": "2509.14112", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14112", "abs": "https://arxiv.org/abs/2509.14112", "authors": ["Muqsit Azeem", "Jan Kretinsky", "Maximilian Weininger"], "title": "Sound Value Iteration for Simple Stochastic Games", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258. A full version of this\n  paper appears at arXiv:2411.11549", "summary": "Algorithmic analysis of Markov decision processes (MDP) and stochastic games\n(SG) in practice relies on value-iteration (VI) algorithms. Since basic VI does\nnot provide guarantees on the precision of the result, variants of VI have been\nproposed that offer such guarantees. In particular, sound value iteration (SVI)\nnot only provides precise lower and upper bounds on the result, but also\nconverges faster in the presence of probabilistic cycles. Unfortunately, it is\nneither applicable to SG, nor to MDP with end components. In this paper, we\nextend SVI and cover both cases. The technical challenge consists mainly in\nproper treatment of end components, which require different handling than in\nthe literature. Moreover, we provide several optimizations of SVI. Finally, we\nevaluate our prototype implementation experimentally to demonstrate its\npotential on systems with probabilistic cycles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684SVI\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5904\u7406MDP\u548cSG\u4e2d\u7684\u7ec8\u7aef\u7ec4\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u548c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u4ef7\u503c\u8fed\u4ee3\uff08VI\uff09\u7b97\u6cd5\u5728\u5904\u7406MDP\u548cSG\u65f6\u7f3a\u4e4f\u7cbe\u5ea6\u4fdd\u8bc1\uff0c\u800cSVI\u7b97\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u7cbe\u5ea6\u4fdd\u8bc1\uff0c\u4f46\u4e0d\u80fd\u7528\u4e8eSG\u6216\u5177\u6709\u7ec8\u7aef\u7ec4\u4ef6\u7684MDP\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684SVI\u7b97\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5982\u4f55\u5904\u7406\u7ec8\u7aef\u7ec4\u4ef6\uff0c\u5e76\u52a0\u5165\u4e86\u51e0\u79cd\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u539f\u578b\u5b9e\u73b0\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u5177\u6709\u6982\u7387\u5faa\u73af\u7684\u7cfb\u7edf\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "\u6269\u5c55\u7684SVI\u7b97\u6cd5\u80fd\u591f\u6210\u529f\u5904\u7406MDP\u548cSG\u4e2d\u7684\u7ec8\u7aef\u7ec4\u4ef6\uff0c\u5e76\u4e14\u901a\u8fc7\u4f18\u5316\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2509.13479", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.13479", "abs": "https://arxiv.org/abs/2509.13479", "authors": ["Rahul Bouri", "Manikantan R. Nair", "Tribeni Roy"], "title": "From Data to Alloys Predicting and Screening High Entropy Alloys for High Hardness Using Machine Learning", "comment": "23 pages", "summary": "The growing need for structural materials with strength, mechanical\nstability, and durability in extreme environments is driving the development of\nhigh entropy alloys. These are materials with near equiatomic mixing of five or\nmore principal elements, and such compositional complexity often leads to\nimprovements in mechanical properties and high thermal stability, etc. Thus,\nhigh-entropy alloys have found their applications in domains like aerospace,\nbiomedical, energy storage, catalysis, electronics, etc. However, the vast\ncompositional design and experimental exploration of high-entropy alloys are\nboth time consuming and expensive and require a large number of resources.\nMachine learning techniques have thus become essential for accelerating high\nentropy alloys discovery using data driven predictions of promising alloy\ncombinations and their properties. Hence, this work employs a machine learning\nframework that predicts high entropy alloy hardness from elemental descriptors\nsuch as atomic radius, valence electron count, bond strength, etc. Machine\nlearning regression models, like LightGBM, Gradient Boosting Regressor, and\nTransformer encoder, were trained on experimental data. Additionally, a\nlanguage model was also fine tuned to predict hardness from elemental\ndescriptor strings. The results indicate that LightGBM has better accuracy in\npredicting the hardness of high entropy alloys compared to other models used in\nthis study. Further, a combinatorial technique was used to generate over 9\nmillion virtual high entropy alloy candidates, and the trained machine learning\nmodels were used to predict their hardness. This study shows how machine\nlearning-driven high throughput screening and language modelling approaches can\naccelerate the development of next generation high entropy alloys.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u88ab\u7528\u4e8e\u9884\u6d4b\u9ad8\u71b5\u5408\u91d1\u7684\u786c\u5ea6\uff0c\u4ee5\u52a0\u901f\u5176\u7814\u53d1\u3002", "motivation": "\u9ad8\u71b5\u5408\u91d1\u5728\u6781\u7aef\u73af\u5883\u4e0b\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u4f46\u5176\u7814\u53d1\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u9700\u8981\u673a\u5668\u5b66\u4e60\u52a0\u901f\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LightGBM\u3001Gradient Boosting Regressor\u548cTransformer encoder\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u539f\u5b50\u534a\u5f84\u3001\u4ef7\u7535\u5b50\u6570\u3001\u952e\u80fd\u7b49\u5143\u7d20\u63cf\u8ff0\u7b26\u9884\u6d4b\u9ad8\u71b5\u5408\u91d1\u7684\u786c\u5ea6\u3002", "result": "LightGBM\u6a21\u578b\u5728\u9884\u6d4b\u9ad8\u71b5\u5408\u91d1\u786c\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u6bd4\u5176\u4ed6\u6a21\u578b\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u7ec4\u5408\u6280\u672f\u751f\u6210\u4e86\u8d85\u8fc7900\u4e07\u4e2a\u5019\u9009\u5408\u91d1\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e86\u5b83\u4eec\u7684\u786c\u5ea6\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9ad8\u901a\u91cf\u7b5b\u9009\u548c\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3\u9ad8\u71b5\u5408\u91d1\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.13997", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13997", "abs": "https://arxiv.org/abs/2509.13997", "authors": ["Yu Zhu", "Aditya Dhakal", "Pedro Bruel", "Gourav Rattihalli", "Yunming Xiao", "Johann Lombardi", "Dejan Milojicic"], "title": "An RDMA-First Object Storage System with SmartNIC Offload", "comment": null, "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work.", "AI": {"tldr": "ROS2\u662f\u4e00\u4e2aRDMA\u4f18\u5148\u7684\u5bf9\u8c61\u5b58\u50a8\u7cfb\u7edf\uff0c\u5c06DAOS\u5ba2\u6237\u7aef\u5378\u8f7d\u5230SmartNIC\u4e0a\uff0c\u4ee5\u63d0\u9ad8AI\u8bad\u7ec3\u7684I/O\u6027\u80fd\u3002", "motivation": "AI\u8bad\u7ec3\u548c\u63a8\u7406\u5bf9\u5b58\u50a8\u8def\u5f84\u9020\u6210\u6301\u7eed\u7684\u3001\u7ec6\u7c92\u5ea6\u7684I/O\u538b\u529b\uff0c\u9700\u8981\u5bf9\u73b0\u6709\u7684\u57fa\u4e8eTCP\u7684\u5b58\u50a8\u8def\u5f84\u8fdb\u884c\u4f18\u5316\u3002", "method": "ROS2\u91c7\u7528RDMA\u4f18\u5148\u7684\u8bbe\u8ba1\uff0c\u5c06DAOS\u5ba2\u6237\u7aef\u5378\u8f7d\u5230NVIDIA BlueField-3 SmartNIC\u4e0a\uff0c\u901a\u8fc7UCX/libfabric\u5b9e\u73b0\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u5206\u79bb\u63a7\u5236\u5e73\u9762\u548c\u6570\u636e\u5e73\u9762\uff0c\u79fb\u9664\u4e3b\u673a\u5728\u6570\u636e\u8def\u5f84\u4e0a\u7684\u4e2d\u4ecb\u3002", "result": "\u5728\u670d\u52a1\u5668\u7ea7CPU\u4e0a\uff0cRDMA\u5728\u987a\u5e8f\u548c\u968f\u673aI/O\u65b9\u9762\u5747\u4f18\u4e8eTCP\u3002\u5c06DAOS\u5ba2\u6237\u7aef\u5378\u8f7d\u5230BlueField-3\u540e\uff0c\u7aef\u5230\u7aef\u6027\u80fd\u4e0e\u4e3b\u673a\u76f8\u5f53\uff0c\u8868\u660eSmartNIC\u5378\u8f7d\u53ef\u4ee5\u4fdd\u6301RDMA\u6548\u7387\uff0c\u5e76\u652f\u6301DPU\u9a7b\u7559\u529f\u80fd\u3002\u7136\u800c\uff0cSmartNIC\u4e0a\u7684TCP\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u57fa\u4e8eRDMA\u4f18\u5148\u3001SmartNIC\u5378\u8f7d\u7684\u5bf9\u8c61\u5b58\u50a8\u5806\u6808\u4e3a\u6269\u5c55\u73b0\u4ee3LLM\u8bad\u7ec3\u73af\u5883\u4e2d\u7684\u6570\u636e\u4f20\u8f93\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u57fa\u7840\u3002"}}
{"id": "2509.14094", "categories": ["cs.LO", "F.4.1;I.2.3"], "pdf": "https://arxiv.org/pdf/2509.14094", "abs": "https://arxiv.org/abs/2509.14094", "authors": ["Radu Mardare", "Neil Ghani", "Eigil Rischel"], "title": "Metric Equational Theories", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "This paper proposes appropriate sound and complete proof systems for\nalgebraic structures over metric spaces by combining the development of\nQuantitative Equational Theories (QET) with the Enriched Lawvere Theories. We\nextend QETs to Metric Equational Theories (METs) where operations no longer\nhave finite sets as arities (as in QETs and the general theory of universal\nalgebras), but arities are now drawn from countable metric spaces. This\nextension is inspired by the theory of Enriched Lawvere Theories, which\nsuggests that the arities of operations should be the lambda-presentable\nobjects of the underlying lambda-accessible category. In this setting, the\nvalidity of terms in METs can no longer be guaranteed independently of the\nvalidity of equations, as is the case with QET. We solve this problem, and\nadapt the sound and complete proof system for QETs to these more general METs,\ntaking advantage of the specific structure of metric spaces.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c06\u5b9a\u91cf\u65b9\u7a0b\u7406\u8bba\uff08QET\uff09\u4e0e\u5bcc\u96c6Lawvere\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u4e0a\u7684\u4ee3\u6570\u7ed3\u6784\u63d0\u51fa\u5408\u9002\u4e14\u5b8c\u5907\u7684\u8bc1\u660e\u7cfb\u7edf\u3002", "motivation": "\u5c06QET\u4e0e\u5bcc\u96c6Lawvere\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u4e0a\u7684\u4ee3\u6570\u7ed3\u6784\u63d0\u4f9b\u5408\u9002\u7684\u8bc1\u660e\u7cfb\u7edf\u3002", "method": "\u5c06QET\u6269\u5c55\u5230\u5ea6\u91cf\u65b9\u7a0b\u7406\u8bba\uff08MET\uff09\uff0c\u5176\u4e2d\u64cd\u4f5c\u7684\u5143\u6570\u4ece\u53ef\u6570\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u62bd\u53d6\uff0c\u5e76\u636e\u6b64\u8c03\u6574QET\u7684\u8bc1\u660e\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8eMET\u7684\u5065\u5168\u4e14\u5b8c\u5907\u7684\u8bc1\u660e\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u5c06QET\u4e0e\u5bcc\u96c6Lawvere\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u5e76\u8fdb\u884c\u76f8\u5e94\u6269\u5c55\uff0c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u4e0a\u7684\u4ee3\u6570\u7ed3\u6784\u5f00\u53d1\u4e86\u65b0\u7684\u8bc1\u660e\u7cfb\u7edf\u3002"}}
{"id": "2509.13527", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2509.13527", "abs": "https://arxiv.org/abs/2509.13527", "authors": ["Yulia Pimonova", "Michael G. Taylor", "Alice Allen", "Ping Yang", "Nicholas Lubbers"], "title": "Meta-Learning Linear Models for Molecular Property Prediction", "comment": "26 pages, 16 figures", "summary": "Chemists in search of structure-property relationships face great challenges\ndue to limited high quality, concordant datasets. Machine learning (ML) has\nsignificantly advanced predictive capabilities in chemical sciences, but these\nmodern data-driven approaches have increased the demand for data. In response\nto the growing demand for explainable AI (XAI) and to bridge the gap between\npredictive accuracy and human comprehensibility, we introduce LAMeL - a Linear\nAlgorithm for Meta-Learning that preserves interpretability while improving the\nprediction accuracy across multiple properties. While most approaches treat\neach chemical prediction task in isolation, LAMeL leverages a meta-learning\nframework to identify shared model parameters across related tasks, even if\nthose tasks do not share data, allowing it to learn a common functional\nmanifold that serves as a more informed starting point for new unseen tasks.\nOur method delivers performance improvements ranging from 1.1- to 25-fold over\nstandard ridge regression, depending on the domain of the dataset. While the\ndegree of performance enhancement varies across tasks, LAMeL consistently\noutperforms or matches traditional linear methods, making it a reliable tool\nfor chemical property prediction where both accuracy and interpretability are\ncritical.", "AI": {"tldr": "LAMeL\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\u8bc6\u522b\u8de8\u76f8\u5173\u4efb\u52a1\u7684\u5171\u4eab\u6a21\u578b\u53c2\u6570\uff0c\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u5316\u5b66\u6027\u8d28\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\u3002", "motivation": "\u5316\u5b66\u5bb6\u5728\u5bfb\u627e\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u65f6\u9762\u4e34\u6570\u636e\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u6709\u9650\u7684\u6311\u6218\u3002\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u589e\u52a0\u4e86\u5bf9\u6570\u636e\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4eba\u4eec\u4e5f\u65e5\u76ca\u9700\u8981\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u3002", "method": "LAMeL\uff08\u4e00\u79cd\u7ebf\u6027\u5143\u5b66\u4e60\u7b97\u6cd5\uff09\u5229\u7528\u5143\u5b66\u4e60\u6846\u67b6\u6765\u8bc6\u522b\u8de8\u76f8\u5173\u4efb\u52a1\uff08\u5373\u4f7f\u8fd9\u4e9b\u4efb\u52a1\u6ca1\u6709\u5171\u4eab\u6570\u636e\uff09\u7684\u5171\u4eab\u6a21\u578b\u53c2\u6570\uff0c\u4ece\u800c\u5b66\u4e60\u4e00\u4e2a\u901a\u7528\u7684\u51fd\u6570\u6d41\u5f62\uff0c\u4e3a\u65b0\u7684\u672a\u89c1\u4efb\u52a1\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u4fe1\u606f\u8d77\u70b9\u3002", "result": "LAMeL \u7684\u6027\u80fd\u6bd4\u6807\u51c6\u7684\u5cad\u56de\u5f52\u65b9\u6cd5\u63d0\u9ad8\u4e86 1.1 \u5230 25 \u500d\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\u7684\u9886\u57df\u3002", "conclusion": "LAMeL \u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u5316\u5b66\u6027\u8d28\u9884\u6d4b\u65b9\u9762\uff0c\u5176\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u6216\u5339\u914d\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\uff0c\u662f\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u90fd\u81f3\u5173\u91cd\u8981\u7684\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.13827", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.13827", "abs": "https://arxiv.org/abs/2509.13827", "authors": ["Renyuan Liu", "Haoting Zhou", "Chuankai Fang", "Qinbing Fu"], "title": "How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots", "comment": "9 pages, 6 figures", "summary": "Anyone who has tried to swat a fly has likely been frustrated by its\nremarkable agility.This ability stems from its visual neural perception system,\nparticularly the collision-selective neurons within its small brain.For\nautonomous robots operating in complex and unfamiliar environments, achieving\nsimilar agility is highly desirable but often constrained by the trade-off\nbetween computational cost and performance.In this context, insect-inspired\nintelligence offers a parsimonious route to low-power, computationally\nefficient frameworks.In this paper, we propose an attention-driven visuomotor\ncontrol strategy inspired by a specific class of fly visual projection\nneurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated\nescape behaviors.To our knowledge, this represents the first embodiment of an\nLPLC2 neural model in the embedded vision of a physical mobile robot, enabling\ncollision perception and reactive evasion.The model was simplified and\noptimized at 70KB in memory to suit the computational constraints of a\nvision-based micro robot, the Colias, while preserving key neural perception\nmechanisms.We further incorporated multi-attention mechanisms to emulate the\ndistributed nature of LPLC2 responses, allowing the robot to detect and react\nto approaching targets both rapidly and selectively.We systematically evaluated\nthe proposed method against a state-of-the-art locust-inspired collision\ndetection model.Results showed that the fly-inspired visuomotor model achieved\ncomparable robustness, at success rate of 96.1% in collision detection while\nproducing more adaptive and elegant evasive maneuvers.Beyond demonstrating an\neffective collision-avoidance strategy, this work highlights the potential of\nfly-inspired neural models for advancing research into collective behaviors in\ninsect intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u98de\u8747\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\u542f\u53d1\u7684\u6ce8\u610f\u9a71\u52a8\u7684\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u673a\u5668\u4eba\u907f\u649e\uff0c\u5e76\u5728\u540d\u4e3aColias\u7684\u5d4c\u5165\u5f0f\u89c6\u89c9\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8670KB\u7684\u4f18\u5316\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u7c7b\u4f3c\u6606\u866b\u7684\u654f\u6377\u6027\uff0c\u514b\u670d\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u501f\u9274\u6606\u866b\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u3002\u5c24\u5176\u5173\u6ce8\u98de\u8747\u7684LPLC2\u795e\u7ecf\u5143\u53ca\u5176\u9003\u907f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7LPLC2\u795e\u7ecf\u5143\u542f\u53d1\u7684\u6ce8\u610f\u9a71\u52a8\u7684\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u9488\u5bf9\u6027\u5730\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f7f\u5176\u5185\u5b58\u5360\u7528\u4ec5\u4e3a70KB\uff0c\u4ee5\u9002\u5e94\u540d\u4e3aColias\u7684\u5fae\u578b\u673a\u5668\u4eba\u3002\u901a\u8fc7\u591a\u91cd\u6ce8\u610f\u529b\u673a\u5236\u6765\u6a21\u62dfLPLC2\u7684\u5206\u5e03\u5f0f\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u548c\u9009\u62e9\u6027\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u53cd\u5e94\u3002", "result": "\u5728\u4e0e\u4e00\u79cd\u5148\u8fdb\u7684\u8757\u866b\u542f\u53d1\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u540e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u98de\u8747\u542f\u53d1\u6a21\u578b\u5728\u907f\u649e\u65b9\u9762\u8868\u73b0\u51fa96.1%\u7684\u6210\u529f\u7387\uff0c\u5728\u9c81\u68d2\u6027\u65b9\u9762\u76f8\u5f53\uff0c\u5e76\u4e14\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u9002\u5e94\u6027\u548c\u66f4\u4f18\u96c5\u7684\u89c4\u907f\u673a\u52a8\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06LPLC2\u795e\u7ecf\u5143\u6a21\u578b\u5d4c\u5165\u5230\u7269\u7406\u673a\u5668\u4eba\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u907f\u649e\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u98de\u8747\u542f\u53d1\u6a21\u578b\u5728\u63d0\u5347\u673a\u5668\u4eba\u667a\u80fd\u548c\u7814\u7a76\u7fa4\u4f53\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13786", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13786", "abs": "https://arxiv.org/abs/2509.13786", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization", "comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026", "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. We extend Post-Training Quantization (PTQ)\nbaselines with Quantization-Aware Training (QAT), which incorporates\nlow-precision simulation during training for robustness at ultra-low bitwidths.\nOur study applies QAT/PTQ to a neural receiver architecture and evaluates\nacross 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS\nenvironments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit\nQAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT\nmodels are also shown to outperform PTQ models by up to 3 dB, and yield 8x\ncompression. These results demonstrate that QAT is a key enabler of\nlow-complexity and latency-constrained inference at the PHY layer, facilitating\nreal-time processing in 6G edge devices", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\u57286G\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u9700\u8981\u8fdb\u884c\u91cf\u5316\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u672c\u7814\u7a76\u5c06\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\uff0c\u5e76\u57283GPP CDL-B/D\u4fe1\u9053\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c4\u4f4d\u548c8\u4f4dQAT\u6a21\u578b\u5728\u5757\u9519\u8bef\u7387\uff08BLER\uff09\u65b9\u9762\u4e0eFP32\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e868\u500d\u538b\u7f29\u3002", "motivation": "\u57286G\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u4f4e\u5185\u5b58\u5360\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\uff0c\u9700\u8981\u8fdb\u884c\u9ad8\u6548\u7684\u91cf\u5316\uff0c\u540c\u65f6\u4fdd\u8bc1\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u5c06\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\u67b6\u6784\uff0c\u5e76\u4e0e\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u57283GPP Clustered Delay Line (CDL)-B/D\u4fe1\u9053\uff08\u5305\u62ec\u89c6\u8ddd\uff08LoS\uff09\u548c\u975e\u89c6\u8ddd\uff08NLoS\uff09\u73af\u5883\uff09\u4ee5\u53ca\u9ad8\u8fbe40 m/s\u7684\u7528\u6237\u901f\u5ea6\u4e0b\uff0c\u5bf9\u4e0d\u540c\u91cf\u5316\u6bd4\u7279\u5bbd\u5ea6\uff08\u59824\u4f4d\u548c8\u4f4d\uff09\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e0eFP32\u6a21\u578b\u76f8\u6bd4\uff0c4\u4f4d\u548c8\u4f4dQAT\u6a21\u578b\u572810%\u7684\u76ee\u6807\u5757\u9519\u8bef\u7387\uff08BLER\uff09\u4e0b\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684BLER\u6027\u80fd\u3002QAT\u6a21\u578b\u6bd4PTQ\u6a21\u578b\u6027\u80fd\u9ad8\u51fa3 dB\uff0c\u5e76\u5b9e\u73b0\u4e868\u500d\u7684\u538b\u7f29\u7387\u3002", "conclusion": "\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u662f\u5b9e\u73b0\u7269\u7406\u5c42\uff08PHY\uff09\u4f4e\u590d\u6742\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u5173\u952e\u6280\u672f\uff0c\u80fd\u591f\u4fc3\u8fdb6G\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u5904\u7406\u3002"}}
{"id": "2509.14025", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14025", "abs": "https://arxiv.org/abs/2509.14025", "authors": ["Rui Huang", "Zhiyu Gao", "Siyu Tang", "Jialin Zhang", "Lei He", "Ziqian Zhang", "Lin Zhao"], "title": "TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems", "comment": null, "summary": "Modular Aerial Robot Systems (MARS) consist of multiple drone modules that\nare physically bound together to form a single structure for flight. Exploiting\nstructural redundancy, MARS can be reconfigured into different formations to\nmitigate unit or rotor failures and maintain stable flight. Prior work on MARS\nself-reconfiguration has solely focused on maximizing controllability margins\nto tolerate a single rotor or unit fault for rectangular-shaped MARS. We\npropose TransforMARS, a general fault-tolerant reconfiguration framework that\ntransforms arbitrarily shaped MARS under multiple rotor and unit faults while\nensuring continuous in-air stability. Specifically, we develop algorithms to\nfirst identify and construct minimum controllable assemblies containing faulty\nunits. We then plan feasible disassembly-assembly sequences to transport MARS\nunits or subassemblies to form target configuration. Our approach enables more\nflexible and practical feasible reconfiguration. We validate TransforMARS in\nchallenging arbitrarily shaped MARS configurations, demonstrating substantial\nimprovements over prior works in both the capacity of handling diverse\nconfigurations and the number of faults tolerated. The videos and source code\nof this work are available at the anonymous repository:\nhttps://anonymous.4open.science/r/TransforMARS-1030/", "AI": {"tldr": "MARS\u53ef\u4ee5\u901a\u8fc7\u91cd\u6784\u6765\u5bb9\u5fcd\u6545\u969c\uff0c\u4f46\u5148\u524d\u7684\u5de5\u4f5c\u4ec5\u9650\u4e8e\u77e9\u5f62\u5f62\u72b6\u548c\u5355\u70b9\u6545\u969c\u3002TransforMARS\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5bb9\u9519\u91cd\u6784\u6846\u67b6\uff0c\u53ef\u4ee5\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684MARS\u548c\u591a\u70b9\u6545\u969c\uff0c\u5e76\u786e\u4fdd\u98de\u884c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5148\u524d\u5173\u4e8eMARS\u81ea\u91cd\u6784\u7684\u5de5\u4f5c\u4ec5\u9650\u4e8e\u77e9\u5f62\u5f62\u72b6\u5e76\u5bb9\u5fcd\u5355\u70b9\u6545\u969c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684MARS\u548c\u591a\u70b9\u6545\u969c\uff0c\u540c\u65f6\u786e\u4fdd\u98de\u884c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faTransforMARS\u6846\u67b6\uff0c\u5305\u62ec\u8bc6\u522b\u548c\u6784\u5efa\u6700\u5c0f\u53ef\u63a7\u7ec4\u4ef6\uff08\u5305\u542b\u6545\u969c\u5355\u5143\uff09\uff0c\u4ee5\u53ca\u89c4\u5212\u53ef\u884c\u7684\u62c6\u5378-\u7ec4\u88c5\u5e8f\u5217\u4ee5\u5f62\u6210\u76ee\u6807\u914d\u7f6e\u3002", "result": "TransforMARS\u5728\u6311\u6218\u6027\u7684\u4efb\u610f\u5f62\u72b6MARS\u914d\u7f6e\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5728\u5904\u7406\u591a\u6837\u5316\u914d\u7f6e\u548c\u5bb9\u5fcd\u6545\u969c\u6570\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TransforMARS\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5bb9\u9519\u91cd\u6784\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684MARS\u548c\u591a\u70b9\u6545\u969c\uff0c\u5e76\u786e\u4fdd\u98de\u884c\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u7684\u91cd\u6784\u3002"}}
{"id": "2509.13505", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.13505", "abs": "https://arxiv.org/abs/2509.13505", "authors": ["Jaidev Gill", "Jing Shuang Li"], "title": "Identifying Network Structure of Nonlinear Dynamical Systems: Contraction and Kuramoto Oscillators", "comment": "7 pages, 4 figures, in submission", "summary": "In this work, we study the identifiability of network topologies for\nnetworked nonlinear systems when partial measurements of the nodes are taken.\nWe explore scenarios where different candidate topologies can yield similar\nmeasurements, thus limiting identifiability. To do so, we apply the contraction\ntheory framework to facilitate comparisons between candidate topologies. We\nshow that semicontraction in the observable space is a sufficient condition for\ntwo systems to become indistinguishable from one another based on partial\nmeasurements. We apply this framework to study networks of Kuramoto\noscillators, and discuss scenarios in which different topologies (both\nconnected and disconnected) become indistinguishable.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4ec5\u80fd\u89c2\u6d4b\u5230\u90e8\u5206\u8282\u70b9\u7684\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u5316\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u7f51\u7edc\u62d3\u6251\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u6536\u7f29\u7406\u8bba\u6765\u5206\u6790\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u7684\u53ef\u533a\u5206\u6027\u3002", "motivation": "\u7814\u7a76\u7f51\u7edc\u5316\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u7f51\u7edc\u62d3\u6251\u53ef\u8bc6\u522b\u6027\uff0c\u7279\u522b\u662f\u5f53\u53ea\u80fd\u89c2\u6d4b\u5230\u90e8\u5206\u8282\u70b9\u65f6\uff0c\u53ef\u80fd\u5b58\u5728\u7684\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u4ea7\u751f\u76f8\u4f3c\u89c2\u6d4b\u7ed3\u679c\u7684\u95ee\u9898\u3002", "method": "\u5e94\u7528\u6536\u7f29\u7406\u8bba\u6846\u67b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5728\u53ef\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u51fa\u73b0\u7684\u534a\u6536\u7f29\u73b0\u8c61\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u5224\u65ad\u4e24\u4e2a\u7cfb\u7edf\u57fa\u4e8e\u90e8\u5206\u89c2\u6d4b\u7ed3\u679c\u662f\u5426\u4e0d\u53ef\u533a\u5206\uff08\u5373\u62d3\u6251\u662f\u5426\u53ef\u8bc6\u522b\uff09\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u5e93\u83ab\u7279\u632f\u8361\u5668\u7f51\u7edc\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u540c\u7684\u7f51\u7edc\u62d3\u6251\uff08\u5305\u62ec\u8fde\u901a\u548c\u975e\u8fde\u901a\u7f51\u7edc\uff09\u4f1a\u53d8\u5f97\u4e0d\u53ef\u533a\u5206\u3002", "conclusion": "\u5728\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u7684\u53ef\u8bc6\u522b\u6027\u53d7\u5230\u9650\u5236\uff0c\u534a\u6536\u7f29\u662f\u5224\u65ad\u4e0d\u540c\u62d3\u6251\u7cfb\u7edf\u4e0d\u53ef\u533a\u5206\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u5e93\u83ab\u7279\u632f\u8361\u5668\u7f51\u7edc\u7b49\u5177\u4f53\u573a\u666f\u3002"}}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Chain of Action (CoA)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e00\u4e2a\u5355\u4e00\u7684Vision-Language-Action (VLA)\u6a21\u578b\u4e2d\u7edf\u4e00\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u5728Minecraft\u7b49\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u65f6\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u6311\u6218\u3002CoA\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u4e00\u79cd\u63a8\u7406\u6b65\u9aa4\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u6307\u4ee4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u52a8\u4f5c\u751f\u6210\u7684\u6548\u679c\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aOpenHA\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u5728\u5f00\u53d1\u53ef\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u65f6\uff0c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6709\u6548\u9009\u62e9\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u5c1a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u9996\u5148\u5bf9\u5728\u5f00\u653e\u5f0fMinecraft\u73af\u5883\u4e2d\u7528\u4e8eVision-Language-Action (VLA)\u6216\u5206\u5c42\u667a\u80fd\u4f53\u6a21\u578b\u7684\u62bd\u8c61\u52a8\u4f5c\u7a7a\u95f4\u548c\u5206\u8bcd\u5668\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u3001\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Chain of Action (CoA)\u6846\u67b6\uff0c\u5b83\u5c06\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u7edf\u4e00\u5728\u4e00\u4e2a\u5355\u4e00\u7684VLA\u6a21\u578b\u4e2d\uff0c\u5e76\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u6307\u5bfc\u6700\u7ec8\u53ef\u6267\u884c\u52a8\u4f5c\u751f\u6210\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002\u6211\u4eec\u8fd8\u8bad\u7ec3\u4e86\u4e00\u4e2aAll-in-One\u667a\u80fd\u4f53\uff0c\u8be5\u667a\u80fd\u4f53\u5728CoA\u8303\u5f0f\u4e0b\u6df7\u5408\u4e86\u591a\u79cd\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6ca1\u6709\u5355\u4e00\u7684\u52a8\u4f5c\u7a7a\u95f4\u9002\u7528\u4e8e\u6240\u6709\u60c5\u51b5\uff0c\u6700\u4f18\u9009\u62e9\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u3002CoA\u6846\u67b6\u4f7f\u5f97All-in-One\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u6cdb\u5316\u6027\u7684\u7b56\u7565\uff0c\u5e76\u5728\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u4e13\u4e1a\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86OpenHA\u5957\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u5305\u542b800\u591a\u4e2a\u4e0d\u540c\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u6570\u636e\u96c6\u3001\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002 ", "conclusion": "CoA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u901a\u7528\u667a\u80fd\u4f53\u5728\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u4e0a\u7684\u56f0\u5883\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u89c4\u5212\u548c\u63a7\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002OpenHA\u5957\u4ef6\u7684\u53d1\u5e03\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "J\u00fcrgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u7684\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u51e0\u4f55\u8f6e\u5ed3\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u91cf\u5316\u8bc4\u4f30\u6570\u636e\u8868\u793a\uff08\u5982\u964d\u7ef4\u6280\u672f\uff09\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u8fd8\u80fd\u4f30\u8ba1\u6570\u636e\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u5e76\u5e94\u7528\u4e8e\u7ecf\u9a8c\u7f51\u7edc\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u91cf\u5316\u8bc4\u4f30\u6570\u636e\u8868\u793a\uff08\u5982\u964d\u7ef4\u6280\u672f\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u7ed3\u6784\u3002", "method": "\u5229\u7528\u65b0\u5f00\u53d1\u7684\u62bd\u8c61\u622a\u9762\u66f2\u7387\u6982\u5ff5\uff0c\u6784\u5efa\u57fa\u4e8e\u66f2\u7387\u7684\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u51e0\u4f55\u8f6e\u5ed3\uff0c\u8be5\u66f2\u7387\u6982\u5ff5\u6355\u6349\u4e09\u70b9\u4e0e\u5176\u4ed6\u70b9\u4e4b\u95f4\u7684\u5ea6\u91cf\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u7ed3\u6784\u548c\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u66f2\u7387\u7684\u5206\u6790\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30\u6570\u636e\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u5e76\u4f30\u8ba1\u6570\u636e\u7684\u5185\u5728\u7ef4\u5ea6\u3002"}}
{"id": "2509.13902", "categories": ["cond-mat.mes-hall", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2509.13902", "abs": "https://arxiv.org/abs/2509.13902", "authors": ["Konstantin L. Metlov", "Andrei B. Bogatyr\u00ebv"], "title": "Three-dimensional magnetization textures as quaternionic functions", "comment": "5 pages, 2 figures", "summary": "Thanks to the recent progress in bulk full three-dimensional nanoscale\nmagnetization distribution imaging, there is a growing interest to\nthree-dimensional (3D) magnetization textures, promising new high information\ndensity spintronic applications. Compared to 1D domain walls or 2D magnetic\nvortices/skyrmions, they are a much harder challenge to represent, analyze and\nreason about. In this Letter we build analytical representation for such a\ntextures (with arbitrary number of singularity-free hopfions and singular Bloch\npoint pairs) as products of simple quaternionic functions. It can be useful as\na language for expressing theoretical models of 3D magnetization textures and\nspecifying a variety of topologically non-trivial initial conditions for\nmicromagnetic simulations.", "AI": {"tldr": "\u4e09\u7ef4\u78c1\u5316\u7eb9\u7406\u7684\u5206\u6790\u8868\u793a\u53ef\u7528\u4e8e\u65b0\u7684\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u3002", "motivation": "\u4e0e\u4e00\u7ef4\u7574\u58c1\u6216\u4e8c\u7ef4\u78c1\u6da1/\u65af\u683c\u660e\u5b50\u76f8\u6bd4\uff0c\u4e09\u7ef4\u78c1\u5316\u7eb9\u7406\u66f4\u96be\u8868\u793a\u3001\u5206\u6790\u548c\u63a8\u7406\uff0c\u4f46\u5b83\u4eec\u6709\u671b\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u4e2d\u63d0\u4f9b\u66f4\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u3002", "method": "\u901a\u8fc7\u7b80\u5355\u56db\u5143\u6570\u51fd\u6570\u7684\u4e58\u79ef\u6784\u5efa\u4e09\u7ef4\u78c1\u5316\u7eb9\u7406\uff08\u5177\u6709\u4efb\u610f\u6570\u91cf\u7684\u65e0\u5947\u70b9\u970d\u666e\u592b\u548c\u5947\u70b9\u5e03\u6d1b\u8d6b\u70b9\u5bf9\uff09\u7684\u89e3\u6790\u8868\u793a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u4ee5\u8868\u793a\u4e09\u7ef4\u78c1\u5316\u7eb9\u7406\u7684\u6570\u5b66\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u6a21\u62df\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u6790\u8868\u793a\u53ef\u4f5c\u4e3a\u8868\u8fbe\u4e09\u7ef4\u78c1\u5316\u7eb9\u7406\u7406\u8bba\u6a21\u578b\u548c\u4e3a\u5fae\u78c1\u6a21\u62df\u6307\u5b9a\u62d3\u6251\u975e\u5e73\u51e1\u521d\u59cb\u6761\u4ef6\u7684\u8bed\u8a00\u3002"}}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86CL$^2$GEC\uff0c\u4e00\u4e2a\u9488\u5bf9\u4e2d\u6587\u5b66\u672f\u5199\u4f5c\u7684\u6301\u7eed\u5b66\u4e60\u8bed\u6cd5\u7ea0\u9519\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709CGEC\u7cfb\u7edf\u5728\u591a\u5b66\u79d1\u9002\u5e94\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\uff08CGEC\uff09\u7cfb\u7edf\u5728\u591a\u5b66\u79d1\u5b66\u672f\u5199\u4f5c\u65b9\u9762\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\uff0c\u5e76\u4e14\u5ffd\u89c6\u4e86\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u5904\u7406\u9886\u57df\u7279\u5f02\u6027\u8bed\u8a00\u53d8\u5f02\u548c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,000\u4e2a\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u7684CL$^2$GEC\u57fa\u51c6\uff0c\u6db5\u76d610\u4e2a\u5b66\u79d1\u9886\u57df\u3002\u5728\u6301\u7eed\u5b66\u4e60\u7684\u8bbe\u7f6e\u4e0b\uff0c\u6a21\u62df\u4e86\u5bf9\u4e0d\u540c\u5b66\u672f\u5b66\u79d1\u7684\u987a\u5e8f\u66b4\u9732\uff0c\u5e76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u3001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u4ee5\u53ca\u56db\u79cd\u4ee3\u8868\u6027CL\u7b97\u6cd5\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e86\u6807\u51c6GEC\u6307\u6807\u548c\u9002\u5e94\u4efb\u52a1\u7ea7\u53d8\u5f02\u7684CL\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u91cd\u653e\u6216\u7b80\u5355\u987a\u5e8f\u7684\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u5730\u7f13\u89e3\u9057\u5fd8\u3002", "conclusion": "CL$^2$GEC\u57fa\u51c6\u4e3a\u672a\u6765\u5728\u4e0d\u540c\u5b66\u672f\u9886\u57df\u8fdb\u884c\u81ea\u9002\u5e94\u8bed\u6cd5\u7ea0\u9519\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2509.13410", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.13410", "abs": "https://arxiv.org/abs/2509.13410", "authors": ["Ashwat Jain"], "title": "Symmetry Resolved Multipartite Entanglement Entropy", "comment": "17 pages, 2 figures", "summary": "We perform the symmetry resolution of a multipartite entanglement measure,\nnamely the global entanglement $Q$ introduced by Meyer and Wallach [2002, J. of\nMath. Phys., 43, pp. 4273] for all systems of distinguishable particles hosting\na locally acting symmetry. For an ensemble of Haar random states we find\nagreement with equipartition, with leading order behaviour and finite size\ncorrections which follow a power law scaling with the number of local degrees\nof freedom. Implications of this result for the general symmetry-resolved\nmultipartite entanglement paradigm are discussed and some possible experimental\nverification methods are presented.", "AI": {"tldr": "\u6211\u4eec\u5bf9 Meyer \u548c Wallach \u5f15\u5165\u7684\u591a\u65b9\u7ea0\u7f20\u5ea6\u91cf Q \u8fdb\u884c\u4e86\u5bf9\u79f0\u6027\u89e3\u6790\uff0c\u9002\u7528\u4e8e\u6240\u6709\u5b58\u5728\u5c40\u90e8\u5bf9\u79f0\u6027\u7684\u53ef\u533a\u5206\u7c92\u5b50\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u591a\u65b9\u7ea0\u7f20\u5ea6\u91cf Q \u7684\u5bf9\u79f0\u6027\u89e3\u6790\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u53ef\u533a\u5206\u7c92\u5b50\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5bf9\u5177\u6709\u5c40\u90e8\u5bf9\u79f0\u6027\u7684\u591a\u65b9\u53ef\u533a\u5206\u7c92\u5b50\u7cfb\u7edf\uff0c\u5bf9 Meyer \u548c Wallach \u5f15\u5165\u7684\u5168\u5c40\u7ea0\u7f20\u5ea6\u91cf Q \u8fdb\u884c\u5bf9\u79f0\u6027\u89e3\u6790\u3002\u9488\u5bf9 Haar \u968f\u673a\u72b6\u6001\u7684\u96c6\u5408\uff0c\u8fdb\u884c\u4e86\u8ba1\u7b97\u548c\u5206\u6790\u3002", "result": "\u5728 Haar \u968f\u673a\u72b6\u6001\u7684\u96c6\u5408\u4e2d\uff0c\u53d1\u73b0\u5bf9\u79f0\u6027\u89e3\u6790\u540e\u7684\u7ea0\u7f20\u5ea6\u91cf\u4e0e\u5747\u5206\u72b6\u6001\u4e00\u81f4\uff0c\u5176\u524d\u5bfc\u9636\u6570\u884c\u4e3a\u548c\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u5448\u73b0\u51fa\u4e0e\u5c40\u90e8\u81ea\u7531\u5ea6\u6570\u91cf\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7ed3\u679c\u5bf9\u901a\u7528\u7684\u5bf9\u79f0\u6027\u89e3\u6790\u591a\u65b9\u7ea0\u7f20\u8303\u5f0f\u5177\u6709\u542f\u793a\u610f\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u53ef\u80fd\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2509.13542", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13542", "abs": "https://arxiv.org/abs/2509.13542", "authors": ["Chuangtang Wang", "Xiaoyu Guo", "Zixin Zhai", "Meixin Cheng", "Sang-Wook Cheong", "Adam W. Tsen", "Bing Lv", "Liuyan Zhao"], "title": "Tuning Coupled Toroidic and Polar Orders in a Bilayer Antiferromagnet", "comment": "12 pages, 4 figures", "summary": "Magnetic toroidal order features a loop-like arrangement of magnetic dipole\nmoments, thus breaking both spatial inversion (P) and time-reversal (T)\nsymmetries while preserving their combined PT sym-metry. This PT symmetry\nenables a linear magnetoelectric effect, allowing the coupling between magnetic\ntoroidicity and electric polarity. However, the detection and control of\ntwo-dimensional (2D) magnetic toroidal order and the investigation of its\nlinear magnetoelectric response remain largely unexplored. Here, using bilayer\nCrSBr as a platform, which hosts an in-plane layer-antiferromagnetic (AFM)\norder and simultaneously exhibits a magnetic toroidal order, we show compelling\nevidence for tuning this 2D magnetic toroidicity and its induced electric\npolarity through magnetic-field-depend-ent second harmonic generation (SHG).\nUnder an out-of-plane magnetic field, we decompose the SHG signal into a\ntime-reversal-odd component that scales with the magnetic toroidal moment and a\ntime-reversal-even component that is proportional to the electric polarization.\nWhen sweeping the magnetic field from positive to negative values, we observe\nthat the magnetic toroidicity retains its sign but diminishes in magnitude at\nhigher fields while the electric polarity flips its sign and increases in\nstrength at increasing fields below a critical threshold. When applying an\nin-plane electric field along the N\\'eel vector direction, together with an\nout-of-plane field, we find that the magnetic toroidal and electric polar\ndomains are moved in a locked fashion. These findings underscore the promise of\n2D magnetic toroidal order in realizing giant linear magnetoelectric effects,\nopening exciting possi-bilities for next-generation electronic, magnetic,\noptical, and photonic devices enabled by 2D mag-netoelectrics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u53cc\u5c42CrSBr\u7684\u7814\u7a76\uff0c\u9996\u6b21\u5728\u4e8c\u7ef4\u6750\u6599\u4e2d\u5b9e\u73b0\u4e86\u78c1\u73af\u5f62\u5e8f\u7684\u8c03\u63a7\u53ca\u5176\u7ebf\u6027\u78c1\u7535\u54cd\u5e94\u7684\u63a2\u6d4b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e8c\u7ef4\u78c1\u6027\u6750\u6599\u4e2d\u7684\u78c1\u73af\u5f62\u5e8f\u53ca\u5176\u7ebf\u6027\u78c1\u7535\u6548\u5e94\uff0c\u5e76\u4e3a\u76f8\u5173\u5668\u4ef6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5229\u7528\u78c1\u573a\u4f9d\u8d56\u7684\u4e8c\u6b21\u8c10\u6ce2\u4ea7\u751f\uff08SHG\uff09\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u7814\u7a76\u4e86\u53cc\u5c42CrSBr\u7684\u78c1\u73af\u5f62\u5e8f\u548c\u7535\u6781\u5316\u3002\u901a\u8fc7\u65bd\u52a0\u4e0d\u540c\u65b9\u5411\u7684\u78c1\u573a\u548c\u7535\u573a\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u78c1\u73af\u5f62\u5e8f\u4e0e\u7535\u6781\u5316\u7684\u8026\u5408\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53cc\u5c42CrSBr\u5728\u5c42\u95f4\u53cd\u94c1\u78c1\u5e8f\u7684\u540c\u65f6\u4e5f\u8868\u73b0\u51fa\u78c1\u73af\u5f62\u5e8f\u3002\u901a\u8fc7SHG\u4fe1\u53f7\u53ef\u4ee5\u8c03\u63a7\u5176\u78c1\u73af\u5f62\u5e8f\u548c\u7535\u6781\u5316\uff0c\u5e76\u4e14\u89c2\u5bdf\u5230\u4e86\u78c1\u73af\u5f62\u5e8f\u4e0e\u7535\u6781\u5316\u5728\u78c1\u573a\u548c\u7535\u573a\u4f5c\u7528\u4e0b\u7684\u9501\u5b9a\u79fb\u52a8\u73b0\u8c61\u3002", "conclusion": "\u53cc\u5c42CrSBr\u7684\u78c1\u73af\u5f62\u5e8f\u4e3a\u5b9e\u73b0\u5de8\u5927\u7684\u7ebf\u6027\u78c1\u7535\u6548\u5e94\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u6709\u671b\u5728\u4e0b\u4e00\u4ee3\u7535\u5b50\u3001\u78c1\u6027\u3001\u5149\u5b66\u548c\u5149\u5b50\u5668\u4ef6\u4e2d\u5f97\u5230\u5e94\u7528\u3002"}}
{"id": "2509.14041", "categories": ["cs.AR", "cs.CL", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.14041", "abs": "https://arxiv.org/abs/2509.14041", "authors": ["Henry Kao", "Nikhil Sreekumar", "Prabhdeep Singh Soni", "Ali Sedaghati", "Fang Su", "Bryan Chan", "Maziar Goudarzi", "Reza Azimi"], "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching", "comment": null, "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.", "AI": {"tldr": "TRRIP\u662f\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8bd1\u5668\u5206\u6790\u548c\u8f6c\u6362\u4ee3\u7801\u7684\u201c\u6e29\u5ea6\u201d\uff08\u70ed/\u51b7\uff09\uff0c\u5e76\u5229\u7528\u4ee3\u7801\u9875\u5c5e\u6027\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\u5411\u786c\u4ef6\u63d0\u4f9b\u4ee3\u7801\u6e29\u5ea6\u4fe1\u606f\uff0c\u4ece\u800c\u4f18\u5316\u6307\u4ee4\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u51cf\u5c11\u70ed\u4ee3\u7801\u7684\u6dd8\u6c70\u7387\uff0c\u6700\u7ec8\u964d\u4f4eL2 MPKI\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u79fb\u52a8CPU\u8f6f\u4ef6\u56e0\u5176\u590d\u6742\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u5bfc\u81f4\u6307\u4ee4\u6267\u884c\u91cd\u7528\u95f4\u9694\u957f\uff0c\u7ed9\u4f20\u7edf\u7684\u6307\u4ee4\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u5e26\u6765\u6311\u6218\uff0c\u8fdb\u800c\u5bfc\u81f4CPU\u524d\u7aef\u505c\u6ede\u548c\u8d44\u6e90\u9965\u997f\u3002\u968f\u7740\u79fb\u52a8\u5e94\u7528\u590d\u6742\u5ea6\u548c\u4ee3\u7801\u5360\u7528\u7a7a\u95f4\u589e\u957f\u901f\u5ea6\u8d85\u8fc7\u7247\u4e0a\u5185\u5b58\u589e\u957f\u901f\u5ea6\uff0c\u4f20\u7edf\u7684\u786c\u4ef6\u4e2d\u5fc3\u5316\u6307\u4ee4\u7f13\u5b58\u7ba1\u7406\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u3002", "method": "TRRIP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u3002\u5b83\u5141\u8bb8\u7f16\u8bd1\u5668\u6839\u636e\u6307\u4ee4\u7684\u201c\u6e29\u5ea6\u201d\uff08\u70ed/\u51b7\uff09\u6765\u5206\u6790\u3001\u5206\u7c7b\u548c\u8f6c\u6362\u4ee3\u7801\u3002\u7136\u540e\uff0c\u901a\u8fc7\u4e00\u4e2a\u660e\u786e\u5b9a\u4e49\u7684\u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\uff0c\u5229\u7528\u4ee3\u7801\u9875\u5c5e\u6027\u5411\u786c\u4ef6\u63d0\u4f9b\u4ee3\u7801\u6e29\u5ea6\u4fe1\u606f\u7684\u6458\u8981\u3002TRRIP\u7684\u8f7b\u91cf\u7ea7\u786c\u4ef6\u6269\u5c55\u5229\u7528\u4ee3\u7801\u6e29\u5ea6\u5c5e\u6027\u6765\u4f18\u5316\u6307\u4ee4\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u70ed\u4ee3\u7801\u7684\u6dd8\u6c70\u7387\u3002", "result": "TRRIP\u80fd\u591f\u5c06\u6307\u4ee4\u7684L2 MPKI\u964d\u4f4e26.5%\uff0c\u5728\u5df2\u7ecf\u4f7f\u7528PGO\u4f18\u5316\u8fc7\u7684\u79fb\u52a8\u4ee3\u7801\u4e0a\uff0c\u5b9e\u73b0\u4e863.9%\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "TRRIP\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u53ef\u63a5\u53d7\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5bf9\u8f6f\u4ef6\u548c\u786c\u4ef6\u7ec4\u4ef6\u90fd\u6709\u4e25\u683c\u529f\u80fd\u8981\u6c42\u7684\u79fb\u52a8\u7cfb\u7edf\u3002"}}
{"id": "2509.14095", "categories": ["cs.LO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.14095", "abs": "https://arxiv.org/abs/2509.14095", "authors": ["Ga\u00ebtan Regaud", "Martin Zimmermann"], "title": "The Complexity of Generalized HyperLTL with Stuttering and Contexts", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "We settle the complexity of satisfiability and model-checking for generalized\nHyperLTL with stuttering and contexts, an expressive logic for the\nspecification of asynchronous hyperproperties. Such properties cannot be\nspecified in HyperLTL, as it is restricted to synchronous hyperproperties.\nNevertheless, we prove that satisfiability is $\\Sigma_1^1$-complete and thus\nnot harder than for HyperLTL. On the other hand, we prove that model-checking\nis equivalent to truth in second-order arithmetic, and thus much harder than\nthe decidable HyperLTL model-checking problem. The lower bounds for the\nmodel-checking problem hold even when only allowing stuttering or only allowing\ncontexts.", "AI": {"tldr": "Generalized HyperLTL with stuttering and contexts is $\\Sigma_1^1$-complete for satisfiability and harder for model-checking than HyperLTL.", "motivation": "To settle the complexity of satisfiability and model-checking for generalized HyperLTL with stuttering and contexts, which is an expressive logic for specifying asynchronous hyperproperties, unlike the restricted HyperLTL that only handles synchronous hyperproperties.", "method": "Proving $\\Sigma_1^1$-completeness for satisfiability and establishing an equivalence to truth in second-order arithmetic for model-checking. The lower bounds for model-checking are shown to hold even with only stuttering or only contexts.", "result": "Satisfiability is $\\Sigma_1^1$-complete, and model-checking is equivalent to truth in second-order arithmetic, which is significantly harder than HyperLTL model-checking. Lower bounds for model-checking are established even when considering only stuttering or only contexts.", "conclusion": "The satisfiability of generalized HyperLTL with stuttering and contexts is not harder than for HyperLTL, while its model-checking problem is significantly more complex."}}
{"id": "2509.13608", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13608", "abs": "https://arxiv.org/abs/2509.13608", "authors": ["Niruthiha Selvanayagam", "Ted Kurti"], "title": "Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection", "comment": null, "summary": "As Large Multimodal Models (LMMs) become integral to daily digital life,\nunderstanding their safety architectures is a critical problem for AI\nAlignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a\nglobally deployed model, on the difficult task of multimodal hate speech\ndetection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase\ninvestigation on 500 samples to probe the model's reasoning and failure modes.\nOur central finding is the experimental identification of a \"Unimodal\nBottleneck,\" an architectural flaw where the model's advanced multimodal\nreasoning is systematically preempted by context-blind safety filters. A\nquantitative validation of 144 content policy refusals reveals that these\noverrides are triggered in equal measure by unimodal visual 50% and textual 50%\ncontent. We further demonstrate that this safety system is brittle, blocking\nnot only high-risk imagery but also benign, common meme formats, leading to\npredictable false positives. These findings expose a fundamental tension\nbetween capability and safety in state-of-the-art LMMs, highlighting the need\nfor more integrated, context-aware alignment strategies to ensure AI systems\ncan be deployed both safely and effectively.", "AI": {"tldr": "GPT-4o mini\u5728\u591a\u6a21\u6001\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u5b58\u5728\u201c\u5355\u4e00\u6a21\u6001\u74f6\u9888\u201d\u95ee\u9898\uff0c\u5b89\u5168\u8fc7\u6ee4\u5668\u4f1a\u4e0d\u5206\u9752\u7ea2\u7682\u767d\u5730\u963b\u6b62\u5185\u5bb9\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u65e5\u76ca\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7406\u89e3\u5176\u5b89\u5168\u673a\u5236\u5bf9AI\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790OpenAI\u7684GPT-4o mini\u5728\u591a\u6a21\u6001\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u67b6\u6784\u3002", "method": "\u4f7f\u7528Hateful Memes Challenge\u6570\u636e\u96c6\uff0c\u5bf9500\u4e2a\u6837\u672c\u8fdb\u884c\u591a\u9636\u6bb5\u5206\u6790\uff0c\u7814\u7a76GPT-4o mini\u7684\u63a8\u7406\u548c\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u9a8c\u8bc1144\u4e2a\u5185\u5bb9\u7b56\u7565\u62d2\u7edd\u6765\u8bc4\u4f30\u5176\u5b89\u5168\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u4e86\u4e00\u4e2a\u201c\u5355\u4e00\u6a21\u6001\u74f6\u9888\u201d\uff1a\u6a21\u578b\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u88ab\u7f3a\u4e4f\u8bed\u5883\u7406\u89e3\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u7cfb\u7edf\u6027\u5730\u538b\u5236\u3002\u5b89\u5168\u7cfb\u7edf\u963b\u6b62\u4e86\u9ad8\u98ce\u9669\u5185\u5bb9\uff0c\u4f46\u4e5f\u8bef\u4f24\u4e86\u5927\u91cf\u65e0\u5bb3\u7684\u5e38\u89c1Meme\u683c\u5f0f\uff0c\u5bfc\u81f4\u4e86\u53ef\u9884\u6d4b\u7684\u5047\u9633\u6027\u3002\u62d2\u7edd\u7684\u6848\u4f8b\u4e2d\uff0c50%\u7531\u89c6\u89c9\u5185\u5bb9\u89e6\u53d1\uff0c50%\u7531\u6587\u672c\u5185\u5bb9\u89e6\u53d1\u3002", "conclusion": "GPT-4o mini\u7684\u5b89\u5168\u7cfb\u7edf\u5b58\u5728\u201c\u5355\u4e00\u6a21\u6001\u74f6\u9888\u201d\uff0c\u5373\u5b89\u5168\u8fc7\u6ee4\u5668\u4f1a\u4e0d\u8003\u8651\u8bed\u5883\u5730\u538b\u5236\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u5bf9\u65e0\u5bb3\u5185\u5bb9\u7684\u8bef\u5224\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524dLMM\u5728\u80fd\u529b\u4e0e\u5b89\u5168\u4e4b\u95f4\u7684\u6839\u672c\u6027\u77db\u76fe\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u96c6\u6210\u3001\u66f4\u5177\u8bed\u5883\u611f\u77e5\u80fd\u529b\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u80fd\u591f\u5b89\u5168\u6709\u6548\u5730\u90e8\u7f72\u3002"}}
{"id": "2509.13631", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u5229\u7528FLOWER\u548cRAY\u6846\u67b6\uff0c\u7ed3\u5408YOLOS-small\u3001Faster R-CNN with ResNet50\u4ee5\u53caFaster R-CNN with MobileNetV3\u6a21\u578b\uff0c\u5728\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u536b\u661f\u56fe\u50cf\u7684\u9664orestation\u8bc6\u522b\u548c\u5b9a\u4f4d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u4e00\u4e2a\u533a\u57df\u7684\u5730\u7406\u72b6\u51b5\uff0c\u5fc5\u987b\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u51c6\u786e\u8bc6\u522b\u9664orestation\u3002", "method": "\u672c\u6846\u67b6\u5229\u7528FLOWER\u6846\u67b6\u548cRAY\u6846\u67b6\u6267\u884c\u5206\u5e03\u5f0f\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u3002RAY\u786e\u4fdd\u4e86\u6709\u6548\u7684\u5ba2\u6237\u7aef\u751f\u6210\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u9009\u62e9\u7279\u5b9a\u6570\u91cf\u7684\u7528\u6237\u6765\u521b\u5efa\u4eff\u771f\u73af\u5883\u3002\u672c\u6846\u67b6\u7684FL\u4f7f\u7528YOLOS-small\uff08\u4e00\u79cdVision Transformer\u53d8\u4f53\uff09\u3001\u5177\u6709ResNet50\u9aa8\u5e72\u7684Faster R-CNN\u4ee5\u53ca\u5177\u6709MobileNetV3\u9aa8\u5e72\u7684\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u672c\u65b9\u6cd5\u4e3a\u536b\u661f\u56fe\u50cf\u4e0a\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0d\u540c\u7684\u89c6\u89d2\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u5229\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8bc6\u522b\u548c\u5b9a\u4f4d\u4e0d\u540c\u5ba2\u6237\u7aef\u7684\u9664orestation\u3002FL\u5141\u8bb8\u5206\u5e03\u5f0f\u7f51\u7edc\u5ba2\u6237\u7aef\u5728\u4fdd\u6301\u7528\u6237\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u7684\u540c\u65f6\u534f\u540c\u8bad\u7ec3\u6a21\u578b\u3002\u672c\u6846\u67b6\u7684FL\u4f7f\u7528YOLOS-small\u3001\u5177\u6709ResNet50\u9aa8\u5e72\u7684Faster R-CNN\u4ee5\u53ca\u5177\u6709MobileNetV3\u9aa8\u5e72\u7684\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002"}}
{"id": "2509.13807", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13807", "abs": "https://arxiv.org/abs/2509.13807", "authors": ["Ruiqi Kong", "He Chen"], "title": "Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing", "comment": "5 pages, 5 figures", "summary": "WiFi sensing faces a critical reliability challenge due to hardware-induced\nRF distortions, especially with modern, market-dominant WiFi cards supporting\n802.11ac/ax protocols. These cards employ sensitive automatic gain control and\nseparate RF chains, introducing complex and dynamic distortions that render\nexisting compensation methods ineffective. In this paper, we introduce Domino,\na new framework that transforms channel state information (CSI) into channel\nimpulse response (CIR) and leverages it for precise distortion compensation.\nDomino is built on the key insight that hardware-induced distortions impact all\nsignal paths uniformly, allowing the dominant static path to serve as a\nreliable reference for effective compensation through delay-domain processing.\nReal-world respiration monitoring experiments show that Domino achieves at\nleast 2x higher mean accuracy over existing methods, maintaining robust\nperformance with a median error below 0.24 bpm, even using a single antenna in\nboth direct line-of-sight and obstructed scenarios.", "AI": {"tldr": "WiFi sensing \u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\uff0c\u56e0\u4e3a\u73b0\u4ee3 WiFi \u5361\u4f1a\u4ea7\u751f RF \u4fe1\u53f7\u5931\u771f\u3002Domino \u6846\u67b6\u901a\u8fc7\u5c06 CSI \u8f6c\u6362\u4e3a CIR \u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u5229\u7528\u9759\u6001\u4fe1\u53f7\u8def\u5f84\u6765\u8865\u507f\u5931\u771f\u3002", "motivation": "\u73b0\u4ee3 WiFi \u5361\uff08\u652f\u6301 802.11ac/ax \u534f\u8bae\uff09\u4e2d\u7684\u786c\u4ef6 RF \u5931\u771f\u5bf9 WiFi \u611f\u77e5\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u91c7\u7528\u7684\u81ea\u52a8\u589e\u76ca\u63a7\u5236\u548c\u72ec\u7acb\u7684 RF \u94fe\u4f1a\u4ea7\u751f\u590d\u6742\u4e14\u52a8\u6001\u7684\u5931\u771f\uff0c\u4ece\u800c\u4f7f\u73b0\u6709\u8865\u507f\u65b9\u6cd5\u5931\u6548\u3002", "method": "Domino \u6846\u67b6\u5c06\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u8f6c\u6362\u4e3a\u4fe1\u9053\u51b2\u6fc0\u54cd\u5e94\uff08CIR\uff09\uff0c\u5e76\u5229\u7528\u8fd9\u6837\u4e00\u4e2a\u5173\u952e\u7684\u89c1\u89e3\uff0c\u5373\u786c\u4ef6\u5f15\u8d77\u7684\u5931\u771f\u4f1a\u5747\u5300\u5730\u5f71\u54cd\u6240\u6709\u4fe1\u53f7\u8def\u5f84\u3002\u5b83\u4f7f\u7528\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u9759\u6001\u8def\u5f84\u4f5c\u4e3a\u53ef\u9760\u7684\u53c2\u8003\uff0c\u901a\u8fc7\u5ef6\u8fdf\u57df\u5904\u7406\u8fdb\u884c\u6709\u6548\u7684\u5931\u771f\u8865\u507f\u3002", "result": "\u5728\u5b9e\u9645\u7684\u547c\u5438\u76d1\u6d4b\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cDomino \u7684\u5e73\u5747\u51c6\u786e\u5ea6\u81f3\u5c11\u63d0\u9ad8\u4e86 2 \u500d\u3002\u5728\u5355\u5929\u7ebf\u3001\u89c6\u7ebf\u548c\u906e\u6321\u573a\u666f\u4e0b\uff0c\u5176\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0c\u4e2d\u503c\u8bef\u5dee\u4f4e\u4e8e 0.24 bpm\u3002", "conclusion": "Domino \u6846\u67b6\u80fd\u591f\u6709\u6548\u8865\u507f\u7531\u73b0\u4ee3 WiFi \u5361\u5f15\u8d77\u7684 RF \u5931\u771f\uff0c\u4ece\u800c\u63d0\u9ad8 WiFi \u611f\u77e5\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.13719", "categories": ["eess.SY", "cond-mat.mtrl-sci", "cs.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.13719", "abs": "https://arxiv.org/abs/2509.13719", "authors": ["Chenghao Wan", "Conner Cremers", "Ariana B. H\u00f6felmann", "Zhennan Ru", "Calvin H. Lin", "Kesha N. Tamakuwala", "Dolly Mantle", "Pinak Mohapatra", "Juan Rivas-Davila", "Matthew W. Kanan", "Jonathan A. Fan"], "title": "Scale Up Analysis of Inductively Heated Metamaterial Reactors", "comment": null, "summary": "Inductively heated metamaterial reactors, which utilize an open cell lattice\nbaffle structure as a heating susceptor for magnetic induction, are promising\ncandidates for scaled electrified thermochemical reactor operation due to their\nability to support volumetric heating profiles and enhanced heat transfer\nproperties. In this work, we present a systematic scale up analysis of\ninductive metamaterial reactors where we utilize a combination of analytic\nmodeling, numerical simulations, and experiments to project the capabilities\nand performance of scaled reactors. We use reverse water gas shift as a model\nreaction system and show that for reactor configurations featuring a uniform\nmetamaterial susceptor, the total system efficiency increases with scale.\nHowever, the throughput of these scaled reactors is limited by radial\ntemperature gradients. We further show this bottleneck can be overcome by\ntailoring the radial effective conductivity profile of the susceptor, which can\nenable scaled reactors with nearly ideal plug flow-like capabilities. These\nconcepts provide a pathway towards scaled electrified thermochemical reactors\nwith optimal chemical conversion capabilities.", "AI": {"tldr": "\u611f\u5e94\u52a0\u70ed\u8d85\u6750\u6599\u53cd\u5e94\u5668\u901a\u8fc7\u4f18\u5316\u5f84\u5411\u5bfc\u70ed\u6027\uff0c\u53ef\u5b9e\u73b0\u89c4\u6a21\u5316\u3001\u9ad8\u6548\u7387\u548c\u63a5\u8fd1\u7406\u60f3\u7684\u5316\u5b66\u8f6c\u5316\u3002 ", "motivation": "\u611f\u5e94\u52a0\u70ed\u8d85\u6750\u6599\u53cd\u5e94\u5668\u56e0\u5176\u4f53\u79ef\u52a0\u70ed\u548c\u589e\u5f3a\u4f20\u70ed\u7684\u6f5c\u529b\uff0c\u5728\u89c4\u6a21\u5316\u7535\u6c14\u5316\u70ed\u5316\u5b66\u53cd\u5e94\u5668\u64cd\u4f5c\u4e2d\u5177\u6709\u524d\u666f\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u5176\u89c4\u6a21\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u89e3\u6790\u5efa\u6a21\u3001\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u611f\u5e94\u52a0\u70ed\u8d85\u6750\u6599\u53cd\u5e94\u5668\u5728\u9006\u6c34\u7164\u6c14\u53d8\u6362\u53cd\u5e94\u4e2d\u7684\u89c4\u6a21\u5316\u95ee\u9898\u3002", "result": "\u5728\u5177\u6709\u5747\u5300\u8d85\u6750\u6599\u5438\u70ed\u4f53\u7684\u53cd\u5e94\u5668\u914d\u7f6e\u4e2d\uff0c\u7cfb\u7edf\u6548\u7387\u968f\u89c4\u6a21\u589e\u5927\u800c\u63d0\u9ad8\uff0c\u4f46\u5f84\u5411\u6e29\u5ea6\u68af\u5ea6\u9650\u5236\u4e86\u541e\u5410\u91cf\u3002\u901a\u8fc7\u8c03\u6574\u5438\u70ed\u4f53\u7684\u5f84\u5411\u6709\u6548\u5bfc\u70ed\u6027\uff0c\u53ef\u4ee5\u514b\u670d\u8fd9\u4e00\u74f6\u9888\uff0c\u5b9e\u73b0\u63a5\u8fd1\u7406\u60f3\u7684\u585e\u6d41\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8d85\u6750\u6599\u5438\u70ed\u4f53\u7684\u5f84\u5411\u5bfc\u70ed\u6027\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5177\u6709\u6700\u4f73\u5316\u5b66\u8f6c\u5316\u80fd\u529b\u7684\u89c4\u6a21\u5316\u7535\u6c14\u5316\u70ed\u5316\u5b66\u53cd\u5e94\u5668\u3002"}}
{"id": "2509.13531", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.13531", "abs": "https://arxiv.org/abs/2509.13531", "authors": ["Piotr \u0141aszkiewicz", "Maria Carvalho", "Cl\u00e1udia Soares", "Pedro Louren\u00e7o"], "title": "The impact of modeling approaches on controlling safety-critical, highly perturbed systems: the case for data-driven models", "comment": null, "summary": "This paper evaluates the impact of three system models on the reference\ntrajectory tracking error of the LQR optimal controller, in the challenging\nproblem of guidance and control of the state of a system under strong\nperturbations and reconfiguration. We compared a smooth Linear Time Variant\nsystem learned from data (DD-LTV) with state of the art Linear Time Variant\n(LTV) system identification methods, showing its superiority in the task of\nstate propagation. Moreover, we have found that DD-LTV allows for better\nperformance in terms of trajectory tracking error than the standard solutions\nof a Linear Time Invariant (LTI) system model, and comparable performance to a\nlinearized Linear Time Variant (L-LTV) system model. We tested the three\napproaches on the perturbed and time varying spring-mass-damper systems.", "AI": {"tldr": "DD-LTV\u7cfb\u7edf\u6a21\u578b\u5728\u5f3a\u6270\u52a8\u548c\u91cd\u6784\u7cfb\u7edf\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u4f18\u4e8eLTI\uff0c\u4e0eL-LTV\u76f8\u5f53\uff0c\u5e76\u5728\u72b6\u6001\u4f20\u64ad\u65b9\u9762\u4f18\u4e8eLTV\u7cfb\u7edf\u8fa8\u8bc6\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u4e09\u79cd\u7cfb\u7edf\u6a21\u578b\u5bf9LQR\u6700\u4f18\u63a7\u5236\u5668\u5728\u5f3a\u6270\u52a8\u548c\u91cd\u6784\u7cfb\u7edf\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u5e73\u6ed1\u7ebf\u6027\u65f6\u53d8\uff08DD-LTV\uff09\u7cfb\u7edf\u4e0e\u6700\u5148\u8fdb\u7684\u7ebf\u6027\u65f6\u53d8\uff08LTV\uff09\u7cfb\u7edf\u8fa8\u8bc6\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u4e0e\u7ebf\u6027\u65f6\u4e0d\u53d8\uff08LTI\uff09\u548c\u7ebf\u6027\u5316\u7ebf\u6027\u65f6\u53d8\uff08L-LTV\uff09\u7cfb\u7edf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "DD-LTV\u5728\u72b6\u6001\u4f20\u64ad\u65b9\u9762\u4f18\u4e8eLTV\uff0c\u5728\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u65b9\u9762\u4f18\u4e8eLTI\uff0c\u4e0eL-LTV\u76f8\u5f53\u3002", "conclusion": "DD-LTV\u7cfb\u7edf\u6a21\u578b\u5728\u6270\u52a8\u548c\u65f6\u53d8\u5f39\u7c27-\u8d28\u91cf-\u963b\u5c3c\u5668\u7cfb\u7edf\u7684\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13351", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems.", "AI": {"tldr": "PDDL-Instruct \u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u94fe\u5f0f\u601d\u8003\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86 LLMs \u5728 PDDL \u89c4\u5212\u4e2d\u7684\u80fd\u529b\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u8fbe 94%\u3002", "motivation": "LLMs \u5728\u7ed3\u6784\u5316\u7b26\u53f7\u89c4\u5212\uff08\u5c24\u5176\u662f\u5728 PDDL \u9886\u57df\uff09\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u63d0\u9ad8\u5176\u5f62\u5f0f\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa PDDL-Instruct \u6307\u4ee4\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u94fe\u5f0f\u601d\u8003\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u52a8\u4f5c\u9002\u7528\u6027\u3001\u72b6\u6001\u8f6c\u6362\u548c\u89c4\u5212\u6709\u6548\u6027\u63a8\u7406\uff0c\u5e76\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\u3002", "result": "\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u57fa\u4e8e\u94fe\u5f0f\u601d\u8003\u63a8\u7406\u7684\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u89c4\u5212\u51c6\u786e\u7387\u6700\u9ad8\u8fbe 94%\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u6a21\u578b\u6709 66% \u7684\u7edd\u5bf9\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86 LLMs \u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u81ea\u52a8\u5316\u89c4\u5212\u6240\u9700\u7684\u903b\u8f91\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u66f4\u597d\u7684 AI \u89c4\u5212\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\uff0c\u901a\u8fc7\u5206\u67902013\u5e74\u81f32024\u5e74\u6590\u6d4e\u6960\u8fea\u7684Landsat-8\u536b\u661f\u56fe\u50cf\uff0c\u6bd4\u8f83\u4e86\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u7684\u53d8\u5316\uff0c\u5e76\u91cd\u70b9\u5173\u6ce8\u4e86\u57ce\u5e02\u533a\u57df\u7684\u53d8\u5316\u3002", "motivation": "\u6590\u6d4e\u4f5c\u4e3a\u4e00\u4e2a\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff0c\u6b63\u7ecf\u5386\u5feb\u901f\u7684\u57ce\u5e02\u5316\uff0c\u8fd9\u4f53\u73b0\u5728\u5305\u62ec\u4f4f\u623f\u3001\u9053\u8def\u548c\u5de5\u7a0b\u5efa\u8bbe\u5728\u5185\u7684\u5927\u89c4\u6a21\u5f00\u53d1\u9879\u76ee\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u571f\u5730\u8986\u76d6/\u571f\u5730\u5229\u7528\u5efa\u6a21\u548c\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528Landsat-8\u536b\u661f\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u8c37\u6b4c\u5730\u7403\u5f15\u64ce\uff08Google Earth Engine\uff09\u548c\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\uff08k-means\u805a\u7c7b\uff09\u751f\u6210\u571f\u5730\u8986\u76d6\u56fe\u3002\u7814\u7a76\u4e2d\u8fd8\u4f7f\u7528\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u5bf9\u9009\u5b9a\u533a\u57df\u7684\u571f\u5730\u8986\u76d6\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u751f\u6210\u4e86\u53d8\u5316\u68c0\u6d4b\u7684\u53ef\u89c6\u5316\u56fe\uff0c\u4ee5\u76d1\u6d4b\u57ce\u5e02\u533a\u57df\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u6210\u529f\u751f\u6210\u4e862013\u5e74\u81f32024\u5e74\u95f4\u6590\u6d4e\u6960\u8fea\u7684\u571f\u5730\u8986\u76d6\u56fe\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u6e05\u6670\u5730\u5c55\u793a\u4e86\u57ce\u5e02\u533a\u57df\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5e94\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\uff0c\u4e3a\u6590\u6d4e\u6960\u8fea\u7684\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u57ce\u5e02\u533a\u57df\u7684\u53d8\u5316\u76d1\u6d4b\uff0c\u4e3a\u672a\u6765\u7684\u57ce\u5e02\u89c4\u5212\u548c\u7ba1\u7406\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.14058", "categories": ["cond-mat.mes-hall", "cond-mat.str-el", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2509.14058", "abs": "https://arxiv.org/abs/2509.14058", "authors": ["Fei Tan", "Yuzhu Wang", "Xinghao Wang", "Bo Yang"], "title": "Non-universal Thermal Hall Responses in Fractional Quantum Hall Droplets", "comment": null, "summary": "We analytically compute the thermal Hall conductance (THC) of fractional\nquantum Hall droplets under realistic conditions that go beyond the idealized\nlinear edge theory with conformal symmetry. Specifically, we consider\nfinite-size effects at low temperature, nonzero self-energies of quasiholes,\nand general edge dispersions. We derive measurable corrections in THC that\nalign well with the experimental observables. Although the quantized THC is\ncommonly regarded as a topological invariant that is independent of edge\nconfinement, our results show that this quantization remains robust only for\narbitrary edge dispersion in the thermodynamic limit. Furthermore, the THC\ncontributed by Abelian modes can become extremely sensitive to finite-size\neffects and irregular confining potentials in any realistic experimental\nsystem. In contrast, non-Abelian modes show robust THC signatures under\nperturbations, indicating an intrinsic stability of non-Abelian anyons.", "AI": {"tldr": "\u6ef4,\u6ef4,\u6ef4,\u6ef4", "motivation": "\u6ef4,\u6ef4,\u6ef4,\u6ef4", "method": "\u6ef4,\u6ef4,\u6ef4,\u6ef4", "result": "\u6ef4,\u6ef4,\u6ef4,\u6ef4", "conclusion": "\u6ef4,\u6ef4,\u6ef4,\u6ef4"}}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG\u662f\u4e00\u4e2a\u7528\u4e8e\u53d7\u63a7\u6587\u672c\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6765\u589e\u5f3a\u63a7\u5236\u7cbe\u5ea6\u548c\u590d\u6742\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\uff0c\u7279\u522b\u662f\u5728\u53d7\u63a7\u6587\u672c\u751f\u6210\uff08CTG\uff09\u65b9\u9762\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6761\u4ef6\u63a7\u5236\u4ecd\u7136\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u5728\u5b9e\u9645\u573a\u666f\u548c\u5728\u7ebf\u5e94\u7528\u4e2d\uff0c\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\u3001\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u66f4\u7cbe\u786e\u7684\u63a7\u5236\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6AgentCTG\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\uff0c\u6765\u589e\u5f3a\u6587\u672c\u751f\u6210\u7684\u7cbe\u786e\u5ea6\u548c\u590d\u6742\u63a7\u5236\u80fd\u529b\u3002\u7814\u7a76\u4e86\u4e0d\u540c\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u6a21\u5757\u6765\u63d0\u9ad8\u751f\u6210\u6548\u679c\u3002", "result": "AgentCTG\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002\u5728\u89d2\u8272\u9a71\u52a8\u7684\u6539\u5199\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5185\u5bb9\u4ea4\u4ed8\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5728\u7ebf\u5bfc\u822a\u4e2d\u7684\u9a7e\u9a76\u4f53\u9a8c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u76f8\u5173\u6587\u672c\u7684\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u6c89\u6d78\u611f\u7684\u4ea4\u4e92\uff0c\u63d0\u9ad8\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "AgentCTG\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u63a7\u6587\u672c\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u548c\u590d\u6742\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13423", "categories": ["quant-ph", "cond-mat.str-el", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.13423", "abs": "https://arxiv.org/abs/2509.13423", "authors": ["Ryu Hayakawa", "Kazuki Sakamoto", "Chusei Kiumi"], "title": "Computational complexity of Berry phase estimation in topological phases of matter", "comment": "30 pages", "summary": "The Berry phase is a fundamental quantity in the classification of\ntopological phases of matter. In this paper, we present a new quantum algorithm\nand several complexity-theoretical results for the Berry phase estimation (BPE)\nproblems. Our new quantum algorithm achieves BPE in a more general setting than\npreviously known quantum algorithms, with a theoretical guarantee. For the\ncomplexity-theoretic results, we consider three cases. First, we prove\n$\\mathsf{BQP}$-completeness when we are given a guiding state that has a large\noverlap with the ground state. This result establishes an exponential quantum\nspeedup for estimating the Berry phase. Second, we prove\n$\\mathsf{dUQMA}$-completeness when we have \\textit{a priori} bound for ground\nstate energy. Here, $\\mathsf{dUQMA}$ is a variant of the unique witness version\nof $\\mathsf{QMA}$ (i.e., $\\mathsf{UQMA}$), which we introduce in this paper,\nand this class precisely captures the complexity of BPE without the known\nguiding state. Remarkably, this problem turned out to be the first natural\nproblem contained in both $\\mathsf{UQMA}$ and $\\mathsf{co}$-$\\mathsf{UQMA}$.\nThird, we show $\\mathsf{P}^{\\mathsf{dUQMA[log]}}$-hardness and containment in\n$\\mathsf{P}^{\\mathsf{PGQMA[log]}}$ when we have no additional assumption. These\nresults advance the role of quantum computing in the study of topological\nphases of matter and provide a pathway for clarifying the connection between\ntopological phases of matter and computational complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\u548c\u51e0\u4e2a\u5173\u4e8eBerry\u76f8\u4f4d\u4f30\u8ba1\uff08BPE\uff09\u95ee\u9898\u7684\u590d\u6742\u6027\u7406\u8bba\u7ed3\u679c\u3002\u8be5\u7b97\u6cd5\u5728\u66f4\u4e00\u822c\u7684\u8bbe\u5b9a\u4e0b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684BPE\u3002\u590d\u6742\u6027\u65b9\u9762\uff0c\u5728\u6709\u5bf9\u57fa\u6001\u6709\u5927\u91cd\u53e0\u7684\u5f15\u5bfc\u6001\u65f6\uff0c\u8bc1\u660e\u4e86BQP-\u5b8c\u5907\u6027\uff0c\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u52a0\u901f\uff1b\u5728\u6709\u57fa\u6001\u80fd\u91cf\u5148\u9a8c\u754c\u65f6\uff0c\u8bc1\u660e\u4e86dUQMA-\u5b8c\u5907\u6027\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u540c\u65f6\u5c5e\u4e8eUQMA\u548cco-UQMA\u7684\u81ea\u7136\u95ee\u9898\uff1b\u5728\u65e0\u9644\u52a0\u5047\u8bbe\u65f6\uff0c\u8bc1\u660e\u4e86P^dUQMA[log]-\u786c\u5ea6\u548c\u5305\u542b\u4e8eP^PGQMA[log]\u3002\u8fd9\u4e9b\u7ed3\u679c\u4fc3\u8fdb\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u62d3\u6251\u7269\u6001\u7814\u7a76\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e3a\u9610\u660e\u62d3\u6251\u7269\u6001\u4e0e\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "motivation": "\u62d3\u6251\u7269\u6001\u5206\u7c7b\u7684\u57fa\u672c\u91cf\u662fBerry\u76f8\u4f4d\uff0c\u800cBerry\u76f8\u4f4d\u4f30\u8ba1\uff08BPE\uff09\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4e00\u76f4\u662f\u8ba1\u7b97\u590d\u6742\u6027\u7406\u8bba\u548c\u62d3\u6251\u7269\u6001\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5e76\u4ece\u4e09\u4e2a\u65b9\u9762\u7814\u7a76\u4e86BPE\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff1a1. \u5f53\u5b58\u5728\u4e0e\u57fa\u6001\u6709\u5927\u91cd\u53e0\u7684\u5f15\u5bfc\u6001\u65f6\uff0c\u8bc1\u660eBPE\u95ee\u9898\u662fBQP-\u5b8c\u5168\u7684\u30022. \u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u590d\u6742\u6027\u7c7bdUQMA\uff0c\u5e76\u8bc1\u660e\u5f53\u57fa\u6001\u80fd\u91cf\u6709\u5148\u9a8c\u754c\u65f6\uff0cBPE\u95ee\u9898\u662fdUQMA-\u5b8c\u5168\u7684\u30023. \u5f53\u6ca1\u6709\u9644\u52a0\u5047\u8bbe\u65f6\uff0c\u8bc1\u660eBPE\u95ee\u9898\u662fP^dUQMA[log]-\u786c\u7684\uff0c\u5e76\u4e14\u5305\u542b\u4e8eP^PGQMA[log]\u3002", "result": "1. \u5b58\u5728\u5f15\u5bfc\u6001\u65f6\uff0cBPE\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aBQP-\u5b8c\u5168\u30022. \u57fa\u6001\u80fd\u91cf\u6709\u5148\u9a8c\u754c\u65f6\uff0cBPE\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3adUQMA-\u5b8c\u5168\u30023. \u6ca1\u6709\u9644\u52a0\u5047\u8bbe\u65f6\uff0cBPE\u95ee\u9898\u662fP^dUQMA[log]-\u786c\u7684\uff0c\u5e76\u4e14\u5305\u542b\u4e8eP^PGQMA[log]\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u91cf\u5b50\u7b97\u6cd5\u548c\u590d\u6742\u6027\u7406\u8bba\u7ed3\u679c\uff0c\u4e0d\u4ec5\u4e3aBerry\u76f8\u4f4d\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4fdd\u8bc1\uff0c\u800c\u4e14\u4e3a\u7406\u89e3\u62d3\u6251\u7269\u6001\u4e0e\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u6709\u4ef7\u503c\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.13585", "categories": ["eess.SY", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13585", "abs": "https://arxiv.org/abs/2509.13585", "authors": ["Sean Anderson", "Chris Darken", "Jo\u00e3o Hespanha"], "title": "Zero-sum turn games using Q-learning: finite computation with security guarantees", "comment": "8 pages", "summary": "This paper addresses zero-sum ``turn'' games, in which only one player can\nmake decisions at each state. We show that pure saddle-point state-feedback\npolicies for turn games can be constructed from dynamic programming fixed-point\nequations for a single value function or Q-function. These fixed-points can be\nconstructed using a suitable form of Q-learning. For discounted costs,\nconvergence of this form of Q-learning can be established using classical\ntechniques. For undiscounted costs, we provide a convergence result that\napplies to finite-time deterministic games, which we use to illustrate our\nresults. For complex games, the Q-learning iteration must be terminated before\nexploring the full-state, which can lead to policies that cannot guarantee the\nsecurity levels implied by the final Q-function. To mitigate this, we propose\nan ``opponent-informed'' exploration policy for selecting the Q-learning\nsamples. This form of exploration can guarantee that the final Q-function\nprovides security levels that hold, at least, against a given set of policies.\nA numerical demonstration for a multi-agent game, Atlatl, indicates the\neffectiveness of these methods.", "AI": {"tldr": "\u7eaf\u7b56\u7565\u978d\u70b9\u53cd\u9988\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u4e0d\u52a8\u70b9\u65b9\u7a0b\u4eceQ\u5b66\u4e60\u4e2d\u6784\u5efa\uff0c\u9002\u7528\u4e8e\u96f6\u548c\u8f6c\u6298\u5bf9\u7b56\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u96f6\u548c\u8f6c\u6298\u535a\u5f08\uff08turn games\uff09\u4e2d\u7684\u7eaf\u7b56\u7565\u978d\u70b9\u72b6\u6001\u53cd\u9988\u7b56\u7565\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eQ\u5b66\u4e60\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u4e0d\u52a8\u70b9\u65b9\u7a0b\uff0c\u5e76\u7ed3\u5408Q\u5b66\u4e60\u6765\u6784\u5efa\u7eaf\u7b56\u7565\u978d\u70b9\u53cd\u9988\u7b56\u7565\u3002\u5bf9\u4e8e\u6298\u6263\u6210\u672c\uff0c\u5229\u7528\u7ecf\u5178\u65b9\u6cd5\u8bc1\u660e\u4e86Q\u5b66\u4e60\u7684\u6536\u655b\u6027\u3002\u5bf9\u4e8e\u975e\u6298\u6263\u6210\u672c\uff0c\u5728\u6709\u9650\u65f6\u95f4\u786e\u5b9a\u6027\u535a\u5f08\u4e2d\u8bc1\u660e\u4e86\u6536\u655b\u6027\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5bf9\u624b\u77e5\u60c5\u201d\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3Q\u5b66\u4e60\u5728\u6709\u9650\u6b21\u8fed\u4ee3\u4e2d\u53ef\u80fd\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\u7ea7\u522b\u7684\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684Q\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u7eaf\u7b56\u7565\u978d\u70b9\u53cd\u9988\u7b56\u7565\u3002\u5bf9\u4e8e\u6298\u6263\u6210\u672c\uff0cQ\u5b66\u4e60\u53ef\u4fdd\u8bc1\u6536\u655b\u3002\u5bf9\u4e8e\u975e\u6298\u6263\u6210\u672c\uff0c\u5728\u6709\u9650\u65f6\u95f4\u786e\u5b9a\u6027\u535a\u5f08\u4e2d\u4e5f\u5f97\u5230\u4e86\u6536\u655b\u6027\u8bc1\u660e\u3002\u5bf9\u624b\u77e5\u60c5\u63a2\u7d22\u7b56\u7565\u53ef\u4ee5\u4fdd\u8bc1\u6700\u7ec8Q\u51fd\u6570\u63d0\u4f9b\u7684\u5b89\u5168\u7ea7\u522b\u81f3\u5c11\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u7b56\u7565\u96c6\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8eQ\u5b66\u4e60\u7684\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u4e3a\u96f6\u548c\u8f6c\u6298\u535a\u5f08\u6784\u5efa\u7eaf\u7b56\u7565\u978d\u70b9\u53cd\u9988\u7b56\u7565\uff0c\u5e76\u4e14\u901a\u8fc7\u5bf9\u624b\u77e5\u60c5\u63a2\u7d22\u7b56\u7565\u53ef\u4ee5\u63d0\u9ad8\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13551", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.13551", "abs": "https://arxiv.org/abs/2509.13551", "authors": ["Haonan Wang", "Xilong Xu", "Du Li", "Li Yang"], "title": "Valley-Selective Linear Dichroism and Excitonic Effects in Lieb-Lattice Altermagnets", "comment": "20 pages with 4 figures", "summary": "Altermagnets have recently been recognized as a distinct class of magnetic\nmaterials characterized by alternative spin-split electronic structures without\nnet magnetization. Despite intensive studies on their single-particle\nspintronic and valleytronic properties, many-electron interactions and optical\nresponses of altermagnets remain less explored. In this work, we employ\nmany-body perturbation theory to investigate excited states and their strain\ntunability. Using monolayer Mn2WS4 as a representative candidate, we uncover a\nnovel spin valley-dependent excitonic selection rule in two-dimensional\naltermagnetic Lieb lattices. In addition to strongly bound excitons, we find\nthat linearly polarized light selectively excites valley spin-polarized\nexcitons. Moreover, due to the interplay between altermagnetic spin symmetry\nand electronic orbital character, we predict that applying uniaxial strain can\nlift valley degeneracy and enable the selective excitation of spin-polarized\nexcitons, an effect not achievable in previously studied transition-metal\ndichalcogenides. These spin-valley-locked excitonic states and their strain\ntunability offer a robust mechanism for four-fold symmetric altermagnets to\nencode, store, and read valley/spin information.", "AI": {"tldr": "Altermagnets\u5177\u6709\u72ec\u7279\u7684\u81ea\u65cb\u5206\u88c2\u7535\u5b50\u7ed3\u6784\u4f46\u65e0\u51c0\u78c1\u77e9\u3002\u672c\u6587\u5229\u7528\u591a\u4f53\u5fae\u6270\u7406\u8bba\u7814\u7a76\u4e86\u4ee5Mn2WS4\u4e3a\u4ee3\u8868\u7684\u4e8c\u7ef4\u5f39\u7247\u72b6 Lieb \u6676\u683c\u4e2d\u7684\u6fc0\u5b50\u6027\u8d28\u53ca\u5176\u5e94\u53d8\u53ef\u8c03\u6027\uff0c\u53d1\u73b0\u4e86\u65b0\u9896\u7684\u81ea\u65cb-\u8c37\u4f9d\u8d56\u7684\u6fc0\u5b50\u9009\u62e9\u89c4\u5219\uff0c\u5176\u4e2d\u7ebf\u504f\u632f\u5149\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6fc0\u53d1\u8c37\u81ea\u65cb\u6781\u5316\u6fc0\u5b50\uff0c\u5e76\u4e14\u5355\u8f74\u5e94\u53d8\u53ef\u4ee5\u63d0\u5347\u8c37\u7b80\u5e76\u5ea6\uff0c\u5b9e\u73b0\u5148\u524d\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5316\u7269\u4e2d\u65e0\u6cd5\u5b9e\u73b0\u7684\u81ea\u65cb\u6781\u5316\u6fc0\u5b50\u9009\u62e9\u6027\u6fc0\u53d1\uff0c\u4e3a\u4fe1\u606f\u7f16\u7801\u3001\u5b58\u50a8\u548c\u8bfb\u53d6\u63d0\u4f9b\u4e86\u65b0\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u5f39\u5b50\u4f53\u7684\u5355\u7c92\u5b50\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u8c37\u7535\u5b50\u5b66\u6027\u8d28\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bf9\u5176\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u548c\u5149\u5b66\u54cd\u5e94\u7684\u63a2\u7d22\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u591a\u4f53\u5fae\u6270\u7406\u8bba\u7814\u7a76\u5f39\u5b50\u4f53\u7684\u6fc0\u53d1\u6001\u53ca\u5176\u5e94\u53d8\u53ef\u8c03\u6027\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u4f53\u5fae\u6270\u7406\u8bba\uff08MBP\uff09\u7814\u7a76\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u5355\u5c42Mn2WS4\u7684\u6fc0\u53d1\u6001\u53ca\u5176\u5e94\u53d8\u53ef\u8c03\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u65b0\u9896\u7684\u81ea\u65cb-\u8c37\u4f9d\u8d56\u7684\u6fc0\u5b50\u9009\u62e9\u89c4\u5219\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u7ebf\u504f\u632f\u5149\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6fc0\u53d1\u8c37\u81ea\u65cb\u6781\u5316\u6fc0\u5b50\uff0c\u5e76\u9884\u6d4b\u5355\u8f74\u5e94\u53d8\u53ef\u4ee5\u63d0\u5347\u8c37\u7b80\u5e76\u5ea6\uff0c\u5b9e\u73b0\u81ea\u65cb\u6781\u5316\u6fc0\u5b50\u7684\u9009\u62e9\u6027\u6fc0\u53d1\uff0c\u8fd9\u79cd\u6548\u5e94\u5728\u5148\u524d\u7814\u7a76\u7684\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5316\u7269\u4e2d\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002", "conclusion": "\u8fd9\u4e9b\u81ea\u65cb-\u8c37\u9501\u5b9a\u7684\u6fc0\u5b50\u6001\u53ca\u5176\u5e94\u53d8\u53ef\u8c03\u6027\u4e3a\u56db\u91cd\u5bf9\u79f0\u5f39\u5b50\u4f53\u7f16\u7801\u3001\u5b58\u50a8\u548c\u8bfb\u53d6\u8c37/\u81ea\u65cb\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u673a\u5236\u3002"}}
{"id": "2509.13386", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13386", "abs": "https://arxiv.org/abs/2509.13386", "authors": ["Hansol Lim", "Minhyeok Im", "Jonathan Boyack", "Jee Won Lee", "Jongseong Brad Choi"], "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization", "comment": "This work has been submitted to the 2026 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication", "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles\n(EVs) are increasingly being equipped with powerful computers. This enables\nonboard AI systems to optimize charge-aware path optimization customized to\nreflect vehicle's current condition and environment. We present VEGA, a\ncharge-aware EV navigation agent that plans over a charger-annotated road graph\nusing Proximal Policy Optimization (PPO) with budgeted A* teacher-student\nguidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.\nFirst, a physics-informed neural operator (PINO), trained on real vehicle speed\nand battery-power logs, uses recent vehicle speed logs to estimate aerodynamic\ndrag, rolling resistance, mass, motor and regenerative-braking efficiencies,\nand auxiliary load by learning a vehicle-custom dynamics. Second, a\nReinforcement Learning (RL) agent uses these dynamics to optimize a path with\noptimal charging stops and dwell times under SoC constraints. VEGA requires no\nadditional sensors and uses only vehicle speed signals. It may serve as a\nvirtual sensor for power and efficiency to potentially reduce EV cost. In\nevaluation on long routes like San Francisco to New York, VEGA's stops, dwell\ntimes, SoC management, and total travel time closely track Tesla Trip Planner\nwhile being slightly more conservative, presumably due to real vehicle\nconditions such as vehicle parameter drift due to deterioration. Although\ntrained only in U.S. regions, VEGA was able to compute optimal charge-aware\npaths in France and Japan, demonstrating generalizability. It achieves\npractical integration of physics-informed learning and RL for EV eco-routing.", "AI": {"tldr": "VEGA\u662f\u4e00\u4e2a\u5145\u7535\u611f\u77e5\u7535\u52a8\u6c7d\u8f66\u5bfc\u822a\u4ee3\u7406\uff0c\u5229\u7528PPO\u548cA*\u6307\u5bfc\u6765\u4f18\u5316\u5145\u7535\u8def\u7ebf\u3002", "motivation": "\u968f\u7740\u5bf9\u8f6f\u4ef6\u5b9a\u4e49\u6c7d\u8f66\uff08SDV\uff09\u548c\u8f66\u8f7d\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\uff0c\u6709\u5fc5\u8981\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7684\u5145\u7535\u8def\u7ebf\u89c4\u5212\u3002", "method": "VEGA\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff08PINO\uff09\u7528\u4e8e\u5b66\u4e60\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\uff0c\u5229\u7528\u8fd9\u4e9b\u52a8\u529b\u5b66\u5728\u8003\u8651SOC\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8def\u7ebf\u548c\u5145\u7535\u65f6\u95f4\u3002", "result": "VEGA\u5728\u957f\u9014\u8def\u7ebf\uff08\u5982\u65e7\u91d1\u5c71\u5230\u7ebd\u7ea6\uff09\u4e0a\u7684\u8868\u73b0\u4e0e\u7279\u65af\u62c9\u7684\u884c\u7a0b\u89c4\u5212\u5668\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u6cd5\u56fd\u548c\u65e5\u672c\u7b49\u4e0d\u540c\u5730\u533a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VEGA\u6210\u529f\u5730\u5c06\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u7684\u751f\u6001\u8def\u7ebf\u89c4\u5212\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\uff0c\u4ec5\u4f7f\u7528\u8f66\u8f86\u901f\u5ea6\u4fe1\u53f7\uff0c\u5e76\u6709\u53ef\u80fd\u964d\u4f4e\u7535\u52a8\u6c7d\u8f66\u6210\u672c\u3002"}}
{"id": "2509.13621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13621", "abs": "https://arxiv.org/abs/2509.13621", "authors": ["Antonin Sulc", "Thorsten Hellert", "Steven Hunt"], "title": "Unsupervised Anomaly Detection in ALS EPICS Event Logs", "comment": "6 pages, 5 figures, The 20th International Conference on Accelerator\n  and Large Experimental Physics Control Systems", "summary": "This paper introduces an automated fault analysis framework for the Advanced\nLight Source (ALS) that processes real-time event logs from its EPICS control\nsystem. By treating log entries as natural language, we transform them into\ncontextual vector representations using semantic embedding techniques. A\nsequence-aware neural network, trained on normal operational data, assigns a\nreal-time anomaly score to each event. This method flags deviations from\nbaseline behavior, enabling operators to rapidly identify the critical event\nsequences that precede complex system failures.", "AI": {"tldr": "\u81ea\u52a8\u5316\u6545\u969c\u5206\u6790\u6846\u67b6\u5904\u7406ALS\u7684EPICS\u5b9e\u65f6\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u5c06\u65e5\u5fd7\u8f6c\u5316\u4e3a\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u5f02\u5e38\u4e8b\u4ef6\u5e8f\u5217\uff0c\u4ee5\u5feb\u901f\u5b9a\u4f4d\u590d\u6742\u7cfb\u7edf\u6545\u969c\u3002", "motivation": "\u5728ALS\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5904\u7406EPICS\u63a7\u5236\u7cfb\u7edf\u7684\u5b9e\u65f6\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4ee5\u8bc6\u522b\u5bfc\u81f4\u590d\u6742\u7cfb\u7edf\u6545\u969c\u7684\u5173\u952e\u4e8b\u4ef6\u5e8f\u5217\u3002", "method": "\u5c06\u65e5\u5fd7\u6761\u76ee\u89c6\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u6280\u672f\u5c06\u5176\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\u3002\u8bad\u7ec3\u4e00\u4e2a\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u6765\u5904\u7406\u6b63\u5e38\u8fd0\u884c\u6570\u636e\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4e8b\u4ef6\u5206\u914d\u5b9e\u65f6\u5f02\u5e38\u5206\u6570\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6807\u8bb0\u4e0e\u57fa\u7ebf\u884c\u4e3a\u7684\u504f\u5dee\uff0c\u5e76\u4f7f\u64cd\u4f5c\u5458\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u590d\u6742\u7cfb\u7edf\u6545\u969c\u4e4b\u524d\u7684\u5173\u952e\u4e8b\u4ef6\u5e8f\u5217\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6545\u969c\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5904\u7406ALS\u7684EPICS\u5b9e\u65f6\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u901a\u8fc7\u8bc6\u522b\u5f02\u5e38\u4e8b\u4ef6\u5e8f\u5217\u6765\u5e2e\u52a9\u64cd\u4f5c\u5458\u5feb\u901f\u5b9a\u4f4d\u548c\u89e3\u51b3\u590d\u6742\u7cfb\u7edf\u6545\u969c\u3002"}}
{"id": "2509.13739", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13739", "abs": "https://arxiv.org/abs/2509.13739", "authors": ["Zihou Wu", "Yuecheng Li", "Tianchi Liao", "Jian Lou", "Chuan Chen"], "title": "ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning", "comment": "8 pages, 1 figure", "summary": "Federated learning (FL) faces a critical dilemma: existing protection\nmechanisms like differential privacy (DP) and homomorphic encryption (HE)\nenforce a rigid trade-off, forcing a choice between model utility and\ncomputational efficiency. This lack of flexibility hinders the practical\nimplementation. To address this, we introduce ParaAegis, a parallel protection\nframework designed to give practitioners flexible control over the\nprivacy-utility-efficiency balance. Our core innovation is a strategic model\npartitioning scheme. By applying lightweight DP to the less critical, low norm\nportion of the model while protecting the remainder with HE, we create a\ntunable system. A distributed voting mechanism ensures consensus on this\npartitioning. Theoretical analysis confirms the adjustments between efficiency\nand utility with the same privacy. Crucially, the experimental results\ndemonstrate that by adjusting the hyperparameters, our method enables flexible\nprioritization between model accuracy and training time.", "AI": {"tldr": "ParaAegis\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5206\u533a\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u540c\u6001\u52a0\u5bc6\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u3001\u6a21\u578b\u6548\u7528\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff08\u5982\u5dee\u5206\u9690\u79c1DP\u548c\u540c\u6001\u52a0\u5bc6HE\uff09\u5728\u6a21\u578b\u6548\u7528\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u56fa\u5b9a\u7684\u6743\u8861\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002ParaAegis\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7075\u6d3b\u7684\u4fdd\u62a4\u6846\u67b6\u3002", "method": "ParaAegis\u91c7\u7528\u6a21\u578b\u5206\u533a\u7b56\u7565\uff0c\u5bf9\u6a21\u578b\u4e2d\u8303\u6570\u8f83\u4f4e\u7684\u3001\u4e0d\u90a3\u4e48\u5173\u952e\u7684\u90e8\u5206\u5e94\u7528\u8f7b\u91cf\u7ea7DP\uff0c\u800c\u5bf9\u5176\u4f59\u90e8\u5206\u5e94\u7528HE\u8fdb\u884c\u4fdd\u62a4\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6295\u7968\u673a\u5236\u6765\u5c31\u5206\u533a\u8fbe\u6210\u5171\u8bc6\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6548\u7528\u4e4b\u95f4\u53ef\u8c03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u53ef\u4ee5\u7075\u6d3b\u5730\u4f18\u5148\u8003\u8651\u6a21\u578b\u51c6\u786e\u7387\u6216\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "ParaAegis\u6846\u67b6\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8c03\u7684\u9690\u79c1-\u6548\u7528-\u6548\u7387\u5e73\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u533a\u548c\u6df7\u5408\u52a0\u5bc6\u6280\u672f\uff0c\u6ee1\u8db3\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u9700\u6c42\u3002"}}
{"id": "2509.13822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13822", "abs": "https://arxiv.org/abs/2509.13822", "authors": ["Hao Sun", "Shicong Liu", "Xianghao Yu", "Ying Sun"], "title": "Flow Matching-Based Active Learning for Radio Map Construction with Low-Altitude UAVs", "comment": null, "summary": "The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy\nnecessitates precise and real-time radio maps for reliable communication and\nsafe navigation. However, constructing such maps is hindered by the\ninfeasibility of exhaustive measurements due to UAVs' limited flight endurance.\nTo address this, we propose a novel active learning framework for low-altitude\nradio map construction based on limited measurements. First, a Plug-and-Play\n(PnP)-refined flow matching algorithm is introduced, which leverages flow\nmatching as a powerful generative prior within a PnP scheme to reconstruct\nhigh-fidelity radio maps. Second, the generative nature of flow matching is\nexploited to quantify uncertainty by generating an ensemble of radio maps and\ncomputing the location-wise variance. The resulting uncertainty map guides a\nmulti-objective candidate selection and then a trajectory is planned via\nutility-aware path search (UAPS), directing the UAV to the most informative\nlocations while taking travel costs into account. Simulation results\ndemonstrate that our method significantly outperforms the baselines, achieving\nmore than a 70% reduction in normalized mean squared error (NMSE).", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u6d41\u5339\u914d\u7b97\u6cd5\u8fdb\u884c\u4f4e\u7a7a\u6d4b\u7ed8\uff0c\u4ee5\u89e3\u51b3\u65e0\u4eba\u673a\u7eed\u822a\u9650\u5236\u5bfc\u81f4\u7684\u6d4b\u7ed8\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u9700\u8981\u7cbe\u786e\u7684\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u4f46\u65e0\u4eba\u673a\u7eed\u822a\u9650\u5236\u4e86\u6570\u636e\u91c7\u96c6\u7684\u5168\u9762\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u4f4e\u7a7a\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u6846\u67b6\uff1a1. \u4f7f\u7528\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u6d41\u5339\u914d\u7b97\u6cd5\u751f\u6210\u9ad8\u7cbe\u5ea6\u65e0\u7ebf\u7535\u5730\u56fe\u30022. \u5229\u7528\u6d41\u5339\u914d\u7684\u751f\u6210\u7279\u6027\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5730\u56fe\u5e76\u8ba1\u7b97\u65b9\u5dee\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u30023. \u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5019\u9009\u70b9\u9009\u62e9\u548c\u6548\u7528\u611f\u77e5\u8def\u5f84\u641c\u7d22\uff08UAPS\uff09\u6765\u89c4\u5212\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u4ee5\u6700\u9ad8\u6548\u7684\u65b9\u5f0f\u91c7\u96c6\u6570\u636e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u514b\u670d\u65e0\u4eba\u673a\u7eed\u822a\u9650\u5236\uff0c\u901a\u8fc7\u667a\u80fd\u6570\u636e\u91c7\u96c6\u663e\u8457\u63d0\u9ad8\u4f4e\u7a7a\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u6784\u5efa\u7cbe\u5ea6\u3002"}}
{"id": "2509.13545", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13545", "abs": "https://arxiv.org/abs/2509.13545", "authors": ["Sheng Yu", "Boli Chen", "Imad M. Jaimoukha", "Simos A. Evangelou"], "title": "A Game-Theoretic Predictive Control Framework with Statistical Collision Avoidance Constraints for Autonomous Vehicle Overtaking", "comment": null, "summary": "This work develops a control framework for the autonomous overtaking of\nconnected and automated vehicles (CAVs) in a mixed traffic environment, where\nthe overtaken vehicle is an unconnected but interactive human-driven vehicle.\nThe proposed method, termed the Game-Theoretic, PRedictive Overtaking (GT-PRO)\nstrategy, successfully decouples the longitudinal and lateral vehicle dynamics\nof the CAV and comprehensively coordinates these decoupled dynamics via\ninnovative longitudinal and lateral model predictive (MPC) based controllers,\nrespectively. To address the real-time interactive behavior of the human-driven\novertaken vehicle, a dynamic Stackelberg game-based bilevel optimization is\nsolved by the lateral controller to directly control the CAV lateral motion and\npredict the overtaken vehicle longitudinal responses that are subsequently\nshared with a stochastic MPC that governs the CAV longitudinal motion. The\nproposed strategy exploits a comprehensive real-world dataset, which captures\nhuman driver responses when being overtaken, to tune the game-theoretic lateral\ncontroller according to the most common human responses, and to statistically\ncharacterize human uncertainties and hence implement a collision avoidance\nchance constraint for the stochastic longitudinal controller. The simulation\nresults for both polite and aggressive human response case studies of the\novertaken vehicle demonstrate that the proposed GT-PRO can achieve for this\nrange of human driver responsiveness, safer, more efficient, and more\ncomfortable autonomous overtaking, as compared to existing autonomous\novertaking approaches in the literature. Furthermore, the results suggest that\nthe GT-PRO method is capable of real-time implementation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGT-PRO\uff08\u535a\u5f08\u8bba\u9884\u6d4b\u8d85\u8f66\uff09\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u5b9e\u73b0\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08CAVs\uff09\u7684\u81ea\u4e3b\u8d85\u8f66\uff0c\u5176\u4e2d\u88ab\u8d85\u8f66\u8f86\u662f\u65e0\u8fde\u63a5\u4f46\u53ef\u4ea4\u4e92\u7684\u4eba\u9a7e\u8f66\u8f86\u3002", "motivation": "\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u4ea4\u4e92\u7684\u81ea\u4e3b\u8d85\u8f66\u63a7\u5236\u6846\u67b6\u3002", "method": "GT-PRO\u7b56\u7565\u5c06CAV\u7684\u7eb5\u5411\u548c\u6a2a\u5411\u8f66\u8f86\u52a8\u529b\u5b66\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u63a7\u5236\u5668\u8fdb\u884c\u534f\u8c03\u3002\u6a2a\u5411\u63a7\u5236\u5668\u5229\u7528\u52a8\u6001Stackelberg\u535a\u5f08\u6765\u63a7\u5236CAV\u7684\u6a2a\u5411\u8fd0\u52a8\u5e76\u9884\u6d4b\u88ab\u8d85\u8f66\u8f86\u7684\u7eb5\u5411\u54cd\u5e94\u3002\u7eb5\u5411\u63a7\u5236\u5668\u5219\u662f\u4e00\u4e2a\u968f\u673aMPC\uff0c\u63a5\u6536\u88ab\u8d85\u8f66\u8f86\u7684\u9884\u6d4b\u54cd\u5e94\u3002\u8be5\u7b56\u7565\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u6765\u8c03\u6574\u535a\u5f08\u8bba\u6a2a\u5411\u63a7\u5236\u5668\u5e76\u5b9e\u73b0\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u7684\u793c\u8c8c\u548c\u6fc0\u8fdb\u7684\u4eba\u7c7b\u53cd\u5e94\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cGT-PRO\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "GT-PRO\u7b56\u7565\u80fd\u591f\u5b89\u5168\u3001\u9ad8\u6548\u3001\u8212\u9002\u5730\u5b9e\u73b0\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u5bf9\u4eba\u9a7e\u8f66\u8f86\u7684\u81ea\u4e3b\u8d85\u8f66\uff0c\u5e76\u5177\u5907\u5b9e\u65f6\u5b9e\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Agentic UAVs\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5de5\u5177\u8c03\u7528\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709UAV\u7cfb\u7edf\u81ea\u4e3b\u6027\u6c34\u5e73\u4f4e\uff08SAE Level 2-3\uff09\uff0c\u4f9d\u8d56\u89c4\u5219\u548c\u72ed\u9698AI\uff0c\u7f3a\u4e4f\u5728\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u3001\u60c5\u5883\u611f\u77e5\u63a8\u7406\u3001\u81ea\u4e3b\u51b3\u7b56\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u672a\u4f7f\u7528LLM\u4ee3\u7406\u8fdb\u884c\u5b9e\u65f6\u77e5\u8bc6\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u5c42\u67b6\u6784\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u52a8\u4f5c\u3001\u96c6\u6210\u3001\u5b66\u4e60\uff09\u7684Agentic UAVs\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u63a8\u7406\u3001\u6570\u636e\u5e93\u67e5\u8be2\u548c\u7b2c\u4e09\u65b9\u7cfb\u7edf\u4ea4\u4e92\u3002\u539f\u578b\u7cfb\u7edf\u96c6\u6210\u4e86YOLOv11\u76ee\u6807\u68c0\u6d4b\u3001GPT-4\u63a8\u7406\u548c\u672c\u5730Gemma-3\u90e8\u7f72\uff0c\u5e76\u57fa\u4e8eROS2\u548cGazebo\u8fdb\u884c\u4eff\u771f\u3002", "result": "\u5728\u6a21\u62df\u7684\u641c\u7d22\u6551\u63f4\u573a\u666f\u4e2d\uff0cAgentic UAVs\u76f8\u6bd4\u4f20\u7edf\u7cfb\u7edf\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff080.79 vs 0.72\uff09\uff0c\u63d0\u9ad8\u7684\u4eba\u5458\u68c0\u6d4b\u7387\uff0891% vs 75%\uff09\uff0c\u5e76\u663e\u8457\u589e\u52a0\u4e86\u52a8\u4f5c\u63a8\u8350\uff0892% vs 4.5%\uff09\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cAgentic UAVs\u6846\u67b6\u901a\u8fc7\u5f15\u5165LLM\u4ee3\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6c34\u5e73\u7684\u81ea\u4e3b\u6027\u3001\u6001\u52bf\u611f\u77e5\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\u5916\u6765\u7269\u4f53\uff08FOI\uff09\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u7684\u5916\u6765\u7269\u4f53\uff08FOI\uff09\u7684\u6846\u67b6\uff0c\u5e76\u4f18\u5316\u4ee5\u9002\u5e94\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u7684\u90e8\u7f72\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86YOLOv7\u5206\u5272\u6a21\u578b\u7528\u4e8e\u76ee\u6807\u5b9a\u4f4d\uff0c\u57fa\u4e8e\u4e09\u5143\u7ec4\u635f\u5931\u8bad\u7ec3\u7684ConvNeXt\u7279\u5f81\u63d0\u53d6\u5668\u7528\u4e8e\u751f\u6210\u5224\u522b\u6027\u5d4c\u5165\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7279\u5f81\u8f85\u52a9\u7684IoU\u8ddf\u8e2a\u5668\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\u3002\u8be5\u7cfb\u7edf\u8fd8\u652f\u6301\u901a\u8fc7\u6dfb\u52a0\u65b0\u7269\u4f53\u5d4c\u5165\u5230\u53c2\u8003\u6570\u636e\u5e93\u6765\u5b9e\u73b0\u589e\u91cf\u66f4\u65b0\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002\u6700\u540e\uff0c\u8be5\u6846\u67b6\u9488\u5bf9\u5728NVIDIA Jetson\u8bbe\u5907\u4e0a\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u76d1\u63a7\u548c\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5404\u79cd\u5916\u6765\u7269\u4f53\u5165\u4fb5\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728NVIDIA Jetson\u8bbe\u5907\u4e0a\u7684\u786c\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u5bf9\u4e8e\u5b9e\u9645\u8fb9\u7f18\u5e94\u7528\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u3001\u7279\u5f81\u63d0\u53d6\u548c\u8ddf\u8e2a\u6280\u672f\uff0c\u5e76\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u9c81\u68d2\u5730\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u7684\u5916\u6765\u7269\u4f53\uff0c\u5e76\u652f\u6301\u589e\u91cf\u5b66\u4e60\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.14108", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.14108", "abs": "https://arxiv.org/abs/2509.14108", "authors": ["Piotr Majek", "Ireneusz Weymann"], "title": "Spin-dependent signatures of Majorana modes in thermoelectric transport through double quantum dots", "comment": "12 pages, 7 figures", "summary": "We present a comprehensive theoretical analysis of the spin-dependent\nthermoelectric properties of a double quantum dot system coupled to a\ntopological superconducting nanowire and ferromagnetic leads. The study focuses\non the behavior of the Seebeck coefficient and its spin-resolved counterparts,\nwith calculations performed by means of the numerical renormalization group\nmethod. We investigate the low-temperature transport regime, where a complex\ninterplay between the two-stage Kondo effect, the ferromagnet-induced exchange\nfield, and the Majorana coupling occurs. We demonstrate that thermoelectric\nmeasurements can reveal unique signatures of the Majorana interaction that are\nchallenging to isolate in conductance measurements alone. It is shown that the\nexchange field fundamentally alters the thermoelectric response, leading to a\nrich, non-monotonic temperature evolution of the thermopower, which is driven\nby a temperature-dependent competition between the spin channels. Furthermore,\nwe have identified qualitatively different regimes of spin thermopower\ngeneration, controlled by the interplay between the Majorana-induced asymmetry\nand the spin polarization of the leads. Finally, by connecting the system's\nthermoelectric response to the underlying transport asymmetries quantified by\nthe conductance spin polarization, we provide a consistent and unified physical\npicture, proposing thermoelectric transport as a sensitive probe for Majorana\nsignatures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u503c\u91cd\u6b63\u5316\u7fa4\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u53cc\u91cf\u5b50\u70b9\u7cfb\u7edf\u5728\u62d3\u6251\u8d85\u5bfc\u7eb3\u7c73\u7ebf\u548c\u94c1\u78c1\u6e90/\u6f0f\u4e4b\u95f4\u7684\u81ea\u65cb\u76f8\u5173\u70ed\u7535\u6027\u8d28\uff0c\u7279\u522b\u662f\u585e\u8d1d\u514b\u7cfb\u6570\u3002", "motivation": "\u5728\u4f4e\u6e29\u8f93\u8fd0\u533a\uff0c\u7814\u7a76\u4e86\u7531\u4e24\u7ea7\u8fd1\u85e4\u6548\u5e94\u3001\u94c1\u78c1\u8bf1\u5bfc\u4ea4\u6362\u573a\u548c\u9a6c\u7ea6\u62c9\u7eb3\u8026\u5408\u5f15\u8d77\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u65e8\u5728\u63a2\u7d22\u70ed\u7535\u6d4b\u91cf\u5982\u4f55\u63ed\u793a\u9a6c\u7ea6\u62c9\u7eb3\u76f8\u4e92\u4f5c\u7528\u7684\u72ec\u7279\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u6570\u503c\u91cd\u6b63\u5316\u7fa4\u65b9\u6cd5\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u70ed\u7535\u6d4b\u91cf\u6bd4\u5355\u72ec\u7684\u7535\u5bfc\u6d4b\u91cf\u66f4\u80fd\u63ed\u793a\u9a6c\u7ea6\u62c9\u7eb3\u76f8\u4e92\u4f5c\u7528\u7684\u4fe1\u53f7\u3002\u4ea4\u6362\u573a\u4f1a\u6539\u53d8\u70ed\u7535\u54cd\u5e94\uff0c\u5bfc\u81f4\u70ed\u7535\u52bf\u5448\u73b0\u590d\u6742\u7684\u975e\u5355\u8c03\u6e29\u5ea6\u6f14\u5316\u3002\u53d1\u73b0\u4e86\u7531\u9a6c\u7ea6\u62c9\u7eb3\u8bf1\u5bfc\u7684\u4e0d\u5bf9\u79f0\u6027\u548c\u6e90/\u6f0f\u6781\u7684\u81ea\u65cb\u6781\u5316\u63a7\u5236\u7684\u4e0d\u540c\u81ea\u65cb\u70ed\u7535\u4ea7\u751f\u673a\u5236\u3002", "conclusion": "\u70ed\u7535\u8f93\u8fd0\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u654f\u611f\u7684\u63a2\u6d4b\u9a6c\u7ea6\u62c9\u7eb3\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5c06\u7cfb\u7edf\u7684\u70ed\u7535\u54cd\u5e94\u4e0e\u7531\u7535\u5bfc\u81ea\u65cb\u6781\u5316\u91cf\u5316\u7684\u6f5c\u5728\u8f93\u8fd0\u4e0d\u5bf9\u79f0\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u7269\u7406\u56fe\u50cf\u3002"}}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE\u6846\u67b6\u901a\u8fc7\u539f\u751f\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\uff0c\u89e3\u51b3LLM\u7684\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u5904\u7406\u7ed9\u5b9a\u4fe1\u606f\u65f6\uff0c\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u7b54\u6848\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8981\u4e48\u5728\u4e0d\u4e00\u5b9a\u80fd\u63d0\u9ad8\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u5229\u7528\u7387\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u641c\u7d22\u3002", "method": "\u63d0\u51faCARE\uff08Contextual Answer REtrieval\uff09\u6846\u67b6\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u539f\u751f\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u6559\u4f1aLLM\u5728\u5176\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u5e76\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u68c0\u7d22\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u9700\u8981\u6709\u9650\u7684\u6807\u6ce8\u8bc1\u636e\u6570\u636e\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u94fe\u4e2d\u7b56\u7565\u6027\u5730\u68c0\u7d22\u4e0a\u4e0b\u6587\u6807\u8bb0\u6765\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u751f\u6210\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u53cd\u4e8b\u5b9e\u95ee\u7b54\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCARE\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5927\u5927\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3001\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u548c\u5916\u90e8\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4ee3\u8868\u4e86\u4f7fLLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u3001\u66f4\u9ad8\u6548\u7684\u4e00\u4e2a\u6839\u672c\u6027\u8fdb\u5c55\u3002"}}
{"id": "2509.13440", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.13440", "abs": "https://arxiv.org/abs/2509.13440", "authors": ["Yuan Xue", "Zihan Cheng", "Matteo Ippoliti"], "title": "Simulation of bilayer Hamiltonians based on monitored quantum trajectories", "comment": "13 pages, 4 figures", "summary": "In the study of open quantum systems it is often useful to treat mixed states\nas pure states of a fictitious doubled system. In this work we explore the\nopposite approach: mapping isolated bilayer systems to open monolayer systems.\nSpecifically, we show that arbitrary bilayer Hamiltonians possessing an\nantiunitary layer exchange symmetry, and subject to a constraint on the sign of\ninterlayer couplings, can be mapped to Lindbladians on a monolayer system with\nsome of the jump operators postselected on a fixed outcome (\"monitored\").\nLow-energy states of the bilayer Hamiltonian then correspond to late-time\nstates of the monolayer dynamics. Simulating the latter by quantum trajectory\nmethods has the potential of substantially reducing the computational cost of\nestimating low-energy observables in the bilayer Hamiltonian by effectively\nhalving the system size. The overhead due to sampling quantum trajectories can\nbe controlled by a suitable importance sampling scheme. We show that, when the\nquantum trajectories exhibit free fermion dynamics, our approach reduces to the\nauxiliary field quantum Monte Carlo (AFQMC) method. This provides a physically\ntransparent interpretation of the AFQMC sign-free criteria in terms of\nproperties of quantum dynamics. Finally, we benchmark our approach on the 1D\nquantum Ashkin-Teller model.", "AI": {"tldr": "Bilayer systems can be mapped to monitored monolayer systems, simplifying simulations and offering a new perspective on methods like AFQMC.", "motivation": "To explore an opposite approach to treating mixed states, mapping isolated bilayer systems to open monolayer systems for potentially reduced computational cost.", "method": "Mapping arbitrary bilayer Hamiltonians with antiunitary layer exchange symmetry and constraints on interlayer couplings to Lindbladians on a monolayer system with postselected jump operators. Simulating late-time dynamics of the monolayer system using quantum trajectory methods.", "result": "Low-energy states of the bilayer Hamiltonian correspond to late-time states of the monolayer dynamics. The approach can reduce computational cost by effectively halving the system size. For free fermion dynamics, the approach reduces to AFQMC, providing a physical interpretation of its sign-free criteria.", "conclusion": "The proposed method offers a computationally efficient way to study bilayer systems by mapping them to open monolayer systems, with implications for understanding and interpreting existing simulation methods like AFQMC."}}
{"id": "2509.13643", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13643", "abs": "https://arxiv.org/abs/2509.13643", "authors": ["Jianhua Wang", "Yilin Han", "Shifeng Qian", "Zhenxiang Cheng", "Wenhong Wang", "Zhi-Ming Yu", "Xiaotian Wang"], "title": "Electric-Field Control of Terahertz Response via Spin-Corner-Layer Coupling in Altermagnetic Bilayers", "comment": "7 pages, 5 figures", "summary": "Electric field control of electron charge and spin degrees of freedom is\nfundamental to modern semiconductor and spintronic devices. Yet controlling\nelectromagnetic waves with an electric field, particularly in the terahertz\n(THz) band, remains a challenge. Here, we propose a spin-corner-layer coupling\n(SCLC) mechanism in second-order topological altermagnetic bilayers. By using\nan electric field to influence electrons between different layers, the SCLC\nmechanism enables simultaneous control over corner and spin degrees of freedom,\nthereby allowing electric-field tuning of the absorption, emission intensity,\nand even polarization of THz waves. Taking bilayer NiZrI$_6$ nanodisks as a\nprototype, we demonstrate that an ultralow electrostatic field can switch both\nthe spin and the layer polarizations of corner states. This dual switching\nmodulates transition dipole moments and oscillator strengths between different\ncorner states, thereby enabling the manipulation of THz waves. This study\nestablishes a mechanism for the electric-field control of spin and THz waves\nthrough SCLC, yielding important implications for the advancement of THz\nspintronics.", "AI": {"tldr": "\u5229\u7528\u81ea\u65cb-\u89d2-\u5c42\u8026\u5408\uff08SCLC\uff09\u673a\u5236\uff0c\u901a\u8fc7\u7535\u573a\u8c03\u63a7\u592a\u8d6b\u5179\u6ce2\u7684\u5438\u6536\u3001\u53d1\u5c04\u548c\u6781\u5316\u3002", "motivation": "\u5728\u73b0\u4ee3\u534a\u5bfc\u4f53\u548c\u81ea\u65cb\u7535\u5b50\u5b66\u5668\u4ef6\u4e2d\uff0c\u5229\u7528\u7535\u573a\u63a7\u5236\u7535\u5b50\u7684\u7535\u8377\u548c\u81ea\u65cb\u81ea\u7531\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\uff0c\u5229\u7528\u7535\u573a\u63a7\u5236\u7535\u78c1\u6ce2\uff08\u7279\u522b\u662f\u5728\u592a\u8d6b\u5179\u9891\u6bb5\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e8c\u9636\u62d3\u6251\u4ea4\u53d8\u78c1\u6027\u53cc\u5c42\u7ed3\u6784\u4e2d\u7684\u81ea\u65cb-\u89d2-\u5c42\u8026\u5408\uff08SCLC\uff09\u673a\u5236\uff0c\u5229\u7528\u7535\u573a\u5f71\u54cd\u5c42\u95f4\u7535\u5b50\uff0c\u5b9e\u73b0\u5bf9\u89d2\u548c\u81ea\u65cb\u81ea\u7531\u5ea6\u7684\u534f\u540c\u63a7\u5236\u3002", "result": "\u4ee5\u53cc\u5c42NiZrI$_6$\u7eb3\u7c73\u5706\u76d8\u4e3a\u539f\u578b\uff0c\u8bc1\u660e\u4e86\u8d85\u4f4e\u9759\u7535\u573a\u53ef\u4ee5\u5207\u6362\u89d2\u6001\u7684\u81ea\u65cb\u548c\u5c42\u6781\u5316\uff0c\u8fdb\u800c\u8c03\u5236\u4e86\u4e0d\u540c\u89d2\u6001\u4e4b\u95f4\u7684\u8dc3\u8fc1\u5076\u6781\u77e9\u548c\u632f\u8361\u5668\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u592a\u8d6b\u5179\u6ce2\u7684\u64cd\u63a7\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SCLC\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u7535\u573a\u63a7\u5236\u81ea\u65cb\u548c\u592a\u8d6b\u5179\u6ce2\uff0c\u4e3a\u592a\u8d6b\u5179\u81ea\u65cb\u7535\u5b50\u5b66\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u542f\u793a\u3002"}}
{"id": "2509.13434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13434", "abs": "https://arxiv.org/abs/2509.13434", "authors": ["Wei-Chen Li", "Glen Chou"], "title": "A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies", "comment": null, "summary": "We present a computational framework for simulating filaments interacting\nwith rigid bodies through contact. Filaments are challenging to simulate due to\ntheir codimensionality, i.e., they are one-dimensional structures embedded in\nthree-dimensional space. Existing methods often assume that filaments remain\npermanently attached to rigid bodies. Our framework unifies discrete elastic\nrod (DER) modeling, a pressure field patch contact model, and a convex contact\nformulation to accurately simulate frictional interactions between slender\nfilaments and rigid bodies - capabilities not previously achievable. Owing to\nthe convex formulation of contact, each time step can be solved to global\noptimality, guaranteeing complementarity between contact velocity and impulse.\nWe validate the framework by assessing the accuracy of frictional forces and\ncomparing its physical fidelity against baseline methods. Finally, we\ndemonstrate its applicability in both soft robotics, such as a stochastic\nfilament-based gripper, and deformable object manipulation, such as shoelace\ntying, providing a versatile simulator for systems involving complex\nfilament-filament and filament-rigid body interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u4e86\u79bb\u6563\u5f39\u6027\u6746\u6a21\u578b\u3001\u538b\u529b\u573a\u5757\u63a5\u89e6\u6a21\u578b\u548c\u51f8\u63a5\u89e6\u8868\u8ff0\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u7ec6\u4e1d\u4e0e\u521a\u4f53\u4e4b\u95f4\u7684\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7ec6\u4e1d\u4e0e\u521a\u4f53\u6c38\u4e45\u8fde\u63a5\u7684\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u4f18\u6c42\u89e3\u548c\u4e92\u8865\u6027\u4fdd\u8bc1\uff0c\u5e76\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u7b49\u9886\u57df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u7ec6\u4e1d\u4e0e\u521a\u4f53\u6c38\u4e45\u8fde\u63a5\uff0c\u9650\u5236\u4e86\u6a21\u62df\u7684\u771f\u5b9e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u6a21\u62df\u7ec6\u4e1d\u4e0e\u521a\u4f53\u4e4b\u95f4\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u79bb\u6563\u5f39\u6027\u6746\uff08DER\uff09\u5efa\u6a21\u3001\u538b\u529b\u573a\u5757\u63a5\u89e6\u6a21\u578b\u4ee5\u53ca\u51f8\u63a5\u89e6\u8868\u8ff0\uff0c\u4ee5\u7cbe\u786e\u6a21\u62df\u7ec6\u4e1d\u4e0e\u521a\u4f53\u4e4b\u95f4\u7684\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u6c42\u89e3\uff0c\u4fdd\u8bc1\u63a5\u89e6\u901f\u5ea6\u548c\u51b2\u91cf\u7684\u4e92\u8865\u6027\u3002\u901a\u8fc7\u8bc4\u4f30\u6469\u64e6\u529b\u51c6\u786e\u6027\u548c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u7684\u7269\u7406\u4fdd\u771f\u5ea6\u8fdb\u884c\u9a8c\u8bc1\u3002\u6210\u529f\u5e94\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\uff08\u5982\u57fa\u4e8e\u7ec6\u4e1d\u7684\u968f\u673a\u6293\u624b\uff09\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\uff08\u5982\u978b\u5e26\u6346\u7ed1\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5305\u542b\u590d\u6742\u7ec6\u4e1d-\u7ec6\u4e1d\u548c\u7ec6\u4e1d-\u521a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u6a21\u62df\u5668\uff0c\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.13625", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13625", "abs": "https://arxiv.org/abs/2509.13625", "authors": ["Bishnu Bhusal", "Manoj Acharya", "Ramneet Kaur", "Colin Samplawski", "Anirban Roy", "Adam D. Cobb", "Rohit Chadha", "Susmit Jha"], "title": "Privacy-Aware In-Context Learning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models.The proposed method performs inference\non private records and aggregates the resulting per-token output distributions.\nThis enables the generation of longer and coherent synthetic text while\nmaintaining privacy guarantees. Additionally, we propose a simple blending\noperation that combines private and public inference to further enhance\nutility. Empirical evaluations demonstrate that our approach outperforms\nprevious state-of-the-art methods on in-context-learning (ICL) tasks, making it\na promising direction for privacy-preserving text generation while maintaining\nhigh utility.", "AI": {"tldr": "LLMs\u53ef\u80fd\u66b4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u5fae\u8c03\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u5408\u6210\u6587\u672c\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u63d0\u9ad8\u4e86\u6548\u7528\u3002", "motivation": "LLMs\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u7684\u9690\u79c1\u62c5\u5fe7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6846\u67b6\u7684\u79c1\u6709\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u79c1\u6709\u8bb0\u5f55\u6267\u884c\u63a8\u7406\uff0c\u5e76\u805a\u5408\u751f\u6210\u7684\u6bcf\u4e2atoken\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5171\u63a8\u7406\u7684\u6df7\u5408\u64cd\u4f5c\u6765\u589e\u5f3a\u6548\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\uff0c\u751f\u6210\u4e86\u66f4\u957f\u3001\u66f4\u8fde\u8d2f\u7684\u5408\u6210\u6587\u672c\uff0c\u5e76\u5728ICL\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5728\u4fdd\u6301\u9ad8\u6548\u7528\u7684\u540c\u65f6\u8fdb\u884c\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.13855", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13855", "abs": "https://arxiv.org/abs/2509.13855", "authors": ["Shamsiiat Abdurakhmanova", "Alex Jung"], "title": "Graph-Regularized Learning of Gaussian Mixture Models", "comment": null, "summary": "We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in\ndistributed settings with heterogeneous and limited local data. The method\nexploits a provided similarity graph to guide parameter sharing among nodes,\navoiding the transfer of raw data. The resulting model allows for flexible\naggregation of neighbors' parameters and outperforms both centralized and\nlocally trained GMMs in heterogeneous, low-sample regimes.", "AI": {"tldr": "\u56fe\u6b63\u5219\u5316\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u65b9\u6cd5\uff0c\u5728\u5f02\u6784\u548c\u6709\u9650\u7684\u672c\u5730\u6570\u636e\u4e0b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u56fe\u6307\u5bfc\u53c2\u6570\u5171\u4eab\uff0c\u907f\u514d\u539f\u59cb\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u548c\u672c\u5730\u8bad\u7ec3\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\uff0c\u5728\u6570\u636e\u5f02\u6784\u548c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u56fe\u6b63\u5219\u5316\u5b66\u4e60\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u4f3c\u6027\u56fe\u6307\u5bfc\u53c2\u6570\u5171\u4eab\uff0c\u5e76\u5141\u8bb8\u7075\u6d3b\u805a\u5408\u90bb\u5c45\u7684\u53c2\u6570\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5f02\u6784\u3001\u4f4e\u6837\u672c\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u548c\u672c\u5730\u8bad\u7ec3\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3002", "conclusion": "\u56fe\u6b63\u5219\u5316\u5b66\u4e60\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u662f\u4e00\u79cd\u5728\u5206\u5e03\u5f0f\u3001\u5f02\u6784\u548c\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.13851", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13851", "abs": "https://arxiv.org/abs/2509.13851", "authors": ["Hao Su", "Jiangtao Wang", "Yongchao Wang"], "title": "FFT-Free PAPR Reduction Methods for OFDM Signals", "comment": "6 page, 7 figures", "summary": "In this paper, we propose two low-complexity peak to average power\nratio(PAPR) reduction algorithms for orthogonal frequency division\nmultiplexing(OFDM) signals. The main content is as follows: First, a non-convex\noptimization model is established by minimizing the signal distortion power.\nThen, a customized alternating direction method of multipliers(ADMM) algorithm\nis proposed to solve the problem, named time domain ADMM(T-ADMM) along with an\nimproved version called T-ADMM with constrain update(TCU-ADMM). In the\nalgorithms, all subproblems can be solved analytically, and each iteration has\nlinear computational complexity. These algorithms circumvents the challenges\nposed by repeated fast Fourier transform(FFT) and inverse FFT(IFFT) operations\nin traditional PAPR reduction algorithms. Additionally, we prove that the\nT-ADMM algorithm is theoretically guaranteed convergent if proper parameter is\nchosen. Finally, simulation results demonstrate the effectiveness of the\nproposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6OFDM\u4fe1\u53f7PAPR\u7ea6\u51cf\u7b97\u6cd5", "motivation": "OFDM\u4fe1\u53f7\u5b58\u5728\u8f83\u9ad8\u7684PAPR\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u884c\u7ea6\u51cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u5316\u4fe1\u53f7\u5931\u771f\u529f\u7387\u7684\u975e\u51f8\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u7528\u5b9a\u5236\u7684ADMM\u7b97\u6cd5\uff08T-ADMM\u548cTCU-ADMM\uff09\u6c42\u89e3\u3002\u8be5\u7b97\u6cd5\u53ef\u4ee5\u89e3\u6790\u6c42\u89e3\u5b50\u95ee\u9898\uff0c\u8fed\u4ee3\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027\uff0c\u5e76\u907f\u514d\u4e86\u53cd\u590d\u8fdb\u884cFFT/IFFT\u64cd\u4f5c\u3002", "result": "T-ADMM\u7b97\u6cd5\u5728\u53c2\u6570\u9009\u62e9\u5f97\u5f53\u65f6\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6536\u655b\u3002\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684T-ADMM\u548cTCU-ADMM\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4eOFDM\u4fe1\u53f7\u7684PAPR\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002"}}
{"id": "2509.14126", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14126", "abs": "https://arxiv.org/abs/2509.14126", "authors": ["Viktor Lorentz", "Khaled Wahba", "Sayantan Auddy", "Marc Toussaint", "Wolfgang H\u00f6nig"], "title": "CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Collaborative transportation of cable-suspended payloads by teams of Unmanned\nAerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to\ndifferent payload shapes, and provide built-in compliance, making it attractive\nfor applications ranging from disaster relief to precision logistics. However,\nmulti-UAV coordination under disturbances, nonlinear payload dynamics, and\nslack--taut cable modes remains a challenging control problem. To our\nknowledge, no prior work has addressed these cable mode transitions in the\nmulti-UAV context, instead relying on simplifying rigid-link assumptions. We\npropose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for\nmulti-UAV cable-suspended payload transport. Simulation results demonstrate\nthat the learned policies can outperform classical decentralized controllers in\nterms of disturbance rejection and tracking precision, achieving an 80%\nrecovery rate from harsh conditions compared to 44% for the baseline method. We\nalso achieve successful zero-shot sim-to-real transfer and demonstrate that our\npolicies are highly robust under harsh conditions, including wind, random\nexternal disturbances, and transitions between slack and taut cable dynamics.\nThis work paves the way for autonomous, resilient UAV teams capable of\nexecuting complex payload missions in unstructured environments.", "AI": {"tldr": "CrazyMARL\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591aUAV\u534f\u540c\u8fd0\u8f93\u7ebf\u7f06\u60ac\u6302\u7684\u8d1f\u8f7d\uff0c\u80fd\u591f\u5e94\u5bf9\u5e72\u6270\u3001\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u7ebf\u7f06\u6a21\u5f0f\u8f6c\u6362\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591aUAV\u534f\u540c\u8fd0\u8f93\u7ebf\u7f06\u60ac\u6302\u8d1f\u8f7d\u5177\u6709\u63d0\u5347\u8d1f\u8f7d\u80fd\u529b\u3001\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u8d1f\u8f7d\u548c\u63d0\u4f9b\u5185\u7f6e\u987a\u5e94\u6027\u7b49\u4f18\u52bf\uff0c\u5728\u707e\u96be\u6551\u63f4\u548c\u7cbe\u786e\u7269\u6d41\u7b49\u9886\u57df\u6709\u5e94\u7528\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5728\u5e72\u6270\u3001\u975e\u7ebf\u6027\u8d1f\u8f7d\u52a8\u529b\u5b66\u548c\u7ebf\u7f06\u677e\u7d27\u6a21\u5f0f\u8f6c\u6362\u4e0b\u8fdb\u884c\u591aUAV\u534f\u8c03\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u63a7\u5236\u95ee\u9898\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4f9d\u8d56\u4e8e\u7b80\u5316\u7684\u521a\u6027\u8fde\u6746\u5047\u8bbe\uff0c\u672a\u80fd\u89e3\u51b3\u591aUAV\u573a\u666f\u4e0b\u7684\u7ebf\u7f06\u6a21\u5f0f\u8f6c\u6362\u95ee\u9898\u3002", "method": "\u63d0\u51faCrazyMARL\uff0c\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591aUAV\u534f\u540c\u8fd0\u8f93\u7ebf\u7f06\u60ac\u6302\u8d1f\u8f7d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u5b66\u7684\u7b56\u7565\u5728\u5e72\u6270\u6291\u5236\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u5668\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u7387\u8fbe\u523080%\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4e3a44%\u3002\u5b9e\u73b0\u4e86\u6210\u529f\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\uff0c\u5e76\u8bc1\u660e\u4e86\u7b56\u7565\u5728\u5305\u62ec\u98ce\u3001\u968f\u673a\u5916\u90e8\u5e72\u6270\u4ee5\u53ca\u7ebf\u7f06\u677e\u7d27\u52a8\u529b\u5b66\u8f6c\u6362\u5728\u5185\u7684\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u80fd\u591f\u81ea\u4e3b\u3001\u6709\u5f39\u6027\u5730\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u8d1f\u8f7d\u4efb\u52a1\u7684UAV\u56e2\u961f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13558", "categories": ["eess.SY", "cs.SY", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.13558", "abs": "https://arxiv.org/abs/2509.13558", "authors": ["Saad Rahman", "Doyal Sarker", "Tri Ngo", "Roger Bergua", "Daniel Zalkind", "Jason Jonkman", "Tuhin Das"], "title": "Modeling and Verification of Lumped-Parameter, Multibody Structural Dynamics for Offshore Wind Turbines", "comment": null, "summary": "This paper presents the modeling and verification of multibody structural\ndynamics for offshore wind turbines. The flexible tower and support structure\nof a monopile-based offshore wind turbine are modeled using an acausal,\nlumped-parameter, multibody approach that incorporates structural flexibility,\nsoil-structure interaction, and hydrodynamic models. Simulation results are\nbenchmarked against alternative modeling approaches, demonstrating the model's\nability to accurately capture both static and dynamic behaviors under various\nwind and wave conditions while maintaining computational efficiency. This work\nprovides a valuable tool for analyzing key structural characteristics of wind\nturbines, including eigenfrequencies, mode shapes, damping, and internal\nforces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u6d77\u4e0a\u98ce\u529b\u6da1\u8f6e\u673a\u7684\u591a\u4f53\u7ed3\u6784\u52a8\u529b\u5b66\u5efa\u6a21\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u6d77\u4e0a\u98ce\u529b\u6da1\u8f6e\u673a\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u6a21\u62df\u5176\u9759\u6001\u548c\u52a8\u6001\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u591a\u4f53\u7ed3\u6784\u52a8\u529b\u5b66\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u975e\u56e0\u679c\u3001\u96c6\u603b\u53c2\u6570\u3001\u591a\u4f53\u65b9\u6cd5\u5bf9\u57fa\u4e8e\u5355\u6869\u7684\u6d77\u4e0a\u98ce\u529b\u6da1\u8f6e\u673a\u7684\u67d4\u6027\u5854\u67b6\u548c\u652f\u6491\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u8003\u8651\u4e86\u571f\u58e4-\u7ed3\u6784\u76f8\u4e92\u4f5c\u7528\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u4e0e\u66ff\u4ee3\u5efa\u6a21\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5728\u5404\u79cd\u98ce\u6d6a\u6761\u4ef6\u4e0b\u51c6\u786e\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5206\u6790\u98ce\u529b\u6da1\u8f6e\u673a\u7684\u5173\u952e\u7ed3\u6784\u7279\u6027\uff08\u5305\u62ec\u56fa\u6709\u9891\u7387\u3001\u6a21\u6001\u5f62\u72b6\u3001\u963b\u5c3c\u548c\u5185\u529b\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2509.13357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u878d\u5408\u201d\u7684\u8f7b\u91cf\u7ea7\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u5e76\u884c\u7684\u3001\u6a21\u7cca\u96b6\u5c5e\u7684\u7279\u5f81\u901a\u9053\u6765\u589e\u5f3a Transformer \u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u901a\u9053\u5bf9\u8bcd\u5143\u7ea7\u8bed\u4e49\u8fdb\u884c\u7f16\u7801\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bcd\u5143\u7ea7\u8bed\u4e49\u4fe1\u606f\u6765\u589e\u5f3a Transformer \u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u6587\u672c\u751f\u6210\u3002", "method": "\u8be5\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u8bcd\u5143\u521b\u5efa\u4e00\u4e2a\u7531\u53ef\u89e3\u91ca\u7279\u5f81\u7ec4\u6210\u7684\u5411\u91cf\uff08\u4f8b\u5982\u8bcd\u6027\u3001\u6d45\u5c42\u89d2\u8272\u3001\u8fb9\u754c\u6807\u5fd7\u3001\u60c5\u611f\u6781\u6027\u548c\u5f3a\u5ea6\uff09\uff0c\u8fd9\u4e9b\u7279\u5f81\u7684\u503c\u7531\u53ef\u5fae\u96b6\u5c5e\u51fd\u6570\uff08\u4f8b\u5982\u529f\u7387\u6838\uff09\u751f\u6210\u3002\u8fd9\u4e9b\u5411\u91cf\u5f62\u6210\u4e00\u4e2a\u53e5\u5b50\u7ea7\u7684\u8bed\u4e49\u77e9\u9635\uff0c\u901a\u8fc7\u95e8\u63a7\u9002\u914d\u5668\u878d\u5408\u5230 Transformer \u6a21\u578b\u4e2d\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5305\u62ec\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u8bcd\u5143\u9884\u6d4b\u3001\u4e00\u4e2a\u7528\u4e8e\u4ece\u9690\u85cf\u72b6\u6001\u91cd\u5efa\u8bed\u4e49\u7279\u5f81\u7684\u8f85\u52a9\u635f\u5931\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u89c4\u8303\u5316\u5f62\u5bb9\u8bcd\u7c7b\u522b\u5206\u5e03\u7684\u8f7b\u91cf\u7ea7\u7edf\u4e00\u5668\u3002", "result": "\u5728\u5305\u542b\u7528\u4e8e\u6837\u672c\u5916\uff08OOD\uff09\u63a7\u5236\u7684\u4fdd\u7559\u5f62\u5bb9\u8bcd\u7684\u5408\u6210\u53cc\u4ece\u53e5\u8bed\u6599\u5e93\u4e0a\uff0c\u8bed\u4e49\u878d\u5408\u5728\u56f0\u60d1\u5ea6\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u5b9e\u73b0\u4e86\u6781\u6027\u548c\u6807\u70b9\u7b26\u53f7\u7684\u7cbe\u786e\u3001\u7528\u6237\u53ef\u63a7\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u7b80\u6d01\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u589e\u52a0\u4e86\u5c11\u91cf\u5f00\u9500\uff0c\u4e0e\u7ed1\u5b9a\u7684\u8f93\u5165-\u8f93\u51fa\u5d4c\u5165\u5b8c\u5168\u517c\u5bb9\uff0c\u5e76\u4e3a\u6761\u4ef6\u81ea\u7136\u8bed\u8a00\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u9014\u5f84\u3002"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "EdiVal-Agent\u662f\u4e00\u4e2a\u7528\u4e8e\u6307\u4ee4\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89d2\u548c\u4e13\u5bb6\u5de5\u5177\u96c6\u6765\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6307\u4ee4\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f9d\u8d56\u914d\u5bf9\u53c2\u8003\u56fe\u50cf\u7684\u65b9\u6cd5\u8986\u76d6\u8303\u56f4\u6709\u9650\u4e14\u5b58\u5728\u504f\u89c1\uff1b\u4ec5\u4f9d\u8d56\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u591f\u7cbe\u786e\u3002", "method": "EdiVal-Agent\u901a\u8fc7\u5bf9\u8c61\u5206\u89e3\u3001\u5408\u6210\u7f16\u8f91\u6307\u4ee4\uff0c\u5e76\u96c6\u6210VLMs\u3001\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u5668\u3001\u8bed\u4e49\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u6765\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u5408VLMs\u548c\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u5355\u72ec\u4f7f\u7528VLMs\u6216CLIP\u6307\u6807\u66f4\u80fd\u4e0e\u4eba\u7c7b\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u3002\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u672a\u6765\u5de5\u5177\u7684\u96c6\u6210\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "conclusion": "EdiVal-Agent\u80fd\u591f\u8bc6\u522b\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u8f91\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u4e14\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.14109", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2509.14109", "abs": "https://arxiv.org/abs/2509.14109", "authors": ["Bijay Kumar Sahoo", "Abhiram Soori"], "title": "Field-free transverse Josephson diode effect in altermagnets", "comment": "6+3 pages, 9+1 figures", "summary": "We show that altermagnets (AMs) with Rashba spin--orbit coupling can host a\ntransverse Josephson diode effect (TJDE) without any external magnetic field.\nAMs combine zero net magnetization with spin-polarized Fermi surfaces, enabling\nthe simultaneous breaking of inversion and time-reversal symmetries. We propose\na four-terminal Josephson junction where a longitudinal phase bias between\nopposite superconducting terminals generates transverse supercurrents in the\nunbiased terminals. These transverse currents exhibit both a diode-like\nnonreciprocity and a finite anomalous phase offset, revealing a transverse\nanomalous Josephson effect (AJE). For certain parameter regimes, the transverse\ncurrent becomes unidirectional, and the TJDE efficiency can exceed 1000\\%,\ndemonstrating exceptionally strong diode behavior. Remarkably, the magnitude\nand direction of the TJDE and transverse AJE are tunable by rotating the N\\'eel\nvector. Our results establish altermagnets as a versatile platform for\nengineering field-free nonreciprocal superconducting transport in multiterminal\ndevices.", "AI": {"tldr": "Rashba\u8026\u5408\u7684altermagnets\u53ef\u4ee5\u5728\u6ca1\u6709\u5916\u90e8\u78c1\u573a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a2a\u5411\u7ea6\u745f\u592b\u68ee\u4e8c\u6781\u7ba1\u6548\u5e94(TJDE)\u3002", "motivation": "\u7814\u7a76\u5177\u6709Rashba\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u7684altermagnets\u5982\u4f55\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u78c1\u573a\u5373\u53ef\u5de5\u4f5c\u7684\u6a2a\u5411\u7ea6\u745f\u592b\u68ee\u4e8c\u6781\u7ba1\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\uff0c\u5176\u4e2d\u5bf9\u5411\u8d85\u5bfc\u7aef\u5b50\u4e4b\u95f4\u7684\u7eb5\u5411\u76f8\u4f4d\u504f\u538b\u4f1a\u5728\u65e0\u504f\u538b\u7684\u7aef\u5b50\u4e2d\u4ea7\u751f\u6a2a\u5411\u8d85\u7535\u6d41\uff0c\u8fd9\u79cd\u6a2a\u5411\u7535\u6d41\u8868\u73b0\u51fa\u4e8c\u6781\u7ba1\u72b6\u7684\u4e0d\u53ef\u9006\u6027\u548c\u6709\u9650\u7684\u53cd\u5e38\u76f8\u4f4d\u504f\u79fb\uff0c\u63ed\u793a\u4e86\u6a2a\u5411\u53cd\u5e38\u7ea6\u745f\u592b\u68ee\u6548\u5e94(AJE)\u3002", "result": "\u5728\u67d0\u4e9b\u53c2\u6570\u8303\u56f4\u5185\uff0c\u6a2a\u5411\u7535\u6d41\u53d8\u4e3a\u5355\u5411\u7684\uff0cTJDE\u6548\u7387\u53ef\u4ee5\u8d85\u8fc71000%\uff0c\u8868\u73b0\u51fa\u6781\u5f3a\u7684\u4e8c\u6781\u7ba1\u884c\u4e3a\u3002TJDE\u548c\u6a2a\u5411AJE\u7684\u5927\u5c0f\u548c\u65b9\u5411\u53ef\u4ee5\u901a\u8fc7\u65cb\u8f6cN\u00e9el\u77e2\u91cf\u8fdb\u884c\u8c03\u8282\u3002", "conclusion": "altermagnets\u4e3a\u5728\u591a\u7aef\u5b50\u5668\u4ef6\u4e2d\u5de5\u7a0b\u5316\u65e0\u573a\u4e0d\u53ef\u9006\u8d85\u5bfc\u8f93\u8fd0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "LLMs\u5728\u5904\u7406\u6d89\u53ca\u6570\u5b57\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\u7684\u65e5\u8bed\u53e5\u5b50\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6bd4\u8f83\u6027\u8bed\u8a00\u65f6\u3002\u63d0\u793a\u683c\u5f0f\u548c\u5c11\u6837\u672c\u793a\u4f8b\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\u3002\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u6709\u52a9\u4e8e\u63d0\u9ad8\u6a21\u578b\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u76ee\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5904\u7406\u5305\u542b\u6570\u5b57\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u5982\u65e5\u8bed\uff09\u4e2d\u7684\u6bd4\u8f83\u6027\u63a8\u7406\u65b9\u9762\uff0c\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u6027\u65e5\u8bedNLI\u6570\u636e\u96c6\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u4e0d\u540cLLMs\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u4e2d\u8fd8\u4f7f\u7528\u4e86\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u3002", "result": "LLMs\u5728\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u6027NLI\u65f6\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u5bf9\u63d0\u793a\u683c\u5f0f\u548c\u5c11\u6837\u672c\u793a\u4f8b\u4e2d\u7684\u6807\u7b7e\u654f\u611f\u3002\u6a21\u578b\u96be\u4ee5\u5904\u7406\u65e5\u8bed\u7279\u6709\u7684\u8bed\u8a00\u73b0\u8c61\u3002\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "LLMs\u5728\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u6027NLI\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u5904\u7406\u7279\u5b9a\u8bed\u8a00\u73b0\u8c61\u7684\u80fd\u529b\u3002\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.13451", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13451", "abs": "https://arxiv.org/abs/2509.13451", "authors": ["Arijit Chatterjee", "Sakil Khan", "Sachin Jain", "T S Mahesh"], "title": "Direct Experimental Observation of Quantum Mpemba Effect without Bath Engineering", "comment": null, "summary": "The quantum Mpemba effect refers to the phenomenon of a quantum system in an\ninitial state, far away from equilibrium, relaxing much faster than a state\ncomparatively nearer to equilibrium. We experimentally demonstrate that this\nhighly counterintuitive effect can occur naturally during the thermalization of\nquantum systems. Considering dipolar relaxation as the dominant decoherence\nprocess, we theoretically derive the conditions that can lead to the Mpemba\neffect in nuclear spins. After experimentally preparing nuclear spin states\ndictated by those conditions, we observe the occurrence of the Mpemba effect\nwhen they are left to thermalize without any external control. We also\nexperimentally observe the genuine quantum Mpemba effect during thermalization\nof nuclear spins. Our results establish that both these effects are natural in\nthermalization of quantum systems, and may show up without the need for any\nbath engineering.", "AI": {"tldr": "\u91cf\u5b50\u7cfb\u7edf\u6bd4\u63a5\u8fd1\u5e73\u8861\u72b6\u6001\u7684\u7cfb\u7edf\u66f4\u5feb\u5730\u8fbe\u5230\u70ed\u5e73\u8861\uff0c\u8fd9\u88ab\u79f0\u4e3a\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u9a8c\u8bc1\u660e\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\u5728\u91cf\u5b50\u7cfb\u7edf\u70ed\u5316\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u81ea\u7136\u53d1\u751f\u3002", "method": "\u901a\u8fc7\u8003\u8651\u5076\u6781\u5f1b\u8c6b\u4f5c\u4e3a\u4e3b\u5bfc\u9000\u76f8\u5e72\u8fc7\u7a0b\uff0c\u7406\u8bba\u63a8\u5bfc\u51fa\u6838\u81ea\u65cb\u4e2d\u51fa\u73b0\u59c6\u6f58\u5df4\u6548\u5e94\u7684\u6761\u4ef6\u3002\u7136\u540e\uff0c\u5728\u5b9e\u9a8c\u4e0a\u5236\u5907\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\u7684\u6838\u81ea\u65cb\u6001\uff0c\u5e76\u89c2\u5bdf\u5b83\u4eec\u5728\u65e0\u4eba\u4e3a\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u70ed\u5e73\u8861\u7684\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u89c2\u5bdf\u5230\u4e86\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\uff0c\u5e76\u4e14\u5728\u6838\u81ea\u65cb\u70ed\u5316\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u4e86\u771f\u6b63\u7684\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\uff08\u5305\u62ec\u771f\u6b63\u7684\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u5e94\uff09\u662f\u91cf\u5b50\u7cfb\u7edf\u70ed\u5316\u8fc7\u7a0b\u4e2d\u7684\u81ea\u7136\u73b0\u8c61\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u65e0\u9700\u4efb\u4f55\u6d74\u8c03\u63a7\u7684\u60c5\u51b5\u4e0b\u51fa\u73b0\u3002"}}
{"id": "2509.13693", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13693", "abs": "https://arxiv.org/abs/2509.13693", "authors": ["Abdulsalam Aji Suleiman", "Amir Parsi", "Hafiz Muhammad Shakir", "Hamid Reza Rasouli", "Doruk Pehlivano\u011flu", "Talip Serkan Kas\u0131rga"], "title": "Synthesis of Ultra-thin Potassium Tungsten Bronze Single Crystals with Optically Contrasting Domains and Resistive Switching", "comment": null, "summary": "Potassium tungsten bronzes (K$_x$WO$_3$) are nonstoichiometric oxides in\nwhich alkali ions, i.e., K+, occupy one-dimensional tunnels of the hexagonal\nWO6 framework, enabling coupled ionic-lectronic transport. While their bulk and\nnanostructured forms have been studied extensively, controlled synthesis of\nsingle-crystalline mesoscale samples suitable for device fabrication has\nremained limited. Here, we report a solid-liquid-solid (SLS) growth strategy\nthat yields high-quality K$_x$WO$_3$ nanobelts with thicknesses down to ~36 nm\nand lateral sizes exceeding 100 um. The crystals display sharp optical domains\narising from local variations in potassium occupancy, as confirmed by spatially\nresolved Raman spectroscopy and electron diffraction. Under applied bias, these\ndomains vanish irreversibly, consistent with lateral redistribution of K+ ions\nalong the tunnels. Two-terminal devices fabricated from individual nanobelts\nexhibit reproducible bipolar switching with resistance ratios of 10-30,\ncharacteristic short-term and long-term plasticity under pulsed excitation, and\nswitching energies of ~25 nJ. These results establish K$_x$WO$_3$ as a model\ntunnel-structured oxide for studying electric-field-driven alkali-ion\nmigration, while also highlighting its potential for stable, analog resistive\nswitching and iontronic memory applications.", "AI": {"tldr": "\u672c\u6587\u62a5\u9053\u4e86\u4e00\u79cd\u56fa-\u6db2-\u56fa\uff08SLS\uff09\u751f\u957f\u7b56\u7565\uff0c\u6210\u529f\u5236\u5907\u4e86\u9ad8\u8d28\u91cf\u7684\u94be\u94a8\u9752\u94dc\uff08K$_x$WO$_3$\uff09\u7eb3\u7c73\u5e26\uff0c\u539a\u5ea6\u53ef\u8fbe\u7ea636\u7eb3\u7c73\uff0c\u6a2a\u5411\u5c3a\u5bf8\u8d85\u8fc7100\u5fae\u7c73\u3002", "motivation": "\u4e3a\u4e86\u53ef\u63a7\u5408\u6210\u9002\u5408\u5668\u4ef6\u5236\u9020\u7684\u5355\u6676\u4ecb\u89c2\u5c3a\u5bf8\u94be\u94a8\u9752\u94dc\uff08K$_x$WO$_3$\uff09\u6837\u54c1\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u7814\u7a76\u4e2d\u5bf9\u6b64\u7c7b\u6750\u6599\u5408\u6210\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u56fa-\u6db2-\u56fa\uff08SLS\uff09\u751f\u957f\u7b56\u7565\u5236\u5907K$_x$WO$_3$\u7eb3\u7c73\u5e26\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u5206\u8fa8\u62c9\u66fc\u5149\u8c31\u548c\u7535\u5b50\u884d\u5c04\u5206\u6790\u4e86\u5176\u94be\u79bb\u5b50\u5360\u636e\u60c5\u51b5\u3002", "result": "\u5236\u5907\u7684K$_x$WO$_3$\u7eb3\u7c73\u5e26\u8868\u73b0\u51fa\u7531\u94be\u79bb\u5b50\u5360\u636e\u60c5\u51b5\u5c40\u90e8\u53d8\u5316\u5f15\u8d77\u7684\u5149\u5b66\u7574\uff0c\u5916\u52a0\u504f\u538b\u540e\u8fd9\u4e9b\u5149\u5b66\u7574\u4e0d\u53ef\u9006\u5730\u6d88\u5931\uff0c\u8868\u660e\u94be\u79bb\u5b50\u6cbf\u901a\u9053\u91cd\u65b0\u5206\u5e03\u3002\u57fa\u4e8e\u5355\u4e2a\u7eb3\u7c73\u5e26\u5236\u5907\u7684\u53cc\u7aef\u5668\u4ef6\u8868\u73b0\u51fa\u53ef\u91cd\u590d\u7684\u53cc\u6781\u5f00\u5173\u884c\u4e3a\uff0c\u7535\u963b\u6bd4\u4e3a10-30\uff0c\u5e76\u5728\u8109\u51b2\u6fc0\u53d1\u4e0b\u8868\u73b0\u51fa\u7279\u6027\u77ed\u65f6\u548c\u957f\u65f6\u53ef\u5851\u6027\uff0c\u5f00\u5173\u80fd\u91cf\u7ea6\u4e3a25 nJ\u3002", "conclusion": "K$_x$WO$_3$\u53ef\u4f5c\u4e3a\u7814\u7a76\u7535\u573a\u9a71\u52a8\u7684\u78b1\u91d1\u5c5e\u79bb\u5b50\u8fc1\u79fb\u7684\u6a21\u578b\u6750\u6599\uff0c\u5e76\u5728\u7a33\u5b9a\u3001\u6a21\u62df\u7535\u963b\u5f00\u5173\u548c\u79bb\u5b50\u7535\u5b50\u5b58\u50a8\u5668\u5e94\u7528\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2509.13501", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13501", "abs": "https://arxiv.org/abs/2509.13501", "authors": ["Hossein Gholampour", "Logan E. Beaver"], "title": "Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume", "comment": null, "summary": "Many robotic systems must follow planned paths yet pause safely and resume\nwhen people or objects intervene. We present an output-space method for systems\nwhose tracked output can be feedback-linearized to a double integrator (e.g.,\nmanipulators). The approach has two parts. Offline, we perform a pre-run\nreachability check to verify that the motion plan respects speed and\nacceleration magnitude limits. Online, we apply a quadratic program to track\nthe motion plan under the same limits. We use a one-step reachability test to\nbound the maximum disturbance the system is capable of rejecting. When the\nstate coincides with the reference path we recover perfect tracking in the\ndeterministic case, and we correct errors using a KKT-inspired weight. We\ndemonstrate that safety stops and unplanned deviations are handled efficiently,\nand the system returns to the motion plan without replanning. We demonstrate\nour system's improved performance over pure pursuit in simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\u5b89\u5168\u6682\u505c\u548c\u6062\u590d\u7684\u8f93\u51fa\u7a7a\u95f4\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u5148\u8fdb\u884c\u53ef\u8fbe\u6027\u68c0\u67e5\u548c\u5728\u7ebf\u4e8c\u6b21\u89c4\u5212\u6765\u5b9e\u73b0\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8ddf\u968f\u89c4\u5212\u8def\u5f84\u65f6\uff0c\u9700\u8981\u80fd\u591f\u5b89\u5168\u5730\u6682\u505c\u4ee5\u54cd\u5e94\u4eba\u5458\u6216\u7269\u4f53\u7684\u5e72\u9884\uff0c\u5e76\u5728\u5e72\u9884\u7ed3\u675f\u540e\u6062\u590d\u6267\u884c\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u4e3a\u79bb\u7ebf\u548c\u5728\u7ebf\u4e24\u90e8\u5206\u3002\u79bb\u7ebf\u90e8\u5206\u8fdb\u884c\u9884\u8fd0\u884c\u53ef\u8fbe\u6027\u68c0\u67e5\uff0c\u9a8c\u8bc1\u8fd0\u52a8\u89c4\u5212\u662f\u5426\u6ee1\u8db3\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u9650\u5236\u3002\u5728\u7ebf\u90e8\u5206\u4f7f\u7528\u4e8c\u6b21\u89c4\u5212\u6765\u8ddf\u8e2a\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u5e94\u7528\u540c\u4e00\u9650\u5236\u3002\u901a\u8fc7\u4e00\u6b65\u53ef\u8fbe\u6027\u6d4b\u8bd5\u6765\u754c\u5b9a\u7cfb\u7edf\u80fd\u591f\u62d2\u7edd\u7684\u6700\u5927\u5e72\u6270\u3002\u5728\u72b6\u6001\u4e0e\u53c2\u8003\u8def\u5f84\u91cd\u5408\u65f6\u6062\u590d\u7cbe\u786e\u8ddf\u8e2a\uff0c\u5e76\u4f7f\u7528 KKT \u6539\u8fdb\u7684\u6743\u91cd\u6765\u7ea0\u6b63\u8bef\u5dee\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u5904\u7406\u5b89\u5168\u505c\u6b62\u548c\u610f\u5916\u504f\u79bb\uff0c\u5e76\u5728\u4e0d\u91cd\u65b0\u89c4\u5212\u7684\u60c5\u51b5\u4e0b\u8fd4\u56de\u5230\u8fd0\u52a8\u89c4\u5212\u3002\u4e0e\u7eaf\u7cb9\u8ddf\u8e2a\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f93\u51fa\u7a7a\u95f4\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6682\u505c\u548c\u6062\u590d\uff0c\u5e76\u4fdd\u6301\u5bf9\u8fd0\u52a8\u89c4\u5212\u7684\u7cbe\u786e\u8ddf\u8e2a\u3002"}}
{"id": "2509.13633", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13633", "abs": "https://arxiv.org/abs/2509.13633", "authors": ["Jeremy Oon", "Rakhi Manohar Mepparambath", "Ling Feng"], "title": "DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis", "comment": null, "summary": "Despite the significant progress of deep learning models in multitude of\napplications, their adaption in planning and policy related areas remains\nchallenging due to the black-box nature of these models. In this work, we\ndevelop a set of DeepLogit models that follow a novel sequentially constrained\napproach in estimating deep learning models for transport policy analysis. In\nthe first step of the proposed approach, we estimate a convolutional neural\nnetwork (CNN) model with only linear terms, which is equivalent of a\nlinear-in-parameter multinomial logit model. We then estimate other deep\nlearning models by constraining the parameters that need interpretability at\nthe values obtained in the linear-in-parameter CNN model and including higher\norder terms or by introducing advanced deep learning architectures like\nTransformers. Our approach can retain the interpretability of the selected\nparameters, yet provides significantly improved model accuracy than the\ndiscrete choice model. We demonstrate our approach on a transit route choice\nexample using real-world transit smart card data from Singapore. This study\nshows the potential for a unifying approach, where theory-based discrete choice\nmodel (DCM) and data-driven AI models can leverage each other's strengths in\ninterpretability and predictive power. With the availability of larger datasets\nand more complex constructions, such approach can lead to more accurate models\nusing discrete choice models while maintaining its applicability in planning\nand policy-related areas. Our code is available on\nhttps://github.com/jeremyoon/route-choice/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepLogit\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u987a\u5e8f\u7ea6\u675f\u65b9\u6cd5\u6765\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4ea4\u901a\u653f\u7b56\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4f30\u8ba1\u4e00\u4e2a\u7ebf\u6027CNN\u6a21\u578b\uff0c\u7136\u540e\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u9ad8\u9636\u9879\u6216Transformer\u7b49\u9ad8\u7ea7\u67b6\u6784\uff0c\u4ece\u800c\u5728\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u90e8\u5206\u53c2\u6570\u7684\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u65b0\u52a0\u5761\u7684\u516c\u4ea4\u8def\u7ebf\u9009\u62e9\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u7ed3\u5408\u4f20\u7edf\u79bb\u6563\u9009\u62e9\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u4e3a\u4ea4\u901a\u89c4\u5212\u548c\u653f\u7b56\u5206\u6790\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4ea4\u901a\u653f\u7b56\u5206\u6790\u7b49\u9886\u57df\u5e94\u7528\u53d7\u9650\uff0c\u56e0\u5176\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u63d0\u51faDeepLogit\u6a21\u578b\uff0c\u91c7\u7528\u987a\u5e8f\u7ea6\u675f\u65b9\u6cd5\uff1a1. \u4f30\u8ba1\u4e00\u4e2a\u4ec5\u542b\u7ebf\u6027\u9879\u7684CNN\u6a21\u578b\uff08\u7b49\u540c\u4e8e\u7ebf\u6027MNL\u6a21\u578b\uff09\uff1b2. \u7ea6\u675f\u8be5\u7ebf\u6027\u6a21\u578b\u4e2d\u7684\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u9ad8\u9636\u9879\u6216Transformer\u7b49\u9ad8\u7ea7\u67b6\u6784\u6765\u4f30\u8ba1\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u65b0\u52a0\u5761\u516c\u4ea4\u8def\u7ebf\u9009\u62e9\u6848\u4f8b\u4e2d\uff0cDeepLogit\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u90e8\u5206\u53c2\u6570\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "DeepLogit\u6a21\u578b\u5c55\u793a\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u7ed3\u5408\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u7684AI\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u76f8\u4e92\u501f\u9274\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002\u968f\u7740\u6570\u636e\u91cf\u548c\u6a21\u578b\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u79bb\u6563\u9009\u62e9\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u66f4\u597d\u5730\u5e94\u7528\u4e8e\u89c4\u5212\u548c\u653f\u7b56\u9886\u57df\u3002"}}
{"id": "2509.13933", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13933", "abs": "https://arxiv.org/abs/2509.13933", "authors": ["Qiyue Li", "Yingxin Liu", "Hang Qi", "Jieping Luo", "Zhizhang Liu", "Jingjin Wu"], "title": "Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning", "comment": null, "summary": "We consider the client selection problem in wireless Federated Learning (FL),\nwith the objective of reducing the total required time to achieve a certain\nlevel of learning accuracy. Since the server cannot observe the clients'\ndynamic states that can change their computation and communication efficiency,\nwe formulate client selection as a restless multi-armed bandit problem. We\npropose a scalable and efficient approach called the Whittle Index Learning in\nFederated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and\nupdate an approximated Whittle index associated with each client, and then\nselects the clients with the highest indices. Compared to existing approaches,\nWILF-Q does not require explicit knowledge of client state transitions or data\ndistributions, making it well-suited for deployment in practical FL settings.\nExperiment results demonstrate that WILF-Q significantly outperforms existing\nbaseline policies in terms of learning efficiency, providing a robust and\nefficient approach to client selection in wireless FL.", "AI": {"tldr": "WILF-Q\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7Q\u5b66\u4e60\u81ea\u9002\u5e94\u66f4\u65b0\u7684Whittle\u6307\u6570\u6765\u9009\u62e9\u5ba2\u6237\u7aef\uff0c\u4ee5\u51cf\u5c11\u8fbe\u5230\u76ee\u6807\u5b66\u4e60\u7cbe\u5ea6\u6240\u9700\u7684\u65f6\u95f4\u3002", "motivation": "\u5728\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u670d\u52a1\u5668\u65e0\u6cd5\u89c2\u5bdf\u5230\u5ba2\u6237\u7aef\u7684\u52a8\u6001\u72b6\u6001\uff08\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\uff09\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\u4ee5\u51cf\u5c11\u8fbe\u5230\u7279\u5b9a\u5b66\u4e60\u7cbe\u5ea6\u6240\u9700\u7684\u65f6\u95f4\u3002", "method": "\u5c06\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u8e81\u52a8\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aWILF-Q\uff08Whittle Index Learning in Federated Q-learning\uff09\u7684\u65b9\u6cd5\u3002WILF-Q\u5229\u7528Q\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u548c\u66f4\u65b0\u4e0e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u76f8\u5173\u8054\u7684\u8fd1\u4f3cWhittle\u6307\u6570\uff0c\u7136\u540e\u9009\u62e9\u6307\u6570\u6700\u9ad8\u7684\u5ba2\u6237\u7aef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWILF-Q\u5728\u5b66\u4e60\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u7b56\u7565\uff0c\u4e3a\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u800c\u6709\u6548\u7684\u65b9\u6cd5\u3002", "conclusion": "WILF-Q\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u5b83\u4e0d\u9700\u8981\u4e86\u89e3\u5ba2\u6237\u7aef\u72b6\u6001\u8f6c\u79fb\u6216\u6570\u636e\u5206\u5e03\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2509.13940", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13940", "abs": "https://arxiv.org/abs/2509.13940", "authors": ["Weifeng Zhu", "Junyuan Gao", "Shuowen Zhang", "Liang Liu"], "title": "Reconfigurable Intelligent Surface-Assisted Multiuser Tracking and Signal Detection in ISAC", "comment": "6 pages, 6 figures, accepted by IEEE conference", "summary": "This paper investigates the multiuser tracking and signal detection problem\nin integrated sensing and communication (ISAC) systems with the assistance of\nreconfigurable intelligent surfaces (RISs). Due to the diverse and high user\nmobility, the tracking and signal detection performance can be significantly\ndeteriorated without choreographed user state (position and velocity) updating\nprinciple. To tackle this challenge, we manage to establish a comprehensive\nprobabilistic signal model to characterize the interdependencies among user\nstates, transmit signals, and received signals during the tracking procedure.\nBased on the Bayesian problem formulation, we further propose a novel hybrid\nvariational message passing algorithm for the online estimation of user states,\nwhich can iteratively update the posterior probabilities of user states during\neach tracking frame with computational efficiency. Numerical results are\nprovided to demonstrate that the proposed algorithm can significantly improve\nboth of the tracking and signal detection performance over the representative\nBayesian estimation counterparts.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRIS\u7684ISAC\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u8ffd\u8e2a\u548c\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u79fb\u52a8\u6027\u7528\u6237\u72b6\u6001\u66f4\u65b0\u95ee\u9898\u3002", "motivation": "\u4e3a\u89e3\u51b3ISAC\u7cfb\u7edf\u4e2d\u56e0\u7528\u6237\u591a\u6837\u6027\u548c\u9ad8\u79fb\u52a8\u6027\u5bfc\u81f4\u7684\u7528\u6237\u8ffd\u8e2a\u548c\u4fe1\u53f7\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u534f\u540c\u7684\u7528\u6237\u72b6\u6001\uff08\u4f4d\u7f6e\u548c\u901f\u5ea6\uff09\u66f4\u65b0\u673a\u5236\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u6982\u7387\u4fe1\u53f7\u6a21\u578b\u6765\u63cf\u8ff0\u7528\u6237\u72b6\u6001\u3001\u4f20\u8f93\u4fe1\u53f7\u548c\u63a5\u6536\u4fe1\u53f7\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u4f30\u8ba1\u7528\u6237\u72b6\u6001\uff0c\u5e76\u80fd\u5728\u6bcf\u4e2a\u8ffd\u8e2a\u5e27\u5185\u8fdb\u884c\u6709\u6548\u7684\u8fed\u4ee3\u66f4\u65b0\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8ffd\u8e2a\u548c\u4fe1\u53f7\u68c0\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8ffd\u8e2a\u9ad8\u79fb\u52a8\u6027\u7528\u6237\uff0c\u5e76\u63d0\u9ad8ISAC\u7cfb\u7edf\u7684\u4fe1\u53f7\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.14127", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14127", "abs": "https://arxiv.org/abs/2509.14127", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Philip Dames"], "title": "Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks", "comment": null, "summary": "We consider the problem of delivering multiple packages from a single pickup\ndepot to distinct goal locations using a homogeneous fleet of robots with\nlimited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner\nTree Relay Coordination Planning framework that constructs sparse relay trunks\nusing Steiner tree optimization and then synthesizes robot-level pickup, relay,\nand delivery schedules. This framework reframes relays from incidental\nbyproducts into central elements of coordination, offering a contrast with\ntraditional delivery methods that rely on direct source-to-destination\ntransport. Extensive experiments show consistent improvements of up to 34%\ncompared to conventional baselines, underscoring the benefits of incorporating\nrelays into the delivery process. These improvements translate directly to\nenhanced energy efficiency in multi-robot delivery under capacity constraints,\nproviding a scalable framework for real-world logistics.", "AI": {"tldr": "\u4f7f\u7528\u5177\u6709\u6709\u9650\u8f7d\u8d27\u80fd\u529b\u7684\u540c\u8d28\u673a\u5668\u4eba\u8f66\u961f\uff0c\u4ece\u5355\u4e2a\u53d6\u8d27\u70b9\u5411\u4e0d\u540c\u76ee\u6807\u4f4d\u7f6e\u8fd0\u9001\u591a\u4e2a\u5305\u88f9\u3002\u6211\u4eec\u63d0\u51fa\u4e86 VCST-RCP\uff08Voronoi-Constrained Steiner Tree Relay Coordination Planning\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528 Steiner \u6811\u4f18\u5316\u6765\u6784\u5efa\u7a00\u758f\u4e2d\u7ee7\u4e3b\u5e72\uff0c\u7136\u540e\u5408\u6210\u673a\u5668\u4eba\u7ea7\u522b\u7684\u53d6\u4ef6\u3001\u4e2d\u7ee7\u548c\u4ea4\u4ed8\u8ba1\u5212\u3002\u8be5\u6846\u67b6\u5c06\u4e2d\u7ee7\u4ece\u9644\u5e26\u7684\u526f\u4ea7\u54c1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u534f\u8c03\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u4e0e\u4f9d\u8d56\u76f4\u63a5\u6e90\u5230\u76ee\u7684\u5730\u8fd0\u8f93\u7684\u4f20\u7edf\u4ea4\u4ed8\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6027\u80fd\u4e00\u81f4\u63d0\u9ad8\u4e86 34%\uff0c\u8fd9\u51f8\u663e\u4e86\u5c06\u4e2d\u7ee7\u7eb3\u5165\u4ea4\u4ed8\u8fc7\u7a0b\u7684\u4f18\u52bf\u3002\u8fd9\u4e9b\u6539\u8fdb\u76f4\u63a5\u8f6c\u5316\u4e3a\u5bb9\u91cf\u53d7\u9650\u7684\u591a\u673a\u5668\u4eba\u4ea4\u4ed8\u4e2d\u7684\u80fd\u6e90\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u7269\u6d41\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "motivation": "\u5728\u5177\u6709\u6709\u9650\u8f7d\u8d27\u80fd\u529b\u7684\u540c\u8d28\u673a\u5668\u4eba\u8f66\u961f\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u4e86\u4ece\u5355\u4e2a\u53d6\u8d27\u70b9\u5411\u4e0d\u540c\u76ee\u6807\u4f4d\u7f6e\u8fd0\u9001\u591a\u4e2a\u5305\u88f9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86 VCST-RCP\uff08Voronoi-Constrained Steiner Tree Relay Coordination Planning\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528 Steiner \u6811\u4f18\u5316\u6765\u6784\u5efa\u7a00\u758f\u4e2d\u7ee7\u4e3b\u5e72\uff0c\u7136\u540e\u5408\u6210\u673a\u5668\u4eba\u7ea7\u522b\u7684\u53d6\u4ef6\u3001\u4e2d\u7ee7\u548c\u4ea4\u4ed8\u8ba1\u5212\u3002", "result": "\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6027\u80fd\u4e00\u81f4\u63d0\u9ad8\u4e86 34%\u3002", "conclusion": "\u5c06\u4e2d\u7ee7\u7eb3\u5165\u4ea4\u4ed8\u8fc7\u7a0b\u53ef\u4ee5\u63d0\u9ad8\u5bb9\u91cf\u53d7\u9650\u7684\u591a\u673a\u5668\u4eba\u4ea4\u4ed8\u4e2d\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u4ee5\u7528\u4e8e\u5b9e\u9645\u7269\u6d41\u3002"}}
{"id": "2509.13564", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13564", "abs": "https://arxiv.org/abs/2509.13564", "authors": ["Arman Pourghorban", "Dipankar Maity"], "title": "Multi-Attacker Single-Defender Target Defense in Conical Environments", "comment": null, "summary": "We consider a variant of the target defense problem in a planar conical\nenvironment where a single defender is tasked to capture a sequence of incoming\nattackers. The attackers' objective is to breach the target boundary without\nbeing captured by the defender. As soon as the current attacker breaches the\ntarget or gets captured by the defender, the next attacker appears at the\nboundary of the environment and moves radially toward the target with maximum\nspeed. Therefore, the defender's final location at the end of the current game\nbecomes its initial location for the next game. The attackers pick strategies\nthat are advantageous for the current as well as for future engagements between\nthe defender and the remaining attackers. The attackers have their own sensors\nwith limited range, using which they can perfectly detect if the defender is\nwithin their sensing range. We derive equilibrium strategies for all the\nplayers to optimize the capture percentage using the notions of capture\ndistribution. Finally, the theoretical results are verified through numerical\nexamples using Monte Carlo type random trials of experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5e73\u9762\u9525\u5f62\u73af\u5883\u4e2d\uff0c\u5355\u4e2a\u9632\u5fa1\u8005\u9700\u8981\u4f9d\u6b21\u62e6\u622a\u4e00\u7cfb\u5217\u653b\u51fb\u8005\u3002\u653b\u51fb\u8005\u7684\u76ee\u6807\u662f\u7a81\u7834\u9632\u5fa1\u8fb9\u754c\uff0c\u800c\u9632\u5fa1\u8005\u7684\u76ee\u6807\u662f\u62e6\u622a\u653b\u51fb\u8005\u3002\u6bcf\u6b21\u653b\u51fb\u7ed3\u675f\u540e\uff0c\u4e0b\u4e00\u4e2a\u653b\u51fb\u8005\u4f1a\u51fa\u73b0\u5e76\u4ee5\u6700\u5927\u901f\u5ea6\u5411\u76ee\u6807\u79fb\u52a8\uff0c\u9632\u5fa1\u8005\u7684\u6700\u7ec8\u4f4d\u7f6e\u5c06\u6210\u4e3a\u4e0b\u4e00\u8f6e\u6e38\u620f\u7684\u521d\u59cb\u4f4d\u7f6e\u3002\u653b\u51fb\u8005\u4f1a\u9009\u62e9\u5bf9\u5176\u81ea\u8eab\u548c\u540e\u7eed\u4ea4\u6218\u90fd\u6709\u5229\u7684\u7b56\u7565\uff0c\u5e76\u4e14\u62e5\u6709\u6709\u9650\u7684\u4f20\u611f\u5668\u8303\u56f4\uff0c\u53ef\u4ee5\u63a2\u6d4b\u9632\u5fa1\u8005\u662f\u5426\u5728\u8303\u56f4\u5185\u3002\u7814\u7a76\u63a8\u5bfc\u4e86\u6240\u6709\u73a9\u5bb6\u7684\u5747\u8861\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u7279\u522b\u662f\u9632\u5fa1\u8005\u9700\u8981\u8fde\u7eed\u62e6\u622a\u4e00\u7cfb\u5217\u653b\u51fb\u8005\uff0c\u5e76\u8003\u8651\u653b\u51fb\u8005\u91c7\u53d6\u4f18\u5316\u81ea\u8eab\u53ca\u540e\u7eed\u4ea4\u6218\u7684\u7b56\u7565\u3002", "method": "\u63a8\u5bfc\u4e86\u6240\u6709\u73a9\u5bb6\u7684\u5747\u8861\u7b56\u7565\uff0c\u5229\u7528\u6355\u83b7\u5206\u5e03\u7684\u6982\u5ff5\u6765\u4f18\u5316\u6355\u83b7\u7387\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4f8b\u5b50\u548c\u8499\u7279\u5361\u6d1b\u968f\u673a\u8bd5\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u63a8\u5bfc\u4e86\u5728\u7279\u5b9a\u73af\u5883\u548c\u89c4\u5219\u4e0b\uff0c\u9632\u5fa1\u8005\u548c\u653b\u51fb\u8005\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.13364", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAsterisk Operator\u7684\u65b0\u578b\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u90bb\u63a5\u7ed3\u6784\u5e76\u884c\u4f20\u64ad\uff08ASPP\uff09\u7684\u62bd\u8c61\u63a8\u7406\u3002", "motivation": "\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u7531\u9690\u5f0f\u5173\u7cfb\u56fe\u6307\u5bfc\u7684\u5c40\u90e8\u3001\u5e76\u884c\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u8bc1\u660e\u4e86\u8be5\u7b97\u5b50\u5728\u4fdd\u6301\u5c40\u90e8\u8ba1\u7b97\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u62bd\u8c61\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6536\u655b\u7684\u8ba1\u7b97\u8303\u5f0f\u3002", "result": "\u901a\u8fc7\u5bf9ARC2\u6311\u6218\u548c\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u8fdb\u884c\u4e25\u683c\u7684\u6570\u5b66\u5206\u6790\u548c\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u5b50\u7684\u666e\u904d\u6027\u3001\u6536\u655b\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684Embedding-Asterisk\u84b8\u998f\u65b9\u6cd5\uff0c\u5728ARC2\u9a8c\u8bc1\u96c6\u4e0a\u5b9e\u73b0\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a6M\uff0c\u6807\u5fd7\u7740\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u9886\u57df\u7684\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684Transformer\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u56de\u5f523D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u4f4d\u59ff\uff0c\u652f\u6301\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u53ef\u9009\u7684\u51e0\u4f55\u8f93\u5165\uff0c\u91c7\u7528\u5206\u89e3\u8868\u793a\u6cd5\uff08\u6df1\u5ea6\u56fe\u3001\u5c04\u7ebf\u56fe\u3001\u76f8\u673a\u4f4d\u59ff\u3001\u5c3a\u5ea6\u56e0\u5b50\uff09\u6765\u751f\u6210\u5168\u5c40\u4e00\u81f4\u7684\u5ea6\u91cf3D\u573a\u666f\u51e0\u4f55\u3002", "result": "\u5728\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\uff08\u5982SfM\u3001MVS\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\uff09\u4e0a\uff0cMapAnything\u7684\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4e86\u4e13\u7528\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u3002", "conclusion": "MapAnything\u6709\u671b\u6210\u4e3a\u901a\u7528\u76843D\u91cd\u5efa\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2509.14122", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.14122", "abs": "https://arxiv.org/abs/2509.14122", "authors": ["Tomas T. Osterholt", "D. O. Oriekhov", "Lumen Eek", "Cristiane Morais Smith", "Rembert A. Duine"], "title": "Twist-modulated magnetic interactions in bilayer van der Waals materials", "comment": null, "summary": "The ability to control magnetic interactions at the nanoscale is crucial for\nthe development of next-generation spintronic devices and functional magnetic\nmaterials. In this work, we investigate theoretically, by means of many-body\nperturbation theory, how interlayer twisting modulates magnetic interactions in\nbilayer van der Waals systems composed of two ferromagnetic layers. We\ndemonstrate that the relative strengths of the interlayer Heisenberg exchange\ninteraction, the Dzyaloshinskii-Moriya interaction, and the anisotropic\nexchange interaction can be significantly altered by varying the twist angle\nbetween the layers, thus leading to tunable magnetic textures. We further show\nthat these interactions are strongly dependent on the chemical potential,\nenabling additional control via electrostatic gating or doping. Importantly,\nour approach is applicable to arbitrary twist angles and does not rely on the\nconstruction of a Moir\\'e supercell, making it particularly efficient even at\nsmall twist angles.", "AI": {"tldr": "\u901a\u8fc7\u5c42\u95f4\u626d\u8f6c\u548c\u9759\u7535\u8c03\u63a7\u6765\u63a7\u5236\u53cc\u5c42\u8303\u5fb7\u534e\u6750\u6599\u4e2d\u7684\u78c1\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u63a7\u5236\u7eb3\u7c73\u5c3a\u5ea6\u4e0b\u7684\u78c1\u76f8\u4e92\u4f5c\u7528\u5bf9\u4e8e\u5f00\u53d1\u4e0b\u4e00\u4ee3\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u548c\u529f\u80fd\u78c1\u6027\u6750\u6599\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u591a\u4f53\u5fae\u6270\u7406\u8bba\uff0c\u7814\u7a76\u5c42\u95f4\u626d\u8f6c\u5982\u4f55\u8c03\u8282\u53cc\u5c42\u8303\u5fb7\u534e\u7cfb\u7edf\u4e2d\u53cc\u5c42\u94c1\u78c1\u5c42\u7684\u78c1\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u53ef\u4ee5\u901a\u8fc7\u6539\u53d8\u5c42\u95f4\u626d\u8f6c\u89d2\u6765\u663e\u8457\u6539\u53d8\u5c42\u95f4\u6d77\u68ee\u5821\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u3001Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u548c\u5404\u5411\u5f02\u6027\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u7684\u76f8\u5bf9\u5f3a\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u8c03\u7684\u78c1\u7eb9\u7406\u3002\u8fd9\u4e9b\u76f8\u4e92\u4f5c\u7528\u8fd8\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5316\u5b66\u52bf\uff0c\u53ef\u4ee5\u901a\u8fc7\u9759\u7535\u95e8\u63a7\u6216\u63ba\u6742\u8fdb\u884c\u8c03\u63a7\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u626d\u8f6c\u89d2\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u83ab\u5c14\u8d85\u80de\u7684\u6784\u5efa\uff0c\u5373\u4f7f\u5728\u5c0f\u626d\u8f6c\u89d2\u4e0b\u4e5f\u6548\u7387\u5f88\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u5c42\u95f4\u626d\u8f6c\u548c\u9759\u7535\u8c03\u63a7\u53ef\u4ee5\u6709\u6548\u63a7\u5236\u53cc\u5c42\u8303\u5fb7\u534e\u6750\u6599\u4e2d\u7684\u78c1\u76f8\u4e92\u4f5c\u7528\u548c\u78c1\u7eb9\u7406\u3002"}}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "LLMs \u7ecf\u8fc7 DSPy \u4f18\u5316\u540e\uff0c\u5728\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316 EHR \u6570\u636e\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u4e13\u95e8\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u66f4\u7b80\u5355\u3001\u66f4\u5177\u9002\u5e94\u6027\u3002", "motivation": "\u63a2\u7d22 LLMs \u5728\u5904\u7406\u5305\u542b\u65f6\u95f4\u5e8f\u5217\u7b49\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e DSPy \u7684\u63d0\u793a\u4f18\u5316\u6765\u8c03\u6574\u6307\u4ee4\u8c03\u8c10 LLMs\uff0c\u4ee5\u8054\u5408\u5904\u7406\u4e34\u5e8a\u7b14\u8bb0\u548c\u7ed3\u6784\u5316 EHR \u8f93\u5165\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u4e0e\u4e13\u95e8\u7684\u3001\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u7ecf\u8fc7 DSPy \u4f18\u5316\u7684 LLMs \u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316 EHR \u6570\u636e\uff0c\u4e3a\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13453", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13453", "abs": "https://arxiv.org/abs/2509.13453", "authors": ["Christopher K. Long", "Crispin H. W. Barnes"], "title": "From virtual Z gates to virtual Z pulses", "comment": "13 + (10) pages, 4 figs, comments are welcome", "summary": "Virtual $Z$ gates have become integral for implementing fast, high-fidelity\nsingle-qubit operations. However, virtual $Z$ gates require that the system's\ntwo-qubit gates are microwave-activated or normalise the single-qubit $Z$\nrotations$\\unicode{x2014}$the group generated by $X$, $\\operatorname{SWAP}$,\nand arbitrary phase gates. Herein, we extend the theory of virtual $Z$ gates to\nthe pulse-level, which underlies both gate design and the recent advancements\nof pulse-level quantum algorithms. These algorithms attempt to utilise the full\npotential of present-day noisy intermediate-scale quantum (NISQ) devices by\nremoving overheads associated with the compilation and transpilation of gates.\nTo extend the theory of virtual $Z$ gates, we derive a platform-agnostic\ntheoretical framework for virtual $Z$ pulses by employing time dilations of the\npulse sequences that control the quantum processor. Additionally, we provide\nworked examples of the implementation of virtual $Z$ pulses on both\nsemiconductor spin qubit and superconducting quantum processor architectures.\nMoreover, we present a general overview of the hardware support for virtual $Z$\npulses. We find virtual $Z$ pulses (and thus, virtual $Z$ gates) can be used on\nhardware that, with previous methods, did not support the virtual $Z$ gate.\nFinally, we present two additional applications of virtual $Z$ pulses to\npulse-level algorithms. First, broadening the class of Hamiltonians that can be\nnatively simulated in an analogue manner. Second, increasing the expressibility\nof pulse-based variational quantum algorithms.", "AI": {"tldr": "\u865a\u62dfZ\u95e8\u5728\u8109\u51b2\u5c42\u9762\u88ab\u7406\u8bba\u5316\uff0c\u6269\u5c55\u4e86\u5176\u5728NISQ\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u534a\u5bfc\u4f53\u548c\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u7684\u5b9e\u73b0\uff0c\u540c\u65f6\u8fd8\u62d3\u5c55\u4e86\u5176\u5728\u6a21\u62df\u548c\u53d8\u5206\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u865a\u62dfZ\u95e8\u5bf9\u4e8e\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u4fdd\u771f\u7684\u5355\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\u6709\u7279\u5b9a\u8981\u6c42\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u865a\u62dfZ\u95e8\u7406\u8bba\u6269\u5c55\u5230\u8109\u51b2\u5c42\u9762\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u5f53\u524d\u7684NISQ\u8bbe\u5907\uff0c\u51cf\u5c11\u95e8\u7f16\u8bd1\u548c\u8f6c\u8bd1\u7684\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u5bf9\u63a7\u5236\u91cf\u5b50\u5904\u7406\u5668\u7684\u8109\u51b2\u5e8f\u5217\u8fdb\u884c\u65f6\u95f4\u81a8\u80c0\uff0c\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u865a\u62dfZ\u8109\u51b2\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u548c\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u67b6\u6784\u4e0a\u5b9e\u73b0\u865a\u62dfZ\u8109\u51b2\u7684\u5b9e\u4f8b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u865a\u62dfZ\u8109\u51b2\uff08\u53ca\u5176\u5bf9\u5e94\u7684\u865a\u62dfZ\u95e8\uff09\u53ef\u4ee5\u5728\u4e4b\u524d\u4e0d\u652f\u6301\u865a\u62dfZ\u95e8\u7684\u786c\u4ef6\u4e0a\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u865a\u62dfZ\u8109\u51b2\u8fd8\u53ef\u4ee5\u62d3\u5bbd\u53ef\u539f\u751f\u6a21\u62df\u7684\u54c8\u5bc6\u987f\u91cf\u7c7b\u522b\uff0c\u5e76\u589e\u5f3a\u8109\u51b2\u5f0f\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u8868\u73b0\u529b\u3002", "conclusion": "\u865a\u62dfZ\u8109\u51b2\u4e3a\u5728NISQ\u8bbe\u5907\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u901a\u7528\u7684\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u5c24\u5176\u662f\u5728\u95e8\u64cd\u4f5c\u548c\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.13700", "categories": ["cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2509.13700", "abs": "https://arxiv.org/abs/2509.13700", "authors": ["Arghya Paul", "Kanak Raj", "Prince Raj Lawrence Raj", "Pratim Kumar"], "title": "Thermal Degradation Mechanisms and Stability Enhancement Strategies in Perovskite Solar Cells: A Review", "comment": null, "summary": "Perovskite Solar Cells (PSCs) have garnered global research interest owing to\ntheir superior photovoltaic (PV) performance. The future of photovoltaic\ntechnology lies in PSCs since they can produce power with performance on par\nwith the best silicon solar cells while being less expensive. PSCs have\nenormous potential; in just ten years, their efficiency increased from 3.8% to\n25.2%, and research into new developments is still ongoing. Thermal instability\nis PSCs' main disadvantage, despite their high efficiency, flexibility, and\nlightweight nature. This paper looks at how temperature affects the ways that\nhole transport layers (HTLs) like spiro-OMeTAD and perovskite layers,\nespecially MAPbI3, degrade. Elevated temperatures cause MAPbI3 to degrade into\nPbI2, CH3I, and NH3, with decomposition rates affected by moisture, oxygen, and\nenvironmental factors. Mixed cation compositions, such as Cs-MA-FA, have higher\nthermal stability, whereas MA+ cations break-down faster under heat stress.\nHTLs deteriorate due to morphological changes and the hydrophilicity of dopant\nadditions like Li-TFSI and t-BP. Alternative dopant-free HTMs, such as P3HT and\ninorganic materials including CuSCN, NiOx, and Cu2O, have shown improved\nthermal stability and efficiency. Hybrid HTLs, dopant-free designs, and\ninterface tweaks are all viable solutions for increasing the stability of PSC.\nAddressing thermal stability issues remains crucial for the development of more\nreliable and efficient PSC technology.", "AI": {"tldr": "\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\uff08PSCs\uff09\u56e0\u5176\u4f18\u5f02\u7684\u5149\u4f0f\u6027\u80fd\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u70ed\u7a33\u5b9a\u6027\u5dee\u662f\u5176\u4e3b\u8981\u7f3a\u70b9\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6e29\u5ea6\u5bf9\u7a7a\u7a74\u4f20\u8f93\u5c42\uff08HTLs\uff09\u548c\u9499\u949b\u77ff\u5c42\uff08\u7279\u522b\u662fMAPbI3\uff09\u964d\u89e3\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u70ed\u7a33\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\uff08PSCs\uff09\u5728\u6548\u7387\u3001\u67d4\u6027\u548c\u8f7b\u8d28\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u70ed\u4e0d\u7a33\u5b9a\u6027\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u6e29\u5ea6\u5982\u4f55\u5f71\u54cd\u5173\u952e\u7ec4\u4ef6\uff08HTLs\u548c\u9499\u949b\u77ff\u5c42\uff09\u7684\u964d\u89e3\u673a\u5236\uff0c\u5e76\u4e3a\u63d0\u9ad8PSCs\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9ad8\u6e29\u5bf9MAPbI3\uff08\u5982\u5206\u89e3\u4e3aPbI2, CH3I, NH3\uff09\u548cHTLs\uff08\u5982\u5f62\u6001\u53d8\u5316\u548c\u63ba\u6742\u5242\u4eb2\u6c34\u6027\uff09\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u6210\u5206\uff08\u5982Cs-MA-FA\uff09\u548c\u66ff\u4ee3\u6750\u6599\uff08\u5982P3HT, CuSCN, NiOx, Cu2O\uff09\u5bf9\u70ed\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u6e29\u4f1a\u5bfc\u81f4MAPbI3\u5206\u89e3\uff0c\u5176\u901f\u7387\u53d7\u6e7f\u5ea6\u548c\u6c27\u6c14\u7b49\u56e0\u7d20\u5f71\u54cd\u3002Cs-MA-FA\u7b49\u6df7\u5408\u9633\u79bb\u5b50\u7ec4\u6210\u7684\u9499\u949b\u77ff\u70ed\u7a33\u5b9a\u6027\u66f4\u9ad8\u3002HTLs\u4e2d\u7684\u63ba\u6742\u5242\uff08\u5982Li-TFSI\u548ct-BP\uff09\u4f1a\u5bfc\u81f4\u5f62\u6001\u53d8\u5316\u548c\u4eb2\u6c34\u6027\u589e\u52a0\uff0c\u4ece\u800c\u52a0\u901f\u964d\u89e3\u3002\u7136\u800c\uff0cP3HT\u3001CuSCN\u3001NiOx\u548cCu2O\u7b49\u65e0\u63ba\u6742\u5242HTMs\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u70ed\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u63d0\u9ad8PSCs\u70ed\u7a33\u5b9a\u6027\u7684\u53ef\u884c\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u6df7\u5408\u9633\u79bb\u5b50\u3001\u5f00\u53d1\u65e0\u63ba\u6742\u5242HTLs\uff08\u5982P3HT\u548c\u65e0\u673a\u6750\u6599\uff09\u4ee5\u53ca\u4f18\u5316\u754c\u9762\u3002\u89e3\u51b3\u70ed\u7a33\u5b9a\u6027\u95ee\u9898\u5bf9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u9ad8\u6548\u7684PSCs\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.13534", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13534", "abs": "https://arxiv.org/abs/2509.13534", "authors": ["Chunxin Zheng", "Kai Chen", "Zhihai Bi", "Yulin Li", "Liang Pan", "Jinni Zhou", "Haoang Li", "Jun Ma"], "title": "Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning", "comment": null, "summary": "Whole-body manipulation (WBM) for humanoid robots presents a promising\napproach for executing embracing tasks involving bulky objects, where\ntraditional grasping relying on end-effectors only remains limited in such\nscenarios due to inherent stability and payload constraints. This paper\nintroduces a reinforcement learning framework that integrates a pre-trained\nhuman motion prior with a neural signed distance field (NSDF) representation to\nachieve robust whole-body embracing. Our method leverages a teacher-student\narchitecture to distill large-scale human motion data, generating kinematically\nnatural and physically feasible whole-body motion patterns. This facilitates\ncoordinated control across the arms and torso, enabling stable multi-contact\ninteractions that enhance the robustness in manipulation and also the load\ncapacity. The embedded NSDF further provides accurate and continuous geometric\nperception, improving contact awareness throughout long-horizon tasks. We\nthoroughly evaluate the approach through comprehensive simulations and\nreal-world experiments. The results demonstrate improved adaptability to\ndiverse shapes and sizes of objects and also successful sim-to-real transfer.\nThese indicate that the proposed framework offers an effective and practical\nsolution for multi-contact and long-horizon WBM tasks of humanoid robots.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548c\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NSDF\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u62e5\u62b1\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6293\u53d6\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5\u5728\u5904\u7406\u7b28\u91cd\u7269\u4f53\u65f6\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u9650\u5236\uff0c\u800c\u5168\u8eab\u64cd\u4f5c\uff08WBM\uff09\u662f\u6267\u884c\u62e5\u62b1\u7b49\u4efb\u52a1\u7684\u66f4\u4f18\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u67b6\u6784\uff0c\u6574\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548c\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NSDF\uff09\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u5168\u8eab\u62e5\u62b1\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u70bc\u5927\u89c4\u6a21\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\uff0c\u751f\u6210\u8fd0\u52a8\u5b66\u4e0a\u81ea\u7136\u4e14\u7269\u7406\u4e0a\u53ef\u884c\u7684\u5168\u8eab\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4ece\u800c\u534f\u8c03\u624b\u81c2\u548c\u8eaf\u5e72\u7684\u63a7\u5236\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u591a\u63a5\u89e6\u4ea4\u4e92\uff0c\u63d0\u9ad8\u64cd\u4f5c\u9c81\u68d2\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\u3002NSDF\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u51e0\u4f55\u611f\u77e5\uff0c\u589e\u5f3a\u4e86\u5bf9\u957f\u5468\u671f\u4efb\u52a1\u7684\u63a5\u89e6\u611f\u77e5\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u5f62\u72b6\u548c\u5c3a\u5bf8\u7269\u4f53\u7684\u9002\u5e94\u6027\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u591a\u63a5\u89e6\u548c\u957f\u5468\u671f\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13634", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13634", "abs": "https://arxiv.org/abs/2509.13634", "authors": ["Md Bokhtiar Al Zami", "Md Raihan Uddin", "Dinh C. Nguyen"], "title": "Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs", "comment": "15 pages, under revision at IEEE Internet of Things Journal", "summary": "Federated learning (FL) has gained popularity as a privacy-preserving method\nof training machine learning models on decentralized networks. However to\nensure reliable operation of UAV-assisted FL systems, issues like as excessive\nenergy consumption, communication inefficiencies, and security vulnerabilities\nmust be solved. This paper proposes an innovative framework that integrates\nDigital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to\ntackle these challenges. UAVs act as mobile base stations, allowing scattered\ndevices to train FL models locally and upload model updates for aggregation. By\nincorporating DT technology, our approach enables real-time system monitoring\nand predictive maintenance, improving UAV network efficiency. Additionally,\nZero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification\nwithout exposing sensitive data. To optimize energy efficiency and resource\nmanagement, we introduce a dynamic allocation strategy that adjusts UAV flight\npaths, transmission power, and processing rates based on network conditions.\nUsing block coordinate descent and convex optimization techniques, our method\nsignificantly reduces system energy consumption by up to 29.6% compared to\nconventional FL approaches. Simulation results demonstrate improved learning\nperformance, security, and scalability, positioning this framework as a\npromising solution for next-generation UAV-based intelligent networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u548c\u96f6\u77e5\u8bc6\u8054\u90a6\u5b66\u4e60\uff08zkFed\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u65e0\u4eba\u673a\uff08UAV\uff09\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7cfb\u7edf\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u3001\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u65e0\u4eba\u673a\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\uff0c\u5fc5\u987b\u89e3\u51b3\u80fd\u6e90\u6d88\u8017\u8fc7\u9ad8\u3001\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u65e0\u4eba\u673a\u4f5c\u4e3a\u79fb\u52a8\u57fa\u7ad9\uff0c\u5b9e\u73b0\u672c\u5730\u5316\u6a21\u578b\u8bad\u7ec3\u548c\u66f4\u65b0\u805a\u5408\u3002\u901a\u8fc7\u96c6\u6210\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u63d0\u9ad8\u7f51\u7edc\u6548\u7387\u3002\u5229\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKPs\uff09\u5728\u4e0d\u66b4\u9732\u654f\u611f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6a21\u578b\u9a8c\u8bc1\uff0c\u589e\u5f3a\u5b89\u5168\u6027\u3002\u5f15\u5165\u52a8\u6001\u5206\u914d\u7b56\u7565\uff0c\u6839\u636e\u7f51\u7edc\u6761\u4ef6\u8c03\u6574\u65e0\u4eba\u673a\u822a\u7ebf\u3001\u4f20\u8f93\u529f\u7387\u548c\u5904\u7406\u901f\u7387\uff0c\u4ee5\u4f18\u5316\u80fd\u6e90\u6548\u7387\u548c\u8d44\u6e90\u7ba1\u7406\u3002", "result": "\u901a\u8fc7\u91c7\u7528\u5757\u5750\u6807\u4e0b\u964d\u548c\u51f8\u4f18\u5316\u6280\u672f\uff0c\u4e0e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cfb\u7edf\u80fd\u8017\u663e\u8457\u964d\u4f4e\u4e86\u9ad8\u8fbe29.6%\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b66\u4e60\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u6709\u6240\u63d0\u5347\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u4eba\u673a\u667a\u80fd\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14098", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.14098", "abs": "https://arxiv.org/abs/2509.14098", "authors": ["Doru Thom Popovici", "Harlin Lee", "Mauro Del Ben", "Naoki Yoshioka", "Nobuyasu Ito", "Katherine Klymko", "Daan Camps", "Anastasiia Butko"], "title": "A Closeness Centrality-based Circuit Partitioner for Quantum Simulations", "comment": "14 pages, 10 figures", "summary": "Simulating quantum circuits (QC) on high-performance computing (HPC) systems\nhas become an essential method to benchmark algorithms and probe the potential\nof large-scale quantum computation despite the limitations of current quantum\nhardware. However, these simulations often require large amounts of resources,\nnecessitating the use of large clusters with thousands of compute nodes and\nlarge memory footprints. In this work, we introduce an end-to-end framework\nthat provides an efficient partitioning scheme for large-scale QCs alongside a\nflexible code generator to offer a portable solution that minimizes data\nmovement between compute nodes. By formulating the distribution of quantum\nstates and circuits as a graph problem, we apply closeness centrality to assess\ngate importance and design a fast, scalable partitioning method. The resulting\npartitions are compiled into highly optimized codes that run seamlessly on a\nwide range of supercomputers, providing critical insights into the performance\nand scalability of quantum algorithm simulations.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u91cf\u5b50\u6001\u548c\u7535\u8def\u7684\u5206\u5e03\u6784\u5efa\u4e3a\u56fe\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u7d27\u5bc6\u5ea6\u4e2d\u5fc3\u6027\u8bc4\u4f30\u95e8\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u4e3a\u5927\u578b\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u5212\u5206\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u4e86\u4e00\u4e2a\u4fbf\u643a\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8de8\u8ba1\u7b97\u8282\u70b9\u7684\u6570\u636e\u79fb\u52a8\uff0c\u4ece\u800c\u4e3a\u91cf\u5b50\u7b97\u6cd5\u6a21\u62df\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u786c\u4ef6\u9650\u5236\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u6765\u6a21\u62df\u91cf\u5b50\u7535\u8def\uff0c\u4f46\u8fd9\u4e9b\u6a21\u62df\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u5bb9\u7eb3\u5927\u578b\u96c6\u7fa4\u548c\u5185\u5b58\u5360\u7528\u7684\u7cfb\u7edf\u3002", "method": "\u5c06\u91cf\u5b50\u6001\u548c\u7535\u8def\u7684\u5206\u5e03\u6784\u5efa\u4e3a\u56fe\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u7d27\u5bc6\u5ea6\u4e2d\u5fc3\u6027\u6765\u8bc4\u4f30\u95e8\u7684\u91cd\u8981\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u5212\u5206\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u4ee3\u7801\u751f\u6210\u5668\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8de8\u8ba1\u7b97\u8282\u70b9\u7684\u6570\u636e\u79fb\u52a8\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u5927\u578b\u91cf\u5b50\u7535\u8def\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u5212\u5206\u65b9\u6848\uff0c\u5e76\u751f\u6210\u53ef\u79fb\u690d\u7684\u3001\u7ecf\u8fc7\u9ad8\u5ea6\u4f18\u5316\u7684\u4ee3\u7801\uff0c\u8fd9\u4e9b\u4ee3\u7801\u53ef\u4ee5\u5728\u5404\u79cd\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u65e0\u7f1d\u8fd0\u884c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u9ad8\u6548\u5730\u6a21\u62df\u5927\u578b\u91cf\u5b50\u7535\u8def\uff0c\u901a\u8fc7\u56fe\u8bba\u548c\u4f18\u5316\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u91cf\u5b50\u7b97\u6cd5\u6a21\u62df\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2509.13961", "categories": ["eess.SP", "92C55, 68T10, 93C85", "I.5.4; J.3; H.1.2"], "pdf": "https://arxiv.org/pdf/2509.13961", "abs": "https://arxiv.org/abs/2509.13961", "authors": ["Lorenza Angelini", "Dimitar Stanev", "Marta P\u0142onka", "Rafa\u0142 Klimas", "Natan Napi\u00f3rkowski", "Gabriela Gonz\u00e1lez Chan", "Lisa Bunn", "Paul S Glazier", "Richard Hosking", "Jenny Freeman", "Jeremy Hobart", "Jonathan Marsden", "Licinio Craveiro", "Mike D Rinderknecht", "Mattia Zanon"], "title": "Adaptive and robust smartphone-based step detection in multiple sclerosis", "comment": "66 pages total, 6 figures, 1 table, 23 supplementary appendix pages,\n  2 supplementary figures, 6 supplementary tables", "summary": "Background: Many attempts to validate gait pipelines that process sensor data\nto detect gait events have focused on the detection of initial contacts only in\nsupervised settings using a single sensor. Objective: To evaluate the\nperformance of a gait pipeline in detecting initial/final contacts using a step\ndetection algorithm adaptive to different test settings, smartphone wear\nlocations, and gait impairment levels. Methods: In GaitLab (ISRCTN15993728),\nhealthy controls (HC) and people with multiple sclerosis (PwMS; Expanded\nDisability Status Scale 0.0-6.5) performed supervised Two-Minute Walk Test\n[2MWT] (structured in-lab overground and treadmill 2MWT) during two on-site\nvisits carrying six smartphones and unsupervised walking activities (structured\nand unstructured real-world walking) daily for 10-14 days using a single\nsmartphone. Reference gait data were collected with a motion capture system or\nGait Up sensors. The pipeline's performance in detecting initial/final contacts\nwas evaluated through F1 scores and absolute temporal error with respect to\nreference measurement systems. Results: We studied 35 HC and 93 PwMS.\nInitial/final contacts were accurately detected across all smartphone wear\nlocations. Median F1 scores for initial/final contacts on in-lab 2MWT were\n>=98.2%/96.5% in HC and >=98.5%/97.7% in PwMS. F1 scores remained high on\nstructured (HC: 100% [0.3%]/100% [0.2%]; PwMS: 99.5% [1.9%]/99.4% [2.5%]) and\nunstructured real-world walking (HC: 97.8% [2.6%]/97.8% [2.8%]; PwMS: 94.4%\n[6.2%]/94.0% [6.5%]). Median temporal errors were <=0.08 s. Neither age, sex,\ndisease severity, walking aid use, nor setting (outdoor/indoor) impacted\npipeline performance (all p>0.05). Conclusion: This gait pipeline accurately\nand consistently detects initial and final contacts in PwMS across different\nsmartphone locations and environments, highlighting its potential for\nreal-world gait assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u6b65\u6001\u5206\u6790\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u4e0d\u540c\u573a\u666f\u3001\u4e0d\u540c\u4f69\u6234\u4f4d\u7f6e\u548c\u4e0d\u540c\u6b65\u6001\u635f\u4f24\u7a0b\u5ea6\u4e0b\u7684\u521d\u59cb\u63a5\u89e6\u548c\u6700\u7ec8\u63a5\u89e6\u3002", "motivation": "\u76ee\u524d\u5df2\u6709\u7684\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u9a8c\u8bc1\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u4e00\u4f20\u611f\u5668\u548c\u76d1\u7763\u573a\u666f\u4e0b\u7684\u521d\u59cb\u63a5\u89e6\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u590d\u6742\u73b0\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728\u5b9e\u9a8c\u5ba4\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u6536\u96c6\u4e86\u5065\u5eb7\u5bf9\u7167\u7ec4\u548c\u591a\u53d1\u6027\u786c\u5316\u75c7\u60a3\u8005\u7684\u6b65\u6001\u6570\u636e\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u8bb0\u5f55\u6570\u636e\uff0c\u5e76\u4e0e\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u6216Gait Up\u4f20\u611f\u5668\u6536\u96c6\u7684\u53c2\u8003\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528F1\u5206\u6570\u548c\u7edd\u5bf9\u65f6\u95f4\u8bef\u5dee\u6765\u8bc4\u4f30\u6b65\u6001\u5206\u6790\u6d41\u7a0b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6b65\u6001\u5206\u6790\u6d41\u7a0b\u5728\u6240\u6709\u667a\u80fd\u624b\u673a\u4f69\u6234\u4f4d\u7f6e\u90fd\u80fd\u51c6\u786e\u68c0\u6d4b\u521d\u59cb\u63a5\u89e6\u548c\u6700\u7ec8\u63a5\u89e6\uff0c\u65e0\u8bba\u662f\u5728\u5b9e\u9a8c\u5ba4\u8fd8\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u884c\u8d70\u6d3b\u52a8\u4e2d\uff0c\u5176F1\u5206\u6570\u5747\u8f83\u9ad8\uff0c\u4e14\u65f6\u95f4\u8bef\u5dee\u8f83\u5c0f\u3002\u6b64\u5916\uff0c\u5e74\u9f84\u3001\u6027\u522b\u3001\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u3001\u52a9\u884c\u5668\u4f7f\u7528\u60c5\u51b5\u6216\u73af\u5883\uff08\u5ba4\u5185/\u5ba4\u5916\uff09\u5747\u4e0d\u5f71\u54cd\u8be5\u6d41\u7a0b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6b65\u6001\u5206\u6790\u6d41\u7a0b\u80fd\u591f\u51c6\u786e\u3001\u4e00\u81f4\u5730\u68c0\u6d4b\u591a\u53d1\u6027\u786c\u5316\u75c7\u60a3\u8005\u5728\u4e0d\u540c\u667a\u80fd\u624b\u673a\u4f69\u6234\u4f4d\u7f6e\u548c\u73af\u5883\u4e0b\u7684\u521d\u59cb\u63a5\u89e6\u548c\u6700\u7ec8\u63a5\u89e6\uff0c\u663e\u793a\u51fa\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u6b65\u6001\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13567", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13567", "abs": "https://arxiv.org/abs/2509.13567", "authors": ["Praveen Verma", "Di Shi", "Yanzhu Ye", "Fengyu Wang", "Ying Zhang"], "title": "Impact of Solar Integration on Grid Security: Unveiling Vulnerabilities in Load Redistribution Attacks", "comment": "Accepted in 16th IEEE PowerTech 2025, Kiel, Germany", "summary": "Load redistribution (LR) attacks represent a practical and sophisticated form\nof false data injection (FDI) attacks, where the attacker manipulates grid data\nto influence economic operations of the grid through misleading security\nconstrained economic dispatch (SCED) decisions. Traditionally, LR attack models\noperate under the assumption that generator measurements are secure and immune\nto tampering. However, the increasing integration of solar generation into\npower grids challenges this assumption, exposing new vulnerabilities. This\npaper proposes an enhanced load redistribution attack model, addressing new\nvulnerabilities introduced by the increasing integration of solar generation in\npower grids. The study demonstrates that manipulating solar generation data\nsignificantly disrupts grid economics, with peak impacts during periods of high\nsolar generation.", "AI": {"tldr": "\u653b\u51fb\u8005\u64cd\u7eb5\u7535\u7f51\u6570\u636e\u4ee5\u5f71\u54cd\u7ecf\u6d4e\u8fd0\u884c\uff0c\u5e76\u5229\u7528\u4e86\u592a\u9633\u80fd\u53d1\u7535\u6574\u5408\u7684\u6f0f\u6d1e\u3002", "motivation": "\u4f20\u7edf\u7684\u8d1f\u8f7d\u518d\u5206\u914d\uff08LR\uff09\u653b\u51fb\u6a21\u578b\u5047\u8bbe\u53d1\u7535\u673a\u6d4b\u91cf\u662f\u5b89\u5168\u7684\uff0c\u4f46\u592a\u9633\u80fd\u53d1\u7535\u7684\u6574\u5408\u5e26\u6765\u4e86\u65b0\u7684\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u8d1f\u8f7d\u518d\u5206\u914d\u653b\u51fb\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u592a\u9633\u80fd\u53d1\u7535\u6574\u5408\u5e26\u6765\u7684\u65b0\u6f0f\u6d1e\u3002", "result": "\u64cd\u7eb5\u592a\u9633\u80fd\u53d1\u7535\u6570\u636e\u4f1a\u4e25\u91cd\u6270\u4e71\u7535\u7f51\u7ecf\u6d4e\uff0c\u5c24\u5176\u662f\u5728\u592a\u9633\u80fd\u53d1\u7535\u9ad8\u5cf0\u671f\u3002", "conclusion": "\u592a\u9633\u80fd\u53d1\u7535\u7684\u6574\u5408\u4e3aLR\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "Agent^2\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u53cc\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\u4e86\u5168\u81ea\u52a8\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u7684RL\u4ee3\u7406\u5f00\u53d1\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u3001\u8017\u65f6\u4e14\u6613\u5931\u8d25\uff0c\u9650\u5236\u4e86\u5176\u53ef\u53ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0RL\u4ee3\u7406\u8bbe\u8ba1\u7684\u5168\u81ea\u52a8\u5316\u3002", "method": "Agent^2\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff1a\u751f\u6210\u4ee3\u7406\uff08Generator Agent\uff09\u8d1f\u8d23\u6839\u636e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u548c\u73af\u5883\u4ee3\u7801\u751f\u6210RL\u4ee3\u7406\uff0c\u76ee\u6807\u4ee3\u7406\uff08Target Agent\uff09\u662f\u751f\u6210\u7684RL\u4ee3\u7406\u3002\u8be5\u6846\u67b6\u5c06RL\u5f00\u53d1\u5206\u89e3\u4e3aMDP\u5efa\u6a21\u548c\u7b97\u6cd5\u4f18\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08Model Context Protocol\uff09\u5b9e\u73b0\u8de8\u73af\u5883\u548c\u7b97\u6cd5\u7684\u6807\u51c6\u5316\u521b\u5efa\uff0c\u540c\u65f6\u5305\u542b\u81ea\u9002\u5e94\u8bad\u7ec3\u7ba1\u7406\u548c\u667a\u80fd\u53cd\u9988\u5206\u6790\u3002", "result": "\u5728MuJoCo\u3001MetaDrive\u3001MPE\u548cSMAC\u7b49\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgent^2\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe55%\uff0c\u5e73\u5747\u63d0\u5347\u663e\u8457\u3002", "conclusion": "Agent^2\u901a\u8fc7\u5b9e\u73b0\u771f\u6b63\u7684\u7aef\u5230\u7aef\u3001\u95ed\u73af\u81ea\u52a8\u5316\uff0c\u5f00\u521b\u4e86\u667a\u80fd\u4ee3\u7406\u8bbe\u8ba1\u548c\u4f18\u5316\u5176\u4ed6\u4ee3\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u662f\u81ea\u52a8\u5316AI\u7cfb\u7edf\u7684\u4e00\u4e2a\u6839\u672c\u6027\u7a81\u7834\u3002"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "\u63d0\u51faSCM-PR\u6846\u67b6\uff0c\u5229\u7528RGB\u56fe\u50cf\u7684\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u6fc0\u5149\u96f7\u8fbe\u5730\u56fe\u7684\u9c81\u68d2\u5b9a\u4f4d\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRGB\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5bf9\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u53d8\u5316\u654f\u611f\uff0c\u800c\u73b0\u6709\u7684\u8de8\u6a21\u6001\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7cbe\u7ec6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u4ecd\u6709\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faSCM-PR\u6846\u67b6\uff0c\u5305\u542bVMamba\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6RGB\u7279\u5f81\uff0c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\uff08SAFF\uff09\u6a21\u5757\u878d\u5408\u63cf\u8ff0\u7b26\u548c\u5206\u5272\u63a9\u7801\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7684\u6fc0\u5149\u96f7\u8fbe\u63cf\u8ff0\u7b26\uff0c\u4ee5\u53ca\u7528\u4e8e\u6539\u8fdb\u5339\u914d\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u6ce8\u610f\u529b\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u591a\u89c6\u89d2\u8bed\u4e49\u51e0\u4f55\u5339\u914d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCM-PR\u76f8\u6bd4\u5176\u4ed6\u8de8\u6a21\u6001\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SCM-PR\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5728\u6fc0\u5149\u96f7\u8fbe\u5730\u56fe\u4e2d\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "LLM\u5e7b\u89c9\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002DSCC-HS\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u5e72\u9884\uff0c\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u4ee3\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u88ab\u8bad\u7ec3\u6210\u4e8b\u5b9e\u5bf9\u9f50\u4ee3\u7406\uff08FAP\uff09\u548c\u5e7b\u89c9\u68c0\u6d4b\u4ee3\u7406\uff08HDP\uff09\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8fd9\u4e24\u4e2a\u4ee3\u7406\u4f1a\u5b9e\u65f6\u5730\u6307\u5bfc\u5927\u578b\u76ee\u6807\u6a21\u578b\u3002DSCC-HS\u53ef\u4ee5\u5373\u63d2\u5373\u7528\uff0c\u65e0\u9700\u4fee\u6539\u76ee\u6807\u6a21\u578b\u3002\u5728TruthfulQA\u548cBioGEN\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSCC-HS\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728TruthfulQA\u4e0a\u5b9e\u73b0\u4e8699.2%\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u7387\uff08FCR\uff09\uff0c\u5728BioGEN\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad8\u7684FActScore\uff0846.50\uff09\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7b\u89c9\u662f\u5176\u53ef\u9760\u90e8\u7f72\u7684\u4e00\u4e2a\u91cd\u5927\u969c\u788d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a**\u52a8\u6001\u81ea\u589e\u5f3a\u6821\u51c6\u5e7b\u89c9\u6291\u5236\uff08DSCC-HS\uff09** \u7684\u65b0\u9896\u3001\u4e3b\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5e72\u9884\u3002DSCC-HS\u53d7\u5230\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\u7684\u542f\u53d1\uff0c\u4f7f\u7528\u4e00\u4e2a\u7d27\u51d1\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u5bf9\u6297\u6027\u89d2\u8272\u8bad\u7ec3\u4e3a\u4e8b\u5b9e\u5bf9\u9f50\u4ee3\u7406\uff08FAP\uff09\u548c\u5e7b\u89c9\u68c0\u6d4b\u4ee3\u7406\uff08HDP\uff09\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u4e9b\u4ee3\u7406\u901a\u8fc7\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u6ce8\u5165\u5b9e\u65f6\u8f6c\u5411\u5411\u91cf\uff08FAP\u548cHDP logits\u4e4b\u5dee\uff09\u6765\u52a8\u6001\u5730\u5f15\u5bfc\u5927\u578b\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5728TruthfulQA\u548cBioGEN\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSCC-HS\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728TruthfulQA\u4e0a\uff0c\u5b83\u8fbe\u5230\u4e8699.2%\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u7387\uff08FCR\uff09\u3002\u5728\u957f\u683c\u5f0f\u7684BioGEN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u83b7\u5f97\u4e86\u6700\u9ad8\u7684FActScore\uff0c\u4e3a46.50\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86DSCC-HS\u662f\u63d0\u9ad8LLM\u4e8b\u5b9e\u6027\u7684\u4e00\u4e2a\u539f\u5219\u6027\u548c\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13470", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13470", "abs": "https://arxiv.org/abs/2509.13470", "authors": ["Jay Lawrence"], "title": "The original Wigner's-Friend scenarios", "comment": "6 pages, no figures", "summary": "We describe the Wigner's-Friend scenario according to Wigner, then a similar\nbut earlier version according to Everett. Wigner and Everett provide different\nresolutions of essentially the same paradox. Decoherence theory provides a\nthird resolution. Despite different interpretations (or their absence), these\nthree stories fit together to form a consistent picture without a paradox.", "AI": {"tldr": "Wigner's-Friend", "motivation": "Wigner's-Friend scenario", "method": "Decoherence theory", "result": "Three stories fit together to form a consistent picture without a paradox.", "conclusion": "Despite different interpretations (or their absence), these three stories fit together to form a consistent picture without a paradox."}}
{"id": "2509.13728", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13728", "abs": "https://arxiv.org/abs/2509.13728", "authors": ["K. Larbaoui", "A. Lakdja", "G. Bassou"], "title": "Phase stability and structural properties of the K$_{x}$Ca$_{1-x}$N novel ferromagnetic alloy from first-principles", "comment": "7 pages, 5 figures", "summary": "We study the structural properties and phase stability of the\nK$_{x}$Ca$_{1-x}$N alloy using the regular-solution model based on the total\nenergy of the mixing. The pseudopotential approach was used along with PBE\nfunctional of Perdew, Burke, and Ernzerhof (PBE). We investigated the\nbond-lengths distribution as a function of composition $x$. We also predicted\nthe phase separation of the two partially miscible components and calculated\nthe enthalpy $\\Delta H$ using the interaction parameter $\\Omega$. We observe an\nasymmetry about $x=0.46$ in the phase diagram due to the $x$-dependant\ninteraction parameter $\\Omega=12.69-1.32x$ kcal/mole. The equilibrium\nsolubility limit, known as the miscibility gap is found to be around 3033 K.", "AI": {"tldr": "KxCa1-xN\u5408\u91d1\u5177\u6709\u4e0d\u5bf9\u79f0\u76f8\u56fe\uff0c\u76f8\u5206\u79bb\u53d1\u751f\u57283033K\uff0c\u8fd9\u662f\u7531x\u4f9d\u8d56\u7684\u76f8\u4e92\u4f5c\u7528\u53c2\u6570\u5f15\u8d77\u7684\u3002", "motivation": "\u7814\u7a76KxCa1-xN\u5408\u91d1\u7684\u7ed3\u6784\u6027\u8d28\u548c\u76f8\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6df7\u5408\u603b\u80fd\u91cf\u7684\u6b63\u5219\u6eb6\u6db2\u6a21\u578b\uff0c\u7ed3\u5408\u8d5d\u52bf\u65b9\u6cd5\u548cPBE\u51fd\u6570\u3002\u7814\u7a76\u4e86\u952e\u957f\u5206\u5e03\uff0c\u5e76\u8ba1\u7b97\u4e86\u7113\u3002", "result": "\u9884\u6d4b\u4e86\u4e24\u79cd\u90e8\u5206\u6df7\u6eb6\u7ec4\u5206\u7684\u76f8\u5206\u79bb\uff0c\u5e76\u4f7f\u7528\u76f8\u4e92\u4f5c\u7528\u53c2\u6570\u8ba1\u7b97\u4e86\u7113\u3002\u53d1\u73b0\u76f8\u56fe\u5728x=0.46\u5904\u4e0d\u5bf9\u79f0\uff0c\u76f8\u4e92\u4f5c\u7528\u53c2\u6570\u4e3a\u03a9=12.69-1.32x kcal/mole\u3002\u76f8\u56fe\u7684\u6df7\u6eb6\u95f4\u9699\u7ea6\u4e3a3033K\u3002", "conclusion": "KxCa1-xN\u5408\u91d1\u7684\u76f8\u56fe\u4e0d\u5bf9\u79f0\uff0c\u8fd9\u662f\u7531\u4e8ex\u4f9d\u8d56\u7684\u76f8\u4e92\u4f5c\u7528\u53c2\u6570\u3002"}}
{"id": "2509.13541", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13541", "abs": "https://arxiv.org/abs/2509.13541", "authors": ["Ayberk Acar", "Fangjie Li", "Hao Li", "Lidia Al-Zogbi", "Kanyifeechukwu Jane Oguine", "Susheela Sharma Stern", "Jesse F. d'Almeida", "Robert J. Webster III", "Ipek Oguz", "Jie Ying Wu"], "title": "Semantic 3D Reconstructions with SLAM for Central Airway Obstruction", "comment": "5 pages, 2 figures, 1 table", "summary": "Central airway obstruction (CAO) is a life-threatening condition with\nincreasing incidence, caused by tumors in and outside of the airway.\nTraditional treatment methods such as bronchoscopy and electrocautery can be\nused to remove the tumor completely; however, these methods carry a high risk\nof complications. Recent advances allow robotic interventions with lesser risk.\nThe combination of robot interventions with scene understanding and mapping\nalso opens up the possibilities for automation. We present a novel pipeline\nthat enables real-time, semantically informed 3D reconstructions of the central\nairway using monocular endoscopic video.\n  Our approach combines DROID-SLAM with a segmentation model trained to\nidentify obstructive tissues. The SLAM module reconstructs the 3D geometry of\nthe airway in real time, while the segmentation masks guide the annotation of\nobstruction regions within the reconstructed point cloud. To validate our\npipeline, we evaluate the reconstruction quality using ex vivo models.\n  Qualitative and quantitative results show high similarity between ground\ntruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By\nintegrating segmentation directly into the SLAM workflow, our system produces\nannotated 3D maps that highlight clinically relevant regions in real time.\nHigh-speed capabilities of the pipeline allows quicker reconstructions compared\nto previous work, reflecting the surgical scene more accurately.\n  To the best of our knowledge, this is the first work to integrate semantic\nsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our\nframework is modular and can generalize to other anatomies or procedures with\nminimal changes, offering a promising step toward autonomous robotic\ninterventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DROID-SLAM\u548c\u8bed\u4e49\u5206\u5272\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u4e2d\u5fc3\u6c14\u9053\u963b\u585e\uff08CAO\uff09\u8fdb\u884c\u5b9e\u65f6\u3001\u8bed\u4e49\u611f\u77e5\u7684\u4e09\u7ef4\u91cd\u5efa\u3002", "motivation": "\u4e2d\u5fc3\u6c14\u9053\u963b\u585e\uff08CAO\uff09\u7684\u53d1\u751f\u7387\u4e0d\u65ad\u589e\u52a0\uff0c\u4f20\u7edf\u6cbb\u7597\u65b9\u6cd5\u98ce\u9669\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5b89\u5168\u6709\u6548\u7684\u6cbb\u7597\u65b9\u6cd5\uff0c\u673a\u5668\u4eba\u5e72\u9884\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u6620\u5c04\u4e3a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "\u7ed3\u5408DROID-SLAM\u548c\u8bad\u7ec3\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u5b9e\u65f6\u91cd\u5efa\u6c14\u9053\u7684\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8bc6\u522b\u548c\u6807\u6ce8\u963b\u585e\u533a\u57df\u3002", "result": "\u4f7f\u7528\u79bb\u4f53\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u91cd\u5efa\u8d28\u91cf\u9ad8\uff0c\u4e0eCT\u626b\u63cf\u9ad8\u5ea6\u76f8\u4f3c\uff08Chamfer\u8ddd\u79bb\u4e3a0.62 mm\uff09\uff0c\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u9ad8\u4eae\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u7684\u5e26\u6ce8\u91ca\u4e09\u7ef4\u5730\u56fe\uff0c\u5e76\u4e14\u91cd\u5efa\u901f\u5ea6\u5feb\uff0c\u6bd4\u5148\u524d\u7684\u5de5\u4f5c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u624b\u672f\u573a\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u8bed\u4e49\u5206\u5272\u4e0e\u5b9e\u65f6\u5355\u773cSLAM\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5185\u7aa5\u955cCAO\u573a\u666f\uff0c\u4e3a\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6b65\u9aa4\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u5177\u6709\u6a21\u5757\u5316\u548c\u53ef\u63a8\u5e7f\u6027\u3002"}}
{"id": "2509.13636", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13636", "abs": "https://arxiv.org/abs/2509.13636", "authors": ["Yasin Hasanpoor", "Bahram Tarvirdizadeh", "Khalil Alipour", "Mohammad Ghamari"], "title": "Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images", "comment": "14 pages 7 images 2 tables", "summary": "This study introduces a novel method that transforms multimodal physiological\nsignalsphotoplethysmography (PPG), galvanic skin response (GSR), and\nacceleration (ACC) into 2D image matrices to enhance stress detection using\nconvolutional neural networks (CNNs). Unlike traditional approaches that\nprocess these signals separately or rely on fixed encodings, our technique\nfuses them into structured image representations that enable CNNs to capture\ntemporal and cross signal dependencies more effectively. This image based\ntransformation not only improves interpretability but also serves as a robust\nform of data augmentation. To further enhance generalization and model\nrobustness, we systematically reorganize the fused signals into multiple\nformats, combining them in a multi stage training pipeline. This approach\nsignificantly boosts classification performance. While demonstrated here in the\ncontext of stress detection, the proposed method is broadly applicable to any\ndomain involving multimodal physiological signals, paving the way for more\naccurate, personalized, and real time health monitoring through wearable\ntechnologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5c06PPG\u3001GSR\u548cACC\u7b49\u751f\u7406\u4fe1\u53f7\u8f6c\u6362\u4e3a2D\u56fe\u50cf\u77e9\u9635\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528CNN\u8fdb\u884c\u538b\u529b\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u751f\u7406\u4fe1\u53f7\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u9ad8\u538b\u529b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5c06PPG\u3001GSR\u548cACC\u4fe1\u53f7\u878d\u5408\u62102D\u56fe\u50cf\u77e9\u9635\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u6c34\u7ebf\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u6709\u6548\u7528\u4e8e\u538b\u529b\u68c0\u6d4b\uff0c\u8fd8\u80fd\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6d89\u53ca\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u7684\u9886\u57df\uff0c\u4e3a\u53ef\u7a7f\u6234\u6280\u672f\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u4e2a\u6027\u5316\u548c\u5b9e\u65f6\u7684\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.13975", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13975", "abs": "https://arxiv.org/abs/2509.13975", "authors": ["Ilker Bayram"], "title": "Classification Filtering", "comment": null, "summary": "We consider a streaming signal in which each sample is linked to a latent\nclass. We assume that multiple classifiers are available, each providing class\nprobabilities with varying degrees of accuracy. These classifiers are employed\nfollowing a straightforward and fixed policy. In this setting, we consider the\nproblem of fusing the output of the classifiers while incorporating the\ntemporal aspect to improve classification accuracy. We propose a state-space\nmodel and develop a filter tailored for realtime execution. We demonstrate the\neffectiveness of the proposed filter in an activity classification application\nbased on inertial measurement unit (IMU) data from a wearable device.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u591a\u5206\u7c7b\u5668\u8f93\u51fa\u5e76\u8003\u8651\u65f6\u5e8f\u4fe1\u606f\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u6d41\u5f0f\u4fe1\u53f7\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u5b9e\u65f6\u5904\u7406\u7684\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u5728IMU\u6d3b\u52a8\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u5668\u5728\u5904\u7406\u6d41\u5f0f\u4fe1\u53f7\u65f6\uff0c\u5355\u72ec\u4f7f\u7528\u65f6\u51c6\u786e\u7387\u6709\u9650\uff0c\u5e76\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u4fe1\u53f7\u7684\u65f6\u95f4\u76f8\u5173\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u878d\u5408\u591a\u4e2a\u5206\u7c7b\u5668\u8f93\u51fa\u5e76\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e00\u4e2a\u4e3a\u5b9e\u65f6\u5904\u7406\u5b9a\u5236\u7684\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u878d\u5408\u591a\u4e2a\u5206\u7c7b\u5668\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u7eb3\u5165\u65f6\u95f4\u4fe1\u606f\u4ee5\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u5728\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6570\u636e\u7684\u6d3b\u52a8\u5206\u7c7b\u5e94\u7528\u4e2d\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6ee4\u6ce2\u5668\u80fd\u6709\u6548\u63d0\u9ad8\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u5b9a\u5236\u6ee4\u6ce2\u5668\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u591a\u4e2a\u5206\u7c7b\u5668\u7684\u8f93\u51fa\uff0c\u5e76\u5229\u7528\u6d41\u5f0f\u4fe1\u53f7\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u5728IMU\u6d3b\u52a8\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13379", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e8616\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u66f4\u5927\u7684\u6a21\u578b\u901a\u5e38\u5177\u6709\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u8fd9\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u5b83\u5bf9\u4e8e\u53ef\u9760\u8bc4\u4f30\u591a\u6a21\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bc4\u4f3016\u4e2a\u5148\u8fdb\u7684VLMs\uff08\u5f00\u6e90\u548c\u95ed\u6e90\uff09\uff0c\u4f7f\u75283\u79cd\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u57286\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5927\u7684\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\u3002\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u73b0\u666e\u904d\u4e0d\u5982\u5176\u4ed6\u9886\u57df\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6570\u636e\u91cf\u5de8\u5927\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u91c7\u7528\u5747\u5300\u6807\u91cf\u91cf\u5316\uff08USQ\uff09\u3002\u672c\u6587\u63d0\u51fa\u7528\u683c\u5411\u91cf\u91cf\u5316\uff08LVQ\uff09\u66ff\u4ee3USQ\uff0c\u5e76\u9488\u5bf9\u573a\u666f\u4f18\u5316\u683c\u57fa\uff0c\u5b9e\u73b0\u573a\u666f\u81ea\u9002\u5e94LVQ\uff08SALVQ\uff09\uff0c\u5728\u4e0d\u663e\u8457\u589e\u52a0\u5f00\u9500\u548c\u590d\u6742\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u4e863DGS\u7684\u538b\u7f29\u7387-\u5931\u771f\uff08R-D\uff09\u6027\u80fd\u3002SALVQ\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u7f29\u653e\u683c\u57fa\u5411\u91cf\u52a8\u6001\u8c03\u6574\u538b\u7f29\u7387\uff0c\u65e0\u9700\u4e3a\u4e0d\u540c\u6bd4\u7279\u7387\u8bad\u7ec3\u591a\u6a21\u578b\u3002", "motivation": "\u73b0\u67093DGS\u538b\u7f29\u65b9\u6cd5\u4f9d\u8d56\u5747\u5300\u6807\u91cf\u91cf\u5316\uff08USQ\uff09\uff0c\u4f46\u66f4\u590d\u6742\u7684\u91cf\u5316\u5668\u53ef\u80fd\u5728\u6781\u5c0f\u7684\u989d\u5916\u5f00\u9500\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06USQ\u66ff\u6362\u4e3a\u683c\u5411\u91cf\u91cf\u5316\uff08LVQ\uff09\uff0c\u5e76\u9488\u5bf9\u6bcf\u4e2a\u573a\u666f\u4f18\u5316\u683c\u57fa\uff0c\u5f62\u6210\u573a\u666f\u81ea\u9002\u5e94LVQ\uff08SALVQ\uff09\u3002SALVQ\u901a\u8fc7\u7f29\u653e\u683c\u57fa\u5411\u91cf\u53ef\u52a8\u6001\u8c03\u6574\u91cf\u5316\u5668\u5bc6\u5ea6\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6bd4\u7279\u7387\u3002", "result": "SALVQ\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u4e86\u73b0\u67093DGS\u538b\u7f29\u65b9\u6cd5\u7684R-D\u6548\u7387\uff0c\u5e76\u80fd\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u9002\u5e94\u591a\u6bd4\u7279\u7387\u76ee\u6807\uff0c\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "\u573a\u666f\u81ea\u9002\u5e94LVQ\uff08SALVQ\uff09\u662f\u4e00\u79cd\u6709\u6548\u76843DGS\u538b\u7f29\u65b9\u6cd5\uff0c\u5b83\u5728\u7b80\u5355\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u5411\u91cf\u91cf\u5316\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u538b\u7f29\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u8bc6\u522b\u653e\u5c04\u80bf\u7624\u5b66\u9886\u57df\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\u62a5\u544a\u7684\u7b5b\u9009\u5de5\u5177\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u8de8\u673a\u6784\u7684\u6709\u6548\u6027\u3002", "motivation": "\u624b\u52a8\u5ba1\u67e5\u4e8b\u4ef6\u62a5\u544a\u4ee5\u8fdb\u884c\u5b89\u5168\u548c\u8d28\u91cf\u6539\u8fdb\u65e2\u8017\u65f6\u53c8\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u5de5\u5177\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u6765\u81eaInst.\u548cIAEA SAFRON\uff09\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e24\u79cdNLP\u6a21\u578b\uff08SVM\u548cBlueBERT\uff09\u3002\u4ed6\u4eec\u8fd8\u6d4b\u8bd5\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u5728\u4e0d\u540c\u673a\u6784\u6570\u636e\u96c6\u4e4b\u95f4\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u5728Inst.\u6570\u636e\u96c6\u4e0a\uff0cSVM\u548cBlueBERT\u7684\u5206\u7c7b\u6027\u80fd\uff08AUROC\uff09\u5206\u522b\u4e3a0.82\u548c0.81\u3002\u5728\u672a\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\uff0cSF\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u6709\u9650\uff08AUROC\u5206\u522b\u4e3a0.42\u548c0.56\uff09\u3002\u901a\u8fc7\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u7684BlueBERT_TRANSFER\u6a21\u578b\uff0cSF\u6570\u636e\u96c6\u7684\u6027\u80fd\u63d0\u9ad8\u5230AUROC 0.78\u3002\u5728\u7ecf\u8fc7\u4eba\u5de5\u7f16\u8f91\u7684\u6570\u636e\u96c6\u4e0a\uff0cSVM\u548cBlueBERT_TRANSFER\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u8de8\u673a\u6784\u7684NLP\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5730\u4ece\u653e\u5c04\u80bf\u7624\u5b66\u4e8b\u4ef6\u62a5\u544a\u4e2d\u8bc6\u522b\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\uff0c\u5176\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u3002"}}
{"id": "2509.13519", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2509.13519", "abs": "https://arxiv.org/abs/2509.13519", "authors": ["Devjyoti Tripathy", "Juzar Thingna", "Sebastian Deffner"], "title": "Quantum speed limit for the OTOC from an open systems perspective", "comment": "10 pages, 7 figures", "summary": "Scrambling, the delocalization of initially localized quantum information, is\ncommonly characterized by the out-of-time ordered correlator (OTOC). Employing\nthe OTOC-Renyi-2 entropy theorem we derive a quantum speed limit for the OTOC,\nwhich sets an lower bound for the rate with which information can be scrambled.\nThis bound becomes particularly tractable by describing the scrambling of\ninformation in a closed quantum system as an effective decoherence process of\nan open system interacting with an environment. We prove that decay of the OTOC\ncan be bounded by the strength of the system-environment coupling and two-point\nenvironmental correlation functions. We validate our analytic bound numerically\nusing the non-integrable transverse field Ising model. Our results provide a\nuniversal and model-agnostic quantitative framework for understanding the\ndynamical limits of information spreading across quantum many-body physics,\ncondensed matter, and engineered quantum platforms.", "AI": {"tldr": "\u91cf\u5b50\u4fe1\u606f\u5728OTOC\u4e2d\u4ee5\u6307\u6570\u7ea7\u589e\u957f\uff0c\u5e76\u901a\u8fc7\u5bf9OTOC\u8fdb\u884c\u91cd\u6b63\u5316\uff0c\u6211\u4eec\u5f97\u51fa\u4e86\u91cf\u5b50\u901f\u5ea6\u9650\u5236\uff0c\u5b83\u8bbe\u5b9a\u4e86\u4fe1\u606f the rate of scrambling \u7684\u4e0b\u9650\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5c06\u5c01\u95ed\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f scrambling \u63cf\u8ff0\u4e3a\u4e0e\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u7684\u5f00\u653e\u7cfb\u7edf\u7684\u6709\u6548\u9000\u76f8\u5e72\u8fc7\u7a0b\u6765\u89e3\u51b3\u3002", "motivation": "Scrambling\uff08\u91cf\u5b50\u4fe1\u606f\u4ece\u5c40\u90e8\u72b6\u6001\u5230\u79bb\u57df\u72b6\u6001\u7684\u6269\u6563\uff09\u901a\u5e38\u7528OTOC\uff08out-of-time ordered correlator\uff09\u6765\u8868\u5f81\u3002OTOC-Renyi-2\u71b5\u5b9a\u7406\u88ab\u7528\u6765\u63a8\u5bfcOTOC\u7684\u91cf\u5b50\u901f\u5ea6\u9650\u5236\uff0c\u4ece\u800c\u4e3a\u4fe1\u606f scrambling \u7684\u901f\u7387\u8bbe\u5b9a\u4e00\u4e2a\u4e0b\u9650\u3002", "method": "\u5c06\u5c01\u95ed\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f scrambling \u8fc7\u7a0b\uff0c\u7b49\u6548\u5730\u63cf\u8ff0\u4e3a\u4e0e\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u7684\u5f00\u653e\u7cfb\u7edf\u4e2d\u7684\u9000\u76f8\u5e72\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528OTOC-Renyi-2\u71b5\u5b9a\u7406\u63a8\u5bfcOTOC\u7684\u91cf\u5b50\u901f\u5ea6\u9650\u5236\u3002", "result": "OTOC\u7684\u8870\u51cf\u53ef\u4ee5\u88ab\u7cfb\u7edf-\u73af\u5883\u8026\u5408\u5f3a\u5ea6\u548c\u4e24\u70b9\u73af\u5883\u5173\u8054\u51fd\u6570\u6240\u9650\u5236\u3002\u901a\u8fc7\u5bf9\u975e\u79ef\u5206\u5f0f\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\uff0c\u5f97\u51fa\u4e86\u5206\u6790\u8fb9\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5b9a\u91cf\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u91cf\u5b50\u591a\u4f53\u7269\u7406\u3001\u51dd\u805a\u6001\u7269\u7406\u548c\u5de5\u7a0b\u91cf\u5b50\u5e73\u53f0\u4e2d\u4fe1\u606f\u4f20\u64ad\u7684\u52a8\u529b\u5b66\u6781\u9650\u3002"}}
{"id": "2509.13744", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13744", "abs": "https://arxiv.org/abs/2509.13744", "authors": ["Aslam Shaikh", "Tero M\u00e4kinen", "Fran\u00e7ois Vurpillot", "Mikko Alava", "Ivan Lomakin"], "title": "Stochastic ion emission perturbation mechanisms in atom probe tomography: Linking simulations to experiment", "comment": null, "summary": "Field evaporation in atom probe tomography (APT) includes known processes\nrelated to surface migration of atoms, such as the so-called roll-up mechanism.\nThey lead to trajectory aberrations and artefacts on the detector. These\nprocesses are usually neglected in simulations. The inclusion of such processes\nis crucial for providing reliable models for the development and verification\nof APT reconstruction algorithms, a key part of the whole methodology. Here we\ninclude stochastic lateral velocity perturbations and a roll-up mechanism to\nsimulations performed using the Robin--Rolland model. By comparing with\nexperimental data from Al and Ni systems, we find the stochastic perturbation\nenergy distributions that allow us to very accurately reproduce the detector\npatterns seen experimentally and thus greatly improve the accuracy of the\nsimulations. We also explore the possible causes of remaining discrepancies\nbetween the experimental and simulated detector patterns.", "AI": {"tldr": "\u4eff\u771f\u4e2d\u52a0\u5165\u8868\u9762\u8fc1\u79fb\u673a\u5236\u4ee5\u63d0\u9ad8\u539f\u5b50\u63a2\u9488\u65ad\u5c42\u626b\u63cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u539f\u5b50\u63a2\u9488\u65ad\u5c42\u626b\u63cf\uff08APT\uff09\u4e2d\u7684\u573a\u84b8\u53d1\u8fc7\u7a0b\u5305\u542b\u4e0e\u539f\u5b50\u8868\u9762\u8fc1\u79fb\u76f8\u5173\u7684\u5df2\u77e5\u673a\u5236\uff0c\u5982\u201c\u5377\u8d77\u201d\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u4f1a\u5bfc\u81f4\u8f68\u8ff9\u7578\u53d8\u548c\u63a2\u6d4b\u5668\u4f2a\u5f71\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u901a\u5e38\u5728\u6a21\u62df\u4e2d\u88ab\u5ffd\u7565\u3002\u4e3a\u4e86\u5f00\u53d1\u548c\u9a8c\u8bc1APT\u91cd\u5efa\u7b97\u6cd5\uff08\u8fd9\u662f\u6574\u4e2a\u65b9\u6cd5\u5b66\u7684\u5173\u952e\u90e8\u5206\uff09\u63d0\u4f9b\u53ef\u9760\u7684\u6a21\u578b\uff0c\u5305\u542b\u8fd9\u4e9b\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u968f\u673a\u6a2a\u5411\u901f\u5ea6\u6270\u52a8\u548c\u5377\u8d77\u673a\u5236\u52a0\u5165\u5230Robin-Rolland\u6a21\u578b\u6a21\u62df\u4e2d\uff0c\u5e76\u4e0eAl\u548cNi\u7cfb\u7edf\u7684\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\u6570\u636e\uff0c\u627e\u5230\u4e86\u80fd\u591f\u975e\u5e38\u7cbe\u786e\u5730\u91cd\u73b0\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u7684\u63a2\u6d4b\u5668\u56fe\u6848\u7684\u968f\u673a\u6270\u52a8\u80fd\u91cf\u5206\u5e03\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u4e86\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u52a0\u5165\u8868\u9762\u8fc1\u79fb\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8APT\u6a21\u62df\u7684\u51c6\u786e\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u63a2\u7a76\u5b9e\u9a8c\u4e0e\u6a21\u62df\u4e4b\u95f4\u5269\u4f59\u5dee\u5f02\u7684\u53ef\u80fd\u539f\u56e0\u3002"}}
{"id": "2509.13572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13572", "abs": "https://arxiv.org/abs/2509.13572", "authors": ["Ozan Karaali", "Hossam Farag", "Strahinja Dosen", "Cedomir Stefanovic"], "title": "Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference", "comment": "ICAT 2025", "summary": "This study examines the potential of utilizing Vision Language Models (VLMs)\nto improve the perceptual capabilities of semi-autonomous prosthetic hands. We\nintroduce a unified benchmark for end-to-end perception and grasp inference,\nevaluating a single VLM to perform tasks that traditionally require complex\npipelines with separate modules for object detection, pose estimation, and\ngrasp planning. To establish the feasibility and current limitations of this\napproach, we benchmark eight contemporary VLMs on their ability to perform a\nunified task essential for bionic grasping. From a single static image, they\nshould (1) identify common objects and their key properties (name, shape,\norientation, and dimensions), and (2) infer appropriate grasp parameters (grasp\ntype, wrist rotation, hand aperture, and number of fingers). A corresponding\nprompt requesting a structured JSON output was employed with a dataset of 34\nsnapshots of common objects. Key performance metrics, including accuracy for\ncategorical attributes (e.g., object name, shape) and errors in numerical\nestimates (e.g., dimensions, hand aperture), along with latency and cost, were\nanalyzed. The results demonstrated that most models exhibited high performance\nin object identification and shape recognition, while accuracy in estimating\ndimensions and inferring optimal grasp parameters, particularly hand rotation\nand aperture, varied more significantly. This work highlights the current\ncapabilities and limitations of VLMs as advanced perceptual modules for\nsemi-autonomous control of bionic limbs, demonstrating their potential for\neffective prosthetic applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u589e\u5f3a\u534a\u81ea\u4e3b\u5047\u80a2\u624b\u611f\u77e5\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u7aef\u5230\u7aef\u7684\u611f\u77e5\u548c\u6293\u53d6\u63a8\u7406\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u516b\u4e2a\u73b0\u4ee3VLM\u5728\u6267\u884c\u7edf\u4e00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8be5\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u4ece\u5355\u4e2a\u9759\u6001\u56fe\u50cf\u4e2d\u8bc6\u522b\u7269\u4f53\u53ca\u5176\u5c5e\u6027\uff0c\u5e76\u63a8\u65ad\u5408\u9002\u7684\u6293\u53d6\u53c2\u6570\u3002", "motivation": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u5347\u534a\u81ea\u4e3b\u5047\u80a2\u624b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u7b80\u5316\u4f20\u7edf\u7684\u590d\u6742\u5904\u7406\u6d41\u7a0b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u5305\u542b\u7269\u4f53\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u6293\u53d6\u89c4\u5212\uff0c\u5e76\u8bc4\u4f30\u4e86\u516b\u4e2aVLM\u5728\u8bc6\u522b\u7269\u4f53\u5c5e\u6027\uff08\u540d\u79f0\u3001\u5f62\u72b6\u3001\u65b9\u5411\u3001\u5c3a\u5bf8\uff09\u548c\u63a8\u65ad\u6293\u53d6\u53c2\u6570\uff08\u6293\u53d6\u7c7b\u578b\u3001\u8155\u90e8\u65cb\u8f6c\u3001\u624b\u90e8\u5f20\u5f00\u5ea6\u3001\u624b\u6307\u6570\u91cf\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002\u4f7f\u7528\u5305\u542b34\u5f20\u5e38\u89c1\u7269\u4f53\u5feb\u7167\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316JSON\u8f93\u51fa\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u5f62\u72b6\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5c3a\u5bf8\u4f30\u8ba1\u548c\u6293\u53d6\u53c2\u6570\u63a8\u65ad\uff08\u7279\u522b\u662f\u624b\u90e8\u65cb\u8f6c\u548c\u5f20\u5f00\u5ea6\uff09\u65b9\u9762\uff0c\u51c6\u786e\u6027\u5dee\u5f02\u8f83\u5927\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cVLM\u5728\u7269\u4f53\u8bc6\u522b\u548c\u5f62\u72b6\u8bc6\u522b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u7cbe\u786e\u7684\u5c3a\u5bf8\u4f30\u8ba1\u548c\u6293\u53d6\u53c2\u6570\u63a8\u65ad\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u63ed\u793a\u4e86VLM\u4f5c\u4e3a\u5148\u8fdb\u611f\u77e5\u6a21\u5757\u5728\u534a\u81ea\u4e3b\u63a7\u5236\u5047\u80a2\u65b9\u9762\u7684\u5f53\u524d\u80fd\u529b\u548c\u6f5c\u5728\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.13642", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13642", "abs": "https://arxiv.org/abs/2509.13642", "authors": ["Zirun Guo", "Feng Zhang", "Kai Jia", "Tao Jin"], "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators", "comment": null, "summary": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\nare limited to synthetic imagery and struggle with tasks requiring factual\ngrounding or programmatic precision. Our framework empowers a central LLM or\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\ntools, including online image search, diffusion-based generation, code\nexecution, and image editing. The agent is trained to select and apply these\ntools proficiently via a Reinforcement Learning (RL) framework that features a\nhybrid reward system combining rule-based logic with judgments from LLM and\nMLLM evaluators. Trained on a diverse new dataset using four different model\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\nexisting methods by a large margin across four benchmarks. We also introduce a\nnovel test-time scaling strategy that provides further performance gains.\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.", "AI": {"tldr": "LLM-Interleaved (LLM-I)\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5c06\u56fe\u50cf-\u6587\u672c\u751f\u6210\u91cd\u6784\u4e3a\u5de5\u5177\u4f7f\u7528\u95ee\u9898\uff0c\u514b\u670d\u4e86\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u201c\u5355\u4e00\u5de5\u5177\u201d\u7684\u9650\u5236\uff0c\u80fd\u591f\u5904\u7406\u9700\u8981\u4e8b\u5b9e\u4f9d\u636e\u6216\u7a0b\u5e8f\u7cbe\u5ea6\u7684\u95ee\u9898\u3002\u5b83\u4f7f\u4e00\u4e2a\u4e2d\u5fc3LLM/MLLM\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u5728\u7ebf\u56fe\u50cf\u641c\u7d22\u3001\u6269\u6563\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4e13\u4e1a\u89c6\u89c9\u5de5\u5177\u6765\u5b8c\u6210\u4efb\u52a1\u3002\u8be5\u4ee3\u7406\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u903b\u8f91\u4ee5\u53caLLM/MLLM\u8bc4\u4f30\u8005\u7684\u5224\u65ad\u7684\u6df7\u5408\u5956\u52b1\u7cfb\u7edf\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002LLM-I\u5728\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u7b56\u7565\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u4e8b\u5b9e\u4f9d\u636e\u6216\u7a0b\u5e8f\u7cbe\u5ea6\u7684\u56fe\u50cf-\u6587\u672c\u751f\u6210\u4efb\u52a1\u65f6\u5b58\u5728\u201c\u5355\u4e00\u5de5\u5177\u201d\u7684\u74f6\u9888\uff0cLLM-I\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u534f\u8c03\u591a\u79cd\u4e13\u4e1a\u89c6\u89c9\u5de5\u5177\u6765\u589e\u5f3a\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u80fd\u529b\u3002", "method": "LLM-I\u6846\u67b6\u5229\u7528\u4e00\u4e2a\u4e2d\u5fc3LLM/MLLM\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\u6765\u9009\u62e9\u548c\u5e94\u7528\u591a\u79cd\u89c6\u89c9\u5de5\u5177\uff08\u5982\u5728\u7ebf\u56fe\u50cf\u641c\u7d22\u3001\u6269\u6563\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u3001\u56fe\u50cf\u7f16\u8f91\uff09\u3002\u8be5RL\u6846\u67b6\u91c7\u7528\u6df7\u5408\u5956\u52b1\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u903b\u8f91\u548cLLM/MLLM\u8bc4\u4f30\u8005\u7684\u5224\u65ad\u3002", "result": "LLM-I\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u7b56\u7565\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "LLM-I\u6846\u67b6\u901a\u8fc7\u5c06\u56fe\u50cf-\u6587\u672c\u751f\u6210\u89c6\u4e3a\u5de5\u5177\u4f7f\u7528\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u548c\u591a\u6837\u5316\u7684\u89c6\u89c9\u5de5\u5177\u96c6\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13984", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.13984", "abs": "https://arxiv.org/abs/2509.13984", "authors": ["Drake Silbernagel", "Yu Rong", "Isabella Lenz", "Prithvi Hemanth", "Carl Morgenstern", "Owen Ma", "Nolan Matthews", "Nader Zaki", "Kyle W. Martin", "John D. Elgin", "Jacob Holtom", "Daniel W. Bliss", "Kimberly Frey"], "title": "Distributed Coherent Beamforming at 60 GHz Enabled by Optically-Established Coherence", "comment": null, "summary": "We implement and experimentally demonstrate a 60 GHz distributed system\nleveraging an optical time synchronization system that provides precise time\nand frequency alignment between independent elements of the distributed mesh.\nUtilizing such accurate coherence, we perform receive beamforming with\ninterference rejection and transmit nulling. In these configurations, the\nsystem achieves a coherent gain over an incoherent network of N nodes,\nsignificantly improving the relevant signal power ratios. Our system\ndemonstrates extended array phase coherence times, enabling advanced\ntechniques. Results from over-the-air experiments demonstrate a 14.3 dB\nsignal-to-interference-plus-noise improvement in interference-laden scenarios\nwith a contributing 13.5 dB null towards interference in receive beamforming.\nIn transmit nulling, a signal-to-noise ratio (SNR) gain of 7.9 dB is measured\ntowards an intended receiver while maintaining an SNR reduction of 8.9 dB at\nanother receiver. These findings represent the use of distributed coherence in\nthe V band without the use of GPS timing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5229\u7528\u5149\u5b66\u65f6\u95f4\u540c\u6b65\u768460 GHz\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u65f6\u95f4\u548c\u9891\u7387\u5bf9\u9f50\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u5e72\u6270\u6291\u5236\u7684\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u548c\u53d1\u5c04\u96f6\u9677\u3002\u8be5\u7cfb\u7edf\u5728\u5b58\u5728\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\uff0c\u4fe1\u53f7\u5230\u5e72\u6270\u52a0\u566a\u58f0\u7684\u529f\u7387\u6bd4\u63d0\u9ad8\u4e8614.3 dB\uff0c\u53d1\u5c04\u96f6\u9677\u5b9e\u73b0\u4e867.9 dB\u7684\u4fe1\u566a\u6bd4\u589e\u76ca\u3002", "motivation": "\u4e3a\u4e86\u5728\u6ca1\u6709GPS\u7684\u60c5\u51b5\u4e0b\uff0c\u5728V\u6ce2\u6bb5\u5229\u7528\u5206\u5e03\u5f0f\u76f8\u5e72\u6027\u5b9e\u73b0\u7cbe\u786e\u7684\u65f6\u95f4\u548c\u9891\u7387\u5bf9\u9f50\uff0c\u4ee5\u652f\u6301\u9ad8\u7ea7\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u6280\u672f\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u4e2a60 GHz\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u5149\u5b66\u65f6\u95f4\u540c\u6b65\u7cfb\u7edf\u8fdb\u884c\u7cbe\u786e\u7684\u65f6\u95f4\u548c\u9891\u7387\u5bf9\u9f50\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u548c\u53d1\u5c04\u96f6\u9677\u3002", "result": "\u5728\u5e72\u6270\u73af\u5883\u4e0b\uff0c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u5b9e\u73b0\u4e8614.3 dB\u7684\u4fe1\u53f7\u5230\u5e72\u6270\u52a0\u566a\u58f0\u7684\u529f\u7387\u6bd4\u7684\u63d0\u5347\uff0c\u5e76\u4e14\u5bf9\u5e72\u6270\u4fe1\u53f7\u670913.5 dB\u7684\u6291\u5236\u3002\u5728\u53d1\u5c04\u96f6\u9677\u4e2d\uff0c\u5b9e\u73b0\u4e867.9 dB\u7684\u4fe1\u566a\u6bd4\u589e\u76ca\uff0c\u540c\u65f6\u5bf9\u53e6\u4e00\u4e2a\u63a5\u6536\u5668\u7684\u4fe1\u566a\u6bd4\u964d\u4f4e\u4e868.9 dB\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86\u5728\u6ca1\u6709GPS\u7684\u60c5\u51b5\u4e0b\uff0c\u5728V\u6ce2\u6bb5\u5229\u7528\u5206\u5e03\u5f0f\u76f8\u5e72\u6027\u8fdb\u884c\u7cbe\u786e\u7684\u65f6\u95f4\u548c\u9891\u7387\u5bf9\u9f50\uff0c\u5e76\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u5206\u5e03\u5f0f\u6ce2\u675f\u6210\u5f62\u548c\u96f6\u9677\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u53f7\u8d28\u91cf\u3002"}}
{"id": "2509.13674", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13674", "abs": "https://arxiv.org/abs/2509.13674", "authors": ["Yuezhang He", "Hongxi Luo", "Yuancheng Lin", "Carl J. Talsma", "Anna Li", "Zhenqian Wang", "Yujuan Fang", "Pei Liu", "Jesse D. Jenkins", "Eric Larson", "Zheng Li"], "title": "Scaling green hydrogen and CCUS via cement-methanol co-production in China", "comment": null, "summary": "High costs of green hydrogen and of carbon capture, utilization, and\nsequestration (CCUS) have hindered policy ambition and slowed real-world\ndeployment, despite their importance for decarbonizing hard-to-abate sectors,\nincluding cement and methanol. Given the economic challenges of adopting CCUS\nin cement and green hydrogen in methanol production separately, we propose a\nrenewable-powered co-production system that couples electrolytic hydrogen and\nCCUS through molecule exchange. We optimize system configurations using an\nhourly-resolved, process-based model incorporating operational flexibility, and\nexplore integrated strategies for plant-level deployment and CO2 source-sink\nmatching across China. We find that co-production could reduce CO2 abatement\ncosts to USD 41-53 per tonne by 2035, significantly lower than approximately\nUSD 75 for standalone cement CCUS and over USD 120 for standalone\nrenewable-based methanol. Co-production is preferentially deployed at cement\nplants in renewable-rich regions, potentially reshaping national CO2\ninfrastructure planning. This hydrogen-CCUS coupling paradigm could accelerate\nindustrial decarbonization and scaling for other applications.", "AI": {"tldr": "\u901a\u8fc7\u8026\u5408\u7535\u89e3\u6c22\u548c\u78b3\u6355\u83b7\u3001\u5229\u7528\u548c\u5c01\u5b58\uff08CCUS\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u518d\u751f\u80fd\u6e90\u9a71\u52a8\u7684\u8054\u4ea7\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u6602\u7684\u7eff\u8272\u6c22\u548cCCUS\u6210\u672c\uff0c\u5e76\u4e3a\u6c34\u6ce5\u548c\u7532\u9187\u7b49\u96be\u4ee5\u51cf\u6392\u7684\u884c\u4e1a\u63d0\u4f9b\u8131\u78b3\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u9ad8\u6602\u7684\u7eff\u8272\u6c22\u548c\u78b3\u6355\u83b7\u3001\u5229\u7528\u548c\u5c01\u5b58\uff08CCUS\uff09\u6210\u672c\u963b\u788d\u4e86\u653f\u7b56\u7684\u5b9e\u65bd\u548c\u5b9e\u9645\u90e8\u7f72\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5bf9\u4e8e\u6c34\u6ce5\u548c\u7532\u9187\u7b49\u96be\u4ee5\u51cf\u6392\u884c\u4e1a\u7684\u8131\u78b3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u518d\u751f\u80fd\u6e90\u9a71\u52a8\u7684\u8054\u4ea7\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5b50\u4ea4\u6362\u8026\u5408\u7535\u89e3\u6c22\u548cCCUS\u3002\u4f7f\u7528\u8003\u8651\u8fd0\u884c\u7075\u6d3b\u6027\u7684\u9010\u65f6\u6a21\u578b\u4f18\u5316\u7cfb\u7edf\u914d\u7f6e\uff0c\u5e76\u63a2\u7d22\u5728\u4e2d\u56fd\u90e8\u7f72\u548cCO2\u6e90-\u6c47\u5339\u914d\u7684\u96c6\u6210\u7b56\u7565\u3002", "result": "\u8054\u4ea7\u7cfb\u7edf\u52302035\u5e74\u53ef\u4ee5\u5c06CO2\u51cf\u6392\u6210\u672c\u964d\u4f4e\u5230\u6bcf\u542841-53\u7f8e\u5143\uff0c\u663e\u8457\u4f4e\u4e8e\u5355\u72ec\u7684\u6c34\u6ce5CCUS\uff08\u7ea675\u7f8e\u5143\uff09\u548c\u5355\u72ec\u7684\u53ef\u518d\u751f\u80fd\u6e90\u7532\u9187\uff08\u8d85\u8fc7120\u7f8e\u5143\uff09\u3002\u8054\u4ea7\u4f18\u5148\u90e8\u7f72\u5728\u53ef\u518d\u751f\u80fd\u6e90\u4e30\u5bcc\u7684\u5730\u533a\u7684\u6c34\u6ce5\u5382\u3002", "conclusion": "\u8fd9\u79cd\u6c22-CCUS\u8026\u5408\u6a21\u5f0f\u53ef\u4ee5\u52a0\u901f\u5de5\u4e1a\u8131\u78b3\uff0c\u5e76\u4e3a\u5176\u4ed6\u5e94\u7528\u8fdb\u884c\u6269\u5c55\uff0c\u540c\u65f6\u901a\u8fc7\u5c06\u8054\u4ea7\u4f18\u5148\u90e8\u7f72\u5728\u53ef\u518d\u751f\u80fd\u6e90\u4e30\u5bcc\u7684\u5730\u533a\uff0c\u53ef\u80fd\u91cd\u5851\u4e2d\u56fd\u7684CO2\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3002"}}
{"id": "2509.13389", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos N\u00fa\u00f1ez-Molina", "Vicen\u00e7 G\u00f3mez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\uff08Transformer\uff09\u548c\u68af\u5ea6\u4e0b\u964d\uff0c\u4ece\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5f0f\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u95ee\u9898\u89e3\u51b3\uff0c\u5e76\u8bc1\u660e\u4e86Transformer\u80fd\u591f\u51c6\u786e\u8868\u793a\u4e16\u754c\u6a21\u578b\uff0c\u4e14\u53ef\u4ece\u6b63\u53cd\u4f8b\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u3002", "motivation": "\u4ece\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u76d1\u7763\u5f0f\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528Transformer\u67b6\u6784\uff0c\u5e76\u4ece\u6b63\u3001\u53cd\u4f8b\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u3002", "result": "\u8bc1\u660e\u4e86Transformer\u67b6\u6784\u80fd\u591f\u51c6\u786e\u8868\u793a\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u53ef\u4ece\u4e2d\u5b66\u4e60\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86MINGLE\u6a21\u578b\u6765\u8bc6\u522b\u548c\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\u7fa4\u4f53\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b100K\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\u6765\u652f\u6301\u8be5\u4efb\u52a1\u3002", "motivation": "\u7406\u89e3\u516c\u5171\u573a\u6240\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u5145\u6ee1\u6d3b\u529b\u548c\u5305\u5bb9\u6027\u7684\u73af\u5883\u3002\u4ece\u56fe\u50cf\u4e2d\u68c0\u6d4b\u8fd9\u4e9b\u4e92\u52a8\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u5982\u5173\u7cfb\u3001\u90bb\u8fd1\u6027\u548c\u5171\u540c\u79fb\u52a8\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u6bd4\u4f20\u7edf\u7684\u76ee\u6807\u68c0\u6d4b\u66f4\u590d\u6742\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMINGLE\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u6a21\u578b\uff0c\u96c6\u6210\u4e86\uff1a1\uff09\u73b0\u6210\u7684\u4eba\u4f53\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\uff1b2\uff09\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u6765\u5206\u7c7b\u6210\u5bf9\u7684\u793e\u4ea4\u5173\u7cfb\uff1b3\uff09\u8f7b\u91cf\u7ea7\u7684\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u6765\u5b9a\u4f4d\u793e\u4ea4\u8fde\u63a5\u7684\u7fa4\u4f53\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b100K\u5f20\u57ce\u5e02\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6807\u6ce8\u4e86\u4e2a\u4f53\u548c\u793e\u4ea4\u4e92\u52a8\u7fa4\u4f53\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\u3002\u8fd9\u4e9b\u6807\u6ce8\u7ed3\u5408\u4e86\u4eba\u5de5\u6807\u6ce8\u548cMINGLE\u6a21\u578b\u7684\u8f93\u51fa\u3002", "conclusion": "MINGLE\u6a21\u578b\u901a\u8fc7\u6574\u5408\u4eba\u4f53\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u3001\u57fa\u4e8eVLM\u7684\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b\u548c\u7a7a\u95f4\u805a\u5408\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\u68c0\u6d4b\u7684\u6311\u6218\u3002\u65b0\u53d1\u5e03\u7684\u6570\u636e\u96c6\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u6765\u89e3\u51b3\u63d0\u793a\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSPC\u7684\u4e24\u9636\u6bb5\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u538b\u7f29\u63d0\u793a\u3002", "motivation": "\u63d0\u793a\u8fc7\u957f\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u73b0\u6709\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u91cf\u3002", "method": "DSPC\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7c97\u7c92\u5ea6\u9636\u6bb5\u901a\u8fc7TF-IDF\u8fc7\u6ee4\u8bed\u4e49\u76f8\u5173\u6027\u4f4e\u7684\u53e5\u5b50\uff1b\u7ec6\u7c92\u5ea6\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u8d21\u732e\u3001\u8de8\u6a21\u578b\u635f\u5931\u5dee\u5f02\u548c\u4f4d\u7f6e\u91cd\u8981\u6027\u8bc4\u4f30\u6807\u8bb0\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4fee\u526a\u4f4e\u6548\u6807\u8bb0\u3002", "result": "\u5728LLaMA-3.1-8B-Instruct\u548cGPT-3.5-Turbo\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0cDSPC\u5728\u6709\u9650\u7684token\u9884\u7b97\u4e0b\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002\u4f8b\u5982\uff0c\u5728Longbench\u6570\u636e\u96c6\u7684FewShot\u4efb\u52a1\u4e2d\uff0cDSPC\u4f7f\u7528\u7684token\u6570\u91cf\u51cf\u5c11\u4e863\u500d\uff0c\u6027\u80fd\u4ecd\u8fbe\u523049.17\uff0c\u6bd4LongLLMLingua\u63d0\u9ad8\u4e867.76\u3002", "conclusion": "DSPC\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u663e\u8457\u51cf\u5c11token\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13528", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2509.13528", "abs": "https://arxiv.org/abs/2509.13528", "authors": ["Elijah Pelofske", "Marek Rams", "Andreas B\u00e4rtschi", "Piotr Czarnik", "Paolo Braccia", "Lukasz Cincio", "Stephan Eidenbenz"], "title": "Evaluating the Limits of QAOA Parameter Transfer at High-Rounds on Sparse Ising Models With Geometrically Local Cubic Terms", "comment": null, "summary": "The emergent practical applicability of the Quantum Approximate Optimization\nAlgorithm (QAOA) for approximate combinatorial optimization is a subject of\nconsiderable interest. One of the primary limitations of QAOA is the task of\nfinding a set of good parameters. Parameter transfer is a phenomenon where QAOA\nangles trained on problem instances that are self-similar tend to perform well\nfor other problem instances from that similar class. This suggests a\npotentially highly efficient and scalable non-variational learning method for\nQAOA angle finding. We systematically study QAOA parameter transferability from\nsmall problems (16, 27 qubits) onto large problem instances (up to 156 qubits)\nfor heavy-hex graph Ising models with geometrically local higher order terms\nusing the Julia based QAOA simulation tool JuliQAOA to perform classical angle\nfinding for up to 49 QAOA layers. Parameter transfer of the fixed angles is\nvalidated using a combination of full statevector, Projected Entangled Pair\nStates, Matrix Product State, and LOWESA numerical simulations. We find that\nthe QAOA parameter transfer from single instances applied to unseen problem\ninstances does not in general provide monotonically improving performance as a\nfunction of p - there are many cases where the performance temporarily\ndecreases as a function of p - but despite this the transferred angles have a\ngeneral trend of improved expectation value as the QAOA depth increases, in\nmany cases converging close to the true ground-state energy of the 100+ qubit\ninstances. We also sample the hardware-compatible Ising models using the\nensemble of fixed QAOA angles on several superconducting qubit IBM Quantum\nprocessors with 127, 133, and 156 qubits. We find continuous solution quality\nimprovement of the hardware-compatible QAOA circuits run on the IBM NISQ\nprocessors up to p=5 on ibm_fez, p=9 on ibm_torino, and p=10 on ibm_pittsburgh.", "AI": {"tldr": "QAOA\u53c2\u6570\u8fc1\u79fb\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5c3d\u7ba1\u6027\u80fd\u968f\u6df1\u5ea6\u589e\u52a0\u5e76\u975e\u5355\u8c03\uff0c\u4f46\u4ecd\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u5e76\u5728\u5b9e\u9645\u786c\u4ef6\u4e0a\u663e\u793a\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002", "motivation": "QAOA\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u5b58\u5728\u53c2\u6570\u5bfb\u627e\u7684\u5c40\u9650\u6027\uff0c\u800c\u53c2\u6570\u8fc1\u79fb\u73b0\u8c61\uff08\u5373\u5728\u76f8\u4f3c\u95ee\u9898\u5b9e\u4f8b\u4e0a\u8bad\u7ec3\u7684\u53c2\u6570\u53ef\u8fc1\u79fb\u5230\u5176\u4ed6\u76f8\u4f3c\u5b9e\u4f8b\uff09\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "method": "\u901a\u8fc7JuliQAOA\u5de5\u5177\uff0c\u7814\u7a76\u4e86QAOA\u53c2\u6570\u4ece\u5c0f\u578b\u95ee\u9898\uff0816-27\u91cf\u5b50\u6bd4\u7279\uff09\u8fc1\u79fb\u5230\u5927\u578b\u95ee\u9898\uff08\u6700\u591a156\u91cf\u5b50\u6bd4\u7279\uff09\u7684\u91cd-hex\u56feIsing\u6a21\u578b\u4e0a\u7684\u8fc1\u79fb\u6027\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u6570\u503c\u6a21\u62df\u65b9\u6cd5\uff08\u5305\u62ec\u5168\u6001\u77e2\u91cf\u3001PEPS\u3001MPS\u548cLOWESA\uff09\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u540c\u65f6\uff0c\u5728IBM\u7684\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u6d4b\u8bd5\u4e86\u8fc1\u79fb\u7684\u56fa\u5b9aQAOA\u53c2\u6570\u5728\u5b9e\u9645\u786c\u4ef6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cQAOA\u53c2\u6570\u8fc1\u79fb\u7684\u6027\u80fd\u968f\u6df1\u5ea6p\u7684\u589e\u52a0\u5e76\u975e\u5355\u8c03\u9012\u589e\uff0c\u4f46\u6574\u4f53\u4e0a\u671f\u671b\u503c\u5448\u73b0\u63d0\u9ad8\u8d8b\u52bf\uff0c\u5e76\u80fd\u63a5\u8fd1\u771f\u5b9e\u57fa\u6001\u80fd\u91cf\u3002\u5728\u5b9e\u9645\u91cf\u5b50\u786c\u4ef6\u4e0a\uff0cQAOA\u53c2\u6570\u8fc1\u79fb\u5f97\u5230\u7684\u89e3\u7684\u8d28\u91cf\u968f\u7740\u6df1\u5ea6p\u7684\u589e\u52a0\u6301\u7eed\u63d0\u9ad8\uff0c\u5728\u4e0d\u540c\u5904\u7406\u5668\u4e0a\u5206\u522b\u5728p=5\u3001p=9\u548cp=10\u65f6\u8fbe\u5230\u6700\u4f73\u3002", "conclusion": "QAOA\u53c2\u6570\u8fc1\u79fb\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u53ef\u6269\u5c55\u7684\u975e\u53d8\u5206\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4e3aQAOA\u5bfb\u627e\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u91cf\u5b50\u786c\u4ef6\u4e0a\u4e5f\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.13798", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.13798", "abs": "https://arxiv.org/abs/2509.13798", "authors": ["Tugbey Kocabas", "Murat Keceli", "Tanju Gurel", "Milorad Milosevic", "Cem Sevik"], "title": "Thermal Conductivity Limits of MoS$_2$ and MoSe$_2$: Revisiting High-Order Anharmonic Lattice Dynamics with Machine Learning Potentials", "comment": null, "summary": "Group-VI transition metal dichalcogenides (TMDs), MoS$_2$ and MoSe$_2$, have\nemerged as prototypical low-dimensional systems with distinctive phononic and\nelectronic properties, making them attractive for applications in\nnanoelectronics, optoelectronics, and thermoelectrics. Yet, their reported\nlattice thermal conductivities ($\\kappa$) remain highly inconsistent, with\nexperimental values and theoretical predictions differing by more than an order\nof magnitude. These discrepancies stem from uncertainties in measurement\ntechniques, variations in computational protocols, and ambiguities in the\ntreatment of higher-order anharmonic processes. In this study, we critically\nreview these inconsistencies, first by mapping the spread of experimental and\nmodeling results, and then by identifying the methodological origins of\ndivergence. To this end, we bridge first-principles calculations, molecular\ndynamics simulations, and state-of-the-art machine learning force fields\n(MLFFs) including recently developed foundation models. %MACE-OMAT-0, UMA, and\nNEP89. We train and benchmark GAP, MACE, NEP, and \\textsc{HIPHIVE} against\ndensity functional theory (DFT) and rigorously evaluate the impact of third-\nand fourth-order phonon scattering processes on $\\kappa$. The computational\nefficiency of MLFFs enables us to extend convergence tests beyond conventional\nlimits and to validate predictions through homogeneous nonequilibrium molecular\ndynamics as well. Our analysis demonstrates that, contrary to some recent\nclaims, fully converged four-phonon processes contribute negligibly to the\nintrinsic thermal conductivity of both MoS$_2$ and MoSe$_2$. These findings not\nonly refine the intrinsic transport limits of 2D TMDs but also establish\nMLFF-based approaches as a robust and scalable framework for predictive\nmodeling of phonon-mediated thermal transport in low-dimensional materials.", "AI": {"tldr": "Group-VI\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5c5e\u5316\u7269\u7684\u6676\u683c\u70ed\u5bfc\u7387\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u672c\u6587\u901a\u8fc7\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u3001\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u529b\u573a\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u5dee\u5f02\u7684\u6765\u6e90\uff0c\u5e76\u8bc1\u660e\u4e86\u56db\u58f0\u5b50\u8fc7\u7a0b\u5bf9\u70ed\u5bfc\u7387\u7684\u8d21\u732e\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "motivation": "Group-VI\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5c5e\u5316\u7269\u7684\u6676\u683c\u70ed\u5bfc\u7387\u6d4b\u91cf\u503c\u548c\u7406\u8bba\u9884\u6d4b\u503c\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\u8fdb\u884c\u6279\u5224\u6027\u5ba1\u67e5\uff0c\u4ee5\u660e\u786e\u5176\u65b9\u6cd5\u5b66\u6839\u6e90\u3002", "method": "\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u3001\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08\u5305\u62ecMACE-OMAT-0\u3001UMA\u548cNEP89\uff09\uff0c\u5bf9GAP\u3001MACE\u3001NEP\u548cHIPHIVE\u8fdb\u884c\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e25\u683c\u8bc4\u4f30\u4e86\u4e09\u9636\u548c\u56db\u9636\u58f0\u5b50\u6563\u5c04\u8fc7\u7a0b\u5bf9\u6676\u683c\u70ed\u5bfc\u7387\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u5229\u7528\u4e86\u5747\u5300\u975e\u5e73\u8861\u5206\u5b50\u52a8\u529b\u5b66\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4e00\u4e9b\u8fd1\u671f\u58f0\u79f0\u76f8\u53cd\uff0c\u5b8c\u5168\u6536\u655b\u7684\u56db\u58f0\u5b50\u8fc7\u7a0b\u5bf9MoS2\u548cMoSe2\u7684\u672c\u5f81\u70ed\u5bfc\u7387\u7684\u8d21\u732e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u4e3a\u4e8c\u7ef4\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5c5e\u5316\u7269\u7684\u672c\u5f81\u8f93\u8fd0\u6781\u9650\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u8ba4\u8bc6\uff0c\u800c\u4e14\u786e\u7acb\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u529b\u573a\u7684\u65b9\u6cd5\u4f5c\u4e3a\u9884\u6d4b\u4f4e\u7ef4\u6750\u6599\u4e2d\u58f0\u5b50\u4ecb\u5bfc\u7684\u70ed\u8f93\u8fd0\u7684\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2509.13574", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13574", "abs": "https://arxiv.org/abs/2509.13574", "authors": ["Zidong Chen", "Zihao Guo", "Peng Wang", "ThankGod Itua Egbe", "Yan Lyu", "Chenghao Qian"], "title": "Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation", "comment": null, "summary": "Flow matching has emerged as a competitive framework for learning\nhigh-quality generative policies in robotics; however, we find that\ngeneralisation arises and saturates early along the flow trajectory, in\naccordance with recent findings in the literature. We further observe that\nincreasing the number of Euler integration steps during inference\ncounter-intuitively and universally degrades policy performance. We attribute\nthis to (i) additional, uniformly spaced integration steps oversample the\nlate-time region, thereby constraining actions towards the training\ntrajectories and reducing generalisation; and (ii) the learned velocity field\nbecoming non-Lipschitz as integration time approaches 1, causing instability.\nTo address these issues, we propose a novel policy that utilises non-uniform\ntime scheduling (e.g., U-shaped) during training, which emphasises both early\nand late temporal stages to regularise policy training, and a dense-jump\nintegration schedule at inference, which uses a single-step integration to\nreplace the multi-step integration beyond a jump point, to avoid unstable areas\naround 1. Essentially, our policy is an efficient one-step learner that still\npushes forward performance through multi-step integration, yielding up to 23.7%\nperformance gains over state-of-the-art baselines across diverse robotic tasks.", "AI": {"tldr": "Flow matching \u5728\u673a\u5668\u4eba\u9886\u57df\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u5b66\u4e60\u9ad8\u8d28\u91cf\u751f\u6210\u7b56\u7565\u7684\u6709\u6548\u6846\u67b6\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u5df2\u9971\u548c\u3002\u589e\u52a0\u6b27\u62c9\u79ef\u5206\u6b65\u6570\u53cd\u800c\u4f1a\u964d\u4f4e\u7b56\u7565\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\uff08\u5982U\u5f62\uff09\uff0c\u4ee5\u5f3a\u8c03\u8bad\u7ec3\u7684\u65e9\u671f\u548c\u665a\u671f\u9636\u6bb5\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91c7\u7528\u5bc6\u96c6\u8df3\u8f6c\u79ef\u5206\uff08dense-jump integration\uff09\uff0c\u7528\u5355\u6b65\u79ef\u5206\u53d6\u4ee3\u8df3\u8dc3\u70b9\u4e4b\u540e\u7684\u79ef\u5206\uff0c\u4ee5\u907f\u514d\u4e0d\u7a33\u5b9a\u7684\u533a\u57df\u3002\u8be5\u7b56\u7565\u5728\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe23.7%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "Flow matching \u5728\u673a\u5668\u4eba\u9886\u57df\u5b66\u4e60\u751f\u6210\u7b56\u7565\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u65e9\u671f\u9971\u548c\u548c\u589e\u52a0\u79ef\u5206\u6b65\u6570\u53cd\u800c\u964d\u4f4e\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\uff08\u5982U\u5f62\uff09\u6765\u5f3a\u8c03\u8bad\u7ec3\u7684\u65e9\u671f\u548c\u665a\u671f\u9636\u6bb5\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91c7\u7528\u5bc6\u96c6\u8df3\u8f6c\u79ef\u5206\uff08dense-jump integration\uff09\u6765\u907f\u514d\u4e0d\u7a33\u5b9a\u7684\u533a\u57df\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe23.7%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u548c\u5bc6\u96c6\u8df3\u8f6c\u79ef\u5206\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u6709\u6548\u5730\u89e3\u51b3\u4e86 Flow matching \u7b56\u7565\u5728\u6cdb\u5316\u548c\u79ef\u5206\u6b65\u6570\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.13648", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13648", "abs": "https://arxiv.org/abs/2509.13648", "authors": ["Geon Lee", "Bhuvesh Kumar", "Clark Mingxuan Ju", "Tong Zhao", "Kijung Shin", "Neil Shah", "Liam Collins"], "title": "Sequential Data Augmentation for Generative Recommendation", "comment": null, "summary": "Generative recommendation plays a crucial role in personalized systems,\npredicting users' future interactions from their historical behavior sequences.\nA critical yet underexplored factor in training these models is data\naugmentation, the process of constructing training data from user interaction\nhistories. By shaping the training distribution, data augmentation directly and\noften substantially affects model generalization and performance. Nevertheless,\nin much of the existing work, this process is simplified, applied\ninconsistently, or treated as a minor design choice, without a systematic and\nprincipled understanding of its effects.\n  Motivated by our empirical finding that different augmentation strategies can\nyield large performance disparities, we conduct an in-depth analysis of how\nthey reshape training distributions and influence alignment with future targets\nand generalization to unseen inputs. To systematize this design space, we\npropose GenPAS, a generalized and principled framework that models augmentation\nas a stochastic sampling process over input-target pairs with three\nbias-controlled steps: sequence sampling, target sampling, and input sampling.\nThis formulation unifies widely used strategies as special cases and enables\nflexible control of the resulting training distribution. Our extensive\nexperiments on benchmark and industrial datasets demonstrate that GenPAS yields\nsuperior accuracy, data efficiency, and parameter efficiency compared to\nexisting strategies, providing practical guidance for principled training data\nconstruction in generative recommendation.", "AI": {"tldr": "\u751f\u6210\u63a8\u8350\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5bf9\u4e8e\u6a21\u578b\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u5bf9\u5176\u5173\u6ce8\u4e0d\u8db3\u3002", "motivation": "\u4ee5\u5f80\u7684\u751f\u6210\u63a8\u8350\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u65b9\u9762\u5b58\u5728\u7b80\u5316\u3001\u4e0d\u4e00\u81f4\u6216\u88ab\u89c6\u4e3a\u6b21\u8981\u8bbe\u8ba1\u9009\u62e9\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGenPAS\u7684\u901a\u7528\u6846\u67b6\uff0c\u5c06\u6570\u636e\u589e\u5f3a\u5efa\u6a21\u4e3a\u5177\u6709\u4e09\u4e2a\u504f\u5dee\u63a7\u5236\u6b65\u9aa4\uff08\u5e8f\u5217\u91c7\u6837\u3001\u76ee\u6807\u91c7\u6837\u548c\u8f93\u5165\u91c7\u6837\uff09\u7684\u968f\u673a\u91c7\u6837\u8fc7\u7a0b\uff0c\u5e76\u7edf\u4e00\u4e86\u73b0\u6709\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7b56\u7565\u76f8\u6bd4\uff0cGenPAS\u5728\u51c6\u786e\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\u3002", "conclusion": "GenPAS\u4e3a\u751f\u6210\u63a8\u8350\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.14062", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14062", "abs": "https://arxiv.org/abs/2509.14062", "authors": ["Saifur Rahman", "Syed Luqman Shah", "Salman Khan", "Jalal Khan", "Muhammad Irfan", "Maaz Shafi", "Said Muhammad", "Fazal Muhammad", "Mohammad Shahed Akond"], "title": "Distributed Deep Learning with RIS Grouping for Accurate Cascaded Channel Estimation", "comment": "Submitted for Publication", "summary": "Reconfigurable Intelligent Surface (RIS) panels are envisioned as a key\ntechnology for sixth-generation (6G) wireless networks, providing a\ncost-effective means to enhance coverage and spectral efficiency. A critical\nchallenge is the estimation of the cascaded base station (BS)-RIS-user channel,\nsince the passive nature of RIS elements prevents direct channel acquisition,\nincurring prohibitive pilot overhead, computational complexity, and energy\nconsumption. To address this, we propose a deep learning (DL)-based channel\nestimation framework that reduces pilot overhead by grouping RIS elements and\nreconstructing the cascaded channel from partial pilot observations.\nFurthermore, conventional DL models trained under single-user settings suffer\nfrom poor generalization across new user locations and propagation scenarios.\nWe develop a distributed machine learning (DML) strategy in which the BS and\nusers collaboratively train a shared neural network using diverse channel\ndatasets collected across the network, thereby achieving robust generalization.\nBuilding on this foundation, we design a hierarchical DML neural architecture\nthat first classifies propagation conditions and then employs scenario-specific\nfeature extraction to further improve estimation accuracy. Simulation results\nconfirm that the proposed framework substantially reduces pilot overhead and\ncomplexity while outperforming conventional methods and single-user models in\nchannel estimation accuracy. These results demonstrate the practicality and\neffectiveness of the proposed approach for 6G RIS-assisted systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7684\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7ec4RIS\u5355\u5143\u548c\u4ece\u90e8\u5206\u5bfc\u9891\u89c2\u6d4b\u4e2d\u91cd\u5efa\u7ea7\u8054\u4fe1\u9053\u6765\u51cf\u5c11\u5bfc\u9891\u5f00\u9500\u3002\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\uff08DML\uff09\u7b56\u7565\uff0cBS\u548c\u7528\u6237\u534f\u4f5c\u8bad\u7ec3\u5171\u4eab\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\u3002\u8bbe\u8ba1\u5206\u5c42DML\u795e\u7ecf\u67b6\u6784\uff0c\u9996\u5148\u5bf9\u4f20\u64ad\u6761\u4ef6\u8fdb\u884c\u5206\u7c7b\uff0c\u7136\u540e\u91c7\u7528\u573a\u666f\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "RIS\u9762\u677f\u88ab\u8bbe\u60f3\u4e3a6G\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46BS-RIS-\u7528\u6237\u4fe1\u9053\u7684\u7ea7\u8054\u4f30\u8ba1\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3aRIS\u5143\u4ef6\u7684\u88ab\u52a8\u6027\u8d28\u4f1a\u5e26\u6765\u5f88\u9ad8\u7684\u5bfc\u9891\u5f00\u9500\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548c\u80fd\u8017\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7684\u4fe1\u9053\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7ec4RIS\u5355\u5143\u548c\u4ece\u90e8\u5206\u5bfc\u9891\u89c2\u6d4b\u4e2d\u91cd\u5efa\u7ea7\u8054\u4fe1\u9053\u6765\u51cf\u5c11\u5bfc\u9891\u5f00\u9500\u3002\u5f00\u53d1\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\uff08DML\uff09\u7b56\u7565\uff0cBS\u548c\u7528\u6237\u534f\u4f5c\u8bad\u7ec3\u5171\u4eab\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\u3002\u8bbe\u8ba1\u5206\u5c42DML\u795e\u7ecf\u67b6\u6784\uff0c\u9996\u5148\u5bf9\u4f20\u64ad\u6761\u4ef6\u8fdb\u884c\u5206\u7c7b\uff0c\u7136\u540e\u91c7\u7528\u573a\u666f\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u5bfc\u9891\u5f00\u9500\u548c\u590d\u6742\u6027\uff0c\u5e76\u4e14\u5728\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u7528\u6237\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u57286G RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "SteeringControl\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8868\u5f81\u63a7\u5236\u65b9\u6cd5\u5728\u504f\u5dee\u3001\u6709\u5bb3\u751f\u6210\u548c\u5e7b\u89c9\u7b49\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\u4e0a\u7684\u57fa\u51c6\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u8c04\u5a9a\u548c\u5e38\u8bc6\u9053\u5fb7\u7b49\u6b21\u8981\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u5728\u6a21\u578b\u3001\u63a7\u5236\u65b9\u6cd5\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7ec4\u5408\u4e0d\u5f53\u65f6\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u201c\u6982\u5ff5\u7ea0\u7f20\u201d\u95ee\u9898\u3002", "motivation": "\u4ee5\u5f80\u7684\u5bf9\u9f50\u7814\u7a76\u867d\u7136\u5173\u6ce8\u4e86\u8868\u5f81\u63a7\u5236\u7684\u526f\u4f5c\u7528\uff0c\u4f46\u672a\u80fd\u7cfb\u7edf\u5730\u7406\u89e3\u5176\u4e2d\u672a\u88ab\u63a2\u7d22\u7684\u6743\u8861\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SteeringControl\u57fa\u51c6\uff0c\u5305\u542b\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u63a7\u5236\u6846\u67b6\u548c\u7528\u4e8e\u8bc4\u4f30\u7684\u5b89\u5168\u6027\u76f8\u5173\u7684\u4e3b\u8981\u548c\u6b21\u8981\u884c\u4e3a\u6570\u636e\u96c6\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u6d41\u884c\u7684\u63a7\u5236\u65b9\u6cd5\u5728Qwen-2.5-7B\u548cLlama-3.1-8B\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728Qwen-2.5-7B\u548cLlama-3.1-8B\u6a21\u578b\u4e0a\uff0c\u5f3a\u7684\u63a7\u5236\u6027\u80fd\u53d6\u51b3\u4e8e\u63a7\u5236\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u5177\u4f53\u7ec4\u5408\uff0c\u5e76\u4e14\u4e0d\u5f53\u7684\u7ec4\u5408\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u6982\u5ff5\u7ea0\u7f20\u3002", "conclusion": "\u8868\u5f81\u63a7\u5236\u7684\u6027\u80fd\u548c\u526f\u4f5c\u7528\u662f\u76f8\u4e92\u4ea4\u7ec7\u7684\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u7684\u6a21\u578b\u3001\u63a7\u5236\u65b9\u6cd5\u548c\u76ee\u6807\u884c\u4e3a\u8fdb\u884c\u4ed4\u7ec6\u7684\u6743\u8861\u548c\u9009\u62e9\u3002"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap\u901a\u8fc7\u5206\u6790\u8de8\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u6765\u53d1\u73b0\u548c\u7f13\u89e3\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u9690\u85cf\u7684\u6982\u5ff5\u7ea7\u4ee3\u8868\u6027\u504f\u5dee\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8f93\u51fa\u5206\u5e03\u800c\u5ffd\u7565\u6982\u5ff5\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u504f\u89c1\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u6f5c\u5728\u6982\u5ff5\u8868\u793a\u4e2d\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u89c1\u79fb\u9664\u4e0d\u5f7b\u5e95\u3002", "method": "\u63d0\u51faBiasMap\u6846\u67b6\uff0c\u5229\u7528\u8de8\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u6765\u8bc6\u522b\u548c\u91cf\u5316\uff08\u901a\u8fc7IoU\uff09\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\uff09\u4e0e\u8bed\u4e49\uff08\u5982\u804c\u4e1a\uff09\u4e4b\u95f4\u7684\u6982\u5ff5\u7ea0\u7f20\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5f15\u5bfc\u7684\u53bb\u566a\u6269\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\u5e76\u6700\u5c0f\u5316SoftIoU\u6765\u7f13\u89e3\u6982\u5ff5\u7ea0\u7f20\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u867d\u7136\u53ef\u4ee5\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u7684\u5dee\u5f02\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e3\u51b3\u6982\u5ff5\u5c42\u9762\u7684\u8026\u5408\u95ee\u9898\u3002BiasMap\u63d0\u51fa\u7684\u7f13\u89e3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6982\u5ff5\u7ea0\u7f20\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u5206\u5e03\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u4e92\u8865\u3002", "conclusion": "BiasMap\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u80fd\u591f\u6df1\u5165\u63ed\u793a\u548c\u89e3\u51b3\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u9690\u85cf\u7684\u6982\u5ff5\u7ea7\u4ee3\u8868\u6027\u504f\u5dee\uff0c\u5176\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u5728\u5904\u7406\u6982\u5ff5\u7ea0\u7f20\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u5b66\u7684\u65e5\u8bed\u6bd4\u8f83\u7ea7\u63a8\u7406\u7cfb\u7edfccg-jcomp\uff0c\u4ee5\u89e3\u51b3\u65e5\u8bed\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\uff08NLI\uff09\u4e2d\u7684\u6bd4\u8f83\u7ea7\u7406\u89e3\u96be\u9898\u3002", "motivation": "\u65e5\u8bed\u6bd4\u8f83\u7ea7\u5728\u5f62\u6001\u548c\u8bed\u4e49\u4e0a\u4e0e\u82f1\u8bed\u5b58\u5728\u5dee\u5f02\uff0c\u96be\u4ee5\u76f4\u63a5\u5957\u7528\u73b0\u6709\u7684\u57fa\u4e8e\u903b\u8f91\u63a8\u7406\u7684\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65e5\u8bed\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u5b66\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edfccg-jcomp\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5305\u542b\u6bd4\u8f83\u7ea7\u8868\u8fbe\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5c06ccg-jcomp\u7684\u51c6\u786e\u7387\u4e0e\u73b0\u6709\u7684LLMs\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ccg-jcomp\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u7ea7\uff0c\u5e76\u5728\u65e5\u8bedNLI\u4efb\u52a1\u4e0a\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2509.13546", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13546", "abs": "https://arxiv.org/abs/2509.13546", "authors": ["Nam Nguyen", "Michael Yu", "Alan Robertson", "Hiromichi Nishimura", "Samuel J. Elman", "Benjamin Koltenbah"], "title": "End-to-End Complexity Analysis for Quantum Simulation of the Extended Jaynes-Cummings Models", "comment": "51 pages, 17 figures", "summary": "The extended Jaynes-Cummings model (eJCM) is a foundational framework for\ndescribing multi-mode light-matter interactions, with direct applications in\nquantum technologies such as photon addition and quasi-noiseless amplification.\nHowever, the model's complexity makes classical simulation intractable for\nlarge systems that could be of practical interest. In this work, we present a\ncomprehensive, end-to-end framework for the quantum simulation of the eJCM. We\ndevelop explicit quantum algorithms and circuits for simulating the system's\ntime evolution using first and second-order product formulas, analyzing the\ndynamics in both the Schrodinger and interaction pictures. Our analysis\nincludes rigorous, closed-form error bounds that guide the choice of simulation\nparameters, and we extend the methodology to efficiently handle both pure and\nmixed quantum states. Furthermore, we validate our theoretical cost models with\nnumerical simulations and provide a detailed fault-tolerant resource analysis,\ncompiling the simulation circuits for a surface-code architecture to yield\nconcrete estimates for physical qubit counts and execution times. This work\nestablishes a complete roadmap for simulating the eJCM on future quantum\ncomputers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u91cf\u5b50\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6269\u5c55\u7684 Jaynes-Cummings \u6a21\u578b\uff08eJCM\uff09\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bef\u5dee\u5206\u6790\u548c\u5bb9\u9519\u8d44\u6e90\u4f30\u7b97\u3002", "motivation": "eJCM \u5728\u91cf\u5b50\u6280\u672f\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u7ecf\u5178\u6a21\u62df\u5728\u5927\u7cfb\u7edf\u4e0a\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5b50\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u7b2c\u4e00\u548c\u7b2c\u4e8c\u9636\u4e58\u79ef\u516c\u5f0f\u7684\u663e\u5f0f\u91cf\u5b50\u7b97\u6cd5\u548c\u7535\u8def\uff0c\u7528\u4e8e\u6a21\u62df eJCM \u7684\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u5904\u7406\u7eaf\u6001\u548c\u6df7\u5408\u6001\u3002\u8fdb\u884c\u4e86\u8bef\u5dee\u5206\u6790\u3001\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u548c\u5bb9\u9519\u8d44\u6e90\u5206\u6790\u3002", "result": "\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6a21\u62df eJCM \u7684\u5b8c\u6574\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u5177\u4f53\u7684\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u548c\u6267\u884c\u65f6\u95f4\u4f30\u7b97\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6a21\u62df eJCM \u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6846\u67b6\u548c\u8be6\u7ec6\u7684\u5206\u6790\uff0c\u5305\u62ec\u8bef\u5dee\u754c\u9650\u548c\u8d44\u6e90\u4f30\u7b97\u3002"}}
{"id": "2509.13824", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13824", "abs": "https://arxiv.org/abs/2509.13824", "authors": ["Jiazhuang Si", "Shuyuan Liu", "Bing Wang", "Chongze Wang", "Fengzhu Ren", "Yu Jia", "Jun-Hyung Cho"], "title": "Contrasting magnetic anisotropy in CrCl3 and CrBr3: A first-principles study", "comment": "8 pages, 6 figures", "summary": "We present a first-principles study of the contrasting easy magnetization\naxes(EMAs) in the layered chromium trihalides CrCl3 and CrBr3, which exhibit\nin-plane and out-of-plane EMAs, respectively. Using density-functional theory\ncalculations, we show that the EMA is determined by the interplay between\nspin-orbit coupling-induced magnetocrystalline anisotropy energy (SOC-MAE) and\nshape magnetic anisotropy energy(shape-MAE) arising from dipole-dipole\ninteractions. While the Cr d orbitals contribute similarly to the SOC-MAE in\nboth compounds, the key difference stems from the halogen p orbitals. In CrCl3,\nthe localized Cl 3p orbitals favor spin-flip SOC interactions, particularly\nbetween the (px, py) and (py, pz) channels. These channels contribute with\nopposite signs-negative and positive, respectively-leading to partial\ncancellation and a small net SOC-MAE. As a result, the shape-MAE exceeds the\nSOC-MAE in magnitude, favoring an in-plane EMA. In contrast, CrBr3 features\nmore delocalized Br 4p orbitals, enhanced p-d hybridization, and stronger SOC.\nThis leads to stronger spin-conserving SOC interactions, with dominant\ncontributions from both the (px, py) and (py, pz) channels. In this case, the\npositive contribution from the (px, py) channel outweighs the smaller negative\ncontribution from the (py, pz) channel, resulting in a sizable net SOC-MAE. The\nSOC-MAE thus surpasses the shape-MAE and stabilizes an out-of-plane EMA. These\nfindings demonstrate that the contrasting magnetic anisotropies in CrCl3 and\nCrBr3 originate from differences in the spatial distribution, SOC strength, and\nhybridization of the halogen p orbitals, highlighting the critical role of\norbital anisotropy and spin selection rules in governing magnetic behavior in\nlayered semiconductors.", "AI": {"tldr": "CrCl3\u548cCrBr3\u7684\u78c1\u5404\u5411\u5f02\u6027\u5dee\u5f02\u6e90\u4e8e\u5364\u7d20p\u8f68\u9053\u7684\u7a7a\u95f4\u5206\u5e03\u3001\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u5f3a\u5ea6\u548c\u6742\u5316\u884c\u4e3a\u7684\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76CrCl3\u548cCrBr3\u4e2d\u6613\u78c1\u5316\u8f74\uff08EMAs\uff09\u5bf9\u6bd4\u73b0\u8c61\u7684\u539f\u56e0\u3002", "method": "\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u5206\u6790\u4e86\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u8bf1\u5bfc\u7684\u78c1\u6676\u5404\u5411\u5f02\u6027\u80fd\uff08SOC-MAE\uff09\u548c\u5f62\u72b6\u78c1\u5404\u5411\u5f02\u6027\u80fd\uff08shape-MAE\uff09\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "CrCl3\u4e2d\uff0cCl 3p\u8f68\u9053\u5c40\u90e8\u5316\uff0c\u4fc3\u8fdb\u4e86\u6709\u5229\u4e8e\u81ea\u65cb\u7ffb\u8f6c\u7684SOC\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4SOC-MAE\u90e8\u5206\u62b5\u6d88\uff0cshape-MAE\u5360\u4f18\uff0c\u6613\u78c1\u5316\u8f74\u5448\u9762\u5185\u53d6\u5411\u3002CrBr3\u4e2d\uff0cBr 4p\u8f68\u9053\u66f4\u79bb\u57df\u5316\uff0cp-d\u6742\u5316\u589e\u5f3a\uff0cSOC\u66f4\u5f3a\uff0c\u6709\u5229\u4e8e\u81ea\u65cb\u5b88\u6052\u7684SOC\u76f8\u4e92\u4f5c\u7528\uff0cSOC-MAE\u5360\u4f18\uff0c\u6613\u78c1\u5316\u8f74\u5448\u9762\u5916\u53d6\u5411\u3002", "conclusion": "CrCl3\u548cCrBr3\u4e2d\u78c1\u5404\u5411\u5f02\u6027\u7684\u5dee\u5f02\u662f\u5364\u7d20p\u8f68\u9053\u7684\u7a7a\u95f4\u5206\u5e03\u3001SOC\u5f3a\u5ea6\u548c\u6742\u5316\u4f5c\u7528\u5dee\u5f02\u7684\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u8f68\u9053\u5404\u5411\u5f02\u6027\u548c\u81ea\u65cb\u9009\u62e9\u89c4\u5219\u5728\u78c1\u6027\u884c\u4e3a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.13579", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13579", "abs": "https://arxiv.org/abs/2509.13579", "authors": ["Momchil S. Tomov", "Sang Uk Lee", "Hansford Hendrago", "Jinwook Huh", "Teawon Han", "Forbes Howington", "Rafael da Silva", "Gianmarco Bernasconi", "Marc Heim", "Samuel Findler", "Xiaonan Ji", "Alexander Boule", "Michael Napoli", "Kuo Chen", "Jesse Miller", "Boaz Floor", "Yunqing Hu"], "title": "TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning", "comment": null, "summary": "We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.", "AI": {"tldr": "TreeIRL\u7ed3\u5408MCTS\u548cIRL\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u662f\u9996\u4e2a\u5728\u516c\u5f00\u9053\u8def\u4e0a\u4f7f\u7528MCTS\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e73\u8861\u5b89\u5168\u3001\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTreeIRL\u7684\u65b0\u578b\u89c4\u5212\u5668\uff0c\u8be5\u89c4\u5212\u5668\u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u3002MCTS\u7528\u4e8e\u751f\u6210\u4e00\u7cfb\u5217\u5b89\u5168\u4e14\u6709\u6f5c\u529b\u7684\u5019\u9009\u8f68\u8ff9\uff0c\u800c\u6df1\u5ea6IRL\u8bc4\u5206\u51fd\u6570\u5219\u7528\u4e8e\u4ece\u4e2d\u9009\u62e9\u6700\u63a5\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u8f68\u8ff9\u3002", "result": "TreeIRL\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u9a7e\u9a76\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u5728\u6a21\u62df\u548c\u62c9\u65af\u7ef4\u52a0\u65af\u771f\u5b9e\u9053\u8def\u6d4b\u8bd5\u4e2d\uff0cTreeIRL\u5728\u5904\u7406\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u3001\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u3001\u8f66\u8f86\u5207\u5165\u548c\u4ea4\u901a\u4fe1\u53f7\u706f\u7b49\u573a\u666f\u65f6\uff0c\u5747\u5728\u5b89\u5168\u3001\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u4eba\u7c7b\u884c\u4e3a\u6a21\u4eff\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "TreeIRL\u662f\u9996\u4e2a\u5728\u516c\u5f00\u9053\u8def\u4e0a\u6210\u529f\u5e94\u7528MCTS\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u5e76\u91c7\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff08\u5305\u62ec\u5b89\u5168\u3001\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u4eba\u7c7b\u884c\u4e3a\u6a21\u4eff\u6027\uff09\u5bf9\u89c4\u5212\u5668\u8fdb\u884c\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002\u8be5\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u7684\u53ef\u6269\u5c55\u6027\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u74f6\u9888\u63d0\u4f9b\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.13651", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13651", "abs": "https://arxiv.org/abs/2509.13651", "authors": ["Yongkang Du", "Jieyu Zhao", "Yijun Yang", "Tianyi Zhou"], "title": "Controllable Pareto Trade-off between Fairness and Accuracy", "comment": null, "summary": "The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work\nfocuses on finding a single \"optimal\" solution to balance the two objectives,\nwhich is limited considering the diverse solutions on the Pareto front. This\nwork intends to provide controllable trade-offs according to the user's\npreference of the two objectives, which is defined as a reference vector. To\nachieve this goal, we apply multi-objective optimization (MOO), which can find\nsolutions from various regions of the Pareto front. However, it is challenging\nto precisely control the trade-off due to the stochasticity of the training\nprocess and the high dimentional gradient vectors. Thus, we propose\nControllable Pareto Trade-off (CPT) that can effectively train models to\nperform different trade-offs according to users' preferences. CPT 1) stabilizes\nthe fairness update with a moving average of stochastic gradients to determine\nthe update direction, and 2) prunes the gradients by only keeping the gradients\nof the critical parameters. We evaluate CPT on hate speech detection and\noccupation classification tasks. Experiments show that CPT can achieve a\nhigher-quality set of solutions on the Pareto front than the baseline methods.\nIt also exhibits better controllability and can precisely follow the\nhuman-defined reference vectors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPT\uff08Controllable Pareto Trade-off\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728NLP\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u63a7\u7684\u516c\u5e73\u6027-\u51c6\u786e\u6027\u6743\u8861\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u627e\u5230\u5355\u4e00\u6700\u4f18\u89e3\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u8861\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u65f6\uff0c\u901a\u5e38\u53ea\u80fd\u627e\u5230\u4e00\u4e2a\u201c\u6700\u4f18\u201d\u89e3\uff0c\u8fd9\u5ffd\u7565\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u5b58\u5728\u7684\u591a\u79cd\u89e3\u51b3\u65b9\u6848\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u6839\u636e\u7528\u6237\u5bf9\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u504f\u597d\u7684\u4e0d\u540c\u9700\u6c42\uff0c\u6765\u5b9e\u73b0\u53ef\u63a7\u6743\u8861\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u6765\u5bfb\u627e\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u7684\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u4e86\u89e3\u51b3\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u6027\u548c\u9ad8\u7ef4\u68af\u5ea6\u5411\u91cf\u5e26\u6765\u7684\u7cbe\u786e\u63a7\u5236\u96be\u9898\uff0cCPT\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u8fdb\u884c\u4f18\u5316\uff1a1\uff09\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u7684\u79fb\u52a8\u5e73\u5747\u6765\u7a33\u5b9a\u516c\u5e73\u6027\u66f4\u65b0\uff0c\u786e\u5b9a\u66f4\u65b0\u65b9\u5411\uff1b2\uff09\u901a\u8fc7\u53ea\u4fdd\u7559\u5173\u952e\u53c2\u6570\u7684\u68af\u5ea6\u6765\u4fee\u526a\u68af\u5ea6\u3002", "result": "CPT\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u804c\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cCPT\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0cCPT\u8fd8\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u63a7\u6027\uff0c\u80fd\u591f\u7cbe\u786e\u5730\u9075\u5faa\u7528\u6237\u5b9a\u4e49\u7684\u53c2\u8003\u5411\u91cf\u3002", "conclusion": "CPT\u65b9\u6cd5\u6709\u6548\u5730\u5b9e\u73b0\u4e86NLP\u4efb\u52a1\u4e2d\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u7684\u53ef\u63a7\u6743\u8861\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u504f\u597d\u63d0\u4f9b\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u7684\u591a\u79cd\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14072", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14072", "abs": "https://arxiv.org/abs/2509.14072", "authors": ["Vincent Lauinger", "Lennart Schmitz", "Patrick Matalla", "Andrej Rode", "Sebastian Randel", "Laurent Schmalen"], "title": "Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission", "comment": "Accepted and to be presented at the European Conference on Optical\n  Communication (ECOC) 2025", "summary": "We demonstrate the effectiveness of a novel phase-noise-tolerant,\nvariational-autoencoder-based equalization scheme for\nspace-division-multiplexed (SDM) transmission in an experiment over 150km of\nrandomly-coupled multi-core fibers.", "AI": {"tldr": "Phase-noise-tolerant VAE-based equalization improves SDM transmission over 150km.", "motivation": "Improve SDM transmission by addressing phase noise.", "method": "Using a variational autoencoder-based equalization scheme tolerant to phase noise.", "result": "Demonstrated effectiveness in an experiment over 150km of randomly-coupled multi-core fibers.", "conclusion": "The proposed scheme is effective for SDM transmission."}}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "\u7ed9\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7c7b\u4eba\u534f\u4f5c\u5de5\u5177\u548c\u81ea\u4e3b\u6743\uff0c\u53ef\u4ee5\u63d0\u5347\u5176\u5728\u590d\u6742\u7f16\u7a0b\u6311\u6218\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u5bf9\u7b80\u5355\u95ee\u9898\u6548\u679c\u4e0d\u4e00\u3002\u4e0d\u540c\u6a21\u578b\u4f1a\u6839\u636e\u81ea\u8eab\u7279\u70b9\u548c\u95ee\u9898\u96be\u5ea6\uff0c\u81ea\u53d1\u91c7\u7528\u4e0d\u540c\u7684\u534f\u4f5c\u7b56\u7565\uff08\u5982\u5e7f\u6cdb\u4f7f\u7528\u6216\u9009\u62e9\u6027\u4f7f\u7528\u5de5\u5177\uff09\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u884c\u4e3a\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e3b\u52a8\u8f93\u51fa\uff08\u5199\u4f5c\uff09\u76f8\u6bd4\u4e8e\u4fe1\u606f\u83b7\u53d6\uff08\u9605\u8bfb\uff09\u66f4\u80fd\u9a71\u52a8\u6027\u80fd\u63d0\u5347\uff0c\u63d0\u793a\u672a\u6765\u7684\u534f\u4f5c\u754c\u9762\u5e94\u4fa7\u91cd\u4e8e\u63d0\u4f9b\u63a8\u7406\u652f\u6301\u800c\u975e\u666e\u9002\u6027\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u83b7\u5f97\u7c7b\u4f3c\u4eba\u7c7b\u7684\u534f\u4f5c\u5de5\u5177\u548c\u81ea\u4e3b\u6743\u540e\uff0c\u5176\u89e3\u51b3\u95ee\u9898\u7684\u6027\u80fd\u662f\u5426\u4f1a\u5f97\u5230\u63d0\u5347\u3002", "method": "\u4e3a Claude Code \u4ee3\u7406\u914d\u5907\u4e86\u57fa\u4e8e MCP \u7684\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u5fd7\u8bb0\u5f55\u5de5\u5177\uff0c\u5e76\u5141\u8bb8\u5b83\u4eec\u81ea\u884c\u4f7f\u7528\u3002\u5728 34 \u4e2a Aider Polyglot Python \u7f16\u7a0b\u6311\u6218\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728\u6700\u56f0\u96be\u7684\u95ee\u9898\u4e0a\uff0c\u534f\u4f5c\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u6210\u672c\u964d\u4f4e\u4e86 15-40%\uff0c\u4ea4\u4e92\u6b21\u6570\u51cf\u5c11\u4e86 12-27%\uff0c\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u4e86 12-38%\u3002\u4f46\u5728\u5168\u90e8\u6311\u6218\u96c6\u4e0a\u7684\u6548\u679c\u4e0d\u4e00\u3002\u6a21\u578b\uff08\u5982 Sonnet 3.7 \u548c Sonnet 4\uff09\u81ea\u53d1\u91c7\u7528\u4e86\u4e0d\u540c\u7684\u534f\u4f5c\u7b56\u7565\u3002AI \u4ee3\u7406\u503e\u5411\u4e8e\u4e3b\u52a8\u5199\u4f5c\uff082-9\u500d\u4e8e\u9605\u8bfb\uff09\uff0c\u8868\u660e\u7ed3\u6784\u5316\u8868\u8ff0\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u3002", "conclusion": "AI \u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u7c7b\u4f3c\u4eba\u7c7b\u7684\u534f\u4f5c\u5de5\u5177\u5728\u80fd\u529b\u8fb9\u7f18\u53d7\u76ca\uff0c\u8fd9\u8868\u660e\u81ea\u9002\u5e94\u7684\u534f\u4f5c\u754c\u9762\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u7406\u589e\u5f3a\u5668\uff0c\u800c\u4e0d\u662f\u63d0\u4f9b\u666e\u904d\u7684\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hern\u00e1ndez"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aLivePixel\u7684Python\u5de5\u5177\uff0c\u7528\u4e8e\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u5373\u65f6\u6570\u636e\u91c7\u96c6\u548cAI\u6a21\u578b\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u8f6f\u4ef6\u9700\u8981\u9884\u5148\u6536\u96c6\u6570\u636e\u96c6\uff0c\u8fd9\u4e0d\u9002\u5408\u9700\u8981\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u9650\u5236\u4e86\u6309\u9700\u5904\u7406\u6d41\u7a0b\u5e76\u589e\u52a0\u4e86\u4e0d\u5fc5\u8981\u7684\u6b65\u9aa4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLivePixel\u7684Python\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u5b83\u53ef\u4ee5\u4e0e\u663e\u5fae\u955c\u3001\u7f51\u7edc\u6444\u50cf\u5934\u7b49\u6210\u50cf\u7cfb\u7edf\u96c6\u6210\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u5546\u4e1a\u56fe\u5f62\u7f16\u8f91\u8f6f\u4ef6\u7684\u6807\u6ce8\u5de5\u5177\uff08\u5982B\u00e9zier\u6837\u6761\u548c\u4e8c\u503c\u63a9\u7801\uff09\uff0c\u652f\u6301\u975e\u7834\u574f\u6027\u56fe\u5c42\uff0c\u5e76\u5229\u7528OpenCV\u548cNumpy\u4f18\u5316\u5bf9\u8c61\u68c0\u6d4b\u3002", "result": "LivePixel\u80fd\u591f\u5b9e\u65f6\u8fdb\u884c\u56fe\u50cf\u6807\u6ce8\uff0c\u652f\u6301\u591a\u79cd\u89c6\u9891\u8bbe\u5907\uff0c\u5e76\u4e3a\u5bf9\u8c61\u68c0\u6d4b\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u8fc7\u7a0b\u3002", "conclusion": "LivePixel\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5b9e\u65f6\u6807\u6ce8\u5de5\u5177\uff0c\u52a0\u901f\u4e86\u5b9e\u9a8c\u6d41\u7a0b\u4e2dAI\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "LLM\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46LoRA\u5fae\u8c03\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u751a\u81f3\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\u3002", "motivation": "\u63a2\u7d22\u6570\u636e\u9ad8\u6548\u548c\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u3002", "method": "\u7814\u7a76\u4e86\u4e0d\u540c\u7684\u8f6f\u63d0\u793a\u7b56\u7565\uff08prefix-tuning, prompt-tuning, P-tuning, P-tuning V2\uff09\u548cLoRA\u91cd\u53c2\u6570\u5316\uff0c\u4ee5\u53ca\u786c\u63d0\u793a\uff08\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63a8\u7406\uff09\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u7684\u7f16\u7801\u5668\u6a21\u578b\u3001\u4e3b\u8981\u6570\u636e\u96c6\u3001\u5f00\u6e90\u89e3\u7801\u5668\u6a21\u578b\uff08\u5305\u62ecPhi-3.5\u548cSILMA\uff09\u3002", "result": "LLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u65b9\u8a00\u7ec6\u5fae\u5dee\u522b\u7684\u80fd\u529b\u666e\u904d\u8f83\u5f31\u3002\u8f6f\u63d0\u793a\u7684\u7f16\u7801\u5668\u53d8\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u800c\u57fa\u4e8eLoRA\u7684\u5fae\u8c03\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\u3002", "conclusion": "LoRA\u5fae\u8c03\u662f\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u548c\u9ad8\u6027\u80fd\u7684\u6700\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.13659", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13659", "abs": "https://arxiv.org/abs/2509.13659", "authors": ["En-Jui Chang"], "title": "Quantum keystroke logging", "comment": "6 pages", "summary": "We propose a \\textit{quantum keystroke logging} scheme in the context of\nGKP-based quantum communication. The central idea is that, if a communication\nprovider controls the preparation of encoded states, it may be able to infer\nlogical operations applied by users prior to transmission. We show that phase\nestimation can be adapted to this setting despite two obstacles: GKP codewords\nare not eigenstates of the logical operators, and realistic communication\ninvolves one-shot operations rather than repeated applications. Our approach\nrelies on three observations: the geometric phase associated with closed\ntrajectories in phase space manifests as an effective Pauli-$Z$ rotation on an\nancilla; the quantum Fourier transform (QFT) need only reproduce the correct\nprobability distribution and can thus be simplified using auxiliary ancilla;\nand, in oscillator systems, cross-Kerr nonlinearities provide a natural\nmechanism to reduce the circuit depth of the QFT. Together, these tools enable\na malicious provider to extract user inputs without disturbing the transmitted\ncodewords, thereby demonstrating a sufficient condition for quantum keystroke\nlogging.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGKP\u91cf\u5b50\u7f16\u7801\u7684\u91cf\u5b50\u51fb\u952e\u8bb0\u5f55\u65b9\u6848\uff0c\u65e8\u5728\u4f7f\u7528\u6237\u5728\u4f20\u8f93\u524d\u6267\u884c\u7684\u903b\u8f91\u64cd\u4f5c\u80fd\u591f\u88ab\u901a\u4fe1\u63d0\u4f9b\u5546\u63a8\u65ad\u51fa\u6765\u3002", "motivation": "\u901a\u4fe1\u63d0\u4f9b\u5546\u53ef\u80fd\u80fd\u591f\u63a8\u65ad\u51fa\u7528\u6237\u5728\u4f20\u8f93\u524d\u5bf9\u5176\u7f16\u7801\u72b6\u6001\u6267\u884c\u7684\u903b\u8f91\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u51fb\u952e\u8bb0\u5f55\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u76f8\u4f4d\u4f30\u8ba1\u6539\u7f16\u7528\u4e8eGKP\u7f16\u7801\u72b6\u6001\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u76f8\u4f4d\u3001\u7b80\u5316\u7684\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\uff08QFT\uff09\u4ee5\u53ca\u8de8\u514b\u5c14\u975e\u7ebf\u6027\u6765\u964d\u4f4e\u7535\u8def\u6df1\u5ea6\uff0c\u4ece\u800c\u5728\u4e0d\u5e72\u6270\u4f20\u8f93\u7801\u5b57\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u7528\u6237\u8f93\u5165\u3002", "result": "\u8be5\u65b9\u6848\u80fd\u591f\u8ba9\u6076\u610f\u63d0\u4f9b\u5546\u5728\u4e0d\u5e72\u6270\u4f20\u8f93\u7684\u91cf\u5b50\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u65ad\u51fa\u7528\u6237\u5728\u4f20\u8f93\u524d\u5bf9\u91cf\u5b50\u6001\u8fdb\u884c\u7684\u64cd\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u91cf\u5b50\u51fb\u952e\u8bb0\u5f55\u3002", "conclusion": "\u91cf\u5b50\u51fb\u952e\u8bb0\u5f55\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u73b0\u8be5\u76ee\u6807\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13826", "categories": ["cond-mat.mtrl-sci", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13826", "abs": "https://arxiv.org/abs/2509.13826", "authors": ["Sk Mujaffar Hossain", "Seung-Cheol Lee", "Satadeep Bhattacharjee"], "title": "Quantum Simulations of Battery Electrolytes with VQE-qEOM and SQD: Active-Space Design, Dissociation, and Excited States of LiPF$_6$, NaPF$_6$, and FSI Salts", "comment": null, "summary": "Accurate prediction of excited states in battery electrolytes is central to\nunderstanding photostability, oxidative stability, and degradation. We employ\nhybrid quantum-classical algorithms -- the Variational Quantum Eigensolver\n(VQE) for ground states combined with the quantum equation of motion (qEOM) for\nvertical singlet excitations -- to study LiPF$_6$, NaPF$_6$, LiFSI, and NaFSI.\nCompact active spaces were constructed from frontier orbitals, mapped to\nqubits, and reduced via symmetry tapering and commuting-group measurements to\nlower sampling cost. Within $\\sim$10-qubit models, VQE-qEOM agrees closely with\nexact diagonalization of the same Hamiltonians, while sample-based quantum\ndiagonalization (SQD) in larger active spaces recovers near-exact\n(subspace-FCI) energies. The spectra display clear anion and cation trends:\nPF$_6$ salts exhibit higher first-excitation energies (e.g., LiPF$_6$\n$\\approx$13.2 eV) and a compact three-state cluster at 12-13 eV, whereas FSI\nsalts show substantially lower onsets ($\\approx$8-9 eV) with a near-degenerate\n(S$_1$,S$_2$) followed by S$_3$ $\\sim$1.3 eV higher. Substituting Li$^+$ with\nNa$^+$ narrows the gap by $\\sim$0.4-0.8 eV within each anion family. Converting\nS$_1$ to wavelengths places the onsets in the deep-UV (LiPF$_6$ $\\sim$94 nm;\nNaPF$_6$ $\\sim$100 nm; LiFSI $\\sim$141 nm; NaFSI $\\sim$148 nm). All results\npertain to isolated species or embedded clusters appropriate to the NISQ\nregime; solvent shifts can be incorporated a posteriori via classical\n$\\Delta$-solvation or static embedding. These results demonstrate that current\nquantum algorithms can deliver chemically meaningful excitation and binding\ntrends for realistic electrolyte motifs and provide quantitative baselines to\nguide electrolyte screening and design.", "AI": {"tldr": "\u4f7f\u7528\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7b97\u6cd5\u7814\u7a76\u4e86\u56db\u79cd\u7535\u6c60\u7535\u89e3\u8d28\uff08LiPF$_{6}$\u3001NaPF$_{6}$\u3001LiFSI \u548c NaFSI\uff09\u7684\u6fc0\u53d1\u6001\uff0c\u7ed3\u679c\u663e\u793a\u4e86\u9634\u9633\u79bb\u5b50\u548c\u53d6\u4ee3\u6548\u5e94\uff0c\u5e76\u4e3a\u7535\u89e3\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7ebf\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7535\u6c60\u7535\u89e3\u8d28\u7684\u6fc0\u53d1\u6001\u5bf9\u4e8e\u7406\u89e3\u5149\u7a33\u5b9a\u6027\u548c\u6c27\u5316\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u53d8\u5206\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u7ed3\u5408\u91cf\u5b50\u8fd0\u52a8\u65b9\u7a0b\uff08qEOM\uff09\u7b97\u6cd5\uff0c\u7814\u7a76\u4e86\u56db\u79cd\u7535\u6c60\u7535\u89e3\u8d28\u7684\u6fc0\u53d1\u6001\u3002\u901a\u8fc7\u6784\u9020\u7d27\u51d1\u7684\u6d3b\u6027\u7a7a\u95f4\u3001\u6620\u5c04\u5230\u91cf\u5b50\u6bd4\u7279\u3001\u5229\u7528\u5bf9\u79f0\u6027\u526a\u679d\u548c\u5bf9\u6613\u7fa4\u6d4b\u91cf\u6765\u964d\u4f4e\u91c7\u6837\u6210\u672c\u3002", "result": "PF$_{6}$\u76d0\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7b2c\u4e00\u6fc0\u53d1\u80fd\uff08LiPF$_{6}$ $\\approx$13.2 eV\uff09\uff0c\u800cFSI\u76d0\u7684\u6fc0\u53d1\u80fd\u8f83\u4f4e\uff08$\\approx$8-9 eV\uff09\u3002\u9502\u79bb\u5b50\u66ff\u6362\u4e3a\u94a0\u79bb\u5b50\u4f1a\u4f7f\u80fd\u9699\u51cf\u5c0f\u7ea60.4-0.8 eV\u3002\u6fc0\u53d1\u80fd\u5bf9\u5e94\u4e8e\u6df1\u7d2b\u5916\u533a\u57df\uff08LiPF$_{6}$ $\\approx$94 nm\uff1bNaFSI $\\approx$148 nm\uff09\u3002", "conclusion": "\u5f53\u524d\u7684\u91cf\u5b50\u7b97\u6cd5\u80fd\u591f\u4e3a\u5b9e\u9645\u7684\u7535\u89e3\u8d28\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6fc0\u53d1\u548c\u7ed3\u5408\u8d8b\u52bf\uff0c\u5e76\u4e3a\u7535\u89e3\u8d28\u7b5b\u9009\u548c\u8bbe\u8ba1\u63d0\u4f9b\u91cf\u5316\u57fa\u7ebf\u3002"}}
{"id": "2509.13591", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13591", "abs": "https://arxiv.org/abs/2509.13591", "authors": ["Amir-Hossein Shahidzadeh", "Jiyue Zhu", "Kezhou Chen", "Sha Yi", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos", "Xiaolong Wang"], "title": "Object Pose Estimation through Dexterous Touch", "comment": null, "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .", "AI": {"tldr": "\u901a\u8fc7\u4f20\u611f\u5668\u8fd0\u52a8\u63a2\u7d22\u6765\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u7269\u4f53\u59ff\u6001\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u9c81\u68d2\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u6570\u636e\u6709\u9650\u6216\u5bf9\u5149\u7167\u3001\u906e\u6321\u548c\u5916\u89c2\u654f\u611f\u7684\u60c5\u51b5\u4e0b\u3002\u89e6\u89c9\u4f20\u611f\u5668\u901a\u5e38\u63d0\u4f9b\u6709\u9650\u4e14\u5c40\u90e8\u7684\u63a5\u89e6\u4fe1\u606f\uff0c\u96be\u4ee5\u4ece\u90e8\u5206\u6570\u636e\u4e2d\u91cd\u5efa\u59ff\u6001\u3002", "method": "\u5229\u7528\u4f20\u611f\u5668\u8fd0\u52a8\u63a2\u7d22\uff0c\u4e3b\u52a8\u63a7\u5236\u673a\u5668\u4eba\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u884c\u63a2\u7d22\u548c\u6536\u96c6\u89e6\u89c9\u6570\u636e\u3002\u6536\u96c6\u5230\u76843D\u70b9\u4e91\u7528\u4e8e\u8fed\u4ee3\u5730\u4f18\u5316\u7269\u4f53\u7684\u5f62\u72b6\u548c\u59ff\u6001\u3002\u4e00\u4e2a\u624b\u4fdd\u6301\u7269\u4f53\u7a33\u5b9a\uff0c\u53e6\u4e00\u4e2a\u624b\u8fdb\u884c\u4e3b\u52a8\u63a2\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3b\u52a8\u63a2\u7d22\u7269\u4f53\u8868\u9762\uff0c\u5728\u65e0\u5148\u9a8c\u51e0\u4f55\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u5173\u952e\u59ff\u6001\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5b9e\u73b0\u9c81\u68d2\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2509.13686", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13686", "abs": "https://arxiv.org/abs/2509.13686", "authors": ["Bingsheng Peng", "Shutao Zhang", "Xi Zheng", "Ye Xue", "Xinyu Qin", "Tsung-Hui Chang"], "title": "RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization", "comment": null, "summary": "Accurate localized wireless channel modeling is a cornerstone of cellular\nnetwork optimization, enabling reliable prediction of network performance\nduring parameter tuning. Localized statistical channel modeling (LSCM) is the\nstate-of-the-art channel modeling framework tailored for cellular network\noptimization. However, traditional LSCM methods, which infer the channel's\nAngular Power Spectrum (APS) from Reference Signal Received Power (RSRP)\nmeasurements, suffer from critical limitations: they are typically confined to\nsingle-cell, single-grid and single-carrier frequency analysis and fail to\ncapture complex cross-domain interactions. To overcome these challenges, we\npropose RF-LSCM, a novel framework that models the channel APS by jointly\nrepresenting large-scale signal attenuation and multipath components within a\nradiance field. RF-LSCM introduces a multi-domain LSCM formulation with a\nphysics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the\ncross frequency generalization as well as a point-cloud-aided environment\nenhanced method to enable multi-cell and multi-grid channel modeling.\nFurthermore, to address the computational inefficiency of typical neural\nradiance fields, RF-LSCM leverages a low-rank tensor representation,\ncomplemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.\nThis efficient design significantly reduces GPU memory requirements and\ntraining time while preserving fine-grained accuracy. Extensive experiments on\nreal-world multi-cell datasets demonstrate that RF-LSCM significantly\noutperforms state-of-the-art methods, achieving up to a 30% reduction in mean\nabsolute error (MAE) for coverage prediction and a 22% MAE improvement by\neffectively fusing multi-frequency data.", "AI": {"tldr": "RF-LSCM\u662f\u4e00\u4e2a\u65b0\u7684\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8868\u793a\u5927\u89c4\u6a21\u4fe1\u53f7\u8870\u51cf\u548c\u591a\u5f84\u5206\u91cf\uff0c\u5b9e\u73b0\u4e86\u8de8\u57df\uff08\u591a\u5c0f\u533a\u3001\u591a\u7f51\u683c\u3001\u591a\u9891\u6bb5\uff09\u7684\u7cbe\u786e\u4fe1\u9053\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u5f20\u91cf\u8868\u793a\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u672c\u5730\u5316\u7edf\u8ba1\u4fe1\u9053\u5efa\u6a21\uff08LSCM\uff09\u65b9\u6cd5\u5728\u5355\u5c0f\u533a\u3001\u5355\u7f51\u683c\u548c\u5355\u8f7d\u6ce2\u9891\u7387\u5206\u6790\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u8de8\u57df\u4ea4\u4e92\u3002\u4e3a\u4e86\u4f18\u5316\u8702\u7a9d\u7f51\u7edc\u6027\u80fd\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u3001\u66f4\u901a\u7528\u7684\u4fe1\u9053\u6a21\u578b\u3002", "method": "\u63d0\u51faRF-LSCM\u6846\u67b6\uff0c\u901a\u8fc7\u8f90\u5c04\u573a\u8054\u5408\u8868\u793a\u5927\u89c4\u6a21\u4fe1\u53f7\u8870\u51cf\u548c\u591a\u5f84\u5206\u91cf\u3002\u5f15\u5165\u4e86\u7269\u7406\u4fe1\u606f\u7ea6\u675f\u7684\u9891\u7387\u76f8\u5173\u8870\u51cf\u6a21\u578b\uff08FDAM\uff09\u6765\u5b9e\u73b0\u8de8\u9891\u7387\u6cdb\u5316\uff0c\u5e76\u5229\u7528\u70b9\u4e91\u8f85\u52a9\u73af\u5883\u589e\u5f3a\u65b9\u6cd5\u5b9e\u73b0\u591a\u5c0f\u533a\u548c\u591a\u7f51\u683c\u4fe1\u9053\u5efa\u6a21\u3002\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u548c\u5206\u5c42\u5f20\u91cf\u89d2\u5ea6\u5efa\u6a21\uff08HiTAM\uff09\u7b97\u6cd5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u591a\u5c0f\u533a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRF-LSCM\u5728\u8986\u76d6\u9884\u6d4b\u65b9\u9762\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e8630%\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u878d\u5408\u591a\u9891\u6570\u636e\u5c06MAE\u63d0\u9ad8\u4e8622%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "RF-LSCM\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u8de8\u8d8a\u591a\u4e2a\u9886\u57df\uff08\u9891\u7387\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\uff09\u7684\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u7cbe\u786e\u3001\u66f4\u9ad8\u6548\u7684\u4fe1\u9053\u5efa\u6a21\uff0c\u4e3a\u8702\u7a9d\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.14160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14160", "abs": "https://arxiv.org/abs/2509.14160", "authors": ["Adam Umra", "Aya Mostafa Ahmed", "Stefan Roth", "Aydin Sezgin"], "title": "Hardware-Efficient Cognitive Radar: Multi-Target Detection with RL-Driven Transmissive RIS", "comment": "5 pages, 3 figures, submitted to ICASSP 2026", "summary": "Cognitive radar has emerged as a key paradigm for next-generation sensing,\nenabling adaptive, intelligent operation in dynamic and complex environments.\nYet, conventional cognitive multiple-input multiple-output (MIMO) radars offer\nstrong detection performance but suffer from high hardware complexity and power\ndemands. To overcome these limitations, we develop a reinforcement learning\n(RL)-based framework that leverages a transmissive reconfigurable intelligent\nsurface (TRIS) for adaptive beamforming. A state-action-reward-state-action\n(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low\nsignal-to-noise ratio (SNR) conditions while operating with far fewer radio\nfrequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar\nmatches or, for large number of elements, even surpasses MIMO performance with\nreduced cost and energy requirements.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8ba4\u77e5MIMO\u96f7\u8fbe\u65b0\u6846\u67b6\uff0c\u5229\u7528\u900f\u5c04\u5f0f\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08TRIS\uff09\u8fdb\u884c\u81ea\u9002\u5e94\u6ce2\u675f\u5f62\u6210\uff0c\u4ee5\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u6761\u4ef6\u4e0b\u63d0\u5347\u591a\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u80fd\u8017\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5MIMO\u96f7\u8fbe\u867d\u7136\u63a2\u6d4b\u6027\u80fd\u5f3a\uff0c\u4f46\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u529f\u8017\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u4f20\u611f\u5668\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528SARSA\uff08\u72b6\u6001-\u52a8\u4f5c-\u5956\u52b1-\u72b6\u6001-\u52a8\u4f5c\uff09\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574TRIS\u7684\u76f8\u79fb\u6765\u5b9e\u73b0\u81ea\u9002\u5e94\u6ce2\u675f\u5f62\u6210\uff0c\u4ee5\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u63d0\u9ad8\u591a\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u5c04\u9891\u94fe\u7684\u6570\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684TRIS-RL\u96f7\u8fbe\u5728\u4fdd\u6301\u6216\uff08\u5f53TRIS\u5355\u5143\u6570\u66f4\u591a\u65f6\uff09\u8d85\u8d8aMIMO\u96f7\u8fbe\u6027\u80fd\u7684\u540c\u65f6\uff0c\u964d\u4f4e\u4e86\u6210\u672c\u548c\u80fd\u8017\u3002", "conclusion": "\u57fa\u4e8eTRIS\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u8ba4\u77e5MIMO\u96f7\u8fbe\u7684\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u529f\u8017\u95ee\u9898\uff0c\u5e76\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2509.13840", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13840", "abs": "https://arxiv.org/abs/2509.13840", "authors": ["Vinay C K", "Vikas Vazhayil", "Madhav rao"], "title": "Characterizing Human Limb Movements Using An In-House Multi-Channel Non-Invasive Surface-EMG System", "comment": null, "summary": "Electromyography (EMG) signals are obtained from muscle cell activity. The\nrecording and analysis of EMG signals has several applications. The EMG is of\ndiagnostic importance for treating patients suffering from neurological and\nneuromuscular disorders. Conventional methods involve placement of invasive\nelectrodes within the muscles to record EMG signals. The goal is to showcase\nthe usage of surface based EMG signals to characterize all possible human limb\nmovements. An in-house non-invasive EMG signal acquisition system that offers\ncharacterization of human limb actions is a suitable candidate for motor\nimpairment studies and easily extendable to design bionics control specifically\nfor neuromuscular disorder patients. An in-house 8-channel surface-EMG signal\nacquisition system was designed, fabricated, and employed for characterizing\nspecific movements of upper and lower limb. The non-invasive acquisition system\ncaptures the compound electromuscular activity generated from the group of\nmuscles. The EMG acquisition system was designed as a modular structure where\nthe front end analog circuit designs were replicated for all 8 channels, and\nwere designed to function independently. Support vector machine (SVM) as\nclassifier models were developed offline to successfully characterize different\nhuman limb actions. The in house built 8 channel acquisition system with ML\nclassifier models were utilized to successfully characterize movements at\nvarious joints of the upper and lower limb including fingers, wrist, elbow,\nshoulder, knee, and ankle individually.", "AI": {"tldr": "\u5229\u7528\u975e\u4fb5\u5165\u5f0f\u8868\u9762\u808c\u7535\u4fe1\u53f7\u91c7\u96c6\u7cfb\u7edf\u548c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u5206\u7c7b\u5668\uff0c\u6210\u529f\u533a\u5206\u4e86\u4eba\u4f53\u7684\u4e0a\u80a2\u548c\u4e0b\u80a2\u7684\u591a\u79cd\u8fd0\u52a8\u3002", "motivation": "\u4e3a\u4e86\u5728\u795e\u7ecf\u548c\u795e\u7ecf\u808c\u8089\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\uff0c\u63d0\u4f9b\u4e00\u79cd\u6bd4\u4f20\u7edf\u4fb5\u5165\u5f0f\u7535\u6781\u66f4\u4f18\u7684\u808c\u7535\u4fe1\u53f7\uff08EMG\uff09\u8bb0\u5f55\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u8fd0\u52a8\u529f\u80fd\u969c\u788d\u7814\u7a76\u548c\u751f\u7269\u7535\u5b50\u5b66\u63a7\u5236\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u5e76\u5236\u4f5c\u4e86\u4e00\u4e2a8\u901a\u9053\u7684\u4f53\u8868EMG\u4fe1\u53f7\u91c7\u96c6\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u524d\u7aef\u6a21\u62df\u7535\u8def\u4e3a8\u4e2a\u901a\u9053\u72ec\u7acb\u8bbe\u8ba1\u3002\u79bb\u7ebf\u5f00\u53d1\u4e86\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u5229\u7528\u8be5\u91c7\u96c6\u7cfb\u7edf\u548c\u6a21\u578b\u5bf9\u4eba\u4f53\u4e0a\u3001\u4e0b\u80a2\uff08\u5305\u62ec\u624b\u6307\u3001\u8155\u90e8\u3001\u8098\u90e8\u3001\u80a9\u90e8\u3001\u819d\u76d6\u548c\u811a\u8e1d\uff09\u7684\u7279\u5b9a\u8fd0\u52a8\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "result": "\u8be58\u901a\u9053\u975e\u4fb5\u5165\u5f0fEMG\u91c7\u96c6\u7cfb\u7edf\u80fd\u591f\u6355\u6349\u808c\u7fa4\u7684\u590d\u5408\u7535\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7SVM\u5206\u7c7b\u5668\u6210\u529f\u5730\u5bf9\u4e0a\u3001\u4e0b\u80a2\u7684\u591a\u79cd\u72ec\u7acb\u8fd0\u52a8\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u975e\u4fb5\u5165\u5f0f\u8868\u9762EMG\u4fe1\u53f7\u91c7\u96c6\u7cfb\u7edf\u548cSVM\u5206\u7c7b\u5668\u7684\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5730\u5bf9\u4eba\u4f53\u80a2\u4f53\u8fd0\u52a8\u8fdb\u884c\u8868\u5f81\uff0c\u4e3a\u8fd0\u52a8\u529f\u80fd\u969c\u788d\u7684\u7814\u7a76\u548c\u795e\u7ecf\u808c\u8089\u75be\u75c5\u60a3\u8005\u7684\u751f\u7269\u7535\u5b50\u5b66\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13570", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.", "AI": {"tldr": "\u5b66\u751f\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u73b0\u6709AI\u68c0\u6d4b\u5de5\u5177\u5e76\u4e0d\u53ef\u9760\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5b66\u751f\u5728\u4e09\u95e8\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u8bfe\u7a0b\uff08\u62bd\u8c61\u4ee3\u6570\u548c\u62d3\u6251\u5b66\uff09\u4e2d\u7684AI\u4f7f\u7528\u548c\u770b\u6cd5\u3002\u8bfe\u7a0b\u653f\u7b56\u5141\u8bb8\u9002\u5ea6\u4f7f\u7528AI\u3002\u901a\u8fc7\u8c03\u67e5\u548c\u8bbf\u8c08\uff0c\u5206\u6790\u4e86\u5b66\u751f\u5982\u4f55\u4f7f\u7528AI\u5de5\u5177\uff0c\u4ed6\u4eec\u5bf9AI\u7684\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u6559\u5b66\u7684\u542f\u793a\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u5c06\u751f\u6210\u5f0fAI\u878d\u5165\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u6559\u5b66\u7684\u672a\u6765\u8003\u91cf\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5174\u8d77\u4ee5\u53ca\u73b0\u6709AI\u68c0\u6d4b\u5de5\u5177\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5236\u5b9a\u9f13\u52b1\u5b66\u751f\u5b66\u4e60\u548c\u6279\u5224\u6027\u601d\u7ef4\u7684\u653f\u7b56\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u4e09\u95e8\u5305\u542b\u62bd\u8c61\u4ee3\u6570\u548c\u62d3\u6251\u5b66\u7684\u5927\u5b66\u6570\u5b66\u8bfe\u7a0b\u4e2d\u7684\u5b66\u751f\u8fdb\u884c\u8c03\u67e5\u548c\u8bbf\u8c08\uff0c\u5206\u6790\u4e86\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u60c5\u51b5\u548c\u770b\u6cd5\u3002\u8fd9\u4e9b\u8bfe\u7a0b\u90fd\u5141\u8bb8\u5b66\u751f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f7f\u7528\u751f\u6210\u5f0fAI\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86\u5b66\u751f\u5982\u4f55\u4f7f\u7528AI\u5de5\u5177\uff0c\u4ed6\u4eec\u5bf9AI\u7684\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u770b\u6cd5\u5bf9\u6559\u6388\u6570\u5b66\u8bc1\u660e\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u6700\u540e\u8ba8\u8bba\u4e86\u5c06\u751f\u6210\u5f0fAI\u6574\u5408\u5230\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u6559\u5b66\u4e2d\u7684\u672a\u6765\u8003\u91cf\u3002"}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528 Doob \u7684 h-transform \u6548\u7387\u5fae\u8c03\uff08DEFT\uff09\u6765\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ee5\u5b9e\u73b0\u865a\u62df\u8bd5\u7a7f\uff08VTO\uff09\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u548c\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0cDEFT-VTON \u65b9\u6cd5\u5728 VTO \u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 VTO \u65b9\u6cd5\u867d\u7136\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u5b9e\u73b0 VTO\u3002", "method": "\u672c\u7814\u7a76\u5c06 Doob \u7684 h-transform \u6548\u7387\u5fae\u8c03\uff08DEFT\uff09\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u65e0\u6761\u4ef6\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u56fe\u50cf\u6761\u4ef6\u7684 VTO\u3002DEFT \u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53c2\u6570\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684 h-transform \u7f51\u7edc\u6765\u5b66\u4e60\u6761\u4ef6 h-transform\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u9700\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8def\u5f84\u4e0a\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u6027\u6765\u84b8\u998f\u6162\u901f\u4f46\u9ad8\u6027\u80fd\u7684\u6269\u6563\u6a21\u578b\u5230\u5feb\u901f\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u6027\u80fd\u3002\u8be5\u635f\u5931\u7ed3\u5408\u4e86\u4e00\u81f4\u6027\u635f\u5931\u548c\u53bb\u566a\u8bc4\u5206\u5339\u914d\u635f\u5931\uff0c\u5e76\u4ee5\u6570\u636e\u81ea\u9002\u5e94\u7684\u65b9\u5f0f\u8fdb\u884c\u5fae\u8c03\u3002", "result": "DEFT-VTON \u65b9\u6cd5\u5728 VTO \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ec5\u9700 15 \u6b65\u53bb\u566a\u5373\u53ef\u8fbe\u5230\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "DEFT-VTON \u65b9\u6cd5\u901a\u8fc7 DEFT \u548c\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709 VTO \u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u7684\u76ee\u6807\u3002"}}
{"id": "2509.13946", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.13946", "abs": "https://arxiv.org/abs/2509.13946", "authors": ["Oskar Leinonen", "Jonas B. Flaten", "Stian D. Bilek", "\u00d8yvind S. Sch\u00f8yen", "Morten Hjorth-Jensen", "Niyaz R. Beysengulov", "Zachary J. Stewart", "Jared D. Weidman", "Angela K. Wilson"], "title": "Design and Dynamics of High-Fidelity Two-Qubit Gates with Electrons on Helium", "comment": "17 pages, 12 figures", "summary": "Systems of individual electrons electrostatically trapped on condensed noble\ngas surfaces have recently attracted considerable interest as potential\nplatforms for quantum computing. The electrons form the qubits of the system,\nand the purity of the noble gas surface protects the relevant quantum\nproperties of each electron. Previous work has indicated that manipulation of a\nconfining double-well potential for electrons on superfluid helium can generate\nentanglement suitable for two-qubit gate operations. In this work, we\nincorporate a time-dependent tuning of the potential shape to further explore\noperation of two-qubit gates with the superfluid helium system. Through\nnumerical time evolution, we show that fast, high-fidelity two-qubit gates can\nbe achieved. In particular, we simulate operation of the \\sqiswap and CZ gates\nand obtain fidelities of 0.999 and 0.996 with execution times of 2.9~ns and\n9.4~ns, respectively. Furthermore, we examine the stability of these gate\nfidelities under non-ideal execution conditions, which reveals new properties\nto consider in the device design. With the insights gained from this work, we\nbelieve that an experimental realization of two-qubit gates using electrons on\nhelium is feasible.", "AI": {"tldr": "\u5229\u7528\u8d85\u6d41\u6c26\u8868\u9762\u7684\u7535\u5b50\u8fdb\u884c\u91cf\u5b50\u8ba1\u7b97\uff0c\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u7684\u52bf\u9631\u8c03\u63a7\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e24\u6bd4\u7279\u95e8\u64cd\u4f5c\u3002", "motivation": "\u4e3a\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u65b0\u7684\u5e73\u53f0\uff0c\u5e76\u63a2\u7d22\u5728\u8d85\u6d41\u6c26\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u4e24\u6bd4\u7279\u95e8\u64cd\u4f5c\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6570\u503c\u65f6\u95f4\u6f14\u5316\u6a21\u62df\uff0c\u5bf9\u52bf\u9631\u5f62\u72b6\u8fdb\u884c\u65f6\u95f4\u4f9d\u8d56\u6027\u8c03\u63a7\uff0c\u4ee5\u5b9e\u73b0\u4e24\u6bd4\u7279\u95e8\u64cd\u4f5c\u3002", "result": "\u6210\u529f\u6a21\u62df\u4e86 CZ \u548c sqISWAP \u95e8\u64cd\u4f5c\uff0c\u4fdd\u771f\u5ea6\u5206\u522b\u8fbe\u5230 0.996 \u548c 0.999\uff0c\u6267\u884c\u65f6\u95f4\u5206\u522b\u4e3a 9.4 ns \u548c 2.9 ns\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e86\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u4fdd\u771f\u5ea6\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u7684\u52bf\u9631\u8c03\u63a7\uff0c\u53ef\u4ee5\u5728\u8d85\u6d41\u6c26\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e24\u6bd4\u7279\u95e8\u64cd\u4f5c\uff0c\u4e3a\u8be5\u7cfb\u7edf\u7684\u5b9e\u9a8c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u3001\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdbLLM\u7684\u6307\u4ee4\u8c03\u4f18\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u56fa\u5b9a\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u7684\u96be\u5ea6\u6307\u6807\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u53d8\u5316\uff0c\u5bfc\u81f4\u5b66\u4e60\u8f68\u8ff9\u56fa\u5b9a\u4e14\u53ef\u80fd\u6b21\u4f18\u3002", "method": "\u63d0\u51faCAMPUS\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u5b50\u8bfe\u7a0b\u9009\u62e9\u3001\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u8c03\u6574\u548c\u591a\u96be\u5ea6\u8c03\u5ea6\u3002", "result": "CAMPUS\u6846\u67b6\u5728\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u6548\u6307\u4ee4\u8c03\u4f18\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CAMPUS\u901a\u8fc7\u5176\u52a8\u6001\u548c\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4e2d\u7684\u8bfe\u7a0b\u50f5\u5316\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6548\u679c\u3002"}}
{"id": "2509.13859", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13859", "abs": "https://arxiv.org/abs/2509.13859", "authors": ["Nikita Polin", "Shangbin Shen", "Fernando Maccari", "Alex Aubert", "Esmaeil Adabifiroozjaei", "Tatiana Smoliarova", "Yangyiwei Yang", "Xinren Chen", "Yurii Skourski", "Alaukik Saxena", "Andr\u00e1s Kov\u00e1cs", "Rafal E. Dunin-Borkowski", "Michael Farle", "Bai-Xiang Xu", "Leopoldo Molina-Luna", "Oliver Gutfleisch", "Baptiste Gault", "Konstantin Skokov"], "title": "Direct observation of nanoscale pinning centers in Ce(Co0.8Cu0.2)5.4 permanent magnets", "comment": null, "summary": "Permanent magnets containing rare earth elements are essential components for\nthe electrification of society. Ce(Co1-xCux)5 permanent magnets are a model\nsystem known for their substantial coercivity, yet the underlying mechanism\nremains unclear. Here, we investigate Ce(Co0.8Cu0.2)5.4 magnets with a\ncoercivity of ~1 T. Using transmission electron microscopy (TEM) and atom probe\ntomography (APT), we identify a nanoscale cellular structure formed by spinodal\ndecomposition. Cu-poor cylindrical cells (~5-10 nm in diameter, ~20 nm long)\nhave a disordered CeCo5-type structure and a composition Ce(Co0.9Cu0.1)5.3.\nCu-rich cell boundaries are ~ 5 nm thick and exhibit a modified CeCo5\nstructure, with Cu ordered on the Co sites and a composition Ce(Co0.7Cu0.3)5.0.\nMicromagnetic simulations demonstrate that the intrinsic Cu concentration\ngradients up to 12 at.% Cu/nm lead to a spatial variation in magnetocrystalline\nanisotropy and domain wall energy, resulting in effective pinning and high\ncoercivity. Compared to Sm2Co17-type magnets, Ce(Co0.8Cu0.2)5.4 displays a\nfiner-scale variation of conventional pinning with lower structural and\nchemical contrast in its underlying nanostructure. The identification of\nnanoscale chemical segregation in nearly single-phase Ce(Co0.8Cu0.2)5.4 magnets\nprovides a microstructural basis for the long-standing phenomenon of \"giant\nintrinsic magnetic hardness\" in systems such as SmCo5-xMx, highlighting avenues\nfor designing rare-earth-lean permanent magnets via controlled nanoscale\nsegregation.", "AI": {"tldr": "Ce(Co0.8Cu0.2)5.4 \u78c1\u4f53\u7684\u78c1\u6ede\u56destrictly\u5ea6\u673a\u5236\u6e90\u4e8e\u7eb3\u7c73\u5c3a\u5ea6\u4e0b\u7684\u81ea\u65cbodal\u5206\u89e3\u9020\u6210\u7684\u7ec6\u80de\u72b6\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u7531\u5bcc\u94dc\u548c\u8d2b\u94dc\u533a\u57df\u7ec4\u6210\uff0c\u5f15\u8d77\u78c1\u6676\u5404\u5411\u5f02\u6027\u3002\u8fd9\u79cd\u7ed3\u6784\u4e3a\u7a00\u571f\u78c1\u4f53\u63d0\u4f9b\u4e86\u5fae\u89c2\u7ed3\u6784\u57fa\u7840\u3002", "motivation": "Ce(Co1-xCux)5 \u6c38\u78c1\u4f53\u7684\u77eb\u987d\u529b\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u5229\u7528\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c (TEM) \u548c\u539f\u5b50\u63a2\u9488\u5c42\u6790\u6210\u50cf (APT) \u6280\u672f\uff0c\u7ed3\u5408\u5fae\u78c1\u6a21\u62df\uff0c\u7814\u7a76 Ce(Co0.8Cu0.2)5.4 \u78c1\u4f53\u7684\u5fae\u89c2\u7ed3\u6784\u548c\u78c1\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0 Ce(Co0.8Cu0.2)5.4 \u78c1\u4f53\u5b58\u5728\u7eb3\u7c73\u5c3a\u5ea6\u7684\u7ec6\u80de\u72b6\u7ed3\u6784\uff0c\u7531\u8d2b\u94dc\u5706\u67f1\u5f62\u7ec6\u80de\u548c\u5bcc\u94dc\u7ec6\u80de\u8fb9\u754c\u7ec4\u6210\uff0c\u8fd9\u79cd\u7ed3\u6784\u5bfc\u81f4\u4e86\u78c1\u6676\u5404\u5411\u5f02\u6027\u548c\u7574\u58c1\u80fd\u91cf\u7684\u7a7a\u95f4\u53d8\u5316\uff0c\u4ece\u800c\u6709\u6548\u9489\u624e\u78c1\u7574\u58c1\uff0c\u4ea7\u751f\u9ad8\u77eb\u987d\u529b\u3002", "conclusion": "\u7eb3\u7c73\u5c3a\u5ea6\u7684\u5316\u5b66\u504f\u6790\u662f Ce(Co0.8Cu0.2)5.4 \u78c1\u4f53\u5177\u6709\u9ad8\u77eb\u987d\u529b\u7684\u5fae\u89c2\u7ed3\u6784\u57fa\u7840\uff0c\u4e3a\u8bbe\u8ba1\u7a00\u571f\u8d44\u6e90\u6d88\u8017\u5c11\u7684\u6c38\u78c1\u4f53\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.13595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13595", "abs": "https://arxiv.org/abs/2509.13595", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Leg-Arm Coordinated Operation for Curtain Wall Installation", "comment": null, "summary": "With the acceleration of urbanization, the number of high-rise buildings and\nlarge public facilities is increasing, making curtain walls an essential\ncomponent of modern architecture with widespread applications. Traditional\ncurtain wall installation methods face challenges such as variable on-site\nterrain, high labor intensity, low construction efficiency, and significant\nsafety risks. Large panels often require multiple workers to complete\ninstallation. To address these issues, based on a hexapod curtain wall\ninstallation robot, we design a hierarchical optimization-based whole-body\ncontrol framework for coordinated arm-leg planning tailored to three key tasks:\nwall installation, ceiling installation, and floor laying. This framework\nintegrates the motion of the hexapod legs with the operation of the folding arm\nand the serial-parallel manipulator. We conduct experiments on the hexapod\ncurtain wall installation robot to validate the proposed control method,\ndemonstrating its capability in performing curtain wall installation tasks. Our\nresults confirm the effectiveness of the hierarchical optimization-based\narm-leg coordination framework for the hexapod robot, laying the foundation for\nits further application in complex construction site environments.", "AI": {"tldr": "\u57fa\u4e8e\u516d\u8db3\u673a\u5668\u4eba\u8bbe\u8ba1\u4e86\u5206\u5c42\u4f18\u5316\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u8c03\u624b\u81c2-\u817f\u90e8\u89c4\u5212\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5e55\u5899\u5b89\u88c5\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u8fdb\u7a0b\u7684\u52a0\u5feb\uff0c\u9ad8\u5c42\u5efa\u7b51\u548c\u5927\u578b\u516c\u5171\u8bbe\u65bd\u7684\u589e\u52a0\uff0c\u5e55\u5899\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002\u4f20\u7edf\u5b89\u88c5\u65b9\u6cd5\u5b58\u5728\u5730\u5f62\u53d8\u5316\u3001\u52b3\u52a8\u5f3a\u5ea6\u5927\u3001\u6548\u7387\u4f4e\u3001\u5b89\u5168\u98ce\u9669\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u516d\u8db3\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u7684\u5206\u5c42\u4f18\u5316\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u516d\u8db3\u817f\u7684\u8fd0\u52a8\u4e0e\u6298\u53e0\u624b\u81c2\u548c\u4e32\u5e76\u8054\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\uff0c\u4ee5\u5b9e\u73b0\u5899\u4f53\u5b89\u88c5\u3001\u5929\u82b1\u677f\u5b89\u88c5\u548c\u5730\u677f\u94fa\u8bbe\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u7684\u624b\u81c2-\u817f\u90e8\u534f\u8c03\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u5728\u516d\u8db3\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u6267\u884c\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u5206\u5c42\u4f18\u5316\u624b\u81c2-\u817f\u90e8\u534f\u8c03\u6846\u67b6\u5bf9\u4e8e\u516d\u8db3\u673a\u5668\u4eba\u662f\u6709\u6548\u7684\uff0c\u4e3a\u5728\u590d\u6742\u5efa\u7b51\u5de5\u5730\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.14186", "categories": ["eess.SP", "cs.SY", "eess.SY", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.14186", "abs": "https://arxiv.org/abs/2509.14186", "authors": ["Patrick Vincent N. Lubenia", "Taposh Banerjee"], "title": "Quickest Change Detection with Cost-Constrained Experiment Design", "comment": null, "summary": "In the classical quickest change detection problem, an observer performs only\none experiment to monitor a stochastic process. This paper considers the case\nwhere, at each observation time, the decision-maker needs to choose between\nmultiple experiments with different information qualities and costs. The goal\nis to minimize the worst-case average detection delay subject to false alarm\nand cost constraints. An algorithm called the 2E-CUSUM Algorithm has been\ndeveloped to achieve this goal for the two-experiment case. Extensions to\nmultiple-experiment designs are also studied, and 2E-CUSUM is extended\naccordingly. Data efficiency, where the observer has the choice not to perform\nan experiment, is explored as well. The proposed algorithms are analyzed and\nshown to be asymptotically optimal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u6700\u4f18\u5b9e\u9a8c\u9009\u62e9\u4e0b\u7684\u6700\u5feb\u53d8\u5316\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a2E-CUSUM\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u8bc1\u660e\u4e86\u5176\u6e10\u8fd1\u6700\u4f18\u6027\u3002", "motivation": "\u5728\u7ecf\u5178\u7684\u6700\u5feb\u53d8\u5316\u68c0\u6d4b\u95ee\u9898\u4e2d\uff0c\u89c2\u5bdf\u8005\u53ea\u6267\u884c\u4e00\u6b21\u5b9e\u9a8c\u3002\u672c\u8bba\u6587\u8003\u8651\u4e86\u51b3\u7b56\u8005\u5728\u6bcf\u6b21\u89c2\u5bdf\u65f6\u9700\u8981\u5728\u4fe1\u606f\u8d28\u91cf\u548c\u6210\u672c\u4e0d\u540c\u7684\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u9009\u62e9\u7684\u60c5\u51b5\uff0c\u76ee\u6807\u662f\u5728\u865a\u8b66\u548c\u6210\u672c\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a2E-CUSUM\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u53cc\u5b9e\u9a8c\u60c5\u51b5\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u591a\u5b9e\u9a8c\u8bbe\u8ba1\u3002\u8fd8\u63a2\u8ba8\u4e86\u89c2\u5bdf\u8005\u53ef\u4ee5\u9009\u62e9\u4e0d\u6267\u884c\u5b9e\u9a8c\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\u3002", "result": "2E-CUSUM\u7b97\u6cd5\u88ab\u63d0\u51fa\u5e76\u5e94\u7528\u4e8e\u53cc\u5b9e\u9a8c\u60c5\u51b5\uff0c\u5e76\u6269\u5c55\u5230\u591a\u5b9e\u9a8c\u8bbe\u8ba1\u3002\u6570\u636e\u6548\u7387\u95ee\u9898\u4e5f\u88ab\u8003\u8651\u5728\u5185\u3002\u6240\u6709\u63d0\u51fa\u7684\u7b97\u6cd5\u90fd\u7ecf\u8fc7\u5206\u6790\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u88ab\u8bc1\u660e\u662f\u6e10\u8fd1\u6700\u4f18\u7684\u3002"}}
{"id": "2509.13934", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13934", "abs": "https://arxiv.org/abs/2509.13934", "authors": ["Zhixion Chen", "Jiangzhou Wang", "and Hyundong Shin", "Arumugam Nallanathan"], "title": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection", "comment": "14pages, 8 figures", "summary": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches.", "AI": {"tldr": "\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u6570\u636e\u6536\u96c6\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u7eed\u822a\u548c\u901a\u4fe1\u8303\u56f4\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u9700\u8981\u667a\u80fd\u7684\u8f68\u8ff9\u89c4\u5212\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u6210\u672c\u548c\u98ce\u9669\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u79bb\u7ebfRL\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u4ecd\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u98ce\u9669\uff0c\u5e76\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u7684\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5927\u5316\u6570\u636e\u6536\u96c6\u7684\u80fd\u6e90\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5c06\u8d44\u6e90\u5206\u914d\u5b50\u95ee\u9898\u8f6c\u5316\u4e3a\u7b49\u4ef7\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u4ee5\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u8fdb\u884c\u6c42\u89e3\u3002\u63a5\u7740\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u3001\u5e26\u6709\u8bc4\u8bba\u5bb6\u6b63\u5219\u5316\u7684\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u6846\u67b6\uff08LLM-CRDT\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u6709\u6548\u7684\u65e0\u4eba\u673a\u63a7\u5236\u7b56\u7565\u3002LLM-CRDT\u7ed3\u5408\u4e86DT\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u57fa\u4e8e\u8bc4\u8bba\u5bb6\u7684\u4ef7\u503c\u6307\u5bfc\uff0c\u901a\u8fc7\u8bc4\u8bba\u5bb6\u7f51\u7edc\u6b63\u5219\u5316DT\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u6b21\u4f18\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u7b56\u7565\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670dTransformer\u6a21\u578b\u5bf9\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u9884\u8bad\u7ec3\u7684LLM\u4f5c\u4e3aDT\u6a21\u578b\u7684Transformer\u9aa8\u5e72\uff0c\u5e76\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff08LoRA\uff09\uff0c\u80fd\u591f\u4ee5\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u5feb\u901f\u9002\u5e94\u65e0\u4eba\u673a\u63a7\u5236\u4efb\u52a1\u3002\u5e7f\u6cdb\u7684\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cLLM-CRDT\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u7684\u5728\u7ebf\u548c\u79bb\u7ebfRL\u65b9\u6cd5\uff0c\u5728\u80fd\u6e90\u6548\u7387\u65b9\u9762\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684DT\u65b9\u6cd5\u9ad8\u51fa36.7%\u3002", "motivation": "\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u6570\u636e\u6536\u96c6\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u7eed\u822a\u548c\u901a\u4fe1\u8303\u56f4\u7684\u9650\u5236\uff0c\u9700\u8981\u667a\u80fd\u7684\u8f68\u8ff9\u89c4\u5212\u3002\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u98ce\u9669\uff0c\u800c\u79bb\u7ebfRL\u65b9\u6cd5\u5219\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5bf9\u6570\u636e\u96c6\u8d28\u91cf\u8981\u6c42\u9ad8\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u65e0\u4eba\u673a\u6570\u636e\u6536\u96c6\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u80fd\u4ece\u6b21\u4f18\u6570\u636e\u96c6\u4e2d\u6709\u6548\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u7684\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u5c06\u8d44\u6e90\u5206\u914d\u5b50\u95ee\u9898\u8f6c\u5316\u4e3a\u7b49\u4ef7\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u6700\u4f18\u6c42\u89e3\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-CRDT\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u8bc4\u8bba\u5bb6\u7f51\u7edc\u7684\u4ef7\u503c\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5229\u7528\u8bc4\u8bba\u5bb6\u7f51\u7edc\u5bf9DT\u6a21\u578b\u8fdb\u884c\u6b63\u5219\u5316\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u4ece\u6b21\u4f18\u6570\u636e\u96c6\u5b66\u4e60\u6709\u6548\u7b56\u7565\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3aDT\u6a21\u578b\u7684Transformer\u9aa8\u5e72\uff0c\u5e76\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff08LoRA\uff09\uff0c\u964d\u4f4e\u4e86\u5bf9\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u8981\u6c42\u3002", "result": "LLM-CRDT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408DT\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u8bc4\u8bba\u5bb6\u7f51\u7edc\u7684\u4ef7\u503c\u6307\u5bfc\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3LLM\u548cLoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u4e0e\u73b0\u6709\u7684\u5728\u7ebf\u548c\u79bb\u7ebfRL\u65b9\u6cd5\u4ee5\u53ca\u5148\u8fdb\u7684DT\u65b9\u6cd5\u76f8\u6bd4\uff0cLLM-CRDT\u5728\u80fd\u6e90\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u53ef\u8fbe36.7%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684LLM-CRDT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u6570\u636e\u6536\u96c6\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA\u662f\u4e00\u4e2a\u7528\u4e8e\u6307\u5b9a\u57fa\u4e8eLLM\u7684\u793e\u4f1a\u6a21\u62df\u4e2d\u4ee3\u7406\u884c\u4e3a\u7684\u65b0\u9896\u5de5\u5177\u5305\uff0c\u5b83\u901a\u8fc7\u660e\u786e\u7f16\u7a0b\u4ee3\u7406\u7684\u8ba4\u77e5\u504f\u5dee\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u65b9\u6cd5\u5728LLM\u793e\u4f1a\u6a21\u62df\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u4ee3\u7406\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u65e0\u6cd5\u6355\u6349\u63cf\u8ff0\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "method": "CoBRA\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a1. \u8ba4\u77e5\u504f\u5dee\u6307\u6570\uff08CBI\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u4ee3\u7406\u5728\u4e00\u7ec4\u7ecf\u5178\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u53cd\u5e94\u6765\u8861\u91cf\u5176\u8ba4\u77e5\u504f\u5dee\uff1b2. \u884c\u4e3a\u8c03\u8282\u5f15\u64ce\uff08BRE\uff09\uff0c\u7528\u4e8e\u8c03\u6574\u4ee3\u7406\u884c\u4e3a\u4ee5\u5c55\u73b0\u53d7\u63a7\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "result": "CoBRA\u80fd\u591f\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u7cbe\u786e\u7f16\u7a0b\u793e\u4f1a\u4ee3\u7406\u6240\u5c55\u73b0\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u6f14\u793a\u548c\u6280\u672f\u57fa\u51c6\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "CoBRA\u4e3aLLM\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u4ee3\u7406\u884c\u4e3a\u89c4\u8303\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u7cbe\u786e\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u7f16\u7a0b\u8ba4\u77e5\u504f\u5dee\u6765\u63d0\u9ad8\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u548c\u7ec6\u5fae\u6027\u3002"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "\u751f\u6210\u7528\u4e8e\u6539\u5584\u884c\u4eba\u8bc6\u522b\u7684\u5408\u6210\u6570\u636e\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u6dfb\u52a0\u865a\u62df\u884c\u4eba\u7684\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u65b0\u7684\u751f\u6210\u7f51\u7edc\u6765\u6539\u5584\u5149\u7167\u6761\u4ef6\uff0c\u4ee5\u63d0\u9ad8\u771f\u5b9e\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u3002", "motivation": "\u5408\u6210\u6570\u636e\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5bf9\u4e8e\u8986\u76d6\u7279\u5b9a\u4ea4\u901a\u573a\u666f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5e38\u5b58\u5728\u57df\u95f4\u9699\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6765\u751f\u6210\u5305\u542b\u865a\u62df\u884c\u4eba\uff08VRUs\uff09\u7684\u5b9a\u5236\u4ea4\u901a\u573a\u666f\uff0c\u4ee5\u63d0\u9ad8\u884c\u4eba\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06\u865a\u62df\u884c\u4eba\u6dfb\u52a0\u5230Cityscapes\u6570\u636e\u96c6\u7684\u6d41\u7a0b\u3002\u4e3a\u4e86\u63d0\u9ad8\u589e\u5f3a\u7684\u771f\u5b9e\u6027\uff0c\u7814\u7a76\u4eba\u5458\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u6297\u6027\u5730\u5b66\u4e60\u6570\u636e\u96c6\u7684\u5149\u7167\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6570\u636e\u589e\u5f3a\u548c\u65b0\u7684\u751f\u6210\u7f51\u7edc\u67b6\u6784\uff0c\u672c\u7814\u7a76\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e2d\u7684\u57df\u95f4\u9699\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura Garc\u00eda-Sardi\u00f1a", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "\u7814\u7a76\u4e86\u663e\u5f0f\u8bed\u6cd5\u6027\u522b\u6807\u8bb0\u5bf9\u62db\u8058\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6027\u522b\u56e0\u7d20\u7684\u6392\u540d\u6bd4\u8f83\u6307\u6807RBO\u6765\u8bc4\u4f30\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u663e\u5f0f\u8bed\u6cd5\u6027\u522b\u6807\u8bb0\u5982\u4f55\u5f71\u54cd\u62db\u8058\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6027\u522b\u504f\u89c1\u7684\u6307\u6807\u3002", "method": "\u63d0\u51fa\u4f7f\u7528RBO\uff08Rank-Biased Overlap\uff09\u6307\u6807\u6765\u6bd4\u8f83\u6392\u540d\uff0c\u5e76\u751f\u6210\u5305\u542b\u9633\u6027\u548c\u9634\u6027\u5f62\u5f0f\u7684\u804c\u4e1a\u540d\u79f0\u7684\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u522b\u504f\u89c1\u3002", "result": "\u8bc4\u4f30\u4e86\u51e0\u4e2a\u5f00\u7bb1\u5373\u7528\u7684\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u90fd\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u663e\u5f0f\u8bed\u6cd5\u6027\u522b\u6807\u8bb0\u4f1a\u5bf9\u62db\u8058\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u4ea7\u751f\u6027\u522b\u504f\u89c1\u3002"}}
{"id": "2509.13705", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13705", "abs": "https://arxiv.org/abs/2509.13705", "authors": ["Koki Chinzei", "Quoc Hoan Tran", "Norifumi Matsumoto", "Yasuhiro Endo", "Hirotaka Oshima"], "title": "Learning quantum many-body data locally: A provably scalable framework", "comment": "38 pages, 5 figures", "summary": "Machine learning (ML) holds great promise for extracting insights from\ncomplex quantum many-body data obtained in quantum experiments. This approach\ncan efficiently solve certain quantum problems that are classically\nintractable, suggesting potential advantages of harnessing quantum data.\nHowever, addressing large-scale problems still requires significant amounts of\ndata beyond the limited computational resources of near-term quantum devices.\nWe propose a scalable ML framework called Geometrically Local Quantum Kernel\n(GLQK), designed to efficiently learn quantum many-body experimental data by\nleveraging the exponential decay of correlations, a phenomenon prevalent in\nnoncritical systems. In the task of learning an unknown polynomial of quantum\nexpectation values, we rigorously prove that GLQK substantially improves\npolynomial sample complexity in the number of qubits $n$, compared to the\nexisting shadow kernel, by constructing a feature space from local quantum\ninformation at the correlation length scale. This improvement is particularly\nnotable when each term of the target polynomial involves few local subsystems.\nRemarkably, for translationally symmetric data, GLQK achieves constant sample\ncomplexity, independent of $n$. We numerically demonstrate its high scalability\nin two learning tasks on quantum many-body phenomena. These results establish\nnew avenues for utilizing experimental data to advance the understanding of\nquantum many-body physics.", "AI": {"tldr": "ML\u53ef\u7528\u4e8e\u4ece\u91cf\u5b50\u591a\u4f53\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u89c1\u89e3\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u3002 GLQK\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684ML\u6846\u67b6\uff0c\u53ef\u5229\u7528\u76f8\u5173\u6027\u7684\u6307\u6570\u8870\u51cf\u6765\u5904\u7406\u8fd9\u4e9b\u6570\u636e\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u6837\u672c\u590d\u6742\u6027\u3002", "motivation": "\u4ece\u590d\u6742\u7684\u91cf\u5b50\u591a\u4f53\u6570\u636e\u4e2d\u63d0\u53d6\u89c1\u89e3\uff0c\u5e76\u89e3\u51b3\u7ecf\u5178\u4e0a\u96be\u4ee5\u5904\u7406\u7684\u91cf\u5b50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u51e0\u4f55\u5c40\u90e8\u91cf\u5b50\u6838\uff08GLQK\uff09\u7684\u53ef\u6269\u5c55\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u975e\u4e34\u754c\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5173\u8054\u7684\u6307\u6570\u8870\u51cf\u6765\u5b66\u4e60\u91cf\u5b50\u591a\u4f53\u5b9e\u9a8c\u6570\u636e\u3002", "result": "GLQK\u5728\u5b66\u4e60\u91cf\u5b50\u671f\u671b\u503c\u591a\u9879\u5f0f\u65b9\u9762\uff0c\u6837\u672c\u590d\u6742\u6027\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u91cf\u5b50\u6bd4\u7279\u6570n\u65b9\u9762\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\uff0c\u5728\u5e73\u79fb\u5bf9\u79f0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6837\u672c\u590d\u6742\u6027\u4e0en\u65e0\u5173\u3002\u5176\u53ef\u6269\u5c55\u6027\u5df2\u901a\u8fc7\u91cf\u5b50\u591a\u4f53\u73b0\u8c61\u7684\u4e24\u4e2a\u5b66\u4e60\u4efb\u52a1\u5f97\u5230\u6570\u503c\u8bc1\u660e\u3002", "conclusion": "GLQK\u4e3a\u5229\u7528\u5b9e\u9a8c\u6570\u636e\u6765\u589e\u8fdb\u5bf9\u91cf\u5b50\u591a\u4f53\u7269\u7406\u5b66\u7684\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.13870", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13870", "abs": "https://arxiv.org/abs/2509.13870", "authors": ["Oscar Cespedes", "Hari B. Vasili", "Matthew Rogers", "Paul S. Keatley", "Manan Ali", "Bryan Hickey", "Robert J. Hicken"], "title": "Tuning and Suppression of YIG Magnetisation Dynamics via Antiferromagnetic Interface Coupling", "comment": "10 pages, 4 figures", "summary": "The magnetisation dynamics of yttrium iron garnet (Y3Fe5O12, YIG) are key to\nthe operation of spintronic and microwave devices. Here, we report a pathway to\nmanipulate the frequency, damping and absorption of YIG thin films via\ninterface coupling. The growth on YIG of PtMn, a metallic antiferromagnet,\nleads to a power dependence of the oscillation frequency and an increased\nlinewidth at low fields. In gadolinium iron garnet/YIG film bilayers, the two\nfilms couple antiferromagnetically at low temperatures and there is a strong\ndamping of the magnetisation dynamics that is further enhanced at the spin-flop\nfield, suppressing the FMR signal. When combining both GdIG and PtMn\ninterfaces, we can tune the exponent of the power dependence of frequency with\nfield and achieve an almost complete quenching of the magnetisation dynamics\nover a range of fields/frequencies due to non-collinear magnetic order. These\neffects offer a means to tune and suppress magnetisation dynamics for frequency\nfilters, magnonics, spin pumping and other applications.", "AI": {"tldr": "\u901a\u8fc7\u754c\u9762\u8026\u5408\u8c03\u63a7YIG\u8584\u819c\u7684\u78c1\u5316\u52a8\u529b\u5b66\uff0c\u53ef\u5b9e\u73b0\u9891\u7387\u3001\u963b\u5c3c\u548c\u5438\u6536\u7684\u53ef\u8c03\u6027\uff0c\u9002\u7528\u4e8e\u6ee4\u6ce2\u5668\u3001\u81ea\u65cb\u7535\u5b50\u5b66\u7b49\u5e94\u7528\u3002", "motivation": "YIG\u78c1\u5316\u52a8\u529b\u5b66\u5bf9\u81ea\u65cb\u7535\u5b50\u548c\u5fae\u6ce2\u5668\u4ef6\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7814\u7a76\u8c03\u63a7\u65b9\u6cd5\u3002", "method": "\u5728YIG\u8584\u819c\u4e0a\u751f\u957fPtMn\u6216GdIG\uff0c\u7814\u7a76\u754c\u9762\u8026\u5408\u5bf9\u78c1\u5316\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u4e24\u79cd\u754c\u9762\u8fdb\u884c\u8c03\u63a7\u3002", "result": "PtMn/YIG\u8584\u819c\u8868\u73b0\u51fa\u529f\u7387\u4f9d\u8d56\u7684\u632f\u8361\u9891\u7387\u548c\u4f4e\u573a\u4e0b\u7684\u7ebf\u5bbd\u589e\u52a0\uff1bGdIG/YIG\u8584\u819c\u5728\u4f4e\u6e29\u4e0b\u8868\u73b0\u51fa\u53cd\u94c1\u78c1\u8026\u5408\uff0c\u5e76\u4ea7\u751f\u5f3a\u963b\u5c3c\uff0c\u5728\u7279\u5b9a\u573a\u4e0b\u8fdb\u4e00\u6b65\u589e\u5f3a\uff1b\u7ed3\u5408GdIG\u548cPtMn\u754c\u9762\u53ef\u4ee5\u8c03\u63a7\u9891\u7387\u7684\u529f\u7387\u4f9d\u8d56\u6027\uff0c\u5e76\u5728\u4e00\u5b9a\u573a/\u9891\u7387\u8303\u56f4\u5185\u6291\u5236\u78c1\u5316\u52a8\u529b\u5b66\u3002", "conclusion": "\u901a\u8fc7\u754c\u9762\u8026\u5408\uff0c\u7279\u522b\u662f\u7ed3\u5408GdIG\u548cPtMn\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8c03\u63a7YIG\u8584\u819c\u7684\u78c1\u5316\u52a8\u529b\u5b66\uff0c\u4e3a\u6ee4\u6ce2\u5668\u3001\u81ea\u65cb\u7535\u5b50\u5b66\u7b49\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.13649", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13649", "abs": "https://arxiv.org/abs/2509.13649", "authors": ["M\u00e9lon\u00e9 Nyoba Tchonkeu", "Soulaimane Berkane", "Tarek Hamel"], "title": "Barometer-Aided Attitude Estimation", "comment": "6 pages, 4 figures. this manuscript is submitted to IEEE Control\n  Systems Letters (L-CSS) with American Control Conference (ACC) option", "summary": "Accurate and robust attitude estimation is a central challenge for autonomous\nvehicles operating in GNSS-denied or highly dynamic environments. In such\ncases, Inertial Measurement Units (IMUs) alone are insufficient for reliable\ntilt estimation due to the ambiguity between gravitational and inertial\naccelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,\nDoppler radar, or visual odometry, are often used, they can be unavailable,\nintermittent, or costly. This work introduces a barometer-aided attitude\nestimation architecture that leverages barometric altitude measurements to\ninfer vertical velocity and attitude within a nonlinear observer on SO(3). The\ndesign cascades a deterministic Riccati observer with a complementary filter,\nensuring Almost Global Asymptotic Stability (AGAS) under a uniform\nobservability condition while maintaining geometric consistency. The analysis\nhighlights barometer-aided estimation as a lightweight and effective\ncomplementary modality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6c14\u538b\u8ba1\u8f85\u52a9\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728GNSS\u4fe1\u53f7\u5f31\u6216\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u4ec5\u4f9d\u8d56IMU\u8fdb\u884c\u503e\u659c\u4f30\u8ba1\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u5f31\u6216\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u4ec5\u4f9d\u8d56IMU\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u5b58\u5728\u91cd\u529b\u4e0e\u60ef\u6027\u52a0\u901f\u5ea6\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u800c\u8f85\u52a9\u901f\u5ea6\u4f20\u611f\u5668\u53ef\u80fd\u4e0d\u53ef\u7528\u3001\u95f4\u6b47\u6027\u6216\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u538b\u8ba1\u8f85\u52a9\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u975e\u7ebf\u6027SO(3)\u89c2\u6d4b\u5668\u4e2d\u7684\u6c14\u538b\u9ad8\u5ea6\u6d4b\u91cf\u6765\u63a8\u65ad\u5782\u76f4\u901f\u5ea6\u548c\u59ff\u6001\u3002\u8be5\u8bbe\u8ba1\u5c06\u786e\u5b9a\u6027Riccati\u89c2\u6d4b\u5668\u4e0e\u4e92\u8865\u6ee4\u6ce2\u5668\u7ea7\u8054\uff0c\u786e\u4fdd\u5728\u7edf\u4e00\u7684\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff08AGAS\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6c14\u538b\u8ba1\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86IMU\u5728\u7279\u5b9a\u73af\u5883\u4e0b\u59ff\u6001\u4f30\u8ba1\u7684\u4e0d\u8db3\uff0c\u5e76\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "\u6c14\u538b\u8ba1\u8f85\u52a9\u7684\u59ff\u6001\u4f30\u8ba1\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u8865\u5145\u4f20\u611f\u65b9\u5f0f\uff0c\u80fd\u591f\u63d0\u9ad8\u5728GNSS\u4fe1\u53f7\u5f31\u6216\u52a8\u6001\u73af\u5883\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13725", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13725", "abs": "https://arxiv.org/abs/2509.13725", "authors": ["Md Sabbir Ahmed", "Noah French", "Mark Rucker", "Zhiyuan Wang", "Taylor Myers-Brower", "Kaitlyn Petz", "Mehdi Boukhechba", "Bethany A. Teachman", "Laura E. Barnes"], "title": "WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data", "comment": null, "summary": "Social anxiety is a common mental health condition linked to significant\nchallenges in academic, social, and occupational functioning. A core feature is\nelevated momentary (state) anxiety in social situations, yet little prior work\nhas measured or predicted fluctuations in this anxiety throughout the day.\nCapturing these intra-day dynamics is critical for designing real-time,\npersonalized interventions such as Just-In-Time Adaptive Interventions\n(JITAIs). To address this gap, we conducted a study with socially anxious\ncollege students (N=91; 72 after exclusions) using our custom smartwatch-based\nsystem over an average of 9.03 days (SD = 2.95). Participants received seven\necological momentary assessments (EMAs) per day to report state anxiety. We\ndeveloped a base model on over 10,000 days of external heart rate data,\ntransferred its representations to our dataset, and fine-tuned it to generate\nprobabilistic predictions. These were combined with trait-level measures in a\nmeta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety\ndetection in our dataset. To evaluate generalizability, we applied the training\napproach to a separate hold-out set from the TILES-18 dataset-the same dataset\nused for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%\nbalanced accuracy, outperforming prior work by at least 7%.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u624b\u8868\u76d1\u6d4b\u548c\u9884\u6d4b\u5927\u5b66\u751f\u7fa4\u4f53\u7684\u65e5\u5e38\u793e\u4ea4\u7126\u8651\u6c34\u5e73\uff0c\u4ee5\u5f00\u53d1\u5b9e\u65f6\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u5e38\u5bfc\u81f4\u5b66\u4e1a\u3001\u793e\u4ea4\u548c\u804c\u4e1a\u529f\u80fd\u53d7\u635f\u3002\u867d\u7136\u65e5\u5185\u7126\u8651\u6ce2\u52a8\u662f\u5176\u6838\u5fc3\u7279\u5f81\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u5bf9\u5176\u6d4b\u91cf\u548c\u9884\u6d4b\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u5b9e\u65f6\u4e2a\u6027\u5316\u5e72\u9884\uff08\u5982JITAIs\uff09\u7684\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u667a\u80fd\u624b\u8868\uff0c\u5bf991\u540d\uff08\u6392\u9664\u540e72\u540d\uff09\u6709\u793e\u4ea4\u7126\u8651\u7684\u5927\u5b66\u751f\u8fdb\u884c\u4e86\u5e73\u57479.03\u5929\u7684\u76d1\u6d4b\u3002\u7814\u7a76\u4eba\u5458\u6bcf\u65e5\u8fdb\u884c\u4e03\u6b21\u751f\u6001\u77ac\u95f4\u8bc4\u4f30\uff08EMAs\uff09\uff0c\u6536\u96c6\u88ab\u8bd5\u7684\u5373\u65f6\u7126\u8651\u6c34\u5e73\u3002\u57fa\u4e8e\u8d85\u8fc710000\u5929\u7684\u5fc3\u7387\u6570\u636e\u6784\u5efa\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u8fc1\u79fb\u5230\u672c\u7814\u7a76\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u884c\u6982\u7387\u9884\u6d4b\u3002\u8be5\u9884\u6d4b\u7ed3\u679c\u4e0e\u7279\u8d28\u7126\u8651\u6c34\u5e73\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5143\u5b66\u4e60\u5668\u3002\u53e6\u5916\uff0c\u7814\u7a76\u8fd8\u5c06\u8bad\u7ec3\u65b9\u6cd5\u5e94\u7528\u4e8eTILES-18\u6570\u636e\u96c6\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5b66\u751f\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u72b6\u6001\u7126\u8651\u68c0\u6d4b\u65b9\u9762\u8fbe\u5230\u4e8660.4%\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002\u5728TILES-18\u6570\u636e\u96c6\u4e0a\uff0c\u5bf910095\u6b21\u6bcf\u65e5EMAs\u8fdb\u884c\u5206\u6790\uff0c\u6a21\u578b\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u5230\u4e8659.1%\uff0c\u6bd4\u5148\u524d\u7814\u7a76\u81f3\u5c11\u63d0\u9ad8\u4e867%\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u667a\u80fd\u624b\u8868\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u65e5\u5185\u793e\u4ea4\u7126\u8651\u6c34\u5e73\u7684\u65b9\u6cd5\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u79ef\u6781\u6210\u679c\uff0c\u4e3a\u5f00\u53d1\u5b9e\u65f6\u793e\u4ea4\u7126\u8651\u5e72\u9884\u63aa\u65bd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.14201", "categories": ["eess.SP", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14201", "abs": "https://arxiv.org/abs/2509.14201", "authors": ["Guangjin Pan", "Liping Bai", "Zhuojun Tian", "Hui Chen", "Mehdi Bennis", "Henk Wymeersch"], "title": "Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems", "comment": "5 pages, 2 figures", "summary": "Integrated sensing and communication (ISAC) is a core technology for 6G, and\nits application to closed-loop sensing, communication, and control (SCC)\nenables various services. Existing SCC solutions often treat sensing and\ncontrol separately, leading to suboptimal performance and resource usage. In\nthis work, we introduce the active inference framework (AIF) into SCC-enabled\nunmanned aerial vehicle (UAV) systems for joint state estimation, control, and\nsensing resource allocation. By formulating a unified generative model, the\nproblem reduces to minimizing variational free energy for inference and\nexpected free energy for action planning. Simulation results show that both\ncontrol cost and sensing cost are reduced relative to baselines.", "AI": {"tldr": "UAV\u7cfb\u7edf\u4f7f\u7528\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u8fdb\u884c\u4f20\u611f\u3001\u901a\u4fe1\u548c\u63a7\u5236\uff08SCC\uff09\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u72b6\u6001\u4f30\u8ba1\u3001\u63a7\u5236\u548c\u4f20\u611f\u8d44\u6e90\u5206\u914d\uff0c\u5b9e\u73b0\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709SCC\u89e3\u51b3\u65b9\u6848\u5c06\u4f20\u611f\u548c\u63a7\u5236\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u4e0d\u4f73\u3002", "method": "\u5c06\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u5f15\u5165SCC\u4e2d\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u751f\u6210\u6a21\u578b\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u5c0f\u5316\u53d8\u5206\u81ea\u7531\u80fd\uff08\u7528\u4e8e\u63a8\u7406\uff09\u548c\u9884\u671f\u81ea\u7531\u80fd\uff08\u7528\u4e8e\u884c\u52a8\u89c4\u5212\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63a7\u5236\u6210\u672c\u548c\u4f20\u611f\u6210\u672c\u5747\u6709\u6240\u964d\u4f4e\u3002", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u65e0\u4eba\u673a\uff08UAV\uff09\u7cfb\u7edf\u4e2d\u4f20\u611f\u3001\u901a\u4fe1\u548c\u63a7\u5236\uff08SCC\uff09\u7684\u8054\u5408\u72b6\u6001\u4f30\u8ba1\u3001\u63a7\u5236\u548c\u4f20\u611f\u8d44\u6e90\u5206\u914d\uff0c\u964d\u4f4e\u63a7\u5236\u548c\u4f20\u611f\u6210\u672c\u3002"}}
{"id": "2509.13985", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13985", "abs": "https://arxiv.org/abs/2509.13985", "authors": ["Yixun Wen", "Yulong Gao", "Boli Chen"], "title": "Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game", "comment": null, "summary": "Generalized Nash equilibrium problem (GNEP) is fundamental for practical\napplications where multiple self-interested agents work together to make\noptimal decisions. In this work, we study GNEP with shared distributionally\nrobust chance constraints (DRCCs) for incorporating inevitable uncertainties.\nThe DRCCs are defined over the Wasserstein ball, which can be explicitly\ncharacterized even with limited sample data. To determine the equilibrium of\nthe GNEP, we propose an exact approach to transform the original\ncomputationally intractable problem into a deterministic formulation using the\nNikaido-Isoda function. Specifically, we show that when all agents' objectives\nare quadratic in their respective variables, the equilibrium can be obtained by\nsolving a typical mixed-integer nonlinear programming (MINLP) problem, where\nthe integer and continuous variables are decoupled in both the objective\nfunction and the constraints. This structure significantly improves\ncomputational tractability, as demonstrated through a case study on the\ncharging station pricing problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u51b3\u5171\u4eab\u5206\u5e03\u9c81\u68d2\u673a\u4f1a\u7ea6\u675f\uff08DRCCs\uff09\u7684\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff08GNEP\uff09\u7684\u7cbe\u786e\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u5e76\u5229\u7528Nikaido-Isoda\u51fd\u6570\uff0c\u5728\u76ee\u6807\u51fd\u6570\u4e3a\u4e8c\u6b21\u7684\u60c5\u51b5\u4e0b\uff0c\u95ee\u9898\u53ef\u4ee5\u7b80\u5316\u4e3a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4f18\u5316\u51b3\u7b56\u4e2d\u7684\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff08GNEP\uff09\uff0c\u5e76\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff0c\u5f15\u5165\u4e86\u5171\u4eab\u5206\u5e03\u9c81\u68d2\u673a\u4f1a\u7ea6\u675f\uff08DRCCs\uff09\u3002", "method": "\u5c06GNEP\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u5229\u7528Nikaido-Isoda\u51fd\u6570\u3002\u5f53\u6240\u6709\u4e3b\u4f53\u7684\u76ee\u6807\u51fd\u6570\u662f\u5173\u4e8e\u5176\u5404\u81ea\u53d8\u91cf\u7684\u4e8c\u6b21\u51fd\u6570\u65f6\uff0c\u53ef\u4ee5\u5c06\u5747\u8861\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\uff0c\u5176\u4e2d\u6574\u6570\u548c\u8fde\u7eed\u53d8\u91cf\u5728\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u4e2d\u662f\u89e3\u8026\u7684\u3002", "result": "\u5728\u4e3b\u4f53\u76ee\u6807\u51fd\u6570\u4e3a\u4e8c\u6b21\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u4e00\u4e2a\u5145\u7535\u7ad9\u5b9a\u4ef7\u95ee\u9898\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06GNEP\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u6a21\u578b\u548c\u5229\u7528MINLP\u95ee\u9898\u7684\u7ed3\u6784\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5177\u6709\u5171\u4eab\u5206\u5e03\u9c81\u68d2\u673a\u4f1a\u7ea6\u675f\u7684GNEP\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\u3002"}}
{"id": "2509.13615", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e8c\u5143\u5207\u6362\u6307\u4ee4\u7684\u72b6\u6001\u63a7\u5236\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u4ee3\u7406\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u63a7\u5236\u4e2d\u6267\u884c\u5207\u6362\u6307\u4ee4\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\u3002\u4ed6\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a StaR\uff08State-aware Reasoning\uff09\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u3001\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u76ee\u6807\u72b6\u6001\u5e76\u636e\u6b64\u91c7\u53d6\u884c\u52a8\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5207\u6362\u6307\u4ee4\u7684\u6267\u884c\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u901a\u7528\u4efb\u52a1\u6027\u80fd\u4ea7\u751f\u4e86\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u591a\u6a21\u6001\u4ee3\u7406\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u63a7\u5236\u4e2d\u867d\u7136\u4fc3\u8fdb\u4e86\u6709\u6548\u4ea4\u4e92\uff0c\u4f46\u5728\u53ef\u9760\u6267\u884c\u5207\u6362\u63a7\u5236\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a StaR\uff08State-aware Reasoning\uff09\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u6559\u4f1a\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u3001\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u76ee\u6807\u72b6\u6001\u5e76\u636e\u6b64\u91c7\u53d6\u884c\u52a8\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u7684\u5b9e\u9a8c\u4e2d\uff0cStaR \u5c06\u5207\u6362\u6307\u4ee4\u7684\u6267\u884c\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 30% \u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u7684\u8fdb\u4e00\u6b65\u8bc4\u4f30\u8868\u660e\uff0cStaR \u80fd\u591f\u63d0\u5347\u6574\u4f53\u4efb\u52a1\u6027\u80fd\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u8bc4\u4f30\u4e5f\u51f8\u663e\u4e86 StaR \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "StaR \u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001\u4ee3\u7406\u5728 GUI \u63a7\u5236\u4e2d\u6267\u884c\u5207\u6362\u6307\u4ee4\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u901a\u7528\u4efb\u52a1\u6027\u80fd\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u6846\u67b6FunKAN\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5904\u7406\uff0c\u5b83\u5c06Kolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u63a8\u5e7f\u5230\u51fd\u6570\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5085\u91cc\u53f6\u5206\u89e3\u5b66\u4e60\u5185\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u5dee\u548cKANs\u7834\u574f\u7a7a\u95f4\u7ed3\u6784\u7684\u95ee\u9898\u3002FunKAN\u5728Gibbs-ringing\u6291\u5236\u548c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff08\u5305\u62ec\u4e73\u817a\u764c\u3001\u817a\u4f53\u548c\u606f\u8089\u68c0\u6d4b\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u5728IXI\u3001BUSI\u3001GlaS\u548cCVC-ClinicDB\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6KANs\u9aa8\u5e72\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\u4e2d\u7684\u4f2a\u5f71\u548c\u590d\u6742\u89e3\u5256\u53d8\u5f02\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u5dee\u548cKolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u7834\u574f\u56fe\u50cf\u7a7a\u95f4\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFunctional Kolmogorov-Arnold Network (FunKAN) \u7684\u65b0\u9896\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u5b83\u5c06Kolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u63a8\u5e7f\u5230\u51fd\u6570\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eHermite\u51fd\u6570\u7684\u5085\u91cc\u53f6\u5206\u89e3\u6765\u5b66\u4e60\u5185\u51fd\u6570\u3002\u5bf9\u4e8e\u5206\u5272\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86U-FunKAN\u6a21\u578b\u3002", "result": "\u5728IXI\u6570\u636e\u96c6\u4e0a\uff0cFunKAN\u5728Gibbs ringing\u6291\u5236\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002\u5728BUSI\u3001GlaS\u548cCVC-ClinicDB\u6570\u636e\u96c6\u4e0a\uff0cU-FunKAN\u5728\u68c0\u6d4b\u4e73\u817a\u764c\u3001\u817a\u4f53\u548c\u606f\u8089\u65b9\u9762\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u5206\u5272\u6027\u80fd\u3002\u4e0e\u5176\u4ed6\u7684KANs\u9aa8\u5e72\u7f51\u7edc\u76f8\u6bd4\uff0cFunKAN\u5728\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\uff08PSNR, TV\uff09\u548c\u5206\u5272\uff08IoU, F1\uff09\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "FunKAN\u6210\u529f\u5730\u5c06\u7406\u8bba\u51fd\u6570\u903c\u8fd1\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u800c\u6211\u4eec\u7684\u51e0\u4f55\u6846\u67b6\u53ef\u4ee5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u80fd\u591f\u540c\u65f6\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u9ed1\u76d2\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u5206\u6790\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u54cd\u5e94\u5d4c\u5165\u7684\u9ed1\u76d2\u6a21\u578b\u8bbf\u95ee\u8fdb\u884c\u62bd\u6837\u3002\u5728\u5168\u5c40\u5c42\u9762\uff0c\u63d0\u51fa\u51e0\u4f55\u4f53\u79ef\uff08Geometric Volume\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u539f\u578b\u51f8\u5305\u7684\u4f53\u79ef\u3002\u5728\u5c40\u90e8\u5c42\u9762\uff0c\u63d0\u51fa\u51e0\u4f55\u6000\u7591\u5ea6\uff08Geometric Suspicion\uff09\uff0c\u5bf9\u54cd\u5e94\u6309\u53ef\u9760\u6027\u6392\u5e8f\uff0c\u5e76\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u54cd\u5e94\u6765\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u8be5\u6846\u67b6\u5728\u77ed\u683c\u5f0f\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u8bc1\u660e\u4e86\u51f8\u5305\u4f53\u79ef\u4e0e\u71b5\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13708", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.13708", "abs": "https://arxiv.org/abs/2509.13708", "authors": ["Zhong-Sheng Chen", "Wei-Xin Chen", "Fan Wu", "Zhong-Wei Xu", "Jing Ma", "Yun-Kun Jiang", "Huai-Zhi Wu", "Shi-Biao Zheng"], "title": "Observation of topological Phenomena in a Weyl Exceptional Ring with Single Photons", "comment": "11 pages, 6 figures", "summary": "Compared with Hermitian theory, non-Hermitian physics offers a fundamentally\ndifferent mathematical framework, enabling the observation of topological\nphenomena that have no analogue in Hermitian systems. Among these, the\nexceptional point (EP) ring stands out as a quintessential topological feature\nunique to non-Hermitian systems. In this study, we employ single-photon\ninterferometry to overcome the experimental challenge of precise phase control\nin quantum systems, thereby enabling a complete simulation of the non-Hermitian\nEP ring in three-dimensional parameter space without invoking any additional\nsymmetry assumptions. By measuring the non-Hermitian dynamics in\nthree-dimensional parameter space, we determine the system's eigenstates, which\nallows us to characterize the topological band structure of the system under\ndifferent conditions. We describe the topological properties of the EP ring by\nextracting the Chern number and Berry phase for different parameter manifolds\nand observe the topological critical phenomena of the system. Our work paves\nthe way for further exploration of topological non-Hermitian systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5355\u5149\u5b50\u5e72\u6d89\u6280\u672f\uff0c\u5728\u4e09\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u975e\u5384\u7c73\u5947\u5f02\u70b9\u73af\u7684\u5b8c\u6574\u6a21\u62df\uff0c\u5e76\u8868\u5f81\u4e86\u5176\u62d3\u6251\u5e26\u7ed3\u6784\u548c\u62d3\u6251\u4e34\u754c\u73b0\u8c61\u3002", "motivation": "\u975e\u5384\u7c73\u7269\u7406\u63d0\u4f9b\u4e86\u533a\u522b\u4e8e\u5384\u7c73\u7406\u8bba\u7684\u6570\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u5384\u7c73\u7cfb\u7edf\u4e2d\u4e0d\u5b58\u5728\u7684\u62d3\u6251\u73b0\u8c61\uff0c\u5176\u4e2d\u5947\u5f02\u70b9\uff08EP\uff09\u73af\u662f\u975e\u5384\u7c73\u7cfb\u7edf\u7279\u6709\u7684\u62d3\u6251\u7279\u5f81\u3002", "method": "\u5229\u7528\u5355\u5149\u5b50\u5e72\u6d89\u6280\u672f\uff0c\u514b\u670d\u4e86\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7cbe\u786e\u76f8\u4f4d\u63a7\u5236\u7684\u5b9e\u9a8c\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u975e\u5384\u7c73\u5947\u5f02\u70b9\u73af\u5728\u4e09\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5b8c\u6574\u6a21\u62df\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u7684\u5bf9\u79f0\u6027\u5047\u8bbe\u3002\u901a\u8fc7\u6d4b\u91cf\u4e09\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u975e\u5384\u7c73\u52a8\u529b\u5b66\u6765\u786e\u5b9a\u7cfb\u7edf\u7684\u672c\u5f81\u6001\uff0c\u8fdb\u800c\u523b\u753b\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u62d3\u6251\u5e26\u7ed3\u6784\u3002\u901a\u8fc7\u63d0\u53d6\u4e0d\u540c\u53c2\u6570\u6d41\u5f62\u4e0b\u7684\u9648\u6570\uff08Chern number\uff09\u548c\u8d1d\u91cc\u76f8\u4f4d\uff08Berry phase\uff09\u6765\u63cf\u8ff0\u5947\u5f02\u70b9\u73af\u7684\u62d3\u6251\u6027\u8d28\uff0c\u5e76\u89c2\u5bdf\u7cfb\u7edf\u7684\u62d3\u6251\u4e34\u754c\u73b0\u8c61\u3002", "result": "\u6210\u529f\u5728\u4e09\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u6a21\u62df\u4e86\u975e\u5384\u7c73\u5947\u5f02\u70b9\u73af\uff0c\u5e76\u6d4b\u91cf\u4e86\u7cfb\u7edf\u7684\u672c\u5f81\u6001\uff0c\u4ece\u800c\u523b\u753b\u4e86\u62d3\u6251\u5e26\u7ed3\u6784\u3002\u63d0\u53d6\u4e86\u4e0d\u540c\u53c2\u6570\u6d41\u5f62\u4e0b\u7684\u9648\u6570\u548c\u8d1d\u91cc\u76f8\u4f4d\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u62d3\u6251\u4e34\u754c\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u62d3\u6251\u975e\u5384\u7c73\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13916", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13916", "abs": "https://arxiv.org/abs/2509.13916", "authors": ["Jonas A. Finkler", "Yan Lin", "Tao Du", "Jilin Hu", "Morten M. Smedskjaer"], "title": "Inverse Design of Amorphous Materials with Targeted Properties", "comment": null, "summary": "Disordered (amorphous) materials, such as glasses, are emerging as promising\ncandidates for applications within energy storage, nonlinear optics, and\ncatalysis. Their lack of long-range order and complex short- and medium-range\norderings, which depend on composition as well as thermal and pressure history,\noffer a vast materials design space. To this end, relying on machine learning\nmethods instead of trial and error is promising, and among these, inverse\ndesign has emerged as a tool for discovering novel materials with desired\nproperties. Although inverse design methods based on diffusion models have\nshown success for crystalline materials and molecules, similar methods\ntargeting amorphous materials remain less developed, mainly because of the\nlimited availability of large-scale datasets and the requirement for larger\nsimulation cells. In this work, we propose and validate an inverse design\nmethod for amorphous materials, introducing AMDEN (Amorphous Material DEnoising\nNetwork), a diffusion model-based framework that generates structures of\namorphous materials. These low-energy configurations are typically obtained\nthrough a thermal motion-driven random search-like process that cannot be\nreplicated by standard denoising procedures. We therefore introduce an\nenergy-based AMDEN variant that implements Hamiltonian Monte Carlo refinement\nfor generating these relaxed structures. We further introduce several amorphous\nmaterial datasets with diverse properties and compositions to evaluate our\nframework and support future development.", "AI": {"tldr": "AMDNE\u662f\u4e00\u4e2a\u7528\u4e8e\u65e0\u5b9a\u5f62\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6c49\u5bc6\u5c14\u987f\u8499\u7279\u5361\u6d1b\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u65e0\u5b9a\u5f62\u6750\u6599\u5728\u50a8\u80fd\u3001\u975e\u7ebf\u6027\u5149\u5b66\u548c\u50ac\u5316\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5176\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u4e14\u96be\u4ee5\u63a2\u7d22\uff0c\u4f20\u7edf\u7684\u8bd5\u9519\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662f\u9006\u5411\u8bbe\u8ba1\uff0c\u4e3a\u53d1\u73b0\u5177\u6709\u6240\u9700\u7279\u6027\u7684\u65b0\u578b\u65e0\u5b9a\u5f62\u6750\u6599\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u540d\u4e3aAMDNE\uff08\u65e0\u5b9a\u5f62\u6750\u6599\u53bb\u566a\u7f51\u7edc\uff09\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u751f\u6210\u65e0\u5b9a\u5f62\u6750\u6599\u7684\u7ed3\u6784\u3002\u4e3a\u4e86\u514b\u670d\u6807\u51c6\u53bb\u566a\u7a0b\u5e8f\u65e0\u6cd5\u6a21\u62df\u65e0\u5b9a\u5f62\u6750\u6599\u80fd\u91cf\u6700\u4f4e\u6784\u8c61\u751f\u6210\u7684\u6311\u6218\uff0cAMDNE\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u80fd\u91cf\u7684\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u5b9e\u73b0\u4e86\u6c49\u5bc6\u5c14\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "AMDNE\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u5177\u6709\u591a\u79cd\u7279\u6027\u7684\u65e0\u5b9a\u5f62\u6750\u6599\u7ed3\u6784\u3002\u4e3a\u4e86\u8bc4\u4f30\u8be5\u6846\u67b6\u5e76\u652f\u6301\u672a\u6765\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u6784\u5efa\u4e86\u51e0\u4e2a\u5177\u6709\u591a\u6837\u5316\u7279\u6027\u548c\u7ec4\u6210\u7684\u65e0\u5b9a\u5f62\u6750\u6599\u6570\u636e\u96c6\u3002", "conclusion": "AMDNE\u4e3a\u65e0\u5b9a\u5f62\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u6c49\u5bc6\u5c14\u987f\u8499\u7279\u5361\u6d1b\u4f18\u5316\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u65e0\u5b9a\u5f62\u6750\u6599\u7684\u8bbe\u8ba1\u548c\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.13666", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13666", "abs": "https://arxiv.org/abs/2509.13666", "authors": ["Zhenqi Wu", "Abhinav Modi", "Angelos Mavrogiannis", "Kaustubh Joshi", "Nikhil Chopra", "Yiannis Aloimonos", "Nare Karapetyan", "Ioannis Rekleitis", "Xiaomin Lin"], "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring", "comment": "submitted to ICRA 2026", "summary": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.", "AI": {"tldr": "DREAM\u662f\u4e00\u4e2a\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u9a71\u52a8\u7684\u81ea\u4e3b\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u671f\u7684\u6c34\u4e0b\u63a2\u7d22\u548c\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5bfb\u627e\u548c\u63a2\u7d22\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u548c\u9999\u8349VLM\u3002", "motivation": "\u7531\u4e8e\u6d77\u6d0b\u53d8\u6696\u548c\u9178\u5316\u5bf9\u8d1d\u7c7b\uff08\u5982\u7261\u86ce\uff09\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u957f\u671f\u7684\u6c34\u4e0b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f46\u4eba\u529b\u6210\u672c\u9ad8\u4e14\u6c34\u4e0b\u4f5c\u4e1a\u5371\u9669\uff0c\u56e0\u6b64\u9700\u8981\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDREAM\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9a71\u52a8\u7684\u81ea\u4e3b\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u671f\u7684\u6c34\u4e0b\u63a2\u7d22\u548c\u6816\u606f\u5730\u76d1\u6d4b\u3002", "result": "\u5728\u7261\u86ce\u76d1\u6d4b\u4efb\u52a1\u4e2d\uff0cDREAM\u6bd4\u73b0\u6709\u57fa\u7ebf\u5feb31.5%\uff0c\u7261\u86ce\u6570\u91cf\u76f8\u540c\u3002\u4e0e\u9999\u8349VLM\u76f8\u6bd4\uff0cDREAM\u5c11\u7528\u4e8623%\u7684\u6b65\u9aa4\uff0c\u8986\u76d6\u4e868.88%\u7684\u7261\u86ce\u3002\u5728\u5bfb\u627e\u6c89\u8239\u7684\u4efb\u52a1\u4e2d\uff0cDREAM\u6bd4\u9999\u8349\u6a21\u578b\u5c11\u7528\u4e8627.5%\u7684\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86100%\u7684\u8986\u76d6\u7387\uff0c\u800c\u9999\u8349\u6a21\u578b\u5e73\u5747\u8986\u76d6\u7387\u4e3a60.23%\u3002", "conclusion": "DREAM\u6846\u67b6\u5728\u65e0\u9700\u5148\u9a8c\u4f4d\u7f6e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5bfb\u627e\u548c\u63a2\u7d22\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u5728\u6c34\u4e0b\u76d1\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13735", "abs": "https://arxiv.org/abs/2509.13735", "authors": ["Junzhi She", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "State Space Models over Directed Graphs", "comment": "currently undergoing review by IEEE Transactions on Big Data", "summary": "Directed graphs are ubiquitous across numerous domains, where the\ndirectionality of edges encodes critical causal dependencies. However, existing\nGNNs and graph Transformers tailored for directed graphs face two major\nchallenges: (1) effectively capturing long-range causal dependencies derived\nfrom directed edges; (2) balancing accuracy and training efficiency when\nprocessing large-scale graph datasets. In recent years, state space models\n(SSMs) have achieved substantial progress in causal sequence tasks, and their\nvariants designed for graphs have demonstrated state-of-the-art accuracy while\nmaintaining high efficiency across various graph learning benchmarks. However,\nexisting graph state space models are exclusively designed for undirected\ngraphs, which limits their performance in directed graph learning. To this end,\nwe propose an innovative approach DirEgo2Token which sequentializes directed\ngraphs via k-hop ego graphs. This marks the first systematic extension of state\nspace models to the field of directed graph learning. Building upon this, we\ndevelop DirGraphSSM, a novel directed graph neural network architecture that\nimplements state space models on directed graphs via the message-passing\nmechanism. Experimental results demonstrate that DirGraphSSM achieves\nstate-of-the-art performance on three representative directed graph learning\ntasks while attaining competitive performance on two additional tasks with\n1.5$\\times $ to 2$\\times $ training speed improvements compared to existing\nstate-of-the-art models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faDirGraphSSM\uff0c\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u6709\u5411\u56fe\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u6709\u5411\u56fe\u548c\u6d88\u606f\u4f20\u9012\u673a\u5236\u6765\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u6709\u5411\u56fe\u4e0a\u7684\u5e94\u7528\uff0c\u4ece\u800c\u6709\u6548\u6355\u6349\u957f\u8ddd\u79bb\u56e0\u679c\u4f9d\u8d56\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u548c\u56feTransformer\u5728\u5904\u7406\u6709\u5411\u56fe\u65f6\uff0c\u5728\u6355\u6349\u957f\u8ddd\u79bb\u56e0\u679c\u4f9d\u8d56\u548c\u5e73\u8861\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\uff1b\u73b0\u6709\u7684\u56fe\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u4ec5\u9650\u4e8e\u65e0\u5411\u56fe\u3002", "method": "\u63d0\u51faDirEgo2Token\u65b9\u6cd5\uff0c\u901a\u8fc7k-hop\u81ea\u56fe\uff08ego graphs\uff09\u5c06\u6709\u5411\u56fe\u5e8f\u5217\u5316\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u4e86DirGraphSSM\u67b6\u6784\uff0c\u5c06SSMs\u5e94\u7528\u4e8e\u6709\u5411\u56fe\uff0c\u5e76\u5229\u7528\u6d88\u606f\u4f20\u9012\u673a\u5236\u3002", "result": "DirGraphSSM\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u7684\u6709\u5411\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u53e6\u5916\u4e24\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709\u6a21\u578b\u63d0\u9ad8\u4e861.5\u52302\u500d\u3002", "conclusion": "DirGraphSSM\u662f\u9996\u4e2a\u5c06\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u5230\u6709\u5411\u56fe\u5b66\u4e60\u9886\u57df\u7684\u65b9\u6cd5\uff0c\u5728\u6709\u5411\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.14217", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14217", "abs": "https://arxiv.org/abs/2509.14217", "authors": ["Andriy Enttsel", "Weichen Wang", "Mauro Mangia", "Riccardo Rovatti", "Deniz G\u00fcnd\u00fcz"], "title": "Goal-Oriented Joint Source-Channel Coding: Distortion-Classification-Power Trade-off", "comment": "13 pages, 3 figures", "summary": "Joint source-channel coding is a compelling paradigm when low-latency and\nlow-complexity communication is required. This work proposes a theoretical\nframework that integrates classification and anomaly detection within the\nconventional signal reconstruction objective. Assuming a Gaussian scalar source\nand constraining the encoder to piecewise linear mappings, we derive tractable\ndesign rules and explicitly characterize the trade-offs between distortion,\nclassification error, and transmission power.", "AI": {"tldr": "\u8054\u5408\u4fe1\u6e90-\u4fe1\u9053\u7f16\u7801\u5728\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u590d\u6742\u5ea6\u901a\u4fe1\u4e2d\u5177\u6709\u5438\u5f15\u529b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u6574\u5408\u5230\u5e38\u89c4\u4fe1\u53f7\u91cd\u5efa\u76ee\u6807\u4e2d\u7684\u7406\u8bba\u6846\u67b6\u3002\u5728\u5047\u8bbe\u9ad8\u65af\u6807\u91cf\u4fe1\u6e90\u548c\u7ea6\u675f\u7f16\u7801\u5668\u4e3a\u5206\u6bb5\u7ebf\u6027\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u6613\u4e8e\u5904\u7406\u7684\u8bbe\u8ba1\u89c4\u5219\uff0c\u5e76\u660e\u786e\u8868\u5f81\u4e86\u5931\u771f\u3001\u5206\u7c7b\u8bef\u5dee\u548c\u4f20\u8f93\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u8054\u5408\u4fe1\u6e90-\u4fe1\u9053\u7f16\u7801\u5728\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u590d\u6742\u5ea6\u901a\u4fe1\u4e2d\u5177\u6709\u5438\u5f15\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u6574\u5408\u5230\u5e38\u89c4\u4fe1\u53f7\u91cd\u5efa\u76ee\u6807\u4e2d\u7684\u7406\u8bba\u6846\u67b6\u3002\u5728\u5047\u8bbe\u9ad8\u65af\u6807\u91cf\u4fe1\u6e90\u548c\u7ea6\u675f\u7f16\u7801\u5668\u4e3a\u5206\u6bb5\u7ebf\u6027\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u6613\u4e8e\u5904\u7406\u7684\u8bbe\u8ba1\u89c4\u5219\u3002", "result": "\u660e\u786e\u8868\u5f81\u4e86\u5931\u771f\u3001\u5206\u7c7b\u8bef\u5dee\u548c\u4f20\u8f93\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u6574\u5408\u5230\u5e38\u89c4\u4fe1\u53f7\u91cd\u5efa\u76ee\u6807\u4e2d\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u660e\u786e\u8868\u5f81\u4e86\u5931\u771f\u3001\u5206\u7c7b\u8bef\u5dee\u548c\u4f20\u8f93\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.13994", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13994", "abs": "https://arxiv.org/abs/2509.13994", "authors": ["Giacomo Bastianel", "Dirk Van Hertem", "Hakan Ergun", "Line Roald"], "title": "Day-Ahead Transmission Grid Topology Optimization Considering Renewable Energy Sources' Uncertainty", "comment": null, "summary": "The increasing renewable penetration introduces significant uncertainty in\npower system operations. At the same time, the existing transmission grid is\noften already congested, and urgently needed reinforcements are frequently\ndelayed due to several constraints. To address these challenges, adjusting the\ngrid topology based on congestion patterns is considered a non-costly remedy to\nguarantee efficient power transmission. Based on this idea, this paper proposes\na grid topology optimization model combining optimal transmission switching and\nbusbar splitting for AC and hybrid AC/DC grids. The methodology incorporates\nRES forecast uncertainty through a scenario-based stochastic optimization\napproach, using real offshore wind data and K-means clustering to generate\nrepresentative forecast error scenarios. The proposed model includes several\nformulations to be compared with a plain optimal power flow (OPF) model: hourly\noptimizing the topology, one topology for 24 hours, or a limited number of\nswitching actions over a day. The grid topology optimization model is\nformulated as a Mixed-Integer Quadratic Convex Problem, optimized based on the\nday-ahead (D-1) RES forecast and validated for AC-feasibility via an AC-OPF\nformulation. Based on the generation setpoints of the feasibility check, a\nredispatch simulation based on the measured (D) RES realization is then\ncomputed. The methodology is tested on an AC 30-bus test case and a hybrid\nAC/DC 50-bus test case, for a 24-hours (30-bus) and a 14-days (both test cases)\ntime series. The results highlight the economic benefits brought by grid\ntopology optimization for congested test cases with high penetration of RES. In\naddition, the results demonstrate that accounting for RES uncertainty with at\nleast 6 to 8 scenarios leads to lower or comparable total costs to\ndeterministic day-ahead forecasts, even when limiting the frequency of\ntopological actions.", "AI": {"tldr": "\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u7ed9\u7535\u7f51\u8fd0\u8425\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u73b0\u6709\u8f93\u7535\u7f51\u7edc\u62e5\u5835\u4e14\u6269\u5efa\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u4f18\u8f93\u7535\u6f6e\u6d41\u5f00\u5173\u548c\u6bcd\u7ebf\u5206\u6bb5\u7684\u7535\u7f51\u62d3\u6251\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u6df7\u5408\u4ea4\u6d41/\u76f4\u6d41\u7535\u7f51\u3002\u8be5\u6a21\u578b\u8003\u8651\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u7684\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u5904\u7406\uff0c\u4f7f\u7528\u4e86\u771f\u5b9e\u7684\u79bb\u5cb8\u98ce\u7535\u6570\u636e\u548cK-means\u805a\u7c7b\u751f\u6210\u4ee3\u8868\u6027\u7684\u9884\u6d4b\u8bef\u5dee\u573a\u666f\u3002\u6a21\u578b\u63d0\u4f9b\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6848\uff0c\u4f8b\u5982\u6bcf\u5c0f\u65f6\u4f18\u5316\u62d3\u6251\u300124\u5c0f\u65f6\u56fa\u5b9a\u62d3\u6251\u6216\u6bcf\u65e5\u6709\u9650\u6b21\u62d3\u6251\u8c03\u6574\u3002\u6a21\u578b\u88ab\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u51f8\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u65e5\u524d\uff08D-1\uff09\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\uff08AC-OPF\uff09\u8fdb\u884c\u4ea4\u6d41\u53ef\u884c\u6027\u9a8c\u8bc1\u3002\u968f\u540e\uff0c\u57fa\u4e8e\u53ef\u884c\u6027\u68c0\u67e5\u7684\u53d1\u7535\u8bbe\u7f6e\u70b9\uff0c\u8fdb\u884c\u57fa\u4e8e\u5b9e\u9645\uff08D\uff09\u53ef\u518d\u751f\u80fd\u6e90\u60c5\u51b5\u7684\u518d\u8c03\u5ea6\u6a21\u62df\u3002\u8be5\u65b9\u6cd5\u572830\u8282\u70b9\u4ea4\u6d41\u6d4b\u8bd5\u6848\u4f8b\u548c50\u8282\u70b9\u6df7\u5408\u4ea4\u6d41/\u76f4\u6d41\u6d4b\u8bd5\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5206\u522b\u8fdb\u884c\u4e8624\u5c0f\u65f6\u548c14\u5929\u7684\u4eff\u771f\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u9ad8\u4e14\u7535\u7f51\u62e5\u5835\u7684\u60c5\u51b5\u4e0b\uff0c\u7535\u7f51\u62d3\u6251\u4f18\u5316\u80fd\u5e26\u6765\u7ecf\u6d4e\u6548\u76ca\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u9650\u5236\u62d3\u6251\u8c03\u6574\u6b21\u6570\uff0c\u4f7f\u7528\u81f3\u5c116\u52308\u4e2a\u573a\u666f\u6765\u8003\u8651\u53ef\u518d\u751f\u80fd\u6e90\u4e0d\u786e\u5b9a\u6027\uff0c\u4e5f\u80fd\u5b9e\u73b0\u4e0e\u786e\u5b9a\u6027\u65e5\u524d\u9884\u6d4b\u76f8\u5f53\u6216\u66f4\u4f4e\u7684\u603b\u4f53\u6210\u672c\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u7684\u63d0\u9ad8\uff0c\u7535\u7f51\u8fd0\u8425\u9762\u4e34\u7740\u65e5\u76ca\u589e\u957f\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684\u8f93\u7535\u7f51\u7edc\u5e38\u5e38\u5df2\u7ecf\u62e5\u5835\uff0c\u5e76\u4e14\u7531\u4e8e\u5404\u79cd\u9650\u5236\uff0c\u6025\u9700\u7684\u6269\u5efa\u5de5\u7a0b\u7ecf\u5e38\u88ab\u5ef6\u8fdf\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u57fa\u4e8e\u62e5\u5835\u6a21\u5f0f\u8c03\u6574\u7535\u7f51\u62d3\u6251\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u8bc1\u9ad8\u6548\u7684\u7535\u529b\u4f20\u8f93\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u4f18\u8f93\u7535\u6f6e\u6d41\u5f00\u5173\u548c\u6bcd\u7ebf\u5206\u6bb5\u7684\u7535\u7f51\u62d3\u6251\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u6df7\u5408\u4ea4\u6d41/\u76f4\u6d41\u7535\u7f51\u3002\u8be5\u6a21\u578b\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u6765\u5904\u7406\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u771f\u5b9e\u6570\u636e\u548c\u805a\u7c7b\u7b97\u6cd5\u751f\u6210\u9884\u6d4b\u8bef\u5dee\u573a\u666f\u3002\u6a21\u578b\u88ab\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u51f8\u95ee\u9898\uff0c\u5e76\u4e0e\u6700\u4f18\u6f6e\u6d41\uff08OPF\uff09\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u8003\u8651\u4e86\u4e0d\u540c\u7684\u62d3\u6251\u4f18\u5316\u7b56\u7565\uff08\u4f8b\u5982\uff0c\u6bcf\u5c0f\u65f6\u4f18\u5316\u300124\u5c0f\u65f6\u56fa\u5b9a\u62d3\u6251\u3001\u6709\u9650\u6b21\u8c03\u6574\uff09\u3002\u901a\u8fc7\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\uff08AC-OPF\uff09\u8fdb\u884c\u53ef\u884c\u6027\u9a8c\u8bc1\uff0c\u5e76\u8fdb\u884c\u57fa\u4e8e\u5b9e\u9645\u60c5\u51b5\u7684\u518d\u8c03\u5ea6\u6a21\u62df\u3002", "result": "\u572830\u8282\u70b9\u4ea4\u6d41\u6d4b\u8bd5\u6848\u4f8b\u548c50\u8282\u70b9\u6df7\u5408\u4ea4\u6d41/\u76f4\u6d41\u6d4b\u8bd5\u6848\u4f8b\u4e0a\u8fdb\u884c\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7535\u7f51\u62d3\u6251\u4f18\u5316\u5728\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u9ad8\u4e14\u7535\u7f51\u62e5\u5835\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5e26\u6765\u663e\u8457\u7684\u7ecf\u6d4e\u6548\u76ca\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5373\u4f7f\u9650\u5236\u62d3\u6251\u8c03\u6574\u6b21\u6570\uff0c\u4f7f\u75286\u52308\u4e2a\u573a\u666f\u6765\u8003\u8651\u53ef\u518d\u751f\u80fd\u6e90\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e5f\u80fd\u83b7\u5f97\u4e0e\u786e\u5b9a\u6027\u65e5\u524d\u9884\u6d4b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u603b\u6210\u672c\u3002", "conclusion": "\u7535\u7f51\u62d3\u6251\u4f18\u5316\uff0c\u7279\u522b\u662f\u7ed3\u5408\u6700\u4f18\u8f93\u7535\u6f6e\u6d41\u5f00\u5173\u548c\u6bcd\u7ebf\u5206\u6bb5\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u5e94\u5bf9\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u548c\u7535\u7f51\u62e5\u5835\u5e26\u6765\u7684\u6311\u6218\u3002\u8003\u8651\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u7535\u7f51\u8fd0\u8425\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u4f7f\u7528\u9002\u5f53\u6570\u91cf\u7684\u573a\u666f\u8fdb\u884c\u968f\u673a\u4f18\u5316\u53ef\u4ee5\u5e26\u6765\u6bd4\u786e\u5b9a\u6027\u65b9\u6cd5\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.13704", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684GUI\u4ee3\u7406\u5728\u5de5\u4e1a\u7ba1\u7406\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u63d0\u51faInfraMind\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u7ba1\u7406\u8f6f\u4ef6\u590d\u6742\u6027\u9ad8\uff0c\u591a\u4f9b\u5e94\u5546\u96c6\u6210\uff0c\u64cd\u4f5c\u5458\u77ed\u7f3a\uff0cRPA\u7075\u6d3b\u6027\u4e0d\u8db3\uff0cLLM\u4ee3\u7406\u5b58\u5728\u7406\u89e3\u3001\u7cbe\u786e\u5ea6\u3001\u72b6\u6001\u5b9a\u4f4d\u3001\u90e8\u7f72\u548c\u5b89\u5168\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faInfraMind\u6846\u67b6\uff0c\u5305\u542b\u7cfb\u7edf\u6027\u641c\u7d22\u63a2\u7d22\u3001\u5185\u5b58\u9a71\u52a8\u89c4\u5212\u3001\u9ad8\u7ea7\u72b6\u6001\u8bc6\u522b\u3001\u7ed3\u6784\u5316\u77e5\u8bc6\u84b8\u998f\u548c\u591a\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aDCIM\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cInfraMind\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "InfraMind\u4e3a\u5de5\u4e1a\u7ba1\u7406\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e25\u8c28\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\uff0c\u901a\u8fc7\u5b9e\u4f8b\u56fe\u548c\u6743\u91cd\u56fe\u6765\u7a81\u51fa\u4ec7\u6068\u5185\u5bb9\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4ec7\u6068\u89c6\u9891\u65f6\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u4e14\u65e0\u6cd5\u6709\u6548\u5730\u533a\u5206\u548c\u7a81\u51fa\u5185\u5bb9\u4e2d\u7684\u4ec7\u6068\u6210\u5206\uff0c\u540c\u65f6\u5728\u6355\u6349\u89c6\u9891\u7ed3\u6784\u5316\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u9996\u5148\u5c06\u89c6\u9891\u5206\u5272\u6210\u5b9e\u4f8b\u5e76\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u6784\u5efa\u5b9e\u4f8b\u56fe\uff1b\u63a5\u7740\uff0c\u5229\u7528\u4e92\u8865\u6743\u91cd\u56fe\u4e3a\u8fd9\u4e9b\u7279\u5f81\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\uff0c\u4ee5\u7a81\u51fa\u4ec7\u6068\u5b9e\u4f8b\uff1b\u6700\u540e\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7279\u5f81\u548c\u6743\u91cd\u751f\u6210\u89c6\u9891\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u56fe\u6a21\u578b\u6355\u6349\u8de8\u6a21\u6001\u7684\u7ed3\u6784\u5316\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u578b\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7a81\u51fa\u4ec7\u6068\u5185\u5bb9\u548c\u5efa\u6a21\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "2025\u5e74\u7684AutoMin\u4f1a\u8bae\u7eaa\u8981\u4efb\u52a1\u5305\u62ec\u82f1\u6587\u548c\u6377\u514b\u8bed\u7684\u9879\u76ee\u4f1a\u8bae\u548c\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\u7684\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u672c\u6b21AutoMin\u4efb\u52a1\u65e8\u5728\u63a8\u52a8\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "AutoMin\u4efb\u52a1\u5305\u542b\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a1. \u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\uff0c\u6db5\u76d6\u82f1/\u6377\u514b\u8bed\u548c\u9879\u76ee/\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\u30022. \u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\uff0c\u5305\u62ec\u82f1/\u6377\u514b\u8bed\u7684\u5355\u4e00\u8bed\u8a00\u95ee\u7b54\u548c\u57fa\u4e8e\u82f1/\u6377\u514b\u8bed\u4f1a\u8bae\u8bb0\u5f55\u7684\u8de8\u8bed\u8a00\u95ee\u7b54\u3002", "result": "\u5c3d\u7ba12025\u5e74\u53c2\u4e0e\u56e2\u961f\u6570\u91cf\u6709\u6240\u51cf\u5c11\uff08\u8bb0\u5f55\u4efb\u52a11\u4e2a\uff0c\u95ee\u7b54\u4efb\u52a12\u4e2a\uff09\uff0c\u4f46\u4e3b\u529e\u65b9\u5f15\u5165\u4e86\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "2025\u5e74\u7684AutoMin\u4efb\u52a1\u5c55\u793a\u4e86\u5728\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u548c\u95ee\u7b54\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2509.13810", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13810", "abs": "https://arxiv.org/abs/2509.13810", "authors": ["K. M. Kwan", "T. G. McRae", "J. Qin", "D. W. Gould", "S. S. Y. Chua", "J. Junker", "R. Iden", "V. B. Adya", "M. J. Yap", "B. J. J. Slagmolen", "D. E. McClelland", "R. L. Ward"], "title": "Loss-tolerant detection of squeezed states in the 2 um region", "comment": "5 pages, 5 figures, presented at the Sept 2025 LVK conference", "summary": "Squeezed states of light enable quantum-enhanced measurements but are limited\nby optical loss, particularly at 2 um where photodiode efficiency is low. We\nreport the first loss-tolerant, audio-band squeezed light detection at 1984 nm\nby using a phase-sensitive amplifier to amplify the squeezed vacuum prior to\ndetection. This technique increases the effective detection efficiency from 74%\nto 95% and increases the observed squeezing from 4 dB to 8 dB, the highest\nlevel of squeezing observed at this wavelength. Additionally, the\nshot-to-dark-noise clearance increases, extending the effective measurement\nbandwidth toward lower frequencies. This approach is largely\nwavelength-independent, extending high-fidelity quantum measurements to future\ngravitational-wave detectors and related quantum technologies.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u76f8\u4f4d\u654f\u611f\u653e\u5927\u5668\u5728\u63a2\u6d4b\u524d\u653e\u5927\u538b\u7f29\u771f\u7a7a\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86 1984 nm \u6ce2\u957f\u4e0b\u7684\u635f\u8017\u5bb9\u5fcd\u97f3\u9891\u9891\u5e26\u538b\u7f29\u5149\u63a2\u6d4b\uff0c\u4ece\u800c\u5c06\u6709\u6548\u63a2\u6d4b\u6548\u7387\u4ece 74% \u63d0\u9ad8\u5230 95%\uff0c\u5e76\u5c06\u89c2\u6d4b\u5230\u7684\u538b\u7f29\u4ece 4 dB \u63d0\u9ad8\u5230 8 dB\u3002", "motivation": "\u5149\u5b66\u635f\u8017\u9650\u5236\u4e86\u538b\u7f29\u6001\u5149\u5728 2 um \u6ce2\u957f\u4e0b\u7684\u91cf\u5b50\u589e\u5f3a\u6d4b\u91cf\uff0c\u56e0\u4e3a\u6b64\u65f6\u5149\u7535\u4e8c\u6781\u7ba1\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u5728\u63a2\u6d4b\u524d\u4f7f\u7528\u76f8\u4f4d\u654f\u611f\u653e\u5927\u5668\u653e\u5927\u538b\u7f29\u771f\u7a7a\u3002", "result": "\u6709\u6548\u63a2\u6d4b\u6548\u7387\u4ece 74% \u63d0\u9ad8\u5230 95%\uff0c\u89c2\u6d4b\u5230\u7684\u538b\u7f29\u4ece 4 dB \u63d0\u9ad8\u5230 8 dB\uff08\u8fd9\u662f\u8be5\u6ce2\u957f\u4e0b\u7684\u6700\u9ad8\u538b\u7f29\u6c34\u5e73\uff09\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u4fe1\u566a\u6bd4\uff0c\u5c06\u6709\u6548\u6d4b\u91cf\u5e26\u5bbd\u6269\u5c55\u5230\u66f4\u4f4e\u9891\u7387\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u6ce2\u957f\u65e0\u5173\uff0c\u80fd\u591f\u5c06\u9ad8\u4fdd\u771f\u91cf\u5b50\u6d4b\u91cf\u6269\u5c55\u5230\u672a\u6765\u7684\u5f15\u529b\u6ce2\u63a2\u6d4b\u5668\u548c\u76f8\u5173\u7684\u91cf\u5b50\u6280\u672f\u3002"}}
{"id": "2509.14105", "categories": ["cond-mat.mtrl-sci", "cond-mat.other", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2509.14105", "abs": "https://arxiv.org/abs/2509.14105", "authors": ["Shan Jiang", "Qiaochao Xiang", "Beno\u00eet Fauqu\u00e9", "Xiaokang Li", "Zengwei Zhu", "Kamran Behnia"], "title": "Comment on `High-resolution Measurements of Thermal Conductivity Matrix and Search for Thermal Hall Effect in La$_2$CuO$_4$'", "comment": "Comment on arXiv:2507.21403, 3 pages, 3 figures", "summary": "Recently, Jiayi Hu and co-workers reported that they did not resolve any\nthermal Hall signal in La$_2$CuO$_4$ by `high resolution' measurements, setting\nan upper bound of $|\\kappa_{xy}| <2\\times 10^{-3}~$Wm$^{-1}$K$^{-1}$ at 20 K.\nTwo points have apparently escaped their attention. First, thermal Hall signals\nwith an amplitude well below this resolution bound have been detected in\ndisordered perovskites. Second, the longitudinal thermal conductivity of their\nsample is significantly lower than the La$_2$CuO$_4$ sample displaying a\nthermal Hall signal. We find that a moderate reduction of $\\kappa_{xx}$ in\nSrTiO$_3$ is concomitant with a drastic attenuation of $\\kappa_{xy}$. A trend\nemerges across several families of insulators: the amplitude of $\\kappa_{xy}$\nanti-correlates with disorder.", "AI": {"tldr": "\u6587\u7ae0\u6307\u51fa\uff0c\u5c3d\u7ba1Jiayi Hu\u7b49\u4eba\u672a\u80fd\u5728La$_2$CuO$_4$\u4e2d\u68c0\u6d4b\u5230\u70ed\u970d\u5c14\u4fe1\u53f7\uff0c\u4f46\u4f4e\u4e8e\u5176\u5206\u8fa8\u7387\u9608\u503c\u4f46\u6709\u610f\u4e49\u7684\u4fe1\u53f7\u5df2\u5728\u65e0\u5e8f\u9499\u949b\u77ff\u4e2d\u88ab\u89c2\u6d4b\u5230\u3002\u6587\u7ae0\u8fd8\u53d1\u73b0\uff0c\u7eb5\u5411\u70ed\u5bfc\u7387\u7684\u964d\u4f4e\u4e0e\u70ed\u970d\u5c14\u4fe1\u53f7\u7684\u6025\u5267\u8870\u51cf\u76f8\u5173\uff0c\u5e76\u4e14\u5728\u51e0\u7c7b\u7edd\u7f18\u4f53\u4e2d\uff0c\u70ed\u970d\u5c14\u4fe1\u53f7\u7684\u5e45\u5ea6\u4e0e\u65e0\u5e8f\u5ea6\u5448\u8d1f\u76f8\u5173\u3002", "motivation": "\u63a2\u7a76\u70ed\u970d\u5c14\u4fe1\u53f7\u5728\u65e0\u5e8f\u7edd\u7f18\u4f53\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u89e3\u91ca\u4e3a\u4f55\u5728\u67d0\u4e9b\u6750\u6599\uff08\u5982La$_2$CuO4\uff09\u4e2d\u96be\u4ee5\u68c0\u6d4b\u5230\u8be5\u4fe1\u53f7\uff0c\u800c\u5176\u5e45\u5ea6\u4f3c\u4e4e\u4e0e\u6750\u6599\u7684\u65e0\u5e8f\u5ea6\u76f8\u5173\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5177\u6709\u4e0d\u540c\u7eb5\u5411\u70ed\u5bfc\u7387\uff08$\\kappa_{xx}$\uff09\u7684\u6750\u6599\uff08\u5305\u62ecSrTiO$_3$\uff09\u4e2d\u7684\u70ed\u970d\u5c14\u4fe1\u53f7\uff08$\\kappa_{xy}$\uff09\u5e45\u5ea6\uff0c\u5206\u6790\u6837\u54c1\u8d28\u91cf\uff08\u65e0\u5e8f\u5ea6\uff09\u5bf9\u70ed\u970d\u5c14\u4fe1\u53f7\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5728SrTiO$_3$\u4e2d\uff0c$\\kappa_{xx}$\u7684\u9002\u5ea6\u964d\u4f4e\u4f34\u968f\u7740$\\kappa_{xy}$\u7684\u6025\u5267\u8870\u51cf\u3002\u5728\u591a\u79cd\u7edd\u7f18\u4f53\u4e2d\u89c2\u5bdf\u5230\uff0c$\\kappa_{xy}$\u7684\u5e45\u5ea6\u4e0e\u65e0\u5e8f\u5ea6\u5448\u8d1f\u76f8\u5173\u3002", "conclusion": "\u6750\u6599\u7684\u65e0\u5e8f\u5ea6\u662f\u5f71\u54cd\u70ed\u970d\u5c14\u4fe1\u53f7\u5e45\u5ea6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9ad8\u5ea6\u65e0\u5e8f\u7684\u6750\u6599\u53ef\u80fd\u8868\u73b0\u51fa\u8f83\u5f31\u6216\u65e0\u6cd5\u68c0\u6d4b\u5230\u7684\u70ed\u970d\u5c14\u4fe1\u53f7\u3002"}}
{"id": "2509.13691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13691", "abs": "https://arxiv.org/abs/2509.13691", "authors": ["Songhao Huang", "Yuwei Wu", "Guangyao Shi", "Gaurav S. Sukhatme", "Vijay Kumar"], "title": "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics", "comment": null, "summary": "We investigate the problem of automatic domain generation for the Planning\nDomain Definition Language (PDDL) using Large Language Models (LLMs), with a\nparticular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a\nwidely adopted standard in robotic planning, manually designing domains for\ndiverse applications such as surveillance, delivery, and inspection is\nlabor-intensive and error-prone, which hinders adoption and real-world\ndeployment. To address these challenges, we propose SPAR, a framework that\nleverages the generative capabilities of LLMs to automatically produce valid,\ndiverse, and semantically accurate PDDL domains from natural language input. To\nthis end, we first introduce a systematically formulated and validated UAV\nplanning dataset, consisting of ground-truth PDDL domains and associated\nproblems, each paired with detailed domain and action descriptions. Building on\nthis dataset, we design a prompting framework that generates high-quality PDDL\ndomains from language input. The generated domains are evaluated through syntax\nvalidation, executability, feasibility, and interpretability. Overall, this\nwork demonstrates that LLMs can substantially accelerate the creation of\ncomplex planning domains, providing a reproducible dataset and evaluation\npipeline that enables application experts without prior experience to leverage\nit for practical tasks and advance future research in aerial robotics and\nautomated planning.", "AI": {"tldr": "LLM\u53ef\u81ea\u52a8\u751f\u6210UAV\u4efb\u52a1\u7684PDDL\u9886\u57df\uff0c\u89e3\u51b3\u624b\u52a8\u521b\u5efa\u7684\u75db\u70b9\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1PDDL\u9886\u57df\u8017\u65f6\u8d39\u529b\u4e14\u6613\u51fa\u9519\uff0c\u963b\u788d\u4e86\u5176\u5728\u673a\u5668\u4eba\u89c4\u5212\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728UAV\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faSPAR\u6846\u67b6\uff0c\u5229\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u751f\u6210PDDL\u9886\u57df\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542bUAV\u9886\u57df\u548c\u95ee\u9898\u7684PDDL\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u63d0\u793a\u6846\u67b6\u6765\u751f\u6210PDDL\u9886\u57df\u3002\u901a\u8fc7\u8bed\u6cd5\u9a8c\u8bc1\u3001\u53ef\u6267\u884c\u6027\u3001\u53ef\u884c\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6765\u8bc4\u4f30\u751f\u6210\u7684\u9886\u57df\u3002", "result": "SPAR\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684PDDL\u9886\u57df\uff0cLLM\u53ef\u663e\u8457\u52a0\u901f\u590d\u6742\u89c4\u5212\u9886\u57df\u7684\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "conclusion": "LLM\u80fd\u6709\u6548\u89e3\u51b3PDDL\u9886\u57df\u521b\u5efa\u7684\u6311\u6218\uff0cSPAR\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4f7f\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u5e94\u7528\uff0c\u5e76\u63a8\u52a8\u4e86\u822a\u7a7a\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.13974", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13974", "abs": "https://arxiv.org/abs/2509.13974", "authors": ["Amirhossein Shahbazinia", "Jonathan Dan", "Jose A. Miranda", "Giovanni Ansaloni", "David Atienza"], "title": "Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection", "comment": null, "summary": "Objective: Epilepsy, a prevalent neurological disease, demands careful\ndiagnosis and continuous care. Seizure detection remains challenging, as\ncurrent clinical practice relies on expert analysis of electroencephalography,\nwhich is a time-consuming process and requires specialized knowledge.\nAddressing this challenge, this paper explores automated epileptic seizure\ndetection using deep learning, focusing on personalized continual learning\nmodels that adapt to each patient's unique electroencephalography signal\nfeatures, which evolve over time. Methods: In this context, our approach\naddresses the challenge of integrating new data into existing models without\ncatastrophic forgetting, a common issue in static deep learning models. We\npropose EpiSMART, a continual learning framework for seizure detection that\nuses a size-constrained replay buffer and an informed sample selection strategy\nto incrementally adapt to patient-specific electroencephalography signals. By\nselectively retaining high-entropy and seizure-predicted samples, our method\npreserves critical past information while maintaining high performance with\nminimal memory and computational requirements. Results: Validation on the\nCHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score\nover a trained baseline without updates in all other patients. On average,\nEpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,\nmaking it suitable for real-time deployment in wearable systems.\nConclusion:EpiSMART enables robust and personalized seizure detection under\nrealistic and resource-constrained conditions by effectively integrating new\ndata into existing models without degrading past knowledge. Significance: This\nframework advances automated seizure detection by providing a continual\nlearning approach that supports patient-specific adaptation and practical\ndeployment in wearable healthcare systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpiSMART\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u9002\u5e94\u60a3\u8005\u7684\u8111\u7535\u56fe\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u90e8\u7f72\u7684\u8d44\u6e90\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u7684\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u4f9d\u8d56\u4e8e\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u4eba\u5de5\u5206\u6790\u8111\u7535\u56fe\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpiSMART\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5927\u5c0f\u53d7\u9650\u7684\u91cd\u653e\u7f13\u51b2\u533a\u548c\u4fe1\u606f\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u7559\u9ad8\u71b5\u548c\u9884\u6d4b\u53d1\u4f5c\u7684\u6837\u672c\uff0c\u6765\u9010\u6b65\u9002\u5e94\u60a3\u8005\u7279\u5b9a\u7684\u8111\u7535\u56fe\u4fe1\u53f7\uff0c\u4ee5\u6574\u5408\u65b0\u6570\u636e\u800c\u4e0d\u4f1a\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0cEpiSMART\u5728F1\u5206\u6570\u4e0a\u6bd4\u672a\u66f4\u65b0\u7684\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8621%\uff0c\u5e73\u5747\u6bcf\u5929\u4ec5\u97006.46\u5206\u949f\u7684\u6807\u8bb0\u6570\u636e\u548c6.28\u6b21\u66f4\u65b0\uff0c\u9002\u5408\u5728\u53ef\u7a7f\u6234\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "EpiSMART\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u6709\u6548\u6574\u5408\u65b0\u6570\u636e\u4e14\u4e0d\u635f\u5bb3\u5df2\u6709\u77e5\u8bc6\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u4e2a\u6027\u5316\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u3002"}}
{"id": "2509.14047", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14047", "abs": "https://arxiv.org/abs/2509.14047", "authors": ["Taiki Nakano", "Ahmed Aboudonia", "Jaap Eising", "Andrea Martinelli", "Florian D\u00f6rfler", "John Lygeros"], "title": "Dissipativity-Based Data-Driven Decentralized Control of Interconnected Systems", "comment": null, "summary": "We propose data-driven decentralized control algorithms for stabilizing\ninterconnected systems. We first derive a data-driven condition to synthesize a\nlocal controller that ensures the dissipativity of the local subsystems. Then,\nwe propose data-driven decentralized stability conditions for the global system\nbased on the dissipativity of each local system. Since both conditions take the\nform of linear matrix inequalities and are based on dissipativity theory, this\nyields a unified pipeline, resulting in a data-driven decentralized control\nalgorithm. As a special case, we also consider stabilizing systems\ninterconnected through diffusive coupling and propose a control algorithm. We\nvalidate the effectiveness and the scalability of the proposed control\nalgorithms in numerical examples in the context of microgrids.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u4e92\u8054\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e92\u8054\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u7b97\u6cd5\u3002", "method": "\u9996\u5148\uff0c\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6761\u4ef6\u6765\u5408\u6210\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u4ee5\u786e\u4fdd\u5c40\u90e8\u5b50\u7cfb\u7edf\u7684\u8017\u6563\u6027\u3002\u7136\u540e\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u5c40\u90e8\u7cfb\u7edf\u7684\u8017\u6563\u6027\uff0c\u63d0\u51fa\u4e86\u5168\u5c40\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u7a33\u5b9a\u6027\u6761\u4ef6\u3002\u8fd9\u4e24\u79cd\u6761\u4ef6\u90fd\u91c7\u7528\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u7684\u5f62\u5f0f\uff0c\u5e76\u57fa\u4e8e\u8017\u6563\u6027\u7406\u8bba\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d41\u7a0b\uff0c\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u4e13\u95e8\u8ba8\u8bba\u4e86\u901a\u8fc7\u6269\u6563\u8026\u5408\u4e92\u8054\u7684\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u7b97\u6cd5\u5728\u5fae\u7535\u7f51\u7b49\u4e92\u8054\u7cfb\u7edf\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u7a33\u5b9a\u4e92\u8054\u7cfb\u7edf\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.", "AI": {"tldr": "LLMs\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u7cbe\u5ea6\u4efb\u52a1\uff08\u5982\u6570\u503c\u8ba1\u7b97\u548c\u7b26\u53f7\u8ba1\u7b97\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faTHOR\uff08Tool-Integrated Hierarchical Optimization via RL\uff09\uff0c\u4e00\u4e2a\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u63a8\u7406\u7684\u65b9\u6cd5\u3002THOR\u901a\u8fc7TIRGen\u6784\u5efa\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528RL\u7b56\u7565\u8fdb\u884c\u5206\u5c42\u4f18\u5316\uff0c\u540c\u65f6\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\u548c\u4ee3\u7801\u751f\u6210\u3002\u6b64\u5916\uff0cTHOR\u8fd8\u5305\u542b\u4e00\u4e2a\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff0c\u80fd\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5229\u7528\u5de5\u5177\u53cd\u9988\u52a8\u6001\u4fee\u6b63\u9519\u8bef\u8def\u5f84\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTHOR\u5728\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5904\u7406\u9ad8\u7cbe\u5ea6\u6570\u5b66\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6574\u5408\u5916\u90e8\u5de5\u5177\u6765\u5f25\u8865\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5728\u6784\u5efa\u6570\u636e\u96c6\u3001\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4f18\u5316\u548c\u589e\u5f3a\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "THOR\uff08Tool-Integrated Hierarchical Optimization via RL\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u5229\u7528TIRGen\uff08\u4e00\u4e2a\u591a\u667a\u80fd\u4f53actor-critic\u7ba1\u9053\uff09\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u96c6\u3002\u5176\u6b21\uff0c\u91c7\u7528\u4e00\u79cdRL\u7b56\u7565\u8fdb\u884c\u5206\u5c42\u4f18\u5316\uff0c\u540c\u65f6\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u5229\u7528\u4e2d\u95f4\u5de5\u5177\u8c03\u7528\u7684\u6210\u529f\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u3002\u6700\u540e\uff0c\u5f15\u5165\u4e00\u4e2a\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff0c\u5229\u7528\u5373\u65f6\u5de5\u5177\u53cd\u9988\u52a8\u6001\u4fee\u6b63\u63a8\u7406\u8def\u5f84\u3002", "result": "THOR\u65b9\u6cd5\u5728\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "THOR\u901a\u8fc7TIRGen\u3001RL\u5206\u5c42\u4f18\u5316\u548c\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5de5\u5177\u96c6\u6210\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u5355\u89c6\u89d2\u7ed3\u80a0\u955c\u89c6\u9891\u4e2d\u751f\u6210\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5177\u67093D\u91cd\u5efa\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u5904\u7406\u7ed3\u80a0\u955c\u89c6\u9891\u7684 temporal consistency \u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "method": "ColonCrafter\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u4ece\u5408\u6210\u7684\u7ed3\u80a0\u955c\u5e8f\u5217\u4e2d\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u98ce\u683c\u8f6c\u6362\u6280\u672f\uff0c\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u89c6\u9891\u3002", "result": "ColonCrafter\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u548c\u7ed3\u80a0\u955c\u4e13\u7528\u65b9\u6cd5\u3002", "conclusion": "\u5c3d\u7ba1\u5b8c\u6574\u7684\u8f68\u8ff93D\u91cd\u5efa\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u4f46ColonCrafter\u57283D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u5176\u6f5c\u529b\u3002"}}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5fb7\u56fd\u65b9\u8a00\u65f6\u8868\u73b0\u51fa\u663e\u8457\u7684\u504f\u89c1\uff0c\u5c06\u8d1f\u9762\u7279\u8d28\u4e0e\u65b9\u8a00\u4f7f\u7528\u8005\u76f8\u5173\u8054\uff0c\u5e76\u4e14\u5728\u660e\u786e\u63d0\u53ca\u65b9\u8a00\u4f7f\u7528\u8005\u65f6\u4f1a\u52a0\u5267\u8fd9\u79cd\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u6587\u5316\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u2014\u2014\u5fb7\u8bed\u65b9\u8a00\u2014\u2014\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u662f\u5426\u4f1a\u88ab\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u590d\u5236\uff0c\u4ee5\u8bc4\u4f30LLMs\u7684\u504f\u89c1\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u79cd\u5fb7\u8bed\u65b9\u8a00\u53ca\u5176\u6807\u51c6\u5fb7\u8bed\u5bf9\u5e94\u53e5\u7684\u65b0\u8bc4\u4f30\u8bed\u6599\u5e93\uff0c\u5e76\u901a\u8fc7\u8054\u60f3\u4efb\u52a1\u548c\u51b3\u7b56\u4efb\u52a1\u6765\u8bc4\u4f30LLMs\u7684\u65b9\u8a00\u547d\u540d\u504f\u89c1\u548c\u65b9\u8a00\u4f7f\u7528\u504f\u89c1\u3002", "result": "\u6240\u6709\u88ab\u8bc4\u4f30\u7684LLMs\u5728\u8054\u60f3\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65b9\u8a00\u547d\u540d\u548c\u65b9\u8a00\u4f7f\u7528\u504f\u89c1\uff0c\u5c06\u8d1f\u9762\u5f62\u5bb9\u8bcd\u4e0e\u5fb7\u8bed\u65b9\u8a00\u4f7f\u7528\u8005\u76f8\u5173\u8054\u3002\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u4e5f\u590d\u5236\u4e86\u8fd9\u4e9b\u504f\u89c1\u3002\u4e0e\u5148\u524d\u5173\u4e8e\u660e\u786e\u63d0\u53ca\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4f1a\u6700\u5c0f\u5316\u504f\u89c1\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u672c\u7814\u7a76\u53d1\u73b0\u660e\u786e\u6807\u6ce8\u8bed\u8a00\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff08\u5fb7\u8bed\u65b9\u8a00\u4f7f\u7528\u8005\uff09\u53cd\u800c\u4f1a\u6bd4\u9690\u6027\u7ebf\u7d22\uff08\u5982\u65b9\u8a00\u4f7f\u7528\uff09\u653e\u5927\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684LLMs\u5728\u5904\u7406\u5fb7\u8bed\u65b9\u8a00\u65f6\u5b58\u5728\u663e\u8457\u7684\u504f\u89c1\uff0c\u8fd9\u4e0e\u793e\u4f1a\u4e0a\u5bf9\u624b\u8bed\u4f7f\u7528\u8005\u5b58\u5728\u7684\u523b\u677f\u5370\u8c61\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u660e\u786e\u63d0\u53ca\u65b9\u8a00\u4f7f\u7528\u8005\u8eab\u4efd\u4f1a\u52a0\u5267\u8fd9\u79cd\u504f\u89c1\uff0c\u8fd9\u4e0e\u4ee5\u5f80\u5173\u4e8e\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5bf9\u504f\u89c1\u5f71\u54cd\u7684\u7814\u7a76\u7ed3\u679c\u76f8\u6096\u3002"}}
{"id": "2509.13821", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13821", "abs": "https://arxiv.org/abs/2509.13821", "authors": ["Frederik M\u00f8ller", "Gabriel Fern\u00e1ndez-Fern\u00e1ndez", "Thomas Schweigler", "Paulin de Schoulepnikoff", "J\u00f6rg Schmiedmayer", "Gorka Mu\u00f1oz-Gil"], "title": "Learning Minimal Representations of Many-Body Physics from Snapshots of a Quantum Simulator", "comment": "13 pages, 7 figures", "summary": "Analog quantum simulators provide access to many-body dynamics beyond the\nreach of classical computation. However, extracting physical insights from\nexperimental data is often hindered by measurement noise, limited observables,\nand incomplete knowledge of the underlying microscopic model. Here, we develop\na machine learning approach based on a variational autoencoder (VAE) to analyze\ninterference measurements of tunnel-coupled one-dimensional Bose gases, which\nrealize the sine-Gordon quantum field theory. Trained in an unsupervised\nmanner, the VAE learns a minimal latent representation that strongly correlates\nwith the equilibrium control parameter of the system. Applied to\nnon-equilibrium protocols, the latent space uncovers signatures of frozen-in\nsolitons following rapid cooling, and reveals anomalous post-quench dynamics\nnot captured by conventional correlation-based methods. These results\ndemonstrate that generative models can extract physically interpretable\nvariables directly from noisy and sparse experimental data, providing\ncomplementary probes of equilibrium and non-equilibrium physics in quantum\nsimulators. More broadly, our work highlights how machine learning can\nsupplement established field-theoretical techniques, paving the way for\nscalable, data-driven discovery in quantum many-body systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5206\u6790\u4e86\u96a7\u9053\u8026\u5408\u7684\u4e00\u7ef4\u73bb\u8272\u6c14\u4f53\u5e72\u6d89\u6d4b\u91cf\u6570\u636e\uff0c\u6210\u529f\u5730\u4ece\u5608\u6742\u4e14\u4e0d\u5b8c\u6574\u7684\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u4e86\u7269\u7406\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u51bb\u7ed3\u7684\u5b64\u5b50\u548c\u5f02\u5e38\u7684\u540e\u6dec\u706d\u52a8\u529b\u5b66\u3002", "motivation": "\u5728\u91cf\u5b50\u6a21\u62df\u4e2d\uff0c\u6d4b\u91cf\u566a\u58f0\u3001\u6709\u9650\u7684\u53ef\u89c2\u6d4b\u91cf\u4ee5\u53ca\u5bf9\u5fae\u89c2\u6a21\u578b\u7684\u77e5\u8bc6\u4e0d\u5b8c\u6574\u7b49\u56e0\u7d20\u963b\u788d\u4e86\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u7269\u7406\u89c1\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5bf9\u96a7\u9053\u8026\u5408\u7684\u4e00\u7ef4\u73bb\u8272\u6c14\u4f53\u5e72\u6d89\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\u3002VAE\u5b66\u4e60\u4e86\u4e00\u4e2a\u6700\u5c0f\u7684\u6f5c\u5728\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u7cfb\u7edf\u7684\u5e73\u8861\u63a7\u5236\u53c2\u6570\u5f3a\u76f8\u5173\u3002", "result": "VAE\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\u80fd\u591f\u63ed\u793a\u51bb\u7ed3\u7684\u5b64\u5b50\u4ee5\u53ca\u672a\u88ab\u4f20\u7edf\u76f8\u5173\u65b9\u6cd5\u6355\u6349\u5230\u7684\u5f02\u5e38\u540e\u6dec\u706d\u52a8\u529b\u5b66\u3002\u8fd9\u8868\u660e\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u4ece\u5608\u6742\u548c\u7a00\u758f\u7684\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u7269\u7406\u53d8\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u751f\u6210\u6a21\u578b\u5728\u4ece\u5608\u6742\u7684\u91cf\u5b50\u6a21\u62df\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u7269\u7406\u89c1\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.14115", "categories": ["cond-mat.mtrl-sci", "00-xx", "J.2; I.6"], "pdf": "https://arxiv.org/pdf/2509.14115", "abs": "https://arxiv.org/abs/2509.14115", "authors": ["Guilherme S. L. Fabris", "Raphael B. de Oliveira", "Marcelo L. Pereira Junior", "Robert Vajtai", "Pulickel M. Ajayan", "Douglas S. Galv\u00e3o"], "title": "From Glaphene to Glaphynes: A Hybridization of 2D Silica Glass and Graphynes", "comment": "22 pages and 5 figures", "summary": "Hybrid two-dimensional (2D) materials have attracted increasing interest as\nplatforms for tailoring electronic properties through interfacial design. Very\nrecently, a novel hybrid 2D material termed glaphene, which combines monolayers\nof 2D silica glass and graphene, was experimentally realized. Inspired by\nglaphenes, we proposed a new class of similar structures named glaphynes, which\nare formed by stacking SiO$_2$ monolayers onto $\\alpha$-, $\\beta$-, and\n$\\gamma$-graphynes. Graphynes are 2D carbon allotropes with the presence of\nacetylenic groups (triple bonds). The glaphynes' structural and electronic\nproperties were investigated using the density functional tight-binding (DFTB)\nmethod, as implemented in the DFTB+ package. Our analysis confirms their\nenergetic and structural stability. We have observed that in the case of\nglaphynes, the electronic proximity effect can indeed open the electronic band\ngap, but not for all cases, even with the formation of Si-O-C bonds between\nsilica and graphynes.", "AI": {"tldr": "glaphenes\uff08\u75312D\u4e8c\u6c27\u5316\u7845\u548c\u77f3\u58a8\u70ef\u7ec4\u6210\u7684\u6df7\u5408\u4e8c\u7ef4\u6750\u6599\uff09\u7684\u63d0\u51fa\u542f\u53d1\u4e86\u6211\u4eec\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u2014\u2014glaphynes\uff0c\u5373\u5c06SiO2\u5355\u5c42\u5806\u53e0\u5728\u77f3\u58a8\u70ef\u4e0a\u3002", "motivation": "\u63a2\u7d22\u6df7\u5408\u4e8c\u7ef4\u6750\u6599\u7684\u7535\u5b50\u7279\u6027\uff0c\u7279\u522b\u662fglaphynes\u7684\u7ed3\u6784\u548c\u7535\u5b50\u7279\u6027\u3002", "method": "\u4f7f\u7528DFT+\u8f6f\u4ef6\u5305\u4e2d\u7684\u5bc6\u5ea6\u6cdb\u51fd\u7d27\u675f\u7f1a\uff08DFTB\uff09\u65b9\u6cd5\u3002", "result": "glaphynes\u5728\u80fd\u91cf\u548c\u7ed3\u6784\u4e0a\u662f\u7a33\u5b9a\u7684\uff0c\u5e76\u4e14\u7535\u5b50\u8fd1\u90bb\u6548\u5e94\u786e\u5b9e\u53ef\u4ee5\u6253\u5f00\u7535\u5b50\u5e26\u9699\uff0c\u4f46\u5e76\u975e\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u5982\u6b64\uff0c\u5373\u4f7f\u5f62\u6210\u4e86Si-O-C\u952e\u3002", "conclusion": "glaphynes\u5728\u7ed3\u6784\u4e0a\u662f\u7a33\u5b9a\u7684\uff0c\u5e76\u4e14\u7535\u5b50\u8fd1\u90bb\u6548\u5e94\u5bf9\u6253\u5f00\u7535\u5b50\u5e26\u9699\u6709\u5f71\u54cd\uff0c\u4f46\u8fd9\u79cd\u5f71\u54cd\u5e76\u975e\u666e\u904d\u5b58\u5728\u3002"}}
{"id": "2509.13692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13692", "abs": "https://arxiv.org/abs/2509.13692", "authors": ["Yadan Zeng", "Jiadong Zhou", "Xiaohan Li", "I-Ming Chen"], "title": "HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion", "comment": "9 pages, 6 figures", "summary": "Point cloud completion is essential for robotic perception, object\nreconstruction and supporting downstream tasks like grasp planning, obstacle\navoidance, and manipulation. However, incomplete geometry caused by\nself-occlusion and sensor limitations can significantly degrade downstream\nreasoning and interaction. To address these challenges, we propose HGACNet, a\nnovel framework that reconstructs complete point clouds of individual objects\nby hierarchically encoding 3D geometric features and fusing them with\nimage-guided priors from a single-view RGB image. At the core of our approach,\nthe Hierarchical Graph Attention (HGA) encoder adaptively selects critical\nlocal points through graph attention-based downsampling and progressively\nrefines hierarchical geometric features to better capture structural continuity\nand spatial relationships. To strengthen cross-modal interaction, we further\ndesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performs\nattention-based feature alignment between hierarchical geometric features and\nstructured visual representations, enabling fine-grained semantic guidance for\ncompletion. In addition, we proposed the contrastive loss (C-Loss) to\nexplicitly align the feature distributions across modalities, improving\ncompletion fidelity under modality discrepancy. Finally, extensive experiments\nconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset\nconfirm the effectiveness of HGACNet, demonstrating state-of-the-art\nperformance as well as strong applicability in real-world robotic manipulation\ntasks.", "AI": {"tldr": "HGACNet\u662f\u4e00\u4e2a\u7528\u4e8e\u70b9\u4e91\u8865\u5168\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u78013D\u51e0\u4f55\u7279\u5f81\u5e76\u878d\u5408\u5355\u89c6\u89d2RGB\u56fe\u50cf\u7684\u89c6\u89c9\u5148\u9a8c\u6765\u91cd\u5efa\u5b8c\u6574\u7684\u70b9\u4e91\u3002", "motivation": "\u70b9\u4e91\u8865\u5168\u662f\u673a\u5668\u4eba\u611f\u77e5\u3001\u7269\u4f53\u91cd\u5efa\u548c\u6293\u53d6\u89c4\u5212\u3001\u907f\u969c\u3001\u64cd\u4f5c\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u57fa\u7840\u3002\u7136\u800c\uff0c\u7531\u4e8e\u81ea\u906e\u6321\u548c\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u51e0\u4f55\u5f62\u72b6\u4f1a\u4e25\u91cd\u5f71\u54cd\u4e0b\u6e38\u63a8\u7406\u548c\u4ea4\u4e92\u3002", "method": "HGACNet\u7684\u6838\u5fc3\u662f\u5206\u5c42\u56fe\u6ce8\u610f\u529b\uff08HGA\uff09\u7f16\u7801\u5668\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u4e0b\u91c7\u6837\u81ea\u9002\u5e94\u5730\u9009\u62e9\u5173\u952e\u7684\u5c40\u90e8\u70b9\uff0c\u5e76\u9010\u6b65\u7ec6\u5316\u5206\u5c42\u51e0\u4f55\u7279\u5f81\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7ed3\u6784\u8fde\u7eed\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u4e3a\u4e86\u52a0\u5f3a\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\uff08MSCF\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5bf9\u5206\u5c42\u51e0\u4f55\u7279\u5f81\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\u4e4b\u95f4\u8fdb\u884c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u4ece\u800c\u4e3a\u8865\u5168\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5bf9\u6bd4\u635f\u5931\uff08C-Loss\uff09\u4ee5\u660e\u786e\u5730\u5bf9\u9f50\u8de8\u6a21\u6001\u7684\u7279\u5f81\u5206\u5e03\uff0c\u63d0\u9ad8\u5728\u6a21\u6001\u5dee\u5f02\u4e0b\u7684\u8865\u5168\u4fdd\u771f\u5ea6\u3002", "result": "\u5728ShapeNet-ViPC\u57fa\u51c6\u548cYCB-Complete\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\u4e86HGACNet\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u7528\u6027\u3002", "conclusion": "HGACNet\u901a\u8fc7\u5206\u5c42\u51e0\u4f55\u7279\u5f81\u7f16\u7801\u548c\u56fe\u50cf\u5f15\u5bfc\u5148\u9a8c\u878d\u5408\uff0c\u5728\u70b9\u4e91\u8865\u5168\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13753", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13753", "abs": "https://arxiv.org/abs/2509.13753", "authors": ["Hyotaek Jeon", "Hyunwook Lee", "Juwon Kim", "Sungahn Ko"], "title": "ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting", "comment": "11 pages, 4 figures, Accepted to CIKM 2025. Code:\n  https://github.com/HyoTaek98/ST_LINK", "summary": "Traffic forecasting represents a crucial problem within intelligent\ntransportation systems. In recent research, Large Language Models (LLMs) have\nemerged as a promising method, but their intrinsic design, tailored primarily\nfor sequential token processing, introduces notable challenges in effectively\ncapturing spatial dependencies. Specifically, the inherent limitations of LLMs\nin modeling spatial relationships and their architectural incompatibility with\ngraph-structured spatial data remain largely unaddressed. To overcome these\nlimitations, we introduce ST-LINK, a novel framework that enhances the\ncapability of Large Language Models to capture spatio-temporal dependencies.\nIts key components are Spatially-Enhanced Attention (SE-Attention) and the\nMemory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary\nposition embeddings to integrate spatial correlations as direct rotational\ntransformations within the attention mechanism. This approach maximizes spatial\nlearning while preserving the LLM's inherent sequential processing structure.\nMeanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to\ncapture complex temporal dependencies and improve the stability of long-term\nforecasting. Comprehensive experiments on benchmark datasets demonstrate that\nST-LINK surpasses conventional deep learning and LLM approaches, and\neffectively captures both regular traffic patterns and abrupt changes.", "AI": {"tldr": "ST-LINK\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u5904\u7406\u7a7a\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u4ea4\u901a\u9884\u6d4b\u7b49\u9700\u8981\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u67b6\u6784\u4e3b\u8981\u9488\u5bf9\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u4e14\u5728\u6574\u5408\u56fe\u7ed3\u6784\u7a7a\u95f4\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u517c\u5bb9\u6027\u3002", "method": "ST-LINK\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u589e\u5f3a\u6ce8\u610f\u529b\uff08SE-Attention\uff09\u548c\u8bb0\u5fc6\u68c0\u7d22\u524d\u9988\u7f51\u7edc\uff08MRFFN\uff09\u6765\u589e\u5f3aLLMs\u3002SE-Attention\u901a\u8fc7\u5c06\u7a7a\u95f4\u76f8\u5173\u6027\u4f5c\u4e3a\u76f4\u63a5\u65cb\u8f6c\u53d8\u6362\u96c6\u6210\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u6765\u6269\u5c55\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u3002MRFFN\u5219\u52a8\u6001\u68c0\u7d22\u548c\u5229\u7528\u5173\u952e\u5386\u53f2\u6a21\u5f0f\uff0c\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u5e76\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cST-LINK\u5728\u6355\u6349\u89c4\u5f8b\u6027\u4ea4\u901a\u6a21\u5f0f\u548c\u7a81\u53d8\u65b9\u9762\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u548cLLMs\u65b9\u6cd5\u3002", "conclusion": "ST-LINK\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLMs\u5728\u4ea4\u901a\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u4fe1\u606f\u5904\u7406\u74f6\u9888\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14029", "categories": ["cs.LG", "eess.SP", "physics.comp-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.14029", "abs": "https://arxiv.org/abs/2509.14029", "authors": ["Samuel Tovey", "Julian Ho\u00dfbach", "Sandro Kuppel", "Tobias Ensslen", "Jan C. Behrends", "Christian Holm"], "title": "Deep Learning-Driven Peptide Classification in Biological Nanopores", "comment": "29 pages (incl. references) 7 figures", "summary": "A device capable of performing real time classification of proteins in a\nclinical setting would allow for inexpensive and rapid disease diagnosis. One\nsuch candidate for this technology are nanopore devices. These devices work by\nmeasuring a current signal that arises when a protein or peptide enters a\nnanometer-length-scale pore. Should this current be uniquely related to the\nstructure of the peptide and its interactions with the pore, the signals can be\nused to perform identification. While such a method would allow for real time\nidentification of peptides and proteins in a clinical setting, to date, the\ncomplexities of these signals limit their accuracy. In this work, we tackle the\nissue of classification by converting the current signals into scaleogram\nimages via wavelet transforms, capturing amplitude, frequency, and time\ninformation in a modality well-suited to machine learning algorithms. When\ntested on 42 peptides, our method achieved a classification accuracy of\n~$81\\,\\%$, setting a new state-of-the-art in the field and taking a step toward\npractical peptide/protein diagnostics at the point of care. In addition, we\ndemonstrate model transfer techniques that will be critical when deploying\nthese models into real hardware, paving the way to a new method for real-time\ndisease diagnosis.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u7eb3\u7c73\u5b54\u7535\u6d41\u4fe1\u53f7\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u86cb\u767d\u8d28\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe81%\uff0c\u4e3a\u5b9e\u65f6\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u3001\u4f4e\u6210\u672c\u3001\u5feb\u901f\u5730\u8fdb\u884c\u75be\u75c5\u8bca\u65ad\u7684\u4e34\u5e8a\u86cb\u767d\u8d28\u5206\u7c7b\u8bbe\u5907\u3002", "method": "\u5c06\u86cb\u767d\u8d28\u901a\u8fc7\u7eb3\u7c73\u5b54\u65f6\u4ea7\u751f\u7684\u7535\u6d41\u4fe1\u53f7\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u572842\u79cd\u80bd\u7684\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea681%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u8fc1\u79fb\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7535\u6d41\u4fe1\u53f7\u8f6c\u5316\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u86cb\u767d\u8d28\u5206\u7c7b\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u73b0\u5b9e\u9645\u7684\u5373\u65f6\u62a4\u7406\u70b9\u80bd/\u86cb\u767d\u8d28\u8bca\u65ad\u548c\u5b9e\u65f6\u75be\u75c5\u8bca\u65ad\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.14065", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14065", "abs": "https://arxiv.org/abs/2509.14065", "authors": ["Jaidev Gill", "Jing Shuang Li"], "title": "Identifying Network Structure of Linear Dynamical Systems: Observability and Edge Misclassification", "comment": "7 pages, 5 figures, in submission", "summary": "This work studies the limitations of uniquely identifying a linear network's\ntopology from partial measurements of its nodes. We show that the set of\nnetworks that are consistent with the measurements are related through the\nnullspace of the observability matrix for the true network. In doing so, we\nillustrate how potentially many networks are fully consistent with the\nmeasurements despite having topologies that are structurally inconsistent with\neach other, an often neglected consideration in the design of topology\ninference methods. We then provide an aggregate characterization of the space\nof possible networks by analytically solving for the most structurally\ndissimilar network. We find that when observing over 6% of nodes in random\nnetwork models (e.g., Erd\\H{o}s-R\\'{e}nyi and Watts-Strogatz) the rate of edge\nmisclassification drops to ~1%. Extending this discussion, we construct a\nfamily of networks that keep measurements $\\epsilon$-\"close\" to each other, and\nconnect the identifiability of these networks to the spectral properties of an\naugmented observability Gramian.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4ece\u90e8\u5206\u8282\u70b9\u6d4b\u91cf\u4e2d\u552f\u4e00\u8bc6\u522b\u7ebf\u6027\u7f51\u7edc\u62d3\u6251\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u7ebf\u6027\u7f51\u7edc\u62d3\u6251\u8bc6\u522b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u62d3\u6251\u63a8\u65ad\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u53ef\u89c2\u5bdf\u6027\u77e9\u9635\u7684\u96f6\u7a7a\u95f4\u6765\u8868\u5f81\u4e0e\u6d4b\u91cf\u4e00\u81f4\u7684\u7f51\u7edc\u96c6\uff0c\u5e76\u89e3\u6790\u6c42\u89e3\u6700\u7ed3\u6784\u4e0a\u4e0d\u76f8\u4f3c\u7684\u7f51\u7edc\u3002", "result": "\u5728\u968f\u673a\u7f51\u7edc\u6a21\u578b\uff08\u5982 Erd\u0151s-R\u00e9nyi \u548c Watts-Strogatz\uff09\u4e2d\uff0c\u5f53\u89c2\u5bdf\u8d85\u8fc7 6% \u7684\u8282\u70b9\u65f6\uff0c\u8fb9\u8bef\u5206\u7c7b\u7387\u4e0b\u964d\u5230\u7ea6 1%\u3002", "conclusion": "\u5f53\u89c2\u5bdf\u8d85\u8fc7 6% \u7684\u8282\u70b9\u65f6\uff0c\u7ebf\u6027\u7f51\u7edc\u62d3\u6251\u7684\u8bc6\u522b\u7387\u5f88\u9ad8\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6784\u9020\u4e00\u4e2a\u4f7f\u6d4b\u91cf\u503c\u5f7c\u6b64\u201c\u63a5\u8fd1\u201d\u7684\u7f51\u7edc\u65cf\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7f51\u7edc\u7684\u53ef\u8bc6\u522b\u6027\u4e0e\u589e\u5f3a\u7684\u53ef\u89c2\u5bdf\u6027 Gramian \u7684\u8c31\u6027\u8d28\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.13773", "categories": ["cs.AI", "cs.IR", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13773", "abs": "https://arxiv.org/abs/2509.13773", "authors": ["Zhipeng Bian", "Jieming Zhu", "Xuyang Xie", "Quanyu Dai", "Zhou Zhao", "Zhenhua Dong"], "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103", "summary": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.", "AI": {"tldr": "MIRA\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u624b\u673a\u4e0a\u7684\u4e00\u952e\u5f0fAI\u4efb\u52a1\u6267\u884c\uff0c\u63d0\u9ad8\u4e86\u6307\u4ee4\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u7b80\u5316\u7528\u6237\u5728\u667a\u80fd\u624b\u673a\u4e0a\u8bbf\u95ee\u9884\u5b9a\u4e49AI\u670d\u52a1\u7684\u64cd\u4f5c\uff0c\u5e76\u5e94\u5bf9\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u53d1\u5c55\u548c\u5728\u667a\u80fd\u624b\u673a\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u63a8\u8350\u6d41\u7a0b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u6765\u63d0\u53d6\u5173\u952e\u5b9e\u4f53\u3001\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u7cbe\u786e\u6307\u4ee4\u3002\u8be5\u65b9\u6cd5\u8fd8\u5305\u62ec\u4e00\u4e2a\u589e\u5f3a\u63a8\u7406\u7684\u6a21\u677f\u673a\u5236\u548c\u4e00\u4e2a\u57fa\u4e8e\u524d\u7f00\u6811\u7684\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u8f93\u51fa\u7684\u8fde\u8d2f\u6027\u548c\u4e0e\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0cMIRA\u5728\u6307\u4ee4\u63a8\u8350\u7684\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MIRA\u6846\u67b6\u5728\u667a\u80fd\u624b\u673a\u4e0a\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6d41\u7545\u3001\u66f4\u9ad8\u6548\u7684AI\u670d\u52a1\u4ea4\u4e92\u65b9\u5f0f\uff0c\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8\u7528\u6237\u4e0e\u8bbe\u5907\u4e92\u52a8\u7684\u65b9\u5f0f\u3002"}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5185\u5b58\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982\u5fae\u578b\u98de\u884c\u5668\uff09\u4e0a\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6548\u679c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f53\u7d20\u7a7a\u95f4\u5408\u5e76\u5197\u4f59\u7684\u9ad8\u65af\u70b9\u5e76\u4f7f\u7528Patch-Grid\uff08PG\uff09\u521d\u59cb\u5316\u6765\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u4e3b\u8981\u5728\u9ad8\u6027\u80fdGPU\u4e0a\u8fd0\u884c\uff0c\u4f46\u5ffd\u7565\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982\u5fae\u578b\u98de\u884c\u5668\uff09\u7684\u5e94\u7528\u3002\u8fd9\u4e9b\u5e73\u53f0\u5728\u7cfb\u7edf\u6027\u80fd\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f53\u7d20\u7a7a\u95f4\u6839\u636e\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76\u5197\u4f593D\u9ad8\u65af\u539f\u8bed\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u901a\u8fc7Patch-Grid\uff08PG\uff09\u70b9\u91c7\u6837\u521d\u59cb\u53163D\u9ad8\u65af\u539f\u8bed\uff0c\u4ee5\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5728\u4f53\u7d20\u7a7a\u95f4\u5408\u5e76\u5197\u4f59\u9ad8\u65af\u70b9\uff0c\u5728\u4e0d\u5f71\u54cd\u7cfb\u7edf\u8fd0\u884c\u65f6\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86GPU\u5185\u5b58\u4f7f\u7528\u3002\u901a\u8fc7Patch-Grid\uff08PG\uff09\u521d\u59cb\u5316\uff0c\u63d0\u9ad8\u4e86\u6e32\u67d3\u8d28\u91cf\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5728\u5185\u5b58\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8fd0\u884c3DGS\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u5408\u5e76\u548c\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "LLM\u5728\u5904\u7406\u590d\u6742\u548c\u654f\u611f\u7684\u793e\u4f1a\u504f\u89c1\u65f6\u53ef\u80fd\u51fa\u73b0\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5206\u679012\u4e2aLLM\u5728\u4e0d\u540c\u7c7b\u578b\u504f\u89c1\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u7279\u5b9a\u7c7b\u578b\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u504f\u597d\uff0c\u4e14\u540c\u6a21\u578b\u5bb6\u65cf\u7684LLM\u5224\u65ad\u4e00\u81f4\u6027\u8f83\u9ad8\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86LLM\u5bf9HVSB\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540cLLM\u5728\u7406\u89e3\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u504f\u597d\u81ea\u8eab\u751f\u6210\u7684\u89e3\u91ca\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5bf9\u5c0f\u578bLM\u8fdb\u884c\u5fae\u8c03\uff0c\u53d1\u73b0\u5176\u751f\u6210\u7684\u89e3\u91ca\u66f4\u5177\u53ef\u8bfb\u6027\uff0c\u4f46\u6a21\u578b\u53ef\u4fe1\u5ea6\u76f8\u5bf9\u8f83\u4f4e\u3002", "motivation": "\u7814\u7a76LLM\u5728\u5904\u7406\u793e\u4f1a\u504f\u89c1\u65f6\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u7279\u522b\u662f\u63a2\u8ba8\u5176\u5728\u4e0d\u540c\u7c7b\u578b\u504f\u89c1\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4ee5\u53caLLM\u5bf9HVSB\u7684\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u5206\u679012\u4e2aLLM\u5728\u56db\u79cd\u6570\u636e\u96c6\u548c\u56db\u79cd\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u3001\u4e0d\u540c\u7c7b\u578b\u573a\u666f\u4e0b\u7684\u5bf9\u9f50\u7387\u548c\u653b\u51fb\u6210\u529f\u7387\u3002\u540c\u65f6\uff0c\u7814\u7a76LLM\u5bf9HVSB\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5bf9\u5c0f\u578bLM\u8fdb\u884c\u5fae\u8c03\u4ee5\u8bc4\u4f30\u5176\u89e3\u91ca\u80fd\u529b\u3002", "result": "LLM\u5728\u7279\u5b9a\u7c7b\u578b\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u504f\u597d\uff0c\u4e14\u540c\u6a21\u578b\u5bb6\u65cf\u7684LLM\u5224\u65ad\u4e00\u81f4\u6027\u8f83\u9ad8\u3002LLM\u5728\u7406\u89e3HVSB\u65b9\u9762\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u504f\u597d\u81ea\u8eab\u751f\u6210\u7684\u89e3\u91ca\u3002\u5fae\u8c03\u540e\u7684\u5c0f\u578bLM\u751f\u6210\u7684\u89e3\u91ca\u53ef\u8bfb\u6027\u66f4\u9ad8\uff0c\u4f46\u6a21\u578b\u53ef\u4fe1\u5ea6\u8f83\u4f4e\u3002", "conclusion": "LLM\u5728\u793e\u4f1a\u504f\u89c1\u573a\u666f\u4e0b\u7684\u5bf9\u9f50\u60c5\u51b5\u5e76\u975e\u5b8c\u5168\u7531\u6a21\u578b\u89c4\u6a21\u51b3\u5b9a\uff0c\u5b58\u5728\u573a\u666f\u504f\u597d\u548c\u5bb6\u65cf\u4e00\u81f4\u6027\u3002LLM\u5bf9HVSB\u7684\u7406\u89e3\u80fd\u529b\u76f8\u4f3c\uff0c\u4f46\u5b58\u5728\u201c\u81ea\u8bf4\u81ea\u8bdd\u201d\u7684\u503e\u5411\u3002\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u63d0\u5347\u5c0f\u578bLM\u7684\u89e3\u91ca\u53ef\u8bfb\u6027\uff0c\u4f46\u9700\u6743\u8861\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.13856", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13856", "abs": "https://arxiv.org/abs/2509.13856", "authors": ["S. V. Mousavi"], "title": "Trajectory-based Measure of Nonlocality in the Double Caldeira-Leggett Formalism", "comment": "Accepted for Publication in Physica Scripta", "summary": "We investigate the dynamics of quantum correlations in bipartite systems\ninitially prepared in a squeezed state, comparing closed-system unitary\nevolution under the Schr\\\"odinger equation with open-system dynamics governed\nby the Caldeira-Leggett master equation in the high-temperature, weak-coupling\nregime, all within the Bohmian mechanics framework. Quantum nonlocality is\nquantified via the sensitivity of the Bohmian velocity of one particle to the\nposition of the other. Our results show that in both distinct (local) and\ncommon bath scenarios, nonlocal correlations initially grow from zero, reach a\npeak, and then decay. In the case of local baths, the decay is smooth and\nmonotonic; although the peak value increases with temperature, its temporal\nwidth (measured via the full width at half maximum) decreases, indicating a\nshorter duration of nonlocal correlations. For a common bath, the initial\ngrowth and decay are followed by revivals and oscillations, whose amplitude and\ntiming vary with temperature. These non-monotonic behaviors arise despite the\nMarkovian nature of the underlying dynamics and reflect the nontrivial role of\nsystem-bath correlations. We also analyze how both temperature and the\nsqueezing decay parameter affect the structure of Bohmian trajectories and the\nevolution of nonlocal correlations. This trajectory-based, velocity-sensitivity\nmeasure offers an intuitive and quantitative understanding of entanglement\ndegradation, decoherence, and their characteristic time scales. Our findings\nemphasize how the structure of the environment critically shapes the observable\ndynamics of quantum correlations, even in Markovian regimes.", "AI": {"tldr": "\u91cf\u5b50\u5173\u8054\u5728\u95ed\u5408\u548c\u5f00\u653e\u7684\u4e8c\u5206\u7cfb\u7edf\u52a8\u529b\u5b66\u4e2d\u7684\u6f14\u5316\uff0c\u5229\u7528\u73bb\u59c6\u529b\u5b66\u548c\u901f\u5ea6\u654f\u611f\u6027\u91cf\u5316\u975e\u5c40\u57df\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u6e29\u5ea6\u548c\u538b\u7f29\u8870\u51cf\u53c2\u6570\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5728\u95ed\u5408\u548c\u5f00\u653e\u7684\u4e8c\u5206\u91cf\u5b50\u7cfb\u7edf\u4e2d\u91cf\u5b50\u5173\u8054\uff08\u7279\u522b\u662f\u91cf\u5b50\u975e\u5c40\u57df\u6027\uff09\u7684\u52a8\u529b\u5b66\u6f14\u5316\uff0c\u5e76\u7406\u89e3\u73af\u5883\u7ed3\u6784\uff08\u5c40\u90e8\u6d74 vs. \u5171\u540c\u6d74\uff09\u548c\u6e29\u5ea6\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u6f14\u5316\u3002", "method": "\u5728\u73bb\u59c6\u529b\u5b66\u6846\u67b6\u4e0b\uff0c\u5229\u7528\u859b\u5b9a\u8c14\u65b9\u7a0b\u6a21\u62df\u95ed\u5408\u7cfb\u7edf\u7684\u5e7a\u6b63\u6f14\u5316\uff0c\u5229\u7528Caldeira-Leggett\u4e3b\u65b9\u7a0b\u6a21\u62df\u5f00\u653e\u7cfb\u7edf\uff08\u9ad8\u6e29\u5ea6\uff0c\u5f31\u8026\u5408\uff09\u7684\u52a8\u529b\u5b66\u3002\u901a\u8fc7\u4e00\u4e2a\u7c92\u5b50\u7684\u901f\u5ea6\u5bf9\u53e6\u4e00\u4e2a\u7c92\u5b50\u4f4d\u7f6e\u7684\u654f\u611f\u6027\u6765\u91cf\u5316\u91cf\u5b50\u975e\u5c40\u57df\u6027\u3002", "result": "\u5728\u5c40\u90e8\u6d74\u548c\u5171\u540c\u6d74\u7684\u60c5\u51b5\u4e0b\uff0c\u975e\u5c40\u57df\u5173\u8054\u7684\u6f14\u5316\u90fd\u8868\u73b0\u51fa\u5148\u589e\u957f\u3001\u8fbe\u5230\u5cf0\u503c\u7136\u540e\u8870\u51cf\u7684\u8d8b\u52bf\u3002\u5728\u5c40\u90e8\u6d74\u4e2d\uff0c\u8870\u51cf\u662f\u5e73\u6ed1\u7684\uff1b\u6e29\u5ea6\u5347\u9ad8\u4f1a\u589e\u52a0\u5cf0\u503c\uff0c\u4f46\u7f29\u77ed\u6301\u7eed\u65f6\u95f4\u3002\u5728\u5171\u540c\u6d74\u4e2d\uff0c\u9664\u4e86\u589e\u957f\u548c\u8870\u51cf\uff0c\u8fd8\u4f1a\u51fa\u73b0\u590d\u73b0\u548c\u632f\u8361\uff0c\u5176\u5e45\u5ea6\u548c\u65f6\u95f4\u968f\u6e29\u5ea6\u53d8\u5316\u3002\u5206\u6790\u4e86\u6e29\u5ea6\u548c\u538b\u7f29\u8870\u51cf\u53c2\u6570\u5bf9\u73bb\u59c6\u8f68\u8ff9\u7ed3\u6784\u548c\u975e\u5c40\u57df\u5173\u8054\u6f14\u5316\u7684\u5f71\u54cd\u3002", "conclusion": "\u73af\u5883\u7684\u7ed3\u6784\uff08\u5373\u4f7f\u5728\u9a6c\u5c14\u53ef\u592b\u6761\u4ef6\u4e0b\uff09\u5bf9\u91cf\u5b50\u5173\u8054\u7684\u53ef\u89c2\u6d4b\u52a8\u529b\u5b66\u6709\u7740\u91cd\u8981\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u8f68\u8ff9\u7684\u901f\u5ea6\u654f\u611f\u6027\u5ea6\u91cf\u53ef\u4ee5\u76f4\u89c2\u4e14\u5b9a\u91cf\u5730\u7406\u89e3\u7ea0\u7f20\u9000\u5316\u3001\u9000\u76f8\u5e72\u53ca\u5176\u7279\u5f81\u65f6\u95f4\u5c3a\u5ea6\u3002"}}
{"id": "2509.14200", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.14200", "abs": "https://arxiv.org/abs/2509.14200", "authors": ["Gyanu P. Kafle", "Zhen-Fei Liu"], "title": "Phonon-assisted photoluminescence of bilayer MoS$_2$ from first principles", "comment": "12 pages, 6 figures", "summary": "In indirect band gap materials, phonon-assisted processes are key mechanisms\nfor photoluminescence (PL). Using a first-principles many-body approach, we\nsystematically investigate the phonon-assisted PL in bilayer MoS$_2$ and its\ndependence on temperature and external tensile strain. The effects of phonons\nare accounted for using a supercell approach: we identify the phonon momenta\nthat are important to PL, construct supercells that are commensurate with these\nphonons, and examine the changes in the optical absorption after explicit\ndisplacements of atoms along each phonon mode. The PL intensity is then\nobtained via the van Roosbroeck-Shockley relationship from the optical\nabsorption spectra. This approach enables us to investigate phonon-absorption\nand phonon-emission processes separately and how each process depends on\ntemperature. Our results reveal that optical phonons associated with\nout-of-plane vibrations of S atoms and in-plane vibrations of Mo atoms\ncontribute most to the indirect PL for unstrained bilayer MoS$_2$.\nAdditionally, we also discuss how the PL spectra and the phonon contributions\nevolve with strain. In particular, we show that at high strain, additional\nphonon channels become available due to the modulation of the electronic band\nstructure.", "AI": {"tldr": "\u6211\u4eec\u7814\u7a76\u4e86\u53cc\u5c42MoS2\u5728\u6e29\u5ea6\u548c\u5e94\u53d8\u4e0b\u7684\u58f0\u5b50\u8f85\u52a9\u5149\u81f4\u53d1\u5149\u3002\u6211\u4eec\u53d1\u73b0\u7279\u5b9a\u58f0\u5b50\u6a21\u5f0f\u5bf9\u53d1\u5149\u8d21\u732e\u6700\u5927\uff0c\u5e76\u4e14\u5e94\u53d8\u4f1a\u6539\u53d8\u53d1\u5149\u884c\u4e3a\u3002", "motivation": "\u95f4\u63a5\u5e26\u9699\u6750\u6599\u7684\u5149\u81f4\u53d1\u5149\uff08PL\uff09\u4e3b\u8981\u4f9d\u8d56\u58f0\u5b50\u8f85\u52a9\u8fc7\u7a0b\uff0c\u7406\u89e3\u8fd9\u4e9b\u8fc7\u7a0b\u5bf9\u4e8e\u8c03\u8282\u6750\u6599\u7684\u5149\u5b66\u6027\u8d28\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u591a\u4f53\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d85\u80de\u65b9\u6cd5\u6765\u6a21\u62df\u58f0\u5b50\u6548\u5e94\u3002\u901a\u8fc7\u6539\u53d8\u539f\u5b50\u4f4d\u7f6e\u6765\u6a21\u62df\u58f0\u5b50\u6a21\u5f0f\uff0c\u5e76\u8ba1\u7b97\u5149\u5b66\u5438\u6536\u5149\u8c31\u3002\u5229\u7528van Roosbroeck-Shockley\u5173\u7cfb\u4ece\u5149\u5b66\u5438\u6536\u5149\u8c31\u5f97\u5230PL\u5f3a\u5ea6\uff0c\u5e76\u5206\u522b\u5206\u6790\u58f0\u5b50\u5438\u6536\u548c\u58f0\u5b50\u53d1\u5c04\u8fc7\u7a0b\u5bf9\u6e29\u5ea6\u7684\u4f9d\u8d56\u6027\u3002", "result": "\u5bf9\u4e8e\u672a\u52a0\u5e94\u53d8\u7684\u53cc\u5c42MoS2\uff0c\u4e0eS\u539f\u5b50\u5c42\u5916\u632f\u52a8\u548cMo\u539f\u5b50\u5c42\u5185\u632f\u52a8\u76f8\u5173\u7684\u5149\u5b66\u58f0\u5b50\u5bf9\u95f4\u63a5PL\u7684\u8d21\u732e\u6700\u5927\u3002\u968f\u7740\u5e94\u53d8\u589e\u52a0\uff0c\u7535\u5b50\u80fd\u5e26\u7ed3\u6784\u53d1\u751f\u8c03\u5236\uff0c\u4f7f\u5f97\u5728\u9ad8\u5e94\u53d8\u4e0b\u51fa\u73b0\u989d\u5916\u7684\u58f0\u5b50\u53d1\u5149\u901a\u9053\u3002\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86PL\u5149\u8c31\u548c\u58f0\u5b50\u8d21\u732e\u5982\u4f55\u968f\u5e94\u53d8\u6f14\u53d8\u3002", "conclusion": "\u53cc\u5c42MoS2\u7684\u58f0\u5b50\u8f85\u52a9\u5149\u81f4\u53d1\u5149\u5bf9\u6e29\u5ea6\u548c\u5e94\u53d8\u654f\u611f\u3002\u7279\u5b9a\u58f0\u5b50\u6a21\u5f0f\u662f\u5f71\u54cd\u53d1\u5149\u5f3a\u5ea6\u7684\u5173\u952e\uff0c\u800c\u5e94\u53d8\u53ef\u4ee5\u8c03\u63a7\u80fd\u5e26\u7ed3\u6784\uff0c\u4ece\u800c\u5f15\u5165\u65b0\u7684\u53d1\u5149\u901a\u9053\uff0c\u8fd9\u4e3a\u901a\u8fc7\u5e94\u53d8\u5de5\u7a0b\u4f18\u5316\u6750\u6599\u7684\u5149\u7535\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.13720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13720", "abs": "https://arxiv.org/abs/2509.13720", "authors": ["Tianle Zeng", "Jianwei Peng", "Hanjing Ye", "Guangcheng Chen", "Senzi Luo", "Hong Zhang"], "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility", "comment": "Page:https://tianlezeng.github.io/EzReal/", "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces\nmany challenges; we specifically address a coupled one: long-range targets that\nreduce to tiny projections and intermittent visibility due to partial or\ncomplete occlusion. We present a unified, lightweight closed-loop system built\non an aligned multi-scale image tile hierarchy. Through hierarchical\ntarget-saliency fusion, it summarizes localized semantic contrast into a stable\ncoarse-layer regional saliency that provides the target direction and indicates\ntarget visibility. This regional saliency supports visibility-aware heading\nmaintenance through keyframe memory, saliency-weighted fusion of historical\nheadings, and active search during temporary invisibility. The system avoids\nwhole-image rescaling, enables deterministic bottom-up aggregation, supports\nzero-shot navigation, and runs efficiently on a mobile robot. Across simulation\nand real-world outdoor trials, the system detects semantic targets beyond 150m,\nmaintains a correct heading through visibility changes with 82.6% probability,\nand improves overall task success by 17.5% compared with the SOTA methods,\ndemonstrating robust ZSON toward distant and intermittently observable targets.", "AI": {"tldr": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u5c42\u6b21\u7ed3\u6784\u548c\u663e\u8457\u6027\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u8fdc\u8ddd\u79bb\u3001\u95f4\u6b47\u6027\u53ef\u89c1\u76ee\u6807\u7684\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u8fdc\u8ddd\u79bb\u3001\u6613\u88ab\u906e\u6321\u7684\u76ee\u6807\u5bfc\u822a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u9f50\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\u74e6\u7247\u5c42\u6b21\u7ed3\u6784\u7684\u7edf\u4e00\u3001\u8f7b\u91cf\u7ea7\u95ed\u73af\u7cfb\u7edf\u3002\u901a\u8fc7\u5206\u5c42\u76ee\u6807\u663e\u8457\u6027\u878d\u5408\uff0c\u5c06\u5c40\u90e8\u8bed\u4e49\u5bf9\u6bd4\u603b\u7ed3\u4e3a\u7a33\u5b9a\u7684\u533a\u57df\u663e\u8457\u6027\uff0c\u4ee5\u63d0\u4f9b\u76ee\u6807\u65b9\u5411\u548c\u53ef\u89c1\u6027\u6307\u793a\u3002\u8be5\u533a\u57df\u663e\u8457\u6027\u652f\u6301\u901a\u8fc7\u5173\u952e\u5e27\u8bb0\u5fc6\u3001\u663e\u8457\u6027\u52a0\u6743\u7684\u5386\u53f2\u822a\u5411\u878d\u5408\u548c\u4e34\u65f6\u4e0d\u53ef\u89c1\u671f\u95f4\u7684\u4e3b\u52a8\u641c\u7d22\u6765\u8fdb\u884c\u7684\u53ef\u89c1\u6027\u611f\u77e5\u822a\u5411\u7ef4\u6301\u3002", "result": "\u7cfb\u7edf\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6237\u5916\u8bd5\u9a8c\u4e2d\uff0c\u80fd\u591f\u68c0\u6d4b\u8d85\u8fc7150\u7c73\u7684\u76ee\u6807\uff0c\u5e76\u4ee582.6%\u7684\u6982\u7387\u5728\u53ef\u89c1\u6027\u53d8\u5316\u4e2d\u4fdd\u6301\u6b63\u786e\u7684\u822a\u5411\uff0c\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e8617.5%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u5bf9\u8fdc\u5904\u548c\u95f4\u6b47\u6027\u53ef\u89c1\u76ee\u6807\u7684\u96f6\u6837\u672c\u5bfc\u822a\u3002"}}
{"id": "2509.13763", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13763", "abs": "https://arxiv.org/abs/2509.13763", "authors": ["Zongxin Shen", "Yanyong Huang", "Bin Wang", "Jinyuan Chang", "Shiyu Liu", "Tianrui Li"], "title": "Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning", "comment": null, "summary": "Multi-view unsupervised feature selection (MUFS) has recently received\nincreasing attention for its promising ability in dimensionality reduction on\nmulti-view unlabeled data. Existing MUFS methods typically select\ndiscriminative features by capturing correlations between features and\nclustering labels. However, an important yet underexplored question remains:\n\\textit{Are such correlations sufficiently reliable to guide feature\nselection?} In this paper, we analyze MUFS from a causal perspective by\nintroducing a novel structural causal model, which reveals that existing\nmethods may select irrelevant features because they overlook spurious\ncorrelations caused by confounders. Building on this causal perspective, we\npropose a novel MUFS method called CAusal multi-view Unsupervised feature\nSelection leArning (CAUSA). Specifically, we first employ a generalized\nunsupervised spectral regression model that identifies informative features by\ncapturing dependencies between features and consensus clustering labels. We\nthen introduce a causal regularization module that can adaptively separate\nconfounders from multi-view data and simultaneously learn view-shared sample\nweights to balance confounder distributions, thereby mitigating spurious\ncorrelations. Thereafter, integrating both into a unified learning framework\nenables CAUSA to select causally informative features. Comprehensive\nexperiments demonstrate that CAUSA outperforms several state-of-the-art\nmethods. To our knowledge, this is the first in-depth study of causal\nmulti-view feature selection in the unsupervised setting.", "AI": {"tldr": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6df7\u6dc6\u56e0\u7d20\uff0c\u672c\u6587\u63d0\u51faCAUSA\u4ece\u56e0\u679c\u89d2\u5ea6\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709MUFS\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u4e0e\u805a\u7c7b\u6807\u7b7e\u7684\u76f8\u5173\u6027\u6765\u9009\u62e9\u7279\u5f81\uff0c\u4f46\u8fd9\u79cd\u76f8\u5173\u6027\u53ef\u80fd\u53d7\u6df7\u6dc6\u56e0\u7d20\u5f71\u54cd\uff0c\u5bfc\u81f4\u9009\u62e9\u4e0d\u76f8\u5173\u7684\u7279\u5f81\u3002", "method": "\u63d0\u51faCAUSA\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u5e7f\u4e49\u65e0\u76d1\u7763\u8c31\u56de\u5f52\u6a21\u578b\u8bc6\u522b\u4fe1\u606f\u7279\u5f81\uff0c\u7136\u540e\u5f15\u5165\u56e0\u679c\u6b63\u5219\u5316\u6a21\u5757\u5206\u79bb\u6df7\u6dc6\u56e0\u7d20\u5e76\u5b66\u4e60\u6837\u672c\u6743\u91cd\u4ee5\u5e73\u8861\u6df7\u6dc6\u56e0\u7d20\u5206\u5e03\uff0c\u6700\u540e\u5c06\u4e24\u8005\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\u8fdb\u884c\u56e0\u679c\u7279\u5f81\u9009\u62e9\u3002", "result": "CAUSA\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4ece\u56e0\u679c\u89d2\u5ea6\u6df1\u5165\u7814\u7a76\u4e86\u65e0\u76d1\u7763\u591a\u89c6\u56fe\u7279\u5f81\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86CAUSA\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5ffd\u7565\u6df7\u6dc6\u56e0\u7d20\u800c\u9009\u62e9\u4e0d\u76f8\u5173\u7279\u5f81\u7684\u95ee\u9898\u3002"}}
{"id": "2509.14106", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14106", "abs": "https://arxiv.org/abs/2509.14106", "authors": ["Yudong Li", "Yirui Cong", "Shimin Wang", "Martin Guay", "Jiuxiang Dong"], "title": "Asymptotic Boundedness of Distributed Set-Membership Filtering", "comment": null, "summary": "Asymptotic boundedness is a crucial property of Distributed Set-Membership\nFiltering (DSMFing) that prevents the unbounded growth of the set estimates\ncaused by the wrapping effect. However, this important property remains\nunderinvestigated, compared to its noise-free and stochastic-noise\ncounterparts, i.e., the convergence of Distributed Observers (DOs) and the\nbounded error covariance of Distributed Kalman Filters (DKFs). This paper\nstudies the asymptotic boundedness of DSMFing for linear discrete-time systems.\nA novel concept, termed the Collective Observation-Information Tower (COIT), is\nintroduced to characterize the fundamental relationship between the structure\nof graphs and the set estimates, which enables the boundedness analysis.\nLeveraging the COIT, an easily verifiable sufficient condition for the\nasymptotic boundedness of linear DSMFing is established. Surprisingly, the\nsufficient condition generalizes the well-known collective detectability\ncondition for DOs and DKFs; it links DSMFs to existing distributed estimation\nmethods and reveals the unique characteristic of DSMFs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u96c6\u5408\u6210\u5458\u8fc7\u6ee4\uff08DSMFing\uff09\u7684\u6e10\u8fd1\u6709\u754c\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u96c6\u4f53\u89c2\u6d4b-\u4fe1\u606f\u5854\u201d\uff08COIT\uff09\u7684\u65b0\u6982\u5ff5\u6765\u5206\u6790\u56fe\u7ed3\u6784\u4e0e\u96c6\u5408\u4f30\u8ba1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u5f97\u51fa\u4e86\u4e00\u4e2a\u6613\u4e8e\u9a8c\u8bc1\u7684\u5145\u5206\u6761\u4ef6\uff0c\u8be5\u6761\u4ef6\u63a8\u5e7f\u4e86DOs\u548cDKFs\u4e2d\u7684\u96c6\u4f53\u53ef\u68c0\u6d4b\u6027\u6761\u4ef6\uff0c\u5e76\u63ed\u793a\u4e86DSMFs\u7684\u72ec\u7279\u4e4b\u5904\u3002", "motivation": "DSMFing\u7684\u6e10\u8fd1\u6709\u754c\u6027\u5bf9\u4e8e\u9632\u6b62\u96c6\u5408\u4f30\u8ba1\u7531\u4e8e\u5305\u88c5\u6548\u5e94\u800c\u65e0\u754c\u589e\u957f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f8\u8f83\u4e8e\u5176\u65e0\u566a\u58f0\u548c\u968f\u673a\u566a\u58f0\u7684\u5bf9\u5e94\u7269\uff08DOs\u7684\u6536\u655b\u6027\u548cDKFs\u7684\u6709\u754c\u8bef\u5dee\u534f\u65b9\u5dee\uff09\uff0c\u8be5\u6027\u8d28\u7684\u7814??\u5c1a\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u96c6\u4f53\u89c2\u6d4b-\u4fe1\u606f\u5854\u201d\uff08COIT\uff09\u7684\u65b0\u6982\u5ff5\uff0c\u7528\u4e8e\u8868\u5f81\u56fe\u7ed3\u6784\u4e0e\u96c6\u5408\u4f30\u8ba1\u4e4b\u95f4\u7684\u57fa\u672c\u5173\u7cfb\uff0c\u4ee5\u5b9e\u73b0\u6709\u754c\u6027\u5206\u6790\u3002\u5229\u7528COIT\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u6613\u4e8e\u9a8c\u8bc1\u7684\u7ebf\u6027DSMFing\u6e10\u8fd1\u6709\u754c\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u96c6\u4f53\u89c2\u6d4b-\u4fe1\u606f\u5854\u201d\uff08COIT\uff09\u7684\u65b0\u6982\u5ff5\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u6613\u4e8e\u9a8c\u8bc1\u7684\u5145\u5206\u6761\u4ef6\uff0c\u8be5\u6761\u4ef6\u63a8\u5e7f\u4e86DOs\u548cDKFs\u4e2d\u7684\u96c6\u4f53\u53ef\u68c0\u6d4b\u6027\u6761\u4ef6\uff0c\u8fde\u63a5\u4e86DSMFs\u4e0e\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86DSMFs\u7684\u72ec\u7279\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u89e3\u51b3\u4e86\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684DSMFing\u6e10\u8fd1\u6709\u754c\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7684COIT\u6982\u5ff5\u548c\u5145\u5206\u6761\u4ef6\u4e0d\u4ec5\u4e3a\u5206\u6790\u8be5\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u8fd8\u7edf\u4e00\u4e86\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\u4e2d\u7684\u4e00\u4e9b\u5173\u952e\u6982\u5ff5\uff0c\u5e76\u7a81\u51fa\u4e86DSMFs\u7684\u72ec\u7279\u6027\u3002"}}
{"id": "2509.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13880", "abs": "https://arxiv.org/abs/2509.13880", "authors": ["Mingwei Zhang", "Zhenhao Gu", "Liangda Fang", "Cunjing Ge", "Ziliang Chen", "Zhao-Rong Lai", "Quanlong Guan"], "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques", "comment": null, "summary": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u6574\u6570\u7ebf\u6027\u7ea6\u675f\u6a21\u578b\u8ba1\u6570\uff08MCILC\uff09\u7684\u7cbe\u786e\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u6df7\u5408\u6574\u6570\u89c4\u5212\u7684\u7b80\u5316\u6280\u672f\uff0c\u5728\u968f\u673a\u548c\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bb8\u591a\u5e94\u7528\uff08\u5305\u62ec\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8fd0\u7b79\u5b66\u548c\u4f18\u5316\u9886\u57df\uff09\u90fd\u5f52\u7ed3\u4e3a\u6574\u6570\u7ebf\u6027\u7ea6\u675f\u6a21\u578b\u8ba1\u6570\uff08MCILC\uff09\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u7cbe\u786eMCILC\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u4e86\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e2d\u7684\u51e0\u79cd\u6709\u6548\u7b80\u5316\u6280\u672f\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684MCILC\u8ba1\u6570\u5668\u548c\u547d\u9898\u6a21\u578b\u8ba1\u6570\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u57282840\u4e2a\u968f\u673a\u57fa\u51c6\u6d4b\u8bd5\u548c4131\u4e2a\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5728\u968f\u673a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e861718\u4e2a\u5b9e\u4f8b\uff0c\u800c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4ec5\u89e3\u51b3\u4e861470\u4e2a\u5b9e\u4f8b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u662f\u552f\u4e00\u80fd\u591f\u89e3\u51b3\u6240\u67094131\u4e2a\u5e94\u7528\u5b9e\u4f8b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MCILC\u95ee\u9898\u4e0a\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u8f68\u8ff9\u9884\u6d4b\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u548c\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u90e8\u7f72\u65f6\u4f1a\u9047\u5230\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e4b\u95f4\u7684\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u7f55\u89c1\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4ea4\u901a\u573a\u666f\u4ea7\u751fOOD\uff08\u5206\u5e03\u5916\uff09\u60c5\u51b5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684OOD\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\uff0c\u800c\u8f68\u8ff9\u7ea7\u522b\u7684OOD\u68c0\u6d4b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u673a\u5236\u6765\u5e94\u5bf9\u590d\u6742\u7684\u9a7e\u9a76\u73af\u5883\u3002\u5b83\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u5373\u4f7f\u662f\u5206\u5e03\u5185\u6837\u672c\u4e5f\u5b58\u5728\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u3001\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u7684\u52a8\u6001\u6a21\u5f0f\u7684\u9884\u6d4b\u8bef\u5dee\u95ee\u9898\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8bef\u5dee\u6a21\u5f0f\uff0c\u663e\u8457\u6539\u5584\u4e86\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u3002\u5728\u8f68\u8ff9\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u548c\u57fa\u4e8e\u89c6\u89c9\u7684OOD\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u5b9e\u73b0\u53ef\u9760\u7684\u3001\u611f\u77e5\u9a7e\u9a76\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u5176\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684OOD\u68c0\u6d4b\u95ee\u9898\u3002"}}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCER\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u4ee5\u89e3\u51b3\u533b\u7597\u9886\u57df\u9519\u8bef\u4fe1\u606f\u7684\u6311\u6218\u3002", "motivation": "\u533b\u7597\u9886\u57df\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u3001\u672a\u7ecf\u8bc1\u5b9e\u7684\u7597\u6cd5\uff09\u5bf9\u516c\u4f17\u5065\u5eb7\u548c\u5bf9\u533b\u7597\u7cfb\u7edf\u7684\u4fe1\u4efb\u6784\u6210\u5a01\u80c1\u3002\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u9762\u4e34\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u672f\u8bed\u3001\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u53ca\u4f9d\u636e\u79d1\u5b66\u8bc1\u636e\u7684\u91cd\u8981\u6027\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u4e86\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u53ca\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\u3002\u5b83\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u548c\u5148\u8fdb\u7684\u68c0\u7d22\u6280\u672f\uff0c\u7528\u4e8e\u68c0\u7d22\u9ad8\u8d28\u91cf\u7684\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\uff0c\u4ece\u800c\u6709\u6548\u964d\u4f4e\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u98ce\u9669\uff0c\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCER\u6846\u67b6\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u786e\u4fdd\u4e86\u7ed3\u679c\u7684\u53ef\u9a8c\u8bc1\u6027\u3002"}}
{"id": "2509.13862", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13862", "abs": "https://arxiv.org/abs/2509.13862", "authors": ["Yunpeng Zi", "Muchun Yang", "D. L. Zhou"], "title": "Quantum Algorithm of the GLMY Homology on Digraphs", "comment": "11 pages, 2 figures", "summary": "Quantum algorithms for topological data analysis provide significant\nadvantage over the best classical algorithm. Different from the previous\nsimplical complex on points cloud, the GLMY homology introduced by Alexander\nGrigor'yan, Yong Lin, Yuri Muranov and Shing-Tung Yau, is defined on digraph\nand is a arising realm in Topological Data Analysis (TDA), which attracts more\nand more attention recently. We propose a quantum algorithm for the GLMY\nhomology with significant advantage over the best classical algorithm. We\ndesign a universal encoding protocol for the quantum states and boundary\noperators of GLMY homology on digraphs. And a property of the GLMY homology is\nproved for the theoretical guarantee of the quantum algorithm. The quantum\nalgorithm for GLMY homology gives a quadratic speedup in general cases, and it\ngives an exponential quantum advantage in the case of the input data is given\nas a specification of paths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\u7684 GLMY \u540c\u8c03\u7fa4\uff0c\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e8c\u6b21\u52a0\u901f\uff0c\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6307\u6570\u52a0\u901f\u3002", "motivation": "\u4e4b\u524d\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5355\u7eaf\u590d\u5f62\uff0c\u800c GLMY \u540c\u8c03\u7fa4\u662f\u5728\u6709\u5411\u56fe\u4e0a\u5b9a\u4e49\u7684\uff0c\u5e76\u4e14\u662f\u62d3\u6251\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u5174\u65b9\u5411\uff0c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e GLMY \u540c\u8c03\u7fa4\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e86 GLMY \u540c\u8c03\u7fa4\u5728\u6709\u5411\u56fe\u4e0a\u7684\u91cf\u5b50\u6001\u548c\u8fb9\u754c\u7b97\u5b50\u7684\u901a\u7528\u7f16\u7801\u534f\u8bae\uff0c\u5e76\u8bc1\u660e\u4e86\u4e00\u4e2a GLMY \u540c\u8c03\u7fa4\u7684\u6027\u8d28\u4ee5\u63d0\u4f9b\u91cf\u5b50\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u8be5\u91cf\u5b50\u7b97\u6cd5\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e8c\u6b21\u52a0\u901f\uff0c\u5728\u8f93\u5165\u6570\u636e\u4e3a\u8def\u5f84\u89c4\u8303\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6307\u6570\u91cf\u5b50\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u91cf\u5b50\u7b97\u6cd5\u5728\u8ba1\u7b97 GLMY \u540c\u8c03\u7fa4\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u4e14\u5728\u7406\u8bba\u4e0a\u5f97\u5230\u4e86\u4fdd\u8bc1\u3002"}}
{"id": "2509.14207", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.14207", "abs": "https://arxiv.org/abs/2509.14207", "authors": ["Sayan Banik", "Ashis K. Nandy"], "title": "Skyrmion-Antiskyrmion Lattice: A Net-Zero Topological Phase in Low-Symmetry Frustrated Chiral Magnets", "comment": null, "summary": "We report the discovery of a thermodynamically stable skyrmion-antiskyrmion\nlattice in two-dimensional heterostructures, a novel state exhibiting a\nnet-zero global topological charge owing to an equal population of skyrmions\nand antiskyrmions. This surprising coexistence of oppositely charged solitons\nremarkably circumvents their anticipated annihilation. We demonstrate the\nformation and evolution of this phase in Fe films on C1v -symmetric (110)\nsurfaces of GaAs and CdTe semiconductors. Specifically, we reveal a series of\nmagnetic field-induced phase transitions: cycloidal spin-spiral to\nskyrmion-antiskyrmion lattice to conical spin-spiral to ferromagnet. The\nremarkable stability of the net-zero lattice is attributed to symmetry-enforced\nanisotropic magnetic interactions. Lowering interfacial symmetry to C1v thus\nenables frustrated chiral magnets, uniquely manifesting in thermodynamically\nstable net-zero topological soliton lattices, as revealed by our findings.", "AI": {"tldr": "\u5728\u4e8c\u7ef4\u5f02\u8d28\u7ed3\u6784\u4e2d\u53d1\u73b0\u4e86\u70ed\u529b\u5b66\u7a33\u5b9a\u7684 Skyrmion-antiskyrmion \u6676\u683c\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u72b6\u6001\uff0c\u7531\u4e8e Skyrmion \u548c antiskyrmion \u7684\u6570\u91cf\u76f8\u7b49\uff0c\u56e0\u6b64\u5177\u6709\u51c0\u96f6\u5168\u5c40\u62d3\u6251\u7535\u8377\u3002", "motivation": "\u63a2\u7d22\u5177\u6709\u51c0\u96f6\u5168\u5c40\u62d3\u6251\u7535\u8377\u7684\u65b0\u578b Skyrmion \u72b6\u6001\uff0c\u5e76\u89e3\u91ca\u5176\u5f62\u6210\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5728 GaAs \u548c CdTe \u534a\u5bfc\u4f53\u7684 C1v \u5bf9\u79f0 (110) \u8868\u9762\u4e0a\u7684 Fe \u819c\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u7814\u7a76\u78c1\u573a\u5f15\u8d77\u7684\u76f8\u53d8\u3002", "result": "\u89c2\u5bdf\u5230\u4ece\u73af\u72b6\u81ea\u65cb\u87ba\u65cb\u5230 Skyrmion-antiskyrmion \u6676\u683c\u518d\u5230\u9525\u5f62\u81ea\u65cb\u87ba\u65cb\u518d\u5230\u94c1\u78c1\u4f53\u7684\u78c1\u573a\u8bf1\u5bfc\u76f8\u53d8\u3002\u8bc1\u660e\u4e86 C1v \u4f4e\u5bf9\u79f0\u6027\u662f\u5f62\u6210\u70ed\u529b\u5b66\u7a33\u5b9a\u51c0\u96f6\u62d3\u6251\u5b64\u5b50\u6676\u683c\u7684\u5173\u952e\u3002", "conclusion": "\u901a\u8fc7\u964d\u4f4e\u754c\u9762\u5bf9\u79f0\u6027\u81f3 C1v\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53cd\u5e38\u78c1\u4f53\u7684\u72ec\u7279\u8868\u73b0\uff0c\u5373\u70ed\u529b\u5b66\u7a33\u5b9a\u7684\u51c0\u96f6\u62d3\u6251\u5b64\u5b50\u6676\u683c\u3002"}}
{"id": "2509.13731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13731", "abs": "https://arxiv.org/abs/2509.13731", "authors": ["Jeongwoo Park", "Seabin Lee", "Changmin Park", "Wonjong Lee", "Changjoo Nam"], "title": "Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings", "comment": null, "summary": "The industrial insertion of flexible flat cables (FFCs) into receptacles\npresents a significant challenge owing to the need for submillimeter precision\nwhen handling the deformable cables. In manufacturing processes, FFC insertion\nwith robotic manipulators often requires laborious human-guided trajectory\ngeneration. While Reinforcement Learning (RL) offers a solution to automate\nthis task without modeling complex properties of FFCs, the nondeterminism\ncaused by the deformability of FFCs requires significant efforts and time on\ntraining. Moreover, training directly in a real environment is dangerous as\nindustrial robots move fast and possess no safety measure. We propose an RL\nalgorithm for FFC insertion that leverages a foundation model-based real-to-sim\napproach to reduce the training time and eliminate the risk of physical damages\nto robots and surroundings. Training is done entirely in simulation, allowing\nfor random exploration without the risk of physical damages. Sim-to-real\ntransfer is achieved through semantic segmentation masks which leave only those\nvisual features relevant to the insertion tasks such as the geometric and\nspatial information of the cables and receptacles. To enhance generality, we\nuse a foundation model, Segment Anything Model 2 (SAM2). To eleminate human\nintervention, we employ a Vision-Language Model (VLM) to automate the initial\nprompting of SAM2 to find segmentation masks. In the experiments, our method\nexhibits zero-shot capabilities, which enable direct deployments to real\nenvironments without fine-tuning.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408SAM2\u548cVLM\uff0c\u81ea\u52a8\u5316\u4e86\u67d4\u6027\u6241\u5e73\u7535\u7f06\uff08FFC\uff09\u7684\u63d2\u5165\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u98ce\u9669\u3002", "motivation": "\u67d4\u6027\u6241\u5e73\u7535\u7f06\uff08FFC\uff09\u7684\u63d2\u5165\u9700\u8981\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u7ed9\u673a\u5668\u4eba\u64cd\u4f5c\u5e26\u6765\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8017\u65f6\u7684\u624b\u5de5\u8f68\u8ff9\u751f\u6210\u6216RL\u8bad\u7ec3\uff0c\u4e14\u5728\u7269\u7406\u73af\u5883\u4e2d\u8bad\u7ec3\u5b58\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u4eff\u771f\u5230\u73b0\u5b9e\uff08sim-to-real\uff09\u65b9\u6cd5\u3002\u901a\u8fc7SAM2\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u63d0\u53d6\u5173\u952e\u7684\u51e0\u4f55\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528VLM\u81ea\u52a8\u751f\u6210SAM2\u7684\u63d0\u793a\u3002\u6240\u6709\u8bad\u7ec3\u5728\u4eff\u771f\u73af\u5883\u4e2d\u5b8c\u6210\uff0c\u4ee5\u907f\u514d\u7269\u7406\u635f\u574f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u96f6\u6837\u672c\uff08zero-shot\uff09\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u73af\u5883\uff0c\u6210\u529f\u5b9e\u73b0\u4e86FFC\u7684\u81ea\u52a8\u5316\u63d2\u5165\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684RL\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3FFC\u63d2\u5165\u7684\u6311\u6218\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u6d88\u9664\u7269\u7406\u98ce\u9669\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2509.13783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13783", "abs": "https://arxiv.org/abs/2509.13783", "authors": ["Tianshuo Zhang", "Wenzhe Zhai", "Rui Yann", "Jia Gao", "He Cao", "Xianglei Xing"], "title": "Floating-Body Hydrodynamic Neural Networks", "comment": null, "summary": "Fluid-structure interaction is common in engineering and natural systems,\nwhere floating-body motion is governed by added mass, drag, and background\nflows. Modeling these dissipative dynamics is difficult: black-box neural\nmodels regress state derivatives with limited interpretability and unstable\nlong-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks\n(FHNN), a physics-structured framework that predicts interpretable hydrodynamic\nparameters such as directional added masses, drag coefficients, and a\nstreamfunction-based flow, and couples them with analytic equations of motion.\nThis design constrains the hypothesis space, enhances interpretability, and\nstabilizes integration. On synthetic vortex datasets, FHNN achieves up to an\norder-of-magnitude lower error than Neural ODEs, recovers physically consistent\nflow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN\nmore effectively handles dissipative dynamics while preserving\ninterpretability, which bridges the gap between black-box learning and\ntransparent system identification.", "AI": {"tldr": "FHNN\u662f\u4e00\u79cd\u7269\u7406\u7ed3\u6784\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u53ef\u89e3\u91ca\u7684\u6c34\u52a8\u529b\u5b66\u53c2\u6570\uff08\u5982\u65b9\u5411\u6027\u9644\u52a0\u8d28\u91cf\u3001\u963b\u529b\u7cfb\u6570\u548c\u57fa\u4e8e\u6d41\u51fd\u6570\u7684\u6d41\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u8fd0\u52a8\u7684\u89e3\u6790\u65b9\u7a0b\u8026\u5408\uff0c\u4ece\u800c\u5728\u5408\u6210\u6da1\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4Neural ODE\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u8bef\u5dee\uff0c\u5e76\u6062\u590d\u4e86\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u6d41\u573a\u3002", "motivation": "\u5bf9\u6d6e\u4f53\u8fd0\u52a8\u4e2d\u7684\u8017\u6563\u52a8\u529b\u5b66\u8fdb\u884c\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u7684\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u6709\u9650\u4e14\u957f\u671f\u9884\u6d4b\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFHNN\uff08Floating-Body Hydrodynamic Neural Networks\uff09\u7684\u7269\u7406\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u53ef\u89e3\u91ca\u7684\u6c34\u52a8\u529b\u5b66\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u4e0e\u8fd0\u52a8\u7684\u89e3\u6790\u65b9\u7a0b\u8026\u5408\u3002", "result": "\u5728\u5408\u6210\u6da1\u6d41\u6570\u636e\u96c6\u4e0a\uff0cFHNN\u7684\u8bef\u5dee\u6bd4Neural ODE\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u6062\u590d\u4e86\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u6d41\u573a\u3002", "conclusion": "FHNN\u5728\u5904\u7406\u8017\u6563\u52a8\u529b\u5b66\u65b9\u9762\u6bd4\u4f20\u7edf\u7684Hamiltonian\u548cLagrangian\u795e\u7ecf\u7f51\u7edc\u66f4\u6709\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5f25\u5408\u4e86\u9ed1\u76d2\u5b66\u4e60\u4e0e\u900f\u660e\u7cfb\u7edf\u8bc6\u522b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.14121", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14121", "abs": "https://arxiv.org/abs/2509.14121", "authors": ["Marco A. Gomez", "Christopher D. Cruz-Ancona"], "title": "Safe Sliding Mode Control in Position for Double Integrator Systems", "comment": "6 pages, 3 figures, accepted at 2025 22 th International Conference\n  on Electrical Engineering, Computing Science and Automatic Control", "summary": "We address the problem of robust safety control design for double integrator\nsystems. We show that, when the constraints are defined only on position\nstates, it is possible to construct a safe sliding domain from the dynamic of a\nsimple integrator that is already safe. On this domain, the closed-loop\ntrajectories remain robust and safe against uncertainties and disturbances.\nFurthermore, we design a controller gain that guarantees convergence to the\nsafe sliding domain while avoiding the given unsafe set. The concept is\ninitially developed for first-order sliding mode and is subsequently\ngeneralized to an adaptive framework, ensuring that trajectories remain\nconfined to a predefined vicinity of the sliding domain, outside the unsafe\nregion.", "AI": {"tldr": "\u5f53\u7ea6\u675f\u4ec5\u5728\u4f4d\u7f6e\u72b6\u6001\u4e0a\u5b9a\u4e49\u65f6\uff0c\u53ef\u4ee5\u4ece\u5df2\u6709\u7684\u5b89\u5168\u79ef\u5206\u5668\u52a8\u529b\u5b66\u6784\u5efa\u4e00\u4e2a\u5b89\u5168\u7684\u6ed1\u52a8\u57df\uff0c\u4ee5\u5b9e\u73b0\u53cc\u79ef\u5206\u5668\u7cfb\u7edf\u7684\u9c81\u68d2\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u8bbe\u8ba1\u53cc\u79ef\u5206\u5668\u7cfb\u7edf\u7684\u9c81\u68d2\u5b89\u5168\u63a7\u5236\u5668\uff0c\u7279\u522b\u662f\u5728\u7ea6\u675f\u4ec5\u9650\u4e8e\u4f4d\u7f6e\u72b6\u6001\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528\u4e00\u9636\u79ef\u5206\u5668\u7684\u5b89\u5168\u52a8\u529b\u5b66\u6765\u6784\u5efa\u4e00\u4e2a\u5b89\u5168\u7684\u6ed1\u52a8\u57df\uff0c\u5e76\u5728\u6b64\u57df\u4e0a\u8bbe\u8ba1\u4e00\u4e2a\u63a7\u5236\u5668\u589e\u76ca\uff0c\u4ee5\u786e\u4fdd\u95ed\u73af\u8f68\u8ff9\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u4e0b\u4fdd\u6301\u9c81\u68d2\u548c\u5b89\u5168\uff0c\u540c\u65f6\u907f\u514d\u7ed9\u5b9a\u7684\u4e0d\u5b89\u5168\u96c6\u5408\u3002\u8be5\u65b9\u6cd5\u4ece\u4e00\u9636\u6ed1\u6a21\u6982\u5ff5\u5f00\u59cb\uff0c\u5e76\u63a8\u5e7f\u5230\u81ea\u9002\u5e94\u6846\u67b6\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b89\u5168\u7684\u6ed1\u52a8\u57df\uff0c\u5e76\u786e\u5b9a\u4e86\u4e00\u4e2a\u63a7\u5236\u5668\u589e\u76ca\uff0c\u8be5\u589e\u76ca\u53ef\u5c06\u8f68\u8ff9\u6536\u655b\u5230\u8be5\u6ed1\u52a8\u57df\uff0c\u5e76\u4f7f\u5176\u4fdd\u6301\u5728\u4e0d\u5b89\u5168\u533a\u57df\u4e4b\u5916\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u53cc\u79ef\u5206\u5668\u7cfb\u7edf\u8bbe\u8ba1\u9c81\u68d2\u7684\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5373\u4f7f\u5728\u53ea\u6709\u4f4d\u7f6e\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u8fd0\u884c\uff0c\u5e76\u80fd\u4fdd\u8bc1\u7cfb\u7edf\u8f68\u8ff9\u5728\u5b89\u5168\u8303\u56f4\u5185\u3002"}}
{"id": "2509.13968", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13968", "abs": "https://arxiv.org/abs/2509.13968", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.", "AI": {"tldr": "\u8ba4\u77e5\u53ef\u80fd\u901a\u8fc7\u64cd\u7eb5\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u548c\u4fe1\u606f\u6d41\u7684\u91cd\u5927\u8f6c\u53d8\u800c\u6f14\u53d8\u3002\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u6a21\u578b\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u524d\u9988\u3001\u9012\u5f52\u548c\u5c42\u72b6\u7f51\u7edc\u5728\u5b66\u4e60\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u8bed\u6cd5\u65b9\u9762\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9012\u5f52\u7f51\u7edc\u5728\u5904\u7406\u8f93\u5165\u7c7b\u578b\u548c\u5b66\u4e60\u590d\u6742\u8bed\u6cd5\u65b9\u9762\u6bd4\u524d\u9988\u7f51\u7edc\u6709\u5b9a\u6027\u6269\u5c55\uff0c\u5176\u8bad\u7ec3\u96be\u5ea6\u6784\u6210\u4e86\u8fc7\u6e21\u969c\u788d\u548c\u5076\u53d1\u6027\u4e0d\u53ef\u9006\u6027\u3002\u5c42\u72b6\u7f51\u7edc\u5728\u8bed\u6cd5\u5b66\u4e60\u65b9\u9762\u5e76\u672a\u8868\u73b0\u51fa\u4f18\u52bf\u3002\u603b\u4e4b\uff0c\u4fe1\u606f\u6d41\u7684\u67d0\u4e9b\u53d8\u5316\u53ef\u4ee5\u4ea7\u751f\u8ba4\u77e5\u7ee9\u6548\u7684\u8f6c\u53d8\u3002", "motivation": "\u63a2\u8ba8\u8ba4\u77e5\u662f\u5426\u50cf\u751f\u7269\u8fdb\u5316\u4e00\u6837\uff0c\u901a\u8fc7\u6539\u53d8\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u548c\u4fe1\u606f\u6d41\u7684\u91cd\u5927\u8f6c\u53d8\u800c\u6f14\u53d8\u3002", "method": "\u4f7f\u7528\u524d\u9988\u3001\u9012\u5f52\u548c\u5c42\u72b6\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u6a21\u578b\uff0c\u5728\u63a7\u5236\u7f51\u7edc\u5927\u5c0f\u548c\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u8bd5\u5b83\u4eec\u5b66\u4e60\u4e0d\u540c\u590d\u6742\u5ea6\u4eba\u5de5\u8bed\u6cd5\u7684\u80fd\u529b\u3002", "result": "\u9012\u5f52\u7f51\u7edc\u5728\u5904\u7406\u8f93\u5165\u7c7b\u578b\u548c\u5b66\u4e60\u590d\u6742\u8bed\u6cd5\u65b9\u9762\u6bd4\u524d\u9988\u7f51\u7edc\u8868\u73b0\u51fa\u5b9a\u6027\u6269\u5c55\uff1b\u9012\u5f52\u7f51\u7edc\u7684\u8bad\u7ec3\u96be\u5ea6\u6784\u6210\u8fc7\u6e21\u969c\u788d\u548c\u5076\u53d1\u6027\u4e0d\u53ef\u9006\u6027\uff1b\u5c42\u72b6\u7f51\u7edc\u5728\u8bed\u6cd5\u5b66\u4e60\u65b9\u9762\u6ca1\u6709\u8868\u73b0\u51fa\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u4fe1\u606f\u6d41\u7684\u67d0\u4e9b\u53d8\u5316\u53ef\u4ee5\u4ea7\u751f\u8ba4\u77e5\u7ee9\u6548\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u536b\u661f\u56fe\u50cf\u5bf9\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u4e9a\u9a6c\u900a\u96e8\u6797\u68ee\u6797\u780d\u4f10\u7684\u65b9\u6cd5\uff0c\u5e76\u8f85\u4ee5\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u8fdb\u884c\u53d8\u5316\u6ce8\u91ca\u3002", "motivation": "\u4e9a\u9a6c\u900a\u96e8\u6797\u68ee\u6797\u780d\u4f10\u5bf9\u5168\u7403\u78b3\u6392\u653e\u548c\u751f\u7269\u591a\u6837\u6027\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u56e0\u6b64\u76d1\u6d4b\u68ee\u6797\u780d\u4f10\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6bd4\u8f83\u4e0d\u540c\u65e5\u671f\u7684\u540c\u4e00\u533a\u57df\u7684\u536b\u661f\u56fe\u50cf\uff0c\u4ee5\u8bc6\u522b\u68ee\u6797\u8986\u76d6\u7684\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u6765\u81ea\u52a8\u6ce8\u91ca\u68c0\u6d4b\u5230\u7684\u53d8\u5316\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u68c0\u6d4b\u68ee\u6797\u780d\u4f10\u548c\u751f\u6210\u76f8\u5173\u6ce8\u91ca\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u548c\u7814\u7a76\u4e9a\u9a6c\u900a\u68ee\u6797\u780d\u4f10\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u7528\u7684\u5de5\u5177\uff0c\u5e76\u4e14\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "CER\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\u7684\u65b0\u578b\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u80fd\u529b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u5e7b\u89c9\u98ce\u9669\uff0c\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u3001\u6709\u636e\u53ef\u67e5\u7684\u6765\u6e90\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u9519\u8bef\u4fe1\u606f\uff0c\u4ece\u75ab\u82d7\u72b9\u8c6b\u5230\u672a\u7ecf\u8bc1\u5b9e\u7684\u7597\u6cd5\uff0c\u5bf9\u516c\u4f17\u5065\u5eb7\u548c\u5bf9\u533b\u7597\u7cfb\u7edf\u7684\u4fe1\u4efb\u6784\u6210\u4e86\u5a01\u80c1\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u672f\u8bed\u590d\u6742\u3001\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u53ca\u5fc5\u987b\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\uff0c\u9a8c\u8bc1\u751f\u7269\u533b\u5b66\u58f0\u660e\u4ecd\u7136\u5177\u6709\u72ec\u7279\u7684\u6311\u6218\u6027\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u4e86\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u4ee5\u53ca\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCER\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u5f0f\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13904", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.13904", "abs": "https://arxiv.org/abs/2509.13904", "authors": ["Matthias Kolb", "Thomas Spielauer", "Thomas Weigner", "Giovanni Boero", "Dennis R\u00e4tzel", "Philipp Haslinger"], "title": "Coherent Driving of a Quantum System with Modulated Free-Space Electrons", "comment": "18 pages, 13 figures", "summary": "Control of quantum systems typically relies on the interaction with\nelectromagnetic radiation. In this study, we experimentally show that the\nelectromagnetic near-field of a spatially modulated freespace electron beam can\nbe used to drive spin systems, demonstrating free-electron-bound-electron\nresonant interaction. By periodically deflecting the electron beam of a\nscanning electron microscope in close proximity to a spin-active solid-state\nsample, and sweeping the deflection frequency across the spin resonance, we\ndirectly observe phase coherent coupling between the electron beam's nearfield\nand the two spin states. This method relies only on classically shaping the\nelectron beams transversal correlations and has the potential to enable\ncoherent control of quantum systems with unprecedented, electron microscopic\nresolution, opening novel possibilities for advanced spectroscopic tools in\nnanotechnology.", "AI": {"tldr": "\u901a\u8fc7\u64cd\u7eb5\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u7684\u7535\u5b50\u675f\uff0c\u53ef\u4ee5\u4ee5\u7eb3\u7c73\u7ea7\u5206\u8fa8\u7387\u76f8\u5e72\u5730\u63a7\u5236\u91cf\u5b50\u81ea\u65cb\u7cfb\u7edf\u3002", "motivation": "\u4ee5\u5f80\u63a7\u5236\u91cf\u5b50\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u7535\u78c1\u8f90\u5c04\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u81ea\u7531\u7535\u5b50\u7684\u8fd1\u573a\u4e0e\u91cf\u5b50\u81ea\u65cb\u7cfb\u7edf\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u76f8\u5e72\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u5728\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\u4e2d\uff0c\u4f7f\u7535\u5b50\u675f\u5728\u63a5\u8fd1\u81ea\u65cb\u6d3b\u6027\u56fa\u6001\u6837\u54c1\u65f6\u53d1\u751f\u5468\u671f\u6027\u504f\u8f6c\uff0c\u5e76\u626b\u63cf\u504f\u8f6c\u9891\u7387\u4ee5\u5339\u914d\u81ea\u65cb\u5171\u632f\u9891\u7387\uff0c\u4ece\u800c\u89c2\u5bdf\u7535\u5b50\u675f\u8fd1\u573a\u4e0e\u81ea\u65cb\u4e4b\u95f4\u7684\u76f8\u5e72\u8026\u5408\u3002", "result": "\u5b9e\u9a8c\u76f4\u63a5\u89c2\u5bdf\u5230\u4e86\u7535\u5b50\u675f\u8fd1\u573a\u4e0e\u81ea\u65cb\u4e24\u79cd\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u5e72\u8026\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u7ecf\u5178\u5730\u5851\u9020\u7535\u5b50\u675f\u7684\u6a2a\u5411\u5173\u8054\uff0c\u6709\u6f5c\u529b\u4ee5\u7a7a\u524d\u7684\u7535\u5b50\u663e\u5fae\u955c\u5206\u8fa8\u7387\u5b9e\u73b0\u91cf\u5b50\u7cfb\u7edf\u7684\u76f8\u5e72\u63a7\u5236\uff0c\u4e3a\u7eb3\u7c73\u6280\u672f\u4e2d\u7684\u5148\u8fdb\u5149\u8c31\u5de5\u5177\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.13733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13733", "abs": "https://arxiv.org/abs/2509.13733", "authors": ["Xiaolin Zhou", "Tingyang Xiao", "Liu Liu", "Yucheng Wang", "Maiyue Chen", "Xinrui Meng", "Xinjie Wang", "Wei Feng", "Wei Sui", "Zhizhong Su"], "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph", "comment": "8 pages", "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic\nsystems, with broad applications for the deployment of embodied agents in\nreal-world environments. Despite recent advances, existing approaches are\nlimited in long-range spatial reasoning, often exhibiting low success rates and\nhigh inference latency, particularly in long-range navigation tasks. To address\nthese limitations, we propose FSR-VLN, a vision-language navigation system that\ncombines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow\nNavigation Reasoning (FSR). The HMSG provides a multi-modal map representation\nsupporting progressive retrieval, from coarse room-level localization to\nfine-grained goal view and object identification. Building on HMSG, FSR first\nperforms fast matching to efficiently select candidate rooms, views, and\nobjects, then applies VLM-driven refinement for final goal selection. We\nevaluated FSR-VLN across four comprehensive indoor datasets collected by\nhumanoid robots, utilizing 87 instructions that encompass a diverse range of\nobject categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all\ndatasets, measured by the retrieval success rate (RSR), while reducing the\nresponse time by 82% compared to VLM-based methods on tour videos by activating\nslow reasoning only when fast intuition fails. Furthermore, we integrate\nFSR-VLN with speech interaction, planning, and control modules on a Unitree-G1\nhumanoid robot, enabling natural language interaction and real-time navigation.", "AI": {"tldr": "FSR-VLN\u7ed3\u5408\u4e86\u5206\u5c42\u591a\u6a21\u6001\u573a\u666f\u56fe\uff08HMSG\uff09\u548c\u5feb\u6162\u5bfc\u822a\u63a8\u7406\uff08FSR\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u7cfb\u7edf\u5728\u957f\u8ddd\u79bb\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u5bfc\u822a\u65b9\u9762\u5b58\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3001\u6210\u529f\u7387\u4f4e\u548c\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFSR-VLN\u7cfb\u7edf\uff0c\u5229\u7528HMSG\u63d0\u4f9b\u591a\u6a21\u6001\u5730\u56fe\u8868\u793a\uff0c\u652f\u6301\u4ece\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u68c0\u7d22\u3002FSR\u9996\u5148\u8fdb\u884c\u5feb\u901f\u5339\u914d\u4ee5\u9009\u62e9\u5019\u9009\u5bf9\u8c61\uff0c\u7136\u540e\u901a\u8fc7VLM\u9a71\u52a8\u7684\u7cbe\u70bc\u8fdb\u884c\u6700\u7ec8\u76ee\u6807\u9009\u62e9\u3002FSR-VLN\u5728\u6a21\u578b\u4e2d\u96c6\u6210\u4e86\u8bed\u97f3\u4ea4\u4e92\u3001\u89c4\u5212\u548c\u63a7\u5236\u6a21\u5757\u3002", "result": "FSR-VLN\u5728\u56db\u4e2a\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\uff0c\u68c0\u7d22\u6210\u529f\u7387\uff08RSR\uff09\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u54cd\u5e94\u65f6\u95f4\u51cf\u5c11\u4e8682%\u3002", "conclusion": "FSR-VLN\u5728\u63d0\u9ad8VLN\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5e76\u6210\u529f\u96c6\u6210\u5230\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5b9e\u65f6\u5bfc\u822a\u3002"}}
{"id": "2509.13805", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.13805", "abs": "https://arxiv.org/abs/2509.13805", "authors": ["Florian Wiesner", "Matthias Wessling", "Stephen Baek"], "title": "Towards a Physics Foundation Model", "comment": null, "summary": "Foundation models have revolutionized natural language processing through a\n``train once, deploy anywhere'' paradigm, where a single pre-trained model\nadapts to countless downstream tasks without retraining. Access to a Physics\nFoundation Model (PFM) would be transformative -- democratizing access to\nhigh-fidelity simulations, accelerating scientific discovery, and eliminating\nthe need for specialized solver development. Yet current physics-aware machine\nlearning approaches remain fundamentally limited to single, narrow domains and\nrequire retraining for each new system. We present the General Physics\nTransformer (GPhyT), trained on 1.8 TB of diverse simulation data, that\ndemonstrates foundation model capabilities are achievable for physics. Our key\ninsight is that transformers can learn to infer governing dynamics from\ncontext, enabling a single model to simulate fluid-solid interactions, shock\nwaves, thermal convection, and multi-phase dynamics without being told the\nunderlying equations. GPhyT achieves three critical breakthroughs: (1) superior\nperformance across multiple physics domains, outperforming specialized\narchitectures by up to 29x, (2) zero-shot generalization to entirely unseen\nphysical systems through in-context learning, and (3) stable long-term\npredictions through 50-timestep rollouts. By establishing that a single model\ncan learn generalizable physical principles from data alone, this work opens\nthe path toward a universal PFM that could transform computational science and\nengineering.", "AI": {"tldr": "Transformer\u6a21\u578bGPhyT\u5728\u7269\u7406\u5b66\u9886\u57df\u5b9e\u73b0\u4e86\u201c\u4e00\u6b21\u8bad\u7ec3\uff0c\u968f\u5904\u90e8\u7f72\u201d\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6a21\u62df\u591a\u79cd\u7269\u7406\u73b0\u8c61\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u7684\u7269\u7406\u57fa\u7840\u6a21\u578b\uff08PFM\uff09\uff0c\u4ee5\u666e\u53ca\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\uff0c\u5e76\u51cf\u5c11\u5bf9\u4e13\u4e1a\u6c42\u89e3\u5668\u5f00\u53d1\u7684\u9700\u6c42\u3002\u76ee\u524d\u7684\u7269\u7406\u611f\u77e5\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ec5\u9650\u4e8e\u72ed\u7a84\u7684\u9886\u57df\uff0c\u5e76\u4e14\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u7cfb\u7edf\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7269\u7406Transformer\uff08GPhyT\uff09\uff0c\u8be5\u6a21\u578b\u57281.8 TB\u7684\u591a\u6837\u5316\u6a21\u62df\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528Transformer\u63a8\u65ad\u52a8\u529b\u5b66\u65b9\u7a0b\u3002", "result": "GPhyT\u5728\u591a\u4e2a\u7269\u7406\u9886\u57df\u53d6\u5f97\u4e86\u4f18\u4e8e\u4e13\u95e8\u67b6\u6784\u591a\u8fbe29\u500d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u7406\u7cfb\u7edf\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u8fd8\u80fd\u901a\u8fc750\u4e2a\u65f6\u95f4\u6b65\u7684\u6eda\u52a8\u9884\u6d4b\u5b9e\u73b0\u957f\u671f\u7a33\u5b9a\u9884\u6d4b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5355\u4e2a\u6a21\u578b\u4ec5\u901a\u8fc7\u6570\u636e\u5373\u53ef\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7269\u7406\u539f\u7406\uff0c\u4e3a\u5b9e\u73b0\u80fd\u591f\u53d8\u9769\u8ba1\u7b97\u79d1\u5b66\u548c\u5de5\u7a0b\u7684\u901a\u7528PFM\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.14168", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14168", "abs": "https://arxiv.org/abs/2509.14168", "authors": ["Walden Marshall"], "title": "Factored Output Feedback Controller Synthesis with Locality Constraints for Spatially-Invariant Systems", "comment": "9 pages, 2 figures", "summary": "We consider H2 output feedback controller synthesis with pre-specified\nconstraints on spatial communication distance (locality) for\nspatially-invariant systems using two factored controller frameworks: the\nsystem-level parameterization and the input-output parameterization. In our\nmain result, we show that in both frameworks, output feedback controller\nsynthesis with locality constraints can be formulated as a convex problem in\nfinitely many transfer function variables, admitting the use of standard\nnumerical solution techniques. The number of decision variables in the optimal\ncontroller design problem scales linearly with the distance of allowed\ncommunication. We also show that the optimal controller design problems for the\nsystem-level and input-ouptput parameterizations are equivalent for the chosen\nsystem of interest. We present numerical examples to illustrate the tradeoff\nbetween communication sparsity and performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u7a7a\u95f4\u4e0d\u53d8\u6027\u7684\u7cfb\u7edf\u7684H2\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u5668\u7efc\u5408\u95ee\u9898\uff0c\u5e76\u5bf9\u901a\u4fe1\u8ddd\u79bb\uff08\u5c40\u90e8\u6027\uff09\u65bd\u52a0\u4e86\u9884\u5b9a\u7ea6\u675f\u3002", "motivation": "\u5728\u5177\u6709\u7a7a\u95f4\u4e0d\u53d8\u6027\u7684\u7cfb\u7edf\u7684\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u5177\u6709\u901a\u4fe1\u8ddd\u79bb\uff08\u5c40\u90e8\u6027\uff09\u7ea6\u675f\u7684H2\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u5668\u7efc\u5408\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u5206\u89e3\u7684\u63a7\u5236\u5668\u6846\u67b6\uff08\u7cfb\u7edf\u7ea7\u53c2\u6570\u5316\u548c\u8f93\u5165-\u8f93\u51fa\u53c2\u6570\u5316\uff09\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u5c06\u63a7\u5236\u5668\u7efc\u5408\u95ee\u9898\u5236\u5b9a\u4e3a\u6709\u9650\u7684\u4f20\u9012\u51fd\u6570\u53d8\u91cf\u4e2d\u7684\u51f8\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u4e24\u79cd\u53c2\u6570\u5316\u6846\u67b6\u4e0b\uff0c\u5177\u6709\u5c40\u90e8\u6027\u7ea6\u675f\u7684\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u5668\u7efc\u5408\u95ee\u9898\u90fd\u53ef\u4ee5\u88ab\u8868\u8ff0\u4e3a\u51f8\u95ee\u9898\uff0c\u5e76\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684\u6570\u503c\u6280\u672f\u6c42\u89e3\u3002\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\u95ee\u9898\u7684\u51b3\u7b56\u53d8\u91cf\u6570\u91cf\u4e0e\u5141\u8bb8\u7684\u901a\u4fe1\u8ddd\u79bb\u6210\u7ebf\u6027\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6240\u9009\u7684\u7cfb\u7edf\uff0c\u7cfb\u7edf\u7ea7\u548c\u8f93\u5165-\u8f93\u51fa\u53c2\u6570\u5316\u7684\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\u95ee\u9898\u662f\u7b49\u4ef7\u7684\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5141\u8bb8\u5728\u63a7\u5236\u5668\u8bbe\u8ba1\u4e2d\u8003\u8651\u901a\u4fe1\u7ea6\u675f\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u8bf4\u660e\u901a\u4fe1\u7a00\u758f\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.14030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.", "AI": {"tldr": "CrowdAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u5730\u7ba1\u7406\u548c\u4f18\u5316NLP\u4efb\u52a1\u7684\u6570\u636e\u6807\u6ce8\u8fc7\u7a0b\uff0c\u80fd\u591f\u52a8\u6001\u6574\u5408LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e76\u8fdb\u884c\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u4ee5\u53ca\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\u3002", "motivation": "\u73b0\u6709\u7684NLP\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u6807\u6ce8\u672c\u8eab\uff0c\u800c\u5ffd\u7565\u4e86\u6574\u5408\u4e0d\u540c\u6807\u6ce8\u6e90\uff08LLMs\u3001SLMs\u3001\u4eba\u7c7b\u4e13\u5bb6\uff09\u7684\u52a8\u6001\u8fc7\u7a0b\u63a7\u5236\uff0c\u4ee5\u53ca\u5728\u8d28\u91cf\u548c\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrowdAgent\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5bf9\u4efb\u52a1\u8fdb\u884c\u5408\u7406\u5206\u914d\uff0c\u5e76\u6574\u5408LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5b9e\u73b0\u534f\u540c\u6807\u6ce8\u3002\u5b83\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u6d41\u7a0b\u63a7\u5236\uff0c\u5305\u62ec\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u4ee5\u53ca\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86CrowdAgent\u7684\u6709\u6548\u6027\u3002", "conclusion": "CrowdAgent\u901a\u8fc7\u6574\u5408LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e76\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u6d41\u7a0b\u63a7\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u7ba1\u7406\u548c\u4f18\u5316NLP\u6570\u636e\u7684\u6807\u6ce8\u8fc7\u7a0b\uff0c\u5728\u8d28\u91cf\u548c\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2509.13590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528Google Gemini 2.5 Flash\u7684\u591a\u6a21\u6001\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u5206\u6790\uff0c\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u80bf\u7624\u5e76\u751f\u6210\u591a\u6a21\u6001\uff08CT\u3001MRI\u3001X-ray\u3001Ultrasound\uff09\u7684\u4e34\u5e8a\u62a5\u544a\uff0c\u5b9e\u73b0\u4e8680\u50cf\u7d20\u7684\u5e73\u5747\u4f4d\u7f6e\u6d4b\u91cf\u504f\u5dee\uff0c\u5e76\u5177\u6709\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528AI\u5728\u533b\u7597\u5f71\u50cf\u8bca\u65ad\u548c\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u7684\u667a\u80fd\u6846\u67b6\u3002", "method": "\u6574\u5408Google Gemini 2.5 Flash\uff0c\u5229\u7528\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u7ed3\u5408\u5750\u6807\u9a8c\u8bc1\u548c\u9ad8\u65af\u6982\u7387\u6a21\u578b\uff0c\u5b9e\u73b0\u8de8CT\u3001MRI\u3001X-ray\u3001Ultrasound\u7684\u80bf\u7624\u68c0\u6d4b\u548c\u62a5\u544a\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u591a\u5c42\u7ea7\u53ef\u89c6\u5316\u548c\u7528\u6237\u53cb\u597d\u7684Gradio\u754c\u9762\u3002", "result": "\u5728\u8de8\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u4f4d\u7f6e\u6d4b\u91cf\u5e73\u5747\u504f\u5dee\u4e3a80\u50cf\u7d20\uff0c\u5e76\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u548c\u653e\u5c04\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e7f\u6cdb\u5e94\u7528\u524d\u9700\u8981\u8fdb\u884c\u4e34\u5e8a\u9a8c\u8bc1\u548c\u591a\u4e2d\u5fc3\u8bc4\u4f30\u3002"}}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "LLMs\u5728\u8bcd\u4e49\u6d88\u6b67\uff08WSD\uff09\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5728WSD\u4efb\u52a1\u4e0a\u80fd\u4e0e\u4e13\u4e1a\u7cfb\u7edf\u5ab2\u7f8e\uff0c\u5e76\u80fd\u51c6\u786e\u89e3\u91ca\u8bcd\u4e49\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u8bcd\u4e49\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u662f\u5426\u771f\u6b63\u638c\u63e1\u4e86\u8bcd\u8bed\u7684\u542b\u4e49\u3002", "method": "1. \u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18LLMs\u7684\u8bcd\u4e49\u6d88\u6b67\uff08WSD\uff09\u80fd\u529b\uff0c\u5e76\u4e0e\u4e13\u7528WSD\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\u3002\n2. \u8bc4\u4f30\u4e24\u4e2a\u9876\u5c16LLMs\uff08\u5f00\u6e90\u548c\u95ed\u6e90\uff09\u5728\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u89e3\u91ca\u548c\u793a\u4f8b\u751f\u6210\u4e09\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7406\u89e3\u8bcd\u4e49\u7684\u80fd\u529b\u3002", "result": "\u5728WSD\u4efb\u52a1\u4e2d\uff0cGPT-4o\u548cDeepSeek-V3\u7b49\u9886\u5148\u6a21\u578b\u7684\u8868\u73b0\u4e0e\u4e13\u7528WSD\u7cfb\u7edf\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u548c\u96be\u5ea6\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\n\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLMs\u80fd\u591f\u4ee5\u9ad8\u8fbe98%\u7684\u51c6\u786e\u7387\u89e3\u91ca\u8bcd\u8bed\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u542b\u4e49\uff0c\u5176\u4e2d\u81ea\u7531\u89e3\u91ca\u4efb\u52a1\u7684\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLMs\u4e0d\u4ec5\u5728WSD\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4e0e\u4e13\u4e1a\u7cfb\u7edf\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u80fd\u529b\uff0c\u800c\u4e14\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u80fd\u6709\u6548\u4e14\u51c6\u786e\u5730\u89e3\u91ca\u8bcd\u8bed\u7684\u542b\u4e49\uff0c\u5c24\u5176\u64c5\u957f\u81ea\u7531\u5f62\u5f0f\u7684\u89e3\u91ca\u3002"}}
{"id": "2509.13909", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13909", "abs": "https://arxiv.org/abs/2509.13909", "authors": ["Xavier Bonnetain", "Johanna Loyer", "Andr\u00e9 Schrottenloher", "Yixin Shen"], "title": "A Tight Quantum Algorithm for Multiple Collision Search", "comment": null, "summary": "Searching for collisions in random functions is a fundamental computational\nproblem, with many applications in symmetric and asymmetric cryptanalysis. When\none searches for a single collision, the known quantum algorithms match the\nquery lower bound. This is not the case for the problem of finding multiple\ncollisions, despite its regular appearance as a sub-component in sieving-type\nalgorithms.\n  At EUROCRYPT 2019, Liu and Zhandry gave a query lower bound $\\Omega(2^{m/3 +\n2k/3})$ for finding $2^k$ collisions in a random function with m-bit output. At\nEUROCRYPT 2023, Bonnetain et al. gave a quantum algorithm matching this bound\nfor a large range of $m$ and $k$, but not all admissible values. Like many\nprevious collision-finding algorithms, theirs is based on the MNRS quantum walk\nframework, but it chains the walks by reusing the state after outputting a\ncollision.\n  In this paper, we give a new algorithm that tackles the remaining non-optimal\nrange, closing the problem. Our algorithm is tight (up to a polynomial factor)\nin queries, and also in time under a quantum RAM assumption. The idea is to\nextend the chained walk to a regime in which several collisions are returned at\neach step, and the ``walks'' themselves only perform a single diffusion layer.", "AI": {"tldr": "\u5bfb\u627e\u591a\u4e2a\u78b0\u649e\u7684\u91cf\u5b50\u7b97\u6cd5\u5728\u67e5\u8be2\u548c\u65f6\u95f4\u4e0a\u90fd\u5df2\u8fbe\u5230\u6700\u4f18\u3002", "motivation": "\u5bfb\u627e\u591a\u4e2a\u78b0\u649e\u5728\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u5bc6\u7801\u5206\u6790\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5df2\u6709\u7684\u91cf\u5b50\u7b97\u6cd5\u5e76\u672a\u5728\u6240\u6709\u53c2\u6570\u4e0b\u90fd\u8fbe\u5230\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6269\u5c55\u4e86\u94fe\u5f0f\u91cf\u5b50\u884c\u8d70\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u8fd4\u56de\u591a\u4e2a\u78b0\u649e\uff0c\u5e76\u4e14\u201c\u884c\u8d70\u201d\u53ea\u6267\u884c\u4e00\u4e2a\u6269\u6563\u5c42\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u67e5\u8be2\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u4f18\uff08\u6700\u591a\u76f8\u5dee\u4e00\u4e2a\u591a\u9879\u5f0f\u56e0\u5b50\uff09\uff0c\u5e76\u5728\u91cf\u5b50\u968f\u673a\u8bbf\u95ee\u5b58\u50a8\u5668\uff08qRAM\uff09\u5047\u8bbe\u4e0b\uff0c\u5728\u65f6\u95f4\u65b9\u9762\u4e5f\u8fbe\u5230\u4e86\u6700\u4f18\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5bfb\u627e\u591a\u4e2a\u78b0\u649e\u95ee\u9898\u7684\u6700\u4f18\u6027\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u67e5\u8be2\u548c\u65f6\u95f4\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u4f18\u3002"}}
{"id": "2509.13736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13736", "abs": "https://arxiv.org/abs/2509.13736", "authors": ["Muyuan Ma", "Long Cheng", "Lijun Han", "Xiuze Xia", "Houcheng Li"], "title": "Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning", "comment": null, "summary": "Wearable exoskeletons can augment human strength and reduce muscle fatigue\nduring specific tasks. However, developing personalized and task-generalizable\nassistance algorithms remains a critical challenge. To address this, a\nmeta-imitation learning approach is proposed. This approach leverages a\ntask-specific neural network to predict human elbow joint movements, enabling\neffective assistance while enhancing generalization to new scenarios. To\naccelerate data collection, full-body keypoint motions are extracted from\npublicly available RGB video and motion-capture datasets across multiple tasks,\nand subsequently retargeted in simulation. Elbow flexion trajectories generated\nin simulation are then used to train the task-specific neural network within\nthe model-agnostic meta-learning (MAML) framework, which allows the network to\nrapidly adapt to novel tasks and unseen users with only a few gradient updates.\nThe adapted network outputs personalized references tracked by a\ngravity-compensated PD controller to ensure stable assistance. Experimental\nresults demonstrate that the exoskeleton significantly reduces both muscle\nactivation and metabolic cost for new users performing untrained tasks,\ncompared to performing without exoskeleton assistance. These findings suggest\nthat the proposed framework effectively improves task generalization and user\nadaptability for wearable exoskeleton systems.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u7684\u5143\u5b66\u4e60\uff08MAML\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4ece\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u5e76\u91cd\u65b0\u5b9a\u5411\u5230\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5168\u8eab\u5173\u952e\u70b9\u8fd0\u52a8\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u80fd\u591f\u9884\u6d4b\u4eba\u7c7b\u8098\u90e8\u5173\u8282\u7684\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a8\u5e7f\u5230\u65b0\u573a\u666f\u7684\u6709\u6548\u8f85\u52a9\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u65e0\u5916\u9aa8\u9abc\u8f85\u52a9\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u65b0\u7528\u6237\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u5316\u548c\u4ee3\u8c22\u6210\u672c\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u7528\u6237\u9002\u5e94\u6027\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u4e2a\u6027\u5316\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u4efb\u52a1\u7684\u8f85\u52a9\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u5728\u589e\u5f3a\u4eba\u7c7b\u529b\u91cf\u548c\u51cf\u5c11\u808c\u8089\u75b2\u52b3\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5143\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u516c\u5f00\u7684RGB\u89c6\u9891\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u4e2d\u7684\u5168\u8eab\u5173\u952e\u70b9\u8fd0\u52a8\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u91cd\u5b9a\u5411\uff0c\u751f\u6210\u8098\u90e8\u5c48\u66f2\u8f68\u8ff9\u3002\u7136\u540e\uff0c\u5728\u6a21\u578b\u65e0\u5173\u7684\u5143\u5b66\u4e60\uff08MAML\uff09\u6846\u67b6\u5185\uff0c\u4f7f\u7528\u8fd9\u4e9b\u8f68\u8ff9\u6765\u8bad\u7ec3\u4e00\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u672a\u89c1\u8fc7\u7684\u7528\u6237\u3002\u8be5\u7f51\u7edc\u9884\u6d4b\u7684\u4eba\u7c7b\u8098\u90e8\u5173\u8282\u8fd0\u52a8\u5c06\u4f5c\u4e3a\u91cd\u529b\u8865\u507f\u7684\u6bd4\u4f8b-\u5fae\u5206\uff08PD\uff09\u63a7\u5236\u5668\u7684\u4e00\u4e2a\u53c2\u8003\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u8f85\u52a9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u65e0\u5916\u9aa8\u9abc\u8f85\u52a9\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u964d\u4f4e\u65b0\u7528\u6237\u5728\u6267\u884c\u672a\u7ecf\u8bad\u7ec3\u7684\u4efb\u52a1\u65f6\u7684\u808c\u8089\u6d3b\u5316\u6c34\u5e73\u548c\u4ee3\u8c22\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u7528\u6237\u9002\u5e94\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8f85\u52a9\u3002"}}
{"id": "2509.13818", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13818", "abs": "https://arxiv.org/abs/2509.13818", "authors": ["Zheng-an Wang", "Yanbo J. Wang", "Jiachi Zhang", "Qi Xu", "Yilun Zhao", "Jintao Li", "Yipeng Zhang", "Bo Yang", "Xinkai Gao", "Xiaofeng Cao", "Kai Xu", "Pengpeng Hao", "Xuan Yang", "Heng Fan"], "title": "Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment", "comment": null, "summary": "Quantum Machine Learning (QML) offers a new paradigm for addressing complex\nfinancial problems intractable for classical methods. This work specifically\ntackles the challenge of few-shot credit risk assessment, a critical issue in\ninclusive finance where data scarcity and imbalance limit the effectiveness of\nconventional models. To address this, we design and implement a novel hybrid\nquantum-classical workflow. The methodology first employs an ensemble of\nclassical machine learning models (Logistic Regression, Random Forest, XGBoost)\nfor intelligent feature engineering and dimensionality reduction. Subsequently,\na Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as\nthe core classifier. This framework was evaluated through numerical simulations\nand deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting\nprocessor. On a real-world credit dataset of 279 samples, our QNN achieved a\nrobust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive\nAUC of 0.88 in the hardware experiment. This performance surpasses a suite of\nclassical benchmarks, with a particularly strong result on the recall metric.\nThis study provides a pragmatic blueprint for applying quantum computing to\ndata-constrained financial scenarios in the NISQ era and offers valuable\nempirical evidence supporting its potential in high-stakes applications like\ninclusive finance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u91d1\u878d\u9886\u57df\u4e2d\u6570\u636e\u7a00\u758f\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u5728\u91cf\u5b50\u4e91\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u4e2d\uff0c\u7279\u522b\u662f\u666e\u60e0\u91d1\u878d\uff0c\u9762\u4e34\u7740\u6570\u636e\u7a00\u758f\u548c\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u4e0d\u4f73\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\u4f7f\u7528\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u548c\u964d\u7ef4\uff0c\u7136\u540e\u5229\u7528\u53c2\u6570\u8fc1\u79fb\u89c4\u5219\u8bad\u7ec3\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u4f5c\u4e3a\u6838\u5fc3\u5206\u7c7b\u5668\uff0c\u6700\u540e\u5728Quafu\u91cf\u5b50\u4e91\u5e73\u53f0\u7684ScQ-P21\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4fe1\u7528\u6570\u636e\u96c6\uff08279\u4e2a\u6837\u672c\uff09\u4e0a\uff0cQNN\u5728\u6a21\u62df\u4e2d\u53d6\u5f97\u4e860.852 +/- 0.027\u7684\u5e73\u5747AUC\uff0c\u5728\u91cf\u5b50\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e860.88\u7684AUC\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u591a\u4e2a\u7ecf\u5178\u57fa\u51c6\u6a21\u578b\uff0c\u5728\u53ec\u56de\u7387\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728NISQ\uff08\u5608\u6742\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50\uff09\u65f6\u4ee3\u5c06\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u4e8e\u6570\u636e\u53d7\u9650\u7684\u91d1\u878d\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u84dd\u56fe\uff0c\u5e76\u4e3a\u5728\u666e\u60e0\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u53d1\u6325\u5176\u6f5c\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002"}}
{"id": "2509.14195", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14195", "abs": "https://arxiv.org/abs/2509.14195", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning", "comment": "8 pages, 3 figures", "summary": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u63a7\u5236\u5668\uff0c\u5728\u7ecf\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u4e8c\u9636\u5b66\u4e60\u80fd\u591f\u4fc3\u8fdb\u5fc3\u667a\u8868\u5f81\u7684\u51fa\u73b0\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u5bfc\u822a\u4efb\u52a1\u3002", "motivation": "\u5728\u9ad8\u7ea7\u8ba4\u77e5\u4e2d\uff0c\u5fc3\u667a\u8868\u5f81\uff08\u5373\u53cd\u6620\u5916\u90e8\u73af\u5883\u7684\u5185\u90e8\u6a21\u578b\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u96be\u4ee5\u5b9e\u8bc1\u3002\u8be5\u7814\u7a76\u65e8\u5728\u7ecf\u9a8c\u4e0a\u9a8c\u8bc1\u4e8c\u9636\u5b66\u4e60\uff08\u5b66\u4e60\u9002\u5e94\u4e00\u9636\u5b66\u4e60\u7684\u673a\u5236\uff09\u80fd\u591f\u4fc3\u8fdb\u5fc3\u667a\u8868\u5f81\u7684\u51fa\u73b0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5305\u542bGCN\uff08\u4e00\u9636\u5b66\u4e60\u8005\uff09\u548cMLP\u63a7\u5236\u5668\uff08\u4e8c\u9636\u5b66\u4e60\u8005\uff09\u7684\u5206\u5c42\u67b6\u6784\u3002GCN\u5c06\u8282\u70b9\u7279\u5f81\u6620\u5c04\u5230\u6700\u4f18\u5bfc\u822a\u8def\u5f84\u7684\u9884\u6d4b\uff0c\u800cMLP\u5728\u9762\u5bf9\u7ed3\u6784\u65b0\u9896\u7684\u8ff7\u5bab\u73af\u5883\u65f6\u52a8\u6001\u8c03\u6574GCN\u7684\u53c2\u6570\u3002", "result": "\u4e8c\u9636\u5b66\u4e60\u5728\u8ba4\u77e5\u7cfb\u7edf\u5f62\u6210\u4e0e\u73af\u5883\u7ed3\u6784\u540c\u6784\u7684\u5185\u90e8\u5fc3\u667a\u56fe\u65f6\u5c24\u5176\u6709\u6548\u3002\u5728\u672a\u89c1\u8fc7\u7684\u8ff7\u5bab\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u5fc3\u667a\u8868\u5f81\u5728\u6700\u5927\u5316\u4e8c\u9636\u5b66\u4e60\u7684\u6709\u6548\u6027\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u8be5\u7814\u7a76\u7684\u91cf\u5316\u548c\u8d28\u6027\u7ed3\u679c\u4e3a\u8fd9\u4e00\u89c2\u70b9\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u652f\u6301\u3002"}}
{"id": "2509.13605", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "CLAP\u7b97\u6cd5\u4ece2D\u63a8\u5e7f\u52303D\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u5173\u7cfb\u3002", "motivation": "\u5c06CLAP\u7b97\u6cd5\u4ece2D\u63a8\u5e7f\u52303D\uff0c\u5e76\u7814\u7a76\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u5173\u7cfb\u3002", "method": "\u5c06CLAP\u7b97\u6cd5\u63a8\u5e7f\u52303D\uff0c\u5e76\u5206\u6790\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u5173\u7cfb\u3002", "result": "CLAP\u7b97\u6cd5\u5df2\u6210\u529f\u63a8\u5e7f\u52303D\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u8054\u7cfb\u3002", "conclusion": "CLAP\u7b97\u6cd5\u7684\u63a8\u5e7f\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u53ef\u7528\u4e8e\u5904\u7406\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08mRAG\uff09\u7cfb\u7edf\u5728\u5904\u7406\u591a\u8bed\u8a00\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u65f6\uff0c\u5b58\u5728\u5bf9\u82f1\u8bed\u6765\u6e90\u7684\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e2d\u95f4\u4e0a\u4e0b\u6587\u4f4d\u7f6e\u7684\u6587\u6863\u4e2d\uff0c\u5e76\u4e14\u6709\u65f6\u4f1a\u727a\u7272\u6587\u6863\u76f8\u5173\u6027\u4ee5\u504f\u597d\u8bed\u8a00\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08mRAG\uff09\u7cfb\u7edf\u4e2d\u4e0d\u540c\u6587\u6863\u8bed\u8a00\u7684\u6df7\u5408\u662f\u5426\u4f1a\u5bf9\u751f\u6210\u548c\u5f15\u7528\u4ea7\u751f\u975e\u9884\u671f\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u53d7\u63a7\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u6765\u8861\u91cf\u8bed\u8a00\u504f\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u6863\u76f8\u5173\u6027\u7b49\u56e0\u7d20\u4e0d\u53d8\u3002\u5b9e\u9a8c\u8de8\u8d8a\u516b\u79cd\u8bed\u8a00\u548c\u516d\u79cd\u5f00\u653e\u6743\u91cd\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u67e5\u8be2\u4e3a\u82f1\u8bed\u65f6\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5f15\u7528\u82f1\u8bed\u6765\u6e90\uff0c\u5e76\u4e14\u8fd9\u79cd\u504f\u89c1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e2d\u6bb5\u4e0a\u4e0b\u6587\u7684\u6587\u6863\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u6b64\u5916\uff0c\u6a21\u578b\u6709\u65f6\u4f1a\u4e3a\u4e86\u8bed\u8a00\u504f\u597d\u800c\u727a\u7272\u6587\u6863\u76f8\u5173\u6027\u3002", "conclusion": "\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5728\u5f15\u7528\u65f6\u5b58\u5728\u8bed\u8a00\u504f\u89c1\uff0c\u5e76\u975e\u603b\u662f\u4ee5\u4fe1\u606f\u91cf\u4e3a\u552f\u4e00\u9a71\u52a8\u56e0\u7d20\uff0c\u8fd9\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u548c\u5f71\u54cd\u5f15\u7528\u884c\u4e3a\u3002"}}
{"id": "2509.13917", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.13917", "abs": "https://arxiv.org/abs/2509.13917", "authors": ["Yan Chen Jiang", "Lu Ma", "Chuan Wang", "Tie Jun Wang"], "title": "Global Mean-Amplitude Enhanced Spiking Neural Network Coherent Ising Machine", "comment": null, "summary": "The coherent Ising machine (CIM) is a quantum-inspired computing platform\nthat leverages optical parametric oscillation dynamics to solve combinatorial\noptimization problems by searching for the ground state of an Ising\nHamiltonian. Conventional CIM implementations face challenges in handling\nnon-uniform coupling strengths and maintaining amplitude stability during\ncomputation. In this paper, a new global mean-amplitude feedback-enhanced\nspiking neural network CIM (GFSNN-CIM) is introduced with a physics-driven\namplitude stabilization mechanism to dynamically balance nonlinear gain\nsaturation and coupling effects. This modification enhances synchronization in\nthe optical pulse network, leading to more robust convergence under varying\ninteraction strengths. Experimental validation on Max-Cut problems demonstrates\nthat the GFSNN-CIM achieves up to a 27% improvement in solution success rates\ncompared to conventional spiking neural network CIM, with scalability improving\nas problem complexity increases. Further application to the traffic assignment\nproblem (TAP) confirms the method's generality; the GFSNN-CIM achieves\nnear-continuous accuracy (deviations < 0.035%) even at coarse discretization,\nwhile large-scale tests on Beijing's road network (481 spins) validate its\nreal-world applicability. These advances establish a physics-consistent\noptimization framework, where optical pulse dynamics directly encode\ncombinatorial problems, paving the way for scalable, high-performance CIM\nimplementations in complex optimization tasks.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u578b\u7684\u76f8\u5e72\u4f0a\u8f9b\u673a\uff08CIM\uff09\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u5e45\u5ea6\u53cd\u9988\u589e\u5f3a\u7684SNN\uff08GFSNN-CIM\uff09\u6765\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfCIM\u5728\u5904\u7406\u975e\u5747\u5300\u8026\u5408\u5f3a\u5ea6\u548c\u7ef4\u6301\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u7684\u5e45\u5ea6\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7269\u7406\u9a71\u52a8\u5e45\u5ea6\u7a33\u5b9a\u673a\u5236\u7684GFSNN-CIM\uff0c\u8be5\u673a\u5236\u52a8\u6001\u5e73\u8861\u975e\u7ebf\u6027\u589e\u76ca\u9971\u548c\u548c\u8026\u5408\u6548\u5e94\uff0c\u589e\u5f3a\u4e86\u5149\u5b66\u8109\u51b2\u7f51\u7edc\u7684\u540c\u6b65\u6027\u3002", "result": "GFSNN-CIM\u5728Max-Cut\u95ee\u9898\u4e0a\u6bd4\u4f20\u7edfSNN-CIM\u63d0\u9ad8\u4e8627%\u7684\u89e3\u6210\u529f\u7387\uff0c\u5e76\u4e14\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u53ef\u6269\u5c55\u6027\u4e5f\u5f97\u5230\u63d0\u9ad8\u3002\u5728\u4ea4\u901a\u5206\u914d\u95ee\u9898\uff08TAP\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u5728\u7c97\u7565\u79bb\u6563\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u5b9e\u73b0\u63a5\u8fd1\u8fde\u7eed\u7684\u7cbe\u5ea6\uff08\u504f\u5dee<0.035%\uff09\u3002\u5728\u5317\u4eac\u7684\u9053\u8def\u7f51\u7edc\uff08481\u4e2a\u81ea\u65cb\uff09\u4e0a\u8fdb\u884c\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "GFSNN-CIM\u5efa\u7acb\u4e86\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5176\u4e2d\u5149\u5b66\u8109\u51b2\u52a8\u529b\u5b66\u76f4\u63a5\u7f16\u7801\u7ec4\u5408\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684CIM\u5b9e\u73b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13737", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13737", "abs": "https://arxiv.org/abs/2509.13737", "authors": ["Renjie Wang", "Shangke Lyu", "Donglin Wang"], "title": "Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control", "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable progress in legged\nlocomotion control, it often suffers from performance degradation in\nout-of-distribution (OOD) conditions and discrepancies between the simulation\nand the real environments. Instead of mainly relying on domain randomization\n(DR) to best cover the real environments and thereby close the sim-to-real gap\nand enhance robustness, this work proposes an emerging decoupled framework that\nacquires fast online adaptation ability and mitigates the sim-to-real problems\nin unfamiliar environments by isolating stance-leg control and swing-leg\ncontrol. Various simulation and real-world experiments demonstrate its\neffectiveness against horizontal force disturbances, uneven terrains, heavy and\nbiased payloads, and sim-to-real gap.", "AI": {"tldr": "RL\u5728\u8fd0\u52a8\u63a7\u5236\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u548c\u4eff\u771f\u5230\u73b0\u5b9e\uff08sim-to-real\uff09\u7684\u5dee\u5f02\u65b9\u9762\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\u7684\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u5728\u7ebf\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u51cf\u8f7b\u4e86\u5728\u4e0d\u719f\u6089\u73af\u5883\u4e2d\u7684sim-to-real\u95ee\u9898\u3002", "motivation": "RL\u5728\u8fd0\u52a8\u63a7\u5236\u9886\u57df\u867d\u7136\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728OOD\u6761\u4ef6\u548csim-to-real\u5dee\u5f02\u65b9\u9762\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u5c06\u7ad9\u7acb\u817f\u63a7\u5236\u548c\u6446\u52a8\u817f\u63a7\u5236\u5206\u5f00\uff0c\u4ee5\u83b7\u5f97\u5feb\u901f\u7684\u5728\u7ebf\u9002\u5e94\u80fd\u529b\u5e76\u51cf\u8f7bsim-to-real\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5404\u79cd\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u62b5\u6297\u6c34\u5e73\u529b\u5e72\u6270\u3001\u4e0d\u5e73\u5766\u5730\u5f62\u3001\u91cd\u504f\u8f7d\u8377\u4ee5\u53casim-to-real\u5dee\u5f02\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89e3\u8026\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347RL\u5728OOD\u6761\u4ef6\u548csim-to-real\u5dee\u5f02\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13841", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2509.13841", "abs": "https://arxiv.org/abs/2509.13841", "authors": ["Qingqi Zhao", "Heng Xiao"], "title": "An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction", "comment": "This preprint is also available at ESS Open Archive:\n  https://essopenarchive.org/users/960205/articles/1329010", "summary": "Accurate prediction of permeability in porous media is essential for modeling\nsubsurface flow. While pure data-driven models offer computational efficiency,\nthey often lack generalization across scales and do not incorporate explicit\nphysical constraints. Pore network models (PNMs), on the other hand, are\nphysics-based and efficient but rely on idealized geometric assumptions to\nestimate pore-scale hydraulic conductance, limiting their accuracy in complex\nstructures. To overcome these limitations, we present an end-to-end\ndifferentiable hybrid framework that embeds a graph neural network (GNN) into a\nPNM. In this framework, the analytical formulas used for conductance\ncalculations are replaced by GNN-based predictions derived from pore and throat\nfeatures. The predicted conductances are then passed to the PNM solver for\npermeability computation. In this way, the model avoids the idealized geometric\nassumptions of PNM while preserving the physics-based flow calculations. The\nGNN is trained without requiring labeled conductance data, which can number in\nthe thousands per pore network; instead, it learns conductance values by using\na single scalar permeability as the training target. This is made possible by\nbackpropagating gradients through both the GNN (via automatic differentiation)\nand the PNM solver (via a discrete adjoint method), enabling fully coupled,\nend-to-end training. The resulting model achieves high accuracy and generalizes\nwell across different scales, outperforming both pure data-driven and\ntraditional PNM approaches. Gradient-based sensitivity analysis further reveals\nphysically consistent feature influences, enhancing model interpretability.\nThis approach offers a scalable and physically informed framework for\npermeability prediction in complex porous media, reducing model uncertainty and\nimproving accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5d4c\u5165\u5b54\u9699\u7f51\u7edc\u6a21\u578b\uff08PNM\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u591a\u5b54\u4ecb\u8d28\u7684\u6e17\u900f\u7387\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u7684\u6548\u7387\u548c\u7269\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u6e17\u900f\u7387\u5bf9\u4e8e\u6a21\u62df\u5730\u4e0b\u6d41\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u6216\u51e0\u4f55\u5047\u8bbe\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5c06 GNN \u5d4c\u5165 PNM\u3002GNN \u66ff\u4ee3\u4e86 PNM \u4e2d\u8ba1\u7b97\u6d41\u5bfc\u7684\u89e3\u6790\u516c\u5f0f\uff0c\u76f4\u63a5\u4ece\u5b54\u9699\u548c\u5589\u9053\u7279\u5f81\u9884\u6d4b\u6d41\u5bfc\u3002\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u548c\u79bb\u6563\u4f34\u968f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86 GNN \u548c PNM \u6c42\u89e3\u5668\u7684\u5168\u8026\u5408\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4ec5\u4f7f\u7528\u6807\u91cf\u6e17\u900f\u7387\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u4e0e\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u548c\u4f20\u7edf PNM \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u68af\u5ea6\u5206\u6790\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u7269\u7406\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u590d\u6742\u591a\u5b54\u4ecb\u8d28\u7684\u6e17\u900f\u7387\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7269\u7406\u4fe1\u606f\u4e30\u5bcc\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u964d\u4f4e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2010.01052", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2010.01052", "abs": "https://arxiv.org/abs/2010.01052", "authors": ["Jaume Banus", "Maxime Sermesant", "Oscar Camara", "Marco Lorenzi"], "title": "Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets", "comment": null, "summary": "The use of mechanistic models in clinical studies is limited by the lack of\nmulti-modal patients data representing different anatomical and physiological\nprocesses. For example, neuroimaging datasets do not provide a sufficient\nrepresentation of heart features for the modeling of cardiovascular factors in\nbrain disorders. To tackle this problem we introduce a probabilistic framework\nfor joint cardiac data imputation and personalisation of cardiovascular\nmechanistic models, with application to brain studies with incomplete heart\ndata. Our approach is based on a variational framework for the joint inference\nof an imputation model of cardiac information from the available features,\nalong with a Gaussian Process emulator that can faithfully reproduce\npersonalised cardiovascular dynamics. Experimental results on UK Biobank show\nthat our model allows accurate imputation of missing cardiac features in\ndatasets containing minimal heart information, e.g. systolic and diastolic\nblood pressures only, while jointly estimating the emulated parameters of the\nlumped model. This allows a novel exploration of the heart-brain joint\nrelationship through simulation of realistic cardiac dynamics corresponding to\ndifferent conditions of brain anatomy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fc3\u810f\u6570\u636e\u586b\u8865\u548c\u5fc3\u8840\u7ba1\u529b\u5b66\u6a21\u578b\u4e2a\u6027\u5316\u7684\u6982\u7387\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4e34\u5e8a\u7814\u7a76\u4e2d\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8111\u90e8\u7814\u7a76\u4e2d\u5fc3\u810f\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u4e34\u5e8a\u7814\u7a76\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4ee3\u8868\u4e0d\u540c\u89e3\u5256\u548c\u751f\u7406\u8fc7\u7a0b\u7684\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\uff0c\u529b\u5b66\u6a21\u578b\u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002\u4f8b\u5982\uff0c\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u8868\u793a\u5fc3\u810f\u7279\u5f81\uff0c\u963b\u788d\u4e86\u5bf9\u8111\u90e8\u75be\u75c5\u4e2d\u5fc3\u8840\u7ba1\u56e0\u7d20\u7684\u5efa\u6a21\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u73b0\u6709\u7279\u5f81\u4e2d\u8054\u5408\u63a8\u65ad\u5fc3\u810f\u4fe1\u606f\u7684\u586b\u8865\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u6a21\u62df\u5668\uff0c\u4ee5\u5fe0\u5b9e\u5730\u518d\u73b0\u4e2a\u6027\u5316\u7684\u5fc3\u8840\u7ba1\u52a8\u529b\u5b66\u3002", "result": "\u5728UK Biobank\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u5730\u586b\u8865\u4ec5\u5305\u542b\u6536\u7f29\u538b\u548c\u8212\u5f20\u538b\u7b49\u5c11\u91cf\u5fc3\u810f\u4fe1\u606f\u7684\u8111\u90e8\u7814\u7a76\u6570\u636e\u96c6\u4e2d\u7f3a\u5931\u7684\u5fc3\u810f\u7279\u5f81\uff0c\u5e76\u540c\u65f6\u4f30\u8ba1\u96c6\u603b\u6a21\u578b\u7684\u6a21\u62df\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u8111\u90e8\u89e3\u5256\u6761\u4ef6\u4e0b\u903c\u771f\u7684\u5fc3\u810f\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u5bf9\u5fc3-\u8111\u8054\u5408\u5173\u7cfb\u7684\u521b\u65b0\u6027\u63a2\u7d22\u3002"}}
{"id": "2509.13629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "SAMIR\u662f\u4e00\u4e2a\u5229\u7528SAM\u6a21\u578b\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\u548c\u8bbe\u8ba1\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\uff0c\u63d0\u9ad8\u4e86\u914d\u51c6\u7cbe\u5ea6\uff0c\u5e76\u5728\u5fc3\u810f\u548c\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u901a\u5e38\u4e0d\u6613\u83b7\u5f97\u7684\u89e3\u5256\u5148\u9a8c\uff08\u5982\u5206\u5272\u63a9\u7801\u6216\u6807\u5fd7\u70b9\uff09\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u9879\u76ee\u65e8\u5728\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u4ece\u800c\u63d0\u9ad8\u914d\u51c6\u7cbe\u5ea6\u3002", "method": "SAMIR\u6846\u67b6\u9996\u5148\u4f7f\u7528SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u76843D\u5934\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7ec6\u5316\u7279\u5f81\uff0c\u4ee5\u9002\u5e94\u5c40\u90e8\u5f62\u53d8\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u6765\u6307\u5bfc\u7c97\u5230\u7cbe\u7684\u7279\u5f81\u5339\u914d\u548c\u89e3\u5256\u5bf9\u9f50\u3002", "result": "SAMIR\u5728\u5fc3\u810f\u56fe\u50cf\u914d\u51c6\uff08ACDC\u6570\u636e\u96c6\uff09\u548c\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\uff08\u8179\u90e8\u6570\u636e\u96c6\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5206\u522b\u53d6\u5f97\u4e862.68%\u548c6.44%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SAMIR\u901a\u8fc7\u5229\u7528SAM\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6a21\u62df\u89e3\u5256\u4e00\u81f4\u6027\u548c\u5f62\u53d8\u6a21\u5f0f\uff0c\u4ece\u800c\u5728\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "wmt25 \u4efb\u52a1\u7684\u63d0\u4ea4\uff0c\u4f7f\u7528 COMET \u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u8bad\u7ec3\uff0c\u6765\u9884\u6d4b\u5206\u6bb5\u9519\u8bef\u8303\u56f4\u6807\u6ce8 (ESA) \u5206\u6570\u3002", "motivation": "\u5728 WMT25 \u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u4e2d\u63d0\u4ea4\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528 COMET \u6846\u67b6\uff0c\u901a\u8fc7\u62fc\u63a5\u9886\u57df\u5185\u4eba\u5de5\u6807\u6ce8\u7684\u53e5\u5b50\u5e76\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u5206\u6570\u6765\u6784\u5efa\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\u3002\u6574\u5408 MQM\u3001SQM \u548c DA \u7b49\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u5176\u91cf\u8868\uff0c\u5e76\u8bad\u7ec3\u591a\u8bed\u8a00\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u6e90\u3001\u5047\u8bbe\u548c\u53c2\u8003\u7ffb\u8bd1\u7684\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5305\u542b\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u63d0\u9ad8\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e8e\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.13771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13771", "abs": "https://arxiv.org/abs/2509.13771", "authors": ["Mengzhu Li", "Yunyu Zhou", "He Ying", "F. Richard Yu"], "title": "CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs", "comment": null, "summary": "Signed Distance Fields (SDFs) are a fundamental representation in robot\nmotion planning. Their configuration-space counterpart, the Configuration Space\nDistance Field (CDF), directly encodes distances in joint space, offering a\nunified representation for optimization and control. However, existing CDF\nformulations face two major challenges in high-degree-of-freedom (DoF) robots:\n(1) they effectively return only a single nearest collision configuration,\nneglecting the multi-modal nature of minimal-distance collision configurations\nand leading to gradient ambiguity; and (2) they rely on sparse sampling of the\ncollision boundary, which often fails to identify the true closest\nconfigurations, producing oversmoothed approximations and geometric distortion\nin high-dimensional spaces. We propose CDFlow, a novel framework that addresses\nthese limitations by learning a continuous flow in configuration space via\nNeural Ordinary Differential Equations (Neural ODEs). We redefine the problem\nfrom finding a single nearest point to modeling the distribution of\nminimal-distance collision configurations. We also introduce an adaptive\nrefinement sampling strategy to generate high-fidelity training data for this\ndistribution. The resulting Neural ODE implicitly models this multi-modal\ndistribution and produces a smooth, consistent gradient field-derived as the\nexpected direction towards the distribution-that mitigates gradient ambiguity\nand preserves sharp geometric features. Extensive experiments on high-DoF\nmotion planning tasks demonstrate that CDFlow significantly improves planning\nefficiency, trajectory quality, and robustness compared to existing CDF-based\nmethods, enabling more robust and efficient planning for collision-aware robots\nin complex environments.", "AI": {"tldr": "CDFlow\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecfODE\u5b66\u4e60\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\uff0c\u4ee5\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684CDF\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684CDF\u65b9\u6cd5\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1. \u5b83\u4eec\u53ea\u8fd4\u56de\u4e00\u4e2a\u6700\u8fd1\u7684\u78b0\u649e\u914d\u7f6e\uff0c\u5ffd\u7565\u4e86\u6700\u5c0f\u8ddd\u79bb\u78b0\u649e\u914d\u7f6e\u7684\u591a\u6a21\u6001\u6027\u8d28\uff0c\u5bfc\u81f4\u68af\u5ea6\u6a21\u7cca\uff1b2. \u5b83\u4eec\u4f9d\u8d56\u4e8e\u7a00\u758f\u7684\u78b0\u649e\u8fb9\u754c\u91c7\u6837\uff0c\u8fd9\u901a\u5e38\u4f1a\u5931\u8d25\uff0c\u65e0\u6cd5\u8bc6\u522b\u771f\u6b63\u7684\u6700\u8fd1\u914d\u7f6e\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u7684\u8fd1\u4f3c\u548c\u51e0\u4f55\u5931\u771f\u3002", "method": "CDFlow\u901a\u8fc7\u5b66\u4e60\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4f7f\u7528\u795e\u7ecfODE\u3002\u5b83\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ece\u5bfb\u627e\u5355\u4e2a\u6700\u8fd1\u70b9\u5230\u5bf9\u6700\u5c0f\u8ddd\u79bb\u78b0\u649e\u914d\u7f6e\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7ec6\u5316\u91c7\u6837\u7b56\u7565\u6765\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "CDFlow\u80fd\u591f\u9690\u5f0f\u5730\u5bf9\u591a\u6a21\u6001\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4ea7\u751f\u4e00\u4e2a\u5e73\u6ed1\u3001\u4e00\u81f4\u7684\u68af\u5ea6\u573a\uff0c\u4ece\u800c\u51cf\u8f7b\u68af\u5ea6\u6a21\u7cca\u5e76\u4fdd\u7559\u5c16\u9510\u7684\u51e0\u4f55\u7279\u5f81\u3002\u5728\u9ad8\u81ea\u7531\u5ea6\u8fd0\u52a8\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u4e8eCDF\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cCDFlow\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u3001\u8f68\u8ff9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CDFlow\u901a\u8fc7\u5b66\u4e60\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\uff0c\u5e76\u5bf9\u6700\u5c0f\u8ddd\u79bb\u78b0\u649e\u914d\u7f6e\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2dCDF\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u3001\u66f4\u9ad8\u6548\u7684\u78b0\u649e\u611f\u77e5\u673a\u5668\u4eba\u89c4\u5212\u3002"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "TTS\u6280\u672f\uff08\u5982Self-Consistency, SC\uff09\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u63a8\u7406\u94fe\u5e76\u8fdb\u884c\u591a\u6570\u6295\u7968\u6765\u63d0\u9ad8LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "motivation": "Self-Consistency\uff08SC\uff09\u6280\u672f\u867d\u7136\u6709\u6548\uff0c\u4f46\u5176\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faSlim-SC\uff0c\u4e00\u79cd\u9010\u6b65\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u601d\u8003\u5c42\u9762\u8bc6\u522b\u548c\u53bb\u9664\u5197\u4f59\u7684\u63a8\u7406\u94fe\u6765\u964d\u4f4eSC\u7684\u5f00\u9500\u3002", "result": "\u5728\u4e09\u4e2aSTEM\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSlim-SC\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u5ef6\u8fdf\u548cKVC\u4f7f\u7528\u91cf\u5206\u522b\u964d\u4f4e\u4e86\u9ad8\u8fbe45%\u548c26%\u3002", "conclusion": "Slim-SC\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\uff08TTS\uff09\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4eSC\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.14026", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14026", "abs": "https://arxiv.org/abs/2509.14026", "authors": ["Jiun-Cheng Jiang", "Morris Yu-Chao Huang", "Tianlong Chen", "Hsi-Sheng Goan"], "title": "Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks", "comment": "45 pages", "summary": "Variational quantum circuits (VQCs) are central to quantum machine learning,\nwhile recent progress in Kolmogorov-Arnold networks (KANs) highlights the power\nof learnable activation functions. We unify these directions by introducing\nquantum variational activation functions (QVAFs), realized through single-qubit\ndata re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We\nshow that DARUAN with trainable weights in data pre-processing possesses an\nexponentially growing frequency spectrum with data repetitions, enabling an\nexponential reduction in parameter size compared with Fourier-based activations\nwithout loss of expressivity. Embedding DARUAN into KANs yields\nquantum-inspired KANs (QKANs), which retain the interpretability of KANs while\nimproving their parameter efficiency, expressivity, and generalization. We\nfurther introduce two novel techniques to enhance scalability, feasibility and\ncomputational efficiency, such as layer extension and hybrid QKANs (HQKANs) as\ndrop-in replacements of multi-layer perceptrons (MLPs) for feed-forward\nnetworks in large-scale models. We provide theoretical analysis and extensive\nexperiments on function regression, image classification, and autoregressive\ngenerative language modeling, demonstrating the efficiency and scalability of\nQKANs. DARUANs and QKANs offer a promising direction for advancing quantum\nmachine learning on both noisy intermediate-scale quantum (NISQ) hardware and\nclassical quantum simulators.", "AI": {"tldr": "DARUANs and QKANs are novel quantum-inspired machine learning models that improve parameter efficiency and expressivity by unifying VQCs and KANs.", "motivation": "Unify variational quantum circuits (VQCs) and Kolmogorov-Arnold networks (KANs) by introducing learnable activation functions for quantum machine learning.", "method": "Introduce Quantum Variational Activation Functions (QVAFs) realized by DatA Re-Uploading ActivatioNs (DARUANs) with trainable weights. Embed DARUANs into KANs to create quantum-inspired KANs (QKANs). Develop layer extension and hybrid QKANs (HQKANs) for scalability and efficiency. Conduct theoretical analysis and experiments on function regression, image classification, and language modeling.", "result": "DARUANs exhibit exponentially growing frequency spectrum, enabling exponential reduction in parameter size. QKANs retain KAN interpretability while improving parameter efficiency, expressivity, and generalization. HQKANs serve as drop-in replacements for MLPs in large-scale models. Experiments demonstrate the efficiency and scalability of QKANs.", "conclusion": "DARUANs and QKANs present a promising direction for advancing quantum machine learning on NISQ hardware and classical simulators."}}
{"id": "2509.13774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13774", "abs": "https://arxiv.org/abs/2509.13774", "authors": ["Piaopiao Jin", "Qi Wang", "Guokang Sun", "Ziwen Cai", "Pinjia He", "Yangwei You"], "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach", "comment": null, "summary": "Vision-language-action (VLA) models demonstrate strong generalization in\nrobotic manipulation but face challenges in complex, real-world tasks. While\nsupervised fine-tuning with demonstrations is constrained by data quality,\nreinforcement learning (RL) offers a promising alternative. We propose a\nhuman-in-the-loop dual-actor fine-tuning framework grounded in RL. The\nframework integrates a primary actor for robust multi-task performance with a\nrefinement actor for latent-space adaptation. Beyond standard physical\ninterventions, we introduce a lightweight talk-and-tweak scheme that converts\nhuman corrections into semantically grounded language commands, thereby\ngenerating a new dataset for policy learning. In real-world multi-task\nexperiments, our approach achieves 100% success across three tasks within 101\nminutes of online fine-tuning. For long-horizon tasks, it sustains a 50%\nsuccess rate over 12 consecutive operations. Furthermore, the framework scales\neffectively to multi-robot training, achieving up to a 2 times improvement in\nefficiency when using dual robots. The experiment videos are available at\nhttps://sites.google.com/view/hil-daft/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165'talk-and-tweak'\u673a\u5236\uff0c\u5c06\u4eba\u7c7b\u7684\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u7b56\u7565\u5b66\u4e60\u7684\u6709\u6548\u6570\u636e\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u4efb\u52a1\u548c\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u540c\u65f6\u8fd8\u5c55\u793a\u4e86\u591a\u673a\u5668\u4eba\u534f\u540c\u8bad\u7ec3\u7684\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u76d1\u7763\u5fae\u8c03\u53d7\u6570\u636e\u8d28\u91cf\u9650\u5236\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRL\u7684\u4eba\u7c7b\u5728\u73af\u53ccActor\u5fae\u8c03\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4e3bActor\u548c\u4e00\u4e2a\u7528\u4e8e\u6f5c\u5728\u7a7a\u95f4\u81ea\u9002\u5e94\u7684\u7cbe\u70bcActor\uff0c\u5e76\u5f15\u5165\u4e86'talk-and-tweak'\u673a\u5236\uff0c\u5c06\u4eba\u7c7b\u7684\u8bed\u8a00\u4fee\u6b63\u8f6c\u5316\u4e3a\u65b0\u7684\u7b56\u7565\u5b66\u4e60\u6570\u636e\u3002", "result": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728101\u5206\u949f\u7684\u5728\u7ebf\u5fae\u8c03\u540e\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff1b\u5728\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u572812\u6b21\u8fde\u7eed\u64cd\u4f5c\u4e2d\u4fdd\u6301\u4e8650%\u7684\u6210\u529f\u7387\uff1b\u53cc\u673a\u5668\u4eba\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u4e862\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4eba\u7c7b\u5728\u73af\u53ccActor\u5fae\u8c03\u6846\u67b6\u901a\u8fc7\u5f15\u5165'talk-and-tweak'\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.13866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13866", "abs": "https://arxiv.org/abs/2509.13866", "authors": ["Sitong Chen", "Shen Nie", "Jiacheng Sun", "Zijin Feng", "Zhenguo Li", "Ji-Rong Wen", "Chongxuan Li"], "title": "Masked Diffusion Models as Energy Minimization", "comment": null, "summary": "We present a systematic theoretical framework that interprets masked\ndiffusion models (MDMs) as solutions to energy minimization problems in\ndiscrete optimal transport. Specifically, we prove that three distinct energy\nformulations--kinetic, conditional kinetic, and geodesic energy--are\nmathematically equivalent under the structure of MDMs, and that MDMs minimize\nall three when the mask schedule satisfies a closed-form optimality condition.\nThis unification not only clarifies the theoretical foundations of MDMs, but\nalso motivates practical improvements in sampling. By parameterizing\ninterpolation schedules via Beta distributions, we reduce the schedule design\nspace to a tractable 2D search, enabling efficient post-training tuning without\nmodel modification. Experiments on synthetic and real-world benchmarks\ndemonstrate that our energy-inspired schedules outperform hand-crafted\nbaselines, particularly in low-step sampling settings.", "AI": {"tldr": "\u63a9\u853d\u6269\u6563\u6a21\u578b(MDM)\u88ab\u89e3\u91ca\u4e3a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\u3002MDM\u5728\u6ee1\u8db3\u7279\u5b9a\u6700\u4f18\u6761\u4ef6\u65f6\uff0c\u80fd\u6700\u5c0f\u5316\u52a8\u80fd\u3001\u6761\u4ef6\u52a8\u80fd\u548c\u6d4b\u5730\u7ebf\u80fd\u91cf\u3002\u8fd9\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u4e0d\u4ec5\u9610\u660e\u4e86MDM\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd8\u901a\u8fc7\u53c2\u6570\u5316\u63d2\u503c\u8868\uff08\u4f7f\u7528Beta\u5206\u5e03\uff09\u6fc0\u53d1\u4e86\u91c7\u6837\u65b9\u6cd5\u7684\u6539\u8fdb\uff0c\u5c06\u8868\u8bbe\u8ba1\u7b80\u5316\u4e3a\u4e8c\u7ef4\u641c\u7d22\uff0c\u4fbf\u4e8e\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5fae\u8c03\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u80fd\u91cf\u542f\u53d1\u5f0f\u8868\u5728\u4f4e\u6b65\u91c7\u6837\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u7ebf\u3002 ", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5176\u89e3\u91ca\u4e3a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u6539\u8fdb\u91c7\u6837\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06MDM\u89e3\u91ca\u4e3a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\u3002\u8bc1\u660e\u4e86\u4e09\u79cd\u80fd\u91cf\u5f62\u5f0f\uff08\u52a8\u80fd\u3001\u6761\u4ef6\u52a8\u80fd\u3001\u6d4b\u5730\u7ebf\u80fd\u91cf\uff09\u5728MDM\u7ed3\u6784\u4e0b\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u786e\u5b9a\u4e86MDM\u5728\u6ee1\u8db3\u7279\u5b9a\u6700\u4f18\u6761\u4ef6\u65f6\u6700\u5c0f\u5316\u8fd9\u4e9b\u80fd\u91cf\u3002\u901a\u8fc7Beta\u5206\u5e03\u53c2\u6570\u5316\u63d2\u503c\u8868\uff0c\u5c06\u8868\u8bbe\u8ba1\u7b80\u5316\u4e3a\u4e8c\u7ef4\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u8fdb\u884c\u9ad8\u6548\u7684\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u80fd\u91cf\u542f\u53d1\u5f0f\u63d2\u503c\u8868\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6b65\u91c7\u6837\u8bbe\u7f6e\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u7ebf\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06MDM\u7edf\u4e00\u5728\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u6846\u67b6\u4e0b\uff0c\u9610\u660e\u4e86\u5176\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u80fd\u91cf\u542f\u53d1\u5f0f\u65b9\u6cd5\u6539\u8fdb\u4e86\u91c7\u6837\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6b65\u91c7\u6837\u573a\u666f\u4e0b\u3002"}}
{"id": "2509.13943", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13943", "abs": "https://arxiv.org/abs/2509.13943", "authors": ["Salim Oyinlola", "Nitesh Subedi", "Soumik Sarkar"], "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation", "comment": "Presented at the Research Experience for Undergraduates (REU)\n  Symposium at the Translational AI Centre in Iowa State University", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios.", "AI": {"tldr": "\u9879\u76ee\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8ba9\u5355\u67b6\u65e0\u4eba\u673a\u80fd\u591f\u81ea\u4e3b\u5bfc\u822a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u70b9\u5bf9\u70b9\u64cd\u4f5c\u3002", "motivation": "\u65e0\u4eba\u673a\uff08UAVs\uff09\u5728\u81ea\u52a8\u5316\u68c0\u67e5\u3001\u4ea4\u4ed8\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u9700\u8981\u53ef\u9760\u7684\u81ea\u4e3b\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86ROS\u548c\u4e00\u4e2aGym\u517c\u5bb9\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u901a\u8fc7\u5e26\u6709\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u7684\u8bd5\u9519\u4ea4\u4e92\u6765\u8bad\u7ec3\u65e0\u4eba\u673a\u5728\u907f\u514d\u78b0\u649e\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u540c\u65f6\u9ad8\u6548\u5730\u5230\u8fbe\u76ee\u6807\u70b9\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u7b56\u7565\u6210\u529f\u90e8\u7f72\u5230\u771f\u5b9e\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\uff0c\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u65e0\u4eba\u673a\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u76d1\u7ba1\u6210\u529f\u6267\u884c\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8eRL\u7684\u63a7\u5236\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8fdb\u884c\u70b9\u5bf9\u70b9\u65e0\u4eba\u673a\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.13652", "categories": ["cs.CV", "I.4.8; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS\u662f\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u91cd\u5efa\u76843D\u573a\u666f\u7684\u76f4\u63a5\u5bf9\u9f50\u6765\u4f30\u8ba1\u76f8\u673a\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5ea6\u91cf\u5c3a\u5ea6\u3001\u5927\u57fa\u7ebf\u548c\u7eb9\u7406\u7a00\u758f\u8868\u9762\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u4e24\u89c6\u56fe\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u5ea6\u91cf\u5c3a\u5ea6\uff0c\u5728\u5904\u7406\u5927\u57fa\u7ebf\u548c\u7eb9\u7406\u7a00\u758f/\u53cd\u5c04\u8868\u9762\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800cGARPS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GARPS\u5229\u7528\u5ea6\u91cf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u548c\u9ad8\u65af\u573a\u666f\u91cd\u5efa\u5668\u4e3a\u6bcf\u4e2a\u56fe\u50cf\u83b7\u5f97\u5ea6\u91cf3D\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u53ef\u5fae\u7684GMM\u5bf9\u9f50\u76ee\u6807\u6765\u4f18\u5316\u521d\u59cb\u4f4d\u59ff\uff0c\u8be5\u76ee\u6807\u7efc\u5408\u8003\u8651\u4e86\u51e0\u4f55\u7ed3\u6784\u3001\u89c6\u70b9\u65e0\u5173\u989c\u8272\u3001\u5404\u5411\u5f02\u6027\u534f\u65b9\u5dee\u548c\u8bed\u4e49\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGARPS\u7684\u6027\u80fd\u4f18\u4e8e\u5305\u62ecMASt3R\u5728\u5185\u7684\u4f20\u7edf\u548c\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "GARPS\u7684\u6210\u529f\u5c55\u793a\u4e86\u7ed3\u5408\u5355\u89c6\u56fe\u611f\u77e5\u548c\u591a\u89c6\u56fe\u51e0\u4f55\u6765\u83b7\u5f97\u9c81\u68d2\u4e14\u5ea6\u91cf\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "ES-CoT\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u5e76\u63d0\u524d\u505c\u6b62\u6765\u7f29\u77ed\u957f\u94fe\u601d\u8003\uff08CoT\uff09\u7684\u751f\u6210\uff0c\u4ece\u800c\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u94fe\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301LLM\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7ed3\u675f\u65f6\uff0c\u63d0\u793aLLM\u8f93\u51fa\u5f53\u524d\u7684\u6700\u7ec8\u7b54\u6848\uff08\u6b65\u9aa4\u7b54\u6848\uff09\uff0c\u5e76\u8ddf\u8e2a\u8fde\u7eed\u76f8\u540c\u6b65\u9aa4\u7b54\u6848\u7684\u8fd0\u884c\u957f\u5ea6\u4f5c\u4e3a\u7b54\u6848\u6536\u655b\u7684\u5ea6\u91cf\u3002\u5f53\u8fd0\u884c\u957f\u5ea6\u51fa\u73b0\u6025\u5267\u589e\u52a0\u5e76\u8d85\u8fc7\u6700\u5c0f\u9608\u503c\u65f6\uff0c\u63d0\u524d\u7ec8\u6b62\u751f\u6210\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cES-CoT\u5e73\u5747\u5c06\u63a8\u7406\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u4e86\u7ea641%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "ES-CoT\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e0e\u81ea\u6d3d\u63d0\u793a\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u4e14\u5728\u8d85\u53c2\u6570\u9009\u62e9\u4e0a\u4fdd\u6301\u7a33\u5065\u3002"}}
{"id": "2509.13780", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13780", "abs": "https://arxiv.org/abs/2509.13780", "authors": ["Weishuai Zeng", "Shunlin Lu", "Kangning Yin", "Xiaojie Niu", "Minyue Dai", "Jingbo Wang", "Jiangmiao Pang"], "title": "Behavior Foundation Model for Humanoid Robots", "comment": null, "summary": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control.", "AI": {"tldr": "BFM\u662f\u4e00\u4e2a\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709Whole-body control\uff08WBC\uff09\u7cfb\u7edf\u4efb\u52a1\u7279\u5b9a\u3001\u6cdb\u5316\u6027\u5dee\u7684\u7f3a\u70b9\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6355\u6349\u901a\u7528\u7684\u884c\u4e3a\u77e5\u8bc6\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709WBC\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u9700\u8981\u5927\u91cf\u7684\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u73b0\u5b9e\u573a\u666f\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff08BFM\uff09\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u63a9\u7801\u5728\u7ebf\u84b8\u998f\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a21\u884c\u4e3a\u5206\u5e03\uff0c\u6355\u6349\u5e7f\u6cdb\u3001\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u77e5\u8bc6\u3002", "result": "BFM\u5728\u6a21\u62df\u548c\u7269\u7406\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u79cdWBC\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u3002", "conclusion": "BFM\u662f\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u9886\u57df\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u8fdb\u6b65\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13895", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13895", "abs": "https://arxiv.org/abs/2509.13895", "authors": ["Zhanting Zhou", "Jinshan Lai", "Fengchun Zhang", "Zeqin Wu", "Fengli Zhang"], "title": "FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning", "comment": "4 page main text for conference", "summary": "Non-IID data and partial participation induce client drift and inconsistent\nlocal optima in federated learning, causing unstable convergence and accuracy\nloss. We present FedSSG, a stochastic sampling-guided, history-aware drift\nalignment method. FedSSG maintains a per-client drift memory that accumulates\nlocal model differences as a lightweight sketch of historical gradients;\ncrucially, it gates both the memory update and the local alignment term by a\nsmooth function of the observed/expected participation ratio (a\nphase-by-expectation signal derived from the server sampler). This\nstatistically grounded gate stays weak and smooth when sampling noise dominates\nearly, then strengthens once participation statistics stabilize, contracting\nthe local-global gap without extra communication. Across CIFAR-10/100 with\n100/500 clients and 2-15 percent participation, FedSSG consistently outperforms\nstrong drift-aware baselines and accelerates convergence; on our benchmarks it\nimproves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and\nabout +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about\n4.5x faster target-accuracy convergence on average. The method adds only O(d)\nclient memory and a constant-time gate, and degrades gracefully to a mild\nregularizer under near-IID or uniform sampling. FedSSG shows that sampling\nstatistics can be turned into a principled, history-aware phase control to\nstabilize and speed up federated training.", "AI": {"tldr": "FedSSG\u662f\u4e00\u79cd\u968f\u673a\u91c7\u6837\u5f15\u5bfc\u7684\u5386\u53f2\u611f\u77e5\u6f02\u79fb\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ef4\u62a4\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u6f02\u79fb\u5185\u5b58\u5e76\u6839\u636e\u53c2\u4e0e\u7387\u8c03\u6574\u672c\u5730\u5bf9\u9f50\u9879\uff0c\u4ee5\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u90e8\u5206\u53c2\u4e0e\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u51c6\u786e\u7387\u635f\u5931\u95ee\u9898\u3002", "motivation": "\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u90e8\u5206\u53c2\u4e0e\u4f1a\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u5c40\u90e8\u6700\u4f18\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u5f15\u53d1\u6536\u655b\u4e0d\u7a33\u5b9a\u548c\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "FedSSG\u7ef4\u62a4\u4e00\u4e2a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u6f02\u79fb\u5185\u5b58\uff0c\u7d2f\u79ef\u5c40\u90e8\u6a21\u578b\u5dee\u5f02\u4f5c\u4e3a\u5386\u53f2\u68af\u5ea6\u7684\u8f7b\u91cf\u7ea7\u8349\u56fe\uff1b\u5173\u952e\u5728\u4e8e\uff0c\u5b83\u901a\u8fc7\u4e00\u4e2a\u5e73\u6ed1\u51fd\u6570\u6765\u63a7\u5236\u5185\u5b58\u66f4\u65b0\u548c\u5c40\u90e8\u5bf9\u9f50\u9879\uff0c\u8be5\u51fd\u6570\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684/\u9884\u671f\u7684\u53c2\u4e0e\u7387\uff08\u6765\u81ea\u670d\u52a1\u5668\u91c7\u6837\u5668\u7684\u57fa\u4e8e\u671f\u671b\u7684\u76f8\u4f4d\u4fe1\u53f7\uff09\u3002", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528100/500\u4e2a\u5ba2\u6237\u7aef\u548c2-15%\u7684\u53c2\u4e0e\u7387\uff0cFedSSG\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u611f\u77e5\u6f02\u79fb\u57fa\u7ebf\uff0c\u5e76\u52a0\u901f\u6536\u655b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u9ad8\u7ea60.9\uff08CIFAR-10\uff09\u548c2.7\uff08CIFAR-100\uff09\uff0c\u5e76\u4e14\u5e73\u5747\u6536\u655b\u5230\u76ee\u6807\u51c6\u786e\u7387\u7684\u901f\u5ea6\u63d0\u9ad8\u4e86\u7ea64.5\u500d\u3002", "conclusion": "FedSSG\u8868\u660e\uff0c\u91c7\u6837\u7edf\u8ba1\u6570\u636e\u53ef\u4ee5\u8f6c\u5316\u4e3a\u4e00\u79cd\u6709\u539f\u5219\u7684\u3001\u5386\u53f2\u611f\u77e5\u7684\u76f8\u4f4d\u63a7\u5236\uff0c\u4ee5\u7a33\u5b9a\u548c\u52a0\u901f\u8054\u90a6\u8bad\u7ec3\u3002"}}
{"id": "2509.14040", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14040", "abs": "https://arxiv.org/abs/2509.14040", "authors": ["Zewen Yang", "Xiaobing Dai", "Dongfa Zhang", "Yu Li", "Ziyang Meng", "Bingkun Huang", "Hamid Sadeghian", "Sami Haddadin"], "title": "Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning", "comment": null, "summary": "Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86Prompt2Auto\uff0c\u4e00\u4e2a\u51e0\u4f55\u4e0d\u53d8\u7684\u4e00\u67aa\u9ad8\u65af\u8fc7\u7a0b\uff08GeoGP\uff09\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u5355\u4e2a\u8fd0\u52a8\u63d0\u793a\u6267\u884c\u4eba\u7c7b\u5f15\u5bfc\u7684\u81ea\u52a8\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u7684\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5750\u6807\u53d8\u6362\uff0c\u800cPrompt2Auto\u901a\u8fc7\u51e0\u4f55\u4e0d\u53d8\u6027\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u5355\u6b21\u6f14\u793a\u5b66\u4e60\u3002", "method": "Prompt2Auto\u4f7f\u7528\u57fa\u4e8e\u5750\u6807\u53d8\u6362\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\uff0c\u5f3a\u5236\u6267\u884c\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u7f29\u653e\u4e0d\u53d8\u6027\uff0c\u5e76\u652f\u6301\u591a\u6b65\u9884\u6d4b\u3002GeoGP\u5bf9\u7528\u6237\u8fd0\u52a8\u63d0\u793a\u4e2d\u7684\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u652f\u6301\u591a\u6280\u80fd\u81ea\u4e3b\u6027\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u4e24\u4e2a\u771f\u5b9e\u7684\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\uff0cPrompt2Auto\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u80fd\u591f\u8de8\u4efb\u52a1\u6cdb\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6f14\u793a\u8d1f\u62c5\u3002", "conclusion": "Prompt2Auto\u662f\u4e00\u4e2a\u6709\u6548\u7684\u3001\u80fd\u591f\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u5355\u6b21\u6f14\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u51e0\u4f55\u4e0d\u53d8\u6027\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u6f14\u793a\u7684\u9700\u8981\u3002"}}
{"id": "2509.13662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "\u901a\u8fc7\u53ef\u5fae\u5206\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u5377\u79ef\u7f51\u7edc\u4e2d\u7684\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5b9e\u73b0\u9ad8\u6548\u80fd\u548c\u9ad8\u7cbe\u5ea6\u7684\u67e5\u627e\u7f51\u7edc\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u5bc6\u96c6\uff0c\u7279\u522b\u662f\u4e58\u6cd5\u8fd0\u7b97\uff0c\u9650\u5236\u4e86\u5176\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u53ef\u5fae\u5206\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5e76\u63d0\u51fa\u8bad\u7ec3\u7b56\u7565\u4ee5\u4f18\u5316\u67e5\u627e\u8868\uff0c\u6784\u5efa\u67e5\u627e\u7f51\u7edc\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u67e5\u627e\u7f51\u7edc\u5728\u80fd\u8017\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u5377\u79ef\u7f51\u7edc\u7684\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "\u67e5\u627e\u7f51\u7edc\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u3001\u56fe\u50cf\u548c\u70b9\u4e91\u6570\u636e\u4e0a\u5747\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14008", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "Hala\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bedNLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u5f25\u5408\u73b0\u6709\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u5904\u7406\u80fd\u529b\u4e0a\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528translate-and-tune\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7FP8\u538b\u7f29\u7684AR\u2194EN\u6559\u5e08\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53cc\u8bed\u76d1\u7763\u6570\u636e\uff0c\u7136\u540e\u5728\u6b64\u6570\u636e\u4e0a\u5fae\u8c03\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578bLFM2-1.2B\uff0c\u6700\u540e\u901a\u8fc7slerp\u5408\u5e76\u6280\u672f\u8bad\u7ec3\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684Hala\u6a21\u578b\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u4e2d\u5fc3\u7684\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHala\u6a21\u578b\u5728\u201cnano\u201d\u548c\u201csmall\u201d\u4e24\u4e2a\u7c7b\u522b\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f18\u4e8e\u5176\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "Hala\u6a21\u578b\u5bb6\u65cf\u5728\u963f\u62c9\u4f2f\u8bedNLP\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u4e86translate-and-tune\u6d41\u6c34\u7ebf\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u793e\u533a\u53d1\u5e03\u4e86\u76f8\u5173\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2509.14125", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14125", "abs": "https://arxiv.org/abs/2509.14125", "authors": ["Kim Vall\u00e9e", "Damian Markham"], "title": "Formalizing contextuality in sequential scenarios", "comment": "25 pages, 1 figure", "summary": "This paper provides a framework for characterizing sequential scenarios,\nallowing for the identification of contextuality given empirical data, and then\nprovides precise operational interpretations in terms of the possible hidden\nvariable model explanations of that data. Sequential scenarios are different in\nessence from non-local scenarios as each measurement instrument is allowed to\nchange the state as it enters subsequent measurement instruments. Thus, it is\nnecessary to formulate the possible state update in any hidden variable model\ndescription. Here we develop such hidden variable models for sequential\nscenarios, and we propose the notion of no-disturbance: an instrument $A$ does\nnot disturb another instrument $B$ if the statistics of $B$ are independent of\nwhether $A$ was measured or not. We define non-contextuality inequalities for\nthe sequential scenario, and show that violation implies that the data cannot\nbe explained by a hidden variable model that is both deterministic and not\ndisturbing in this sense. We further provide a translation from standard\ncontextuality frameworks to ours, providing sequential versions which carry\nover the same inequalities and measures of contextuality, but now with the\nsequential interpretations stated.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8868\u5f81\u987a\u5e8f\u573a\u666f\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u9690\u85cf\u53d8\u91cf\u6a21\u578b\u89e3\u91ca\u7684\u6570\u636e\u64cd\u4f5c\u89e3\u91ca\u3002", "motivation": "\u987a\u5e8f\u573a\u666f\u7684\u6d4b\u91cf\u4eea\u5668\u53ef\u4ee5\u6539\u53d8\u5176\u72b6\u6001\uff0c\u8fd9\u4e0e\u975e\u5c40\u90e8\u573a\u666f\u4e0d\u540c\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u9690\u85cf\u53d8\u91cf\u6a21\u578b\u4e2d\u7684\u72b6\u6001\u66f4\u65b0\u8fdb\u884c\u5047\u8bbe\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u4e8e\u987a\u5e8f\u573a\u666f\u7684\u9690\u85cf\u53d8\u91cf\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u65e0\u5e72\u6270\u201d\u7684\u6982\u5ff5\uff08\u4e00\u4e2a\u4eea\u5668\u662f\u5426\u88ab\u6d4b\u91cf\u4e0d\u5f71\u54cd\u53e6\u4e00\u4e2a\u4eea\u5668\u7684\u7edf\u8ba1\u6570\u636e\uff09\u3002\u63a8\u5bfc\u4e86\u987a\u5e8f\u573a\u666f\u7684\u975e\u4e0a\u4e0b\u6587\u6027\u4e0d\u7b49\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u8fdd\u53cd\u610f\u5473\u7740\u6570\u636e\u65e0\u6cd5\u7528\u786e\u5b9a\u6027\u4e14\u65e0\u5e72\u6270\u7684\u9690\u85cf\u53d8\u91cf\u6a21\u578b\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u6807\u51c6\u4e0a\u4e0b\u6587\u6027\u6846\u67b6\u8f6c\u6362\u4e3a\u987a\u5e8f\u7248\u672c\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u987a\u5e8f\u573a\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u9690\u85cf\u53d8\u91cf\u6a21\u578b\u89e3\u91ca\u3002\u975e\u4e0a\u4e0b\u6587\u6027\u4e0d\u7b49\u5f0f\u7684\u8fdd\u53cd\u53ef\u4ee5\u6392\u9664\u67d0\u4e9b\u7c7b\u578b\u7684\u9690\u85cf\u53d8\u91cf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u975e\u4e0a\u4e0b\u6587\u6027\u4e0d\u7b49\u5f0f\u4e3a\u7406\u89e3\u548c\u5206\u6790\u987a\u5e8f\u573a\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.13802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13802", "abs": "https://arxiv.org/abs/2509.13802", "authors": ["Takuya Kiyokawa", "Ryunosuke Takebayashi", "Kensuke Harada"], "title": "Shell-Type Soft Jig for Holding Objects during Disassembly", "comment": "6 pages, 8 figures", "summary": "This study addresses a flexible holding tool for robotic disassembly. We\npropose a shell-type soft jig that securely and universally holds objects,\nmitigating the risk of component damage and adapting to diverse shapes while\nenabling soft fixation that is robust to recognition, planning, and control\nerrors. The balloon-based holding mechanism ensures proper alignment and stable\nholding performance, thereby reducing the need for dedicated jig design, highly\naccurate perception, precise grasping, and finely tuned trajectory planning\nthat are typically required with conventional fixtures. Our experimental\nresults demonstrate the practical feasibility of the proposed jig through\nperformance comparisons with a vise and a jamming-gripper-inspired soft jig.\nTests on ten different objects further showed representative successes and\nfailures, clarifying the jig's limitations and outlook.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u673a\u5668\u4eba\u62c6\u5378\u7684\u67d4\u6027\u5939\u6301\u5de5\u5177\u2014\u2014\u58f3\u5f0f\u8f6f\u5939\u5177\uff0c\u8be5\u5939\u5177\u80fd\u5b89\u5168\u3001\u901a\u7528\u5730\u5939\u6301\u7269\u4f53\uff0c\u51cf\u5c11\u635f\u574f\u98ce\u9669\uff0c\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\uff0c\u5e76\u80fd\u5bb9\u5fcd\u8bc6\u522b\u3001\u89c4\u5212\u548c\u63a7\u5236\u9519\u8bef\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u88c5\u7f6e\u5728\u673a\u5668\u4eba\u62c6\u5378\u4e2d\u9700\u8981\u4e13\u7528\u8bbe\u8ba1\u3001\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3001\u6293\u53d6\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u67d4\u6027\u5939\u5177\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u58f3\u5f0f\u8f6f\u5939\u5177\uff0c\u91c7\u7528\u57fa\u4e8e\u6c14\u7403\u7684\u5939\u6301\u673a\u5236\uff0c\u5b9e\u73b0\u67d4\u6027\u56fa\u5b9a\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u7684\u7269\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u864e\u94b3\u548c\u57fa\u4e8e\u963b\u585e\u7684\u8f6f\u5939\u5177\u76f8\u6bd4\uff0c\u8be5\u8f6f\u5939\u5177\u5728\u5341\u79cd\u4e0d\u540c\u7269\u4f53\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u4e5f\u660e\u786e\u4e86\u5176\u5c40\u9650\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u58f3\u5f0f\u8f6f\u5939\u5177\u4e3a\u673a\u5668\u4eba\u62c6\u5378\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u9c81\u68d2\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u5bf9\u9ad8\u7cbe\u5ea6\u8bbe\u5907\u548c\u89c4\u5212\u7684\u9700\u6c42\u3002"}}
{"id": "2509.13906", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.13906", "abs": "https://arxiv.org/abs/2509.13906", "authors": ["Afrin Dange", "Sunita Sarawagi"], "title": "TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates", "comment": "Accepted at CIKM 2025", "summary": "Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art\nperformance in univariate forecasting on new time series simply by conditioned\non a brief history of past values. Their success demonstrates that large-scale\npretraining across diverse domains can acquire the inductive bias to generalize\nfrom temporal patterns in a brief history. However, most TSFMs are unable to\nleverage covariates -- future-available exogenous variables critical for\naccurate forecasting in many applications -- due to their domain-specific\nnature and the lack of associated inductive bias. We propose TFMAdapter, a\nlightweight, instance-level adapter that augments TSFMs with covariate\ninformation without fine-tuning. Instead of retraining, TFMAdapter operates on\nthe limited history provided during a single model call, learning a\nnon-parametric cascade that combines covariates with univariate TSFM forecasts.\nHowever, such learning would require univariate forecasts at all steps in the\nhistory, requiring too many calls to the TSFM. To enable training on the full\nhistorical context while limiting TSFM invocations, TFMAdapter uses a two-stage\nmethod: (1) generating pseudo-forecasts with a simple regression model, and (2)\ntraining a Gaussian Process regressor to refine predictions using both pseudo-\nand TSFM forecasts alongside covariates. Extensive experiments on real-world\ndatasets demonstrate that TFMAdapter consistently outperforms both foundation\nmodels and supervised baselines, achieving a 24-27\\% improvement over base\nfoundation models with minimal data and computational overhead. Our results\nhighlight the potential of lightweight adapters to bridge the gap between\ngeneric foundation models and domain-specific forecasting needs.", "AI": {"tldr": "TFMAdapter\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u53ef\u4ee5\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u5229\u7528\u534f\u53d8\u91cf\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u3002\u5b83\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u975e\u53c2\u6570\u7ea7\u8054\u6765\u7ed3\u5408\u534f\u53d8\u91cf\u548cTSFM\u7684\u9884\u6d4b\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u5728\u5355\u53d8\u91cf\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5229\u7528\u534f\u53d8\u91cf\u4fe1\u606f\uff0c\u800c\u534f\u53d8\u91cf\u5bf9\u4e8e\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "TFMAdapter\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5b9e\u4f8b\u7ea7\u9002\u914d\u5668\u6765\u589e\u5f3aTSFM\u3002\u5b83\u5728\u5355\u6b21\u6a21\u578b\u8c03\u7528\u4e2d\uff0c\u5229\u7528\u6709\u9650\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u5b66\u4e60\u4e00\u4e2a\u975e\u53c2\u6570\u7ea7\u8054\uff0c\u5c06\u534f\u53d8\u91cf\u4e0eTSFM\u7684\u5355\u53d8\u91cf\u9884\u6d4b\u76f8\u7ed3\u5408\u3002\u4e3a\u4e86\u5728\u8bad\u7ec3\u65f6\u9650\u5236TSFM\u7684\u8c03\u7528\u6b21\u6570\uff0cTFMAdapter\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u7b80\u5355\u7684\u56de\u5f52\u6a21\u578b\u751f\u6210\u4f2a\u9884\u6d4b\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5668\uff0c\u5229\u7528\u4f2a\u9884\u6d4b\u3001TSFM\u9884\u6d4b\u548c\u534f\u53d8\u91cf\u6765\u4f18\u5316\u9884\u6d4b\u3002", "result": "TFMAdapter\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u76d1\u7763\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u80fd\u591f\u6301\u7eed\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e8624-27%\uff0c\u5e76\u4e14\u6570\u636e\u548c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6709\u6f5c\u529b\u5f25\u5408\u901a\u7528\u57fa\u7840\u6a21\u578b\u4e0e\u7279\u5b9a\u9886\u57df\u9884\u6d4b\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.14075", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14075", "abs": "https://arxiv.org/abs/2509.14075", "authors": ["Yu Li", "Hamid Sadeghian", "Zewen Yang", "Valentin Le Mesle", "Sami Haddadin"], "title": "Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots", "comment": null, "summary": "Robotic-assisted minimally invasive surgery (RAMIS) requires precise\nenforcement of the remote center of motion (RCM) constraint to ensure safe tool\nmanipulation through a trocar. Achieving this constraint under dynamic and\ninteractive conditions remains challenging, as existing control methods either\nlack robustness at the torque level or do not guarantee consistent RCM\nconstraint satisfaction. This paper proposes a constraint-consistent torque\ncontroller that treats the RCM as a rheonomic holonomic constraint and embeds\nit into a projection-based inverse-dynamics framework. The method unifies\ntask-level and kinematic formulations, enabling accurate tool-tip tracking\nwhile maintaining smooth and efficient torque behavior. The controller is\nvalidated both in simulation and on a RAMIS training platform, and is\nbenchmarked against state-of-the-art approaches. Results show improved RCM\nconstraint satisfaction, reduced required torque, and robust performance by\nimproving joint torque smoothness through the consistency formulation under\nclinically relevant scenarios, including spiral trajectories, variable\ninsertion depths, moving trocars, and human interaction. These findings\ndemonstrate the potential of constraint-consistent torque control to enhance\nsafety and reliability in surgical robotics. The project page is available at:\nhttps://rcmpc-cube.github.io", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u4e00\u81f4\u6027\u529b\u77e9\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\uff08RAMIS\uff09\u4e2d\u7cbe\u786e\u6267\u884c\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\uff08RCM\uff09\u7ea6\u675f\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684 RAMIS \u63a7\u5236\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u6216 RCM \u7ea6\u675f\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u548c\u4ea4\u4e92\u5f0f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06 RCM \u89c6\u4e3a\u6d41\u53d8\u5b8c\u6574\u7ea6\u675f\u5e76\u5c06\u5176\u5d4c\u5165\u57fa\u4e8e\u6295\u5f71\u7684\u9006\u52a8\u529b\u5b66\u6846\u67b6\u7684\u7ea6\u675f\u4e00\u81f4\u6027\u529b\u77e9\u63a7\u5236\u5668\u3002\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86\u4efb\u52a1\u7ea7\u548c\u8fd0\u52a8\u5b66\u8868\u8ff0\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5de5\u5177\u5c16\u7aef\u8ddf\u8e2a\u548c\u826f\u597d\u7684\u529b\u77e9\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c RAMIS \u8bad\u7ec3\u5e73\u53f0\u9a8c\u8bc1\uff0c\u8be5\u63a7\u5236\u5668\u5728 RCM \u7ea6\u675f\u6ee1\u8db3\u5ea6\u3001\u6240\u9700\u529b\u77e9\u548c\u5173\u8282\u529b\u77e9\u5e73\u6ed1\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u4e34\u5e8a\u76f8\u5173\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u7ea6\u675f\u4e00\u81f4\u6027\u529b\u77e9\u63a7\u5236\u6709\u6f5c\u529b\u63d0\u9ad8\u624b\u672f\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.13676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528SAM\u751f\u6210\u8bed\u4e49\u8d85\u50cf\u7d20\u6765\u538b\u7f29\u89c6\u89c9\u6807\u8bb0\uff0c\u51cf\u5c11\u4e8693%\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u4ece\u800c\u52a0\u901f\u4e86MLLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684MLLM-SAM\u6846\u67b6\u5728Referring Image Segmentation\uff08RIS\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46MLLM\u7684\u89c6\u89c9\u6807\u8bb0\u5197\u4f59\u5bfc\u81f4\u8ba1\u7b97\u91cf\u5927\u3002\u4f20\u7edf\u7684\u89c6\u89c9\u6295\u5f71\u5668\u96be\u4ee5\u5728\u51cf\u5c11\u6807\u8bb0\u6570\u91cf\u548c\u4fdd\u6301\u8bed\u4e49\u6e05\u6670\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u8d85\u50cf\u7d20\u6765\u8bc6\u522b\u201c\u89c6\u89c9\u8bcd\u201d\uff0c\u5e76\u901a\u8fc7\u538b\u7f29\u548c\u6295\u5f71\u8fd9\u4e9b\u8d85\u50cf\u7d20\u6765\u751f\u6210\u89c6\u89c9\u6807\u8bb0\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u7f29\u77ed\u6807\u8bb0\u5e8f\u5217\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u8bed\u4e49\u8d85\u50cf\u7d20\u4f4d\u7f6e\u5d4c\u5165\u548c\u8bed\u4e49\u8d85\u50cf\u7d20\u805a\u5408\u5668\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u51e0\u4f55\u4f4d\u7f6e\u7684\u611f\u77e5\u548c\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u51cf\u5c11\u4e8693%\uff0c\u540c\u65f6\u6ca1\u6709\u964d\u4f4e\u6027\u80fd\uff0c\u663e\u8457\u52a0\u5feb\u4e86MLLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5728RIS\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u538b\u7f29\u89c6\u89c9\u6295\u5f71\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u89c6\u89c9\u6295\u5f71\u5668\u901a\u8fc7\u538b\u7f29\u8bed\u4e49\u8d85\u50cf\u7d20\u6765\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u4f4d\u7f6e\u5d4c\u5165\u548c\u805a\u5408\u5668\u4fdd\u7559\u5173\u952e\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86RIS\u4efb\u52a1\u4e2dMLLM\u7684\u6548\u7387\u3002"}}
{"id": "2509.14023", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "\u8bed\u97f3\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5e94\u8d85\u8d8a\u6587\u672c\uff0c\u7eb3\u5165\u97f3\u9891\u8bc4\u4f30\uff0c\u4ee5\u83b7\u5f97\u66f4\u81ea\u7136\u3001\u66f4\u53ef\u9760\u7684\u7ffb\u8bd1\u7cfb\u7edf\u6392\u540d\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u5ffd\u7565\u4e86\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u6d89\u53ca\u8bed\u97f3\u7684\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u81ea\u7136\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u65b9\u5f0f\uff08Amazon Mechanical Turk\uff09\u5bf910\u4e2a\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6587\u672c\u548c\u97f3\u9891\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u548c\u81ea\u590d\u5236\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u97f3\u9891\u8bc4\u4f30\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u57fa\u4e8e\u97f3\u9891\u7684\u4f17\u5305\u8bc4\u4f30\u7ed3\u679c\u4e0e\u57fa\u4e8e\u6587\u672c\u7684\u8bc4\u4f30\u7ed3\u679c\u5728\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u4e00\u81f4\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u533a\u5206\u51fa\u4e0d\u540c\u7684\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u8fd9\u5f52\u56e0\u4e8e\u8bed\u97f3\u4f5c\u4e3a\u4e00\u79cd\u66f4\u4e30\u5bcc\u3001\u66f4\u81ea\u7136\u7684\u6a21\u5f0f\u3002", "conclusion": "\u8bed\u97f3\u8bc4\u4f30\u5e94\u88ab\u7eb3\u5165\u672a\u6765\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\u4e2d\uff0c\u56e0\u4e3a\u5b83\u80fd\u63d0\u4f9b\u6bd4\u7eaf\u6587\u672c\u8bc4\u4f30\u66f4\u5168\u9762\u3001\u66f4\u53ef\u9760\u7684\u7ffb\u8bd1\u7cfb\u7edf\u6bd4\u8f83\u3002"}}
{"id": "2509.14163", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14163", "abs": "https://arxiv.org/abs/2509.14163", "authors": ["Chi-Sheng Chen", "En-Jui Kuo"], "title": "Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures", "comment": null, "summary": "Diffusion models typically employ static or heuristic classifier-free\nguidance (CFG) schedules, which often fail to adapt across timesteps and noise\nconditions. In this work, we introduce a quantum reinforcement learning (QRL)\ncontroller that dynamically adjusts CFG at each denoising step. The controller\nadopts a hybrid quantum--classical actor--critic architecture: a shallow\nvariational quantum circuit (VQC) with ring entanglement generates policy\nfeatures, which are mapped by a compact multilayer perceptron (MLP) into\nGaussian actions over $\\Delta$CFG, while a classical critic estimates value\nfunctions. The policy is optimized using Proximal Policy Optimization (PPO)\nwith Generalized Advantage Estimation (GAE), guided by a reward that balances\nclassification confidence, perceptual improvement, and action regularization.\nExperiments on CIFAR-10 demonstrate that our QRL policy improves perceptual\nquality (LPIPS, PSNR, SSIM) while reducing parameter count compared to\nclassical RL actors and fixed schedules. Ablation studies on qubit number and\ncircuit depth reveal trade-offs between accuracy and efficiency, and extended\nevaluations confirm robust generation under long diffusion schedules.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e00\u79cd\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\uff08QRL\uff09\u63a7\u5236\u5668\uff0c\u52a8\u6001\u8c03\u6574\u6269\u6563\u6a21\u578b\u4e2d\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u5e76\u51cf\u5c11\u53c2\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u4f7f\u7528\u9759\u6001\u6216\u542f\u53d1\u5f0fCFG\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u6b65\u548c\u566a\u58f0\u6761\u4ef6\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178 actor-critic \u67b6\u6784\u7684QRL\u63a7\u5236\u5668\u3002\u5176\u4e2d\uff0c\u4f7f\u7528\u6d45\u5c42\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u751f\u6210\u7b56\u7565\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u6620\u5c04\u4e3a$\\\\\\Delta$CFG\u7684\u52a8\u4f5c\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u7ecf\u5178Critic\u4f30\u8ba1\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1\uff08GAE\uff09\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002\u5956\u52b1\u51fd\u6570\u7ed3\u5408\u4e86\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u3001\u611f\u77e5\u6539\u8fdb\u548c\u52a8\u4f5c\u6b63\u5219\u5316\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60Actor\u548c\u56fa\u5b9a\u8c03\u5ea6\u76f8\u6bd4\uff0cQRL\u7b56\u7565\u5728\u63d0\u5347\u611f\u77e5\u8d28\u91cf\uff08LPIPS\u3001PSNR\u3001SSIM\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\u5bf9\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5728\u957f\u6269\u6563\u8c03\u5ea6\u4e0b\u751f\u6210\u6548\u679c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "QRL\u63a7\u5236\u5668\u80fd\u591f\u52a8\u6001\u8c03\u6574CFG\uff0c\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.13815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13815", "abs": "https://arxiv.org/abs/2509.13815", "authors": ["Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "Soft Regrasping Tool Inspired by Jamming Gripper", "comment": "6 pages, 9 figures", "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7 the jamming transition \u73b0\u8c61\u542f\u53d1\u7684\u8f6f\u5939\u5177\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u88c5\u914d\u4e2d\u7684\u91cd\u6293\u53d6\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u521a\u6027\u5939\u5177\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u5939\u5177\u5728\u9002\u5e94\u4e0d\u540c\u96f6\u4ef6\u5f62\u72b6\u65b9\u9762\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u96f6\u4ef6\u8fdb\u884c\u4e13\u95e8\u8bbe\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u88c5\u914d\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5c06\u4e09\u89d2\u5f62\u9525\u5f62\u5de5\u5177\u538b\u5165\u819c\u4e2d\u5e76\u6392\u51fa\u7a7a\u6c14\u6765\u5f62\u6210\u4e00\u4e2a\u53ef\u5bb9\u7eb3\u96f6\u4ef6\u7684\u7a33\u5b9a\u7a7a\u8154\uff0c\u5e76\u4f18\u5316\u51b2\u538b\u6df1\u5ea6\u4ee5\u5e73\u8861\u653e\u7f6e\u7a33\u5b9a\u6027\u548c\u6293\u624b\u53ef\u53ca\u6027\u3002", "result": "\u5728\u5bf9\u5341\u4e2a\u4e0d\u540c\u5f62\u72b6\u7684\u673a\u68b0\u96f6\u4ef6\u8fdb\u884c\u653e\u7f6e\u5b9e\u9a8c\u540e\uff0c\u5927\u591a\u6570\u96f6\u4ef6\u7684\u653e\u7f6e\u6210\u529f\u7387\u8d85\u8fc780%\uff0c\u5706\u67f1\u5f62\u96f6\u4ef6\u7684\u6210\u529f\u7387\u8d85\u8fc790%\u3002\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u662f\u51e0\u4f55\u7ea6\u675f\u548c\u819c\u7684\u5c5e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f6f\u5939\u5177\u80fd\u591f\u5b9e\u73b0\u901a\u7528\u3001\u7cbe\u786e\u548c\u53ef\u91cd\u590d\u7684\u91cd\u6293\u53d6\uff0c\u4e3a\u88c5\u914d\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u521a\u6027\u5939\u5177\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u6f5c\u529b\u3002"}}
{"id": "2509.13908", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13908", "abs": "https://arxiv.org/abs/2509.13908", "authors": ["Priyobrata Mondal", "Faizanuddin Ansari", "Swagatam Das"], "title": "APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness", "comment": null, "summary": "Ensuring fairness in machine learning models is critical, especially when\nbiases compound across intersecting protected attributes like race, gender, and\nage. While existing methods address fairness for single attributes, they fail\nto capture the nuanced, multiplicative biases faced by intersectional\nsubgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first\nframework to explicitly model intersectional fairness as a joint optimization\nproblem over the Cartesian product of sensitive attributes. APFEx combines\nthree key innovations- (1) an adaptive multi-objective optimizer that\ndynamically switches between Pareto cone projection, gradient weighting, and\nexploration strategies to navigate fairness-accuracy trade-offs, (2)\ndifferentiable intersectional fairness metrics enabling gradient-based\noptimization of non-smooth subgroup disparities, and (3) theoretical guarantees\nof convergence to Pareto-optimal solutions. Experiments on four real-world\ndatasets demonstrate APFEx's superiority, reducing fairness violations while\nmaintaining competitive accuracy. Our work bridges a critical gap in fair ML,\nproviding a scalable, model-agnostic solution for intersectional fairness.", "AI": {"tldr": "APFEx\u662f\u7b2c\u4e00\u4e2a\u89e3\u51b3\u591a\u91cd\u4ea4\u53c9\u516c\u5e73\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89e3\u51b3\u7cbe\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u56e0\u79cd\u65cf\u3001\u6027\u522b\u548c\u5e74\u9f84\u7b49\u4ea4\u53c9\u53d7\u4fdd\u62a4\u5c5e\u6027\u800c\u4ea7\u751f\u7684\u590d\u5408\u504f\u5dee\uff0c\u5bfc\u81f4\u5bf9\u4ea4\u53c9\u7fa4\u4f53\u5b58\u5728\u7ec6\u5fae\u7684\u3001\u4e58\u6027\u7684\u504f\u5dee\u3002", "method": "APFEx\u91c7\u7528\u81ea\u9002\u5e94\u591a\u76ee\u6807\u4f18\u5316\u5668\uff0c\u8be5\u4f18\u5316\u5668\u80fd\u591f\u52a8\u6001\u5207\u6362\u5e15\u7d2f\u6258\u9525\u6295\u5f71\u3001\u68af\u5ea6\u52a0\u6743\u548c\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u516c\u5e73-\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5b83\u8fd8\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u4ea4\u53c9\u516c\u5e73\u6027\u6307\u6807\u6765\u5b9e\u73b0\u975e\u5e73\u6ed1\u5b50\u7fa4\u5dee\u5f02\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u5e76\u5177\u6709\u6536\u655b\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPFEx\u5728\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u516c\u5e73\u6027\u8fdd\u89c4\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u4ea4\u53c9\u516c\u5e73\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "APFEx\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4ea4\u53c9\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2509.13681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "FishBEV\u662f\u4e00\u4e2a\u4e3a\u9c7c\u773c\u76f8\u673a\u8bbe\u8ba1\u7684\u65b0\u9896BEV\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e09\u79cd\u521b\u65b0\u673a\u5236\u89e3\u51b3\u4e86\u9c7c\u773c\u76f8\u673a\u5e26\u6765\u7684\u51e0\u4f55\u7578\u53d8\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u7b49\u6311\u6218\uff0c\u5e76\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9488\u5b54\u76f8\u673a\u7684BEV\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u9c7c\u773c\u76f8\u673a\uff0c\u56e0\u4e3a\u9c7c\u773c\u76f8\u673a\u5b58\u5728\u4e25\u91cd\u7684\u51e0\u4f55\u7578\u53d8\u3001\u6a21\u7cca\u7684\u591a\u89c6\u56fe\u5bf9\u5e94\u4ee5\u53ca\u4e0d\u7a33\u5b9a\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4e25\u91cd\u5f71\u54cd\u4e86BEV\u6027\u80fd\u3002", "method": "FishBEV\u6846\u67b6\u5f15\u5165\u4e86\u4e09\u79cd\u4e92\u8865\u7684\u521b\u65b0\uff1a1. \u7578\u53d8\u9c81\u68d2\u591a\u5c3a\u5ea6\u63d0\u53d6\uff08DRME\uff09\u9aa8\u5e72\u7f51\u7edc\uff0c\u7528\u4e8e\u5728\u7578\u53d8\u4e0b\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u5e76\u4fdd\u6301\u5c3a\u5ea6\u4e00\u81f4\u6027\uff1b2. \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\uff08U-SCA\uff09\u673a\u5236\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u8de8\u89c6\u56fe\u5bf9\u9f50\uff1b3. \u8ddd\u79bb\u611f\u77e5\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\uff08D-TSA\uff09\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5730\u5e73\u8861\u8fd1\u573a\u7ec6\u8282\u548c\u8fdc\u573a\u4e0a\u4e0b\u6587\u4ee5\u786e\u4fdd\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFishBEV\u5728\u73af\u7ed5\u89c6\u56fe\u9c7c\u773cBEV\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u9762\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FishBEV\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684DRME\u3001U-SCA\u548cD-TSA\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9c7c\u773c\u76f8\u673a\u5728BEV\u5206\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.14031", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Pawe\u0142 M\u0105ka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "\u6a21\u578b\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u76f8\u5173\u7ffb\u8bd1\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u7a00\u758f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\u6bd4\u4f8b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u786e\u8ba4\u4e86\u7a00\u758f\u6027\u662f\u5173\u952e\u74f6\u9888\u3002\u540c\u65f6\u53d1\u73b0\uff0c\u4e00\u79cd\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6539\u8fdb\u4e0d\u9002\u7528\u4e8e\u5176\u4ed6\u73b0\u8c61\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e5f\u6709\u9650\u3002\u6700\u540e\u63d0\u51fa\u4e86\u4e24\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u5355\u8bed\u548c\u591a\u8bed\u73af\u5883\u4e0b\u5206\u522b\u63d0\u5347\u4e866%\u548c8%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7ffb\u8bd1\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\uff08\u5982\u4ee3\u8bcd\u6d88\u6b67\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u88ab\u8ba4\u4e3a\u662f\u7531\u4e8e\u6807\u51c6\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u7684\u4f8b\u5b50\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u4e0d\u540c\u6bd4\u4f8b\u4e0a\u4e0b\u6587\u76f8\u5173\u793a\u4f8b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5730\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u7a00\u758f\u6027\u5bf9\u5355\u8bed\u548c\u591a\u8bed\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5206\u6790\u4e86\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6cdb\u5316\u6027\u4ee5\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u3002\u6700\u540e\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u65e8\u5728\u5229\u7528\u73b0\u6709\u6570\u636e\u4ee5\u6539\u8fdb\u4e0a\u4e0b\u6587\u5229\u7528\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u7a00\u758f\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u786e\u8ba4\u7a00\u758f\u6027\u662f\u5173\u952e\u74f6\u9888\u3002\u53d1\u73b0\u6539\u8fdb\u4e00\u79cd\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6027\u80fd\u5e76\u4e0d\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u73b0\u8c61\u3002\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u6709\u9650\uff0c\u5e76\u975e\u5728\u540c\u4e00\u8bed\u7cfb\u7684\u8bed\u8a00\u4e4b\u95f4\u6709\u663e\u8457\u66f4\u9ad8\u7684\u8fc1\u79fb\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u5355\u8bed\u548c\u591a\u8bed\u8bbe\u7f6e\u4e0b\u5206\u522b\u5e26\u6765\u4e86\u9ad8\u8fbe6%\u548c8%\u7684ctxPro\u8bc4\u4f30\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u7a00\u758f\u6027\u662f\u5bfc\u81f4\u7ffb\u8bd1\u6a21\u578b\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u76f8\u5173\u4efb\u52a1\u65f6\u6027\u80fd\u4e0d\u4f73\u7684\u5173\u952e\u56e0\u7d20\u3002\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6539\u8fdb\u4e0d\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e14\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u6709\u9650\u3002\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u6709\u6548\u5229\u7528\u73b0\u6709\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u7ffb\u8bd1\u51c6\u786e\u7387\u3002"}}
{"id": "2509.14164", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14164", "abs": "https://arxiv.org/abs/2509.14164", "authors": ["M. Javad Zakeri", "Armando Perez-Leija", "Andrea Blanco-Redondo"], "title": "High-dimensional topological photonic entanglement", "comment": null, "summary": "The robust generation and manipulation of high-dimensional quantum states\nlies at the heart of modern quantum computation. The use of topology to\nresiliently encode and transport quantum information has been widely\ninvestigated in condensed matter and has recently penetrated quantum photonics.\nHowever, a route to scale up to a large number of entangled topological\nphotonic modes had been missing. Here, we propose and experimentally\ndemonstrate a method to generate high-dimensional topological photonic\nentanglement. Our platform relies on carefully designed silicon photonic\nwaveguide topological superlattices, which support nonlinear generation of\nenergy-time entangled photon pairs on a superposition of multiple topological\nmodes. Our measurements and theoretical analysis reveal entanglement of up to\nfive topological modes with resilience to nanofabrication imperfections. This\nstudy, at the intersection of nonlinear integrated photonics, quantum\ninformation, and topology, opens a research avenue toward scalable,\nfault-tolerant quantum photonic states.", "AI": {"tldr": "A method for generating and experimentally demonstrating high-dimensional topological photonic entanglement using silicon photonic waveguide topological superlattices has been developed, showing entanglement of up to five modes with resilience to imperfections, paving the way for scalable, fault-tolerant quantum photonic states.", "motivation": "The robust generation and manipulation of high-dimensional quantum states are crucial for quantum computation. While topology has been used to encode and transport quantum information in condensed matter and photonics, a scalable method for entangled topological photonic modes was lacking.", "method": "The proposed method utilizes silicon photonic waveguide topological superlattices to generate energy-time entangled photon pairs on a superposition of multiple topological modes. This is achieved through nonlinear generation within the superlattices.", "result": "The experiment successfully demonstrated entanglement of up to five topological modes. The system showed resilience to nanofabrication imperfections, as confirmed by theoretical analysis and measurements.", "conclusion": "This research introduces a novel route toward scalable, fault-tolerant quantum photonic states by integrating nonlinear integrated photonics, quantum information, and topology. The demonstrated method for generating high-dimensional topological photonic entanglement is a significant step forward for the field."}}
{"id": "2509.13816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13816", "abs": "https://arxiv.org/abs/2509.13816", "authors": ["Yude Li", "Zhexuan Zhou", "Huizhe Li", "Youmin Gong", "Jie Mei"], "title": "Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation", "comment": null, "summary": "Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex\nenvironments is a critical capability. However, modern end-to-end navigation\nfaces a key challenge: the high-frequency control loop needed for agile flight\nconflicts with low-frequency perception streams, which are limited by sensor\nupdate rates and significant computational cost. This mismatch forces\nconventional synchronous models into undesirably low control rates. To resolve\nthis, we propose an asynchronous reinforcement learning framework that\ndecouples perception and control, enabling a high-frequency policy to act on\nthe latest IMU state for immediate reactivity, while incorporating perception\nfeatures asynchronously. To manage the resulting data staleness, we introduce a\ntheoretically-grounded Temporal Encoding Module (TEM) that explicitly\nconditions the policy on perception delays, a strategy complemented by a\ntwo-stage curriculum to ensure stable and efficient training. Validated in\nextensive simulations, our method was successfully deployed in zero-shot\nsim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate\nand demonstrates robust, agile navigation in cluttered real-world environments.\nOur source code will be released for community reference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u9ad8\u9891\u63a7\u5236\u4e0e\u4f4e\u9891\u611f\u77e5\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9891\u54cd\u5e94\u548c\u7a33\u5065\u7684\u654f\u6377\u5bfc\u822a\u3002", "motivation": "\u9ad8\u9891\u63a7\u5236\u4e0e\u4f4e\u9891\u611f\u77e5\u4e4b\u95f4\u7684\u51b2\u7a81\u9650\u5236\u4e86\u4f20\u7edf\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u548c\u63a7\u5236\u5206\u79bb\uff0c\u5e76\u5f15\u5165\u4e86\u65f6\u95f4\u7f16\u7801\u6a21\u5757\uff08TEM\uff09\u6765\u5904\u7406\u6570\u636e\u5ef6\u8fdf\uff0c\u540c\u65f6\u4f7f\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u7a33\u5065\u3001\u654f\u6377\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86100Hz\u7684\u63a7\u5236\u9891\u7387\u3002", "conclusion": "\u8be5\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u9ad8\u9891\u63a7\u5236\u548c\u4f4e\u9891\u611f\u77e5\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2509.13914", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13914", "abs": "https://arxiv.org/abs/2509.13914", "authors": ["Divya Thuremella", "Yi Yang", "Simon Wanna", "Lars Kunze", "Daniele De Martini"], "title": "Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction", "comment": "Accepted 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "This work explores the application of ensemble modeling to the\nmultidimensional regression problem of trajectory prediction for vehicles in\nurban environments. As newer and bigger state-of-the-art prediction models for\nautonomous driving continue to emerge, an important open challenge is the\nproblem of how to combine the strengths of these big models without the need\nfor costly re-training. We show how, perhaps surprisingly, combining\nstate-of-the-art deep learning models out-of-the-box (without retraining or\nfine-tuning) with a simple confidence-weighted average method can enhance the\noverall prediction. Indeed, while combining trajectory prediction models is not\nstraightforward, this simple approach enhances performance by 10% over the best\nprediction model, especially in the long-tailed metrics. We show that this\nperformance improvement holds on both the NuScenes and Argoverse datasets, and\nthat these improvements are made across the dataset distribution. The code for\nour work is open source.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u96c6\u6210\u8d77\u6765\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd10%\uff0c\u5c24\u5176\u662f\u5728\u957f\u5c3e\u6307\u6807\u4e0a\uff0c\u5e76\u5728NuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\u5747\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u51fa\u73b0\uff0c\u5982\u4f55\u6709\u6548\u7ed3\u5408\u8fd9\u4e9b\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u9884\u8bad\u7ec3\u7684\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u96c6\u6210\u3002", "result": "\u5728NuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u5c06\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u4e8610%\uff0c\u5e76\u4e14\u5728\u957f\u5c3e\u6307\u6807\u4e0a\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u5373\u4f7f\u662f\u7b80\u5355\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5982\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\uff0c\u4e5f\u80fd\u5728\u4e0d\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u591a\u4e2a\u5148\u8fdb\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2509.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u6837\u6761\u57fa\u7840\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u5177\u6709\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53c2\u6570\u91cf\u5c11\u7b49\u4f18\u70b9\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u5bf9\u533b\u5b66\u56fe\u50cf\u8fdb\u884c\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6837\u6761\u57fa\u7840\u7684KANs\u6a21\u578b\uff08SBTAYLOR-KAN, SBRBF-KAN, SBWAVELET-KAN\uff09\uff0c\u5229\u7528\u6837\u6761\u51fd\u6570\u903c\u8fd1\u80fd\u529b\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u975e\u7ebf\u6027\u3002\u6a21\u578b\u65e0\u9700\u9884\u5904\u7406\uff0c\u53ef\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7Grad-CAM\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "SBTAYLOR-KAN\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\uff08\u8111MRI\u3001\u80f8\u90e8X\u5149\u3001\u80ba\u7ed3\u6838X\u5149\u3001\u76ae\u80a4\u75c5\u53d8\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u8fbe98.93%\uff0c\u4e14\u5728\u4ec5\u4f7f\u752830%\u6570\u636e\u65f6\u4ecd\u4fdd\u630186%\u4ee5\u4e0a\u51c6\u786e\u7387\u3002\u8be5\u6a21\u578b\u53c2\u6570\u91cf\u8fdc\u5c11\u4e8e\u4f20\u7edfCNN\uff082,872 vs 24.18M\uff09\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u8be5\u6837\u6761\u57fa\u7840KANs\u6846\u67b6\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6709\u9650\u6570\u636e\u96c6\u548c\u6570\u636e\u7a00\u758f\u573a\u666f\u4e0b\u7684\u4e34\u5e8aAI\u5e94\u7528\u6311\u6218\u3002"}}
{"id": "2509.14034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "LLM\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u7cfb\u7edf\u4e2d\u5b58\u5728\u6c9f\u901a\u548c\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConfMAD\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u7cfb\u7edf\u867d\u7136\u5229\u7528\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u6a21\u62df\u8fa9\u8bba\uff0c\u4f46LLMs\u5728\u8868\u8fbe\u5176\u4f18\u52bf\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u4e0d\u6070\u5f53\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u4f1a\u5bfc\u81f4\u4ee3\u7406\u4eba\u56fa\u5b88\u9519\u8bef\u4fe1\u5ff5\u6216\u8fc7\u65e9\u6536\u655b\u5230\u6b21\u4f18\u7b54\u6848\uff0c\u4ece\u800c\u964d\u4f4e\u8fa9\u8bba\u6548\u679c\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConfMAD\u7684MAD\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u6574\u4e2a\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u6574\u5408\u4e86\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u4f7fLLMs\u80fd\u591f\u660e\u786e\u4f20\u8fbe\u5176\u7f6e\u4fe1\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u7f6e\u4fe1\u5ea6\u5982\u4f55\u5f71\u54cd\u8fa9\u8bba\u52a8\u6001\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u673a\u5236\uff0c\u7528\u4e8e\u589e\u5f3aMAD\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684MAD\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.14196", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.14196", "abs": "https://arxiv.org/abs/2509.14196", "authors": ["Talal Ahmed Chowdhury", "Vladimir Korepin", "Vincent R. Pascuzzi", "Kwangmin Yu"], "title": "Quantum Utility in Simulating the Real-time Dynamics of the Fermi-Hubbard Model using Superconducting Quantum Computers", "comment": "18 pages, 10 figures", "summary": "The Fermi-Hubbard model is a fundamental model in condensed matter physics\nthat describes strongly correlated electrons. On the other hand, quantum\ncomputers are emerging as powerful tools for exploring the complex dynamics of\nthese quantum many-body systems. In this work, we demonstrate the quantum\nsimulation of the one-dimensional Fermi-Hubbard model using IBM's\nsuperconducting quantum computers, employing over 100 qubits. We introduce a\nfirst-order Trotterization scheme and extend it to an optimized second-order\nTrotterization for the time evolution in the Fermi-Hubbard model, specifically\ntailored for the limited qubit connectivity of quantum architectures, such as\nIBM's platforms. Notably, both Trotterization approaches are scalable and\nmaintain a constant circuit depth at each Trotter step, regardless of the qubit\ncount, enabling us to precisely investigate the relaxation dynamics in the\nFermi-Hubbard model by measuring the expectation value of the N\\'eel observable\n(staggered magnetization) for time-evolved quantum states. Finally, our\nsuccessful measurement of expectation values in such large-scale quantum\nmany-body systems, especially at longer time scales with larger entanglement,\nhighlights the quantum utility of superconducting quantum platforms over\nconventional classical approximation methods.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u4f7f\u7528100\u591a\u91cf\u5b50\u6bd4\u7279\u7684IBM\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u673a\u6a21\u62df\u4e86\u4e00\u7ef4\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u9636\u548c\u4e8c\u9636Trotter\u5316\u65b9\u6848\uff0c\u4ee5\u7814\u7a76\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\uff0c\u6d4b\u91cf\u4e86N'eel\u53ef\u89c2\u6d4b\u91cf\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u5904\u7406\u590d\u6742\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\u662f\u63cf\u8ff0\u5f3a\u5173\u8054\u7535\u5b50\u7684\u57fa\u672c\u6a21\u578b\uff0c\u800c\u91cf\u5b50\u8ba1\u7b97\u673a\u4e3a\u63a2\u7d22\u8fd9\u4e9b\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u590d\u6742\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002", "method": "\u4f7f\u7528IBM\u7684\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u673a\uff0c\u5229\u7528\u4e00\u9636\u548c\u4f18\u5316\u7684\u4e8c\u9636Trotter\u5316\u65b9\u6848\u6765\u6a21\u62df\u4e00\u7ef4\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u6d4b\u91cfN'eel\u53ef\u89c2\u6d4b\u91cf\u3002", "result": "\u6210\u529f\u5730\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5927\u89c4\u6a21\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u6d4b\u91cf\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u66f4\u5927\u7ea0\u7f20\u7684\u8f83\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u5728\u5904\u7406\u5927\u5c3a\u5ea6\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u65b9\u9762\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7ecf\u5178\u8fd1\u4f3c\u65b9\u6cd5\u7684\u91cf\u5b50\u4f18\u52bf\u3002"}}
{"id": "2509.13711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff09\u7684\u8fdb\u6b65\uff0c\u4f7f\u5f97\u5b83\u4eec\u6709\u53ef\u80fd\u88ab\u6ee5\u7528\uff0c\u4f8b\u5982\u5ec9\u4ef7\u5730\u590d\u5236\u827a\u672f\u5bb6\u7684\u521b\u4f5c\u98ce\u683c\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a StyleProtect \u7684\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u66f4\u65b0\u9009\u5b9a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u6709\u6548\u9632\u5fa1\u7ecf\u8fc7\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u5bf9\u827a\u672f\u98ce\u683c\u7684\u6a21\u4eff\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\uff0c\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f7f\u5f97\u6076\u610f\u884c\u4e3a\u8005\u80fd\u591f\u5ec9\u4ef7\u5730\u590d\u5236\u827a\u672f\u5bb6\u7684\u521b\u4f5c\u98ce\u683c\uff0c\u8fd9\u4fc3\u4f7f\u4eba\u4eec\u9700\u8981\u7814\u7a76\u4fdd\u62a4\u827a\u672f\u4f5c\u54c1\u514d\u53d7\u98ce\u683c\u6a21\u4eff\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a StyleProtect \u7684\u9ad8\u6548\u8f7b\u91cf\u7ea7\u4fdd\u62a4\u7b56\u7565\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u8bc6\u522b\u5bf9\u827a\u672f\u98ce\u683c\u7279\u522b\u654f\u611f\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u4ec5\u66f4\u65b0\u8fd9\u4e9b\u9009\u5b9a\u7684\u5c42\u6765\u5b9e\u73b0\u3002\u4ed6\u4eec\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5c42\u5bf9\u98ce\u683c\u548c\u5185\u5bb9\u8868\u793a\u7684\u6fc0\u6d3b\u5f3a\u5ea6\uff0c\u5e76\u8bc4\u4f30\u8fd9\u4e9b\u6fc0\u6d3b\u5f3a\u5ea6\u4e0e\u4ece\u5916\u90e8\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u8861\u91cf\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStyleProtect \u80fd\u591f\u6709\u6548\u5730\u9632\u5fa1\u9488\u5bf9\u5fae\u8c03\u6269\u6563\u6a21\u578b\u7684\u827a\u672f\u98ce\u683c\u6a21\u4eff\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u72ec\u7279\u827a\u672f\u4f5c\u54c1\u548c\u52a8\u753b\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u6027\u80fd\u3002", "conclusion": "StyleProtect \u662f\u4e00\u79cd\u6709\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0\u9009\u5b9a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u4fdd\u62a4\u827a\u672f\u4f5c\u54c1\u514d\u53d7\u7ecf\u8fc7\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u6a21\u4eff\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002"}}
{"id": "2509.14036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u7684 sign \u8bed\u8a00\u7ffb\u8bd1 (QB-SLT) \u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u63d0\u9ad8\u7ffb\u8bd1\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SSL-SSAW \u7684\u878d\u5408\u65b9\u6cd5\uff0c\u5728 CSL-Daily-QA \u548c PHOENIX-2014T-QA \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5f25\u5408\u804b\u54d1\u4eba\u548c\u5065\u542c\u4eba\u4e4b\u95f4\u7684\u6c9f\u901a\u9e3f\u6c9f\uff0c\u5e76\u5229\u7528\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6765\u6539\u8fdb sign \u8bed\u8a00\u7ffb\u8bd1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SSL-SSAW \u7684\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u5f15\u5165 Sigmoid \u81ea\u6ce8\u610f\u529b\u52a0\u6743 (SSAW) \u6a21\u5757\u8fdb\u884c\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u5229\u7528\u95ee\u53e5\u6587\u672c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u8868\u793a\u548c\u7ffb\u8bd1\u80fd\u529b\u3002", "result": "\u5728 CSL-Daily-QA \u548c PHOENIX-2014T-QA \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u95ee\u53e5\u8f85\u52a9\u53ef\u4ee5\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u8bcd\u6761\u8f85\u52a9\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5bf9\u8bdd\u5728\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u95ee\u9898\u7684 sign \u8bed\u8a00\u7ffb\u8bd1 (QB-SLT) \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u63d0\u9ad8\u7ffb\u8bd1\u6548\u7387\uff0cSSL-SSAW \u878d\u5408\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u53d6\u548c\u5229\u7528\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684 sign \u8bed\u8a00\u7ffb\u8bd1\u3002"}}
{"id": "2509.14212", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14212", "abs": "https://arxiv.org/abs/2509.14212", "authors": ["Georgios N. Tsigaridas", "Aristides I. Kechriniotis", "Christos A. Tsonos", "Konstantinos K. Delibasis"], "title": "Localized degenerate solutions to the massless Dirac and Weyl equations", "comment": null, "summary": "In this article we present a general class of degenerate solutions to the\nmassless Dirac and Weyl equations, which can describe localized particles\nand/or particles with variable energy, that could be interpreted as virtual\nparticles appearing either in the vacuum or in materials supporting massless\nDirac or Weyl particles. These solutions exist in a wide range of\nelectromagnetic 4-potentials and fields, which are explicitly calculated. In\naddition, we describe a method for controlling the transverse spatial\ndistribution of Weyl particles and separating them according to their helicity\nand direction of motion using suitable magnetic fields.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u7c7b\u8d28\u91cf\u9a6c\u7ea6\u62c9\u7eb3\u8d39\u7c73\u5b50\u7684\u9000\u5316\u89e3\uff0c\u53ef\u4ee5\u63cf\u8ff0\u5c40\u57df\u5316\u7684\u7c92\u5b50\u548c/\u6216\u5177\u6709\u53ef\u53d8\u80fd\u91cf\u7684\u7c92\u5b50\uff0c\u8fd9\u53ef\u80fd\u88ab\u89e3\u91ca\u4e3a\u51fa\u73b0\u5728\u771f\u7a7a\u6216\u652f\u6301\u8d28\u91cf\u9a6c\u7ea6\u62c9\u7eb3\u8d39\u7c73\u5b50\u7684\u6750\u6599\u4e2d\u7684\u865a\u7c92\u5b50\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u7c7b\u8d28\u91cf\u9a6c\u7ea6\u62c9\u7eb3\u8d39\u7c73\u5b50\u7684\u9000\u5316\u89e3\uff0c\u4ee5\u63cf\u8ff0\u5c40\u57df\u5316\u7684\u7c92\u5b50\u548c/\u6216\u5177\u6709\u53ef\u53d8\u80fd\u91cf\u7684\u7c92\u5b50\uff0c\u5e76\u5c06\u5176\u89e3\u91ca\u4e3a\u865a\u7c92\u5b50\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u8ba1\u7b97\u5f97\u5230\u7684\u7535\u78c14\u52bf\u548c\u573a\uff0c\u4ee5\u53ca\u4f7f\u7528\u5408\u9002\u7684\u78c1\u573a\u6765\u63a7\u5236\u5916\u5c14\u7c92\u5b50\u7684\u6a2a\u5411\u7a7a\u95f4\u5206\u5e03\u548c\u6839\u636e\u5176\u87ba\u65cb\u5ea6\u548c\u8fd0\u52a8\u65b9\u5411\u5206\u79bb\u5b83\u4eec\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u7c7b\u9000\u5316\u7684\u8d28\u91cf\u9a6c\u7ea6\u62c9\u7eb3\u8d39\u7c73\u5b50\u89e3\uff0c\u5e76\u8ba1\u7b97\u4e86\u76f8\u5e94\u7684\u7535\u78c14\u52bf\u548c\u573a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u5916\u5c14\u7c92\u5b50\u6a2a\u5411\u7a7a\u95f4\u5206\u5e03\u5e76\u6309\u87ba\u65cb\u5ea6\u548c\u8fd0\u52a8\u65b9\u5411\u5206\u79bb\u5b83\u4eec\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u7c7b\u8d28\u91cf\u9a6c\u7ea6\u62c9\u7eb3\u8d39\u7c73\u5b50\u7684\u9000\u5316\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u548c\u5206\u79bb\u5916\u5c14\u7c92\u5b50\u6a2a\u5411\u7a7a\u95f4\u5206\u5e03\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13832", "abs": "https://arxiv.org/abs/2509.13832", "authors": ["Teng Wang", "Haojun Jiang", "Yuxuan Wang", "Zhenguo Sun", "Xiangjie Yan", "Xiang Li", "Gao Huang"], "title": "UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography", "comment": null, "summary": "Carotid ultrasound is crucial for the assessment of cerebrovascular health,\nparticularly the internal carotid artery (ICA). While previous research has\nexplored automating carotid ultrasound, none has tackled the challenging ICA.\nThis is primarily due to its deep location, tortuous course, and significant\nindividual variations, which greatly increase scanning complexity. To address\nthis, we propose a Hierarchical Transformer-based decision architecture, namely\nUltraHiT, that integrates high-level variation assessment with low-level action\ndecision. Our motivation stems from conceptualizing individual vascular\nstructures as morphological variations derived from a standard vascular model.\nThe high-level module identifies variation and switches between two low-level\nmodules: an adaptive corrector for variations, or a standard executor for\nnormal cases. Specifically, both the high-level module and the adaptive\ncorrector are implemented as causal transformers that generate predictions\nbased on the historical scanning sequence. To ensure generalizability, we\ncollected the first large-scale ICA scanning dataset comprising 164\ntrajectories and 72K samples from 28 subjects of both genders. Based on the\nabove innovations, our approach achieves a 95% success rate in locating the ICA\non unseen individuals, outperforming baselines and demonstrating its\neffectiveness. Our code will be released after acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86UltraHiT\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42Transformer\u7684\u51b3\u7b56\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u52a8\u626b\u63cf\u9888\u5185\u52a8\u8109\uff08ICA\uff09\uff0c\u6210\u529f\u7387\u8fbe\u523095%\u3002", "motivation": "\u9888\u5185\u52a8\u8109\uff08ICA\uff09\u7684\u81ea\u52a8\u8d85\u58f0\u626b\u63cf\u7531\u4e8e\u5176\u6df1\u5728\u3001\u5f2f\u66f2\u548c\u4e2a\u4f53\u5dee\u5f02\u5927\u7b49\u539f\u56e0\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUltraHiT\u7684\u5206\u5c42Transformer\u51b3\u7b56\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u9ad8\u7ea7\u53d8\u5f02\u6027\u8bc4\u4f30\u548c\u4f4e\u7ea7\u52a8\u4f5c\u51b3\u7b56\u3002\u9ad8\u7ea7\u6a21\u5757\u8bc6\u522b\u53d8\u5f02\u6027\uff0c\u5e76\u6839\u636e\u60c5\u51b5\u5728\u81ea\u9002\u5e94\u6821\u6b63\u5668\uff08\u7528\u4e8e\u5904\u7406\u53d8\u5f02\u6027\uff09\u548c\u6807\u51c6\u6267\u884c\u5668\uff08\u7528\u4e8e\u6b63\u5e38\u60c5\u51b5\uff09\u4e4b\u95f4\u5207\u6362\u3002\u9ad8\u7ea7\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6821\u6b63\u5668\u5747\u91c7\u7528\u56e0\u679cTransformer\u5b9e\u73b0\uff0c\u5e76\u4f7f\u7528\u4e86\u5305\u542b164\u6761\u8f68\u8ff9\u548c72K\u6837\u672c\u7684\u9888\u5185\u52a8\u8109\u626b\u63cf\u6570\u636e\u96c6\u3002", "result": "\u5728\u5bf9\u672a\u89c1\u8fc7\u7684\u4e2a\u4f53\u8fdb\u884cICA\u5b9a\u4f4d\u65f6\uff0cUltraHiT\u5b9e\u73b0\u4e8695%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UltraHiT\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u9888\u5185\u52a8\u8109\u7684\u81ea\u52a8\u8d85\u58f0\u626b\u63cf\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth\u662f\u4e00\u4e2a\u7ed3\u5408\u8fd0\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u7eb9\u7406\u7a00\u758f\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\uff0c\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u5e76\u5728KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u4f4e\u7eb9\u7406\u6216\u52a8\u6001\u533a\u57df\u65f6\u5b58\u5728\u6df1\u5ea6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faUM-Depth\u6846\u67b6\uff0c\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u878d\u5165\u8bad\u7ec3\u6d41\u7a0b\u548c\u7f51\u7edc\u7ed3\u6784\uff0c\u5229\u7528\u5149\u6d41\u5728\u6559\u5e08\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u589e\u5f3a\u5bf9\u5149\u5ea6\u4fe1\u53f7\u8f83\u5f31\u533a\u57df\u7684\u76d1\u7763\u3002", "result": "\u5728KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\u7ed3\u679c\u3002", "conclusion": "UM-Depth\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "Canary-1B-v2\u662f\u4e00\u4e2a\u652f\u630125\u79cd\u8bed\u8a00\u7684\u5feb\u901f\u3001\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u6a21\u578b\uff0c\u5728\u82f1\u8bedASR\u4e0a\u4f18\u4e8eWhisper-large-v3\u4e14\u901f\u5ea6\u5feb10\u500d\uff0c\u5e76\u53d1\u5e03\u4e86Parakeet-TDT-0.6B-v3\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u5feb\u901f\u3001\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\uff08AST\uff09\u6a21\u578b\u3002", "method": "\u4f7f\u7528FastConformer\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\uff0c\u652f\u630125\u79cd\u4e3b\u8981\u4e3a\u6b27\u6d32\u7684\u8bed\u8a00\u3002\u57281.7M\u5c0f\u65f6\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u589e\u52a0\u4e86\u975e\u8bed\u97f3\u97f3\u9891\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u52a8\u6001\u6570\u636e\u5e73\u8861\u3002\u4f7f\u7528NeMo Forced Aligner\uff08NFA\uff09\u548c\u8f85\u52a9CTC\u6a21\u578b\u6765\u751f\u6210\u65f6\u95f4\u6233\u3002", "result": "Canary-1B-v2\u5728\u82f1\u8bedASR\u4e0a\u6027\u80fd\u4f18\u4e8eWhisper-large-v3\uff0c\u901f\u5ea6\u5feb10\u500d\u3002\u5728\u591a\u8bed\u8a00ASR\u548cAST\u65b9\u9762\uff0c\u5176\u6027\u80fd\u4e0eSeamless-M4T-v2-large\u7b49\u5927\u578b\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u76f8\u5f53\u3002Parakeet-TDT-0.6B-v3\u63d0\u4f9b\u4e86600M\u53c2\u6570\u7684\u591a\u8bed\u8a00ASR\u3002", "conclusion": "Canary-1B-v2\u5728\u591a\u8bed\u8a00ASR\u548cAST\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901f\u5ea6\u5feb\u4e14\u9c81\u68d2\u3002"}}
{"id": "2509.13833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13833", "abs": "https://arxiv.org/abs/2509.13833", "authors": ["Zhikai Zhang", "Jun Guo", "Chao Chen", "Jilong Wang", "Chenghuai Lin", "Yunrui Lian", "Han Xue", "Zhenrong Wang", "Maoqi Liu", "Huaping Liu", "He Wang", "Li Yi"], "title": "Track Any Motions under Any Disturbances", "comment": null, "summary": "A foundational humanoid motion tracker is expected to be able to track\ndiverse, highly dynamic, and contact-rich motions. More importantly, it needs\nto operate stably in real-world scenarios against various dynamics\ndisturbances, including terrains, external forces, and physical property\nchanges for general practical use. To achieve this goal, we propose Any2Track\n(Track Any motions under Any disturbances), a two-stage RL framework to track\nvarious motions under multiple disturbances in the real world. Any2Track\nreformulates dynamics adaptability as an additional capability on top of basic\naction execution and consists of two key components: AnyTracker and AnyAdapter.\nAnyTracker is a general motion tracker with a series of careful designs to\ntrack various motions within a single policy. AnyAdapter is a history-informed\nadaptation module that endows the tracker with online dynamics adaptability to\novercome the sim2real gap and multiple real-world disturbances. We deploy\nAny2Track on Unitree G1 hardware and achieve a successful sim2real transfer in\na zero-shot manner. Any2Track performs exceptionally well in tracking various\nmotions under multiple real-world disturbances.", "AI": {"tldr": "Any2Track\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u8ddf\u8e2a\u5404\u79cd\u8fd0\u52a8\u548c\u6270\u52a8\u3002", "motivation": "\u5b9e\u73b0\u80fd\u591f\u8ddf\u8e2a\u5404\u79cd\u52a8\u6001\u3001\u63a5\u89e6\u4e30\u5bcc\u8fd0\u52a8\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u5404\u79cd\u5e72\u6270\u4e0b\u7a33\u5b9a\u8fd0\u884c\u7684\u4eba\u5f62\u8fd0\u52a8\u8ddf\u8e2a\u5668\u3002", "method": "\u63d0\u51faAny2Track\uff0c\u4e00\u4e2a\u5305\u542bAnyTracker\u548cAnyAdapter\u4e24\u90e8\u5206\u7684\u53cc\u9636\u6bb5RL\u6846\u67b6\u3002AnyTracker\u8d1f\u8d23\u8ddf\u8e2a\u5404\u79cd\u8fd0\u52a8\uff0cAnyAdapter\u901a\u8fc7\u5386\u53f2\u4fe1\u606f\u8fdb\u884c\u5728\u7ebf\u52a8\u6001\u9002\u5e94\uff0c\u4ee5\u514b\u670d\u57df\u8f6c\u79fb\u548c\u73b0\u5b9e\u4e16\u754c\u5e72\u6270\u3002", "result": "\u5728Unitree G1\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684Sim2Real\u8fc1\u79fb\uff0c\u5e76\u5728\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u5e72\u6270\u4e0b\u6210\u529f\u8ddf\u8e2a\u4e86\u5404\u79cd\u8fd0\u52a8\u3002", "conclusion": "Any2Track\u80fd\u591f\u6210\u529f\u5730\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8ddf\u8e2a\u5404\u79cd\u8fd0\u52a8\u548c\u6270\u52a8\u3002"}}
{"id": "2509.13722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "TQF\u901a\u8fc7\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5916\u89c2\u3001\u5e27\u5185\u4ea4\u4e92\u548c\u5e27\u95f4\u8fd0\u52a8\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u7ed3\u5408\u8bed\u8a00\u7ebf\u7d22\u548c\u89c6\u89c9\u5f15\u5bfc\u52a8\u6001\u6784\u5efa\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u4e24\u4e2a\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u589e\u5f3a\u4e86\u5bf9\u8c61\u4ee4\u724c\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u7684\u65b9\u6cd5\u5728\u6307\u4ee3\u8868\u89c2\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08RVOS\uff09\u4e2d\u5b58\u5728\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5bb9\u6613\u53d7\u5230\u5916\u89c2\u6216\u8fd0\u52a8\u76f8\u4f3c\u7684\u5e72\u6270\u56e0\u7d20\u7684\u8bef\u5bfc\u3002", "method": "\u63d0\u51faTriple Query Former (TQF)\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5916\u89c2\u67e5\u8be2\u3001\u5e27\u5185\u4ea4\u4e92\u67e5\u8be2\u548c\u5e27\u95f4\u8fd0\u52a8\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u7ebf\u7d22\u548c\u89c6\u89c9\u5f15\u5bfc\u52a8\u6001\u6784\u5efa\u67e5\u8be2\u3002\u5f15\u5165\u4e86\u4e24\u4e2a\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\uff1a\u5e27\u5185\u4ea4\u4e92\u805a\u5408\u548c\u5e27\u95f4\u8fd0\u52a8\u805a\u5408\uff0c\u4ee5\u589e\u5f3a\u5bf9\u8c61\u4ee4\u724c\u8868\u793a\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u591a\u4e2aRVOS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TQF\u7684\u4f18\u52bf\u4ee5\u53ca\u7ed3\u6784\u5316\u67e5\u8be2\u8bbe\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "TQF\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u67e5\u8be2\u8bbe\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86RVOS\u4e2d\u7684\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.14161", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS\u662f\u4e00\u4e2a\u5305\u542b113\u79cd\u4ee3\u7801\u8f6c\u6362\u8bed\u8a00\u5bf9\uff08\u8de8\u8d8a52\u79cd\u8bed\u8a00\uff09\u7684\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4ee3\u7801\u8f6c\u6362\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u7cfb\u7edf\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b4\u4e2a\u6d4b\u8bd5\u96c6\u548c1\u4e2a\u8bad\u7ec3\u96c6\u7684\u6570\u636e\u96c6\u3002\u6d4b\u8bd5\u96c6\u5305\u62ec\u771f\u5b9e\u8bed\u97f3\u548c\u751f\u6210\u8bed\u97f3\uff0c\u8986\u76d6\u591a\u79cd\u8bed\u8a00\u5bf9\uff08X-\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u666e\u901a\u8bdd\u3001\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed\uff09\u3002\u8bad\u7ec3\u96c6\u5305\u542b128\u5c0f\u65f6\u7684\u751f\u6210\u8bed\u97f3\u6570\u636e\u3002", "result": "CS-FLEURS\u6570\u636e\u96c6\u5305\u542b\u4e86113\u79cd\u4ee3\u7801\u8f6c\u6362\u8bed\u8a00\u5bf9\uff0c\u8986\u76d652\u79cd\u8bed\u8a00\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u97f3\u6570\u636e\uff08\u771f\u5b9e\u8bed\u97f3\u3001\u751f\u6210\u8bed\u97f3\uff09\u548c\u591a\u79cd\u8bed\u8a00\u7ec4\u5408\uff0c\u4e3a\u4ee3\u7801\u8f6c\u6362\u8bed\u97f3\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002", "conclusion": "\u5e0c\u671bCS-FLEURS\u6570\u636e\u96c6\u80fd\u591f\u62d3\u5bbd\u672a\u6765\u4ee3\u7801\u8f6c\u6362\u8bed\u97f3\u7814\u7a76\u7684\u8303\u56f4\uff0c\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7814\u7a76\u3002"}}
{"id": "2509.13839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13839", "abs": "https://arxiv.org/abs/2509.13839", "authors": ["Motonari Kambara", "Komei Sugiura"], "title": "Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models", "comment": "Published in Advanced Robotics", "summary": "In this work, we address the problem of predicting the future success of\nopen-vocabulary object manipulation tasks. Conventional approaches typically\ndetermine success or failure after the action has been carried out. However,\nthey make it difficult to prevent potential hazards and rely on failures to\ntrigger replanning, thereby reducing the efficiency of object manipulation\nsequences. To overcome these challenges, we propose a model, which predicts the\nalignment between a pre-manipulation egocentric image with the planned\ntrajectory and a given natural language instruction. We introduce a Multi-Level\nTrajectory Fusion module, which employs a state-of-the-art deep state-space\nmodel and a transformer encoder in parallel to capture multi-level time-series\nself-correlation within the end effector trajectory. Our experimental results\nindicate that the proposed method outperformed existing methods, including\nfoundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6267\u884c\u524d\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u4f5c\u6267\u884c\u540e\u624d\u5224\u65ad\u6210\u8d25\uff0c\u96be\u4ee5\u9884\u9632\u5371\u9669\u5e76\u4f9d\u8d56\u5931\u8d25\u89e6\u53d1\u91cd\u89c4\u5212\uff0c\u964d\u4f4e\u4e86\u64cd\u4f5c\u5e8f\u5217\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\uff0c\u9884\u6d4b\u9884\u64cd\u4f5c\u7684\u5355\u89c6\u89d2\u56fe\u50cf\u4e0e\u8ba1\u5212\u8f68\u8ff9\u53ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u3002\u5f15\u5165\u591a\u5c42\u6b21\u8f68\u8ff9\u878d\u5408\u6a21\u5757\uff0c\u5e76\u884c\u91c7\u7528\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cTransformer\u7f16\u7801\u5668\uff0c\u4ee5\u6355\u6349\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u5185\u7684\u591a\u5c42\u6b21\u65f6\u95f4\u5e8f\u5217\u81ea\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u5305\u62ec\u57fa\u7840\u6a21\u578b\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.14000", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14000", "abs": "https://arxiv.org/abs/2509.14000", "authors": ["Ivana Kesi\u0107", "Alja\u017e Blatnik", "Carolina Fortuna", "Bla\u017e Bertalani\u010d"], "title": "Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations", "comment": "20 pages, 4 figures", "summary": "Global Navigation Satellite Systems (GNSS) are increasingly disrupted by\nintentional jamming, degrading availability precisely when positioning and\ntiming must remain operational. We address this by reframing jamming mitigation\nas dynamic graph regression and introducing a receiver-centric deep temporal\ngraph network that predicts, and thus corrects, the receivers horizontal\ndeviation in real time. At each 1 Hz epoch, the satellite receiver environment\nis represented as a heterogeneous star graph (receiver center, tracked\nsatellites as leaves) with time varying attributes (e.g., SNR, azimuth,\nelevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM\n(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a\nshort history to output the 2D deviation vector applied for on the fly\ncorrection.\n  We evaluate on datasets from two distinct receivers under three jammer\nprofiles, continuous wave (cw), triple tone (cw3), and wideband FM, each\nexercised at six power levels between -45 and -70 dBm, with 50 repetitions per\nscenario (prejam/jam/recovery). Against strong multivariate time series\nbaselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains\nthe lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm\n(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and\n4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode\ndatasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),\noutperforming Seq2Point, MLP, and CNN. A split study shows superior data\nefficiency: with only 10\\% training data our approach remains well ahead of\nbaselines (20 cm vs. 36-42 cm).", "AI": {"tldr": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3GNSS\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u9884\u6d4b\u548c\u6821\u6b63\u63a5\u6536\u5668\u6c34\u5e73\u504f\u5dee\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "GNSS\u5728\u88ab\u6545\u610f\u5e72\u6270\u65f6\u53ef\u7528\u6027\u4f1a\u4e0b\u964d\uff0c\u800c\u6b64\u65f6\u6070\u6070\u9700\u8981\u4fdd\u6301\u5b9a\u4f4d\u548c\u6388\u65f6\u529f\u80fd\u6b63\u5e38\u3002", "method": "\u5c06\u5e72\u6270\u7f13\u89e3\u89c6\u4e3a\u52a8\u6001\u56fe\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u63a5\u6536\u5668\u4e3a\u4e2d\u5fc3\u7684\u6df1\u5ea6\u65f6\u95f4\u56fe\u7f51\u7edc\uff08HeteroGCLSTM\uff09\uff0c\u8be5\u7f51\u7edc\u53ef\u4ee5\u5b9e\u65f6\u9884\u6d4b\u5e76\u6821\u6b63\u63a5\u6536\u5668\u7684\u6c34\u5e73\u504f\u5dee\u3002\u6bcf\u4e2a1 Hz\u7684\u91c7\u6837\u5468\u671f\uff0c\u536b\u661f\u63a5\u6536\u5668\u73af\u5883\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u5f02\u6784\u661f\u56fe\uff0c\u5e76\u4f7f\u7528\u5305\u542b\u65f6\u53d8\u5c5e\u6027\u7684\u5355\u5c42HeteroGCLSTM\u6765\u805a\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u8f93\u51fa2D\u504f\u5dee\u5411\u91cf\u8fdb\u884c\u5b9e\u65f6\u6821\u6b63\u3002", "result": "\u5728\u4e24\u79cd\u4e0d\u540c\u63a5\u6536\u5668\u548c\u4e09\u79cd\u5e72\u6270\uff08\u8fde\u7eed\u6ce2\u3001\u4e09\u97f3\u8c03\u3001\u5bbd\u5e26FM\uff09\u7684\u591a\u79cd\u529f\u7387\u7ea7\u522b\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0eMLP\u3001CNN\u7b49\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u83b7\u5f97\u6700\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3002\u5728-45 dBm\u65f6\uff0cMAE\u57283.64 cm\u52307.74 cm\u4e4b\u95f4\uff1b\u5728-60\u5230-70 dBm\u65f6\uff0cMAE\u63d0\u9ad8\u52301.65-2.08 cm\u3002\u5728\u6df7\u5408\u6a21\u5f0f\u6570\u636e\u96c6\u4e0a\uff0cMAE\u4e3a3.78 cm\uff08GP01\uff09\u548c4.25 cm\uff08ublox10\uff09\u3002\u5728\u4ec5\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u6a21\u578b\u4ecd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6570\u636e\u6548\u7387\uff0820 cm vs. 36-42 cm\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684HeteroGCLSTM\u6a21\u578b\u5728GNSS\u5e72\u6270\u4e0b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u548c\u6821\u6b63\u63a5\u6536\u5668\u7684\u6c34\u5e73\u504f\u5dee\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "InstanceVG \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u901a\u7528\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\uff08GREC \u548c GRES\uff09\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u4e86\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u786e\u4fdd\u8de8\u7c92\u5ea6\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5404\u81ea\u72ec\u7acb\u5730\u5904\u7406 GREC \u548c GRES \u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u8054\u5408\u8bad\u7ec3\u7684\u4e00\u81f4\u6027\u548c\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u4e14\u5728 GRES \u4efb\u52a1\u4e2d\u5ffd\u89c6\u4e86\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa InstanceVG \u6846\u67b6\uff0c\u5229\u7528\u5b9e\u4f8b\u67e5\u8be2\u7edf\u4e00\u5904\u7406 GREC\uff08\u8fb9\u754c\u6846\uff09\u548c GRES\uff08\u50cf\u7d20\u7ea7\u63a9\u7801\uff09\u4efb\u52a1\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u67e5\u8be2\u5206\u914d\u5148\u9a8c\u53c2\u8003\u70b9\uff0c\u4ee5\u5b9e\u73b0\u70b9\u3001\u6846\u3001\u63a9\u7801\u7684\u4e00\u81f4\u6027\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u7684\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cInstanceVG \u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InstanceVG \u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406 GREC \u548c GRES \u4efb\u52a1\u5e76\u6574\u5408\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u7684\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u5b9e\u73b0\u4e86\u8de8\u7c92\u5ea6\u7684\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.14171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "MLLMs\u5728\u521b\u9020\u529b\u65b9\u9762\u9700\u8981\u5173\u8054\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6b67\u4e49\u3002\u6211\u4eec\u63d0\u51fa\u4e86AssoCiAm\u57fa\u51c6\uff0c\u901a\u8fc7\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u89e3\u51b3\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u8bc4\u4f30\u4e2d\u7684\u6b67\u4e49\u4f1a\u4f7fMLLMs\u7684\u884c\u4e3a\u66f4\u968f\u673a\u3002", "motivation": "\u521b\u9020\u529b\u662fMLLMs\u5b9e\u73b0AGI\u7684\u5173\u952e\u80fd\u529b\uff0c\u800c\u5173\u8054\u6027\u662f\u521b\u9020\u529b\u7684\u57fa\u7840\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u5173\u8054\u6027\u7684\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5173\u8054\u4efb\u52a1\u4e2d\u7684\u56fa\u6709\u9650\u6b67\u4e49\uff0c\u5f71\u54cd\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faAssoCiAm\u57fa\u51c6\uff0c\u5c06\u6b67\u4e49\u5206\u89e3\u4e3a\u5185\u90e8\u6b67\u4e49\u548c\u5916\u90e8\u6b67\u4e49\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u6765\u89c4\u907f\u6b67\u4e49\uff0c\u4ee5\u8bc4\u4f30\u5173\u8054\u80fd\u529b\u3002", "result": "\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b0\u8ba4\u77e5\u4e0e\u5173\u8054\u6027\u4e4b\u95f4\u5b58\u5728\u5f3a\u70c8\u7684\u6b63\u76f8\u5173\u3002\u8bc4\u4f30\u4e2d\u7684\u6b67\u4e49\u4f1a\u5bfc\u81f4MLLMs\u7684\u884c\u4e3a\u66f4\u968f\u673a\u3002\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53ef\u4ee5\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u3002", "conclusion": "AssoCiAm\u57fa\u51c6\u901a\u8fc7\u89e3\u51b3\u8bc4\u4f30\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u5730\u8bc4\u4f30MLLMs\u7684\u5173\u8054\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u8ba4\u77e5\u4e0e\u5173\u8054\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2509.13857", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13857", "abs": "https://arxiv.org/abs/2509.13857", "authors": ["Nguyen Hoang Khoi Tran", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap", "comment": "8 pages, 5 figures", "summary": "Reliable global localization is critical for autonomous vehicles, especially\nin environments where GNSS is degraded or unavailable, such as urban canyons\nand tunnels. Although high-definition (HD) maps provide accurate priors, the\ncost of data collection, map construction, and maintenance limits scalability.\nOpenStreetMap (OSM) offers a free and globally available alternative, but its\ncoarse abstraction poses challenges for matching with sensor data. We propose\nInterKey, a cross-modal framework that leverages road intersections as\ndistinctive landmarks for global localization. Our method constructs compact\nbinary descriptors by jointly encoding road and building imprints from point\nclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,\norientation determination, and area-equalized sampling strategies, enabling\nrobust cross-modal matching. Experiments on the KITTI dataset demonstrate that\nInterKey achieves state-of-the-art accuracy, outperforming recent baselines by\na large margin. The framework generalizes to sensors that can produce dense\nstructural point clouds, offering a scalable and cost-effective solution for\nrobust vehicle localization.", "AI": {"tldr": "InterKey\u5229\u7528\u8def\u53e3\u4f5c\u4e3a\u5730\u6807\uff0c\u7ed3\u5408\u70b9\u4e91\u548cOSM\u6570\u636e\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u4e8c\u503c\u63cf\u8ff0\u7b26\u5b9e\u73b0\u53ef\u9760\u7684\u5168\u7403\u5b9a\u4f4d\uff0c\u514b\u670d\u4e86HD\u5730\u56fe\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u5f31\u6216\u4e0d\u53ef\u7528\u7684\u73af\u5883\u4e2d\uff08\u5982\u57ce\u5e02\u5ce1\u8c37\u548c\u96a7\u9053\uff09\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u53ef\u9760\u7684\u5168\u7403\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6210\u672c\u9ad8\u6602\uff0c\u800cOSM\u6570\u636e\u514d\u8d39\u4f46\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "InterKey\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u8def\u53e3\u4f5c\u4e3a\u5730\u6807\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u70b9\u4e91\u548cOSM\u6570\u636e\u4e2d\u7684\u9053\u8def\u548c\u5efa\u7b51\u5370\u8bb0\u6765\u6784\u5efa\u7d27\u51d1\u7684\u4e8c\u503c\u63cf\u8ff0\u7b26\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u5dee\u5f02\u7f13\u89e3\u3001\u65b9\u5411\u786e\u5b9a\u548c\u9762\u79ef\u5747\u8861\u91c7\u6837\u7b56\u7565\u6765\u5f25\u5408\u6a21\u6001\u95f4\u9699\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u5339\u914d\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInterKey\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InterKey\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u9c81\u68d2\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u80fd\u751f\u6210\u5bc6\u96c6\u7ed3\u6784\u70b9\u4e91\u7684\u4f20\u611f\u5668\u3002"}}
{"id": "2509.14024", "categories": ["cs.LG", "68T07, 68P27, 92-08, 92-10"], "pdf": "https://arxiv.org/pdf/2509.14024", "abs": "https://arxiv.org/abs/2509.14024", "authors": ["Raouf Kerkouche", "Henrik Zunker", "Mario Fritz", "Martin J. K\u00fchn"], "title": "Differentially private federated learning for localized control of infectious disease dynamics", "comment": "18 pages, 6 figures", "summary": "In times of epidemics, swift reaction is necessary to mitigate epidemic\nspreading. For this reaction, localized approaches have several advantages,\nlimiting necessary resources and reducing the impact of interventions on a\nlarger scale. However, training a separate machine learning (ML) model on a\nlocal scale is often not feasible due to limited available data. Centralizing\nthe data is also challenging because of its high sensitivity and privacy\nconstraints. In this study, we consider a localized strategy based on the\nGerman counties and communities managed by the related local health authorities\n(LHA). For the preservation of privacy to not oppose the availability of\ndetailed situational data, we propose a privacy-preserving forecasting method\nthat can assist public health experts and decision makers. ML methods with\nfederated learning (FL) train a shared model without centralizing raw data.\nConsidering the counties, communities or LHAs as clients and finding a balance\nbetween utility and privacy, we study a FL framework with client-level\ndifferential privacy (DP). We train a shared multilayer perceptron on sliding\nwindows of recent case counts to forecast the number of cases, while clients\nexchange only norm-clipped updates and the server aggregated updates with DP\nnoise. We evaluate the approach on COVID-19 data on county-level during two\nphases. As expected, very strict privacy yields unstable, unusable forecasts.\nAt a moderately strong level, the DP model closely approaches the non-DP model:\n$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in\nNovember 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,\nclient-level DP-FL can deliver useful county-level predictions with strong\nprivacy guarantees, and viable privacy budgets depend on epidemic phase,\nallowing privacy-compliant collaboration among health authorities for local\nforecasting.", "AI": {"tldr": "Localized epidemic forecasting using federated learning with differential privacy achieves strong privacy guarantees and useful predictions.", "motivation": "Localized epidemic forecasting is crucial for swift reaction but faces data limitations and privacy concerns. Centralizing data is challenging due to sensitivity and privacy constraints. This paper addresses the need for a privacy-preserving localized strategy using federated learning.", "method": "The study proposes a privacy-preserving forecasting method using federated learning (FL) with client-level differential privacy (DP). A shared multilayer perceptron is trained on case count data. Clients exchange norm-clipped updates, and the server aggregates these updates with DP noise. The framework treats counties/communities/local health authorities as clients.", "result": "The DP-FL model, at a moderately strong privacy level, achieved results comparable to a non-DP model: R^2=0.94 (vs. 0.95) and MAPE of 26% in Nov 2020; R^2=0.88 (vs. 0.93) and MAPE of 21% in Mar 2022. Very strict privacy settings led to unusable forecasts, while moderate settings maintained utility.", "conclusion": "Client-level DP-FL can provide valuable county-level epidemic predictions with robust privacy protections. The viability of privacy budgets is dependent on the epidemic phase, enabling privacy-compliant collaboration among health authorities for local forecasting."}}
{"id": "2509.13754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFMFA\u7684\u6587\u672c\u5230\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u5339\u914d\u6548\u679c\uff0c\u5e76\u5f15\u5165A-SDM\u548cEFA\u6a21\u5757\u6765\u89e3\u51b3\u6b63\u6837\u672c\u5bf9\u9f50\u4e0d\u51c6\u786e\u548c\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u9a8c\u8bc1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u5e76\u4e14\u4e3b\u8981\u5173\u6ce8\u786c\u8d1f\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u9519\u8bef\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\u3002", "method": "\u63d0\u51faFMFA\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u76f8\u4f3c\u6027\u5206\u5e03\u5339\u914d\uff08A-SDM\uff09\u6a21\u5757\u4ee5\u62c9\u8fd1\u672a\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\uff0c\u4ee5\u53ca\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff08EFA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u7a00\u758f\u5316\u76f8\u4f3c\u6027\u77e9\u9635\u548c\u786c\u7f16\u7801\u65b9\u5f0f\u52a0\u5f3a\u663e\u5f0f\u8de8\u6a21\u6001\u7ec6\u7c92\u5ea6\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5c40\u90e8\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cFMFA\u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u5168\u5c40\u5339\u914d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "FMFA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u4e2d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002"}}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7aef\u5230\u7aef\u91d1\u878d\u987e\u95ee\u7684\u76d1\u7763\u6570\u636e\u3002\u901a\u8fc7\u5728\u4e00\u4e2a\u5305\u542b19k\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen-3-8B\u6a21\u578b\uff0c\u5e76\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u548cLLM\u8bc4\u5ba1\u7814\u7a76\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u8868\u73b0\u4e0e\u53c2\u6570\u91cf\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e\u4e8680%\u3002", "motivation": "\u5f53\u524d\u7684\u4e2a\u6027\u5316\u91d1\u878d\u5efa\u8bae\u9700\u8981\u8003\u8651\u7528\u6237\u7684\u76ee\u6807\u3001\u9650\u5236\u3001\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u53f8\u6cd5\u7ba1\u8f96\u533a\u3002\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4e3a\u6295\u8d44\u8005\u548c\u91d1\u878d\u89c4\u5212\u5e08\u63d0\u4f9b\u652f\u6301\u7cfb\u7edf\uff0c\u800c\u9488\u5bf9\u9884\u7b97\u3001\u503a\u52a1\u7ba1\u7406\u3001\u9000\u4f11\u548c\u9057\u4ea7\u89c4\u5212\u7b49\u66f4\u5e7f\u6cdb\u7684\u4e2a\u4eba\u7406\u8d22\u4efb\u52a1\u7684\u4ee3\u7406\u5f0f\u65b9\u6cd5\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u4e14\u6536\u76ca\u4e0d\u4f73\uff08\u4e0d\u5230\u9884\u671f\u6536\u76ca\u768425%\uff09\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u76f8\u5173\u7684\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\uff0c\u4ee5\u6784\u5efa\u7528\u4e8e\u7aef\u5230\u7aef\u91d1\u878d\u987e\u95ee\u7684\u76d1\u7763\u6570\u636e\u3002\u4ed6\u4eec\u5229\u7528\u8fd9\u4e2a\u6846\u67b6\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b19,000\u4e2a\u6837\u672c\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Qwen-3-8B\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u548c\u76f2\u6d4bLLM\u966a\u5ba1\u56e2\u7814\u7a76\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7684\u6570\u636e\u7b56\u5212\u548c\u884c\u4e3a\u6574\u5408\uff0cQwen-3-8B\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e2a\u6027\u5316\u6307\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u53c2\u6570\u91cf\u663e\u8457\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0814-32B\u53c2\u6570\uff09\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e\u4e8680%\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u4ed4\u7ec6\u7684\u6570\u636e\u7b56\u5212\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u539f\u7406\u7684\u6574\u5408\uff0c\u53ef\u4ee5\u5728\u8f83\u5c0f\u7684\u6a21\u578b\uff08\u5982Qwen-3-8B\uff09\u4e0a\u5b9e\u73b0\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u91d1\u878d\u54a8\u8be2\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002\u8fd9\u4e3a\u5f00\u53d1\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\u7684\u4e2a\u6027\u5316\u91d1\u878d\u5efa\u8bae\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2509.13861", "categories": ["cs.RO", "I.6.0; A.0"], "pdf": "https://arxiv.org/pdf/2509.13861", "abs": "https://arxiv.org/abs/2509.13861", "authors": ["G\u00f6rkem K\u0131l\u0131n\u00e7 Soylu", "Neziha Akalin", "Maria Riveiro"], "title": "Using Petri Nets for Context-Adaptive Robot Explanations", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://www.ro-man2025.org/", "summary": "In human-robot interaction, robots must communicate in a natural and\ntransparent manner to foster trust, which requires adapting their communication\nto the context. In this paper, we propose using Petri nets (PNs) to model\ncontextual information for adaptive robot explanations. PNs provide a formal,\ngraphical method for representing concurrent actions, causal dependencies, and\nsystem states, making them suitable for analyzing dynamic interactions between\nhumans and robots. We demonstrate this approach through a scenario involving a\nrobot that provides explanations based on contextual cues such as user\nattention and presence. Model analysis confirms key properties, including\ndeadlock-freeness, context-sensitive reachability, boundedness, and liveness,\nshowing the robustness and flexibility of PNs for designing and verifying\ncontext-adaptive explanations in human-robot interactions.", "AI": {"tldr": "\u4f7f\u7528 Petri  \u0905\u092a\u0918\u093e\u0924\u6a21\u578b\u6765\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u81ea\u9002\u5e94\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u3002", "motivation": "\u4e3a\u4e86\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5efa\u7acb\u4fe1\u4efb\uff0c\u673a\u5668\u4eba\u9700\u8981\u4ee5\u81ea\u7136\u3001\u900f\u660e\u7684\u65b9\u5f0f\u8fdb\u884c\u901a\u4fe1\uff0c\u5e76\u6839\u636e\u4e0a\u4e0b\u6587\u8c03\u6574\u5176\u901a\u4fe1\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4f7f\u7528 Petri  \u0905\u092a\u0918\u093e\u0924\uff08PNs\uff09\u6765\u4e3a\u673a\u5668\u4eba\u89e3\u91ca\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u5efa\u6a21\u3002PNs \u662f\u4e00\u79cd\u6b63\u5f0f\u7684\u3001\u56fe\u5f62\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u5e76\u53d1\u52a8\u4f5c\u3001\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u548c\u7cfb\u7edf\u72b6\u6001\uff0c\u9002\u7528\u4e8e\u5206\u6790\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u673a\u5668\u4eba\u6839\u636e\u7528\u6237\u6ce8\u610f\u529b\u3001\u5728\u573a\u60c5\u51b5\u7b49\u4e0a\u4e0b\u6587\u7ebf\u7d22\u63d0\u4f9b\u89e3\u91ca\u7684\u573a\u666f\u6765\u6f14\u793a\u8fd9\u79cd\u65b9\u6cd5\u3002\u6a21\u578b\u5206\u6790\u786e\u8ba4\u4e86\u5173\u952e\u5c5e\u6027\uff0c\u5982\u65e0\u6b7b\u9501\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u53ef\u8fbe\u6027\u3001\u6709\u754c\u6027\u548c\u6d3b\u6027\u3002", "conclusion": "PNs \u4e3a\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u9002\u5e94\u4e0a\u4e0b\u6587\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.13756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u989c\u8272\u6620\u5c04\u6a21\u5757\u6765\u89e3\u51b3\u73b0\u6709\u6280\u672f\u4e2d\u989c\u8272\u7f16\u8f91\u7cbe\u5ea6\u4e0d\u8db3\u548c\u63a7\u5236\u4e0d\u8fde\u7eed\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u989c\u8272\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u96be\u4ee5\u5b9e\u73b0\u8fde\u7eed\u63a7\u5236\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u56fa\u6709\u6b67\u4e49\u6027\u548c\u79bb\u6563\u6027\uff0c\u4ee5\u53ca\u7ebf\u6027\u63d2\u503c\u65b9\u6cd5\u7f3a\u4e4f\u7cbe\u786e\u7684\u989c\u8272\u63a7\u5236\u548c\u5bf9\u989c\u8272\u53d8\u5316\u8303\u56f4\u7684\u672a\u77e5\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u663e\u5f0f\u5730\u5efa\u6a21\u4e86\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u548c\u56fe\u50cfRGB\u503c\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8be5\u6a21\u5757\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u7684RGB\u503c\u9884\u6d4b\u76f8\u5e94\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u989c\u8272\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u7528\u6237\u53ef\u4ee5\u6307\u5b9a\u76ee\u6807RGB\u8303\u56f4\uff0c\u4ee5\u751f\u6210\u5728\u6240\u9700\u8303\u56f4\u5185\u5177\u6709\u8fde\u7eed\u989c\u8272\u53d8\u5316\u7684\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u989c\u8272\u8fde\u7eed\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u3001\u8fde\u7eed\u4e14\u53ef\u63a7\u7684\u989c\u8272\u7f16\u8f91\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u989c\u8272\u6620\u5c04\u6a21\u5757\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u3001\u8fde\u7eed\u548c\u53ef\u63a7\u7684\u6587\u672c\u9a71\u52a8\u989c\u8272\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.14197", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4e8675\u5e74\u6765\u82f1\u56fd\u8bae\u4f1a\u8fa9\u8bba\u4e2d\u4e0e\u79fb\u6c11\u76f8\u5173\u7684\u8a00\u8bba\uff0c\u5e76\u4e0e\u7f8e\u56fd\u56fd\u4f1a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u7f8e\u56fd\u65e5\u76ca\u4e24\u6781\u5316\u7684\u8a00\u8bba\u4e0d\u540c\uff0c\u82f1\u56fd\u8bae\u4f1a\u7684\u8de8\u515a\u6d3e\u6001\u5ea6\u76f8\u5bf9\u4e00\u81f4\uff0c\u4f46\u5de5\u515a\u548c\u4fdd\u5b88\u515a\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u7684\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u82f1\u56fd\u8bae\u4f1a\u8a00\u8bba\u4e2d\uff0c\u5173\u4e8e\u8fb9\u5883\u63a7\u5236\u548c\u975e\u6cd5\u79fb\u6c11\u7b49\u5b89\u5168\u5316\u53d9\u4e8b\u7684\u589e\u52a0\uff0c\u800c\u793e\u4f1a\u878d\u5408\u7b49\u9762\u5411\u878d\u5408\u7684\u53d9\u4e8b\u5219\u6709\u6240\u51cf\u5c11\u3002\u6b64\u5916\uff0c\u5173\u4e8e\u56fd\u5bb6\u79fb\u6c11\u6cd5\u7684\u8ba8\u8bba\u9010\u6e10\u88ab\u56fd\u9645\u6cd5\u548c\u4eba\u6743\u8ba8\u8bba\u6240\u53d6\u4ee3\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u7684\u653f\u6cbb\u548c\u5386\u53f2\u8bed\u5883\u4e0b\u7684\u8a00\u8bba\u5206\u6790\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5bf9\u82f1\u56fd\u8bae\u4f1a75\u5e74\u6765\u7684\u79fb\u6c11\u76f8\u5173\u8a00\u8bba\u8fdb\u884c\u5927\u89c4\u6a21\u8ba1\u7b97\u5206\u6790\uff0c\u5e76\u4e0e\u7f8e\u56fd\u56fd\u4f1a\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u4e86\u89e3\u5176\u6f14\u53d8\u8d8b\u52bf\u548c\u8de8\u515a\u6d3e\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u5e0c\u671b\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u53d9\u4e8b\u6846\u67b6\u63d0\u53d6\uff0c\u6df1\u5165\u7406\u89e3\u79fb\u6c11\u8a00\u8bba\u7684\u7ec6\u5fae\u4e4b\u5904\uff0c\u5e76\u5c55\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u548c\u5386\u53f2\u8bed\u5883\u4e0b\u7684\u8a00\u8bba\u5206\u6790\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5f00\u653e\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u82f1\u56fd\u8bae\u4f1a75\u5e74\u6765\u7684\u8fa9\u8bba\u8a00\u8bba\u8fdb\u884c\u6807\u6ce8\uff0c\u8bc6\u522b\u5bf9\u79fb\u6c11\u7684\u9ad8\u5c42\u7acb\u573a\uff0c\u5e76\u8ffd\u8e2a\u8de8\u8d8a\u65f6\u95f4\u548c\u653f\u515a\u7684\u51c0\u60c5\u611f\u503e\u5411\u3002\u9488\u5bf9\u82f1\u56fd\u8bae\u4f1a\u8a00\u8bba\uff0c\u7814\u7a76\u8fd8\u6269\u5c55\u4e86\u4e00\u4e2a\u534a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7684\u53d9\u4e8b\u6846\u67b6\uff0c\u4ee5\u6355\u6349\u79fb\u6c11\u8a00\u8bba\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7f8e\u56fd\u7684\u56fd\u4f1a\u8a00\u8bba\u65e5\u76ca\u4e24\u6781\u5316\uff0c\u800c\u82f1\u56fd\u8bae\u4f1a\u7684\u8de8\u515a\u6d3e\u6001\u5ea6\u5219\u76f8\u5bf9\u4e00\u81f4\uff0c\u5de5\u515a\u548c\u4fdd\u5b88\u515a\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u7684\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\uff0c\u5e76\u57282025\u5e74\u8fbe\u5230\u8d1f\u9762\u6c34\u5e73\u3002\u5728\u82f1\u56fd\u8bae\u4f1a\u8a00\u8bba\u5206\u6790\u4e2d\uff0c\u5173\u4e8e\u8fb9\u5883\u63a7\u5236\u548c\u975e\u6cd5\u79fb\u6c11\u7b49\u5b89\u5168\u5316\u53d9\u4e8b\u6709\u6240\u589e\u52a0\uff0c\u800c\u793e\u4f1a\u878d\u5408\u7b49\u9762\u5411\u878d\u5408\u7684\u53d9\u4e8b\u6709\u6240\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u5173\u4e8e\u56fd\u5bb6\u79fb\u6c11\u6cd5\u7684\u8ba8\u8bba\u9010\u6e10\u88ab\u56fd\u9645\u6cd5\u548c\u4eba\u6743\u8ba8\u8bba\u6240\u53d6\u4ee3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u8ba1\u7b97\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\uff0c\u5c55\u793a\u4e86LLMs\u5728\u653f\u6cbb\u548c\u5386\u53f2\u8bed\u5883\u4e0b\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u8a00\u8bba\u5206\u6790\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u82f1\u56fd\u548c\u7f8e\u56fd\u5728\u79fb\u6c11\u8a00\u8bba\u4e0a\u7684\u4e0d\u540c\u8d8b\u52bf\uff0c\u4ee5\u53ca\u82f1\u56fd\u8bae\u4f1a\u8a00\u8bba\u4e2d\u53d9\u4e8b\u7126\u70b9\u7684\u8f6c\u53d8\uff0c\u4e3a\u7406\u89e3\u548c\u5206\u6790\u653f\u6cbb\u8a00\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u89c1\u89e3\u3002"}}
{"id": "2509.14061", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14061", "abs": "https://arxiv.org/abs/2509.14061", "authors": ["Chiara De Luca", "Elisa Donati"], "title": "Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing", "comment": null, "summary": "Queen bee presence is essential for the health and stability of honeybee\ncolonies, yet current monitoring methods rely on manual inspections that are\nlabor-intensive, disruptive, and impractical for large-scale beekeeping. While\nrecent audio-based approaches have shown promise, they often require high power\nconsumption, complex preprocessing, and are susceptible to ambient noise. To\novercome these limitations, we propose a lightweight, multimodal system for\nqueen detection based on environmental sensor fusion-specifically, temperature,\nhumidity, and pressure differentials between the inside and outside of the\nhive. Our approach employs quantized decision tree inference on a commercial\nSTM32 microcontroller, enabling real-time, low-power edge computing without\ncompromising accuracy. We show that our system achieves over 99% queen\ndetection accuracy using only environmental inputs, with audio features\noffering no significant performance gain. This work presents a scalable and\nsustainable solution for non-invasive hive monitoring, paving the way for\nautonomous, precision beekeeping using off-the-shelf, energy-efficient\nhardware.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u4f20\u611f\u5668\u878d\u5408\uff08\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u5185\u5916\u538b\u5dee\uff09\u7684\u8f7b\u91cf\u7ea7\u3001\u591a\u6a21\u6001\u8702\u540e\u68c0\u6d4b\u7cfb\u7edf\uff0c\u53ef\u5728STM32\u5fae\u63a7\u5236\u5668\u4e0a\u8fdb\u884c\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7684\u8fb9\u7f18\u8ba1\u7b97\uff0c\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u4e14\u65e0\u9700\u97f3\u9891\u7279\u5f81\uff0c\u4e3a\u7cbe\u51c6\u517b\u8702\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u871c\u8702\u8702\u540e\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8017\u65f6\u3001\u5e72\u6270\u5927\u4e14\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u517b\u8702\u7684\u624b\u52a8\u68c0\u67e5\u3002\u867d\u7136\u57fa\u4e8e\u97f3\u9891\u7684\u65b9\u6cd5\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u529f\u8017\u9ad8\u3001\u9884\u5904\u7406\u590d\u6742\u548c\u6613\u53d7\u73af\u5883\u566a\u58f0\u5e72\u6270\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u4f20\u611f\u5668\uff08\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u5185\u5916\u538b\u5dee\uff09\u878d\u5408\u7684\u8f7b\u91cf\u7ea7\u3001\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u5e76\u5728\u5546\u7528STM32\u5fae\u63a7\u5236\u5668\u4e0a\u4f7f\u7528\u91cf\u5316\u51b3\u7b56\u6811\u63a8\u7406\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7684\u8fb9\u7f18\u8ba1\u7b97\u3002", "result": "\u8be5\u7cfb\u7edf\u4ec5\u4f7f\u7528\u73af\u5883\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u8d85\u8fc799%\u7684\u8702\u540e\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u97f3\u9891\u7279\u5f81\u5e76\u672a\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u6301\u7eed\u7684\u975e\u4fb5\u5165\u5f0f\u8702\u5de2\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u73b0\u6210\u7684\u3001\u8282\u80fd\u7684\u786c\u4ef6\uff0c\u4e3a\u5b9e\u73b0\u81ea\u4e3b\u3001\u7cbe\u51c6\u7684\u517b\u8702\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13760", "abs": "https://arxiv.org/abs/2509.13760", "authors": ["Jinwoo Jeon", "JunHyeok Oh", "Hayeong Lee", "Byung-Jun Lee"], "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation", "comment": null, "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u5206\u6790\u7684\u8fed\u4ee3\u5f0f\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u7528\u6237\u610f\u56fe\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u793a\uff0c\u4f46\u5ffd\u7565\u4e86\u751f\u6210\u56fe\u50cf\u672c\u8eab\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u7684\u8f93\u51fa\u6216\u4e0d\u5fc5\u8981\u7684\u4fee\u6539\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u5f0f\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u540c\u65f6\u5206\u6790\u8f93\u5165\u7684\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u6587\u672c\u548c\u89c6\u89c9\u5b89\u5168\u4fe1\u53f7\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4fdd\u6301\u4e0e\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u5728\u53ef\u9760\u6027\u65b9\u9762\u4e0e\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u6587\u672c\u5230\u56fe\u50cf\u5185\u5bb9\u3002"}}
{"id": "2509.14233", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hern\u00e1ndez-Cano", "Alexander H\u00e4gele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank \u010eurech", "Ido Hakimi", "Juan Garc\u00eda Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabol\u010dec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian B\u00f6sch", "Maximilian B\u00f6ther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "Mar\u00eda Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike L\u00fcbeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendonc\u00e7a", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "L\u00e9o Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tram\u00e8r", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5957\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u5f00\u653e\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\u6570\u636e\u5408\u89c4\u6027\u548c\u591a\u8bed\u8a00\u8868\u793a\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u653e\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\u6570\u636e\u5408\u89c4\u6027\u548c\u591a\u8bed\u8a00\u8868\u793a\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5c0a\u91cdrobots.txt\u6392\u9664\u89c4\u5219\uff0c\u8fc7\u6ee4\u975e\u8bb8\u53ef\u3001\u6709\u6bd2\u548c\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u5185\u5bb9\u3002\u91c7\u7528Goldfish\u76ee\u6807\u6291\u5236\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u6a21\u578b\u572815\u4e07\u4ebf\uff0815T\uff09\u7684\u5305\u542b1800\u591a\u79cd\u8bed\u8a00\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5176\u4e2d\u7ea640%\u4e3a\u975e\u82f1\u8bed\u5185\u5bb9\u3002\u6a21\u578b\u53d1\u5e03\u4e868B\u548c70B\u4e24\u4e2a\u89c4\u6a21\u7684\u7248\u672c\u3002", "result": "\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cApertus\u8fbe\u5230\u4e86\u4e0e\u5b8c\u5168\u5f00\u653e\u6a21\u578b\u76f8\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u6c34\u5e73\u3002", "conclusion": "Apertus\u5728\u591a\u8bed\u8a00\u80fd\u529b\u548c\u6570\u636e\u5408\u89c4\u6027\u65b9\u9762\u4e3a\u5f00\u653e\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u5f00\u53d1\u5de5\u4ef6\u4ee5\u4f9b\u5ba1\u8ba1\u548c\u6269\u5c55\u3002"}}
{"id": "2509.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13903", "abs": "https://arxiv.org/abs/2509.13903", "authors": ["Artem Lykov", "Jeffrin Sam", "Hung Khang Nguyen", "Vladislav Kozlovskiy", "Yara Mahmoud", "Valerii Serpiva", "Miguel Altamirano Cabrera", "Mikhail Konenkov", "Dzmitry Tsetserukou"], "title": "PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models", "comment": "submitted to IEEE conference", "summary": "We introduce PhysicalAgent, an agentic framework for robotic manipulation\nthat integrates iterative reasoning, diffusion-based video generation, and\nclosed-loop execution. Given a textual instruction, our method generates short\nvideo demonstrations of candidate trajectories, executes them on the robot, and\niteratively re-plans in response to failures. This approach enables robust\nrecovery from execution errors. We evaluate PhysicalAgent across multiple\nperceptual modalities (egocentric, third-person, and simulated) and robotic\nembodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing\nagainst state-of-the-art task-specific baselines. Experiments demonstrate that\nour method consistently outperforms prior approaches, achieving up to 83%\nsuccess on human-familiar tasks. Physical trials reveal that first-attempt\nsuccess is limited (20-30%), yet iterative correction increases overall success\nto 80% across platforms. These results highlight the potential of video-based\ngenerative reasoning for general-purpose robotic manipulation and underscore\nthe importance of iterative execution for recovering from initial failures. Our\nframework paves the way for scalable, adaptable, and robust robot control.", "AI": {"tldr": "PhysicalAgent\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fed\u4ee3\u63a8\u7406\u3001\u57fa\u4e8e\u6269\u6563\u89c6\u9891\u751f\u6210\u548c\u95ed\u73af\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u4ece\u6267\u884c\u9519\u8bef\u4e2d\u6062\u590d\uff0c\u5e76\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u548c\u611f\u77e5\u6a21\u5f0f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u66f4\u901a\u7528\u3001\u66f4\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6267\u884c\u9519\u8bef\u548c\u9002\u5e94\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u65b9\u9762\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u96c6\u6210\u63a8\u7406\u3001\u751f\u6210\u548c\u95ed\u73af\u6267\u884c\u7684\u6846\u67b6\u3002", "method": "PhysicalAgent\u6846\u67b6\u6574\u5408\u4e86\u8fed\u4ee3\u63a8\u7406\u3001\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u4ee5\u53ca\u95ed\u73af\u6267\u884c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u63a5\u6536\u6587\u672c\u6307\u4ee4\uff0c\u751f\u6210\u5019\u9009\u8f68\u8ff9\u7684\u77ed\u89c6\u9891\u6f14\u793a\uff0c\u7136\u540e\u5728\u673a\u5668\u4eba\u4e0a\u6267\u884c\u8fd9\u4e9b\u8f68\u8ff9\uff0c\u5e76\u5728\u9047\u5230\u5931\u8d25\u65f6\u8fdb\u884c\u8fed\u4ee3\u5f0f\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728\u591a\u79cd\u611f\u77e5\u6a21\u5f0f\uff08\u5305\u62ec\u4e3b\u89c6\u89d2\u3001\u7b2c\u4e09\u4eba\u79f0\u548c\u6a21\u62df\uff09\u548c\u673a\u5668\u4eba\u5b9e\u4f53\uff08\u5305\u62ec\u53cc\u81c2UR3\u3001Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u548c\u6a21\u62dfGR1\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPhysicalAgent\u7684\u6210\u529f\u7387\u9ad8\u8fbe83%\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u9996\u6b21\u5c1d\u8bd5\u7684\u6210\u529f\u7387\u8f83\u4f4e\uff0820-30%\uff09\uff0c\u4f46\u901a\u8fc7\u8fed\u4ee3\u4fee\u6b63\uff0c\u6574\u4f53\u6210\u529f\u7387\u53ef\u63d0\u9ad8\u523080%\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u9891\u7684\u751f\u6210\u5f0f\u63a8\u7406\u548c\u8fed\u4ee3\u5f0f\u6267\u884c\u5bf9\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u901a\u7528\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4ece\u521d\u59cb\u6267\u884c\u5931\u8d25\u4e2d\u6062\u590d\u65b9\u9762\u3002PhysicalAgent\u7684\u6210\u529f\u5c55\u793a\u4e86\u5176\u5728\u63a8\u52a8\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u9c81\u68d2\u7684\u673a\u5668\u4eba\u63a7\u5236\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14077", "abs": "https://arxiv.org/abs/2509.14077", "authors": ["Yuhao Wang", "Enlu Zhou"], "title": "Online Bayesian Risk-Averse Reinforcement Learning", "comment": null, "summary": "In this paper, we study the Bayesian risk-averse formulation in reinforcement\nlearning (RL). To address the epistemic uncertainty due to a lack of data, we\nadopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the\nparameter uncertainty of the unknown underlying model. We derive the asymptotic\nnormality that characterizes the difference between the Bayesian risk value\nfunction and the original value function under the true unknown distribution.\nThe results indicate that the Bayesian risk-averse approach tends to\npessimistically underestimate the original value function. This discrepancy\nincreases with stronger risk aversion and decreases as more data become\navailable. We then utilize this adaptive property in the setting of online RL\nas well as online contextual multi-arm bandits (CMAB), a special case of online\nRL. We provide two procedures using posterior sampling for both the general RL\nproblem and the CMAB problem. We establish a sub-linear regret bound, with the\nregret defined as the conventional regret for both the RL and CMAB settings.\nAdditionally, we establish a sub-linear regret bound for the CMAB setting with\nthe regret defined as the Bayesian risk regret. Finally, we conduct numerical\nexperiments to demonstrate the effectiveness of the proposed algorithm in\naddressing epistemic uncertainty and verifying the theoretical properties.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u503c\u51fd\u6570\u4e0e\u539f\u59cb\u503c\u51fd\u6570\u7684\u5dee\u5f02\u7279\u5f81\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f1a\u60b2\u89c2\u5730\u4f4e\u4f30\u503c\u51fd\u6570\uff0c\u4e14\u8fd9\u79cd\u4f4e\u4f30\u7a0b\u5ea6\u4e0e\u98ce\u9669\u89c4\u907f\u7a0b\u5ea6\u6210\u6b63\u6bd4\uff0c\u4e0e\u6570\u636e\u91cf\u6210\u53cd\u6bd4\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6587\u7ae0\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u91c7\u6837\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5177\u6709\u4e9a\u7ebf\u6027\u9057\u61be\u754c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u672c\u7814\u7a76\u91c7\u7528\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08BRMDP\uff09\u6765\u5904\u7406\u672a\u77e5\u6a21\u578b\u53c2\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u63a8\u5bfc\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u503c\u51fd\u6570\u4e0e\u539f\u59cb\u503c\u51fd\u6570\u5728\u771f\u5b9e\u672a\u77e5\u5206\u5e03\u4e0b\u5dee\u5f02\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u3002\u5e76\u5229\u7528\u8be5\u6027\u8d28\uff0c\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u95ee\u9898\u4e2d\uff0c\u901a\u8fc7\u540e\u9a8c\u91c7\u6837\u8bbe\u8ba1\u4e86\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u548cCMAB\u95ee\u9898\u7684\u4e24\u79cd\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u503e\u5411\u4e8e\u60b2\u89c2\u5730\u4f4e\u4f30\u539f\u59cb\u503c\u51fd\u6570\uff0c\u8fd9\u79cd\u5dee\u5f02\u968f\u7740\u98ce\u9669\u89c4\u907f\u7a0b\u5ea6\u7684\u589e\u5f3a\u800c\u589e\u5927\uff0c\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u52a0\u800c\u51cf\u5c0f\u3002\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u901a\u7528RL\u548cCMAB\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0\u4e86\u4e9a\u7ebf\u6027\u9057\u61be\u754c\u3002\u6b64\u5916\uff0c\u5728CMAB\u8bbe\u7f6e\u4e0b\uff0c\u4ee5\u8d1d\u53f6\u65af\u98ce\u9669\u9057\u61be\u4e3a\u5b9a\u4e49\u7684\u9057\u61be\u4e5f\u5b9e\u73b0\u4e86\u4e9a\u7ebf\u6027\u754c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u6027\u8d28\u3002"}}
{"id": "2509.13762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13762", "abs": "https://arxiv.org/abs/2509.13762", "authors": ["Kai Chen", "Jin Xiao", "Leheng Zhang", "Kexuan Shi", "Shuhang Gu"], "title": "Task-Aware Image Signal Processor for Advanced Visual Perception", "comment": null, "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.", "AI": {"tldr": "TA-ISP\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684RAW\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\u6765\u589e\u5f3a\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709RAW\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faTA-ISP\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u8f7b\u91cf\u7ea7\u3001\u591a\u5c3a\u5ea6\u7684\u8c03\u5236\u7b97\u5b50\u6765\u5904\u7406RAW\u56fe\u50cf\uff0c\u5b9e\u73b0\u5168\u5c40\u3001\u533a\u57df\u548c\u50cf\u7d20\u7ea7\u522b\u7684\u7edf\u8ba1\u4fe1\u606f\u91cd\u5851\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cTA-ISP\u5728\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\uff09\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "TA-ISP\u662f\u4e00\u4e2a\u6709\u6548\u7684\u3001\u8f7b\u91cf\u7ea7\u7684RAW\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\u751f\u6210\u9762\u5411\u4efb\u52a1\u7684\u8868\u793a\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002"}}
{"id": "2509.13926", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13926", "abs": "https://arxiv.org/abs/2509.13926", "authors": ["Huilin Yin", "Yiming Kan", "Daniel Watzenig"], "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning", "comment": "8 pages, 2 figures, accepted by ICCVW Author list updated to match\n  the camera-ready version, in compliance with conference policy", "summary": "In recent years, end-to-end autonomous driving has attracted increasing\nattention for its ability to jointly model perception, prediction, and planning\nwithin a unified framework. However, most existing approaches underutilize the\nonline mapping module, leaving its potential to enhance trajectory planning\nlargely untapped. This paper proposes MAP (Map-Assisted Planning), a novel\nmap-assisted end-to-end trajectory planning framework. MAP explicitly\nintegrates segmentation-based map features and the current ego status through a\nPlan-enhancing Online Mapping module, an Ego-status-guided Planning module, and\na Weight Adapter based on current ego status. Experiments conducted on the\nDAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%\nreduction in L2 displacement error, a 56.2% reduction in off-road rate, and a\n44.5% improvement in overall score compared to the UniV2X baseline, even\nwithout post-processing. Furthermore, it achieves top ranking in Track 2 of the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS\nWorkshop @CVPR2025, outperforming the second-best model by 39.5% in terms of\noverall score. These results highlight the effectiveness of explicitly\nleveraging semantic map features in planning and suggest new directions for\nimproving structure design in end-to-end autonomous driving systems. Our code\nis available at https://gitee.com/kymkym/map.git", "AI": {"tldr": "MAP\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5730\u56fe\u8f85\u52a9\u7aef\u5230\u7aef\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5728\u7ebf\u5730\u56fe\u7279\u5f81\u548c\u5f53\u524d\u81ea\u6211\u72b6\u6001\u6765\u63d0\u5347\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u5728DAIR-V2X-seq-SPD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5728\u7ebf\u5730\u56fe\u6a21\u5757\u7684\u6f5c\u529b\u6765\u589e\u5f3a\u8f68\u8ff9\u89c4\u5212\u3002", "method": "MAP\u6846\u67b6\u663e\u5f0f\u5730\u96c6\u6210\u4e86\u57fa\u4e8e\u5206\u5272\u7684\u5730\u56fe\u7279\u5f81\u548c\u5f53\u524d\u81ea\u6211\u72b6\u6001\uff0c\u901a\u8fc7\u4e00\u4e2a\u589e\u5f3a\u5728\u7ebf\u5730\u56fe\u6a21\u5757\u3001\u4e00\u4e2a\u81ea\u6211\u72b6\u6001\u5f15\u5bfc\u89c4\u5212\u6a21\u5757\u548c\u4e00\u4e2a\u57fa\u4e8e\u5f53\u524d\u81ea\u6211\u72b6\u6001\u7684\u6743\u91cd\u9002\u914d\u5668\u3002", "result": "\u5728DAIR-V2X-seq-SPD\u6570\u636e\u96c6\u4e0a\uff0cMAP\u76f8\u6bd4UniV2X\u57fa\u7ebf\uff0cL2\u4f4d\u79fb\u8bef\u5dee\u51cf\u5c11\u4e8616.6%\uff0c\u8d8a\u91ce\u7387\u964d\u4f4e\u4e8656.2%\uff0c\u603b\u4f53\u5f97\u5206\u63d0\u9ad8\u4e8644.5%\u3002\u5728MEIS Workshop @CVPR2025\u7684V2X\u534f\u4f5c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5bTrack 2\u4e2d\uff0cMAP\u7684\u603b\u4f53\u5f97\u5206\u6bd4\u7b2c\u4e8c\u540d\u9ad8\u51fa39.5%\u3002", "conclusion": "\u663e\u5f0f\u5229\u7528\u8bed\u4e49\u5730\u56fe\u7279\u5f81\u8fdb\u884c\u89c4\u5212\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e3a\u6539\u8fdb\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.14078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14078", "abs": "https://arxiv.org/abs/2509.14078", "authors": ["Robiul Islam", "Dmitry I. Ignatov", "Karl Kaberg", "Roman Nabatchikov"], "title": "Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques", "comment": null, "summary": "This study investigates classifier performance across EEG frequency bands\nusing various optimizers and evaluates efficient class prediction for the left\nand right hemispheres. Three neural network architectures - a deep dense\nnetwork, a shallow three-layer network, and a convolutional neural network\n(CNN) - are implemented and compared using the TensorFlow and PyTorch\nframeworks. Results indicate that the Adagrad and RMSprop optimizers\nconsistently perform well across different frequency bands, with Adadelta\nexhibiting robust performance in cross-model evaluations. Specifically, Adagrad\nexcels in the beta band, while RMSprop achieves superior performance in the\ngamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among\nthe models, the CNN demonstrates the second highest accuracy, particularly in\ncapturing spatial features of EEG data. The deep dense network shows\ncompetitive performance in learning complex patterns, whereas the shallow\nthree-layer network, sometimes being less accurate, provides computational\nefficiency. SHAP (Shapley Additive Explanations) plots are employed to identify\nefficient class prediction, revealing nuanced contributions of EEG frequency\nbands to model accuracy. Overall, the study highlights the importance of\noptimizer selection, model architecture, and EEG frequency band analysis in\nenhancing classifier performance and understanding feature importance in\nneuroimaging-based classification tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u4f18\u5316\u5668\u5728\u4e0d\u540c\u8111\u7535\u56fe\uff08EEG\uff09\u9891\u5e26\u4e0a\u7684\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e86\u5de6\u53f3\u534a\u7403\u7684\u9ad8\u6548\u7c7b\u522b\u9884\u6d4b\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u8111\u7535\u56fe\uff08EEG\uff09\u9891\u5e26\u4e0a\u7684\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u9ad8\u6548\u7c7b\u522b\u9884\u6d4b\u7684\u56e0\u7d20\u3002", "method": "\u5b9e\u73b0\u4e86\u6df1\u5ea6\u5bc6\u96c6\u7f51\u7edc\u3001\u6d45\u5c42\u4e09\u5c42\u7f51\u7edc\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5e76\u4f7f\u7528TensorFlow\u548cPyTorch\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5229\u7528SHAP\u56fe\u5206\u6790\u4e86EEG\u9891\u5e26\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002", "result": "Adagrad\u548cRMSprop\u4f18\u5316\u5668\u5728\u4e0d\u540c\u9891\u5e26\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0cAdadelta\u5728\u8de8\u6a21\u578b\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a33\u5065\u3002CNN\u5728\u6355\u6349\u7a7a\u95f4\u7279\u5f81\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6df1\u5ea6\u5bc6\u96c6\u7f51\u7edc\u5728\u5b66\u4e60\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6d45\u5c42\u7f51\u7edc\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u4f18\u5316\u5668\u7684\u9009\u62e9\u3001\u6a21\u578b\u67b6\u6784\u548cEEG\u9891\u5e26\u5206\u6790\u5bf9\u4e8e\u63d0\u9ad8\u5206\u7c7b\u5668\u6027\u80fd\u548c\u7406\u89e3\u795e\u7ecf\u5f71\u50cf\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.13766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13766", "abs": "https://arxiv.org/abs/2509.13766", "authors": ["Huichun Liu", "Xiaosong Li", "Yang Liu", "Xiaoqi Cheng", "Haishu Tan"], "title": "NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset", "comment": null, "summary": "Visual degradation caused by rain streak artifacts in low-light conditions\nsignificantly hampers the performance of nighttime surveillance and autonomous\nnavigation. Existing image deraining techniques are primarily designed for\ndaytime conditions and perform poorly under nighttime illumination due to the\nspatial heterogeneity of rain distribution and the impact of light-dependent\nstripe visibility. In this paper, we propose a novel Nighttime Deraining\nLocation-enhanced Perceptual Network(NDLPNet) that effectively captures the\nspatial positional information and density distribution of rain streaks in\nlow-light environments. Specifically, we introduce a Position Perception Module\n(PPM) to capture and leverage spatial contextual information from input data,\nenhancing the model's capability to identify and recalibrate the importance of\ndifferent feature channels. The proposed nighttime deraining network can\neffectively remove the rain streaks as well as preserve the crucial background\ninformation. Furthermore, We construct a night scene rainy (NSR) dataset\ncomprising 900 image pairs, all based on real-world nighttime scenes, providing\na new benchmark for nighttime deraining task research. Extensive qualitative\nand quantitative experimental evaluations on both existing datasets and the NSR\ndataset consistently demonstrate our method outperform the state-of-the-art\n(SOTA) methods in nighttime deraining tasks. The source code and dataset is\navailable at https://github.com/Feecuin/NDLPNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591c\u95f4\u53bb\u96e8\u7f51\u7edcNDLPNet\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5\u6a21\u5757\uff08PPM\uff09\u589e\u5f3a\u7a7a\u95f4\u5b9a\u4f4d\u548c\u5bc6\u5ea6\u4fe1\u606f\uff0c\u4ee5\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u6709\u6548\u53bb\u9664\u96e8\u7eb9\u5e76\u4fdd\u7559\u80cc\u666f\u4fe1\u606f\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684NSR\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u53bb\u96e8\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u767d\u5929\u6761\u4ef6\uff0c\u5728\u591c\u95f4\u5149\u7167\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u96e8\u7684\u5206\u5e03\u5177\u6709\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u5e76\u4e14\u6761\u7eb9\u53ef\u89c1\u6027\u53d7\u5149\u7ebf\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591c\u95f4\u53bb\u96e8\u5b9a\u4f4d\u589e\u5f3a\u611f\u77e5\u7f51\u7edc\uff08NDLPNet\uff09\uff0c\u5f15\u5165\u4e86\u4f4d\u7f6e\u611f\u77e5\u6a21\u5757\uff08PPM\uff09\u6765\u6355\u83b7\u548c\u5229\u7528\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u8bc6\u522b\u548c\u91cd\u65b0\u6821\u51c6\u4e0d\u540c\u7279\u5f81\u901a\u9053\u7684\u91cd\u8981\u6027\u3002", "result": "NDLPNet\u80fd\u591f\u6709\u6548\u53bb\u9664\u96e8\u7eb9\u5e76\u4fdd\u7559\u5173\u952e\u7684\u80cc\u666f\u4fe1\u606f\u3002\u5728\u73b0\u6709\u6570\u636e\u96c6\u548cNSR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591c\u95f4\u53bb\u96e8\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684NDLPNet\u80fd\u591f\u6709\u6548\u53bb\u9664\u591c\u95f4\u573a\u666f\u4e2d\u7684\u96e8\u7eb9\uff0c\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u4e3a\u591c\u95f4\u53bb\u96e8\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2509.14113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14113", "abs": "https://arxiv.org/abs/2509.14113", "authors": ["Alessandro Brusaferri", "Danial Ramin", "Andrea Ballarino"], "title": "From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting", "comment": "6 pages", "summary": "While neural networks are achieving high predictive accuracy in multi-horizon\nprobabilistic forecasting, understanding the underlying mechanisms that lead to\nfeature-conditioned outputs remains a significant challenge for forecasters. In\nthis work, we take a further step toward addressing this critical issue by\nintroducing the Quantile Neural Basis Model, which incorporates the\ninterpretability principles of Quantile Generalized Additive Models into an\nend-to-end neural network training framework. To this end, we leverage shared\nbasis decomposition and weight factorization, complementing Neural Models for\nLocation, Scale, and Shape by avoiding any parametric distributional\nassumptions. We validate our approach on day-ahead electricity price\nforecasting, achieving predictive performance comparable to distributional and\nquantile regression neural networks, while offering valuable insights into\nmodel behavior through the learned nonlinear mappings from input features to\noutput predictions across the horizon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cQuantile Neural Basis Model\u201d\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u671f\u6982\u7387\u9884\u6d4b\u4e2d\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5c06Quantile Generalized Additive Models\u7684\u53ef\u89e3\u91ca\u6027\u539f\u7406\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u5171\u4eab\u57fa\u5206\u89e3\u548c\u6743\u91cd\u5206\u89e3\u6280\u672f\uff0c\u5728\u4e0d\u4f9d\u8d56\u4efb\u4f55\u53c2\u6570\u5206\u5e03\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u80fd\u63d0\u4f9b\u5173\u4e8e\u6a21\u578b\u5982\u4f55\u4ece\u8f93\u5165\u7279\u5f81\u6620\u5c04\u5230\u8f93\u51fa\u9884\u6d4b\u7684\u5b9d\u8d35\u89c1\u89e3\u3002", "motivation": "\u5f53\u524d\u591a\u671f\u6982\u7387\u9884\u6d4b\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5176\u7279\u5f81\u6761\u4ef6\u8f93\u51fa\u7684\u5e95\u5c42\u673a\u5236\u5bf9\u9884\u6d4b\u8005\u6765\u8bf4\u4ecd\u96be\u4ee5\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQuantile Neural Basis Model\uff0c\u8be5\u6a21\u578b\u5c06Quantile Generalized Additive Models\u7684\u53ef\u89e3\u91ca\u6027\u539f\u7406\u5d4c\u5165\u5230\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002\u5229\u7528\u5171\u4eab\u57fa\u5206\u89e3\u548c\u6743\u91cd\u5206\u89e3\u6280\u672f\uff0c\u907f\u514d\u4e86\u53c2\u6570\u5206\u5e03\u5047\u8bbe\u3002", "result": "\u5728\u65e5\u524d\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u53d6\u5f97\u4e86\u4e0e\u5206\u5e03\u548c\u5206\u4f4d\u6570\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "Quantile Neural Basis Model\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u63d0\u4f9b\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u8f93\u5165\u7279\u5f81\u5230\u8f93\u51fa\u9884\u6d4b\u7684\u975e\u7ebf\u6027\u6620\u5c04\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002"}}
{"id": "2509.13767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13767", "abs": "https://arxiv.org/abs/2509.13767", "authors": ["Daiqi Liu", "Tom\u00e1s Arias-Vergara", "Johannes Enk", "Fangxu Xing", "Maureen Stone", "Jerry L. Prince", "Jana Hutter", "Andreas Maier", "Jonghye Woo", "Paula Andrea P\u00e9rez-Toro"], "title": "VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI", "comment": "Preprint submitted to ICASSP", "summary": "Accurately segmenting articulatory structures in real-time magnetic resonance\nimaging (rtMRI) remains challenging, as most existing methods rely almost\nentirely on visual cues. Yet synchronized acoustic and phonological signals\nprovide complementary context that can enrich visual information and improve\nprecision. In this paper, we introduce VocSegMRI, a multimodal framework that\nintegrates video, audio, and phonological inputs through cross-attention fusion\nfor dynamic feature alignment. To further enhance cross-modal representation,\nwe incorporate a contrastive learning objective that improves segmentation\nperformance even when the audio modality is unavailable at inference. Evaluated\non a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art\nperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance\n(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.\nAblation studies confirm the contributions of cross-attention and contrastive\nlearning to segmentation precision and robustness. These results highlight the\nvalue of integrative multimodal modeling for accurate vocal tract analysis.", "AI": {"tldr": "VocSegMRI\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u8fdb\u884c\u52a8\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53d1\u97f3\u5668\u5b98\u7684\u5b9e\u65f6MRI\u5206\u5272\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5b9e\u65f6MRI\u53d1\u97f3\u5668\u5b98\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u5ffd\u7565\u4e86\u540c\u6b65\u7684\u58f0\u97f3\u548c\u8bed\u97f3\u5b66\u4fe1\u53f7\u6240\u80fd\u63d0\u4f9b\u7684\u4e92\u8865\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51faVocSegMRI\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6280\u672f\u6574\u5408\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u8868\u793a\uff0c\u5373\u4f7f\u5728\u63a8\u7406\u65f6\u97f3\u9891\u6a21\u6001\u4e0d\u53ef\u7528\u4e5f\u80fd\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002", "result": "\u5728USC-75 rtMRI\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b50\u96c6\u4e0a\uff0cVocSegMRI\u5b9e\u73b0\u4e860.95\u7684Dice\u5206\u6570\u548c4.20 mm\u768495th percentile Hausdorff Distance (HD_95)\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6574\u5408\u591a\u6a21\u6001\u5efa\u6a21\u5bf9\u4e8e\u7cbe\u786e\u7684\u58f0\u5e26\u5206\u6790\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u4e8e\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2509.13948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13948", "abs": "https://arxiv.org/abs/2509.13948", "authors": ["Benedict Barrow", "Roger K. Moore"], "title": "The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://ro-man2025.org/", "summary": "Trust and the perception of trustworthiness play an important role in\ndecision-making and our behaviour towards others, and this is true not only of\nhuman-human interactions but also of human-robot interactions. While\nsignificant advances have been made in recent years in the field of social\nrobotics, there is still some way to go before we fully understand the factors\nthat influence human trust in robots. This paper presents the results of a\nstudy into the first impressions created by a social robot's facial features,\nbased on the hypothesis that a `babyface' engenders trust. By manipulating the\nback-projected face of a Furhat robot, the study confirms that eye shape and\nsize have a significant impact on the perception of trustworthiness. The work\nthus contributes to an understanding of the design choices that need to be made\nwhen developing social robots so as to optimise the effectiveness of\nhuman-robot interaction.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u673a\u5668\u4eba\u7684", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5f71\u54cd\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u4fe1\u4efb\u5ea6\u7684\u56e0\u7d20\uff0c\u7279\u522b\u662f\u673a\u5668\u4eba\u9762\u90e8\u7279\u5f81\uff08\u5982\u5a03\u5a03\u8138\uff09\u5bf9\u4fe1\u4efb\u611f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u64cd\u63a7Furhat\u673a\u5668\u4eba\u7684\u9762\u90e8\u7279\u5f81\uff08\u7279\u522b\u662f\u773c\u775b\u7684\u5f62\u72b6\u548c\u5927\u5c0f\uff09\uff0c\u8fdb\u884c\u4e00\u9879\u7814\u7a76\uff0c\u4ee5\u9a8c\u8bc1'\u5a03\u5a03\u8138'\u5047\u8bf4\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\uff0c\u773c\u775b\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u5bf9\u611f\u77e5\u5230\u7684\u53ef\u4fe1\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u7406\u89e3\u5728\u8bbe\u8ba1\u793e\u4ea4\u673a\u5668\u4eba\u65f6\uff0c\u9700\u8981\u505a\u51fa\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.14129", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14129", "abs": "https://arxiv.org/abs/2509.14129", "authors": ["Kit T. Rodolfa", "Erika Salomon", "Jin Yao", "Steve Yoder", "Robert Sullivan", "Kevin McGuire", "Allie Dickinson", "Rob MacDougall", "Brian Seidler", "Christina Sung", "Claire Herdeman", "Rayid Ghani"], "title": "Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy", "comment": null, "summary": "Many incarcerated individuals face significant and complex challenges,\nincluding mental illness, substance dependence, and homelessness, yet jails and\nprisons are often poorly equipped to address these needs. With little support\nfrom the existing criminal justice system, these needs can remain untreated and\nworsen, often leading to further offenses and a cycle of incarceration with\nadverse outcomes both for the individual and for public safety, with\nparticularly large impacts on communities of color that continue to widen the\nalready extensive racial disparities in criminal justice outcomes. Responding\nto these failures, a growing number of criminal justice stakeholders are\nseeking to break this cycle through innovative approaches such as\ncommunity-driven and alternative approaches to policing, mentoring, community\nbuilding, restorative justice, pretrial diversion, holistic defense, and social\nservice connections. Here we report on a collaboration between Johnson County,\nKansas, and Carnegie Mellon University to perform targeted, proactive mental\nhealth outreach in an effort to reduce reincarceration rates.\n  This paper describes the data used, our predictive modeling approach and\nresults, as well as the design and analysis of a field trial conducted to\nconfirm our model's predictive power, evaluate the impact of this targeted\noutreach, and understand at what level of reincarceration risk outreach might\nbe most effective. Through this trial, we find that our model is highly\npredictive of new jail bookings, with more than half of individuals in the\ntrial's highest-risk group returning to jail in the following year. Outreach\nwas most effective among these highest-risk individuals, with impacts on mental\nhealth utilization, EMS dispatches, and criminal justice involvement.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e0e\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u5408\u4f5c\uff0c\u5728\u7ea6\u7ff0\u900a\u53bf\u5f00\u5c55\u4e86\u9488\u5bf9\u6027\u7684\u5fc3\u7406\u5065\u5eb7\u5916\u5c55\u670d\u52a1\uff0c\u4ee5\u671f\u51cf\u5c11\u91cd\u72af\u7387\u3002", "motivation": "\u8bb8\u591a\u88ab\u76d1\u7981\u7684\u4e2a\u4eba\u9762\u4e34\u5fc3\u7406\u75be\u75c5\u3001\u7269\u8d28\u4f9d\u8d56\u548c\u65e0\u5bb6\u53ef\u5f52\u7b49\u590d\u6742\u6311\u6218\uff0c\u800c\u76d1\u72f1\u548c\u770b\u5b88\u6240\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u7684\u8d44\u6e90\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u73b0\u6709\u5211\u4e8b\u53f8\u6cd5\u7cfb\u7edf\u652f\u6301\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fd9\u4e9b\u95ee\u9898\u6076\u5316\uff0c\u5f15\u53d1\u8fdb\u4e00\u6b65\u7684\u72af\u7f6a\u548c\u76d1\u7981\u5faa\u73af\uff0c\u5bf9\u4e2a\u4eba\u3001\u516c\u4f17\u5b89\u5168\u4ee5\u53ca\u5c11\u6570\u65cf\u88d4\u793e\u533a\u9020\u6210\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u63cf\u8ff0\u4e86\u6240\u4f7f\u7528\u7684\u6570\u636e\u3001\u9884\u6d4b\u6a21\u578b\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u4ee5\u53ca\u5b9e\u5730\u8bd5\u9a8c\u7684\u8bbe\u8ba1\u548c\u5206\u6790\uff0c\u65e8\u5728\u786e\u8ba4\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8bc4\u4f30\u76ee\u6807\u5916\u5c55\u7684\u5f71\u54cd\uff0c\u5e76\u4e86\u89e3\u4f55\u79cd\u518d\u76d1\u7981\u98ce\u9669\u6c34\u5e73\u4e0b\u5916\u5c55\u6700\u4e3a\u6709\u6548\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8be5\u6a21\u578b\u80fd\u9ad8\u5ea6\u9884\u6d4b\u65b0\u7684\u76d1\u72f1\u9884\u8ba2\uff0c\u5728\u8bd5\u9a8c\u7684\u6700\u9ad8\u98ce\u9669\u7fa4\u4f53\u4e2d\uff0c\u8d85\u8fc7\u4e00\u534a\u7684\u4eba\u5728\u6b21\u5e74\u91cd\u8fd4\u76d1\u72f1\u3002\u5916\u5c55\u670d\u52a1\u5bf9\u6700\u9ad8\u98ce\u9669\u4e2a\u4f53\u6700\u4e3a\u6709\u6548\uff0c\u5bf9\u5fc3\u7406\u5065\u5eb7\u5229\u7528\u3001\u6025\u6551\u533b\u7597\u6d3e\u9063\u548c\u5211\u4e8b\u53f8\u6cd5\u53c2\u4e0e\u4ea7\u751f\u4e86\u5f71\u54cd\u3002", "conclusion": "\u9488\u5bf9\u9ad8\u98ce\u9669\u91cd\u72af\u4e2a\u4f53\u8fdb\u884c\u5fc3\u7406\u5065\u5eb7\u5916\u5c55\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u518d\u76d1\u7981\u7387\uff0c\u5e76\u5bf9\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u5229\u7528\u3001\u6025\u6551\u533b\u7597\u6d3e\u9063\u548c\u5211\u4e8b\u53f8\u6cd5\u53c2\u4e0e\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2509.13768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13768", "abs": "https://arxiv.org/abs/2509.13768", "authors": ["Jianhui Chang"], "title": "Generative Image Coding with Diffusion Prior", "comment": null, "summary": "As generative technologies advance, visual content has evolved into a complex\nmix of natural and AI-generated images, driving the need for more efficient\ncoding techniques that prioritize perceptual quality. Traditional codecs and\nlearned methods struggle to maintain subjective quality at high compression\nratios, while existing generative approaches face challenges in visual fidelity\nand generalization. To this end, we propose a novel generative coding framework\nleveraging diffusion priors to enhance compression performance at low bitrates.\nOur approach employs a pre-optimized encoder to generate generalized\ncompressed-domain representations, integrated with the pretrained model's\ninternal features via a lightweight adapter and an attentive fusion module.\nThis framework effectively leverages existing pretrained diffusion models and\nenables efficient adaptation to different pretrained models for new\nrequirements with minimal retraining costs. We also introduce a distribution\nrenormalization method to further enhance reconstruction fidelity. Extensive\nexperiments show that our method (1) outperforms existing methods in visual\nfidelity across low bitrates, (2) improves compression performance by up to 79%\nover H.266/VVC, and (3) offers an efficient solution for AI-generated content\nwhile being adaptable to broader content types.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6269\u6563\u5148\u9a8c\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\uff0c\u4ee5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u63d0\u9ad8\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89c6\u89c9\u5185\u5bb9\u6df7\u5408\u4e86\u81ea\u7136\u548cAI\u751f\u6210\u7684\u56fe\u50cf\uff0c\u8fd9\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u6280\u672f\u6765\u4f18\u5148\u8003\u8651\u611f\u77e5\u8d28\u91cf\u3002\u4f20\u7edf\u7684\u7f16\u7801\u5668\u548c\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u96be\u4ee5\u4fdd\u6301\u4e3b\u89c2\u8d28\u91cf\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u9884\u4f18\u5316\u7684\u7f16\u7801\u5668\u751f\u6210\u901a\u7528\u7684\u538b\u7f29\u57df\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\u96c6\u6210\u3002\u8be5\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5e76\u80fd\u4ee5\u6700\u5c0f\u7684\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9002\u5e94\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u6ee1\u8db3\u65b0\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5e03\u91cd\u6574\u5316\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0eH.266/VVC\u76f8\u6bd4\uff0c\u538b\u7f29\u6027\u80fd\u63d0\u9ad8\u4e8679%\uff0c\u5e76\u4e3aAI\u751f\u6210\u7684\u5185\u5bb9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e5f\u80fd\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u5185\u5bb9\u7c7b\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\u5229\u7528\u6269\u6563\u5148\u9a8c\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u538b\u7f29\u6027\u80fd\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.13949", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.13949", "abs": "https://arxiv.org/abs/2509.13949", "authors": ["Jannick Strangh\u00f6ner", "Philipp Hartmann", "Marco Braun", "Sebastian Wrede", "Klaus Neumann"], "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks", "comment": "8 pages, 5 figures, submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.", "AI": {"tldr": "SHaRe-RL\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u64cd\u4f5c\u539f\u8bed\u3001\u4eba\u7c7b\u6f14\u793a\u548c\u5728\u7ebf\u6821\u6b63\u4ee5\u53ca\u7ea6\u675f\u529b\u6765\u63d0\u9ad8\u5de5\u4e1a\u88c5\u914d\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u9ad8\u6df7\u5408\u4f4e\u4ea7\u91cf\uff08HMLV\uff09\u5de5\u4e1a\u88c5\u914d\u9700\u8981\u9ad8\u7cbe\u5ea6\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u6027\u4ee5\u9002\u5e94\u4ea7\u54c1\u53d8\u5316\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "SHaRe-RL\u6846\u67b6\u901a\u8fc7\uff08i\uff09\u5c06\u6280\u80fd\u5206\u89e3\u4e3a\u64cd\u4f5c\u539f\u8bed\uff0c\uff08ii\uff09\u7ed3\u5408\u4eba\u7c7b\u6f14\u793a\u548c\u5728\u7ebf\u6821\u6b63\uff0c\u4ee5\u53ca\uff08iii\uff09\u901a\u8fc7\u6bcf\u8f74\u67d4\u987a\u6027\u6765\u7ea6\u675f\u4ea4\u4e92\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u957f\u65f6\u7a0b\u3001\u5bcc\u63a5\u89e6\u7684\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u7684\u5728\u7ebf\u9ad8\u6548\u5b89\u5168\u5b66\u4e60\u3002", "result": "\u57280.2-0.4\u6beb\u7c73\u95f4\u9699\u7684\u5de5\u4e1aHarting\u8fde\u63a5\u5668\u6a21\u5757\u63d2\u5165\u5b9e\u9a8c\u4e2d\uff0cSHaRe-RL\u5728\u5b9e\u9645\u53ef\u884c\u7684\u65f6\u9650\u5185\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u9700\u8981\u673a\u5668\u4eba\u6216\u5f3a\u5316\u5b66\u4e60\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u8fc7\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u53ef\u4ee5\u5bf9\u5b66\u4e60\u505a\u51fa\u6709\u610f\u4e49\u7684\u8d21\u732e\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u5b89\u5168\u3001\u66f4\u7a33\u5065\u3001\u66f4\u7ecf\u6d4e\u5730\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u5de5\u4e1a\u88c5\u914d\u3002"}}
{"id": "2509.14158", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14158", "abs": "https://arxiv.org/abs/2509.14158", "authors": ["Feng Ruan", "Keli Liu", "Michael Jordan"], "title": "A Compositional Kernel Model for Feature Learning", "comment": null, "summary": "We study a compositional variant of kernel ridge regression in which the\npredictor is applied to a coordinate-wise reweighting of the inputs. Formulated\nas a variational problem, this model provides a simple testbed for feature\nlearning in compositional architectures. From the perspective of variable\nselection, we show how relevant variables are recovered while noise variables\nare eliminated. We establish guarantees showing that both global minimizers and\nstationary points discard noise coordinates when the noise variables are\nGaussian distributed. A central finding is that $\\ell_1$-type kernels, such as\nthe Laplace kernel, succeed in recovering features contributing to nonlinear\neffects at stationary points, whereas Gaussian kernels recover only linear\nones.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u6838\u5cad\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u5750\u6807\u91cd\u52a0\u6743\u5b9e\u73b0\u7279\u5f81\u5b66\u4e60\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u53d8\u91cf\u9009\u62e9\u548c\u566a\u58f0\u5254\u9664\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u7528\u4e8e\u7279\u5f81\u5b66\u4e60\u7684\u7ec4\u5408\u6838\u5cad\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u4ece\u53d8\u91cf\u9009\u62e9\u7684\u89d2\u5ea6\u5206\u6790\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u6838\u5cad\u56de\u5f52\u6a21\u578b\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u53d8\u5206\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u6062\u590d\u76f8\u5173\u53d8\u91cf\u548c\u5254\u9664\u566a\u58f0\u53d8\u91cf\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u65af\u566a\u58f0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u6062\u590d\u76f8\u5173\u53d8\u91cf\u5e76\u5254\u9664\u566a\u58f0\u53d8\u91cf\u3002\u5168\u5c40\u6700\u5c0f\u5316\u70b9\u548c\u7a33\u5b9a\u70b9\u5728\u566a\u58f0\u53d8\u91cf\u4e3a\u9ad8\u65af\u5206\u5e03\u65f6\uff0c\u5747\u80fd\u5254\u9664\u566a\u58f0\u5750\u6807\u3002$\\\\ell_1$-\u578b\u6838\uff08\u5982\u62c9\u666e\u62c9\u65af\u6838\uff09\u5728\u7a33\u5b9a\u70b9\u80fd\u591f\u6062\u590d\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u800c\u9ad8\u65af\u6838\u53ea\u80fd\u6062\u590d\u7ebf\u6027\u7279\u5f81\u3002", "conclusion": "\u8be5\u7ec4\u5408\u6838\u5cad\u56de\u5f52\u6a21\u578b\u4e3a\u7279\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u53d8\u91cf\u9009\u62e9\u548c\u566a\u58f0\u5254\u9664\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f$\\\\ell_1$-\u578b\u6838\u5728\u6062\u590d\u975e\u7ebf\u6027\u7279\u5f81\u65b9\u9762\u4f18\u4e8e\u9ad8\u65af\u6838\u3002"}}
{"id": "2509.13769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13769", "abs": "https://arxiv.org/abs/2509.13769", "authors": ["Yuechen Luo", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Lei Yang", "Qimao Chen", "Ziang Luo", "Zixun Xie", "Shengyin Jiang", "Jiaxin Liu", "Long Chen", "Bing Wang", "Zhi-xin Yang"], "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.", "AI": {"tldr": "AdaThinkDrive\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c(VLA)\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4e00\u79cd\u53d7\u5feb\u6162\u601d\u8003\u542f\u53d1\u7684\u53cc\u6a21\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u51b3\u7b56\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684Chain of Thought\uff08CoT\uff09\u63a8\u7406\u6280\u672f\u5728\u7b80\u5355\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f1a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u800c\u65e0\u6cd5\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u533a\u5206\u573a\u666f\u5e76\u9009\u62e9\u6027\u5e94\u7528\u63a8\u7406\u7684\u673a\u5236\u3002", "method": "AdaThinkDrive\u9996\u5148\u5728\u5305\u62ecQA\u548c\u8f68\u8ff9\u5728\u5185\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u83b7\u53d6\u4e16\u754c\u77e5\u8bc6\u548c\u9a7e\u9a76\u5e38\u8bc6\u3002\u5728SFT\u9636\u6bb5\uff0c\u5f15\u5165\u4e86\u4e24\u79cd\u6a21\u5f0f\u7684\u6570\u636e\u96c6\uff1a\u5feb\u901f\u56de\u7b54\uff08\u65e0CoT\uff09\u548c\u6162\u901f\u601d\u8003\uff08\u6709CoT\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u533a\u5206\u9700\u8981\u63a8\u7406\u7684\u573a\u666f\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u601d\u8003\u5956\u52b1\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e86\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u4e0b\u7684\u8f68\u8ff9\u8d28\u91cf\u6765\u5956\u52b1\u6a21\u578b\u9009\u62e9\u6027\u5730\u5e94\u7528CoT\u3002", "result": "\u5728Navsim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaThinkDrive\u7684PDMS\u8fbe\u5230\u4e8690.3\uff0c\u6bd4\u4ec5\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e861.7\u4e2a\u70b9\u3002\u4e0e\u4ece\u4e0d\u601d\u8003\u548c\u59cb\u7ec8\u601d\u8003\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cAdaThinkDrive\u7684PDMS\u5206\u522b\u63d0\u9ad8\u4e862.0\u548c1.4\u3002\u4e0e\u59cb\u7ec8\u601d\u8003\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8614%\u3002", "conclusion": "AdaThinkDrive\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u6709\u6548\u5730\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CoT\u65b9\u6cd5\u5728\u7b80\u5355\u573a\u666f\u4e0b\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.13956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13956", "abs": "https://arxiv.org/abs/2509.13956", "authors": ["Zewei Yang", "Zengqi Peng", "Jun Ma"], "title": "SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning", "comment": null, "summary": "Autonomous parking is a critical component for achieving safe and efficient\nurban autonomous driving. However, unstructured environments and dynamic\ninteractions pose significant challenges to autonomous parking tasks. To\naddress this problem, we propose SEG-Parking, a novel end-to-end offline\nreinforcement learning (RL) framework to achieve interaction-aware autonomous\nparking. Notably, a specialized parking dataset is constructed for parking\nscenarios, which include those without interference from the opposite vehicle\n(OV) and complex ones involving interactions with the OV. Based on this\ndataset, a goal-conditioned state encoder is pretrained to map the fused\nperception information into the latent space. Then, an offline RL policy is\noptimized with a conservative regularizer that penalizes out-of-distribution\nactions. Extensive closed-loop experiments are conducted in the high-fidelity\nCARLA simulator. Comparative results demonstrate the superior performance of\nour framework with the highest success rate and robust generalization to\nout-of-distribution parking scenarios. The related dataset and source code will\nbe made publicly available after the paper is accepted.", "AI": {"tldr": "SEG-Parking\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u81ea\u4e3b\u505c\u8f66\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u5bf9\u81ea\u4e3b\u505c\u8f66\u4efb\u52a1\u5e26\u6765\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SEG-Parking\u3002", "method": "SEG-Parking\u6846\u67b6\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6709\u65e0\u5bf9\u9762\u8f66\u8f86\uff08OV\uff09\u4ea4\u4e92\u7684\u505c\u8f66\u6570\u636e\u96c6\uff0c\u5e76\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u76ee\u6807\u6761\u4ef6\u72b6\u6001\u7f16\u7801\u5668\u3002\u7136\u540e\uff0c\u4f7f\u7528\u5305\u542b\u79bb\u7ebf\u7b56\u7565\u4f18\u5316\u548c\u8fc7\u62df\u5408\u884c\u4e3a\u60e9\u7f5a\u7684\u4fdd\u5b88\u6b63\u5219\u5316\u5668\u6765\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u7684\u95ed\u73af\u5b9e\u9a8c\u8868\u660e\uff0cSEG-Parking\u6846\u67b6\u5728\u6700\u9ad8\u6210\u529f\u7387\u548c\u5bf9\u5206\u5e03\u5916\u505c\u8f66\u573a\u666f\u7684\u9c81\u68d2\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "SEG-Parking\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u4ea4\u4e92\u611f\u77e5\u81ea\u4e3b\u505c\u8f66\uff0c\u5e76\u5728\u5404\u79cd\u505c\u8f66\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14167", "categories": ["cs.LG", "q-bio.QM", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.14167", "abs": "https://arxiv.org/abs/2509.14167", "authors": ["Md Rezwan Jaher", "Abul Mukid Mohammad Mukaddes", "A. B. M. Abdul Malek"], "title": "Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework", "comment": "43 pages, 10 figures (including supplementary material)", "summary": "Many critical healthcare decisions are challenged by the inability to measure\nkey underlying parameters. Glaucoma, a leading cause of irreversible blindness\ndriven by elevated intraocular pressure (IOP), provides a stark example. The\nprimary determinant of IOP, a tissue property called trabecular meshwork\npermeability, cannot be measured in vivo, forcing clinicians to depend on\nindirect surrogates. This clinical challenge is compounded by a broader\ncomputational one: developing predictive models for such ill-posed inverse\nproblems is hindered by a lack of ground-truth data and prohibitive cost of\nlarge-scale, high-fidelity simulations. We address both challenges with an\nend-to-end framework to noninvasively estimate unmeasurable variables from\nsparse, routine data. Our approach combines a multi-stage artificial\nintelligence architecture to functionally separate the problem; a novel data\ngeneration strategy we term PCDS that obviates the need for hundreds of\nthousands of costly simulations, reducing the effective computational time from\nyears to hours; and a Bayesian engine to quantify predictive uncertainty. Our\nframework deconstructs a single IOP measurement into its fundamental components\nfrom routine inputs only, yielding estimates for the unmeasurable tissue\npermeability and a patient's outflow facility. Our noninvasively estimated\noutflow facility achieved excellent agreement with state-of-the-art tonography\nwith precision comparable to direct physical instruments. Furthermore, the\nnewly derived permeability biomarker demonstrates high accuracy in stratifying\nclinical cohorts by disease risk, highlighting its diagnostic potential. More\nbroadly, our framework establishes a generalizable blueprint for solving\nsimilar inverse problems in other data-scarce, computationally-intensive\ndomains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u3001\u65b0\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff08PCDS\uff09\u548c\u8d1d\u53f6\u65af\u63a8\u7406\uff0c\u4ece\u7a00\u758f\u7684\u5e38\u89c4\u6570\u636e\u4e2d\u65e0\u521b\u5730\u4f30\u8ba1\u4e0d\u53ef\u6d4b\u91cf\u7684\u53c2\u6570\uff0c\u4ee5\u89e3\u51b3\u773c\u79d1\u9886\u57df\uff08\u5982\u9752\u5149\u773c\uff09\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u65e0\u6cd5\u6d4b\u91cf\u5173\u952e\u53c2\u6570\uff08\u5982\u623f\u6c34\u7f51\u683c\u819c\u6e17\u900f\u6027\uff09\uff0c\u4e34\u5e8a\u4e0a\u5728\u6cbb\u7597\u9752\u5149\u773c\u7b49\u75be\u75c5\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u4e0a\u4e5f\u96be\u4ee5\u89e3\u51b3\u6b64\u7c7b\u75c5\u6001\u9006\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u548c\u9ad8\u4fdd\u771f\u6a21\u62df\u6210\u672c\u9ad8\u6602\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u9636\u6bb5\u4eba\u5de5\u667a\u80fd\u67b6\u6784\u6765\u5206\u79bb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aPCDS\u7684\u65b0\u578b\u6570\u636e\u751f\u6210\u7b56\u7565\u4ee5\u51cf\u5c11\u6a21\u62df\u65f6\u95f4\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u5f15\u64ce\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u5355\u4e00\u7684\u773c\u538b\u6d4b\u91cf\u503c\u4e2d\u63a8\u65ad\u51fa\u4e0d\u53ef\u6d4b\u91cf\u7684\u7ec4\u7ec7\u6e17\u900f\u6027\u548c\u60a3\u8005\u7684\u623f\u6c34\u6d41\u51fa facility\u3002\u65e0\u521b\u4f30\u8ba1\u7684\u623f\u6c34\u6d41\u51fa facility \u4e0e\u6700\u5148\u8fdb\u7684\u773c\u538b\u6d4b\u91cf\u6280\u672f\u5177\u6709\u826f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u65b0\u5174\u7684\u6e17\u900f\u6027\u751f\u7269\u6807\u5fd7\u7269\u5728\u6839\u636e\u75be\u75c5\u98ce\u9669\u5bf9\u4e34\u5e8a\u961f\u5217\u8fdb\u884c\u5206\u5c42\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u5176\u4ed6\u6570\u636e\u7a00\u758f\u3001\u8ba1\u7b97\u5bc6\u96c6\u578b\u9886\u57df\u7684\u7c7b\u4f3c\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u8bca\u65ad\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13776", "abs": "https://arxiv.org/abs/2509.13776", "authors": ["Chao Shuai", "Gaojian Wang", "Kun Pan", "Tong Wu", "Fanli Jin", "Haohan Tan", "Mengxiang Li", "Zhenguang Liu", "Feng Lin", "Kui Ren"], "title": "Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization", "comment": "The 3rd Place, IJCAI 2025 Workshop on Deepfake Detection,\n  Localization, and Interpretability", "summary": "While the pursuit of higher accuracy in deepfake detection remains a central\ngoal, there is an increasing demand for precise localization of manipulated\nregions. Despite the remarkable progress made in classification-based\ndetection, accurately localizing forged areas remains a significant challenge.\nA common strategy is to incorporate forged region annotations during model\ntraining alongside manipulated images. However, such approaches often neglect\nthe complementary nature of local detail and global semantic context, resulting\nin suboptimal localization performance. Moreover, an often-overlooked aspect is\nthe fusion strategy between local and global predictions. Naively combining the\noutputs from both branches can amplify noise and errors, thereby undermining\nthe effectiveness of the localization.\n  To address these issues, we propose a novel approach that independently\npredicts manipulated regions using both local and global perspectives. We\nemploy morphological operations to fuse the outputs, effectively suppressing\nnoise while enhancing spatial coherence. Extensive experiments reveal the\neffectiveness of each module in improving the accuracy and robustness of\nforgery localization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u4f2a\u9020\u533a\u57df\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u9884\u6d4b\u7ed3\u679c\uff0c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u5bf9\u4f2a\u9020\u533a\u57df\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0c\u5e76\u4e14\u5728\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u9884\u6d4b\u65f6\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u4ece\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u72ec\u7acb\u9884\u6d4b\u4f2a\u9020\u533a\u57df\uff0c\u5e76\u91c7\u7528\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u9884\u6d4b\u7ed3\u679c\uff0c\u4ee5\u6291\u5236\u566a\u58f0\u5e76\u589e\u5f3a\u7a7a\u95f4\u76f8\u5e72\u6027\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4e2d\u7684\u6bcf\u4e2a\u6a21\u5757\u90fd\u80fd\u6709\u6548\u63d0\u9ad8\u4f2a\u9020\u533a\u57df\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6df1\u5ea6\u4f2a\u9020\u533a\u57df\u5b9a\u4f4d\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13965", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13965", "abs": "https://arxiv.org/abs/2509.13965", "authors": ["Abhijeet Nayak", "D\u00e9bora N. P. Oliveira", "Samiran Gode", "Cordelia Schmid", "Wolfram Burgard"], "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies", "comment": null, "summary": "Generative navigation policies have made rapid progress in improving\nend-to-end learned navigation. Despite their promising results, this paradigm\nhas two structural problems. First, the sampled trajectories exist in an\nabstract, unscaled space without metric grounding. Second, the control strategy\ndiscards the full path, instead moving directly towards a single waypoint. This\nleads to short-sighted and unsafe actions, moving the robot towards obstacles\nthat a complete and correctly scaled path would circumvent. To address these\nissues, we propose MetricNet, an effective add-on for generative navigation\nthat predicts the metric distance between waypoints, grounding policy outputs\nin real-world coordinates. We evaluate our method in simulation with a new\nbenchmarking framework and show that executing MetricNet-scaled waypoints\nsignificantly improves both navigation and exploration performance. Beyond\nsimulation, we further validate our approach in real-world experiments.\nFinally, we propose MetricNav, which integrates MetricNet into a navigation\npolicy to guide the robot away from obstacles while still moving towards the\ngoal.", "AI": {"tldr": "\u751f\u6210\u5bfc\u822a\u7b56\u7565\u5b58\u5728\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u63d0\u51faMetricNet\u4ee5\u89e3\u51b3\u5ea6\u91cf\u548c\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5bfc\u822a\u7b56\u7565\u5728\u5ea6\u91cf\u5c3a\u5ea6\u548c\u8def\u5f84\u89c4\u5212\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u673a\u5668\u4eba\u505a\u51fa\u77ed\u89c6\u548c\u4e0d\u5b89\u5168\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51faMetricNet\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5bfc\u822a\u7684\u9644\u52a0\u6a21\u5757\uff0c\u7528\u4e8e\u9884\u6d4b\u822a\u70b9\u4e4b\u95f4\u7684\u5ea6\u91cf\u8ddd\u79bb\uff0c\u5e76\u5c06\u7b56\u7565\u8f93\u51fa\u6620\u5c04\u5230\u771f\u5b9e\u4e16\u754c\u5750\u6807\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faMetricNav\uff0c\u5c06MetricNet\u96c6\u6210\u5230\u5bfc\u822a\u7b56\u7565\u4e2d\uff0c\u5f15\u5bfc\u673a\u5668\u4eba\u907f\u5f00\u969c\u788d\u7269\u5e76\u5411\u76ee\u6807\u524d\u8fdb\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cMetricNet\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u548c\u63a2\u7d22\u6027\u80fd\uff0cMetricNav\u80fd\u591f\u5f15\u5bfc\u673a\u5668\u4eba\u907f\u5f00\u969c\u788d\u7269\u3002", "conclusion": "MetricNet\u80fd\u591f\u89e3\u51b3\u751f\u6210\u5bfc\u822a\u7b56\u7565\u7684\u5ea6\u91cf\u5c3a\u5ea6\u95ee\u9898\uff0cMetricNav\u5219\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.14169", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14169", "abs": "https://arxiv.org/abs/2509.14169", "authors": ["Ziming Wei", "Zichen Kong", "Yuan Wang", "David Z. Pan", "Xiyuan Tang"], "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits", "comment": null, "summary": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TopoSizing\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6a21\u62df\u548c\u6df7\u5408\u4fe1\u53f7\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002\u5b83\u7ed3\u5408\u4e86\u56fe\u7b97\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u7406\u89e3\u7535\u8def\u7ed3\u6784\uff0c\u5e76\u5c06\u8fd9\u4e9b\u77e5\u8bc6\u878d\u5165\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u3001\u9886\u57df\u77e5\u8bc6\u5d4c\u5165\u3001\u4f18\u5316\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u7535\u8def\u7406\u89e3\u80fd\u529b\uff0c\u8981\u4e48\u6210\u672c\u9ad8\u6602\u4e14\u6cdb\u5316\u6027\u5dee\u3002LLM\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u5e38\u9700\u4eba\u5de5\u5e72\u9884\u3002", "method": "TopoSizing\u6846\u67b6\u9996\u5148\u4f7f\u7528\u56fe\u7b97\u6cd5\u5c06\u7535\u8def\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316\u7684\u8bbe\u5907-\u6a21\u5757-\u9636\u6bb5\u8868\u793a\u3002\u7136\u540e\uff0cLLM\u4ee3\u7406\u901a\u8fc7\u8fed\u4ee3\u7684\u5047\u8bbe-\u9a8c\u8bc1-\u7cbe\u70bc\u5faa\u73af\u8fdb\u884c\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u751f\u6210\u663e\u5f0f\u6ce8\u89e3\u3002\u6700\u540e\uff0c\u5c06\u9a8c\u8bc1\u540e\u7684\u89c1\u89e3\u6574\u5408\u5230\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u521d\u59cb\u91c7\u6837\u548c\u505c\u6ede\u89e6\u53d1\u7684\u4fe1\u4efb\u533a\u57df\u66f4\u65b0\u6765\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "result": "TopoSizing\u6846\u67b6\u901a\u8fc7\u5c06\u7535\u8def\u7ed3\u6784\u77e5\u8bc6\u8f6c\u5316\u4e3a\u4f18\u5316\u6536\u76ca\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "conclusion": "TopoSizing\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56fe\u7b97\u6cd5\u548cLLM\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7535\u8def\u7684\u9c81\u68d2\u7406\u89e3\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u4f18\u5316\u589e\u76ca\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u3001\u77e5\u8bc6\u5d4c\u5165\u548c\u4f18\u5316\u6548\u7387\u65b9\u9762\u7684\u75db\u70b9\u3002"}}
{"id": "2509.13784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13784", "abs": "https://arxiv.org/abs/2509.13784", "authors": ["Hanfang Liang", "Bing Wang", "Shizhen Zhang", "Wen Jiang", "Yizhuo Yang", "Weixiang Guo", "Shenghai Yuan"], "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling", "comment": "8 pages, 6 figures", "summary": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cVariable-Rate Spatial Event Mamba\u201d\u7684\u65b0\u578b\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u5904\u7406\u67b6\u6784\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u80fd\u6839\u636e\u4e8b\u4ef6\u901f\u7387\u81ea\u9002\u5e94\u8c03\u6574\u5904\u7406\u901f\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u5c06\u4e8b\u4ef6\u6d41\u8f6c\u6362\u4e3a\u4e2d\u95f4\u8868\u793a\uff08\u5982\u5e27\u3001\u4f53\u7d20\u7f51\u683c\u6216\u70b9\u4e91\uff09\uff0c\u8fd9\u9700\u8981\u9884\u5b9a\u4e49\u7684\u65f6\u95f4\u7a97\u53e3\uff0c\u5bfc\u81f4\u7a97\u53e3\u5ef6\u8fdf\u3002\u540c\u65f6\uff0c\u9010\u70b9\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cVariable-Rate Spatial Event Mamba\u201d\u7684\u65b0\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u56e0\u679c\u7a7a\u95f4\u90bb\u57df\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u3002\u5728\u63a8\u7406\u65f6\uff0c\u4e00\u4e2a\u63a7\u5236\u5668\u80fd\u6839\u636e\u4e8b\u4ef6\u901f\u7387\u81ea\u9002\u5e94\u5730\u8c03\u6574\u5904\u7406\u901f\u5ea6\uff0c\u4ee5\u5e73\u8861\u7a97\u53e3\u5ef6\u8fdf\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u8be5\u65b9\u6cd5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548cMamba\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u7684\u65f6\u7a7a\u4fe1\u606f\u63d0\u53d6\u3002\u63a8\u7406\u65f6\uff0c\u81ea\u9002\u5e94\u901f\u7387\u8c03\u6574\u673a\u5236\u80fd\u591f\u4f18\u5316\u5ef6\u8fdf\u3002", "conclusion": "Variable-Rate Spatial Event Mamba \u67b6\u6784\u80fd\u591f\u76f4\u63a5\u5904\u7406\u4e8b\u4ef6\u6d41\uff0c\u6709\u6548\u6355\u6349\u65f6\u7a7a\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u901f\u7387\u8c03\u6574\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13972", "abs": "https://arxiv.org/abs/2509.13972", "authors": ["Asier Bikandi", "Miguel Fernandez-Cortizas", "Muhammad Shaheer", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM Informed Visual SLAM for Construction Monitoring", "comment": "8 pages, 5 tables, 4 figures", "summary": "Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring\nconstruction sites, where aligning the evolving as-built state with the\nas-planned design enables early error detection and reduces costly rework.\nLiDAR-based SLAM achieves high geometric precision, but its sensors are\ntypically large and power-demanding, limiting their use on portable platforms.\nVisual SLAM offers a practical alternative with lightweight cameras already\nembedded in most mobile devices. however, visually mapping construction\nenvironments remains challenging: repetitive layouts, occlusions, and\nincomplete or low-texture structures often cause drift in the trajectory map.\nTo mitigate this, we propose an RGB-D SLAM system that incorporates the\nBuilding Information Model (BIM) as structural prior knowledge. Instead of\nrelying solely on visual cues, our system continuously establishes\ncorrespondences between detected wall and their BIM counterparts, which are\nthen introduced as constraints in the back-end optimization. The proposed\nmethod operates in real time and has been validated on real construction sites,\nreducing trajectory error by an average of 23.71% and map RMSE by 7.14%\ncompared to visual SLAM baselines. These results demonstrate that BIM\nconstraints enable reliable alignment of the digital plan with the as-built\nscene, even under partially constructed conditions.", "AI": {"tldr": "RGB-D SLAM\u7cfb\u7edf\u7ed3\u5408BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u65bd\u5de5\u73b0\u573a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c06\u8f68\u8ff9\u8bef\u5dee\u5e73\u5747\u964d\u4f4e23.71%\uff0c\u5730\u56feRMSE\u964d\u4f4e7.14%\u3002", "motivation": "\u5efa\u7b51\u65bd\u5de5\u76d1\u63a7\u9700\u8981SLAM\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7684LiDAR-SLM\u56e0\u4f20\u611f\u5668\u4f53\u79ef\u5927\u3001\u529f\u8017\u9ad8\u800c\u53d7\u9650\uff0c\u800c\u89c6\u89c9SLAM\u5728\u91cd\u590d\u6027\u5e03\u5c40\u3001\u906e\u6321\u3001\u7eb9\u7406\u7f3a\u5931\u7b49\u95ee\u9898\u4e0a\u5b58\u5728\u8f68\u8ff9\u6f02\u79fb\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684SLAM\u65b9\u6cd5\u3002 ", "method": "\u63d0\u51fa\u4e00\u79cdRGB-D SLAM\u7cfb\u7edf\uff0c\u5c06BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u68c0\u6d4b\u5230\u7684\u5899\u4f53\u4e0eBIM\u6a21\u578b\u5bf9\u5e94\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5bf9\u5e94\u5173\u7cfb\u4f5c\u4e3a\u7ea6\u675f\u5f15\u5165\u540e\u7aef\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9SLAM\u7684\u6311\u6218\u3002", "result": "\u5728\u5b9e\u9645\u65bd\u5de5\u73b0\u573a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e0e\u89c6\u89c9SLAM\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5c06\u8f68\u8ff9\u8bef\u5dee\u5e73\u5747\u964d\u4f4e23.71%\uff0c\u5730\u56feRMSE\u964d\u4f4e7.14%\u3002", "conclusion": "BIM\u7ea6\u675f\u80fd\u591f\u5b9e\u73b0\u6570\u5b57\u89c4\u5212\u4e0e\u5b9e\u9645\u573a\u666f\u7684\u53ef\u9760\u5bf9\u9f50\uff0c\u5373\u4f7f\u5728\u90e8\u5206\u65bd\u5de5\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u3002"}}
{"id": "2509.14172", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14172", "abs": "https://arxiv.org/abs/2509.14172", "authors": ["Ziyuan Chen", "Zhenghui Zhao", "Zhangye Han", "Miancan Liu", "Xianhang Ye", "Yiqing Li", "Hongbo Min", "Jinkui Ren", "Xiantao Zhang", "Guitao Cao"], "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning", "comment": null, "summary": "With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps.", "AI": {"tldr": "TGPO\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u72b6\u8f68\u8ff9\u8868\u793a\u3001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u548c\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aWeb\u4ee3\u7406\u8fdb\u884c\u8bad\u7ec3\u65f6\u9762\u4e34\u7684\u4fe1\u7528\u5206\u914d\u9519\u8bef\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u5956\u52b1\u7a00\u758f\u7b49\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f7f\u7528\u5927\u578b\u6a21\u578b\u4f5c\u4e3aWeb\u4ee3\u7406\u5df2\u6210\u4e3a\u81ea\u52a8\u5316Web\u4ea4\u4e92\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3Web\u4ee3\u7406\u9762\u4e34\u4fe1\u7528\u5206\u914d\u9519\u8bef\u3001\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u548c\u5956\u52b1\u7a00\u758f\u7b49\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTree-Guided Preference Optimization (TGPO) \u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8f68\u8ff9\u8868\u793a\uff0c\u5408\u5e76\u8bed\u4e49\u4e0a\u76f8\u540c\u7684\u72b6\u6001\u4ee5\u6d88\u9664\u6807\u7b7e\u51b2\u7a81\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u4e00\u4e2a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5b50\u76ee\u6807\u8fdb\u5c55\u3001\u5197\u4f59\u68c0\u6d4b\u548c\u52a8\u4f5c\u9a8c\u8bc1\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u5956\u52b1\u3002\u540c\u65f6\uff0c\u5229\u7528\u52a8\u6001\u52a0\u6743\u673a\u5236\u5728\u8bad\u7ec3\u4e2d\u4f18\u5148\u8003\u8651\u9ad8\u5f71\u54cd\u529b\u51b3\u7b56\u70b9\u3002", "result": "\u5728Online-Mind2Web\u548c\u81ea\u5efa\u7684C-WebShop\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eTGPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u5197\u4f59\u6b65\u9aa4\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "TGPO\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728\u8bad\u7ec3Web\u4ee3\u7406\u65f6\u9047\u5230\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13789", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13789", "abs": "https://arxiv.org/abs/2509.13789", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Zhifei Xu", "Zhi Yao", "Wenyi Zeng", "Weijia Jia"], "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching", "comment": null, "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBlock-Wise Caching (BWCache) \u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7684\u4e2d\u95f4\u7279\u5f81\u6765\u52a0\u901f\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\uff0c\u5728\u4fdd\u8bc1\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.24\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684DiT\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u89c6\u89c9\u8d28\u91cf\uff0c\u8981\u4e48\u65e0\u6cd5\u5728\u5408\u9002\u7684\u7c92\u5ea6\u4e0a\u91cd\u7528\u4e2d\u95f4\u7279\u5f81\u3002", "method": "\u63d0\u51faBlock-Wise Caching (BWCache) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u5730\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u8de8\u6269\u6563\u65f6\u95f4\u6b65\u7684\u7279\u5f81\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u76f8\u4f3c\u6027\u6307\u793a\u5668\uff0c\u4ec5\u5f53\u76f8\u90bb\u65f6\u95f4\u6b65\u7684\u5757\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u5f02\u4f4e\u4e8e\u9608\u503c\u65f6\u624d\u89e6\u53d1\u7279\u5f81\u91cd\u7528\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "BWCache\u5728\u591a\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.24\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "BWCache\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u52a0\u901fDiT\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u6548\u679c\u3002"}}
{"id": "2509.13836", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13836", "abs": "https://arxiv.org/abs/2509.13836", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08VHBench-10\uff09\u548c\u4e00\u79cd\u540d\u4e3a VisionWeaver \u7684\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8def\u7531\u7f51\u7edc\uff0c\u6765\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5177\u6709\u4e0d\u540c\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4\u5176\u5728\u5e7b\u89c9\u65b9\u9762\u7684\u8868\u73b0\u5404\u5f02\u3002VisionWeaver \u901a\u8fc7\u52a8\u6001\u805a\u5408\u591a\u4e2a\u4e13\u4e1a\u5316\u4e13\u5bb6\u7684\u89c6\u89c9\u7279\u5f81\u6765\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9009\u62e9\u5bf9\u51c6\u786e\u89e3\u91ca\u89c6\u89c9\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u8bad\u7ec3\u8303\u5f0f\u53ef\u80fd\u5bfc\u81f4\u5176\u4ea7\u751f\u4e0d\u540c\u7684\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VHBench-10 \u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7ea6 10,000 \u4e2a\u6837\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30 LVLMs \u5728\u5341\u79cd\u7ec6\u7c92\u5ea6\u5e7b\u89c9\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a VisionWeaver \u7684\u65b0\u9896\u4e0a\u4e0b\u6587\u611f\u77e5\u8def\u7531\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u4f7f\u7528\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u751f\u6210\u8def\u7531\u4fe1\u53f7\uff0c\u5e76\u52a8\u6001\u805a\u5408\u6765\u81ea\u591a\u4e2a\u4e13\u4e1a\u5316\u4e13\u5bb6\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u901a\u8fc7 VHBench-10 \u7684\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u4e0d\u540c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u786e\u5b9e\u5177\u6709\u72ec\u7279\u7684\u5e7b\u89c9\u7279\u5f81\u3002\u5b9e\u9a8c\u8bc1\u660e VisionWeaver \u80fd\u591f\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9009\u62e9\u5bf9 LVLMs \u7684\u5e7b\u89c9\u8868\u73b0\u6709\u91cd\u8981\u5f71\u54cd\u3002\u63d0\u51fa\u7684 VisionWeaver \u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u8def\u7531\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002"}}
{"id": "2509.13998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13998", "abs": "https://arxiv.org/abs/2509.13998", "authors": ["Bailey Dacre", "Rodrigo Moreno", "Serhat Demirtas", "Ziqiao Wang", "Yuhao Jiang", "Jamie Paik", "Kasper Stoy", "Andr\u00e9s Fa\u00ed\u00f1a"], "title": "Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array", "comment": null, "summary": "Object manipulation is a fundamental challenge in robotics, where systems\nmust balance trade-offs among manipulation capabilities, system complexity, and\nthroughput. Distributed manipulator systems (DMS) use the coordinated motion of\nactuator arrays to perform complex object manipulation tasks, seeing widespread\nexploration within the literature and in industry. However, existing DMS\ndesigns typically rely on high actuator densities and impose constraints on\nobject-to-actuator scale ratios, limiting their adaptability. We present a\nnovel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles\ninterconnected by a compliant surface layer. Unlike conventional DMS, our\napproach enables manipulation not only at the actuator end effectors but also\nacross a flexible surface connecting all actuators; creating a continuous,\ncontrollable manipulation surface. We analyse the combined workspace of such a\nsystem, derive simple motion primitives, and demonstrate its capabilities to\ntranslate simple geometric objects across an array of tiles. By leveraging the\ninter-tile connective material, our approach significantly reduces actuator\ndensity, increasing the area over which an object can be manipulated by x1.84\nwithout an increase in the number of actuators. This design offers a lower cost\nand complexity alternative to traditional high-density arrays, and introduces\nnew opportunities for manipulation strategies that leverage the flexibility of\nthe interconnected surface.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u64cd\u7eb5\u5668\u7cfb\u7edf\uff08DMS\uff09\uff0c\u8be5\u7cfb\u7edf\u4f7f\u75283-DoF\u3001\u53d7\u6298\u7eb8\u542f\u53d1\u7684\u673a\u5668\u4eba\u74e6\u7247\u9635\u5217\uff0c\u7531\u67d4\u6027\u8868\u9762\u5c42\u4e92\u8054\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u3001\u66f4\u4f4e\u6210\u672c\u7684\u7269\u4f53\u64cd\u7eb5\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u64cd\u7eb5\u5668\u7cfb\u7edf\uff08DMS\uff09\u8bbe\u8ba1\u901a\u5e38\u9700\u8981\u9ad8\u6267\u884c\u5668\u5bc6\u5ea6\u5e76\u5bf9\u7269\u4f53\u4e0e\u6267\u884c\u5668\u7684\u5c3a\u5bf8\u6bd4\u4f8b\u6709\u9650\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9002\u5e94\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u578bDMS\u8bbe\u8ba1\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283-DoF\u3001\u53d7\u6298\u7eb8\u542f\u53d1\u7684\u673a\u5668\u4eba\u74e6\u7247\u9635\u5217\uff0c\u5e76\u7531\u67d4\u6027\u8868\u9762\u5c42\u4e92\u8054\u7684\u65b0\u578bDMS\u8bbe\u8ba1\u3002\u8be5\u8bbe\u8ba1\u4e0d\u4ec5\u53ef\u4ee5\u5728\u6267\u884c\u5668\u672b\u7aef\u6267\u884c\u5668\u6267\u884c\u64cd\u4f5c\uff0c\u8fd8\u53ef\u4ee5\u5728\u8fde\u63a5\u6240\u6709\u6267\u884c\u5668\u7684\u67d4\u6027\u8868\u9762\u4e0a\u8fdb\u884c\u64cd\u4f5c\uff0c\u4ece\u800c\u521b\u5efa\u4e00\u4e2a\u8fde\u7eed\u3001\u53ef\u63a7\u7684\u64cd\u4f5c\u8868\u9762\u3002\u5bf9\u7cfb\u7edf\u7684\u7ec4\u5408\u5de5\u4f5c\u7a7a\u95f4\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63a8\u5bfc\u4e86\u7b80\u5355\u7684\u8fd0\u52a8\u539f\u8bed\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u3001\u9ad8\u5bc6\u5ea6\u7684DMS\u76f8\u6bd4\uff0c\u8be5\u65b0\u578bDMS\u8bbe\u8ba1\u80fd\u591f\u4ee51.84\u500d\u7684\u6bd4\u4f8b\u589e\u52a0\u64cd\u4f5c\u5bf9\u8c61\u7684\u9762\u79ef\uff0c\u800c\u65e0\u9700\u589e\u52a0\u6267\u884c\u5668\u7684\u6570\u91cf\u3002\u8fd9\u8868\u660e\u8be5\u8bbe\u8ba1\u5728\u964d\u4f4e\u6210\u672c\u548c\u590d\u6742\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b0\u578bDMS\u8bbe\u8ba1\u901a\u8fc7\u5229\u7528\u74e6\u7247\u95f4\u7684\u8fde\u63a5\u6750\u6599\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6267\u884c\u5668\u5bc6\u5ea6\uff0c\u5e76\u6269\u5c55\u4e86\u64cd\u4f5c\u8303\u56f4\u3002\u5b83\u4e3a\u4f20\u7edf\u9ad8\u5bc6\u5ea6\u9635\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u66f4\u4f4e\u3001\u590d\u6742\u5ea6\u66f4\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e3a\u5229\u7528\u4e92\u8054\u8868\u9762\u7075\u6d3b\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u64cd\u4f5c\u7b56\u7565\u673a\u4f1a\u3002"}}
{"id": "2509.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14181", "abs": "https://arxiv.org/abs/2509.14181", "authors": ["Yifan Hu", "Jie Yang", "Tian Zhou", "Peiyuan Liu", "Yujin Tang", "Rong Jin", "Liang Sun"], "title": "Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting", "comment": null, "summary": "Representation learning techniques like contrastive learning have long been\nexplored in time series forecasting, mirroring their success in computer vision\nand natural language processing. Yet recent state-of-the-art (SOTA) forecasters\nseldom adopt these representation approaches because they have shown little\nperformance advantage. We challenge this view and demonstrate that explicit\nrepresentation alignment can supply critical information that bridges the\ndistributional gap between input histories and future targets. To this end, we\nintroduce TimeAlign, a lightweight, plug-and-play framework that learns\nauxiliary features via a simple reconstruction task and feeds them back to any\nbase forecaster. Extensive experiments across eight benchmarks verify its\nsuperior performance. Further studies indicate that the gains arises primarily\nfrom correcting frequency mismatches between historical inputs and future\noutputs. We also provide a theoretical justification for the effectiveness of\nTimeAlign in increasing the mutual information between learned representations\nand predicted targets. As it is architecture-agnostic and incurs negligible\noverhead, TimeAlign can serve as a general alignment module for modern deep\nlearning time-series forecasting systems. The code is available at\nhttps://github.com/TROUBADOUR000/TimeAlign.", "AI": {"tldr": "TimeAlign\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8f85\u52a9\u7279\u5f81\u6765\u5f25\u5408\u8f93\u5165\u5386\u53f2\u548c\u672a\u6765\u76ee\u6807\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u8868\u793a\u5b66\u4e60\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5df2\u88ab\u63a2\u7d22\uff0c\u4f46\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u5668\u5f88\u5c11\u91c7\u7528\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u663e\u793a\u51fa\u660e\u663e\u7684\u6027\u80fd\u4f18\u52bf\u3002\u672c\u7814\u7a76\u6311\u6218\u4e86\u8fd9\u4e00\u89c2\u70b9\uff0c\u8ba4\u4e3a\u663e\u5f0f\u7684\u8868\u793a\u5bf9\u9f50\u53ef\u4ee5\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\uff0c\u4ee5\u5f25\u5408\u8f93\u5165\u5386\u53f2\u548c\u672a\u6765\u76ee\u6807\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\u3002", "method": "TimeAlign\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u91cd\u5efa\u4efb\u52a1\u6765\u5b66\u4e60\u8f85\u52a9\u7279\u5f81\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7279\u5f81\u53cd\u9988\u7ed9\u4efb\u4f55\u57fa\u7840\u9884\u6d4b\u5668\u3002\u8be5\u6846\u67b6\u662f\u8f7b\u91cf\u7ea7\u7684\u3001\u5373\u63d2\u5373\u7528\u7684\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTimeAlign \u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u8868\u660e\uff0c\u6027\u80fd\u7684\u63d0\u5347\u4e3b\u8981\u5f52\u56e0\u4e8e\u7ea0\u6b63\u4e86\u5386\u53f2\u8f93\u5165\u548c\u672a\u6765\u8f93\u51fa\u4e4b\u95f4\u7684\u9891\u7387\u4e0d\u5339\u914d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u8bc1\u660e\u4e86 TimeAlign \u5728\u589e\u52a0\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e0e\u9884\u6d4b\u76ee\u6807\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TimeAlign \u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u5bf9\u9f50\u6a21\u5757\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.13792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13792", "abs": "https://arxiv.org/abs/2509.13792", "authors": ["Inder Pal Singh", "Nidhal Eddine Chenni", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation", "comment": null, "summary": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\uff08SPE\uff09\u5173\u952e\u70b9\u56de\u5f52\u7684\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08SDA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6807\u8bb0\u7684\u5408\u6210\u6570\u636e\u548c\u6709\u9650\u7684\u6807\u8bb0\u771f\u5b9e\u6570\u636e\uff0c\u8054\u5408\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\uff0c\u4ee5\u51cf\u5c11\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u8bef\u5dee\u3002", "motivation": "\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u5408\u6210\u5230\u771f\u5b9e\u7684\u57df\u95f4\u9699\uff0c\u800c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u6709\u5c11\u91cf\u6807\u8bb0\u76ee\u6807\u6837\u672c\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u4e0d\u53d8\u8868\u793a\u548c\u98ce\u9669\uff08LIRR\uff09\u8303\u5f0f\u7684\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08SDA\uff09\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\uff0c\u5229\u7528\u6807\u8bb0\u7684\u5408\u6210\u6570\u636e\u548c\u6709\u9650\u7684\u6807\u8bb0\u771f\u5b9e\u6570\u636e\u3002", "result": "\u5728SPEED+\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6e90\u6570\u636e\u3001\u5fae\u8c03\u548cOracle\u57fa\u7ebf\u3002\u5373\u4f7f\u53ea\u67095%\u7684\u6807\u8bb0\u76ee\u6807\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4f7f\u7528\u66f4\u591a\u6807\u8bb0\u6570\u636e\u8bad\u7ec3\u7684Oracle\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u8f7b\u91cf\u7ea7\u3001\u9aa8\u5e72\u7f51\u7edc\u65e0\u5173\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4e3a\u5728\u771f\u5b9e\u592a\u7a7a\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u90e8\u7f72\u7684\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\u3002"}}
{"id": "2509.14010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14010", "abs": "https://arxiv.org/abs/2509.14010", "authors": ["Zong Chen", "Shaoyang Li", "Ben Liu", "Min Li", "Zhouping Yin", "Yiqun Li"], "title": "Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization", "comment": null, "summary": "Wheel-legged robots with integrated manipulators hold great promise for\nmobile manipulation in logistics, industrial automation, and human-robot\ncollaboration. However, unified control of such systems remains challenging due\nto the redundancy in degrees of freedom, complex wheel-ground contact dynamics,\nand the need for seamless coordination between locomotion and manipulation. In\nthis work, we present the design and whole-body motion control of an\nomnidirectional wheel-legged quadrupedal robot equipped with a dexterous\nmanipulator. The proposed platform incorporates independently actuated steering\nmodules and hub-driven wheels, enabling agile omnidirectional locomotion with\nhigh maneuverability in structured environments. To address the challenges of\ncontact-rich interaction, we develop a contact-aware whole-body dynamic\noptimization framework that integrates point-contact modeling for manipulation\nwith line-contact modeling for wheel-ground interactions. A warm-start strategy\nis introduced to accelerate online optimization, ensuring real-time feasibility\nfor high-dimensional control. Furthermore, a unified kinematic model tailored\nfor the robot's 4WIS-4WID actuation scheme eliminates the need for mode\nswitching across different locomotion strategies, improving control consistency\nand robustness. Simulation and experimental results validate the effectiveness\nof the proposed framework, demonstrating agile terrain traversal, high-speed\nomnidirectional mobility, and precise manipulation under diverse scenarios,\nunderscoring the system's potential for factory automation, urban logistics,\nand service robotics in semi-structured environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6e\u817f\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u53ca\u5176\u6574\u4f53\u80b2\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u96c6\u6210\u673a\u68b0\u81c2\u7684\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u6709\u671b\u5728\u7269\u6d41\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u4eba\u673a\u534f\u4f5c\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u6574\u4f53\u80b2\u52a8\u63a7\u5236\u9762\u4e34\u81ea\u7531\u5ea6\u5197\u4f59\u3001\u8f6e\u5730\u63a5\u89e6\u52a8\u529b\u5b66\u590d\u6742\u4ee5\u53ca\u79fb\u52a8\u4e0e\u64cd\u4f5c\u534f\u8c03\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u72ec\u7acb\u7684\u8f6c\u5411\u9a71\u52a8\u6a21\u5757\u548c\u8f6e\u6bc2\u9a71\u52a8\u8f6e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u673a\u52a8\u6027\u7684\u5168\u5411\u79fb\u52a8\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u63a5\u89e6\u7684\u6574\u4f53\u80b2\u52a8\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u64cd\u4f5c\u7684\u70b9\u63a5\u89e6\u6a21\u578b\u548c\u8f66\u8f6e\u7684\u7ebf\u63a5\u89e6\u6a21\u578b\u3002\u5f15\u5165\u4e86\u70ed\u542f\u52a8\u7b56\u7565\u52a0\u901f\u5728\u7ebf\u4f18\u5316\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\u4ee5\u6d88\u9664\u4e0d\u540c\u79fb\u52a8\u7b56\u7565\u95f4\u7684\u6a21\u5f0f\u5207\u6362\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u654f\u6377\u7684\u5730\u5f62\u7a7f\u8d8a\u3001\u9ad8\u901f\u5168\u5411\u79fb\u52a8\u4ee5\u53ca\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u7cbe\u786e\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u53ca\u5176\u6574\u4f53\u80b2\u52a8\u63a7\u5236\u6846\u67b6\u5728\u5de5\u5382\u81ea\u52a8\u5316\u3001\u57ce\u5e02\u7269\u6d41\u548c\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13688", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13688", "abs": "https://arxiv.org/abs/2509.13688", "authors": ["James Jincheng", "Youcheng Cai", "Ligang Liu"], "title": "CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion", "comment": null, "summary": "Controllable, high-fidelity mesh editing remains a significant challenge in\n3D content creation. Existing generative methods often struggle with complex\ngeometries and fail to produce detailed results. We propose CraftMesh, a novel\nframework for high-fidelity generative mesh manipulation via Poisson Seamless\nFusion. Our key insight is to decompose mesh editing into a pipeline that\nleverages the strengths of 2D and 3D generative models: we edit a 2D reference\nimage, then generate a region-specific 3D mesh, and seamlessly fuse it into the\noriginal model. We introduce two core techniques: Poisson Geometric Fusion,\nwhich utilizes a hybrid SDF/Mesh representation with normal blending to achieve\nharmonious geometric integration, and Poisson Texture Harmonization for\nvisually consistent texture blending. Experimental results demonstrate that\nCraftMesh outperforms state-of-the-art methods, delivering superior global\nconsistency and local detail in complex editing tasks.", "AI": {"tldr": "CraftMesh\u901a\u8fc7\u7ed3\u54082D\u548c3D\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5229\u7528\u6cca\u677e\u65e0\u7f1d\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u53ef\u63a7\u7f51\u683c\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u548c\u751f\u6210\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "CraftMesh\u5c06\u7f51\u683c\u7f16\u8f91\u5206\u89e3\u4e3a2D\u56fe\u50cf\u7f16\u8f91\u3001\u751f\u6210\u533a\u57df\u7279\u5b9a3D\u7f51\u683c\uff0c\u7136\u540e\u65e0\u7f1d\u878d\u5408\u5230\u539f\u59cb\u6a21\u578b\u4e2d\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u6cca\u677e\u51e0\u4f55\u878d\u5408\uff08\u7ed3\u5408SDF/Mesh\u8868\u793a\u548c\u6cd5\u7ebf\u6df7\u5408\uff09\u548c\u6cca\u677e\u7eb9\u7406\u534f\u8c03\u6280\u672f\u3002", "result": "CraftMesh\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0c\u5728\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "CraftMesh\u4e3a\u9ad8\u4fdd\u771f\u5ea6\u7684\u53ef\u63a7\u7f51\u683c\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2509.13795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13795", "abs": "https://arxiv.org/abs/2509.13795", "authors": ["Jiayu Yuan", "Ming Dai", "Enhui Zheng", "Chao Su", "Nanxing Chen", "Qiming Hu", "Shibo Zhu", "Yibin Cao"], "title": "SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments", "comment": null, "summary": "Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been\nextensively investigated for Global Navigation Satellite System (GNSS)-denied\nenvironments. However, existing retrieval-based approaches face limitations in\ndataset availability and persistent challenges including suboptimal real-time\nperformance, environmental sensitivity, and limited generalization capability,\nparticularly in dynamic or temporally varying environments. To overcome these\nlimitations, we present a large-scale Multi-Altitude Flight Segments dataset\n(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted\nAdaptive Particle Filter (SWA-PF) method. This approach integrates robust\nsemantic features from both UAV-captured images and satellite imagery through\ntwo key innovations: a semantic weighting mechanism and an optimized particle\nfiltering architecture. Evaluated using our dataset, the proposed method\nachieves 10x computational efficiency gain over feature extraction methods,\nmaintains global positioning errors below 10 meters, and enables rapid 4 degree\nof freedom (4-DoF) pose estimation within seconds using accessible\nlow-resolution satellite maps. Code and dataset will be available at\nhttps://github.com/YuanJiayuuu/SWA-PF.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MAFS\u6570\u636e\u96c6\u548cSWA-PF\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u5bfc\u822a\u65e0\u4eba\u673a\u5728GNSS\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5b9a\u4f4d\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u53ef\u7528\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u73af\u5883\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6216\u65f6\u53d8\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSWA-PF\u7684\u65b0\u578b\u8bed\u4e49\u52a0\u6743\u81ea\u9002\u5e94\u7c92\u5b50\u6ee4\u6ce2\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u4eba\u673a\u56fe\u50cf\u548c\u536b\u661f\u56fe\u50cf\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u8bed\u4e49\u52a0\u6743\u673a\u5236\u548c\u4f18\u5316\u7684\u7c92\u5b50\u6ee4\u6ce2\u67b6\u6784\u6765\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u9ad8\u5ea6\u98de\u884c\u7247\u6bb5\uff08MAFS\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u53ef\u53d8\u9ad8\u5ea6\u573a\u666f\u3002", "result": "\u6240\u63d0\u51fa\u7684SWA-PF\u65b9\u6cd5\u5728MAFS\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5176\u8ba1\u7b97\u6548\u7387\u6bd4\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u63d0\u9ad8\u4e8610\u500d\uff0c\u5168\u5c40\u5b9a\u4f4d\u8bef\u5dee\u4fdd\u6301\u572810\u7c73\u4ee5\u5185\uff0c\u5e76\u80fd\u5728\u51e0\u79d2\u949f\u5185\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u536b\u661f\u5730\u56fe\u5b9e\u73b0\u5feb\u901f\u76844\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "SWA-PF\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728GNSS\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u65e0\u4eba\u673a\u5b9a\u4f4d\u3002"}}
{"id": "2509.14216", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14216", "abs": "https://arxiv.org/abs/2509.14216", "authors": ["Johnny R. Zhang", "Xiaomei Mi", "Gaoyuan Du", "Qianyi Sun", "Shiqi Wang", "Jiaxuan Li", "Wenhua Zhou"], "title": "A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training", "comment": "69 pages, 10 figures. Preprint", "summary": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Banach-Bregman\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u52a0\u901f\u6536\u655b\uff0c\u63d0\u9ad8\u4e86AI\u4efb\u52a1\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u4f18\u5316\u7406\u8bba\u4e3b\u8981\u5c40\u9650\u4e8e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5904\u7406\u975e\u6b27\u51e0\u91cc\u5f97\u8bbe\u7f6e\uff0c\u5982\u955c\u9762\u4e0b\u964d\u3001Bregman\u8fd1\u90bb\u65b9\u6cd5\u3001\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u548cKL\u6563\u5ea6\u6b63\u5219\u5316\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7b49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Banach-Bregman\u6846\u67b6\uff0c\u5229\u7528Bregman\u6295\u5f71\u548cBregman-Fejer\u5355\u8c03\u6027\uff0c\u7edf\u4e00\u4e86\u968f\u673a\u903c\u8fd1\u3001\u955c\u9762\u4e0b\u964d\u3001\u81ea\u7136\u68af\u5ea6\u3001\u81ea\u9002\u5e94\u65b9\u6cd5\u548c\u955c\u9762-Prox\u7b49\u591a\u79cd\u4f18\u5316\u7b97\u6cd5\u3002\u8be5\u6846\u67b6\u652f\u6301\u8d85\u677e\u5f1b\uff08\u03bb > 2\uff09\uff0c\u9002\u7528\u4e8e\u7075\u6d3b\u7684\u51e0\u4f55\u8bbe\u7f6e\uff0c\u5e76\u89e3\u91ca\u4e86\u5176\u52a0\u901f\u6548\u5e94\u3002", "result": "\u5728UCI\u57fa\u51c6\u6d4b\u8bd5\u3001Transformer\u8bad\u7ec3\u3001Actor-Critic\u5f3a\u5316\u5b66\u4e60\u548cWikiText-2\u6570\u636e\u96c6\uff08\u4f7f\u7528distilGPT-2\uff09\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8620%\uff0c\u65b9\u5dee\u964d\u4f4e\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u3002", "conclusion": "Banach-Bregman\u51e0\u4f55\u4e3a\u7edf\u4e00\u4f18\u5316\u7406\u8bba\u548c\u5b9e\u8df5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3aAI\u6838\u5fc3\u8303\u5f0f\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.13801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13801", "abs": "https://arxiv.org/abs/2509.13801", "authors": ["Wenlve Zhou", "Zhiheng Zhou", "Tiantao Xian", "Yikui Zhai", "Weibin Wu", "Biyun Ma"], "title": "Masked Feature Modeling Enhances Adaptive Segmentation", "comment": null, "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer models from a labeled source domain to an unlabeled target domain.\nWhile auxiliary self-supervised tasks-particularly contrastive learning-have\nimproved feature discriminability, masked modeling approaches remain\nunderexplored in this setting, largely due to architectural incompatibility and\nmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), a\nnovel auxiliary task that performs feature masking and reconstruction directly\nin the feature space. Unlike existing masked modeling methods that reconstruct\nlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFM\naligns its learning target with the main segmentation task, ensuring\ncompatibility with standard architectures like DeepLab and DAFormer without\nmodifying the inference pipeline. To facilitate effective reconstruction, we\nintroduce a lightweight auxiliary module, Rebuilder, which is trained jointly\nbut discarded during inference, adding zero computational overhead at test\ntime. Crucially, MFM leverages the segmentation decoder to classify the\nreconstructed features, tightly coupling the auxiliary objective with the\npixel-wise prediction task to avoid interference with the primary task.\nExtensive experiments across various architectures and UDA benchmarks\ndemonstrate that MFM consistently enhances segmentation performance, offering a\nsimple, efficient, and generalizable strategy for unsupervised domain-adaptive\nsemantic segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63a9\u7801\u7279\u5f81\u5efa\u6a21\uff08MFM\uff09\u7684\u65b0\u578b\u8f85\u52a9\u4efb\u52a1\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\uff0c\u5b83\u76f4\u63a5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u63a9\u7801\u548c\u91cd\u5efa\uff0c\u4ee5\u63d0\u9ad8\u7279\u5f81\u53ef\u8fa8\u522b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u5229\u7528\u8f85\u52a9\u81ea\u76d1\u7763\u4efb\u52a1\uff08\u5982\u5bf9\u6bd4\u5b66\u4e60\uff09\u63d0\u9ad8\u7279\u5f81\u53ef\u8fa8\u522b\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u8fdb\u5c55\uff0c\u4f46\u63a9\u7801\u5efa\u6a21\u65b9\u6cd5\u5728\u8be5\u9886\u57df\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u67b6\u6784\u4e0d\u517c\u5bb9\u548c\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u7279\u5f81\u5efa\u6a21\uff08MFM\uff09\u4efb\u52a1\uff0c\u5b83\u76f4\u63a5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u63a9\u7801\u548c\u91cd\u5efa\u3002MFM \u4f7f\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8f85\u52a9\u6a21\u5757\u201cRebuilder\u201d\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6a21\u5757\u5728\u63a8\u7406\u65f6\u88ab\u4e22\u5f03\u3002MFM \u5229\u7528\u5206\u5272\u89e3\u7801\u5668\u5bf9\u91cd\u5efa\u7684\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u8f85\u52a9\u76ee\u6807\u4e0e\u50cf\u7d20\u7ea7\u9884\u6d4b\u4efb\u52a1\u7d27\u5bc6\u8026\u5408\u3002", "result": "MFM \u5728\u5404\u79cd\u67b6\u6784\u548c UDA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684 UDA \u8bed\u4e49\u5206\u5272\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "MFM \u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u6709\u6548\u7684\u3001\u53ef\u6cdb\u5316\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a9\u7801\u548c\u91cd\u5efa\uff0c\u4ece\u800c\u63d0\u9ad8\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e0d\u4f1a\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.14199", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14199", "abs": "https://arxiv.org/abs/2509.14199", "authors": ["Haichao Zhang", "Wenhao Chai", "Shwai He", "Ang Li", "Yun Fu"], "title": "Dense Video Understanding with Gated Residual Tokenization", "comment": null, "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.", "AI": {"tldr": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u5e27\u7387\u91c7\u6837\uff0c\u5ffd\u7565\u4e86\u5bc6\u96c6\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\u7684\u4efb\u52a1\uff08\u5982\u8bb2\u5ea7\u7406\u89e3\uff09\u6765\u8bf4\u662f\u4e0d\u591f\u7684\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5bc6\u96c6\u89c6\u9891\u7406\u89e3\uff08DVU\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u95e8\u63a7\u6b8b\u5dee\u5206\u8bcd\uff08GRT\uff09\u6280\u672f\uff0c\u5305\u62ec\u8fd0\u52a8\u8865\u507f\u548c\u8bed\u4e49\u573a\u666f\u5185\u5206\u8bcd\u5408\u5e76\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9ad8\u5e27\u7387\u89c6\u9891\u7406\u89e3\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u8bc4\u4f30\u5bc6\u96c6\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86DIVE\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\u5728\u5904\u7406\u9700\u8981\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u91c7\u7528\u4f4e\u5e27\u7387\u91c7\u6837\uff0c\u4e22\u5931\u4e86\u5bc6\u96c6\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548ctoken\u6570\u91cf\u7ebf\u6027\u589e\u957f\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u95e8\u63a7\u6b8b\u5dee\u5206\u8bcd\uff08GRT\uff09\u6280\u672f\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u6846\u67b6\uff1a1. \u8fd0\u52a8\u8865\u507f\u7684\u95e8\u63a7\u5206\u8bcd\uff1a\u5229\u7528\u50cf\u7d20\u7ea7\u8fd0\u52a8\u4f30\u8ba1\u6765\u8df3\u8fc7\u9759\u6001\u533a\u57df\u7684\u5206\u8bcd\uff0c\u5b9e\u73b0token\u6570\u91cf\u7684\u4e9a\u7ebf\u6027\u589e\u957f\u548c\u8ba1\u7b97\u91cf\u7684\u51cf\u5c11\u30022. \u8bed\u4e49\u573a\u666f\u5185\u5206\u8bcd\u5408\u5e76\uff1a\u878d\u5408\u573a\u666f\u5185\u9759\u6001\u533a\u57df\u7684token\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u5197\u4f59\u5e76\u4fdd\u7559\u52a8\u6001\u8bed\u4e49\u3002", "result": "\u5728DIVE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cGRT\u7684\u6027\u80fd\u4f18\u4e8e\u66f4\u5927\u7684VLLM\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u968f\u7740\u5e27\u7387\u7684\u589e\u52a0\uff0c\u6027\u80fd\u5448\u6b63\u76f8\u5173\uff0c\u8bc1\u660e\u4e86\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u7684\u91cd\u8981\u6027\u4ee5\u53caGRT\u5728\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9ad8\u5e27\u7387\u89c6\u9891\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "GRT\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u9ad8\u5e27\u7387\u89c6\u9891\u7684\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLLMs\u5728\u5904\u7406\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5728DIVE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u8868\u660e\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u5bf9\u4e8e\u89c6\u9891\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u800cGRT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14219", "categories": ["cs.LG", "math.DS", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.14219", "abs": "https://arxiv.org/abs/2509.14219", "authors": ["Jiaqi Yao", "Lewis Mitchell", "John Maclean", "Hemanth Saratchandran"], "title": "Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems", "comment": null, "summary": "Data-driven modeling of nonlinear dynamical systems is often hampered by\nmeasurement noise. We propose a denoising framework, called Runge-Kutta and\nTotal Variation Based Implicit Neural Representation (RKTV-INR), that\nrepresents the state trajectory with an implicit neural representation (INR)\nfitted directly to noisy observations. Runge-Kutta integration and total\nvariation are imposed as constraints to ensure that the reconstructed state is\na trajectory of a dynamical system that remains close to the original data. The\ntrained INR yields a clean, continuous trajectory and provides accurate\nfirst-order derivatives via automatic differentiation. These denoised states\nand derivatives are then supplied to Sparse Identification of Nonlinear\nDynamics (SINDy) to recover the governing equations. Experiments demonstrate\neffective noise suppression, precise derivative estimation, and reliable system\nidentification.", "AI": {"tldr": "RKTV-INR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Runge-Kutta\u79ef\u5206\u548c\u5168\u53d8\u5206\u7ea6\u675f\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u89c2\u6d4b\u4e2d\u5b66\u4e60\u9690\u5f0f\u795e\u7ecf\u8868\u5f81\uff0c\u4ee5\u53bb\u566a\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u8f68\u8ff9\uff0c\u5e76\u4e3aSINDy\u63d0\u4f9b\u51c6\u786e\u7684\u72b6\u6001\u548c\u5bfc\u6570\u4fe1\u606f\u4ee5\u8bc6\u522b\u7cfb\u7edf\u65b9\u7a0b\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u5e38\u53d7\u6d4b\u91cf\u566a\u58f0\u7684\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRKTV-INR\u7684\u53bb\u566a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06Runge-Kutta\u79ef\u5206\u548c\u5168\u53d8\u5206\u4f5c\u4e3a\u7ea6\u675f\uff0c\u76f4\u63a5\u5c06\u72b6\u6001\u8f68\u8ff9\u8868\u793a\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\uff0c\u5e76\u76f4\u63a5\u62df\u5408\u5230\u566a\u58f0\u89c2\u6d4b\u4e2d\u3002\u7136\u540e\uff0c\u5c06\u53bb\u566a\u540e\u7684\u72b6\u6001\u548c\u5bfc\u6570\u63d0\u4f9b\u7ed9SINDy\u4ee5\u6062\u590d\u63a7\u5236\u65b9\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u6291\u5236\u566a\u58f0\uff0c\u7cbe\u786e\u4f30\u8ba1\u5bfc\u6570\uff0c\u5e76\u53ef\u9760\u5730\u8bc6\u522b\u7cfb\u7edf\u3002", "conclusion": "RKTV-INR\u6846\u67b6\u80fd\u591f\u4ece\u566a\u58f0\u89c2\u6d4b\u4e2d\u6062\u590d\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u63a7\u5236\u65b9\u7a0b\u3002"}}
{"id": "2509.13809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13809", "abs": "https://arxiv.org/abs/2509.13809", "authors": ["Nick Theisen", "Kenny Schlegel", "Dietrich Paulus", "Peer Neubert"], "title": "Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET", "comment": "Accepted for publication at IEEE CASE 2025", "summary": "The classification of pixel spectra of hyperspectral images, i.e. spectral\nclassification, is used in many fields ranging from agricultural, over medical\nto remote sensing applications and is currently also expanding to areas such as\nautonomous driving. Even though for full hyperspectral images the\nbest-performing methods exploit spatial-spectral information, performing\nclassification solely on spectral information has its own advantages, e.g.\nsmaller model size and thus less data required for training. Moreover, spectral\ninformation is complementary to spatial information and improvements on either\npart can be used to improve spatial-spectral approaches in the future.\nRecently, 1D-Justo-LiuNet was proposed as a particularly efficient model with\nvery few parameters, which currently defines the state of the art in spectral\nclassification. However, we show that with limited training data the model\nperformance deteriorates. Therefore, we investigate MiniROCKET and\nHDC-MiniROCKET for spectral classification to mitigate that problem. The model\nextracts well-engineered features without trainable parameters in the feature\nextraction part and is therefore less vulnerable to limited training data. We\nshow that even though MiniROCKET has more parameters it outperforms\n1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the\ngeneral case", "AI": {"tldr": "1D-Justo-LiuNet\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u800cMiniROCKET\u548cHDC-MiniROCKET\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u5c3d\u7ba1\u5168\u5149\u8c31\u56fe\u50cf\u7684\u6700\u4f73\u65b9\u6cd5\u5229\u7528\u4e86\u7a7a\u95f4-\u5149\u8c31\u4fe1\u606f\uff0c\u4f46\u4ec5\u57fa\u4e8e\u5149\u8c31\u4fe1\u606f\u7684\u5206\u7c7b\u5177\u6709\u6a21\u578b\u5c3a\u5bf8\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u5c11\u7b49\u4f18\u70b9\u3002\u6b64\u5916\uff0c\u5149\u8c31\u4fe1\u606f\u662f\u5bf9\u7a7a\u95f4\u4fe1\u606f\u7684\u8865\u5145\uff0c\u672a\u6765\u53ef\u4ee5\u6539\u8fdb\u7a7a\u95f4-\u5149\u8c31\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528MiniROCKET\u548cHDC-MiniROCKET\u8fdb\u884c\u5149\u8c31\u5206\u7c7b\uff0c\u4ee5\u89e3\u51b31D-Justo-LiuNet\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002MiniROCKET\u548cHDC-MiniROCKET\u5728\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4e0d\u542b\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u63d0\u53d6\u7684\u7279\u5f81\u5de5\u7a0b\u826f\u597d\uff0c\u56e0\u6b64\u4e0d\u592a\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u5f71\u54cd\u3002", "result": "MiniROCKET\u5177\u6709\u6bd41D-Justo-LiuNet\u66f4\u591a\u7684\u53c2\u6570\uff0c\u4f46\u5728\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u4f18\u4e8e1D-Justo-LiuNet\uff0c\u5e76\u4e14\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0e\u5176\u76f8\u5f53\u3002", "conclusion": "MiniROCKET\u548cHDC-MiniROCKET\u662f\u89e3\u51b31D-Justo-LiuNet\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.14223", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14223", "abs": "https://arxiv.org/abs/2509.14223", "authors": ["Dmitrii Krasheninnikov", "Richard E. Turner", "David Krueger"], "title": "Language models' activations linearly encode training-order recency", "comment": null, "summary": "We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples for the six training datasets encode the training\norder: when projected into a 2D subspace, these centroids are arranged exactly\nin the order of training and lie on a straight line. Further, we show that\nlinear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities,\ngeneralizing to entities unseen during the probes' own training. The model can\nalso be fine-tuned to explicitly report an unseen entity's training stage (~80%\naccuracy). Interestingly, this temporal signal does not seem attributable to\nsimple differences in activation magnitudes, losses, or model confidence. Our\npaper demonstrates that models are capable of differentiating information by\nits acquisition time, and carries significant implications for how they might\nmanage conflicting data and respond to knowledge modifications.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u4fe1\u606f\u5b66\u4e60\u65f6\u95f4\u533a\u5206\u4fe1\u606f\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u533a\u5206\u4fe1\u606f\u5b66\u4e60\u7684\u65f6\u95f4\uff0c\u4ee5\u53ca\u8fd9\u79cd\u533a\u5206\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5728\u516d\u4e2a\u72ec\u7acb\u7684\u4f46\u76f8\u4f3c\u7684\u6570\u636e\u96c6\u4e0a\u987a\u5e8f\u5fae\u8c03Llama-3.2-1B\u6a21\u578b\uff0c\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u5df2\u77e5\u8bad\u7ec3\u987a\u5e8f\u7684\u6a21\u578b\u3002\u7136\u540e\u5206\u6790\u6d4b\u8bd5\u6837\u672c\u7684\u5e73\u5747\u6fc0\u6d3b\u503c\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u6765\u533a\u5206\u201c\u65e9\u671f\u201d\u548c\u201c\u665a\u671f\u201d\u5b66\u4e60\u7684\u5b9e\u4f53\u3002", "result": "\u6a21\u578b\u57282D\u5b50\u7a7a\u95f4\u4e2d\u7684\u8d28\u5fc3\u6392\u5217\u987a\u5e8f\u4e0e\u8bad\u7ec3\u987a\u5e8f\u4e00\u81f4\uff0c\u4f4d\u4e8e\u4e00\u6761\u76f4\u7ebf\u4e0a\u3002\u7ebf\u6027\u63a2\u9488\u80fd\u591f\u51c6\u786e\u533a\u5206\u65e9\u671f\u548c\u665a\u671f\u5b66\u4e60\u7684\u5b9e\u4f53\uff08\u7ea690%\u7684\u51c6\u786e\u7387\uff09\uff0c\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u3002\u6a21\u578b\u8fd8\u80fd\u901a\u8fc7\u5fae\u8c03\u6765\u62a5\u544a\u5b9e\u4f53\u5b66\u4e60\u9636\u6bb5\uff08\u7ea680%\u7684\u51c6\u786e\u7387\uff09\u3002\u8fd9\u79cd\u65f6\u95f4\u4fe1\u53f7\u5e76\u975e\u6e90\u4e8e\u6fc0\u6d3b\u5e45\u5ea6\u3001\u635f\u5931\u6216\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u7b80\u5355\u5dee\u5f02\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u533a\u5206\u4fe1\u606f\u83b7\u53d6\u7684\u65f6\u95f4\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5904\u7406\u51b2\u7a81\u6570\u636e\u548c\u77e5\u8bc6\u66f4\u65b0\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14063", "abs": "https://arxiv.org/abs/2509.14063", "authors": ["Sundhar Vinodh Sangeetha", "Chih-Yuan Chiu", "Sarah H. Q. Li", "Shreyas Kousik"], "title": "Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace", "comment": "The last two authors advised equally. Submitted to the 2026 IEEE\n  International Conference on Robotics and Automation. 8 pages, 6 figures", "summary": "Autonomous aircraft must safely operate in untowered airspace, where\ncoordination relies on voice-based communication among human pilots. Safe\noperation requires an aircraft to predict the intent, and corresponding goal\nlocation, of other aircraft. This paper introduces a multimodal framework for\naircraft goal prediction that integrates natural language understanding with\nspatial reasoning to improve autonomous decision-making in such environments.\nWe leverage automatic speech recognition and large language models to\ntranscribe and interpret pilot radio calls, identify aircraft, and extract\ndiscrete intent labels. These intent labels are fused with observed\ntrajectories to condition a temporal convolutional network and Gaussian mixture\nmodel for probabilistic goal prediction. Our method significantly reduces goal\nprediction error compared to baselines that rely solely on motion history,\ndemonstrating that language-conditioned prediction increases prediction\naccuracy. Experiments on a real-world dataset from an untowered airport\nvalidate the approach and highlight its potential to enable socially aware,\nlanguage-conditioned robotic motion planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u81ea\u4e3b\u98de\u673a\u5728\u65e0\u5854\u53f0\u7a7a\u57df\u4e2d\u7684\u610f\u56fe\u548c\u76ee\u6807\u4f4d\u7f6e\uff0c\u4ee5\u63d0\u9ad8\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5728\u65e0\u5854\u53f0\u7a7a\u57df\uff0c\u81ea\u4e3b\u98de\u673a\u9700\u8981\u9884\u6d4b\u5176\u4ed6\u98de\u673a\u7684\u610f\u56fe\u548c\u76ee\u6807\u4f4d\u7f6e\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u5229\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5f55\u548c\u89e3\u91ca\u98de\u884c\u5458\u7684\u65e0\u7ebf\u7535\u547c\u53eb\uff0c\u8bc6\u522b\u98de\u673a\u5e76\u63d0\u53d6\u79bb\u6563\u7684\u610f\u56fe\u6807\u7b7e\u3002\u5c06\u8fd9\u4e9b\u610f\u56fe\u6807\u7b7e\u4e0e\u89c2\u6d4b\u5230\u7684\u8f68\u8ff9\u878d\u5408\uff0c\u4ee5\u6761\u4ef6\u5316\u4e00\u4e2a\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u8fdb\u884c\u6982\u7387\u76ee\u6807\u9884\u6d4b\u3002", "result": "\u4e0e\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5386\u53f2\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u76ee\u6807\u9884\u6d4b\u8bef\u5dee\uff0c\u8868\u660e\u8bed\u8a00\u6761\u4ef6\u9884\u6d4b\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65e0\u5854\u53f0\u673a\u573a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u5b9e\u73b0\u5177\u6709\u793e\u4f1a\u610f\u8bc6\u3001\u8bed\u8a00\u6761\u4ef6\u7ea6\u675f\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13834", "abs": "https://arxiv.org/abs/2509.13834", "authors": ["Nguyen Lan Vi Vu", "Thanh-Huy Nguyen", "Thien Nguyen", "Daisuke Kihara", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation", "comment": "Accepted to BMVC 2025", "summary": "Semi-supervised learning has been employed to alleviate the need for\nextensive labeled data for histopathology image segmentation, but existing\nmethods struggle with noisy pseudo-labels due to ambiguous gland boundaries and\nmorphological misclassification. This paper introduces Semi-MOE, to the best of\nour knowledge, the first multi-task Mixture-of-Experts framework for\nsemi-supervised histopathology image segmentation. Our approach leverages three\nspecialized expert networks: A main segmentation expert, a signed distance\nfield regression expert, and a boundary prediction expert, each dedicated to\ncapturing distinct morphological features. Subsequently, the Multi-Gating\nPseudo-labeling module dynamically aggregates expert features, enabling a\nrobust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate\nmanual tuning while dynamically balancing multiple learning objectives, we\npropose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and\nCRAG benchmarks show that our method outperforms state-of-the-art approaches in\nlow-label settings, highlighting the potential of MoE-based architectures in\nadvancing semi-supervised segmentation. Our code is available at\nhttps://github.com/vnlvi2k3/Semi-MoE.", "AI": {"tldr": "Semi-MOE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u534a\u76d1\u7763\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u5272\u3001\u7b26\u53f7\u8ddd\u79bb\u573a\u56de\u5f52\u548c\u8fb9\u754c\u9884\u6d4b\u4e13\u5bb6\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5728\u4f4e\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u65f6\uff0c\u7531\u4e8e\u817a\u4f53\u8fb9\u754c\u6a21\u7cca\u548c\u5f62\u6001\u5b66\u5206\u7c7b\u9519\u8bef\uff0c\u5bb9\u6613\u53d7\u5230\u5e26\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSemi-MOE\u7684\u591a\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u4e1a\u4e13\u5bb6\u7f51\u7edc\uff1a\u4e3b\u5206\u5272\u4e13\u5bb6\u3001\u7b26\u53f7\u8ddd\u79bb\u573a\u56de\u5f52\u4e13\u5bb6\u548c\u8fb9\u754c\u9884\u6d4b\u4e13\u5bb6\u3002\u901a\u8fc7\u591a\u95e8\u63a7\u4f2a\u6807\u7b7e\u6a21\u5757\u52a8\u6001\u805a\u5408\u4e13\u5bb6\u7279\u5f81\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u878d\u5408-\u7cbe\u70bc\u4f2a\u6807\u7b7e\u673a\u5236\u3002\u5f15\u5165\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\uff0c\u4ee5\u6d88\u9664\u624b\u52a8\u8c03\u6574\u5e76\u52a8\u6001\u5e73\u8861\u591a\u4e2a\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728GlaS\u548cCRAG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eSemi-MOE\u5728\u4f4e\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Semi-MOE\u5728\u89e3\u51b3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5e76\u8bc1\u660e\u4e86MoE\u67b6\u6784\u5728\u63a8\u8fdb\u534a\u76d1\u7763\u5206\u5272\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14225", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.14225", "abs": "https://arxiv.org/abs/2509.14225", "authors": ["Benjamin Sterling", "Yousef El-Laham", "M\u00f3nica F. Bugallo"], "title": "Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics", "comment": "5 pages, 2 figures, 1 table", "summary": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9ad8\u9636 Langevin \u52a8\u529b\u5b66\u9632\u5fa1\u6269\u6563\u6a21\u578b\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u548c\u8054\u5408\u6269\u6563\u8fc7\u7a0b\u6765\u589e\u5f3a\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u51f8\u663e\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u9632\u5fa1\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u76f8\u6bd4\u5176\u4ed6\u751f\u6210\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u7684\u6297\u653b\u51fb\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u963b\u5c3c\u9ad8\u9636 Langevin \u52a8\u529b\u5b66\uff08critically-damped higher-order Langevin dynamics\uff09\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u591a\u4e2a\u8f85\u52a9\u53d8\u91cf\u548c\u4e00\u4e2a\u8054\u5408\u6269\u6563\u8fc7\u7a0b\uff0c\u65e8\u5728\u901a\u8fc7\u8f85\u52a9\u53d8\u91cf\u5f15\u5165\u5916\u90e8\u968f\u673a\u6027\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u7834\u574f\u654f\u611f\u8f93\u5165\u6570\u636e\uff0c\u4ece\u800c\u62b5\u5fa1\u6210\u5458\u63a8\u65ad\u653b\u51fb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5e76\u5728\u4e00\u4e2a\u73a9\u5177\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u63a5\u6536\u8005\u64cd\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUROC\uff09\u548c Fr\u00e9chet Inception Distance\uff08FID\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5173\u952e\u963b\u5c3c\u9ad8\u9636 Langevin \u52a8\u529b\u5b66\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9632\u5fa1\u6269\u6563\u6a21\u578b\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.14082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14082", "abs": "https://arxiv.org/abs/2509.14082", "authors": ["Valerii Serpiva", "Artem Lykov", "Faryal Batool", "Vladislav Kozlovskiy", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video", "comment": "Submitted to conference", "summary": "We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.", "AI": {"tldr": "FlightDiffusion\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff08FPV\uff09\u89c6\u9891\u4e2d\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u4ece\u5355\u5e27\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u914d\u6709\u76f8\u5e94\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ece\u800c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u63a8\u7406\u9a71\u52a8\u7684\u5bfc\u822a\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u591a\u6837\u5316\u7684FPV\u8f68\u8ff9\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\uff0c\u5e76\u53ef\u4ee5\u5927\u89c4\u6a21\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\uff0c\u5e76\u4e3a\u65e0\u4eba\u673a\u63d0\u4f9b\u5927\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u5b9e\u73b0\u63a8\u7406\u9a71\u52a8\u7684\u5bfc\u822a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff08FlightDiffusion\uff09\uff0c\u53ef\u4ee5\u4eceFPV\u89c6\u9891\u4e2d\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u3002\u8be5\u6a21\u578b\u80fd\u591f\u4ece\u5355\u5e27\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u751f\u6210\u76f8\u5e94\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u7528\u4e8e\u63a8\u7406\u9a71\u52a8\u7684\u5bfc\u822a\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5229\u7528\u5176\u751f\u6210\u80fd\u529b\u6765\u5408\u6210\u591a\u6837\u5316\u7684FPV\u8f68\u8ff9\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u4ece\u800c\u4e3a\u8bad\u7ec3\u751f\u6210\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u7684\u8f68\u8ff9\u5728\u7269\u7406\u4e0a\u662f\u5408\u7406\u7684\uff0c\u5e76\u4e14\u662f\u53ef\u6267\u884c\u7684\uff0c\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u4e3a0.25\u7c73\uff08RMSE\u4e3a0.28\u7c73\uff09\uff0c\u5e73\u5747\u65b9\u5411\u8bef\u5dee\u4e3a0.19\u5f27\u5ea6\uff08RMSE\u4e3a0.24\u5f27\u5ea6\uff09\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3001\u8f68\u8ff9\u89c4\u5212\u7684\u5e73\u6ed1\u6027\u4ee5\u53ca\u5bf9\u672a\u89c1\u60c5\u51b5\u7684\u9002\u5e94\u6027\u3002\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u6210\u529f\u7387\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u5dee\u5f02\uff08M=0.628, SD=0.162 vs M=0.617, SD=0.177\uff09\uff0c\u8868\u660e\u4e86\u826f\u597d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "FlightDiffusion\u63d0\u4f9b\u4e86\u4e00\u79cd\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u3001\u52a8\u4f5c\u751f\u6210\u548c\u6570\u636e\u5408\u6210\u7684\u8303\u5f0f\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4e0b\u6e38\u5bfc\u822a\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u826f\u597d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.14230", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14230", "abs": "https://arxiv.org/abs/2509.14230", "authors": ["Mengting Ai", "Tianxin Wei", "Sirui Chen", "Jingrui He"], "title": "NIRVANA: Structured pruning reimagined for large language models compression", "comment": null, "summary": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.", "AI": {"tldr": "NIRVANA\u662f\u4e00\u79cd\u65b0\u7684\u5927\u6a21\u578b\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\uff0c\u5e76\u4e14\u65e0\u9700\u6602\u8d35\u7684\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u6062\u590d\u6280\u672f\u3002", "method": "NIRVANA\u4f7f\u7528\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u5207\u7ebf\u6838\uff08Ntk\uff09\u7684\u4e00\u9636\u663e\u8457\u6027\u6807\u51c6\uff0c\u5e76\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u6839\u636e\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u6a21\u5757\u7684\u4e0d\u540c\u91cd\u8981\u6027\u6765\u8c03\u6574\u526a\u679d\u5f3a\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u57fa\u4e8eKL\u6563\u5ea6\u7684\u6821\u51c6\u6570\u636e\u9009\u62e9\u7b56\u7565\u6765\u63d0\u9ad8\u526a\u679d\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728Llama3\u3001Qwen\u548cT5\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNIRVANA\u5728\u540c\u7b49\u7a00\u758f\u5ea6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u7684\u96f6\u6837\u672c\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u540e\u7eed\u5fae\u8c03\u63d0\u4f9b\u66f4\u597d\u7684\u57fa\u7840\u3002", "conclusion": "NIRVANA\u662f\u4e00\u79cd\u6709\u7406\u8bba\u4f9d\u636e\u4e14\u5b9e\u7528\u7684LLM\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.13846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13846", "abs": "https://arxiv.org/abs/2509.13846", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation", "comment": "MICCAI 2025: 1st Place in Transformer track and 2nd Place in\n  Convolution track of SSL3D-OpenMind challenge", "summary": "Many recent approaches in representation learning implicitly assume that\nuncorrelated views of a data point are sufficient to learn meaningful\nrepresentations for various downstream tasks. In this work, we challenge this\nassumption and demonstrate that meaningful structure in the latent space does\nnot emerge naturally. Instead, it must be explicitly induced. We propose a\nmethod that aligns representations from different views of the data to align\ncomplementary information without inducing false positives. Our experiments\nshow that our proposed self-supervised learning method, Consistent View\nAlignment, improves performance for downstream tasks, highlighting the critical\nrole of structured view alignment in learning effective representations. Our\nmethod achieved first and second place in the MICCAI 2025 SSL3D challenge when\nusing a Primus vision transformer and ResEnc convolutional neural network,\nrespectively. The code and pretrained model weights are released at\nhttps://github.com/Tenbatsu24/LatentCampus.", "AI": {"tldr": "\u65e0", "motivation": "\u8bb8\u591a\u8fd1\u671f\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u65b9\u6cd5\u90fd\u5047\u8bbe\u6570\u636e\u70b9\u7684\u975e\u76f8\u5173\u89c6\u56fe\u8db3\u4ee5\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u4f46\u8be5\u7814\u7a76\u6311\u6218\u4e86\u8fd9\u4e00\u5047\u8bbe\uff0c\u8868\u660e\u6f5c\u7a7a\u95f4\u4e2d\u7684\u6709\u610f\u4e49\u7ed3\u6784\u5e76\u975e\u81ea\u7136\u51fa\u73b0\uff0c\u800c\u662f\u5fc5\u987b\u88ab\u663e\u5f0f\u5730\u8bf1\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u9f50\u6570\u636e\u4e0d\u540c\u89c6\u56fe\u7684\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u4ea7\u751f\u8bef\u62a5\u7684\u60c5\u51b5\u4e0b\u5bf9\u9f50\u4e92\u8865\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u88ab\u79f0\u4e3a\u4e00\u81f4\u89c6\u56fe\u5bf9\u9f50\uff08Consistent View Alignment\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u5728 MICCAI 2025 SSL3D \u6311\u6218\u8d5b\u4e2d\u5206\u522b\u4f7f\u7528 Primus vision transformer \u548c ResEnc \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u548c\u7b2c\u4e8c\u540d\u7684\u6210\u7ee9\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u89c6\u56fe\u5bf9\u9f50\u5728\u5b66\u4e60\u6709\u6548\u7684\u8868\u793a\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002"}}
{"id": "2509.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14117", "abs": "https://arxiv.org/abs/2509.14117", "authors": ["Ali Abouzeid", "Malak Mansour", "Zezhou Sun", "Dezhen Song"], "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model", "comment": "Under Review", "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.", "AI": {"tldr": "GeoAware-VLA\u901a\u8fc7\u96c6\u6210\u51e0\u4f55\u5148\u9a8c\u6765\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89d2\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6210\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u89c6\u89d2\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b83\u4eec\u96be\u4ee5\u4ece2D\u56fe\u50cf\u4e2d\u63a8\u65ad\u51fa\u7a33\u5065\u76843D\u51e0\u4f55\u4fe1\u606f\u3002", "method": "GeoAware-VLA\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u3001\u56fa\u5b9a\u7684\u51e0\u4f55\u89c6\u89c9\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u6765\u9002\u914d\u8fd9\u4e9b\u5bcc\u542b\u51e0\u4f55\u4fe1\u606f\u7684\u7279\u5f81\u5230\u7b56\u7565\u89e3\u7801\u5668\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u89c6\u89d2\u4e0d\u53d8\u6027\u3002", "result": "GeoAware-VLA\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u5b50\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u63d0\u5347\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u76f8\u673a\u59ff\u6001\u7684\u80fd\u529b\u63d0\u9ad8\u4e862\u500d\u4ee5\u4e0a\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u4e5f\u663e\u793a\u51fa\u6027\u80fd\u7684\u663e\u8457\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u672a\u89c1\u8fc7\u7684\u76f8\u673a\u89d2\u5ea6\u4e0b\u8fdb\u884c\u8bc4\u4f30\u65f6\u3002", "conclusion": "GeoAware-VLA\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5c06\u7a33\u5065\u7684\u51e0\u4f55\u4fe1\u606f\u878d\u5165\u6a21\u578b\u662f\u6784\u5efa\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u673a\u5668\u4eba\u4ee3\u7406\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u548c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u90fd\u6709\u6548\u3002"}}
{"id": "2509.14234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14234", "abs": "https://arxiv.org/abs/2509.14234", "authors": ["Dulhan Jayalath", "Shashwat Goel", "Thomas Foster", "Parag Jain", "Suchin Gururangan", "Cheng Zhang", "Anirudh Goyal", "Alan Schelten"], "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision", "comment": "22 pages, 8 figures, 2 tables", "summary": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u5373\u6559\u5e08\uff08CaT\uff09\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u63a8\u7406\u65f6\u81ea\u8eab\u7684\u63a2\u7d22\u8f6c\u5316\u4e3a\u65e0\u53c2\u8003\u76d1\u7763\u4fe1\u53f7\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u771f\u5b9e\u6807\u7b7e\u7684\u8bad\u7ec3\u540e\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u5728\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u7684\u8bad\u7ec3\u540e\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5bfb\u627e\u65b0\u7684\u5b66\u4e60\u4fe1\u53f7\u6765\u6e90\u3002", "method": "CaT\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u4e00\u4e2a\u53c2\u8003\u4fe1\u53f7\uff08\u7531\u521d\u59cb\u7b56\u7565\u534f\u8c03\u5f97\u51fa\uff09\u5e76\u8fdb\u884c\u4f18\u5316\uff0c\u5c06\u6a21\u578b\u63a8\u7406\u65f6\u7684\u63a2\u7d22\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002\u5bf9\u4e8e\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u4f7f\u7528\u6700\u7ec8\u7b54\u6848\u7684\u7a0b\u5e8f\u7b49\u4ef7\u6027\uff1b\u5bf9\u4e8e\u4e0d\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u4f7f\u7528\u72ec\u7acbLLM\u6cd5\u5b98\u8bc4\u5206\u7684\u81ea\u8bbe\u6807\u51c6\u3002", "result": "CaT\u65b9\u6cd5\u5728MATH-500\u548cHealthBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86Gemma 3 4B\u3001Qwen 3 4B\u548cLlama 3.1 8B\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08CaT-RL\uff09\u540e\uff0c\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "CaT\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u540e\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5c06\u6a21\u578b\u7684\u63a2\u7d22\u8f6c\u5316\u4e3a\u6709\u7528\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2509.13848", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13848", "abs": "https://arxiv.org/abs/2509.13848", "authors": ["Jiayi Pan", "Jiaming Xu", "Yongkang Zhou", "Guohao Dai"], "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "comment": null, "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SpecDiff \u7684\u65b0\u9896\u7279\u5f81\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u81ea\u6211\u63a8\u6d4b\u201d\u7684\u672a\u6765\u4fe1\u606f\u6765\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5728\u4e0d\u635f\u5931\u663e\u8457\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe 3.17 \u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5386\u53f2\u4fe1\u606f\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u548c\u901f\u5ea6\u53d7\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u6269\u6563\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u81ea\u6211\u63a8\u6d4b\u201d\u7684\u672a\u6765\u4fe1\u606f\u5f15\u5165\u8303\u5f0f\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4e86 SpecDiff\u3002SpecDiff \u5305\u542b\u4e00\u4e2a\u7f13\u5b58\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u548c\u4e00\u4e2a\u591a\u5c42\u7279\u5f81\u5206\u7c7b\u7b97\u6cd5\u3002\u7f13\u5b58\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u81ea\u6211\u63a8\u6d4b\u4fe1\u606f\u548c\u5386\u53f2\u4fe1\u606f\u4e3a\u6bcf\u4e2a token \u52a8\u6001\u8ba1\u7b97\u91cd\u8981\u6027\u5f97\u5206\uff0c\u5e76\u636e\u6b64\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002\u591a\u5c42\u7279\u5f81\u5206\u7c7b\u7b97\u6cd5\u5219\u5229\u7528\u7279\u5f81\u91cd\u8981\u6027\u5f97\u5206\u7684\u5dee\u5f02\u5bf9 token \u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u591a\u5c42\u7279\u5f81\u8ba1\u7b97\u7b56\u7565\u3002", "result": "\u5728 Stable Diffusion 3, 3.5, \u548c FLUX \u6a21\u578b\u4e0a\uff0cSpecDiff \u76f8\u6bd4 RFlow \u5728 NVIDIA A800-80GB GPU \u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747 2.80 \u500d\u30012.74 \u500d\u548c 3.17 \u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "SpecDiff \u901a\u8fc7\u878d\u5408\u63a8\u6d4b\u4fe1\u606f\u548c\u5386\u53f2\u4fe1\u606f\uff0c\u514b\u670d\u4e86\u52a0\u901f-\u7cbe\u5ea6\u6743\u8861\u7684\u74f6\u9888\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u6269\u6563\u6a21\u578b\u63a8\u7406\u7684\u6027\u80fd\u8fb9\u754c\u3002"}}
{"id": "2509.13858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13858", "abs": "https://arxiv.org/abs/2509.13858", "authors": ["Qianxin Xia", "Jiawei Du", "Guoming Lu", "Zhiyong Shu", "Jielei Wang"], "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics", "comment": null, "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.", "AI": {"tldr": "EDITS\u6846\u67b6\u5229\u7528\u56fe\u50cf\u5185\u9690\u5f0f\u6587\u672c\u8bed\u4e49\u8fdb\u884c\u6570\u636e\u96c6\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u9ad8\u5c42\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u4e2d\u9ad8\u5c42\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u53d7\u9650\u3002", "method": "EDITS\u6846\u67b6\u9996\u5148\u901a\u8fc7\u5168\u5c40\u8bed\u4e49\u67e5\u8be2\u6a21\u5757\u878d\u5408VLM\u751f\u6210\u7684\u5916\u90e8\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\uff0c\u5f62\u6210\u5148\u9a8c\u805a\u7c7b\u7f13\u51b2\u533a\u3002\u7136\u540e\uff0c\u5c40\u90e8\u8bed\u4e49\u611f\u77e5\u6a21\u5757\u4ece\u7f13\u51b2\u533a\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\uff0c\u6784\u5efa\u56fe\u50cf\u548c\u6587\u672c\u539f\u578b\uff08\u540e\u8005\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5f15\u5bfcLLM\u751f\u6210\uff09\u3002\u6700\u540e\uff0c\u53cc\u539f\u578b\u5f15\u5bfc\u7b56\u7565\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6700\u7ec8\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u7279\u5f81\u4e0eVLM\u751f\u6210\u7684\u6587\u672c\uff0cEDITS\u80fd\u751f\u6210\u5305\u542b\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "EDITS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u6587\u672c\u8bed\u4e49\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u5408\u6210\u3002"}}
{"id": "2509.13863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13863", "abs": "https://arxiv.org/abs/2509.13863", "authors": ["Chu Chen", "Ander Biguri", "Jean-Michel Morel", "Raymond H. Chan", "Carola-Bibiane Sch\u00f6nlieb", "Jizhou Li"], "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction", "comment": null, "summary": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.", "AI": {"tldr": "X\u5c04\u7ebf\u8ba1\u7b97\u673a\u5c42\u6790\u6210\u50cf(CL)\u5728\u5fae\u82af\u7247\u548c\u590d\u5408\u7535\u6c60\u6750\u6599\u7b49\u677f\u72b6\u7ed3\u6784\u65e0\u635f\u68c0\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfCT\u56e0\u51e0\u4f55\u9650\u5236\u800c\u96be\u4ee5\u5904\u7406\u3002\u672c\u6587\u63d0\u51fa\u7684LamiGauss\u7b97\u6cd5\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u5149\u6805\u5316\u548c\u5305\u542b\u5c42\u6790\u503e\u659c\u89d2\u7684\u4e13\u7528\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u53d8\u6362\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u6295\u5f71\u4e2d\u51c6\u786e\u9ad8\u6548\u5730\u91cd\u5efa\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLamiGauss\u4ec5\u4f7f\u75283%\u7684\u5b8c\u6574\u89c6\u56fe\u5373\u53ef\u5b9e\u73b0\u4f18\u4e8e\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002", "motivation": "X\u5c04\u7ebf\u8ba1\u7b97\u673a\u5c42\u6790\u6210\u50cf\uff08CL\uff09\u5728\u677f\u72b6\u7ed3\u6784\uff08\u5982\u5fae\u82af\u7247\u548c\u590d\u5408\u7535\u6c60\u6750\u6599\uff09\u7684\u65e0\u635f\u68c0\u6d4b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u4f20\u7edfCT\u5b58\u5728\u51e0\u4f55\u9650\u5236\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c6\u56fe\u91c7\u96c6\u6761\u4ef6\u4e0b\u91cd\u5efa\u9ad8\u8d28\u91cf\u4f53\u79ef\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLamiGauss\u7684\u91cd\u5efa\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u5149\u6805\u5316\u548c\u4e00\u4e2a\u4e13\u95e8\u7684\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u53d8\u6362\u6a21\u578b\uff08\u5305\u542b\u5c42\u6790\u503e\u659c\u89d2\uff09\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ece\u521d\u6b65\u91cd\u5efa\u4e2d\u6ee4\u9664\u5e38\u89c1\u7684\u5c42\u6790\u4f2a\u5f71\uff0c\u5e76\u5c06\u6a21\u578b\u5bb9\u91cf\u96c6\u4e2d\u5728\u8868\u793a\u771f\u5b9e\u7269\u4f53\u4e0a\uff0c\u76f4\u63a5\u4ece\u7a00\u758f\u6295\u5f71\u8fdb\u884c\u4f18\u5316\u3002", "result": "LamiGauss\u7b97\u6cd5\u4f7f\u75283%\u7684\u5b8c\u6574\u89c6\u56fe\u5373\u53ef\u5b9e\u73b0\u4f18\u4e8e\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "LamiGauss\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u3001\u9ad8\u6548\u5730\u4ece\u6709\u9650\u6570\u636e\u4e2d\u8fdb\u884c\u91cd\u5efa\uff0c\u5e76\u5728\u7a00\u758f\u6295\u5f71\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4f53\u79ef\u91cd\u5efa\u3002"}}
{"id": "2509.14138", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14138", "abs": "https://arxiv.org/abs/2509.14138", "authors": ["Ran Yang", "Zijian An", "Lifeng ZHou", "Yiming Feng"], "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model", "comment": "8 pages, 9 figures, 1 table", "summary": "Long-horizon robotic manipulation tasks require executing multiple\ninterdependent subtasks in strict sequence, where errors in detecting subtask\ncompletion can cascade into downstream failures. Existing\nVision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous\nlow-level control but lack an internal signal for identifying when a subtask\nhas finished, making them brittle in sequential settings. We propose SeqVLA, a\ncompletion-aware extension of $\\pi_0$ that augments the base architecture with\na lightweight detection head perceiving whether the current subtask is\ncomplete. This dual-head design enables SeqVLA not only to generate\nmanipulation actions but also to autonomously trigger transitions between\nsubtasks. We investigate four finetuning strategies that vary in how the action\nand detection heads are optimized (joint vs. sequential finetuning) and how\npretrained knowledge is preserved (full finetuning vs. frozen backbone).\nExperiments are performed on two multi-stage tasks: salad packing with seven\ndistinct subtasks and candy packing with four distinct subtasks. Results show\nthat SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong\nbaselines in overall success rate. In particular, joint finetuning with an\nunfrozen backbone yields the most decisive and statistically reliable\ncompletion predictions, eliminating sequence-related failures and enabling\nrobust long-horizon execution. Our results highlight the importance of coupling\naction generation with subtask-aware detection for scalable sequential\nmanipulation.", "AI": {"tldr": "SeqVLA\u662f\u4e00\u4e2a\u6539\u8fdb\u7684Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u52a0\u4e00\u4e2a\u68c0\u6d4b\u5934\u6765\u611f\u77e5\u5b50\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u957f\u65f6\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b50\u4efb\u52a1\u4f9d\u8d56\u548c\u9519\u8bef\u4f20\u64ad\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSeqVLA\u5728\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u8054\u5408\u5fae\u8c03\u548c\u4e0d\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7684\u7b56\u7565\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u957f\u65f6\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u6309\u987a\u5e8f\u6267\u884c\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u4efb\u52a1\uff0c\u800c\u73b0\u6709VLA\u6a21\u578b\u5728\u68c0\u6d4b\u5b50\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bb9\u6613\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3002SeqVLA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SeqVLA\u5728\u73b0\u6709VLA\u6a21\u578b\uff08\u5982\u03c0\u2080\uff09\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u68c0\u6d4b\u5934\uff0c\u4f7f\u5176\u80fd\u591f\u611f\u77e5\u5f53\u524d\u5b50\u4efb\u52a1\u662f\u5426\u5b8c\u6210\uff0c\u4ece\u800c\u81ea\u4e3b\u89e6\u53d1\u5b50\u4efb\u52a1\u4e4b\u95f4\u7684\u8f6c\u6362\u3002\u7814\u7a76\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u5fae\u8c03\u7b56\u7565\uff08\u8054\u5408\u5fae\u8c03 vs. \u987a\u5e8f\u5fae\u8c03\uff1b\u51bb\u7ed3\u9aa8\u5e72\u7f51 vs. \u4e0d\u51bb\u7ed3\u9aa8\u5e72\u7f51\uff09\u3002", "result": "\u5728\u6c99\u62c9\u6253\u5305\u548c\u7cd6\u679c\u6253\u5305\u4e24\u4e2a\u591a\u9636\u6bb5\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0cSeqVLA\u7684\u6574\u4f53\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf\u6a21\u578b\u03c0\u2080\u548c\u5176\u4ed6\u5f3a\u529b\u57fa\u7ebf\u3002\u8054\u5408\u5fae\u8c03\u4e14\u4e0d\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7684\u7b56\u7565\u5728\u5b8c\u6210\u9884\u6d4b\u65b9\u9762\u6700\u4e3a\u53ef\u9760\uff0c\u6d88\u9664\u4e86\u4e0e\u5e8f\u5217\u76f8\u5173\u7684\u5931\u8d25\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u957f\u65f6\u5e8f\u6267\u884c\u3002", "conclusion": "\u5c06\u52a8\u4f5c\u751f\u6210\u4e0e\u611f\u77e5\u5b50\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\u7684\u68c0\u6d4b\u76f8\u7ed3\u5408\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5e8f\u5217\u5316\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.13864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13864", "abs": "https://arxiv.org/abs/2509.13864", "authors": ["Jovana Videnovic", "Matej Kristan", "Alan Lukezic"], "title": "Distractor-Aware Memory-Based Visual Object Tracking", "comment": "Code available on Github: https://github.com/jovanavidenovic/DAM4SAM", "summary": "Recent emergence of memory-based video segmentation methods such as SAM2 has\nled to models with excellent performance in segmentation tasks, achieving\nleading results on numerous benchmarks. However, these modes are not fully\nadjusted for visual object tracking, where distractors (i.e., objects visually\nsimilar to the target) pose a key challenge. In this paper we propose a\ndistractor-aware drop-in memory module and introspection-based management\nmethod for SAM2, leading to DAM4SAM. Our design effectively reduces the\ntracking drift toward distractors and improves redetection capability after\nobject occlusion. To facilitate the analysis of tracking in the presence of\ndistractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM\noutperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results\non ten. Furthermore, integrating the proposed distractor-aware memory into a\nreal-time tracker EfficientTAM leads to 11% improvement and matches tracking\nquality of the non-real-time SAM2.1-L on multiple tracking and segmentation\nbenchmarks, while integration with edge-based tracker EdgeTAM delivers 4%\nperformance boost, demonstrating a very good generalization across\narchitectures.", "AI": {"tldr": "DAM4SAM\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5185\u5b58\u6a21\u5757\u548c\u7ba1\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u5e72\u6270\u7269\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5185\u5b58\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\uff08\u5982SAM2\uff09\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u5916\u89c2\u76f8\u4f3c\u7684\u5e72\u6270\u7269\u65f6\uff0c\u9762\u4e34\u6311\u6218\uff0c\u5bb9\u6613\u51fa\u73b0\u8ddf\u8e2a\u6f02\u79fb\u548c\u91cd\u6355\u83b7\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eSAM2\u7684\u3001\u6ce8\u610f\u5e72\u6270\u7269\u7684\u5185\u5b58\u6a21\u5757\u548c\u57fa\u4e8e\u5185\u7701\u7684\u7ba1\u7406\u65b9\u6cd5\uff0c\u547d\u540d\u4e3aDAM4SAM\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u51cf\u5c11\u8ddf\u8e2a\u6f02\u79fb\u5e76\u63d0\u9ad8\u7269\u4f53\u906e\u6321\u540e\u7684\u91cd\u6355\u83b7\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDiDi\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u6790\u5728\u6709\u5e72\u6270\u7269\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8ddf\u8e2a\u3002", "result": "DAM4SAM\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eSAM2.1\uff0c\u5e76\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u8bb0\u5f55\u3002\u5c06\u5176\u5185\u5b58\u6a21\u5757\u96c6\u6210\u5230\u5b9e\u65f6\u8ddf\u8e2a\u5668EfficientTAM\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u4e8611%\uff0c\u5e76\u8fbe\u5230\u4e86\u975e\u5b9e\u65f6SAM2.1-L\u7684\u8ddf\u8e2a\u8d28\u91cf\u3002\u4e0e\u57fa\u4e8e\u8fb9\u7f18\u7684\u8ddf\u8e2a\u5668EdgeTAM\u96c6\u6210\u540e\uff0c\u6027\u80fd\u63d0\u5347\u4e864%\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u8de8\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DAM4SAM\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5185\u5b58\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u5e72\u6270\u7269\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u8ddf\u8e2a\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.14143", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14143", "abs": "https://arxiv.org/abs/2509.14143", "authors": ["Zijian An", "Ran Yang", "Yiming Feng", "Lifeng Zhou"], "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping", "comment": "8 pages, 5 figures, 1 table", "summary": "Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.", "AI": {"tldr": "CLAW\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u53ef\u4ee5\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u7cbe\u786e\u7684\u91cd\u91cf\u63a7\u5236\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709VLA\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u6267\u884c\u9700\u8981\u7cbe\u786e\u6570\u503c\u7ea6\u675f\uff08\u5982\u57fa\u4e8e\u6570\u5b57\u9608\u503c\u505c\u6b62\uff09\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u89c2\u5bdf\u5230\u52a8\u4f5c\u6620\u5c04\u662f\u9690\u5f0f\u5f62\u6210\u7684\uff0c\u7f3a\u4e4f\u663e\u5f0f\u7684\u6761\u4ef6\u76d1\u63a7\u673a\u5236\u3002", "method": "CLAW\u6846\u67b6\u5c06\u6761\u4ef6\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\u5206\u79bb\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u5fae\u8c03\u540e\u7684CLIP\u6a21\u578b\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\uff0c\u6301\u7eed\u76d1\u63a7\u7535\u5b50\u79e4\u7684\u8bfb\u6570\uff0c\u5e76\u6839\u636e\u7279\u5b9a\u4efb\u52a1\u7684\u91cd\u91cf\u9608\u503c\u751f\u6210\u79bb\u6563\u6307\u4ee4\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684VLA\u7b56\u7565\uff08$\\\text{\\\\\\pi_0}$) \u63a5\u6536\u8fd9\u4e9b\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u4e0e\u591a\u89c6\u89d2\u76f8\u673a\u89c2\u6d4b\u7ed3\u5408\uff0c\u751f\u6210\u8fde\u7eed\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "CLAW\u5728\u4e09\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff1a\u5355\u7269\u4f53\u6293\u53d6\u548c\u9700\u8981\u53cc\u81c2\u64cd\u4f5c\u7684\u6df7\u5408\u7269\u4f53\u4efb\u52a1\u3002\u5728\u6240\u6709\u6761\u4ef6\u4e0b\uff0cCLAW\u90fd\u80fd\u53ef\u9760\u5730\u6267\u884c\u79f0\u91cd\u611f\u77e5\u884c\u4e3a\uff0c\u5e76\u4e14\u4f18\u4e8e\u539f\u59cb\\\text{\\\\\\pi_0}\u548c\u5fae\u8c03\u540e\u7684\\\text{\\\\\\pi_0}\u6a21\u578b\u3002", "conclusion": "CLAW\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408\u7b26\u53f7\u91cd\u91cf\u63a8\u7406\u548c\u9ad8\u9891\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5bf9\u91cd\u91cf\u7684\u7cbe\u786e\u611f\u77e5\u548c\u63a7\u5236\u3002"}}
{"id": "2509.13873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13873", "abs": "https://arxiv.org/abs/2509.13873", "authors": ["Siam Tahsin Bhuiyan", "Rashedur Rahman", "Sefatul Wasi", "Naomi Yagi", "Syoji Kobashi", "Ashraful Islam", "Saadia Binte Alam"], "title": "Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis", "comment": "Accepted at MICCAI EMERGE 2025", "summary": "Pelvic fractures pose significant diagnostic challenges, particularly in\ncases where fracture signs are subtle or invisible on standard radiographs. To\naddress this, we introduce PelFANet, a dual-stream attention network that fuses\nraw pelvic X-rays with segmented bone images to improve fracture\nclassification. The network em-ploys Fused Attention Blocks (FABlocks) to\niteratively exchange and refine fea-tures from both inputs, capturing global\ncontext and localized anatomical detail. Trained in a two-stage pipeline with a\nsegmentation-guided approach, PelFANet demonstrates superior performance over\nconventional methods. On the AMERI dataset, it achieves 88.68% accuracy and\n0.9334 AUC on visible fractures, while generalizing effectively to invisible\nfracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained\non them. These results highlight the clini-cal potential of anatomy-aware\ndual-input architectures for robust fracture detec-tion, especially in\nscenarios with subtle radiographic presentations.", "AI": {"tldr": "PelFANet\u662f\u4e00\u4e2a\u53cc\u6d41\u6ce8\u610f\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408X\u5149\u7247\u548c\u5206\u5272\u9aa8\u9abc\u56fe\u50cf\u6765\u63d0\u9ad8\u9aa8\u76c6\u9aa8\u6298\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5728AMERI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfX\u5149\u7247\u5728\u8bca\u65ad\u7ec6\u5fae\u9aa8\u6298\u65f6\u7684\u6311\u6218\uff0c\u63d0\u51faPelFANet\u6765\u63d0\u9ad8\u9aa8\u6298\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "PelFANet\u91c7\u7528\u53cc\u6d41\u6ce8\u610f\u7f51\u7edc\u7ed3\u6784\uff0c\u878d\u5408\u539f\u59cbX\u5149\u7247\u548c\u5206\u5272\u9aa8\u9abc\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528Fused Attention Blocks\uff08FABlocks\uff09\u6765\u8fed\u4ee3\u5730\u878d\u5408\u548c\u4f18\u5316\u6765\u81ea\u4e24\u4e2a\u8f93\u5165\u7684\u7279\u5f81\uff0c\u4ee5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u89e3\u5256\u7ec6\u8282\u3002\u8be5\u7f51\u7edc\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u4e86\u5206\u5272\u5f15\u5bfc\u7684\u65b9\u6cd5\u3002", "result": "PelFANet\u5728AMERI\u6570\u636e\u96c6\u4e0a\u5bf9\u53ef\u89c1\u9aa8\u6298\u7684\u51c6\u786e\u7387\u8fbe\u523088.68%\uff0cAUC\u4e3a0.9334\u3002\u5bf9\u4e8e\u4e0d\u53ef\u89c1\u9aa8\u6298\uff0c\u867d\u7136\u672a\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u4f46\u51c6\u786e\u7387\u4ecd\u8fbe\u523082.29%\uff0cAUC\u4e3a0.8688\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\u4e86\u5173\u6ce8\u9aa8\u9abc\u89e3\u5256\u7684\u53cc\u8f93\u5165\u7f51\u7edc\u67b6\u6784\u5728\u9aa8\u6298\u68c0\u6d4b\u4e2d\u7684\u4e34\u5e8a\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728X\u5149\u7247\u8868\u73b0\u4e0d\u660e\u663e\u7684\u75c5\u4f8b\u4e2d\u3002"}}
{"id": "2509.14147", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14147", "abs": "https://arxiv.org/abs/2509.14147", "authors": ["Fanxing Li", "Shengyang Wang", "Fangyu Sun", "Shuyu Wu", "Dexin Zuo", "Wenxian Yu", "Danping Zou"], "title": "StableTracker: Learning to Stably Track Target via Differentiable Simulation", "comment": null, "summary": "FPV object tracking methods heavily rely on handcraft modular designs,\nresulting in hardware overload and cumulative error, which seriously degrades\nthe tracking performance, especially for rapidly accelerating or decelerating\ntargets. To address these challenges, we present \\textbf{StableTracker}, a\nlearning-based control policy that enables quadrotors to robustly follow the\nmoving target from arbitrary perspectives. The policy is trained using\nbackpropagation-through-time via differentiable simulation, allowing the\nquadrotor to maintain the target at the center of the visual field in both\nhorizontal and vertical directions, while keeping a fixed relative distance,\nthereby functioning as an autonomous aerial camera. We compare StableTracker\nagainst both state-of-the-art traditional algorithms and learning baselines.\nSimulation experiments demonstrate that our policy achieves superior accuracy,\nstability and generalization across varying safe distances, trajectories, and\ntarget velocities. Furthermore, a real-world experiment on a quadrotor with an\nonboard computer validated practicality of the proposed approach.", "AI": {"tldr": "StableTracker\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u7b56\u7565\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u6a21\u62df\u548cBPTT\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u3001\u7cbe\u786e\u7684\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edfFPV\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u6a21\u5757\uff0c\u4f1a\u5bfc\u81f4\u786c\u4ef6\u8fc7\u8f7d\u548c\u7d2f\u79ef\u8bef\u5dee\uff0c\u4e25\u91cd\u5f71\u54cd\u8ddf\u8e2a\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5feb\u901f\u52a0\u901f\u6216\u51cf\u901f\u76ee\u6807\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStableTracker\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u65f6\u95f4\uff08BPTT\uff09\u548c\u53ef\u5fae\u5206\u6a21\u62df\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u9c81\u68d2\u5730\u8ddf\u8e2a\u79fb\u52a8\u76ee\u6807\uff0c\u4f7f\u5176\u4fdd\u6301\u5728\u89c6\u91ce\u4e2d\u5fc3\uff0c\u5e76\u4fdd\u6301\u56fa\u5b9a\u7684\u76f8\u5bf9\u8ddd\u79bb\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u4f20\u7edf\u7b97\u6cd5\u548c\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0cStableTracker\u5728\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u5b89\u5168\u8ddd\u79bb\u3001\u8f68\u8ff9\u548c\u76ee\u6807\u901f\u5ea6\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65e0\u4eba\u673a\u5b9e\u9a8c\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "StableTracker\u901a\u8fc7\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edfFPV\u76ee\u6807\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u3001\u7cbe\u786e\u548c\u5b9e\u7528\u7684\u76ee\u6807\u8ddf\u8e2a\u3002"}}
{"id": "2509.13883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13883", "abs": "https://arxiv.org/abs/2509.13883", "authors": ["Zhen Xu", "Guorui Lu", "Chang Gao", "Qinyu Chen"], "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View", "comment": "8 pages", "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.", "AI": {"tldr": "EvHand-FPV\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76843D\u624b\u90e8\u8ddf\u8e2a\uff0c\u5728\u63d0\u9ad82D\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u91cf\uff0c\u9002\u7528\u4e8eXR\u8bbe\u5907\u3002", "motivation": "\u57fa\u4e8e\u5e27\u7684\u624b\u90e8\u8ddf\u8e2a\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u65b9\u9762\u96be\u4ee5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u7684XR\u8bbe\u5907\u7684\u8981\u6c42\u3002\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u529f\u8017\u7684\u7279\u6027\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "EvHand-FPV\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76f8\u673a\uff0c\u7ed3\u5408\u4e86\u624b\u8155\u533a\u57df\uff08ROI\uff09\u5b9a\u4f4d\u3001\u7aef\u5230\u7aef\u6620\u5c04\u7b56\u7565\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u76843D\u624b\u90e8\u8ddf\u8e2a\u3002\u5b83\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684FPV\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "EvHand-FPV\u5728\u771f\u5b9eFPV\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5c062D-AUCp\u4ece0.77\u63d0\u9ad8\u52300.85\uff0c\u53c2\u6570\u91cf\u51cf\u5c1189%\uff08\u4ece11.2M\u964d\u81f31.2M\uff09\uff0c\u6bcf\u79d2\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\uff08FLOPs\uff09\u51cf\u5c1189%\uff08\u4ece1.648G\u964d\u81f30.185G\uff09\u3002\u5728\u5408\u6210\u6570\u636e\u4e0a\uff0c3D-AUCp\u4fdd\u6301\u57280.84\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "EvHand-FPV\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e8b\u4ef6\u76f8\u673a\u624b\u90e8\u8ddf\u8e2a\uff0c\u975e\u5e38\u9002\u5408\u5728XR\u8bbe\u5907\u4e0a\u8fdb\u884c\u7aef\u4fa7\u5e94\u7528\u3002"}}
{"id": "2509.14159", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14159", "abs": "https://arxiv.org/abs/2509.14159", "authors": ["Dayi Dong", "Maulik Bhatt", "Seoyeon Choi", "Negar Mehr"], "title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies", "comment": "9 pages, 4 figures, 5 tables", "summary": "As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.", "AI": {"tldr": "MIMIC-D\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u7684CTDE\u8303\u5f0f\uff0c\u5b83\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u96c6\u4e2d\u5316\u5e76\u6267\u884c\u65f6\u53bb\u4e2d\u5fc3\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u9690\u5f0f\u534f\u8c03\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u65e5\u76ca\u878d\u5165\u793e\u4f1a\uff0c\u5176\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6\u673a\u5668\u4eba\u534f\u8c03\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u4e13\u5bb6\u6f14\u793a\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u534f\u8c03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIMIC-D\u7684\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u3001\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\uff08CTDE\uff09\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u5229\u7528\u6269\u6563\u7b56\u7565\u8fdb\u884c\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u3002\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u8054\u5408\u5b66\u4e60\uff0c\u62e5\u6709\u5168\u90e8\u4fe1\u606f\uff0c\u4f46\u5728\u6267\u884c\u7b56\u7565\u65f6\u4ec5\u4f7f\u7528\u5c40\u90e8\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cMIMIC-D\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u6210\u529f\u6062\u590d\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u591a\u6a21\u6001\u534f\u8c03\u884c\u4e3a\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MIMIC-D\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u663e\u5f0f\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\u3002"}}
{"id": "2509.13907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13907", "abs": "https://arxiv.org/abs/2509.13907", "authors": ["Jiyun Im", "SuBeen Lee", "Miso Lee", "Jae-Pil Heo"], "title": "White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation", "comment": "9 pages, 5 figures", "summary": "Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point\nlabels for an unlabeled point cloud, given only a few labeled examples. To\nextract discriminative representations from the limited support set, existing\nmethods have constructed prototypes using conventional algorithms such as\nfarthest point sampling. However, we point out that its initial randomness\nsignificantly affects FS-PCS performance and that the prototype generation\nprocess remains underexplored despite its prevalence. This motivates us to\ninvestigate an advanced prototype generation method based on attention\nmechanism. Despite its potential, we found that vanilla module suffers from the\ndistributional gap between learnable prototypical tokens and support features.\nTo overcome this, we propose White Aggregation and Restoration Module (WARM),\nwhich resolves the misalignment by sandwiching cross-attention between\nwhitening and coloring transformations. Specifically, whitening aligns the\nsupport features to prototypical tokens before attention process, and\nsubsequently coloring restores the original distribution to the attended\ntokens. This simple yet effective design enables robust attention, thereby\ngenerating representative prototypes by capturing the semantic relationships\namong support features. Our method achieves state-of-the-art performance with a\nsignificant margin on multiple FS-PCS benchmarks, demonstrating its\neffectiveness through extensive experiments.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u767d\u5316\u4e0e\u4fee\u590d\u6a21\u5757\uff08WARM\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u539f\u578b\u751f\u6210\u53d7\u521d\u59cb\u968f\u673a\u6027\u5f71\u54cd\u548c\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u5728\u63d0\u53d6\u6709\u9650\u652f\u6301\u96c6\u4e2d\u7684\u5224\u522b\u6027\u8868\u793a\u65f6\uff0c\u4f9d\u8d56\u4e8e\u6700\u8fdc\u70b9\u91c7\u6837\u7b49\u4f20\u7edf\u65b9\u6cd5\u751f\u6210\u539f\u578b\uff0c\u4f46\u5176\u521d\u59cb\u968f\u673a\u6027\u4f1a\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\uff0c\u4e14\u539f\u578b\u751f\u6210\u8fc7\u7a0b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u7ea7\u539f\u578b\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u767d\u5316\u4e0e\u4fee\u590d\u6a21\u5757\uff08WARM\uff09\u89e3\u51b3\u539f\u578b\u751f\u6210\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002WARM\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u524d\u540e\u5206\u522b\u5e94\u7528\u767d\u5316\u548c\u8272\u5f69\u5316\u53d8\u6362\uff0c\u4ee5\u5bf9\u9f50\u652f\u6301\u7279\u5f81\u548c\u539f\u578b\u4ee4\u724c\uff0c\u5e76\u5728\u6ce8\u610f\u529b\u8fc7\u7a0b\u4e2d\u6062\u590d\u539f\u59cb\u5206\u5e03\u3002", "result": "WARM\u901a\u8fc7\u89e3\u51b3\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u800c\u901a\u8fc7\u6355\u6349\u652f\u6301\u7279\u5f81\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u6765\u751f\u6210\u4ee3\u8868\u6027\u539f\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684WARM\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u7684\u539f\u578b\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\u3002"}}
{"id": "2509.14178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14178", "abs": "https://arxiv.org/abs/2509.14178", "authors": ["Kai Ye", "Yuhang Wu", "Shuyuan Hu", "Junliang Li", "Meng Liu", "Yongquan Chen", "Rui Huang"], "title": "\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video", "comment": null, "summary": "Dexterous manipulation remains a challenging robotics problem, largely due to\nthe difficulty of collecting extensive human demonstrations for learning. In\nthis paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos\nwith one generated video and drives robot skill from it: it combines\ndemonstration generation that leverages video generation with pose and depth\nestimation to yield hand-object trajectories, trajectory optimization that uses\nPhysics-aware Interaction Optimization Model (PIOM) to impose physics\nconsistency, and demonstration learning that retargets human motions to a robot\nhand and stabilizes control with an anchor-based residual Proximal Policy\nOptimization (PPO) policy. Using only generated videos, the learned policy\nachieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates\ncoherent executions on a real robot. We also conduct ablation studies to\nvalidate the contribution of each component and demonstrate the ability to\ndirectly specify tasks using natural language, highlighting the flexibility and\nrobustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined\nvideos to real-world execution.", "AI": {"tldr": "\u4f7f\u7528\u751f\u6210\u7684\u89c6\u9891\u8fdb\u884c\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e8677.3%\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6267\u884c\u3002", "motivation": "\u6536\u96c6\u5927\u91cf\u673a\u5668\u4eba\u64cd\u4f5c\u6f14\u793a\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\textsc{Gen2Real}\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1.\u6f14\u793a\u751f\u6210\uff1a\u5229\u7528\u89c6\u9891\u751f\u6210\u6280\u672f\uff0c\u7ed3\u5408\u59ff\u6001\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u751f\u6210\u624b\u90e8-\u7269\u4f53\u8f68\u8ff9\u30022.\u8f68\u8ff9\u4f18\u5316\uff1a\u4f7f\u7528\u7269\u7406\u611f\u77e5\u4ea4\u4e92\u4f18\u5316\u6a21\u578b\uff08PIOM\uff09\u6765\u4fdd\u8bc1\u7269\u7406\u4e00\u81f4\u6027\u30023.\u6f14\u793a\u5b66\u4e60\uff1a\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u624b\u90e8\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u951a\u70b9\u7684\u6b8b\u5dee\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b56\u7565\u6765\u7a33\u5b9a\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6293\u53d6\u4efb\u52a1\u7684\u6210\u529f\u7387\u4e3a77.3%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7684\u6267\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u4ee5\u9a8c\u8bc1\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u6307\u5b9a\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\\textsc{Gen2Real}\u80fd\u591f\u7075\u6d3b\u4e14\u9c81\u68d2\u5730\u5c06\u60f3\u8c61\u4e2d\u7684\u89c6\u9891\u6293\u53d6\u6280\u80fd\u6cdb\u5316\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u6267\u884c\u4e2d\u3002"}}
{"id": "2509.13919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13919", "abs": "https://arxiv.org/abs/2509.13919", "authors": ["Yuanchen Wu", "Ke Yan", "Shouhong Ding", "Ziyin Zhou", "Xiaoqiang Li"], "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration", "comment": "Accepted by ICML 2025", "summary": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.", "AI": {"tldr": "SRC\u901a\u8fc7\u8fed\u4ee3\u6821\u51c6\u6765\u89e3\u51b3LVLM\u4e2d\u57fa\u672c\u539f\u7406\u548c\u7b54\u6848\u4e4b\u95f4\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u89c6\u89c9\u95ee\u7b54\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5728\u4f7f\u57fa\u672c\u539f\u7406\u4e0e\u751f\u6210\u7684\u7b54\u6848\u4fdd\u6301\u4e00\u81f4\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u63a8\u7406\u4e0d\u4e00\u81f4\u548c\u54cd\u5e94\u4e0d\u6b63\u786e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u57fa\u672c\u539f\u7406\u6821\u51c6\uff08SRC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u8fed\u4ee3\u5730\u6821\u51c6\u57fa\u672c\u539f\u7406\u548c\u7b54\u6848\u4e4b\u95f4\u7684\u5bf9\u9f50\uff1a\uff081\uff09\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u201c\u57fa\u672c\u539f\u7406\u5fae\u8c03\u201d\u65b9\u6cd5\uff0c\u4fee\u6539\u6a21\u578b\u7684\u54cd\u5e94\u683c\u5f0f\uff0c\u8981\u6c42\u5728\u63a8\u5bfc\u7b54\u6848\u4e4b\u524d\u63d0\u4f9b\u57fa\u672c\u539f\u7406\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u3002\uff082\uff09\u4e3a\u6bcf\u4e2a\u6837\u672c\u4ece\u5fae\u8c03\u7684LVLM\u4e2d\u641c\u7d22\u4e00\u7ec4\u591a\u6837\u5316\u7684\u5019\u9009\u7b54\u6848\u3002\uff083\uff09\u4f7f\u7528\u5b9a\u5236\u7684\u8bc4\u5206\u6a21\u578bR-Scorer\u63d0\u51fa\u4e00\u79cd\u6210\u5bf9\u8bc4\u5206\u7b56\u7565\uff0c\u4ee5\u8bc4\u4f30\u5019\u9009\u7b54\u6848\u7684\u57fa\u672c\u539f\u7406\u8d28\u91cf\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\uff084\uff09\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u504f\u597d\u7b56\u5c55\u8fc7\u7a0b\uff0c\u5c06\u5bf9\u9f50\u6821\u51c6\u89e3\u8026\u4e3a\u504f\u597d\u5fae\u8c03\u3002", "result": "SRC\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8LVLM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4ee5\u57fa\u672c\u539f\u7406\u4e3a\u5bfc\u5411\u7684\u5bf9\u9f50\u5728\u6316\u6398LVLM\u6f5c\u529b\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14191", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14191", "abs": "https://arxiv.org/abs/2509.14191", "authors": ["Zhihao Cao", "Hanyu Wu", "Li Wa Tang", "Zizhou Luo", "Zihan Zhu", "Wei Zhang", "Marc Pollefeys", "Martin R. Oswald"], "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping", "comment": null, "summary": "Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.", "AI": {"tldr": "MCGS-SLAM \u662f\u9996\u4e2a\u7eaf\u7cb9\u57fa\u4e8e RGB \u7684\u591a\u6444\u50cf\u5934 SLAM \u7cfb\u7edf\uff0c\u5229\u7528 3D \u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u5efa\u56fe\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5355\u76ee SLAM \u5728\u9c81\u68d2\u6027\u548c\u51e0\u4f55\u8986\u76d6\u8303\u56f4\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MCGS-SLAM\u3002", "method": "MCGS-SLAM \u878d\u5408\u4e86\u6765\u81ea\u591a\u4e2a\u89c6\u89d2\u7684\u5bc6\u96c6 RGB \u8f93\u5165\uff0c\u6784\u5efa\u5e76\u6301\u7eed\u4f18\u5316\u9ad8\u65af\u5730\u56fe\u3002\u7cfb\u7edf\u91c7\u7528\u591a\u6444\u50cf\u5934\u675f\u8c03\u6574\uff08MCBA\uff09\u6765\u8054\u5408\u4f18\u5316\u59ff\u6001\u548c\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u5148\u9a8c\u548c\u5c3a\u5ea6\u4e00\u81f4\u6027\u6a21\u5757\u6765\u4fdd\u8bc1\u8de8\u89c6\u56fe\u7684\u5ea6\u91cf\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCGS-SLAM \u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u8f68\u8ff9\u548c\u903c\u771f\u7684\u91cd\u5efa\uff0c\u5176\u6027\u80fd\u901a\u5e38\u4f18\u4e8e\u5355\u76ee\u57fa\u7ebf\u65b9\u6cd5\u3002\u591a\u6444\u50cf\u5934\u8f93\u5165\u63d0\u4f9b\u7684\u5bbd\u89c6\u573a\u80fd\u591f\u91cd\u5efa\u5355\u76ee\u7cfb\u7edf\u9057\u6f0f\u7684\u4fa7\u89c6\u56fe\u533a\u57df\u3002", "conclusion": "MCGS-SLAM \u8bc1\u660e\u4e86\u591a\u6444\u50cf\u5934\u9ad8\u65af\u6cfc\u6e85 SLAM \u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u5efa\u56fe\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13922", "abs": "https://arxiv.org/abs/2509.13922", "authors": ["Wenkui Yang", "Jie Cao", "Junxian Duan", "Ran He"], "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification", "comment": "Accepted to ICCV 2025", "summary": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\uff08\u5982\u6df1\u5ea6\u4f2a\u9020\u548c\u7248\u6743\u4fb5\u6743\uff09\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAntiPure\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e0d\u6613\u5bdf\u89c9\u7684\u6270\u52a8\uff0c\u4f7f\u56fe\u50cf\u5728\u7ecf\u8fc7\u5e38\u7528\u7684\u201c\u51c0\u5316-\u5b9a\u5236\u201d\u6d41\u7a0b\u540e\u4ecd\u80fd\u4fdd\u6301\u5176\u5b89\u5168\u6027\uff0c\u6709\u6548\u9632\u6b62\u6076\u610f\u7be1\u6539\u3002", "motivation": "\u73b0\u6709\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u5728\u9762\u5bf9\u56fe\u50cf\u51c0\u5316\u8fc7\u7a0b\u65f6\u5bb9\u6613\u5931\u6548\uff0c\u65e0\u6cd5\u6709\u6548\u9632\u6b62\u6df1\u5ea6\u4f2a\u9020\u548c\u7248\u6743\u4fb5\u6743\u7b49\u5b89\u5168\u98ce\u9669\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u62b5\u6297\u51c0\u5316\u8fc7\u7a0b\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u56fe\u50cf\u7684\u5b89\u5168\u6027\u3002", "method": "AntiPure\u901a\u8fc7\u4e24\u79cd\u5f15\u5bfc\u673a\u5236\u5b9e\u73b0\u6297\u51c0\u5316\u80fd\u529b\uff1a1. \u5757\u72b6\u9891\u7387\u5f15\u5bfc\uff1a\u51cf\u5c11\u6a21\u578b\u5bf9\u51c0\u5316\u540e\u56fe\u50cf\u9ad8\u9891\u6210\u5206\u7684\u5f71\u54cd\uff1b2. \u9519\u8bef\u65f6\u95f4\u6b65\u5f15\u5bfc\uff1a\u5e72\u6270\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u53bb\u566a\u7b56\u7565\u3002\u8fd9\u4e24\u79cd\u673a\u5236\u5171\u540c\u4f5c\u7528\uff0c\u5728\u4e0d\u5f71\u54cd\u56fe\u50cf\u611f\u77e5\u7684\u524d\u63d0\u4e0b\uff0c\u5d4c\u5165\u80fd\u591f\u62b5\u6297\u51c0\u5316\u548c\u5b9a\u5236\u6d41\u7a0b\u7684\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAntiPure\u5728\u201c\u51c0\u5316-\u5b9a\u5236\u201d\u5de5\u4f5c\u6d41\u4e2d\uff0c\u76f8\u6bd4\u5176\u4ed6\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u6700\u5c0f\u7684\u611f\u77e5\u5dee\u5f02\u548c\u6700\u5927\u7684\u5931\u771f\u5ea6\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u5176\u6297\u51c0\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u8bc4\u4f30\u51c0\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u538b\u529b\u6d4b\u8bd5\u3002", "conclusion": "AntiPure\u662f\u4e00\u79cd\u6709\u6548\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\uff0c\u80fd\u591f\u62b5\u6297\u5e38\u89c1\u7684\u56fe\u50cf\u51c0\u5316\u548c\u5b9a\u5236\u6d41\u7a0b\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u56fe\u50cf\u7684\u5b89\u5168\u6027\uff0c\u5e76\u4e3a\u8bc4\u4f30\u51c0\u5316\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.14210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14210", "abs": "https://arxiv.org/abs/2509.14210", "authors": ["Seth Farrell", "Chenghao Li", "Hongzhan Yu", "Hesam Mojtahedi", "Sicun Gao", "Henrik I. Christensen"], "title": "GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments", "comment": null, "summary": "We present a cooperative aerial-ground search-and-rescue (SAR) framework that\npairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)\nto achieve rapid victim localization and obstacle-aware navigation in unknown\nenvironments. We dub this framework Guided Long-horizon Integrated Drone Escort\n(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon\nplanning. In our framework, a goal-searching UAV executes real-time onboard\nvictim detection and georeferencing to nominate goals for the ground platform,\nwhile a terrain-scouting UAV flies ahead of the UGV's planned route to provide\nmid-level traversability updates. The UGV fuses aerial cues with local sensing\nto perform time-efficient A* planning and continuous replanning as information\narrives. Additionally, we present a hardware demonstration (using a GEM e6 golf\ncart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission\nperformance and include simulation ablations to assess the planning stack in\nisolation from detection. Empirical results demonstrate that explicit role\nseparation across UAVs, coupled with terrain scouting and guided planning,\nimproves reach time and navigation safety in time-critical SAR missions.", "AI": {"tldr": "GLIDE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff0c\u63d0\u9ad8\u4e86\u641c\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u672a\u77e5\u7684\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u53d7\u5bb3\u8005\u5b9a\u4f4d\u548c\u907f\u969c\u7684\u641c\u7d22\u548c\u6551\u63f4\uff08SAR\uff09\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuided Long-horizon Integrated Drone Escort (GLIDE)\u7684\u534f\u540c\u7a7a\u5730\u641c\u7d22\u548c\u6551\u63f4\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7531\u4e24\u67b6\u65e0\u4eba\u673a\uff08UAV\uff09\u548c\u4e00\u67b6\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGV\uff09\u7ec4\u6210\u3002\u4e00\u67b6\u65e0\u4eba\u673a\u8d1f\u8d23\u5b9e\u65f6\u8fdb\u884c\u53d7\u5bb3\u8005\u68c0\u6d4b\u548c\u5730\u7406\u53c2\u8003\uff0c\u4e3a\u5730\u9762\u5e73\u53f0\u6307\u5b9a\u76ee\u6807\uff1b\u53e6\u4e00\u67b6\u65e0\u4eba\u673a\u5219\u5728UGV\u9884\u5b9a\u8def\u7ebf\u524d\u65b9\u8fdb\u884c\u4fa6\u5bdf\uff0c\u63d0\u4f9b\u5730\u5f62\u53ef\u901a\u884c\u6027\u66f4\u65b0\u3002UGV\u878d\u5408\u4e86\u6765\u81ea\u65e0\u4eba\u673a\u7684\u7ebf\u7d22\u548c\u672c\u5730\u4f20\u611f\u4fe1\u606f\uff0c\u8fdb\u884c\u9ad8\u6548\u7684A*\u89c4\u5212\u548c\u6301\u7eed\u7684\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u786c\u4ef6\u6f14\u793a\uff08\u4f7f\u7528GEM e6\u9ad8\u5c14\u592b\u7403\u8f66\u4f5c\u4e3aUGV\u548c\u4e24\u67b6X500\u65e0\u4eba\u673a\uff09\u548c\u4eff\u771f\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u7aef\u5230\u7aef\u7684SAR\u4efb\u52a1\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u4eba\u673a\u660e\u786e\u7684\u89d2\u8272\u5206\u79bb\u3001\u5730\u5f62\u4fa6\u5bdf\u548c\u5f15\u5bfc\u5f0f\u89c4\u5212\uff0c\u80fd\u591f\u7f29\u77ed\u5230\u8fbe\u65f6\u95f4\u5e76\u63d0\u9ad8\u65f6\u95f4\u5173\u952e\u578bSAR\u4efb\u52a1\u7684\u5bfc\u822a\u5b89\u5168\u6027\u3002", "conclusion": "GLIDE\u6846\u67b6\u901a\u8fc7\u660e\u786e\u7684\u89d2\u8272\u5206\u5de5\u548c\u6709\u6548\u7684\u534f\u540c\u89c4\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u4e2d\u5730\u9762\u534f\u540c\u641c\u7d22\u548c\u6551\u63f4\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.13936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13936", "abs": "https://arxiv.org/abs/2509.13936", "authors": ["Harvey Mannering", "Zhiwu Huang", "Adam Prugel-Bennett"], "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done", "comment": null, "summary": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.", "AI": {"tldr": "\u901a\u8fc7\u589e\u52a0\u521d\u59cb\u566a\u58f0\u4e0e\u901a\u7528\u5f15\u5bfc\u5bf9\u9f50\u7684\u53ef\u80fd\u6027\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u4e2d\u7684\u521d\u59cb\u566a\u58f0\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u5ea6\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u4f7f\u7528\u968f\u673a\u9ad8\u65af\u566a\u58f0\u4f5c\u4e3a\u8d77\u70b9\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6700\u7ec8\u8f93\u51fa\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u5ea6\u3002\u73b0\u6709\u7684\u566a\u58f0\u6c34\u5e73\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u6709\u76d1\u7763\u6570\u636e\u96c6\u3001\u989d\u5916\u7684\u7f51\u7edc\u6216\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u4f18\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u566a\u58f0\u6c34\u5e73\u5f15\u5bfc\uff08NLG\uff09\u7684\u7b80\u5355\u3001\u9ad8\u6548\u3001\u901a\u7528\u7684\u566a\u58f0\u6c34\u5e73\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u521d\u59cb\u566a\u58f0\u4e0e\u901a\u7528\u5f15\u5bfc\u5bf9\u9f50\u7684\u53ef\u80fd\u6027\u6765\u4f18\u5316\u521d\u59cb\u566a\u58f0\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u6570\u636e\u3001\u8f85\u52a9\u7f51\u7edc\u6216\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u8f93\u51fa\u751f\u6210\u8d28\u91cf\u548c\u8f93\u5165\u6761\u4ef6\u9075\u5faa\u5ea6\u3002", "conclusion": "NLG\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u6269\u6563\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u5f15\u5bfc\u65b9\u6cd5\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.14228", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14228", "abs": "https://arxiv.org/abs/2509.14228", "authors": ["Benjamin Shaffer", "Victoria Edwards", "Brooks Kinch", "Nathaniel Trask", "M. Ani Hsieh"], "title": "Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models", "comment": null, "summary": "Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u6bcf\u4e2a\u673a\u5668\u4eba\u643a\u5e26\u7684\u673a\u5668\u5b66\u4e60\u6709\u9650\u5143\u6a21\u578b\u6765\u6307\u5bfc\u4fe1\u606f\u91c7\u6837\uff0c\u4ee5\u5728\u590d\u6742\u6d41\u4f53\u73af\u5883\u4e2d\u5b9e\u73b0\u6e90\u5b9a\u4f4d\u3002", "motivation": "\u5728\u5177\u6709\u65f6\u53d8\u548c\u6df7\u6c8c\u6d41\u52a8\u7684\u590d\u6742\u73af\u5883\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u5b9a\u4f4d\u5316\u5b66\u6cc4\u6f0f\u6e90\u6216\u8ffd\u8e2a\u6ea2\u6cb9\u6269\u6563\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u611f\u6570\u636e\u95f4\u6b47\u4e14\u73af\u5883\u51e0\u4f55\u5f62\u72b6\u590d\u6742\uff0c\u9650\u5236\u4e86\u5bf9\u6d41\u4f53\u6269\u6563\u7684\u5efa\u6a21\u548c\u9884\u6d4b\u80fd\u529b\u3002\u673a\u5668\u4eba\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u6570\u503c\u6a21\u578b\uff0c\u8fd9\u5728\u677f\u8f7d\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u673a\u5668\u4eba\u643a\u5e26\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u6709\u9650\u5143\u6a21\u578b\u6765\u8868\u793a\u5176\u73af\u5883\uff0c\u4ee5\u6307\u5bfc\u4fe1\u606f\u91c7\u6837\u3002\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u8bc4\u4f30\u8fd1\u4f3c\u4e92\u4fe1\u606f\u6807\u51c6\uff0c\u4ee5\u9a71\u52a8\u4fe1\u606f\u7d20\u63a7\u5236\u7b56\u7565\uff0c\u9009\u62e9\u80fd\u6700\u5927\u5316\u6e90\u5b9a\u4f4d\u76ee\u6807\u4fe1\u606f\u91cf\u7684\u4f20\u611f\u533a\u57df\u3002", "result": "\u4e0e\u57fa\u7ebf\u4f20\u611f\u7b56\u7565\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bef\u5dee\u51cf\u5c11\uff1b\u4e0e\u57fa\u7ebf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u6e90\u5b9a\u4f4d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u4fe1\u606f\u7d20\u63a7\u5236\u7b56\u7565\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u6d41\u4f53\u73af\u5883\u4e2d\u7684\u6e90\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2509.13939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13939", "abs": "https://arxiv.org/abs/2509.13939", "authors": ["Gia Khanh Nguyen", "Yifeng Huang", "Minh Hoai"], "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation", "comment": null, "summary": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.", "AI": {"tldr": "PairTally\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u533a\u5206\u548c\u8ba1\u6570\u7279\u5b9a\u7c7b\u578b\u5bf9\u8c61\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u5fae\u5dee\u5f02\u548c\u89c6\u89c9\u6b67\u4e49\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u6570\u6a21\u578b\uff08\u5305\u62ec\u7279\u5b9a\u7c7b\u522b\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5728\u6267\u884c\u7ec6\u7c92\u5ea6\u7684\u3001\u7531\u7528\u6237\u610f\u56fe\u9a71\u52a8\u7684\u8ba1\u6570\u4efb\u52a1\u65b9\u9762\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u8fd9\u4e00\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b681\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6570\u636e\u96c6PairTally\uff0c\u5176\u4e2d\u5305\u542b\u4e24\u4e2a\u5bf9\u8c61\u7c7b\u522b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8de8\u7c7b\u522b\u548c\u7c7b\u522b\u5185\u4e24\u79cd\u8ba1\u6570\u8bbe\u7f6e\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u533a\u5206\u548c\u8ba1\u6570\u7684\u80fd\u529b\u3002\u5bf9\u5305\u62ec\u793a\u4f8b\u9a71\u52a8\u65b9\u6cd5\u3001\u8bed\u8a00\u63d0\u793a\u6a21\u578b\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u8fd1\u671f\u6a21\u578b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u53ef\u9760\u5730\u6309\u7167\u7528\u6237\u610f\u56fe\u8fdb\u884c\u8ba1\u6570\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u548c\u89c6\u89c9\u4e0a\u6a21\u7cca\u7684\u6848\u4f8b\u65f6\u3002", "conclusion": "PairTally\u6570\u636e\u96c6\u4e3a\u8bca\u65ad\u548c\u6539\u8fdb\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u548c\u6267\u884c\u7528\u6237\u7279\u5b9a\u8ba1\u6570\u610f\u56fe\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.14001", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14001", "abs": "https://arxiv.org/abs/2509.14001", "authors": ["Elena Camuffo", "Francesco Barbato", "Mete Ozay", "Simone Milani", "Umberto Michieli"], "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "comment": null, "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.", "AI": {"tldr": "MOCHA\u662f\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u533a\u57df\u7ea7\u8bed\u4e49\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u89c6\u89c9\u5bf9\u8c61\u68c0\u6d4b\u5668\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u67b6\u6784\u5bf9\u9f50\u3002", "motivation": "\u5c06\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaVA\uff09\u7684\u533a\u57df\u7ea7\u591a\u6a21\u6001\u8bed\u4e49\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u7684\u7eaf\u89c6\u89c9\u5bf9\u8c61\u68c0\u6d4b\u5668\uff08\u5982YOLO\uff09\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5176\u6027\u80fd\u3002", "method": "MOCHA\u4f7f\u7528\u4e00\u4e2a\u7ffb\u8bd1\u6a21\u5757\u5c06\u5b66\u751f\u7279\u5f81\u6620\u5c04\u5230\u8054\u5408\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\u6307\u5bfc\u5b66\u751f\u548c\u7ffb\u8bd1\u5668\u7684\u8bad\u7ec3\uff0c\u8be5\u635f\u5931\u51fd\u6570\u540c\u65f6\u5f3a\u5236\u6267\u884c\u5c40\u90e8\u5bf9\u9f50\u548c\u5168\u5c40\u5173\u7cfb\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u5bf9\u8c61\u7ea7\u522b\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u4fee\u6539\u6559\u5e08\u6a21\u578b\u6216\u5728\u63a8\u7406\u65f6\u9700\u8981\u6587\u672c\u8f93\u5165\u3002", "result": "\u5728\u56db\u4e2a\u4e2a\u6027\u5316\u68c0\u6d4b\u57fa\u51c6\u7684\u5c11\u6837\u672c\u573a\u666f\u4e0b\uff0cMOCHA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u5f97\u5206\u63d0\u9ad8\u4e86+10.1\u3002", "conclusion": "MOCHA\u662f\u4e00\u79cd\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5373\u4f7f\u5176\u7ed3\u6784\u7d27\u51d1\uff0c\u4e5f\u80fd\u5b9e\u73b0\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.14012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14012", "abs": "https://arxiv.org/abs/2509.14012", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Tobias Koch"], "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments", "comment": null, "summary": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a YOLO-FEDER FusionNet \u7684\u589e\u5f3a\u578b\u65e0\u4eba\u673a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u3001\u7279\u5f81\u878d\u5408\u548c\u9aa8\u5e72\u7f51\u7edc\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u89c6\u89c9\u590d\u6742\u73af\u5883\u4e2d\uff0c\u80cc\u666f\u6742\u6ce2\u3001\u76ee\u6807\u5c3a\u5ea6\u5c0f\u4ee5\u53ca\u4f2a\u88c5\u6548\u679c\u4f7f\u5f97\u65e0\u4eba\u673a\u68c0\u6d4b\u5145\u6ee1\u6311\u6218\u3002\u4f20\u7edf\u7684\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u5668\uff08\u5982 YOLO\uff09\u5728\u4f4e\u7eb9\u7406\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u76ee\u6807\u4e0e\u80cc\u666f\u533a\u5206\u5ea6\u4f4e\u4e14\u6742\u6ce2\u8f83\u591a\u7684\u573a\u666f\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 YOLO-FEDER FusionNet\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u6280\u672f\u7684\u68c0\u6d4b\u6846\u67b6\u3002\u7814\u7a76\u6539\u8fdb\u4e86\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\uff08\u4f7f\u7528\u5927\u89c4\u6a21\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u5408\u6210\u6570\u636e\u548c\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6837\u672c\uff09\u3001\u7279\u5f81\u878d\u5408\u7b56\u7565\uff08\u8bc4\u4f30\u4e86\u4e2d\u95f4\u591a\u5c3a\u5ea6 FEDER \u7279\u5f81\u7684\u8d21\u732e\uff09\u4ee5\u53ca\u9aa8\u5e72\u7f51\u7edc\u8bbe\u8ba1\uff08\u5728\u591a\u4e2a YOLO \u9aa8\u5e72\u7f51\u7edc\u914d\u7f6e\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u4e2d\u95f4 FEDER \u7279\u5f81\u548c\u9aa8\u5e72\u7f51\u7edc\u5347\u7ea7\u80fd\u591f\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002\u5728\u6700\u4f18\u914d\u7f6e\uff08YOLO-FEDER FusionNet \u7ed3\u5408 YOLOv8l \u9aa8\u5e72\u7f51\u7edc\u548c DWD \u6a21\u5757\u63d0\u53d6\u7684 FEDER \u7279\u5f81\uff09\u4e0b\uff0c\u4e0e\u521d\u59cb\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728 IoU \u9608\u503c\u4e3a 0.5 \u65f6\uff0c\u5047\u9634\u6027\u7387\uff08FNR\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe 39.1 \u4e2a\u767e\u5206\u70b9\uff0c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u63d0\u9ad8\u4e86\u9ad8\u8fbe 62.8 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u5bf9\u8bad\u7ec3\u6570\u636e\u3001\u7279\u5f81\u878d\u5408\u548c\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u7cfb\u7edf\u6027\u6539\u8fdb\uff0cYOLO-FEDER FusionNet \u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.14033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14033", "abs": "https://arxiv.org/abs/2509.14033", "authors": ["Weijie Yin", "Yongjie Ye", "Fangxun Shu", "Yue Liao", "Zijian Kang", "Hongyuan Dong", "Haiyang Yu", "Dingkang Yang", "Jiacong Wang", "Han Wang", "Wenzhuo Liu", "Xiao Liang", "Shuicheng Yan", "Chao Feng"], "title": "SAIL-VL2 Technical Report", "comment": "Technical Report", "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.", "AI": {"tldr": "SAIL-VL2\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u53ca\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5176\u4e3b\u8981\u4f18\u52bf\u5728\u4e8e\u6570\u636e\u5904\u7406\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\u548c\u9ad8\u6548\u7684MoE\u67b6\u6784\u3002", "motivation": "\u4ecb\u7ecdSAIL-VL2\uff0c\u4e00\u4e2a\u7528\u4e8e\u5168\u9762\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u7684\u5f00\u6e90\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08LVM\uff09\uff0c\u4f5c\u4e3aSAIL-VL\u7684\u7ee7\u4efb\u8005\uff0cSAIL-VL2\u57282B\u548c8B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u5728\u5404\u79cd\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4ece\u7ec6\u7c92\u5ea6\u611f\u77e5\u5230\u590d\u6742\u63a8\u7406\u7684\u5f3a\u5927\u80fd\u529b\u3002", "method": "SAIL-VL2\u7684\u6709\u6548\u6027\u7531\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\u9a71\u52a8\uff1a1. \u5177\u6709\u8bc4\u5206\u548c\u8fc7\u6ee4\u7b56\u7565\u7684\u5927\u89c4\u6a21\u6570\u636e\u6574\u7406\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5b57\u5e55\u3001OCR\u3001QA\u548c\u89c6\u9891\u6570\u636e\u7684\u8d28\u91cf\u548c\u5206\u5e03\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u30022. \u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u4ece\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\uff08SAIL-ViT\uff09\u5f00\u59cb\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u6700\u7ec8\u8fbe\u5230\u601d\u7ef4\u878d\u5408SFT-RL\u6df7\u5408\u8303\u5f0f\uff0c\u7cfb\u7edf\u5730\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u30023. \u67b6\u6784\u7684\u8fdb\u6b65\uff0c\u4ece\u5bc6\u96c6\u7684LLM\u6269\u5c55\u5230\u9ad8\u6548\u7684\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bbe\u8ba1\u3002", "result": "SAIL-VL2\u5728106\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u5728MMMU\u548cMathVista\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5728OpenCompass\u6392\u884c\u699c\u4e0a\uff0cSAIL-VL2-2B\u57284B\u53c2\u6570\u89c4\u6a21\u4ee5\u4e0b\u7684\u5b98\u65b9\u53d1\u5e03\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "SAIL-VL2\u901a\u8fc7\u5176\u5148\u8fdb\u7684\u6570\u636e\u5904\u7406\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\u548cMoE\u67b6\u6784\uff0c\u5728\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u5c31\uff0c\u4e3a\u5f00\u6e90\u591a\u6a21\u6001\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2509.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14051", "abs": "https://arxiv.org/abs/2509.14051", "authors": ["Suhang You", "Carla Pitarch-Abaigar", "Sanket Kachole", "Sumedh Sonawane", "Juhyung Ha", "Anish Sudarshan Gada", "David Crandall", "Rakesh Shiradkar", "Spyridon Bakas"], "title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings", "comment": "11 pages, 1 figure, method paper for CHIMERA 2025 Challenge", "summary": "Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7103 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPROFUSEme\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u624b\u672f\u65f6\u9884\u6d4b\u524d\u5217\u817a\u764c\u590d\u53d1\uff08BCR\uff09\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u8fd130%\u7684\u524d\u5217\u817a\u764c\uff08PCa\uff09\u60a3\u8005\u5728\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\uff08RP\uff09\u540e\u4f1a\u51fa\u73b0\u751f\u5316\u590d\u53d1\uff08BCR\uff09\uff0c\u8fd9\u4e0e\u6b7b\u4ea1\u7387\u589e\u52a0\u6709\u5173\u3002\u5728RP\u65f6\u51c6\u786e\u9884\u6d4bBCR\u6709\u52a9\u4e8e\u53ca\u65f6\u7684\u4e34\u5e8a\u51b3\u7b56\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPROFUSEme\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u4e34\u5e8a\u3001\u653e\u5c04\u548c\u75c5\u7406\u6570\u636e\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u4e86Cox\u6bd4\u4f8b\u98ce\u9669\u56de\u5f52\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5185\u90e85\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6\u4e0b\u8fbe\u5230\u4e860.861\uff08$\\sigma=0.112$\uff09\u7684\u5e73\u5747C\u6307\u6570\uff0c\u5728CHIMERA 2025\u6311\u6218\u8d5b\u7684\u9a8c\u8bc1\u6392\u884c\u699c\u4e0a\u8fbe\u5230\u4e860.7103\u7684C\u6307\u6570\uff0c\u4f18\u4e8e\u665a\u671f\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PROFUSEme\u65b9\u6cd5\u5728\u9884\u6d4b\u524d\u5217\u817a\u764c\u60a3\u8005\u7684\u751f\u5316\u590d\u53d1\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14055", "abs": "https://arxiv.org/abs/2509.14055", "authors": ["Gang Cheng", "Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Ju Li", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Feng Wang", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic Replication", "comment": "Project Page: https://humanaigc.github.io/wan-animate/", "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.", "AI": {"tldr": "Wan-Animate \u662f\u4e00\u4e2a\u7528\u4e8e\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u53c2\u8003\u89c6\u9891\u751f\u6210\u9ad8\u4fdd\u771f\u89d2\u8272\u52a8\u753b\uff0c\u6216\u5c06\u89d2\u8272\u65e0\u7f1d\u96c6\u6210\u5230\u53c2\u8003\u89c6\u9891\u4e2d\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u7684\u7edf\u4e00\uff0c\u5e76\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u89d2\u8272\u89c6\u9891\uff0c\u540c\u65f6\u5b9e\u73b0\u65e0\u7f1d\u7684\u73af\u5883\u96c6\u6210\u3002", "method": "\u63d0\u51fa Wan-Animate \u6846\u67b6\uff0c\u57fa\u4e8e Wan \u6a21\u578b\uff0c\u91c7\u7528\u4fee\u6539\u540e\u7684\u8f93\u5165\u8303\u5f0f\u533a\u5206\u53c2\u8003\u6761\u4ef6\u548c\u751f\u6210\u533a\u57df\uff1b\u4f7f\u7528\u7a7a\u95f4\u5bf9\u9f50\u7684\u9aa8\u9abc\u4fe1\u53f7\u590d\u5236\u8eab\u4f53\u8fd0\u52a8\uff0c\u63d0\u53d6\u9690\u5f0f\u9762\u90e8\u7279\u5f81\u91cd\u73b0\u8868\u60c5\uff1b\u5f15\u5165\u8f85\u52a9 Relighting LoRA \u6a21\u5757\u4ee5\u589e\u5f3a\u73af\u5883\u96c6\u6210\uff0c\u4fdd\u6301\u89d2\u8272\u5916\u89c2\u4e00\u81f4\u6027\u5e76\u5e94\u7528\u73af\u5883\u5149\u7167\u548c\u8272\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e Wan-Animate \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Wan-Animate \u6210\u529f\u5b9e\u73b0\u4e86\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u7684\u7edf\u4e00\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2509.14060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14060", "abs": "https://arxiv.org/abs/2509.14060", "authors": ["Jun Du", "Weiwei Xing", "Ming Li", "Fei Richard Yu"], "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement", "comment": null, "summary": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.", "AI": {"tldr": "\u5f53\u524d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u7b97\u6cd5\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8bed\u4e49\u589e\u5f3a\uff08VSE-MOT\uff09\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5168\u5c40\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408MOT-Adapter\u548cVSFM\u6a21\u5757\u8fdb\u884c\u7279\u5f81\u878d\u5408\uff0c\u4ee5\u63d0\u5347\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVSE-MOT\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u67098%-20%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u7b97\u6cd5\u5728\u5904\u7406\u4f4e\u8d28\u91cf\u89c6\u9891\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\uff0c\u63d0\u5347MOT\u7b97\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e0b\u7684\u5e94\u7528\u80fd\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8bed\u4e49\u589e\u5f3a\u5f15\u5bfc\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08VSE-MOT\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e09\u5206\u652f\u7ed3\u6784\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5168\u5c40\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u5e76\u4e0e\u67e5\u8be2\u5411\u91cf\u878d\u5408\u3002\u5f15\u5165\u4e86\u591a\u76ee\u6807\u8ddf\u8e2a\u9002\u914d\u5668\uff08MOT-Adapter\uff09\u5c06\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u9002\u914d\u4e8eMOT\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u89c6\u89c9\u8bed\u4e49\u878d\u5408\u6a21\u5757\uff08VSFM\uff09\u6765\u589e\u5f3a\u7279\u5f81\u878d\u5408\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VSE-MOT\u5728\u771f\u5b9e\u4e16\u754c\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u5176\u8ddf\u8e2a\u6027\u80fd\u6307\u6807\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u7ea68%-20%\uff0c\u5e76\u4e14\u5728\u5e38\u89c4\u573a\u666f\u4e0b\u4e5f\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "VSE-MOT\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u76ee\u6807\u8ddf\u8e2a\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14084", "abs": "https://arxiv.org/abs/2509.14084", "authors": ["Jingyi Yuan", "Jianxiong Ye", "Wenkang Chen", "Chenqiang Gao"], "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration", "comment": null, "summary": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06 DINOv3 \u6a21\u578b\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b (ZSAD)\uff0c\u5e76\u63d0\u51fa\u4e86 AD-DINOv3 \u6846\u67b6\u6765\u89e3\u51b3\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7684\u57df\u504f\u5dee\u4ee5\u53ca\u5168\u5c40\u8bed\u4e49\u504f\u89c1\u5bfc\u81f4\u7684\u7ec6\u5fae\u5f02\u5e38\u8bc6\u522b\u56f0\u96be\u3002\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757 (AACM)\uff0cAD-DINOv3 \u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u901a\u7528 ZSAD \u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b (ZSAD) \u65b9\u6cd5\u591a\u57fa\u4e8e CLIP \u6a21\u578b\uff0c\u4f46\u968f\u7740 DINOv3 \u7b49\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u8868\u5f81\u80fd\u529b\u4e0a\u7684\u7a81\u7834\uff0c\u63a2\u7d22\u5176\u5728 ZSAD \u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6210\u4e3a\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528 DINOv3 \u9762\u4e34\u57df\u504f\u5dee\u548c\u5168\u5c40\u8bed\u4e49\u504f\u89c1\u5bfc\u81f4\u7684\u7ec6\u5fae\u5f02\u5e38\u8bc6\u522b\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684 ZSAD \u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 AD-DINOv3 \u6846\u67b6\uff0c\u5c06\u5f02\u5e38\u68c0\u6d4b\u8868\u8ff0\u4e3a\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4f7f\u7528 DINOv3 \u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff08patch tokens \u548c CLS token\uff09\uff0c\u5e76\u5229\u7528 CLIP \u6587\u672c\u7f16\u7801\u5668\u5904\u7406\u6b63\u5e38\u548c\u5f02\u5e38\u7684\u6587\u672c\u63d0\u793a\u3002\u4e3a\u4e86\u89e3\u51b3\u57df\u504f\u5dee\uff0c\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6765\u6821\u51c6\u4e24\u79cd\u6a21\u6001\u7684\u8868\u5f81\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757 (AACM) \u6765\u5f15\u5bfc CLS token \u5173\u6ce8\u5f02\u5e38\u533a\u57df\uff0c\u800c\u975e\u901a\u7528\u524d\u666f\u8bed\u4e49\uff0c\u4ee5\u589e\u5f3a\u533a\u5206\u80fd\u529b\u3002", "result": "\u5728\u516b\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAD-DINOv3 \u7684\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "AD-DINOv3 \u6846\u67b6\u6210\u529f\u5730\u5c06 DINOv3 \u5e94\u7528\u4e8e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u9002\u914d\u5668\u548c\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u5dee\u548c\u5168\u5c40\u8bed\u4e49\u504f\u89c1\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u7528 ZSAD \u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.14097", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.14097", "abs": "https://arxiv.org/abs/2509.14097", "authors": ["Yaru Chen", "Ruohao Guo", "Liting Gao", "Yang Xiang", "Qingyu Luo", "Zhenbo Li", "Wenwu Wang"], "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing", "comment": null, "summary": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u89c6\u542c\u89c6\u9891\u89e3\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u6bb5\u7ea7\u4f2a\u76d1\u7763\u548c\u7c7b\u522b\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4ee5\u5f80\u7684\u5f31\u76d1\u7763\u89c6\u542c\u89c6\u9891\u89e3\u6790\u65b9\u6cd5\u5728\u6ca1\u6709\u65f6\u95f4\u6233\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u9884\u6d4b\u7684\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u7a33\u5b9a\u7684\u5206\u6bb5\u7ea7\u76d1\u7763\u548c\u7c7b\u522b\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\uff1a1. \u6307\u5bfc\u6027\u7684\u4f2a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u751f\u6210\u53ef\u9760\u7684\u5206\u6bb5\u7ea7\u63a9\u7801\uff1b2. \u7c7b\u522b\u611f\u77e5\u7684\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff08CMA\uff09\u635f\u5931\uff0c\u5bf9\u9f50\u53ef\u9760\u5206\u6bb5\u4e2d\u7684\u97f3\u9891\u548c\u89c6\u89c9\u5d4c\u5165\u3002", "result": "\u5728LLP\u548cUnAV-100\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684EMA\u4f2a\u76d1\u7763\u6846\u67b6\u548cCMA\u635f\u5931\u80fd\u591f\u63d0\u4f9b\u7a33\u5b9a\u7684\u65f6\u5e8f\u76d1\u7763\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u5f31\u76d1\u7763\u89c6\u542c\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14165", "abs": "https://arxiv.org/abs/2509.14165", "authors": ["Michal Szczepanski", "Martyna Poreba", "Karim Haroun"], "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions", "comment": null, "summary": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.", "AI": {"tldr": "ViT\u5728\u8bed\u4e49\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u3002STEP\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5757\u5408\u5e76\u548c\u65e9\u671f\u526a\u679d\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002dCTS\u7f51\u7edc\u5b9e\u73b0\u4e86\u5757\u5408\u5e76\uff0c\u800c\u65e9\u671f\u9000\u51fa\u673a\u5236\u5219\u5220\u9664\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5757\u3002", "motivation": "\u89e3\u51b3Vision Transformers\uff08ViTs\uff09\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSTEP\uff08SuperToken and Early-Pruning\uff09\u6df7\u5408\u6807\u8bb0\u7f29\u51cf\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u5757\u5408\u5e76\u548c\u6807\u8bb0\u526a\u679d\u3002\u6838\u5fc3\u662fdCTS\uff08\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u7b56\u7565\u7f51\u7edc\uff09\uff0c\u7528\u4e8e\u7075\u6d3b\u5730\u5408\u5e76\u6210\u8d85\u5757\u3002\u7f16\u7801\u5668\u5757\u8fd8\u96c6\u6210\u4e86\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u7528\u4e8e\u79fb\u9664\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u8d85\u5757\u3002", "result": "\u5728\u9ad8\u8fbe1024x1024\u7684\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u72ec\u4f7f\u7528dCTS\u53ef\u5c06\u6807\u8bb0\u6570\u91cf\u51cf\u5c112.5\u500d\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e2.6\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad83.4\u500d\uff08\u4f7f\u7528ViT-Large\uff09\u3002\u5b8c\u6574\u7684STEP\u6846\u67b6\u53ef\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e4\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad81.7\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u8d85\u8fc72.0%\u3002\u9ad8\u8fbe40%\u7684\u6807\u8bb0\u53ef\u4ee5\u5728\u5230\u8fbe\u6700\u7ec8\u7f16\u7801\u5668\u5c42\u4e4b\u524d\u88ab\u81ea\u4fe1\u5730\u9884\u6d4b\u5e76\u505c\u6b62\u3002", "conclusion": "STEP\u6846\u67b6\u901a\u8fc7\u5f15\u5165dCTS\u548c\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86ViT\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.14104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14104", "abs": "https://arxiv.org/abs/2509.14104", "authors": ["Leonard Hackel", "Tom Burgert", "Beg\u00fcm Demir"], "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts", "comment": null, "summary": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u8f6f\u6df7\u5408\u4e13\u5bb6\uff08Soft MoE\uff09\u673a\u5236\uff0c\u63d0\u51faCSM\uff08Cross-Sensor Mixture-of-Experts\uff09\u6a21\u578b\uff0c\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdb\u4e86\u9065\u611f\u57fa\u7840\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\uff08RS FMs\uff09\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u8ba1\u7b97\u91cf\u5927\uff0c\u6216\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06Soft MoE\u673a\u5236\u96c6\u6210\u5230CSMAE\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u8de8\u4f20\u611f\u5668\u8868\u793a\u5b66\u4e60\u3002\u5f15\u5165\u4e3b\u9898-\u6c14\u5019\u63cf\u8ff0\u7b26\u9a71\u52a8\u7684\u91c7\u6837\u7b56\u7565\u6765\u6784\u5efa\u8bad\u7ec3\u96c6\u3002", "result": "CSMoE\u6a21\u578b\u5728\u573a\u666f\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u9762\uff0c\u8ba1\u7b97\u9700\u6c42\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u8868\u793a\u6027\u80fd\u3002\u4e0e\u6700\u5148\u8fdb\u7684RS FMs\u76f8\u6bd4\uff0cCSMoE\u5728\u8868\u793a\u80fd\u529b\u3001\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210Soft MoE\u7684\u9002\u914d\u5668\u80fd\u591f\u521b\u5efa\u8ba1\u7b97\u9ad8\u6548\u7684RS FMs\uff0c\u540c\u65f6\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14119", "abs": "https://arxiv.org/abs/2509.14119", "authors": ["Jiabo MA", "Wenqiang Li", "Jinbang Li", "Ziyi Liu", "Linshan Wu", "Fengtao Zhou", "Li Liang", "Ronald Cheong Kin Chan", "Terence T. W. Wong", "Hao Chen"], "title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows", "comment": "the arxiv version of the under review journal paper", "summary": "Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u7ea7\u8054\u914d\u51c6\u673a\u5236\u89e3\u51b3\u914d\u51c6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9519\u4f4d\u6570\u636e\u65f6\u3002", "motivation": "\u4f20\u7edf\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bca\u65ad\u9700\u8981\u591a\u79cd\u67d3\u8272\uff0c\u8fc7\u7a0b\u8017\u65f6\u3001\u8017\u529b\u4e14\u4e0d\u73af\u4fdd\u3002\u865a\u62df\u67d3\u8272\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96be\u4ee5\u83b7\u5f97\u7684\u914d\u51c6\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7ea7\u8054\u914d\u51c6\u673a\u5236\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u751f\u6210\u8f93\u51fa\u548c\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e863.2%\uff0c\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e8610.1%\u3002\u5728\u9519\u4f4d\u6570\u636e\u4e0a\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u4fe1\u566a\u6bd4\u63d0\u9ad8\u4e8623.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u51fa\u8272\u7684\u9c81\u68d2\u6027\uff0c\u7b80\u5316\u4e86\u865a\u62df\u67d3\u8272\u7684\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u865a\u62df\u67d3\u8272\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.14120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14120", "abs": "https://arxiv.org/abs/2509.14120", "authors": ["Sara Concas", "Simone Maurizio La Cava", "Andrea Panzino", "Ester Masala", "Giulia Orr\u00f9", "Gian Luca Marcialis"], "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection", "comment": "Accepted at the 2025 IEEE INTERNATIONAL CONFERENCE ON Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering", "summary": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations.", "AI": {"tldr": "\u793e\u4ea4\u5a92\u4f53\u6ee4\u955c\u4f1a\u964d\u4f4e\u4eba\u8138\u8bc6\u522b\u548cDeepfake\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u6ee4\u955c\u5bf9Deepfake\u548c\u4eba\u8138\u5408\u6210\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u8bc4\u4f30\u591a\u79cd\u5148\u8fdb\u7684Deepfake\u548c\u4eba\u8138\u5408\u6210\u653b\u51fb\u68c0\u6d4b\u5668\u5728\u5e94\u7528\u5e73\u6ed1\u6ee4\u955c\u524d\u540e\u7684\u6027\u80fd\u3002", "result": "\u4eba\u8138\u6ee4\u955c\u4f1a\u5bfc\u81f4Deepfake\u548c\u4eba\u8138\u5408\u6210\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u80fd\u62b5\u6297\u9762\u90e8\u589e\u5f3a\u6280\u672f\u7684\u9c81\u68d2\u6027\u68c0\u6d4b\u6a21\u578b\u3002"}}
{"id": "2509.14142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14142", "abs": "https://arxiv.org/abs/2509.14142", "authors": ["Peng Xu", "Shengwu Xiong", "Jiajun Zhang", "Yaxiong Chen", "Bowen Zhou", "Chen Change Loy", "David A. Clifton", "Kyoung Mu Lee", "Luc Van Gool", "Ruiming He", "Ruilin Yao", "Xinwei Long", "Jirui Huang", "Kai Tian", "Sa Yang", "Yihua Shao", "Jin Feng", "Yue Zhong", "Jiakai Zhou", "Cheng Tang", "Tianyu Zou", "Yifang Zhang", "Junming Liang", "Guoyou Li", "Zhaoxiang Wang", "Qiang Zhou", "Yichen Zhao", "Shili Xiong", "Hyeongjin Nam", "Jaerin Lee", "Jaeyoung Chung", "JoonKyu Park", "Junghun Oh", "Kanggeon Lee", "Wooseok Lee", "Juneyoung Ro", "Turghun Osman", "Can Hu", "Chaoyang Liao", "Cheng Chen", "Chengcheng Han", "Chenhao Qiu", "Chong Peng", "Cong Xu", "Dailin Li", "Feiyu Wang", "Feng Gao", "Guibo Zhu", "Guopeng Tang", "Haibo Lu", "Han Fang", "Han Qi", "Hanxiao Wu", "Haobo Cheng", "Hongbo Sun", "Hongyao Chen", "Huayong Hu", "Hui Li", "Jiaheng Ma", "Jiang Yu", "Jianing Wang", "Jie Yang", "Jing He", "Jinglin Zhou", "Jingxuan Li", "Josef Kittler", "Lihao Zheng", "Linnan Zhao", "Mengxi Jia", "Muyang Yan", "Nguyen Thanh Thien", "Pu Luo", "Qi Li", "Shien Song", "Shijie Dong", "Shuai Shao", "Shutao Li", "Taofeng Xue", "Tianyang Xu", "Tianyi Gao", "Tingting Li", "Wei Zhang", "Weiyang Su", "Xiaodong Dong", "Xiao-Jun Wu", "Xiaopeng Zhou", "Xin Chen", "Xin Wei", "Xinyi You", "Xudong Kang", "Xujie Zhou", "Xusheng Liu", "Yanan Wang", "Yanbin Huang", "Yang Liu", "Yang Yang", "Yanglin Deng", "Yashu Kang", "Ye Yuan", "Yi Wen", "Yicen Tian", "Yilin Tao", "Yin Tang", "Yipeng Lin", "Yiqing Wang", "Yiting Xi", "Yongkang Yu", "Yumei Li", "Yuxin Qin", "Yuying Chen", "Yuzhe Cen", "Zhaofan Zou", "Zhaohong Liu", "Zhehao Shen", "Zhenglin Du", "Zhengyang Li", "Zhenni Huang", "Zhenwei Shao", "Zhilong Song", "Zhiyong Feng", "Zhiyu Wang", "Zhou Yu", "Ziang Li", "Zihan Zhai", "Zijian Zhang", "Ziyang Peng", "Ziyun Xiao", "Zongshu Li"], "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook", "comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''", "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u56de\u987e\u4e86 MARS2 2025 \u6311\u6218\u8d5b\uff0c\u8be5\u6311\u6218\u8d5b\u4e13\u6ce8\u4e8e\u591a\u6a21\u6001\u63a8\u7406\uff0c\u65e8\u5728\u901a\u8fc7\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u6574\u5408\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u4ee5\u8ddf\u4e0a\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u4eca\u5e74\u7684\u6bd4\u8d5b\u4fa7\u91cd\u4e8e\u73b0\u5b9e\u4e16\u754c\u548c\u4e13\u4e1a\u573a\u666f\uff0c\u4ee5\u6269\u5c55\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u53d1\u5e03\u4e86 Lens \u548c AdsQA \u4e24\u4e2a\u6570\u636e\u96c6\u3002\u6bd4\u8d5b\u8bc4\u4f30\u4e86 40 \u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u8bbe\u7acb\u4e86\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\uff1a\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\uff08VG-RS\uff09\u3001\u5177\u6709\u7a7a\u95f4\u610f\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA-SA\uff09\u4ee5\u53ca\u521b\u610f\u5e7f\u544a\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\uff08VR-Ads\uff09\u3002\u6700\u7ec8\uff0c\u5438\u5f15\u4e86 76 \u652f\u56e2\u961f\u6ce8\u518c\uff0c\u63d0\u4ea4\u4e86 40 \u591a\u4e2a\u6709\u6548\u53c2\u8d5b\u4f5c\u54c1\u3002\u6240\u6709\u6570\u636e\u3001\u4ee3\u7801\u548c\u6392\u540d\u5747\u516c\u5f00\u63d0\u4f9b\u3002", "motivation": " MARS2 \u6311\u6218\u8d5b\u65e8\u5728\u6c47\u96c6\u4e0d\u540c\u7684\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u6765\u8ddf\u4e0a\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u5feb\u901f\u53d1\u5c55\u7684\u73b0\u72b6\u3002\u4eca\u5e74\u7684\u6bd4\u8d5b\u7279\u522b\u5173\u6ce8\u73b0\u5b9e\u4e16\u754c\u548c\u4e13\u4e1a\u573a\u666f\uff0c\u4ee5\u62d3\u5bbd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": " MARS2 \u6311\u6218\u8d5b\u53d1\u5e03\u4e86 Lens \u548c AdsQA \u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5206\u522b\u7528\u4e8e\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u901a\u7528\u63a8\u7406\u548c\u5e7f\u544a\u89c6\u9891\u4e2d\u7684\u4e13\u4e1a\u63a8\u7406\u3002\u5bf9 40 \u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec\u901a\u7528 MLLM \u548c\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u8bbe\u7acb\u4e86\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\uff1a\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\uff08VG-RS\uff09\u3001\u5177\u6709\u7a7a\u95f4\u610f\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA-SA\uff09\u548c\u521b\u610f\u5e7f\u544a\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\uff08VR-Ads\uff09\u3002", "result": " \u6bd4\u8d5b\u5438\u5f15\u4e86\u6765\u81ea\u77e5\u540d\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684 76 \u652f\u56e2\u961f\u6ce8\u518c\uff0c\u5e76\u6536\u5230\u4e86 40 \u591a\u4e2a\u6709\u6548\u53c2\u8d5b\u4f5c\u54c1\uff08\u8d85\u8fc7 1200 \u4e2a\uff09\u3002", "conclusion": " MARS2 \u6311\u6218\u8d5b\u6210\u529f\u5730\u6c47\u96c6\u4e86\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6392\u540d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ddf\u8e2a\u548c\u63a8\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u7684\u5e73\u53f0\u3002\u5176\u6210\u679c\u548c\u8d44\u6e90\uff08\u5305\u62ec 40 \u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u548c 15 \u4e2a\u53c2\u4e0e\u8005\u7684\u65b9\u6cd5\uff09\u53ef\u5728 MARS2 \u5de5\u574a\u7f51\u7ad9\u548c GitHub \u4e0a\u516c\u5f00\u83b7\u53d6\uff0c\u5e76\u4f1a\u6301\u7eed\u66f4\u65b0\u3002"}}
{"id": "2509.14149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14149", "abs": "https://arxiv.org/abs/2509.14149", "authors": ["Haotian Li", "Jianbo Jiao"], "title": "An Exploratory Study on Abstract Images and Visual Representations Learned from Them", "comment": "Accepted to BMVC 2025", "summary": "Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.", "AI": {"tldr": "\u62bd\u8c61\u56fe\u50cf\u5305\u542b\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u5982\u4f20\u7edf\u5149\u6805\u56fe\u50cf\uff0c\u672c\u7814\u7a76\u63d0\u51faHAID\u6570\u636e\u96c6\u6765\u63a2\u7a76\u62bd\u8c61\u56fe\u50cf\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u4e0e\u5149\u6805\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u63a2\u7a76\u62bd\u8c61\u56fe\u50cf\u5728\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u4f20\u9012\u548c\u89c6\u89c9\u4efb\u52a1\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u9020\u6210\u5176\u6027\u80fd\u5dee\u8ddd\u7684\u539f\u56e0\u3002", "method": "\u63d0\u51faHAID\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ece\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u751f\u6210\u7684\u591a\u5c42\u7ea7\u62bd\u8c61\u56fe\u50cf\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5728HAID\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5149\u6805\u56fe\u50cf\u548c\u62bd\u8c61\u56fe\u50cf\u8868\u793a\u5728\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u62bd\u8c61\u56fe\u50cf\u4f5c\u4e3a\u4f20\u9012\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u548c\u4fc3\u8fdb\u89c6\u89c9\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.14151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14151", "abs": "https://arxiv.org/abs/2509.14151", "authors": ["Rongyu Zhang", "Jiaming Liu", "Xiaoqi Li", "Xiaowei Chi", "Dan Wang", "Li Du", "Yuan Du", "Shanghang Zhang"], "title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection", "comment": "Accepted by IEEE TCSVT", "summary": "Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.", "AI": {"tldr": "BEVUDA++\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u6df1\u5ea6\u6559\u5e08\uff08RDT\uff09\u548c\u51e0\u4f55\u4e00\u81f4\u5b66\u751f\uff08GCS\uff09\u6a21\u578b\u6765\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76BEV\u611f\u77e5\u4e2d\u7684\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709BEV\u611f\u77e5\u65b9\u6cd5\u5728\u8de8\u57df\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5373\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u6559\u5e08-\u5b66\u751f\u6846\u67b6BEVUDA++\uff0c\u5305\u542b\u53ef\u9760\u6df1\u5ea6\u6559\u5e08\uff08RDT\uff09\u548c\u51e0\u4f55\u4e00\u81f4\u5b66\u751f\uff08GCS\uff09\u6a21\u578b\u3002RDT\u901a\u8fc7\u878d\u5408\u76ee\u6807LiDAR\u548c\u53ef\u9760\u7684\u6df1\u5ea6\u9884\u6d4b\u6765\u751f\u6210\u6df1\u5ea6\u611f\u77e5\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u76ee\u6807\u57df\u7684Voxel\u548cBEV\u7279\u5f81\u63d0\u53d6\u3002GCS\u5c06\u591a\u7a7a\u95f4\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u7684\u51e0\u4f55\u5d4c\u5165\u7a7a\u95f4\uff0c\u4ee5\u7f29\u5c0f\u57df\u95f4\u6570\u636e\u5206\u5e03\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08UEMA\uff09\u6765\u51cf\u5c11\u57df\u8f6c\u79fb\u5f15\u8d77\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u56db\u4e2a\u8de8\u57df\u573a\u666f\u7684\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0cBEVUDA++\u5728BEV 3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5728Day-Night\u9002\u5e94\u6027\u4efb\u52a1\u4e0a\uff0cNDS\u63d0\u5347\u4e8612.9%\uff0cmAP\u63d0\u5347\u4e869.5%\u3002", "conclusion": "BEVUDA++\u6709\u6548\u89e3\u51b3\u4e86BEV\u611f\u77e5\u4e2d\u7684\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u3001\u6df1\u5ea6\u611f\u77e5\u4fe1\u606f\u878d\u5408\u548c\u7edf\u4e00\u51e0\u4f55\u5d4c\u5165\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df3D\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14227", "categories": ["cs.CV", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14227", "abs": "https://arxiv.org/abs/2509.14227", "authors": ["Nisarg A. Shah", "Amir Ziai", "Chaitanya Ekanadham", "Vishal M. Patel"], "title": "Cin\u00e9aste: A Fine-grained Contextual Movie Question Answering Benchmark", "comment": "11 pages, 5 figures, 5 tables", "summary": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.", "AI": {"tldr": "$\\\text{Cin\textasciicircum}aste$ \u662f\u4e00\u4e2a\u5305\u542b 1805 \u4e2a\u7535\u5f71\u573a\u666f\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u7bc7\u7535\u5f71\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u5728\u8bc4\u4f30\u6a21\u578b\u5bf9\u957f\u7bc7\u53d9\u4e8b\u5185\u5bb9\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c$\\\text{Cin\textasciicircum}aste$ \u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "$\\\text{Cin\textasciicircum}aste$ \u6570\u636e\u96c6\u5305\u542b 200 \u90e8\u7535\u5f71\u7684 1805 \u4e2a\u573a\u666f\uff0c\u751f\u6210\u4e86 3119 \u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u5229\u7528 GPT-4o \u7ed3\u5408\u591a\u79cd\u4fe1\u606f\u6e90\u751f\u6210\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\u6d41\u7a0b\uff08\u4e0a\u4e0b\u6587\u72ec\u7acb\u6027\u8fc7\u6ee4\u548c\u4e0a\u4e0b\u6587\u771f\u5b9e\u6027\u8fc7\u6ee4\uff09\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\u3002", "result": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08MLLMs\uff09\u5728 $\\\text{Cin\textasciicircum}aste$ \u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u4ec5\u4e3a 63.15%\uff0c\u8868\u660e\u957f\u65f6\u5e8f\u63a8\u7406\u662f\u5173\u952e\u74f6\u9888\u3002", "conclusion": "$\\\text{Cin\textasciicircum}aste$ \u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d MLLMs \u5728\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u957f\u7bc7\u7535\u5f71\u7406\u89e3\u65b9\u9762\u9762\u4e34\u7684\u91cd\u5927\u6311\u6218\uff0c\u9884\u793a\u7740\u76f8\u5173\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2509.14232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14232", "abs": "https://arxiv.org/abs/2509.14232", "authors": ["Zhaokai Wang", "Penghao Yin", "Xiangyu Zhao", "Changyao Tian", "Yu Qiao", "Wenhai Wang", "Jifeng Dai", "Gen Luo"], "title": "GenExam: A Multidisciplinary Text-to-Image Exam", "comment": null, "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.", "AI": {"tldr": "GenExam \u662f\u7b2c\u4e00\u4e2a\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u8003\u8bd5\u57fa\u51c6\uff0c\u5305\u542b 1,000 \u4e2a\u6837\u672c\uff0c\u8de8\u8d8a 10 \u4e2a\u79d1\u76ee\uff0c\u5e76\u4f7f\u7528\u56db\u7ea7\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u7684\u8003\u8bd5\u98ce\u683c\u63d0\u793a\u3002\u5b83\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5206\u6570\u70b9\uff0c\u7528\u4e8e\u7cbe\u786e\u8bc4\u4f30\u8bed\u4e49\u6b63\u786e\u6027\u548c\u89c6\u89c9\u5408\u7406\u6027\u3002\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u5f97\u5206\u4e0d\u5230 15%\uff0c\u7a81\u663e\u4e86\u5176\u6311\u6218\u6027\uff0c\u5e76\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u8003\u8bd5\u98ce\u683c\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u800c\u5f53\u524d\u7684\u751f\u6210\u57fa\u51c6\u5219\u4fa7\u91cd\u4e8e\u4e16\u754c\u77e5\u8bc6\u548c\u89c6\u89c9\u6982\u5ff5\u7684\u9610\u8ff0\uff0c\u5ffd\u7565\u4e86\u5bf9\u4e25\u683c\u7ed8\u56fe\u8003\u8bd5\u7684\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u3001\u7c7b\u4f3c\u8003\u8bd5\u7684\u57fa\u51c6\u3002", "method": "\u5f15\u5165 GenExam\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u8003\u8bd5\u57fa\u51c6\uff0c\u5305\u542b 1,000 \u4e2a\u6837\u672c\uff0c\u8de8\u8d8a 10 \u4e2a\u79d1\u76ee\uff0c\u5e76\u4f7f\u7528\u56db\u7ea7\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u7684\u8003\u8bd5\u98ce\u683c\u63d0\u793a\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u914d\u6709\u771f\u5b9e\u56fe\u50cf\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u5206\u70b9\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u8bed\u4e49\u6b63\u786e\u6027\u548c\u89c6\u89c9\u5408\u7406\u6027\u7684\u7cbe\u786e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u50cf GPT-Image-1 \u548c Gemini-2.5-Flash-Image \u8fd9\u6837\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u5f97\u5206\u4e5f\u4f4e\u4e8e 15%\uff0c\u800c\u5927\u591a\u6570\u6a21\u578b\u5f97\u5206\u63a5\u8fd1 0%\uff0c\u8fd9\u8868\u660e\u8be5\u57fa\u51c6\u5177\u6709\u5de8\u5927\u7684\u6311\u6218\u6027\u3002", "conclusion": "GenExam \u901a\u8fc7\u5c06\u56fe\u50cf\u751f\u6210\u89c6\u4e3a\u4e00\u573a\u8003\u8bd5\uff0c\u4e3a\u6a21\u578b\u6574\u5408\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u751f\u6210\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u5e76\u4e3a\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u9053\u8def\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
