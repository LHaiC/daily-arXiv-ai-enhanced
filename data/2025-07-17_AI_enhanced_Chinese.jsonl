{"id": "2507.11831", "categories": ["cs.HC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.11831", "abs": "https://arxiv.org/abs/2507.11831", "authors": ["Fernando Koch", "Jessica Nahulan", "Jeremy Fox", "Martin Keen"], "title": "Generative Intelligence Systems in the Flow of Group Emotions", "comment": "8 pages, 10 figures", "summary": "Emotional cues frequently arise and shape group dynamics in interactive\nsettings where multiple humans and artificial agents communicate through shared\ndigital channels. While artificial agents lack intrinsic emotional states, they\ncan simulate affective behavior using synthetic modalities such as text or\nspeech. This work introduces a model for orchestrating emotion contagion,\nenabling agents to detect emotional signals, infer group mood patterns, and\ngenerate targeted emotional responses. The system captures human emotional\nexchanges and uses this insight to produce adaptive, generative responses that\ninfluence group affect in real time. The model supports applications in\ncollaborative, educational, and social environments by shifting affective\ncomputing from individual-level reactions to coordinated, group-level emotion\nmodulation. We present the system architecture and provide experimental results\nthat illustrate its effectiveness in sensing and steering group mood dynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7f16\u6392\u60c5\u7eea\u4f20\u67d3\u7684\u6a21\u578b\uff0c\u4f7f\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u68c0\u6d4b\u3001\u63a8\u65ad\u548c\u54cd\u5e94\u7fa4\u4f53\u60c5\u7eea\uff0c\u4ee5\u8c03\u8282\u534f\u4f5c\u3001\u6559\u80b2\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u7fa4\u4f53\u60c5\u7eea\u3002", "motivation": "\u4e3a\u4e86\u5728\u5305\u542b\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u4e2d\uff0c\u5229\u7528\u60c5\u611f\u7ebf\u7d22\u6765\u5851\u9020\u7fa4\u4f53\u52a8\u6001\uff0c\u5e76\u4f7f\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u6a21\u62df\u60c5\u611f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u60c5\u7eea\u4fe1\u53f7\u3001\u63a8\u65ad\u7fa4\u4f53\u60c5\u7eea\u6a21\u5f0f\u5e76\u751f\u6210\u76ee\u6807\u60c5\u7eea\u53cd\u5e94\u7684\u60c5\u7eea\u4f20\u67d3\u7f16\u6392\u6a21\u578b\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u6355\u6349\u4eba\u7c7b\u7684\u60c5\u7eea\u4ea4\u6d41\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u89c1\u89e3\u6765\u751f\u6210\u9002\u5e94\u6027\u3001\u751f\u6210\u6027\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u65f6\u5f71\u54cd\u7fa4\u4f53\u60c5\u7eea\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u611f\u77e5\u548c\u5f15\u5bfc\u7fa4\u4f53\u60c5\u7eea\u52a8\u6001\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u60c5\u7eea\u4f20\u67d3\u7684\u7f16\u6392\uff0c\u7528\u4e8e\u5728\u534f\u4f5c\u3001\u6559\u80b2\u548c\u793e\u4f1a\u73af\u5883\u7b49\u5e94\u7528\u4e2d\uff0c\u901a\u8fc7\u5b9e\u65f6\u751f\u6210\u9002\u5e94\u6027\u54cd\u5e94\u6765\u8c03\u8282\u7fa4\u4f53\u60c5\u7eea\uff0c\u5c06\u60c5\u611f\u8ba1\u7b97\u4ece\u4e2a\u4f53\u5c42\u9762\u53cd\u5e94\u63d0\u5347\u5230\u7fa4\u4f53\u5c42\u9762\u3002"}}
{"id": "2507.12373", "categories": ["cs.ET", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12373", "abs": "https://arxiv.org/abs/2507.12373", "authors": ["Dariush Pourkeramati", "Gareth Wadge", "Rachel Hassall", "Charlotte Mitchell", "Anish Khadka", "Shiwang Jaiswal", "Andrew Duncan", "Rossella Arcucci"], "title": "Emerging Paradigms in the Energy Sector: Forecasting and System Control Optimisation", "comment": null, "summary": "The energy sector is experiencing rapid transformation due to increasing\nrenewable energy integration, decentralisation of power systems, and a\nheightened focus on efficiency and sustainability. With energy demand becoming\nincreasingly dynamic and generation sources more variable, advanced forecasting\nand optimisation strategies are crucial for maintaining grid stability,\ncost-effectiveness, and environmental sustainability. This paper explores\nemerging paradigms in energy forecasting and management, emphasizing four\ncritical domains: Energy Demand Forecasting integrated with Weather Data,\nBuilding Energy Optimisation, Heat Network Optimisation, and Energy Management\nSystem (EMS) Optimisation within a System of Systems (SoS) framework.\nLeveraging machine learning techniques and Model Predictive Control (MPC), the\nstudy demonstrates substantial enhancements in energy efficiency across scales\n-- from individual buildings to complex interconnected energy networks.\nWeather-informed demand forecasting significantly improves grid resilience and\nresource allocation strategies. Smart building optimisation integrates\npredictive analytics to substantially reduce energy consumption without\ncompromising occupant comfort. Optimising CHP-based heat networks achieves cost\nand carbon savings while adhering to operational and asset constraints. At the\nsystems level, sophisticated EMS optimisation ensures coordinated control of\ndistributed resources, storage solutions, and demand-side flexibility. Through\nreal-world case studies we highlight the potential of AI-driven automation and\nintegrated control solutions in facilitating a resilient, efficient, and\nsustainable energy future.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u901a\u8fc7\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6765\u6539\u8fdb\u80fd\u6e90\u9700\u6c42\u9884\u6d4b\u3001\u5efa\u7b51\u80fd\u6e90\u4f18\u5316\u3001\u70ed\u529b\u7ba1\u7f51\u4f18\u5316\u4ee5\u53ca\u5728\u7cfb\u7edf\u4e4b\u7cfb\u7edf\uff08SoS\uff09\u6846\u67b6\u5185\u8fdb\u884c\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff08EMS\uff09\u4f18\u5316\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u63d0\u9ad8\u80fd\u6e90\u7cfb\u7edf\u7684\u6548\u7387\u3001\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u3001\u7535\u529b\u7cfb\u7edf\u5206\u6563\u5316\u4ee5\u53ca\u5bf9\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u5173\u6ce8\u65e5\u76ca\u589e\u52a0\uff0c\u80fd\u6e90\u884c\u4e1a\u6b63\u5728\u7ecf\u5386\u5feb\u901f\u8f6c\u578b\u3002\u5bf9\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u548c\u4f18\u5316\u7b56\u7565\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u8fd9\u9879\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6765\u4f18\u5316\u80fd\u6e90\u7ba1\u7406\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u4ece\u5355\u4e2a\u5efa\u7b51\u5230\u590d\u6742\u7684\u4e92\u8054\u80fd\u6e90\u7f51\u7edc\u7684\u5404\u4e2a\u5c42\u9762\u4e0a\uff0c\u80fd\u6e90\u6548\u7387\u90fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002\u5929\u6c14\u4fe1\u606f\u9a71\u52a8\u7684\u9700\u6c42\u9884\u6d4b\u63d0\u9ad8\u4e86\u7535\u7f51\u7684\u97e7\u6027\u548c\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002\u667a\u80fd\u5efa\u7b51\u4f18\u5316\u901a\u8fc7\u96c6\u6210\u9884\u6d4b\u5206\u6790\uff0c\u5728\u4e0d\u5f71\u54cd\u5c45\u4f4f\u8005\u8212\u9002\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u3002\u4f18\u5316\u70ed\u7535\u8054\u4ea7\u70ed\u529b\u7ba1\u7f51\u53ef\u5728\u9075\u5b88\u8fd0\u8425\u548c\u8d44\u4ea7\u9650\u5236\u7684\u540c\u65f6\uff0c\u8282\u7701\u6210\u672c\u548c\u51cf\u5c11\u78b3\u6392\u653e\u3002\u5728\u7cfb\u7edf\u5c42\u9762\uff0c\u5148\u8fdb\u7684\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff08EMS\uff09\u4f18\u5316\u786e\u4fdd\u4e86\u5206\u5e03\u5f0f\u8d44\u6e90\u3001\u5b58\u50a8\u89e3\u51b3\u65b9\u6848\u548c\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\u7684\u534f\u8c03\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u548c\u96c6\u6210\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u5728\u4fc3\u8fdb\u6709\u5f39\u6027\u7684\u3001\u9ad8\u6548\u7684\u548c\u53ef\u6301\u7eed\u7684\u80fd\u6e90\u672a\u6765\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11545", "categories": ["cs.DC", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.11545", "abs": "https://arxiv.org/abs/2507.11545", "authors": ["Rhea Pritham Marpu", "Kevin J McNamara", "Preeti Gupta"], "title": "The AI Shadow War: SaaS vs. Edge Computing Architectures", "comment": null, "summary": "The very DNA of AI architecture presents conflicting paths: centralized\ncloud-based models (Software-as-a-Service) versus decentralized edge AI (local\nprocessing on consumer devices). This paper analyzes the competitive\nbattleground across computational capability, energy efficiency, and data\nprivacy. Recent breakthroughs show edge AI challenging cloud systems on\nperformance, leveraging innovations like test-time training and\nmixture-of-experts architectures. Crucially, edge AI boasts a 10,000x\nefficiency advantage: modern ARM processors consume merely 100 microwatts\nforinference versus 1 watt for equivalent cloud processing. Beyond efficiency,\nedge AI secures data sovereignty by keeping processing local, dismantling\nsingle points of failure in centralized architectures. This democratizes access\nthroughaffordable hardware, enables offline functionality, and reduces\nenvironmental impact by eliminating data transmission costs. The edge AI market\nprojects explosive growth from $9 billion in 2025 to $49.6 billion by 2030\n(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical\napplications including personalized education, healthcare monitoring,\nautonomous transport, and smart infrastructure rely on edge AI's ultra-low\nlatency (5-10ms versus 100-500ms for cloud). The convergence of architectural\ninnovation with fundamental physics confirms edge AI's distributed approach\naligns with efficient information processing, signaling the inevitable\nemergence of hybrid edge-cloud ecosystems.", "AI": {"tldr": "\u8fb9\u7f18AI\u51ed\u501f\u66f4\u9ad8\u7684\u80fd\u6548\u3001\u66f4\u597d\u7684\u6570\u636e\u9690\u79c1\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u6b63\u5728\u6311\u6218\u4f20\u7edf\u7684\u4e91AI\u6a21\u578b\uff0c\u5e76\u6709\u671b\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "AI\u67b6\u6784\u5728\u4e2d\u5fc3\u5316\u4e91\u6a21\u578b\u548c\u53bb\u4e2d\u5fc3\u5316\u8fb9\u7f18AI\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81", "method": "\u672c\u6587\u5206\u6790\u4e86AI\u67b6\u6784\u5728\u8ba1\u7b97\u80fd\u529b\u3001\u80fd\u6548\u548c\u6570\u636e\u9690\u79c1\u65b9\u9762\u7684\u7ade\u4e89", "result": "\u8fb9\u7f18AI\u5728\u80fd\u6548\u3001\u6570\u636e\u9690\u79c1\u548c\u4f4e\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4e91AI\uff0c\u9884\u8ba1\u5e02\u573a\u5c06\u5927\u5e45\u589e\u957f\uff0c\u5e76\u5f62\u6210\u6df7\u5408\u8fb9\u7f18-\u4e91\u751f\u6001\u7cfb\u7edf", "conclusion": "\u8fb9\u7f18AI\u901a\u8fc7\u5229\u7528\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u548c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7b49\u521b\u65b0\uff0c\u5728\u6027\u80fd\u4e0a\u6311\u6218\u4e91\u7cfb\u7edf\uff0c\u5e76\u62e5\u670910,000\u500d\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u540c\u65f6\u901a\u8fc7\u672c\u5730\u5904\u7406\u786e\u4fdd\u6570\u636e\u4e3b\u6743\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u548c\u589e\u957f\u3002"}}
{"id": "2507.12384", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12384", "abs": "https://arxiv.org/abs/2507.12384", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "comment": null, "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528 MoS2 \u95ea\u5b58\u6a21\u62df CAM \u7684\u8f6f\u6811\u6a21\u578b\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u8f6f\u8fb9\u754c\uff0c\u80fd\u6709\u6548\u89e3\u51b3 AI \u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u5e76\u80fd\u5728\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\uff08\u5c24\u5176\u662f\u6811\u6a21\u578b\uff09\u5728\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u52a0\u901f\u65b9\u6cd5\u5728\u5904\u7406\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528 MoS2 \u95ea\u5b58\u57fa\u6a21\u62df CAM \u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u8be5 CAM \u5177\u6709\u56fa\u6709\u7684\u8f6f\u8fb9\u754c\uff0c\u53ef\u5b9e\u73b0\u8f6f\u6811\u6a21\u578b\u7684\u6709\u6548\u63a8\u7406\u3002", "result": "\u5728 WDBC \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 96% \u7684\u51c6\u786e\u7387\uff0c\u5728 MNIST \u6570\u636e\u96c6\u4e0a\uff0c\u5728 10% \u7684\u5668\u4ef6\u9608\u503c\u53d8\u5316\u4e0b\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d 0.6%\uff0c\u800c\u4f20\u7edf\u51b3\u7b56\u6811\u4e0b\u964d\u4e86 45.3%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u5177\u6709\u56fa\u6709\u8f6f\u8fb9\u754c\u7684 MoS2 \u95ea\u5b58\u6a21\u62df CAM\uff0c\u5b9e\u73b0\u4e86\u8f6f\u6811\u6a21\u578b\u7684\u6709\u6548\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u9762\u5bf9\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u4fe1\u5ea6\u548c\u6548\u7387\u7684\u4e13\u7528\u786c\u4ef6\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.11681", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.11681", "abs": "https://arxiv.org/abs/2507.11681", "authors": ["Sotiris Kanellopoulos", "Christos Pergaminelis", "Maria Kokkou", "Euripides Markou", "Aris Pagourtzis"], "title": "Finite Pinwheel Scheduling: the k-Visits Problem", "comment": null, "summary": "Pinwheel Scheduling is a fundamental scheduling problem, in which each task\n$i$ is associated with a positive integer $d_i$, and the objective is to\nschedule one task per time slot, ensuring each task perpetually appears at\nleast once in every $d_i$ time slots. Although conjectured to be\nPSPACE-complete, it remains open whether Pinwheel Scheduling is NP-hard (unless\na compact input encoding is used) or even contained in NP.\n  We introduce k-Visits, a finite version of Pinwheel Scheduling, where given n\ndeadlines, the goal is to schedule each task exactly k times. While we observe\nthat the 1-Visit problem is trivial, we prove that 2-Visits is strongly\nNP-complete through a surprising reduction from Numerical 3-Dimensional\nMatching (N3DM). As intermediate steps in the reduction, we define NP-complete\nvariants of N3DM which may be of independent interest. We further extend our\nstrong NP-hardness result to a generalization of k-Visits $k\\geq 2$ in which\nthe deadline of each task may vary throughout the schedule, as well as to a\nsimilar generalization of Pinwheel Scheduling, thus making progress towards\nsettling the complexity of Pinwheel Scheduling.\n  Additionally, we prove that 2-Visits can be solved in linear time if all\ndeadlines are distinct, rendering it one of the rare natural problems which\nexhibit the interesting dichotomy of being in P if their input is a set and\nNP-complete if the input is a multiset. We achieve this through a Turing\nreduction from 2-Visits to a variation of N3DM, which we call Position\nMatching. Based on this reduction, we also show an FPT algorithm for 2-Visits\nparameterized by a value related to how close the input deadlines are to each\nother, as well as a linear-time algorithm for instances with up to two distinct\ndeadlines.", "AI": {"tldr": "Pinwheel Scheduling \u662f\u4e00\u4e2a\u57fa\u672c\u8c03\u5ea6\u95ee\u9898\uff0c\u4f5c\u8005\u5c06\u5176\u63a8\u5e7f\u5230 k-Visits\uff0c\u5e76\u8bc1\u660e k=2 \u7684\u60c5\u51b5\u662f NP-complete \u7684\uff0c\u4f46\u5982\u679c\u622a\u6b62\u65e5\u671f\u4e0d\u540c\u5219\u53ef\u4ee5\u89e3\u51b3\u3002", "motivation": "Pinwheel Scheduling \u662f\u4e00\u4e2a\u57fa\u672c\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u4efb\u52a1 i \u90fd\u4e0e\u4e00\u4e2a\u6b63\u6574\u6570 d_i \u76f8\u5173\u8054\uff0c\u76ee\u6807\u662f\u6bcf 1 \u4e2a\u65f6\u95f4\u69fd\u5b89\u6392\u4e00\u4e2a\u4efb\u52a1\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4efb\u52a1\u5728\u6bcf d_i \u4e2a\u65f6\u95f4\u69fd\u4e2d\u81f3\u5c11\u51fa\u73b0\u4e00\u6b21\u3002\u867d\u7136\u63a8\u6d4b\u5b83\u662f PSPACE-complete\uff0c\u4f46 Pinwheel Scheduling \u662f\u5426\u662f NP-hard\uff08\u9664\u975e\u4f7f\u7528\u7d27\u51d1\u8f93\u5165\u7f16\u7801\uff09\u751a\u81f3\u662f\u5426\u5305\u542b\u5728 NP \u4e2d\u4ecd\u7136\u662f\u516c\u5f00\u7684\u3002\u5f15\u5165 k-Visits\uff0cPinwheel Scheduling \u7684\u6709\u9650\u7248\u672c\uff0c\u5176\u4e2d\u7ed9\u5b9a n \u4e2a\u622a\u6b62\u65e5\u671f\uff0c\u76ee\u6807\u662f\u51c6\u786e\u5730\u8c03\u5ea6\u6bcf\u4e2a\u4efb\u52a1 k \u6b21\u3002", "method": "\u901a\u8fc7\u4ece\u6570\u503c\u4e09\u7ef4\u5339\u914d (N3DM) \u8fdb\u884c\u7684\u60ca\u4eba\u5f52\u7ea6\uff0c\u8bc1\u660e\u4e86 2-Visits \u662f\u5f3a NP-complete\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06 2-Visits \u5f52\u7ea6\u5230\u4f4d\u7f6e\u5339\u914d\uff08N3DM \u7684\u4e00\u79cd\u53d8\u4f53\uff09\u6765\u5c55\u793a\u4e00\u79cd\u9488\u5bf9 2-Visits \u7684 FPT \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7531\u4e0e\u8f93\u5165\u622a\u6b62\u65e5\u671f\u5f7c\u6b64\u63a5\u8fd1\u7684\u7a0b\u5ea6\u76f8\u5173\u7684\u503c\u53c2\u6570\u5316\uff0c\u4ee5\u53ca\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u6700\u591a\u4e24\u4e2a\u4e0d\u540c\u622a\u6b62\u65e5\u671f\u7684\u5b9e\u4f8b\u7684\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "result": "2-Visits \u662f\u5f3a NP-complete\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230 k-Visits (k>=2) \u7684\u53d8\u4f53\uff0c\u5176\u4e2d\u6bcf\u4e2a\u4efb\u52a1\u7684\u622a\u6b62\u65e5\u671f\u5728\u8c03\u5ea6\u4e2d\u53ef\u80fd\u4e0d\u540c\uff0c\u4ee5\u53ca Pinwheel Scheduling \u7684\u7c7b\u4f3c\u53d8\u4f53\u3002", "conclusion": "2-Visits \u95ee\u9898\u5f53\u6240\u6709\u622a\u6b62\u65e5\u671f\u90fd\u4e0d\u540c\u65f6\u53ef\u4ee5\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u8fd9\u4f7f\u5f97\u5b83\u6210\u4e3a\u5c11\u6570\u51e0\u4e2a\u5177\u6709\u6709\u8da3\u4e8c\u5206\u6cd5\u7684\u81ea\u7136\u95ee\u9898\u4e4b\u4e00\uff1a\u5f53\u8f93\u5165\u662f\u96c6\u5408\u65f6\u5c5e\u4e8e P\uff0c\u5f53\u8f93\u5165\u662f\u591a\u91cd\u96c6\u65f6\u662f NP-complete\u3002\u901a\u8fc7\u5c06 2-Visits \u5f52\u7ea6\u5230\u6211\u4eec\u79f0\u4e3a\u4f4d\u7f6e\u5339\u914d\u7684 N3DM \u7684\u53d8\u4f53\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002\u57fa\u4e8e\u6b64\u5f52\u7ea6\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u4e00\u79cd\u9488\u5bf9 2-Visits \u7684 FPT \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7531\u4e0e\u8f93\u5165\u622a\u6b62\u65e5\u671f\u5f7c\u6b64\u63a5\u8fd1\u7684\u7a0b\u5ea6\u76f8\u5173\u7684\u503c\u53c2\u6570\u5316\uff0c\u4ee5\u53ca\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u6700\u591a\u4e24\u4e2a\u4e0d\u540c\u622a\u6b62\u65e5\u671f\u7684\u5b9e\u4f8b\u7684\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002"}}
{"id": "2507.11644", "categories": ["cs.LO", "03B45", "F.4.1"], "pdf": "https://arxiv.org/pdf/2507.11644", "abs": "https://arxiv.org/abs/2507.11644", "authors": ["Olivier Gasquet"], "title": "Comment on Decidability of Quasi-Dense Modal Logics by Lyon and Ostropolski-Nalewaja", "comment": "Comment on arXiv:2405.10094", "summary": "In \\cite{Lyon24} the question of the decidability of quasi-dense modal logics\nis answered, and an upper bound in EXPSPACE is given. Unfortunately, authors'\nintricate proof contains a major flaw that cannot be fixed, leaving the\nquestion wide open. Once identified, this error roughly amounts to assuming\nthat the union of two consistent sets is consistent, which is of course wrong.", "AI": {"tldr": "A previous proof about the decidability of quasi-dense modal logics was found to be flawed, leaving the question open.", "motivation": "To address the flaw in a previous proof concerning the decidability of quasi-dense modal logics.", "method": "The paper identifies a flaw in a previous proof regarding the union of consistent sets, which invalidates the original conclusion.", "result": "The previous proof's conclusion that an upper bound in EXPSPACE exists for the decidability of quasi-dense modal logics is invalidated.", "conclusion": "The decidability of quasi-dense modal logics remains an open question."}}
{"id": "2507.11606", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2507.11606", "abs": "https://arxiv.org/abs/2507.11606", "authors": ["Johanne Bratland Tjernshaugen", "Martin Tang Bruland", "Jacob Linder"], "title": "Designing lattice spin models and magnon gaps with supercurrents", "comment": "6+9 pages, 3+5 figures", "summary": "Controlling magnetic interactions at the level of individual spins is\nrelevant for a variety of quantum applications, such as qubits, memory and\nsensor functionality. Finding ways to exert electrical control over spin\ninteractions, with minimal energy dissipation, is a key objective in this\nendeavour. We show here that spin lattices and magnon gaps can be controlled\nwith a supercurrent. Remarkably, a spin-polarized supercurrent makes the\ninteraction between magnetic adatoms placed on the surface of a superconductor\ndepend not only on their relative distance, but also on their absolute position\nin space. This property permits electric control over the interaction not only\nbetween two individual spins, but in fact over an entire spin lattice, allowing\nfor non-collinear ground states and a practical arena to study the properties\nof different spin Hamiltonians. Moreover, we show that a supercurrent controls\nthe magnon gap in antiferromagnetic and altermagnetic insulators. These results\nprovide an accessible way to realize electrically controlled spin switching and\nmagnon gaps without dissipative currents.", "AI": {"tldr": "\u901a\u8fc7\u8d85\u7535\u6d41\u5b9e\u73b0\u5bf9\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\u548c\u78c1\u5499\u7684\u7535\u63a7\u5236\uff0c\u65e0\u9700\u8017\u6563\u7535\u6d41\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cf\u5b50\u5e94\u7528\uff08\u5982\u91cf\u5b50\u6bd4\u7279\u3001\u5b58\u50a8\u548c\u4f20\u611f\u5668\u529f\u80fd\uff09\u4e2d\u5b9e\u73b0\u5bf9\u5355\u4e2a\u81ea\u65cb\u78c1\u76f8\u4e92\u4f5c\u7528\u7684\u7535\u63a7\u5236\uff0c\u5e76\u6700\u5c0f\u5316\u80fd\u91cf\u8017\u6563\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8ba1\u7b97\u548c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u81ea\u65cb\u6781\u5316\u8d85\u7535\u6d41\u5bf9\u8868\u9762\u78c1\u6027\u5438\u9644\u539f\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8d85\u7535\u6d41\u5bf9\u53cd\u94c1\u78c1\u548c\u4ea4\u53d8\u78c1\u7edd\u7f18\u4f53\u4e2d\u78c1\u5499\u7684\u63a7\u5236\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u4e86\u81ea\u65cb\u6781\u5316\u8d85\u7535\u6d41\u53ef\u4f7f\u78c1\u76f8\u4e92\u4f5c\u7528\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u78c1\u539f\u5b50\u7684\u76f8\u5bf9\u8ddd\u79bb\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u5176\u7a7a\u95f4\u7edd\u5bf9\u4f4d\u7f6e\u3002\u8fd9\u4f7f\u5f97\u5bf9\u6574\u4e2a\u81ea\u65cb\u6676\u683c\u7684\u7535\u63a7\u5236\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u975e\u5171\u7ebf\u57fa\u6001\u3002\u6b64\u5916\uff0c\u8d85\u7535\u6d41\u8fd8\u80fd\u63a7\u5236\u53cd\u94c1\u78c1\u548c\u4ea4\u53d8\u78c1\u7edd\u7f18\u4f53\u4e2d\u7684\u78c1\u5499\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u8d85\u7535\u6d41\u53ef\u4ee5\u63a7\u5236\u81ea\u65cb\u6676\u683c\u548c\u78c1\u5499\uff0c\u5e76\u5b9e\u73b0\u5bf9\u78c1\u76f8\u4e92\u4f5c\u7528\u7684\u7535\u63a7\u5236\uff0c\u800c\u65e0\u9700\u8017\u6563\u7535\u6d41\u3002"}}
{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7814\u7a76\u4e86AI\u5728\u751f\u6001\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86AI\u5982\u4f55\u5e2e\u52a9\u5efa\u7acb\u4eba\u4e0e\u81ea\u7136\u7684\u76f8\u4e92\u4f9d\u5b58\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86AI\u5728\u6570\u636e\u5206\u6790\u3001\u751f\u6001\u6062\u590d\u7b49\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408AI\u548c\u690d\u7269\u4fee\u590d\u7684\u8bbe\u8ba1\u601d\u8def\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u662f\u5426\u80fd\u591f\u4fc3\u8fdb\u4eba\u4e0e\u81ea\u7136\u5173\u7cfb\u7684\u8f6c\u53d8\uff0c\u4ece\u4eba\u7c7b\u4e3b\u5bfc\u8f6c\u5411\u76f8\u4e92\u4f9d\u5b58\uff0c\u5e76\u8003\u5bdfAI\u5728\u5176\u4e2d\u626e\u6f14\u7684\u5a92\u4ecb\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e86\u827a\u672f\u5bb6\u548c\u8bbe\u8ba1\u5e08\u5982\u4f55\u8fd0\u7528AI\u8fdb\u884c\u6570\u636e\u5206\u6790\u3001\u56fe\u50cf\u8bc6\u522b\u548c\u751f\u6001\u6062\u590d\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u690d\u7269\u4fee\u590d\u76f8\u7ed3\u5408\u7684\u8bbe\u8ba1\u8def\u5f84\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u4e0d\u4ec5\u6269\u5c55\u4e86\u521b\u610f\u65b9\u6cd5\uff0c\u8fd8\u91cd\u5851\u4e86\u751f\u6001\u8bbe\u8ba1\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u80fd\u591f\u94fe\u63a5\u79d1\u5b66\u89c1\u89e3\u3001\u827a\u672f\u5b9e\u8df5\u548c\u73af\u5883\u7ba1\u7406\uff0c\u4e3a\u672a\u6765\u53ef\u6301\u7eed\u7684\u3001\u6280\u672f\u652f\u6301\u7684\u751f\u6001\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u81ea\u7136\u5173\u7cfb\u7684\u8f6c\u53d8\u4e2d\uff0c\u4ece\u4eba\u7c7b\u4e3b\u5bfc\u8f6c\u5411\u76f8\u4e92\u4f9d\u5b58\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5c55\u793a\u4e86AI\u5728\u751f\u6001\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11726", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11726", "abs": "https://arxiv.org/abs/2507.11726", "authors": ["Ding Lin", "Jianhui Wang", "Tianqiao Zhao", "Meng Yue"], "title": "A Deep Reinforcement Learning Method for Multi-objective Transmission Switching", "comment": "5 pages", "summary": "Transmission switching is a well-established approach primarily applied to\nminimize operational costs through strategic network reconfiguration. However,\nexclusive focus on cost reduction can compromise system reliability. While\nmulti-objective transmission switching can balance cost savings with\nreliability improvements, feasible solutions become exceedingly difficult to\nobtain as system scale grows, due to the inherent nonlinearity and high\ncomputational demands involved. This paper proposes a deep reinforcement\nlearning (DRL) method for multi-objective transmission switching. The method\nincorporates a dueling-based actor-critic framework to evaluate the relative\nimpact of each line switching decision within the action space, which improves\ndecision quality and enhances both system reliability and cost efficiency.\nNumerical studies on the IEEE 118-bus system verify the effectiveness and\nefficiency of the proposed approach compared to two benchmark DRL algorithms.", "AI": {"tldr": "\u4f7f\u7528DRL\u65b9\u6cd5\u8fdb\u884c\u591a\u76ee\u6807\u8f93\u7535\u6f6e\u6d41\u5f00\u5173\uff0c\u4ee5\u5e73\u8861\u6210\u672c\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e73\u8861\u6210\u672c\u8282\u7ea6\u548c\u53ef\u9760\u6027\u63d0\u9ad8\uff0c\u4f46\u968f\u7740\u7cfb\u7edf\u89c4\u6a21\u7684\u589e\u957f\uff0c\u7531\u4e8e\u56fa\u6709\u7684\u975e\u7ebf\u6027\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\uff0c\u83b7\u5f97\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u6781\u5176\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u5bf9\u51b3\u7684Actor-Critic\u6846\u67b6\u6765\u8bc4\u4f30\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u6bcf\u4e2a\u7ebf\u8def\u5207\u6362\u51b3\u7b56\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "result": "\u6240\u63d0\u51fa\u7684DRL\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "conclusion": "DRL\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u8f93\u7535\u6f6e\u6d41\u5f00\u5173\u95ee\u9898\u4e0a\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u76f8\u6bd4\u4e8e\u57fa\u51c6DRL\u7b97\u6cd5\u3002"}}
{"id": "2507.11740", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.11740", "abs": "https://arxiv.org/abs/2507.11740", "authors": ["Cai Williams", "Chen Wang", "Alexander Ehm", "Dietrich R. T. Zahn", "Maria Saladina", "Carsten Deibel", "Roderick C. I. Mackenzie"], "title": "A Framework to Pinpoint Bottlenecks in Emerging Solar Cells and Disordered Devices via Differential Machine Learning", "comment": null, "summary": "A key challenge in the development of materials for the next generation of\nsolar cells, sensors and transistors is linking macroscopic device performance\nto underlying microscopic properties. For years, fabrication of devices has\nbeen faster than our ability to characterize them. This has led to a random\nwalk of material development, with new materials being proposed faster than our\nunderstanding. We present two neural network-based methods for extracting key\nmaterial parameters, including charge carrier mobility and trap state density,\nin optoelectronic devices such as solar cells. Our methods require solely\nmeasured light current--voltage curve and modest computational resources,\nmaking our approach applicable in even minimally equipped laboratories. Unlike\ntraditional machine learning models, our methods place the final material\nvalues in a non-Gaussian likelihood distribution, allowing confidence\nassessment of each predicted parameter. We demonstrate these techniques using\nfresh PM6:Y12 and degraded PM6:BTP-eC9 organic solar cells.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ec5\u7528\u5149\u7535\u6d41-\u7535\u538b\u66f2\u7ebf\u5373\u53ef\u8bc4\u4f30\u592a\u9633\u80fd\u7535\u6c60\u6750\u6599\u53c2\u6570\uff0c\u5e76\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u76ee\u524d\u7684\u6750\u6599\u5f00\u53d1\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u4e0e\u8868\u5f81\u80fd\u529b\u8131\u8282\uff0c\u5bfc\u81f4\u6750\u6599\u5f00\u53d1\u5982\u540c\u201c\u968f\u673a\u6e38\u8d70\u201d\uff0c\u65b0\u6750\u6599\u7684\u63d0\u51fa\u901f\u5ea6\u8d85\u8fc7\u4e86\u6211\u4eec\u5bf9\u5176\u7684\u7406\u89e3\u3002\u7136\u800c\uff0c\u5c06\u5b8f\u89c2\u5668\u4ef6\u6027\u80fd\u4e0e\u5fae\u89c2\u6027\u8d28\u8054\u7cfb\u8d77\u6765\u662f\u5f00\u53d1\u4e0b\u4e00\u4ee3\u592a\u9633\u80fd\u7535\u6c60\u3001\u4f20\u611f\u5668\u548c\u6676\u4f53\u7ba1\u6750\u6599\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ec5\u9700\u6d4b\u91cf\u5149\u7535\u6d41-\u7535\u538b\u66f2\u7ebf\u548c\u9002\u5ea6\u7684\u8ba1\u7b97\u8d44\u6e90\u5373\u53ef\u63d0\u53d6\u5173\u952e\u6750\u6599\u53c2\u6570\uff0c\u5982\u8f7d\u6d41\u5b50\u8fc1\u79fb\u7387\u548c\u9677\u9631\u6001\u5bc6\u5ea6\u3002", "result": "\u6f14\u793a\u4e86\u8be5\u6280\u672f\u5728PM6:Y12\u548cPM6:BTP-eC9\u6709\u673a\u592a\u9633\u80fd\u7535\u6c60\u4e0a\u7684\u5e94\u7528\uff0c\u8fd9\u4e9b\u7535\u6c60\u6216\u662f\u65b0\u5236\u7684\uff0c\u6216\u662f\u7ecf\u8fc7\u6027\u80fd\u8870\u51cf\u7684\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u5149\u7535\u6d41-\u7535\u538b\u66f2\u7ebf\u6765\u8bc4\u4f30\u6709\u673a\u592a\u9633\u80fd\u7535\u6c60\u7684\u6750\u6599\u53c2\u6570\uff0c\u5e76\u5c06\u9884\u6d4b\u53c2\u6570\u7f6e\u4e8e\u975e\u9ad8\u65af\u4f3c\u7136\u5206\u5e03\u4e2d\uff0c\u4ece\u800c\u5bf9\u6bcf\u4e2a\u9884\u6d4b\u53c2\u6570\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002"}}
{"id": "2507.11709", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.11709", "abs": "https://arxiv.org/abs/2507.11709", "authors": ["Junius Pun", "Xilai Dai", "Grace Zgheib", "Mahesh A. Iyer", "Andrew Boutros", "Vaughn Betz", "Mohamed S. Abdelfattah"], "title": "Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage", "comment": "accepted at FPL 2025", "summary": "Flexibility and customization are key strengths of Field-Programmable Gate\nArrays (FPGAs) when compared to other computing devices. For instance, FPGAs\ncan efficiently implement arbitrary-precision arithmetic operations, and can\nperform aggressive synthesis optimizations to eliminate ineffectual operations.\nMotivated by sparsity and mixed-precision in deep neural networks (DNNs), we\ninvestigate how to optimize the current logic block architecture to increase\nits arithmetic density. We find that modern FPGA logic block architectures\nprevent the independent use of adder chains, and instead only allow adder chain\ninputs to be fed by look-up table (LUT) outputs. This only allows one of the\ntwo primitives -- either adders or LUTs -- to be used independently in one\nlogic element and prevents their concurrent use, hampering area optimizations.\nIn this work, we propose the Double Duty logic block architecture to enable the\nconcurrent use of the adders and LUTs within a logic element. Without adding\nexpensive logic cluster inputs, we use 4 of the existing inputs to bypass the\nLUTs and connect directly to the adder chain inputs. We accurately model our\nchanges at both the circuit and CAD levels using open-source FPGA development\ntools. Our experimental evaluation on a Stratix-10-like architecture\ndemonstrates area reductions of 21.6% on adder-intensive circuits from the\nKratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR\nbenchmarks respectively. These area improvements come without an impact to\ncritical path delay, demonstrating that higher density is feasible on modern\nFPGA architectures by adding more flexibility in how the adder chain is used.\nAveraged across all circuits from our three evaluated benchmark set, our Double\nDuty FPGA architecture improves area-delay product by 9.7%.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684FPGA\u903b\u8f91\u5757\u67b6\u6784\uff08\u53cc\u91cd\u804c\u8d23\uff09\uff0c\u901a\u8fc7\u5141\u8bb8\u52a0\u6cd5\u5668\u548c\u67e5\u627e\u8868\uff08LUT\uff09\u540c\u65f6\u8fd0\u884c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86FPGA\u7684\u9762\u79ef\u6548\u7387\uff08\u5e73\u5747\u964d\u4f4e9.7%\u7684\u9762\u79ef\u5ef6\u8fdf\u79ef\uff09\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a0\u6cd5\u5668\u5bc6\u96c6\u578b\u548c\u6df7\u5408\u7cbe\u5ea6\u7b49\u73b0\u4ee3\u8ba1\u7b97\u4efb\u52a1\u65f6\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7a00\u758f\u6027\u548c\u6df7\u5408\u7cbe\u5ea6\u95ee\u9898\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5982\u4f55\u4f18\u5316FPGA\u7684\u903b\u8f91\u5757\u67b6\u6784\u4ee5\u63d0\u9ad8\u5176\u7b97\u672f\u5bc6\u5ea6\u3002\u73b0\u6709\u7684FPGA\u903b\u8f91\u5757\u67b6\u6784\u9650\u5236\u4e86\u52a0\u6cd5\u5668\u94fe\u7684\u72ec\u7acb\u4f7f\u7528\uff0c\u4ec5\u5141\u8bb8\u5176\u8f93\u5165\u6e90\u81ea\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u963b\u788d\u4e86\u52a0\u6cd5\u5668\u548c\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u5e76\u884c\u4f7f\u7528\uff0c\u4e0d\u5229\u4e8e\u9762\u79ef\u4f18\u5316\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u53cc\u91cd\u804c\u8d23\u903b\u8f91\u5757\u67b6\u6784\uff0c\u901a\u8fc7\u6539\u8fdbFPGA\u903b\u8f91\u5757\u7684\u5e03\u7ebf\u65b9\u5f0f\uff0c\u5141\u8bb8\u52a0\u6cd5\u5668\u94fe\u548c\u67e5\u627e\u8868\uff08LUT\uff09\u540c\u65f6\u4f7f\u7528\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8f93\u5165\u5f15\u811a\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u73b0\u6709\u76844\u4e2a\u8f93\u5165\u5f15\u811a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u7ed5\u8fc7\uff0c\u4f7f\u4fe1\u53f7\u80fd\u591f\u76f4\u63a5\u8f93\u5165\u5230\u52a0\u6cd5\u5668\u94fe\u3002\u7814\u7a76\u5728\u7535\u8def\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\uff08CAD\uff09\u5c42\u9762\u8fdb\u884c\u4e86\u7cbe\u786e\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u5f00\u6e90FPGA\u5f00\u53d1\u5de5\u5177\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5728\u7c7b\u4f3cStratix-10\u7684\u67b6\u6784\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u53cc\u91cd\u804c\u8d23\u903b\u8f91\u5757\u67b6\u6784\u5728Kratos\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u52a0\u6cd5\u5668\u5bc6\u96c6\u578b\u7535\u8def\u4e0a\u5b9e\u73b0\u4e8621.6%\u7684\u9762\u79ef\u7f29\u51cf\uff0c\u5728\u66f4\u901a\u7528\u7684Koios\u548cVTR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u5b9e\u73b0\u4e869.3%\u548c8.2%\u7684\u9762\u79ef\u7f29\u51cf\u3002\u8fd9\u4e9b\u9762\u79ef\u6539\u8fdb\u5e76\u672a\u5bf9\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u589e\u52a0\u52a0\u6cd5\u5668\u94fe\u4f7f\u7528\u7075\u6d3b\u6027\u53ef\u4ee5\u5728\u73b0\u4ee3FPGA\u67b6\u6784\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7684\u5bc6\u5ea6\u3002\u5728\u6240\u6709\u8bc4\u4f30\u7684\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u6240\u6709\u7535\u8def\u4e0a\u5e73\u5747\u6765\u770b\uff0c\u53cc\u91cd\u804c\u8d23FPGA\u67b6\u6784\u5c06\u9762\u79ef\u5ef6\u8fdf\u79ef\u63d0\u9ad8\u4e869.7%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u91cd\u804c\u8d23\u903b\u8f91\u5757\u67b6\u6784\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u6602\u8d35\u903b\u8f91\u96c6\u7fa4\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4f7f\u7528\u73b0\u6709\u76844\u4e2a\u8f93\u5165\u6765\u7ed5\u8fc7\u67e5\u627e\u8868\uff08LUT\uff09\uff0c\u76f4\u63a5\u8fde\u63a5\u5230\u52a0\u6cd5\u5668\u94fe\u8f93\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u67e5\u627e\u8868\uff08LUT\uff09\u548c\u52a0\u6cd5\u5668\u94fe\u7684\u540c\u65f6\u4f7f\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u67b6\u6784\u76f8\u6bd4\uff0c\u8be5\u67b6\u6784\u5728\u52a0\u6cd5\u5668\u5bc6\u96c6\u578b\u7535\u8def\u548c\u66f4\u901a\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9762\u79ef\u5206\u522b\u51cf\u5c11\u4e8621.6%\u30019.3%\u548c8.2%\uff0c\u540c\u65f6\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\uff0c\u5e73\u5747\u9762\u79ef\u5ef6\u8fdf\u79ef\u63d0\u9ad8\u4e869.7%\u3002"}}
{"id": "2507.12007", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.12007", "abs": "https://arxiv.org/abs/2507.12007", "authors": ["Anders Weile Larsen", "Vedran Sekara"], "title": "Predictable Drifts in Collective Cultural Attention: Evidence from Nation-Level Library Takeout Data", "comment": null, "summary": "Predicting changes in consumer attention for cultural products, such as\nbooks, movies, and songs, is notoriously difficult. Past research on predicting\nthe popularity of individual products suggests the existence of intrinsic\nprediction limits. However, little is known about the limits for predicting\ncollective attention across cultural products. Here, we analyze four years of\nnationwide library loan data for approximately 2 million individuals,\ncomprising over 100 million loans of more than 660,000 unique books. We find\nthat culture, as measured by popularity distributions of loaned books, drifts\ncontinually from month to month at a near-constant rate, leading to a growing\ndivergence over time, and that drifts vary between different book genres. By\nlinking book loans to registry data, we investigate the influence of age, sex,\neducational level, and geographical area on cultural drift, finding\nheterogeneous effects from the different demographic groups. Our findings have\nimportant implications for market forecasting and developing robust recommender\nsystems, highlighting the need to account for specific drift dynamics for\ndifferent types of items and demographic groups.", "AI": {"tldr": "\u6587\u5316\u4ea7\u54c1\uff08\u5982\u56fe\u4e66\uff09\u7684\u6d88\u8d39\u8005\u5173\u6ce8\u5ea6\u96be\u4ee5\u9884\u6d4b\uff0c\u4e14\u96c6\u4f53\u5173\u6ce8\u5ea6\u5b58\u5728\u9884\u6d4b\u9650\u5236\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6587\u5316\u4f1a\u968f\u65f6\u95f4\u6301\u7eed\u6f02\u79fb\uff0c\u4e14\u4e0d\u540c\u4e66\u7c4d\u7c7b\u578b\u548c\u4eba\u7fa4\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u8fd9\u5bf9\u4e8e\u5e02\u573a\u9884\u6d4b\u548c\u63a8\u8350\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u9884\u6d4b\u56fe\u4e66\u3001\u7535\u5f71\u3001\u6b4c\u66f2\u7b49\u6587\u5316\u4ea7\u54c1\u7684\u6d88\u8d39\u8005\u5173\u6ce8\u5ea6\u53d8\u5316\uff0c\u4ee5\u53ca\u9884\u6d4b\u8de8\u6587\u5316\u4ea7\u54c1\u7684\u96c6\u4f53\u5173\u6ce8\u5ea6\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56db\u5e74\u95f4\u8fd1200\u4e07\u4eba\u7684\u5168\u56fd\u56fe\u4e66\u9986\u501f\u9605\u6570\u636e\uff08\u5305\u62ec\u8d85\u8fc71\u4ebf\u6b21\u7684\u501f\u9605\u548c\u8d85\u8fc766\u4e07\u672c\u4e0d\u540c\u7684\u4e66\u7c4d\uff09\uff0c\u5e76\u7ed3\u5408\u6ce8\u518c\u6570\u636e\u6765\u7814\u7a76\u5e74\u9f84\u3001\u6027\u522b\u3001\u6559\u80b2\u7a0b\u5ea6\u548c\u5730\u7406\u533a\u57df\u5bf9\u6587\u5316\u6f02\u79fb\u7684\u5f71\u54cd\u3002", "result": "\u6587\u5316\u4f1a\u4ee5\u63a5\u8fd1\u6052\u5b9a\u7684\u901f\u7387\u6301\u7eed\u6f02\u79fb\uff0c\u5bfc\u81f4\u968f\u65f6\u95f4\u7684\u5dee\u5f02\u4e0d\u65ad\u6269\u5927\uff0c\u5e76\u4e14\u8fd9\u79cd\u6f02\u79fb\u5728\u4e0d\u540c\u4e66\u7c4d\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u4e0d\u540c\u7684\u4eba\u53e3\u7fa4\u4f53\u5bf9\u6587\u5316\u6f02\u79fb\u7684\u5f71\u54cd\u7a0b\u5ea6\u4e0d\u540c\u3002", "conclusion": "\u6587\u5316\uff0c\u4ee5\u6240\u501f\u4e66\u7c4d\u7684\u6d41\u884c\u5ea6\u5206\u5e03\u6765\u8861\u91cf\uff0c\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u5176\u7ec4\u5408\u5173\u6ce8\u5ea6\u4f1a\u4ee5\u63a5\u8fd1\u6052\u5b9a\u7684\u901f\u7387\u6301\u7eed\u6f02\u79fb\uff0c\u5e76\u4e14\u8fd9\u79cd\u6f02\u79fb\u5728\u4e0d\u540c\u7684\u4e66\u7c4d\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002)"}}
{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784\u4ea4\u901a\u6d41\u7684\u5206\u5c42\u534f\u540c\u531d\u9053\u6c47\u5165\u63a7\u5236\uff08HCOMC\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7eb5\u5411\u534f\u8c03\u89c4\u5212\u3001\u81ea\u4e3b\u53d8\u9053\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u71c3\u6cb9\u7ecf\u6d4e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f02\u6784\u4ea4\u901a\u6d41\u4e2dCAVs\u5c1a\u672a\u666e\u53ca\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e3a\u4e24\u8f66\u9053\u9ad8\u901f\u516c\u8def\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u534f\u540c\u7684\u531d\u9053\u6c47\u5165\u63a7\u5236\uff08HCOMC\uff09\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdHCOMC\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u6539\u8fdb\u865a\u62df\u8f66\u8f86\u6a21\u578b\u7684\u7eb5\u5411\u534f\u8c03\u89c4\u5212\u6a21\u578b\u3001\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u81ea\u4e3b\u53d8\u9053\u6a21\u578b\u4ee5\u53ca\u4f7f\u7528\u7cbe\u82f1\u975e\u652f\u914d\u6392\u5e8f\u9057\u4f20\u7b97\u6cd5\u7684\u591a\u76ee\u6807\u4f18\u5316\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5206\u6790\u4e86HCOMC\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u548cCAV\u6e17\u900f\u7387\u4e0b\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660eHCOMC\u5177\u6709\u663e\u8457\u7684\u7efc\u5408\u4f18\u52bf\u3002", "conclusion": "HCOMC\u5728\u63d0\u9ad8\u8f66\u8f86\u5b89\u5168\u3001\u7a33\u5b9a\u548c\u5feb\u901f\u5e76\u5165\u3001\u4f18\u5316\u4ea4\u901a\u6548\u7387\u548c\u8282\u7701\u71c3\u6cb9\u6d88\u8017\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u7efc\u5408\u4f18\u52bf\u3002"}}
{"id": "2507.11808", "categories": ["cs.GT", "cs.DM", "Primary 68R10, Secondary 90B06"], "pdf": "https://arxiv.org/pdf/2507.11808", "abs": "https://arxiv.org/abs/2507.11808", "authors": ["Taiki Yamada", "Taisuke Matsubae", "Tomoya Akamatsu"], "title": "New allocation rule based on graph structures and their application to economic phenomena", "comment": "15 pages, 8 figures", "summary": "This study introduces the \\emph{edge-based Shapley value}, a novel allocation\nrule within cooperative game theory, specifically tailored for networked\nsystems, where value is generated through interactions represented by edges.\nTraditional allocation rules, such as the Shapley and Myerson values, evaluate\nplayer contributions based on node-level characteristics, or connected\ncomponents. However, these approaches often fail to adequately capture the\nfunctional role of edges, which are crucial in systems such as supply chains\nand digital platforms, where interactions, rather than individual agents, are\nthe primary drivers of value. Our edge-based Shapley value shifts the\ncharacteristic function from node sets to edge sets, thereby enabling a more\ngranular and context-sensitive evaluation of the contributions. We establish\nits theoretical foundations, demonstrate its relationship to classical\nallocation rules, and show that it retains key properties such as fairness and\nsymmetry. To illustrate its applicability, we present two use cases: content\nplatform networks and supply chain logistics (SCL). In both cases, our method\nproduces intuitive and structurally consistent allocations, particularly in\nscenarios with overlapping routes, exclusive contracts or cost-sensitive paths.\nThis framework offers a new perspective on value attribution in cooperative\nsettings with complex interaction structures and provides practical tools for\nanalyzing real-world economic and logistical networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8fb9\u7f18\u5316\u590f\u666e\u5229\u503c\uff0c\u4e00\u79cd\u65b0\u7684\u5206\u914d\u89c4\u5219\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u5316\u7cfb\u7edf\u4e2d\u7531\u4ea4\u4e92\uff08\u8fb9\u7f18\uff09\u9a71\u52a8\u7684\u4ef7\u503c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u8282\u70b9\u7684\u5206\u914d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5185\u5bb9\u5e73\u53f0\u548c\u4f9b\u5e94\u94fe\u7684\u6848\u4f8b\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u914d\u89c4\u5219\uff08\u5982\u590f\u666e\u5229\u548c\u6885\u5c14\u68ee\u503c\uff09\u901a\u5e38\u57fa\u4e8e\u8282\u70b9\u7ea7\u522b\u6216\u8fde\u63a5\u7ec4\u4ef6\u7684\u7279\u5f81\u6765\u8bc4\u4f30\u73a9\u5bb6\u7684\u8d21\u732e\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u8fb9\u7f18\uff08\u4ea4\u4e92\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u800c\u8fb9\u7f18\u662f\u4f9b\u5e94\u94fe\u548c\u6570\u5b57\u5e73\u53f0\u7b49\u7cfb\u7edf\u4e2d\u4ef7\u503c\u751f\u6210\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8fb9\u7f18\u7684\u590f\u666e\u5229\u503c\uff0c\u5c06\u7279\u5f81\u51fd\u6570\u4ece\u8282\u70b9\u96c6\u8f6c\u79fb\u5230\u8fb9\u96c6\uff0c\u4ee5\u66f4\u7cbe\u7ec6\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u5730\u8bc4\u4f30\u7f51\u7edc\u5316\u7cfb\u7edf\u4e2d\u7684\u8d21\u732e\u3002", "result": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u8fb9\u7f18\u5316\u590f\u666e\u5229\u503c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u8868\u660e\u5b83\u4fdd\u7559\u4e86\u516c\u5e73\u548c\u5bf9\u79f0\u7b49\u5173\u952e\u5c5e\u6027\u3002\u5728\u5185\u5bb9\u5e73\u53f0\u7f51\u7edc\u548c\u4f9b\u5e94\u94fe\u7269\u6d41\uff08SCL\uff09\u7684\u4e24\u4e2a\u7528\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u76f4\u89c2\u4e14\u7ed3\u6784\u4e00\u81f4\u7684\u5206\u914d\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u91cd\u53e0\u8def\u7ebf\u3001\u72ec\u5bb6\u5408\u540c\u6216\u6210\u672c\u654f\u611f\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8fb9\u7f18\u5316\u590f\u666e\u5229\u503c\uff0c\u4e00\u79cd\u65b0\u7684\u5408\u4f5c\u535a\u5f08\u8bba\u5206\u914d\u89c4\u5219\uff0c\u4e13\u95e8\u9488\u5bf9\u7f51\u7edc\u5316\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5c06\u7279\u5f81\u51fd\u6570\u4ece\u8282\u70b9\u96c6\u8f6c\u79fb\u5230\u8fb9\u96c6\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u7cbe\u7ec6\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u5730\u8bc4\u4f30\u8d21\u732e\u3002\u7814\u7a76\u5efa\u7acb\u4e86\u5176\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u7ecf\u5178\u5206\u914d\u89c4\u5219\u7684\u5173\u7cfb\uff0c\u5e76\u8868\u660e\u5b83\u4fdd\u7559\u4e86\u516c\u5e73\u548c\u5bf9\u79f0\u7b49\u5173\u952e\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5185\u5bb9\u5e73\u53f0\u7f51\u7edc\u548c\u4f9b\u5e94\u94fe\u7269\u6d41\uff08SCL\uff09\u7b49\u7528\u4f8b\u4e2d\u4ea7\u751f\u4e86\u76f4\u89c2\u4e14\u7ed3\u6784\u4e00\u81f4\u7684\u5206\u914d\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u91cd\u53e0\u8def\u7ebf\u3001\u72ec\u5bb6\u5408\u540c\u6216\u6210\u672c\u654f\u611f\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.11794", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.11794", "abs": "https://arxiv.org/abs/2507.11794", "authors": ["Nak-Jun Sung", "Jun Ma", "TaeHeon Kim", "Yoo-joo Choi", "Min-Hyung Choi", "Min Hong"], "title": "Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution", "comment": null, "summary": "This study explores the capabilities of WebGPU, an emerging web graphics\nparadigm, for real-time cloth simulation. Traditional WebGL-based methods have\nbeen in handling complex physical simulations due to their emphasis on graphics\nrendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed\nto provide modern 3D graphics and computational capabilities, offers\nsignificant improvements through parallel processing and support for\ncomputational shaders. In this work, we implemented a cloth simulation system\nusing the Mass-Spring Method within the WebGPU framework, integrating collision\ndetection and response handling with the 3D surface model. First, comparative\nperformance evaluations demonstrate that WebGPU substantially outperforms\nWebGL, particularly in high-resolution simulations, maintaining 60 frames per\nsecond (fps) even with up to 640K nodes. The second experiment aimed to\ndetermine the real-time limitations of WebGPU and confirmed that WebGPU can\nhandle real-time collisions between 4K and 100k cloth node models and a 100K\ntriangle surface model in real-time. These experiments also highlight the\nimportance of balancing real-time performance with realistic rendering when\nhandling collisions between cloth models and complex 3D objects. Our source\ncode is available at https://github.com/nakjun/Cloth-Simulation-WebGPU", "AI": {"tldr": "WebGPU\u5728\u5b9e\u65f6\u5e03\u6599\u6a21\u62df\u65b9\u9762\u6bd4WebGL\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u548c\u590d\u6742\u7684\u78b0\u649e\uff0c\u4f46\u4ecd\u9700\u5e73\u8861\u6027\u80fd\u4e0e\u771f\u5b9e\u611f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eWebGL\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u7269\u7406\u6a21\u62df\uff08\u5982\u5e03\u6599\u6a21\u62df\uff09\u65f6\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u56e0\u4e3aWebGL\u66f4\u4fa7\u91cd\u4e8e\u56fe\u5f62\u6e32\u67d3\u800c\u975e\u901a\u7528GPU\uff08GPGPU\uff09\u8ba1\u7b97\u3002WebGPU\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u56fe\u5f62\u548c\u8ba1\u7b97\u8303\u5f0f\uff0c\u6709\u671b\u901a\u8fc7\u5176\u5e76\u884c\u5904\u7406\u548c\u8ba1\u7b97\u7740\u8272\u5668\u80fd\u529b\u6765\u6539\u5584\u8fd9\u4e9b\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eWebGPU\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u73b0\u8d28\u91cf-\u5f39\u7c27\u65b9\u6cd5\uff08Mass-Spring Method\uff09\u6765\u8fdb\u884c\u5e03\u6599\u6a21\u62df\uff0c\u5e76\u96c6\u6210\u4e86\u78b0\u649e\u68c0\u6d4b\u4e0e\u54cd\u5e94\u5904\u7406\u3002", "result": "\u4e0eWebGL\u76f8\u6bd4\uff0cWebGPU\u5728\u5e03\u6599\u6a21\u62df\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u6a21\u62df\u4e0b\uff0c\u5373\u4f7f\u8282\u70b9\u6570\u8fbe\u5230640K\uff0c\u4e5f\u80fd\u7ef4\u630160fps\u3002WebGPU\u80fd\u591f\u5b9e\u65f6\u5904\u74064K\u5230100K\u8282\u70b9\u5e03\u6599\u6a21\u578b\u4e0e100K\u4e09\u89d2\u5f62\u8868\u9762\u6a21\u578b\u4e4b\u95f4\u7684\u78b0\u649e\u3002", "conclusion": "WebGPU\u5728\u5b9e\u65f6\u5e03\u6599\u6a21\u62df\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u9ad8\u6027\u80fd\uff0c\u5e76\u80fd\u5904\u7406\u590d\u6742\u7684\u78b0\u649e\u4ea4\u4e92\uff0c\u5c24\u5176\u662f\u5728\u4e0eWebGL\u7684\u6bd4\u8f83\u4e2d\u3002\u7136\u800c\uff0c\u5728\u8ffd\u6c42\u5b9e\u65f6\u6027\u80fd\u4e0e\u771f\u5b9e\u611f\u6e32\u67d3\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u7684\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "LLMs show unique literary evaluation patterns and biases, similar to human critics, rather than being neutral evaluators.", "motivation": "To explore aesthetic preferences and evaluation patterns in literary assessment by positioning LLMs as subjective literary critics.", "method": "Ten Japanese science fiction short stories were translated into English and evaluated by six state-of-the-art LLMs across seven independent sessions. Principal component analysis and clustering techniques were used for analysis.", "result": "Significant variations in evaluation consistency (alpha ranging from 1.00 to 0.35) and five distinct evaluation patterns were revealed. Evaluation variance across stories differed by up to 4.5-fold, with distinctive evaluation vocabularies for each model confirmed by TF-IDF analysis.", "conclusion": "LLMs may possess individual evaluation characteristics similar to human critical schools, rather than functioning as neutral benchmarkers."}}
{"id": "2507.11566", "categories": ["cs.NE", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11566", "abs": "https://arxiv.org/abs/2507.11566", "authors": ["Fuda van Diggelen", "Tugay Alperen Karag\u00fczel", "Andres Garcia Rincon", "A. E. Eiben", "Dario Floreano", "Eliseo Ferrante"], "title": "Emergent Heterogeneous Swarm Control Through Hebbian Learning", "comment": null, "summary": "In this paper, we introduce Hebbian learning as a novel method for swarm\nrobotics, enabling the automatic emergence of heterogeneity. Hebbian learning\npresents a biologically inspired form of neural adaptation that solely relies\non local information. By doing so, we resolve several major challenges for\nlearning heterogeneous control: 1) Hebbian learning removes the complexity of\nattributing emergent phenomena to single agents through local learning rules,\nthus circumventing the micro-macro problem; 2) uniform Hebbian learning rules\nacross all swarm members limit the number of parameters needed, mitigating the\ncurse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian\nlearning rules based on swarm-level behaviour minimises the need for extensive\nprior knowledge typically required for optimising heterogeneous swarms. This\nwork demonstrates that with Hebbian learning heterogeneity naturally emerges,\nresulting in swarm-level behavioural switching and in significantly improved\nswarm capabilities. It also demonstrates how the evolution of Hebbian learning\nrules can be a valid alternative to Multi Agent Reinforcement Learning in\nstandard benchmarking tasks.", "AI": {"tldr": "Hebbian learning allows robot swarms to automatically develop diverse behaviors (heterogeneity) using only local information, simplifying learning and improving performance, and can replace traditional methods like MARL.", "motivation": "To address challenges in learning heterogeneous control for swarm robotics, including the micro-macro problem, curse of dimensionality, and the need for extensive prior knowledge.", "method": "Introducing Hebbian learning, a biologically inspired neural adaptation relying solely on local information, as a novel method for swarm robotics.", "result": "Demonstrated the natural emergence of heterogeneity, resulting in swarm-level behavioural switching and significantly improved swarm capabilities. Also showed Hebbian learning rules as a valid alternative to MARL in standard benchmarking tasks.", "conclusion": "Hebbian learning enables the automatic emergence of heterogeneity in swarm robotics, leading to behavioural switching and improved swarm capabilities. It offers a viable alternative to Multi Agent Reinforcement Learning for benchmarking tasks."}}
{"id": "2507.11549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11549", "abs": "https://arxiv.org/abs/2507.11549", "authors": ["Wendong Mao", "Mingfan Zhao", "Jianfeng Guan", "Qiwei Dong", "Zhongfeng Wang"], "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search", "comment": null, "summary": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11744", "categories": ["cs.MA", "cs.GT", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.11744", "abs": "https://arxiv.org/abs/2507.11744", "authors": ["Marcin Kowalik", "Przemys\u0142aw Stok\u0142osa", "Mateusz Grabowski", "Janusz Starzyk", "Pawe\u0142 Raif"], "title": "A Cellular Automata Approach to Donation Game", "comment": "16 pages, 12 figures", "summary": "The donation game is a well-established framework for studying the emergence\nand evolution of cooperation in multi-agent systems. The cooperative behavior\ncan be influenced by the environmental noise in partially observable settings\nand by the decision-making strategies of agents, which may incorporate not only\nreputation but also traits such as generosity and forgiveness. Traditional\nsimulations often assume fully random interactions, where cooperation is tested\nbetween randomly selected agent pairs. In this paper, we investigate\ncooperation dynamics using the concept of Stephen Wolfram's one-dimensional\nbinary cellular automata. This approach allows us to explore how cooperation\nevolves when interactions are limited to neighboring agents. We define binary\ncellular automata rules that conform to the donation game mechanics.\nAdditionally, we introduce models of perceptual and action noise, along with a\nmutation matrix governing the probabilistic evolution of agent strategies. Our\nempirical results demonstrate that cooperation is significantly affected by\nagents' mobility and their spatial locality on the game board. These findings\nhighlight the importance of distinguishing between entirely random multi-agent\nsystems and those in which agents are more likely to interact with their\nnearest neighbors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ec6\u80de\u81ea\u52a8\u673a\u4e2d\u5408\u4f5c\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u4ee3\u7406\u7684\u79fb\u52a8\u6027\u548c\u7a7a\u95f4\u5c40\u90e8\u6027\u5bf9\u5408\u4f5c\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u548c\u5f71\u54cd\u5408\u4f5c\u7684\u591a\u79cd\u56e0\u7d20\uff08\u5982\u58f0\u8a89\u3001\u6177\u6168\u548c\u5bbd\u6055\uff09\u4e0b\uff0c\u5408\u4f5c\u7684\u6d8c\u73b0\u548c\u6f14\u53d8\u3002", "method": "\u7814\u7a76\u5408\u4f5c\u52a8\u529b\u5b66\uff0c\u5229\u7528\u7ec6\u80de\u81ea\u52a8\u673a\uff0c\u5b9a\u4e49\u7b26\u5408\u6350\u8d60\u6e38\u620f\u673a\u5236\u7684\u7ec6\u80de\u81ea\u52a8\u673a\u89c4\u5219\uff0c\u5e76\u5f15\u5165\u611f\u77e5\u548c\u884c\u52a8\u566a\u58f0\u6a21\u578b\u4ee5\u53ca\u63a7\u5236\u4ee3\u7406\u7b56\u7565\u6982\u7387\u6f14\u53d8\u7684\u7a81\u53d8\u77e9\u9635\u3002", "result": "\u5408\u4f5c\u53d7\u5230\u4ee3\u7406\u7684\u79fb\u52a8\u6027\u53ca\u5176\u5728\u6e38\u620f\u68cb\u76d8\u4e0a\u7684\u7a7a\u95f4\u5c40\u90e8\u6027\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u533a\u5206\u5b8c\u5168\u968f\u673a\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u66f4\u53ef\u80fd\u4e0e\u6700\u8fd1\u90bb\u5c45\u4ea4\u4e92\u7684\u7cfb\u7edf\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u5408\u4f5c\u884c\u4e3a\u53d7\u5230\u73af\u5883\u566a\u58f0\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4ee5\u53ca\u53ef\u80fd\u5305\u542b\u58f0\u8a89\u3001\u6177\u6168\u548c\u5bbd\u6055\u7b49\u7279\u8d28\u7684\u51b3\u7b56\u7b56\u7565\u7684\u5f71\u54cd\u3002\u4e0e\u4f20\u7edf\u7684\u968f\u673a\u4ea4\u4e92\u6a21\u62df\u4e0d\u540c\uff0c\u672c\u6587\u91c7\u7528\u7ec6\u80de\u81ea\u52a8\u673a\u6765\u7814\u7a76\u5f53\u4ea4\u4e92\u4ec5\u9650\u4e8e\u76f8\u90bb\u4ee3\u7406\u65f6\u5408\u4f5c\u7684\u6f14\u53d8\u3002"}}
{"id": "2507.11724", "categories": ["cs.DS", "cs.NA", "math.NA", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11724", "abs": "https://arxiv.org/abs/2507.11724", "authors": ["Micha\u0142 Derezi\u0144ski", "Aaron Sidford"], "title": "Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure", "comment": null, "summary": "We provide new high-accuracy randomized algorithms for solving linear systems\nand regression problems that are well-conditioned except for $k$ large singular\nvalues. For solving such $d \\times d$ positive definite system our algorithms\nsucceed whp. and run in time $\\tilde O(d^2 + k^\\omega)$. For solving such\nregression problems in a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ our\nmethods succeed whp. and run in time $\\tilde O(\\mathrm{nnz}(\\mathbf{A}) + d^2 +\nk^\\omega)$ where $\\omega$ is the matrix multiplication exponent and\n$\\mathrm{nnz}(\\mathbf{A})$ is the number of non-zeros in $\\mathbf{A}$. Our\nmethods nearly-match a natural complexity limit under dense inputs for these\nproblems and improve upon a trade-off in prior approaches that obtain running\ntimes of either $\\tilde O(d^{2.065}+k^\\omega)$ or $\\tilde O(d^2 +\ndk^{\\omega-1})$ for $d\\times d$ systems. Moreover, we show how to obtain these\nrunning times even under the weaker assumption that all but $k$ of the singular\nvalues have a suitably bounded generalized mean. Consequently, we give the\nfirst nearly-linear time algorithm for computing a multiplicative approximation\nto the nuclear norm of an arbitrary dense matrix. Our algorithms are built on\nthree general recursive preconditioning frameworks, where matrix sketching and\nlow-rank update formulas are carefully tailored to the problems' structure.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u968f\u673a\u7b97\u6cd5\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u6c42\u89e3\u6761\u4ef6\u9002\u4e2d\u7684\u7ebf\u6027\u7cfb\u7edf\u548c\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u8ba1\u7b97\u7a20\u5bc6\u77e9\u9635\u6838\u8303\u6570\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u968f\u673a\u7b97\u6cd5\u6765\u89e3\u51b3\u7ebf\u6027\u7cfb\u7edf\u548c\u56de\u5f52\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u90a3\u4e9b\u9664\u4e86 k \u4e2a\u5927\u5947\u5f02\u503c\u4e4b\u5916\u6761\u4ef6\u9002\u4e2d\u7684\u60c5\u51b5\u3002\u76ee\u7684\u662f\u5728\u63a5\u8fd1\u7406\u8bba\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u5728\u5904\u7406\u8fd9\u7c7b\u95ee\u9898\u65f6\u7684\u6027\u80fd\u548c\u6743\u8861\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7b97\u6cd5\u57fa\u4e8e\u4e09\u4e2a\u901a\u7528\u7684\u9012\u5f52\u9884\u5904\u7406\u6846\u67b6\uff0c\u5e76\u4ed4\u7ec6\u8bbe\u8ba1\u4e86\u77e9\u9635\u7ed8\u5236\uff08sketching\uff09\u548c\u4f4e\u79e9\u66f4\u65b0\u516c\u5f0f\u4ee5\u9002\u5e94\u95ee\u9898\u7684\u7ed3\u6784\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u9ad8\u7cbe\u5ea6\u968f\u673a\u7b97\u6cd5\uff0c\u5728\u5904\u7406\u5177\u6709k\u4e2a\u5927\u5947\u5f02\u503c\u7684\u7ebf\u6027\u7cfb\u7edf\u548c\u56de\u5f52\u95ee\u9898\u65f6\uff0c\u8fd0\u884c\u65f6\u95f4\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5bf9\u4e8ed x d\u6b63\u5b9a\u7cfb\u7edf\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a O(d^2 + k^\u03c9)\uff1b\u5bf9\u4e8en x d\u77e9\u9635\u7684\u56de\u5f52\u95ee\u9898\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a O(nnz(A) + d^2 + k^\u03c9)\u3002\u7814\u7a76\u8fd8\u5b9e\u73b0\u4e86\u8ba1\u7b97\u4efb\u610f\u7a20\u5bc6\u77e9\u9635\u6838\u8303\u6570\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\u548c\u56de\u5f52\u95ee\u9898\u7684\u9ad8\u7cbe\u5ea6\u968f\u673a\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5904\u7406\u5177\u6709k\u4e2a\u5927\u5947\u5f02\u503c\u7684\u9002\u5ea6\u6761\u4ef6\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u8272\u3002\u5bf9\u4e8e d x d \u7684\u6b63\u5b9a\u7cfb\u7edf\uff0c\u7b97\u6cd5\u7684\u6210\u529f\u7387\u4e3a whp\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a O(d^2 + k^\u03c9)\u3002\u5bf9\u4e8e A \u2208 R^(nxd) \u77e9\u9635\u7684\u56de\u5f52\u95ee\u9898\uff0c\u7b97\u6cd5\u7684\u6210\u529f\u7387\u4e3a whp\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a O(nnz(A) + d^2 + k^\u03c9)\uff0c\u5176\u4e2d \u03c9 \u662f\u77e9\u9635\u4e58\u6cd5\u6307\u6570\uff0cnnz(A) \u662f A \u4e2d\u975e\u96f6\u5143\u7d20\u7684\u6570\u91cf\u3002\u8fd9\u4e9b\u65b9\u6cd5\u63a5\u8fd1\u8fd9\u4e9b\u95ee\u9898\u7684\u7a20\u5bc6\u8f93\u5165\u7684\u81ea\u7136\u590d\u6742\u5ea6\u9650\u5236\uff0c\u5e76\u6539\u8fdb\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u6743\u8861\uff0c\u5148\u524d\u65b9\u6cd5\u5728 d x d \u7cfb\u7edf\u4e0a\u8fd0\u884c\u65f6\u95f4\u4e3a O(d^2.065} + k^\u03c9) \u6216 O(d^2 + dk^{\u03c9-1})\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6240\u6709\u5947\u5f02\u503c\uff08\u9664\u4e86 k \u4e2a\uff09\u5177\u6709\u9002\u5f53\u754c\u5b9a\u7684\u5e7f\u4e49\u5747\u503c\u8fd9\u4e00\u8f83\u5f31\u5047\u8bbe\u4e0b\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e9b\u8fd0\u884c\u65f6\u95f4\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u8ba1\u7b97\u4efb\u610f\u7a20\u5bc6\u77e9\u9635\u4e58\u6cd5\u8fd1\u4f3c\u6838\u8303\u6570\uff08nuclear norm\uff09\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u57fa\u4e8e\u4e09\u4e2a\u901a\u7528\u7684\u9012\u5f52\u9884\u5904\u7406\u6846\u67b6\uff0c\u5176\u4e2d\u77e9\u9635\u7ed8\u5236\uff08sketching\uff09\u548c\u4f4e\u79e9\u66f4\u65b0\u516c\u5f0f\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u4ee5\u9002\u5e94\u95ee\u9898\u7684\u7ed3\u6784\u3002"}}
{"id": "2507.11565", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11565", "abs": "https://arxiv.org/abs/2507.11565", "authors": ["Muhammad Faryad"], "title": "Lecture Notes on Quantum Algorithms", "comment": "190 pages", "summary": "These notes begin in Chapter 1 with a review of linear algebra and the\npostulates of quantum mechanics, leading to an explanation of single- and\nmulti-qubit gates. Chapter 2 explores the challenge of constructing arbitrary\nquantum states from given initial states, and introduces circuits for building\noracles. Chapter 3 presents foundational algorithms such as entanglement\ncreation, quantum teleportation, Deutsch-Jozsa, Bernstein-Vazirani, and Simon's\nalgorithm.\n  Chapters 4 and 5 cover algorithms based on the quantum Fourier transform,\nincluding phase estimation, period finding, factoring, and logarithm\ncomputation. These chapters also include complexity analysis and detailed\nquantum circuits suitable for implementation in code. Chapter 6 introduces\nGrover's algorithm for quantum search and amplitude amplification, including\nits realization via Hamiltonian simulation and a method for derandomization.\n  Chapter 7 discusses basic techniques for Hamiltonian simulation, such as\nLie-Trotter decomposition, sparse Hamiltonians, and the linear combination of\nunitaries. It also provides example circuits for simulating Hamiltonians\nexpressed as linear combinations of Pauli operators. Chapter 8 introduces\nvariational quantum algorithms, and Chapter 9 presents an algorithm for\nsimulating fermionic many-particle systems, with an emphasis on molecular\nHamiltonians. It also outlines the key transformations needed to map a\nmolecular Hamiltonian to a form suitable for simulation on a quantum computer.", "AI": {"tldr": "This paper is a textbook-style guide to quantum computing, covering fundamental concepts, key algorithms (like Shor's and Grover's), and advanced topics such as Hamiltonian simulation and quantum chemistry simulations, complete with circuit details and complexity analysis.", "motivation": "To provide a structured and in-depth understanding of quantum algorithms, from basic principles to advanced applications, and to offer practical insights into their implementation.", "method": "The paper reviews linear algebra, quantum mechanics postulates, and systematically introduces quantum gates, algorithms (including those based on quantum Fourier transform and Grover's algorithm), Hamiltonian simulation techniques, and applications to specific problems like fermionic systems. It also discusses complexity analysis and provides detailed quantum circuits.", "result": "The paper details various quantum algorithms and techniques, including their theoretical underpinnings, circuit implementations, and applications in areas like factoring, search, and simulating quantum systems.", "conclusion": "The paper provides a comprehensive overview of quantum algorithms and their implementation, covering foundational concepts, various algorithms like Deutsch-Jozsa and Grover's, and advanced topics such as Hamiltonian simulation and variational algorithms."}}
{"id": "2507.11547", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11547", "abs": "https://arxiv.org/abs/2507.11547", "authors": ["Yingxue Zhao", "Qianyi Chen", "Haoran Li", "Haosu Zhou", "Hamid Reza Attar", "Tobias Pfaff", "Tailin Wu", "Nan Li"], "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming", "comment": null, "summary": "In recent years, various artificial intelligence-based surrogate models have\nbeen proposed to provide rapid manufacturability predictions of material\nforming processes. However, traditional AI-based surrogate models, typically\nbuilt with scalar or image-based neural networks, are limited in their ability\nto capture complex 3D spatial relationships and to operate in a\npermutation-invariant manner. To overcome these issues, emerging graph-based\nsurrogate models are developed using graph neural networks. This study\ndeveloped a new graph neural network surrogate model named Recurrent U\nNet-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate\npredictions of sheet material deformation fields across multiple forming\ntimesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model\ntemporal dynamics and a U-Net inspired graph-based downsample/upsample\nmechanism to handle spatial long-range dependencies. A novel 'node-to-surface'\ncontact representation method was proposed, offering significant improvements\nin computational efficiency for large-scale contact interactions. The RUGNN\nmodel was validated using a cold forming case study and a more complex hot\nforming case study using aluminium alloys. Results demonstrate that the RUGNN\nmodel provides accurate deformation predictions closely matching ground truth\nFE simulations and outperforming several baseline GNN architectures. Model\ntuning was also performed to identify suitable hyperparameters, training\nstrategies, and input feature representations. These results demonstrate that\nRUGNN is a reliable approach to support sheet material forming design by\nenabling accurate manufacturability predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRUGNN\u7684\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u786e\u9884\u6d4b\u6750\u6599\u6210\u578b\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5f62\uff0c\u514b\u670d\u4e86\u4f20\u7edfAI\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8eAI\u7684\u4ee3\u7406\u6a21\u578b\uff08\u901a\u5e38\u4f7f\u7528\u57fa\u4e8e\u6807\u91cf\u6216\u56fe\u50cf\u7684\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\uff09\u5728\u6355\u83b7\u590d\u6742\u76843D\u7a7a\u95f4\u5173\u7cfb\u548c\u4ee5\u6392\u5217\u4e0d\u53d8\u7684\u65b9\u5f0f\u8fd0\u884c\u65f6\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRUGNN\u7684\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRUs\uff09\u6765\u6a21\u62df\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u91c7\u7528\u53d7U-Net\u542f\u53d1\u7684\u57fa\u4e8e\u56fe\u7684\u4e0b\u91c7\u6837/\u4e0a\u91c7\u6837\u673a\u5236\u6765\u5904\u7406\u7a7a\u95f4\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u8282\u70b9\u5230\u8868\u9762\u201d\u63a5\u89e6\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u89c4\u6a21\u63a5\u89e6\u76f8\u4e92\u4f5c\u7528\u7684\u8ba1\u7b97\u6548\u7387\u3002", "result": "RUGNN\u6a21\u578b\u5728\u94dd\u5408\u91d1\u51b7\u6210\u578b\u548c\u70ed\u6210\u578b\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u53d8\u5f62\uff0c\u4e0e\u5730\u9762\u771f\u5b9e\u6709\u9650\u5143\u6a21\u62df\u7ed3\u679c\u975e\u5e38\u543b\u5408\uff0c\u5e76\u4e14\u4f18\u4e8e\u51e0\u79cd\u57fa\u7ebfGNN\u67b6\u6784\u3002", "conclusion": "RUGNN\u6a21\u578b\u662f\u652f\u6301\u7247\u6750\u6210\u578b\u8bbe\u8ba1\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u53ef\u5236\u9020\u6027\u9884\u6d4b\u3002"}}
{"id": "2507.11705", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.11705", "abs": "https://arxiv.org/abs/2507.11705", "authors": ["Saurabh Kumar Sen", "Shivendra Kumar Gupta", "Nagarjuna Patra", "Ajit Singh Jhala", "Poorva Singh"], "title": "Electron and phonon topology in transition metal material TaSi", "comment": "8 pages, 9 figures,", "summary": "The plethora of multifold quasiparticles in topological materials has led to\nsignificant advancements in condensed matter physics, inspiring the\ninvestigation for materials that host both electronic and bosonic multifold\nexcitations. In this work, we explore the electronic and phononic properties of\nTaSi, a non symmorphic chiral topological material crystallizing in space group\nP 2 1 3 (No. 198). This system exhibits multifold fermions, which are higher\nspin generalizations of Weyl fermions, protected by the unique crystalline\nsymmetries of the structure. Using first principles calculations, we predict\nthat electronic band possesses fourfold spin 3/2 Rarita Schwinger (RSW)\nfermions, sixfold excitations (double spin 1), all possessing large Chern\nnumbers C = +4 and Weyl fermions of spin 1/2 with Chern no. -1 in the presence\nof spin orbit coupling (SOC). Additionally, the phononic band structure hosts\nchiral bosonic excitations characterized by Chern numbers C = +-2. The\ncoexistence of chiral electronic and bosonic quasiparticles give rise to exotic\ntransport phenomena, rendering the material as promising candidate for future\napplications in quantum materials, topological electronics, and spintronics.", "AI": {"tldr": "TaSi\u662f\u4e00\u79cd\u5177\u6709\u591a\u91cd\u8d39\u7c73\u5b50\u548c\u624b\u6027\u73bb\u8272\u5b50\u6fc0\u53d1\u7684\u62d3\u6251\u6750\u6599\uff0c\u5728\u91cf\u5b50\u6750\u6599\u3001\u62d3\u6251\u7535\u5b50\u5b66\u548c\u81ea\u65cb\u7535\u5b50\u5b66\u4e2d\u6709\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u5bfb\u627e\u540c\u65f6\u5177\u6709\u7535\u5b50\u548c\u73bb\u8272\u5b50\u591a\u91cd\u6fc0\u53d1\uff08\u51c6\u7c92\u5b50\uff09\u7684\u6750\u6599\uff0c\u4ee5\u671f\u5728\u51dd\u805a\u6001\u7269\u7406\u548c\u91cf\u5b50\u6750\u6599\u7b49\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7814\u7a76\u4e86TaSi\u7684\u7535\u5b50\u548c\u58f0\u5b50\u6027\u8d28\u3002", "result": "\u5728SOC\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0cTaSi\u7684\u7535\u5b50\u80fd\u5e26\u8868\u73b0\u51fa\u56db\u91cd\u7b80\u5e76\u76843/2\u81ea\u65cbRarita-Schwinger\uff08RSW\uff09\u8d39\u7c73\u5b50\u3001\u5177\u6709\u5927\u9648\u6570C=+4\u7684\u516d\u91cd\u6fc0\u53d1\uff08\u53cc\u91cd1\u81ea\u65cb\uff09\u4ee5\u53ca\u5177\u6709-1\u9648\u6570\u76841/2\u81ea\u65cb\u5916\u5c14\u8d39\u7c73\u5b50\u3002\u6b64\u5916\uff0c\u58f0\u5b50\u80fd\u5e26\u7ed3\u6784\u4e5f\u8868\u73b0\u51fa\u624b\u6027\u9648\u6570\u4e3aC=\u00b12\u7684\u73bb\u8272\u5b50\u6fc0\u53d1\u3002", "conclusion": "TaSi\u662f\u4e00\u79cd\u975e\u5bf9\u79f0\u624b\u6027\u62d3\u6251\u6750\u6599\uff0c\u5177\u6709\u53d7\u7a7a\u95f4\u7fa4P 2 1 3\u4fdd\u62a4\u7684\u56db\u91cd\u7b80\u5e76\u5916\u5c14\u8d39\u7c73\u5b50\u3002\u8be5\u6750\u6599\u540c\u65f6\u5177\u6709\u53d7SOC\u4fdd\u62a4\u7684\u516d\u91cd\u6fc0\u53d1\u548c\u624b\u6027 the bosonic excitations\uff0c\u5177\u6709\u5927\u9648\u6570\u3002\u7535\u5b50\u548c\u73bb\u8272\u5b50\u6fc0\u53d1\u7684\u5171\u5b58\u8d4b\u4e88\u4e86TaSi\u5947\u7279\u7684\u8f93\u8fd0\u73b0\u8c61\uff0c\u4f7f\u5176\u6210\u4e3a\u91cf\u5b50\u6750\u6599\u3001\u62d3\u6251\u7535\u5b50\u5b66\u548c\u81ea\u65cb\u7535\u5b50\u5b66\u7b49\u672a\u6765\u5e94\u7528\u7684\u6709\u5e0c\u671b\u7684\u5019\u9009\u8005\u3002"}}
{"id": "2507.11783", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC", "A.1; I.2; I.5; J.3"], "pdf": "https://arxiv.org/pdf/2507.11783", "abs": "https://arxiv.org/abs/2507.11783", "authors": ["Gayal Kuruppu", "Neeraj Wagh", "Yogatheesan Varatharajah"], "title": "Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions", "comment": "20 pages, 5 figures, 2 tables", "summary": "Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.", "AI": {"tldr": "\u65e9\u671fEEG-FMs\u5728\u5229\u7528Transformer\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u7edf\u4e00\u4e14\u6709\u9650\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u6807\u51c6\u5316\u8bc4\u4f30\u3001\u5c55\u793a\u89c4\u6a21\u6548\u5e94\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u4ee5\u63a8\u52a8\u5176\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u6709\u76d1\u7763EEG\u7f16\u7801\u5668\u5728\u5b66\u4e60\u9c81\u68d2EEG\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6602\u8d35\u7684\u4fe1\u53f7\u6ce8\u91ca\uff0c\u56e0\u6b64\u7814\u7a76\u91cd\u70b9\u8f6c\u5411\u901a\u7528\u7684\u81ea\u76d1\u7763EEG\u7f16\u7801\u5668\uff08\u5373EEG\u57fa\u7840\u6a21\u578b\uff0cEEG-FMs\uff09\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684EEG\u7279\u5f81\u63d0\u53d6\u3002\u7136\u800c\uff0c\u65e9\u671fEEG-FMs\u7684\u5b9e\u9645\u51c6\u5907\u60c5\u51b5\u548c\u957f\u671f\u7814\u7a76\u8fdb\u5c55\u7684\u8bc4\u4ef7\u6807\u51c6\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u7b2c\u4e00\u4ee3EEG-FMs\u8fdb\u884c\u7cfb\u7edf\u6027\u3001\u5168\u9762\u7684\u56de\u987e\u3002", "method": "\u672c\u7814\u7a76\u5bf910\u4e2a\u65e9\u671f\u7684EEG-FMs\u8fdb\u884c\u4e86\u56de\u987e\uff0c\u5e76\u5bf9\u5176\u65b9\u6cd5\u3001\u5b9e\u8bc1\u7ed3\u679c\u548c\u73b0\u6709\u7814\u7a76\u5dee\u8ddd\u8fdb\u884c\u4e86\u6279\u5224\u6027\u7efc\u5408\u3002\u5927\u591a\u6570EEG-FMs\u91c7\u7528\u57fa\u4e8e\u5e8f\u5217\u7684\u5efa\u6a21\u65b9\u6848\uff0c\u4f9d\u8d56\u4e8e\u57fa\u4e8eTransformer\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u63a9\u7801\u5e8f\u5217\u6765\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5927\u591a\u6570EEG-FMs\u91c7\u7528\u4e86\u57fa\u4e8e\u5e8f\u5217\u7684\u5efa\u6a21\u65b9\u6848\uff0c\u5229\u7528Transformer\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u63a9\u7801\u5e8f\u5217\u6765\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u7136\u800c\uff0c\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u7edf\u4e00\u4e14\u8bc4\u4f30\u8303\u56f4\u6709\u9650\uff0c\u96be\u4ee5\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7528\u3002\u672a\u6765\u7684\u5de5\u4f5c\u9700\u8981\u5728\u6807\u51c6\u5316\u548c\u73b0\u5b9e\u7684\u8bc4\u4f30\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5e76\u5c55\u793a\u51fa\u66f4\u663e\u8457\u7684\u89c4\u6a21\u6548\u5e94\uff0c\u540c\u65f6\u5728EEG\u8868\u793a\u5b66\u4e60\u6d41\u7a0b\u4e2d\u505a\u51fa\u5408\u4e4e\u539f\u5219\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u9009\u62e9\u3002", "conclusion": "\u672a\u6765\u7684EEG-FMs\u7814\u7a76\u5e94\u91c7\u7528\u6807\u51c6\u5316\u7684\u3001\u73b0\u5b9e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u66f4\u663e\u8457\u7684\u89c4\u6a21\u6548\u5e94\u3002\u540c\u65f6\uff0c\u5e94\u5728EEG\u8868\u793a\u5b66\u4e60\u6d41\u7a0b\u4e2d\u505a\u51fa\u539f\u5219\u6027\u548c\u53ef\u4fe1\u8d56\u7684\u9009\u62e9\uff0c\u901a\u8fc7\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1\u57fa\u51c6\u3001\u8f6f\u4ef6\u5de5\u5177\u3001\u6280\u672f\u65b9\u6cd5\u548c\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8EEG-FMs\u7684\u8f6c\u5316\u6548\u7528\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.11655", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11655", "abs": "https://arxiv.org/abs/2507.11655", "authors": ["Mohimenul Kabir", "Supratik Chakraborty", "Kuldeep S Meel"], "title": "Counting Answer Sets of Disjunctive Answer Set Programs", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Answer Set Programming (ASP) provides a powerful declarative paradigm for\nknowledge representation and reasoning. Recently, counting answer sets has\nemerged as an important computational problem with applications in\nprobabilistic reasoning, network reliability analysis, and other domains. This\nhas motivated significant research into designing efficient ASP counters. While\nsubstantial progress has been made for normal logic programs, the development\nof practical counters for disjunctive logic programs remains challenging.\n  We present SharpASP-SR, a novel framework for counting answer sets of\ndisjunctive logic programs based on subtractive reduction to projected\npropositional model counting. Our approach introduces an alternative\ncharacterization of answer sets that enables efficient reduction while ensuring\nthat intermediate representations remain of polynomial size. This allows\nSharpASP-SR to leverage recent advances in projected model counting technology.\nThrough extensive experimental evaluation on diverse benchmarks, we demonstrate\nthat SharpASP-SR significantly outperforms existing counters on instances with\nlarge answer set counts. Building on these results, we develop a hybrid\ncounting approach that combines enumeration techniques with SharpASP-SR to\nachieve state-of-the-art performance across the full spectrum of disjunctive\nprograms.", "AI": {"tldr": "SharpASP-SR\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u4e2d\u7684\u7b54\u6848\u96c6\u8ba1\u6570\u95ee\u9898\u7ea6\u7b80\u4e3a\u6295\u5f71\u547d\u9898\u6a21\u578b\u8ba1\u6570\u95ee\u9898\u6765\u89e3\u51b3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u8ba1\u6570\u5668\u3002", "motivation": "\u5c3d\u7ba1\u5728\u6b63\u89c4\u903b\u8f91\u7a0b\u5e8f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684ASP\u8ba1\u6570\u5668\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7136\u800c\uff0c\u7b54\u6848\u96c6\u8ba1\u6570\u5df2\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u5728\u6982\u7387\u63a8\u7406\u3001\u7f51\u7edc\u53ef\u9760\u6027\u5206\u6790\u7b49\u9886\u57df\u6709\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSharpASP-SR\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u51cf\u6cd5\u7ea6\u7b80\u5230\u6295\u5f71\u547d\u9898\u6a21\u578b\u8ba1\u6570\uff0c\u7528\u4e8e\u5bf9\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u8fdb\u884c\u7b54\u6848\u96c6\u8ba1\u6570\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u7b54\u6848\u96c6\u7684\u66ff\u4ee3\u8868\u5f81\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u7ea6\u7b80\uff0c\u5e76\u786e\u4fdd\u4e2d\u95f4\u8868\u793a\u7684\u5927\u5c0f\u4e3a\u591a\u9879\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSharpASP-SR\u5728\u5177\u6709\u5927\u91cf\u7b54\u6848\u96c6\u7684\u5b9e\u4f8b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8ba1\u6570\u5668\uff0c\u5e76\u4e14\u7ed3\u5408\u679a\u4e3e\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SharpASP-SR\u901a\u8fc7\u5c06\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u4e2d\u7684\u7b54\u6848\u96c6\u8ba1\u6570\u95ee\u9898\u901a\u8fc7\u51cf\u6cd5\u7ea6\u7b80\u4e3a\u6295\u5f71\u547d\u9898\u6a21\u578b\u8ba1\u6570\u95ee\u9898\uff0c\u5e76\u4fdd\u8bc1\u4e2d\u95f4\u8868\u793a\u7684\u591a\u9879\u5f0f\u5927\u5c0f\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u5229\u7528\u6295\u5f71\u6a21\u578b\u8ba1\u6570\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u5728\u5177\u6709\u5927\u91cf\u7b54\u6848\u96c6\u7684\u5b9e\u4f8b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8ba1\u6570\u5668\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u679a\u4e3e\u6280\u672f\u548cSharpASP-SR\u7684\u6df7\u5408\u8ba1\u6570\u65b9\u6cd5\u5728\u5168\u8c31\u7684\u6790\u53d6\u7a0b\u5e8f\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11614", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.11614", "abs": "https://arxiv.org/abs/2507.11614", "authors": ["Ya. I. Rodionov", "A. V. Rozhkov", "M. E. S. Beck", "A. O. Sboychakov", "K. I. Kugel", "A. L. Rakhmanov"], "title": "Nesting-driven ferromagnetism of itinerant electrons", "comment": null, "summary": "We theoretically investigate a model with electrons and holes whose Fermi\nsurfaces are perfectly nested. The fermions are assumed to be interacting, both\nwith each other and with the lattice. To suppress inhomogeneous states, a\nsufficiently strong long-range Coulomb repulsion is included into the model.\nUsing the mean field approximation, one can demonstrate that in the absence of\ndoping, the ground state of such a model is insulating and possesses a\ndensity-wave order, either SDW, or CDW. Upon doping, a finite ferromagnetic\npolarization emerges. It is argued that the mechanism driving the\nferromagnetism is not of the Stoner type. A phase diagram of the model is\nconstructed, and various properties of the ordered phases are studied.", "AI": {"tldr": "\u7406\u8bba\u7814\u7a76\u4e86\u5177\u6709\u5b8c\u7f8e\u8d39\u7c73\u9762\u5d4c\u5957\u7684\u76f8\u4e92\u4f5c\u7528\u8d39\u7c73\u5b50\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u57fa\u6001\u4e3a\u7edd\u7f18\u4f53\uff08SDW\u6216CDW\uff09\uff0c\u63ba\u6742\u540e\u51fa\u73b0\u975eStoner\u578b\u7684\u94c1\u78c1\u6027\u3002", "motivation": "\u7406\u8bba\u7814\u7a76\u8d39\u7c73\u9762\u5b8c\u7f8e\u5d4c\u5957\u7684\u76f8\u4e92\u4f5c\u7528\u8d39\u7c73\u5b50\u6a21\u578b\uff0c\u4ee5\u7406\u89e3\u5176\u5728\u4e0d\u540c\u63ba\u6742\u4e0b\u7684\u57fa\u6001\u548c\u94c1\u78c1\u5316\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5e73\u5747\u573a\u8fd1\u4f3c\u6765\u7406\u8bba\u7814\u7a76\u4e00\u4e2a\u5305\u542b\u76f8\u4e92\u4f5c\u7528\u7684\u7535\u5b50\u548c\u7a7a\u7a74\u7684\u6a21\u578b\uff0c\u5176\u8d39\u7c73\u9762\u662f\u5b8c\u7f8e\u5d4c\u5957\u7684\u3002\u4e3a\u4e86\u6291\u5236\u4e0d\u5747\u5300\u72b6\u6001\uff0c\u6a21\u578b\u4e2d\u5305\u542b\u8db3\u591f\u5f3a\u7684\u957f\u7a0b\u5e93\u4ed1\u6392\u65a5\u529b\u3002", "result": "\u6784\u5efa\u4e86\u8be5\u6a21\u578b\u7684\u76f8\u56fe\uff0c\u5e76\u7814\u7a76\u4e86\u5404\u79cd\u6709\u5e8f\u76f8\u7684\u6027\u8d28\u3002\u6a21\u578b\u8868\u660e\uff0c\u63ba\u6742\u4f1a\u5bfc\u81f4\u6709\u9650\u7684\u94c1\u78c1\u6781\u5316\u3002", "conclusion": "\u8be5\u6a21\u578b\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u6ca1\u6709\u63ba\u6742\u7684\u60c5\u51b5\u4e0b\uff0c\u8d39\u7c73\u9762\u5b8c\u7f8e\u5d4c\u5957\u7684\u76f8\u4e92\u4f5c\u7528\u8d39\u7c73\u5b50\u7684\u57fa\u6001\u662f\u7edd\u7f18\u7684\uff0c\u5e76\u5177\u6709\u5bc6\u5ea6\u6ce2\u5e8f\uff08SDW\u6216CDW\uff09\u3002\u5728\u63ba\u6742\u540e\uff0c\u4f1a\u51fa\u73b0\u6709\u9650\u7684\u94c1\u78c1\u6781\u5316\uff0c\u5e76\u4e14\u9a71\u52a8\u94c1\u78c1\u5316\u7684\u673a\u5236\u5e76\u975e Stoner \u578b\u3002"}}
{"id": "2507.11633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11633", "abs": "https://arxiv.org/abs/2507.11633", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "comment": "8 pages, ICML MAS workshop", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience.", "AI": {"tldr": "A modular harness for LLM agents (perception, memory, reasoning) improves performance in diverse games without special engineering. It helps analyze module impact, showing memory is key for long puzzles and perception for noisy games.", "motivation": "To create a unified workflow for analyzing how different modules (perception, memory, reasoning) affect the performance of LLM/VLM agents across diverse, dynamic, interactive settings, particularly in multi-turn gaming environments, without requiring domain-specific engineering.", "method": "A modular harness design for LLM agents is introduced, composed of perception, memory, and reasoning components. This harness allows a single LLM or VLM backbone to address various multi-turn gaming environments. The framework facilitates analysis of module performance using classic and modern game suites as testbeds.", "result": "Extensive experiments show the harness consistently improves gameplay performance over unharnessed baselines. Specific findings include memory's dominance in long-horizon puzzles and perception's criticality in vision-noisy arcades, highlighting the distinct contribution patterns of each module.", "conclusion": "The modular harness design effectively advances general-purpose agents by enabling a single LLM or VLM backbone to handle diverse gaming environments without domain-specific engineering. The framework allows for analysis of individual module contributions, showing that memory is crucial for long-horizon tasks and perception for visually noisy ones."}}
{"id": "2507.11749", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11749", "abs": "https://arxiv.org/abs/2507.11749", "authors": ["Jonathan Olivares", "Tyler Depe", "Rakeshkumar Mahto"], "title": "Reconfigurable Battery Systems for Enhanced Fast Charging in Electric Vehicles", "comment": null, "summary": "The adoption of electric vehicles (EVs) is rapidly growing as a key solution\nto reducing greenhouse gas emissions. However, prolonged charging times remain\na significant barrier to widespread EV usage, especially for individuals\nwithout access to fast charging infrastructure. This paper explores the\npotential of reconfigurable battery systems to reduce EV charging times without\ncompromising battery life. We propose innovative battery pack configurations\nthat dynamically adjust the arrangement of cells to optimize charging\nperformance. Simulations were conducted using MATLAB and Simulink to compare\nthe efficiency of various battery configurations, focusing on charging times,\nstate of charge (SOC), voltage, and current under different conditions. The\nresults demonstrate that connecting more batteries in series through\nreconfigurability in battery packs can significantly reduce charging times\nwhile maintaining operational safety. This study offers insights into how\nreconfigurable battery designs can provide a practical solution for faster,\nmore efficient home-based EV charging, making EV ownership more accessible and\nsustainable.", "AI": {"tldr": "This paper investigates reconfigurable battery systems to reduce EV charging times, proposing new configurations and using simulations to show that increased series connections can speed up charging safely, improving EV accessibility and sustainability.", "motivation": "Prolonged charging times for electric vehicles (EVs) are a significant barrier to widespread adoption, particularly for those without access to fast charging infrastructure.", "method": "The paper proposes innovative battery pack configurations that dynamically adjust the arrangement of cells to optimize charging performance. Simulations were conducted using MATLAB and Simulink to compare the efficiency of various battery configurations, focusing on charging times, state of charge (SOC), voltage, and current under different conditions.", "result": "Connecting more batteries in series through reconfigurability in battery packs can significantly reduce charging times while maintaining operational safety.", "conclusion": "The study shows that reconfigurable battery designs can offer a practical solution for faster, more efficient home-based EV charging, making EV ownership more accessible and sustainable."}}
{"id": "2507.11843", "categories": ["physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.11843", "abs": "https://arxiv.org/abs/2507.11843", "authors": ["Tao Wang", "Nan Zhang", "Hongjie Huang", "Yunhe An", "Yunyun Dai", "Yongrui Li", "Nan Yang", "Chaojie Yang", "Xinran Zhou", "Yucheng Zhu", "Yingshan Ma", "Lingling Huang", "Yongtian Wang", "Yang Liu", "Zhiyong Yan"], "title": "Magneto-photoelectrochemical 2D heterojunction platform for biosensing detection", "comment": null, "summary": "Photoelectrochemical (PEC) biosensors exhibit significant potential for\nbiomolecule detection due to their high sensitivity and low background noise.\nHowever, their performance is severely constrained by the rapid recombination\nof photogenerated charge carriers. This study innovatively introduces a\nnon-contact magnetic modulation strategy to suppress electron-hole\nrecombination by manipulating carrier spin states, thereby significantly\nenhancing photoelectric conversion efficiency. Building on this mechanism, we\ndeveloped a novel magnetically modulated PEC biosensing platform based on the\nMXenes/cobalt-doped titanium dioxide (Co-TiO2) heterostructure. This platform\nachieved ultrasensitive detection of protein kinase A (PKA) activity. Compared\nto an identical probe-modified biosensor without magnetic field application,\nthe developed platform demonstrated a 68.75% enhancement in detection\nsensitivity and achieved an ultralow detection limit for PKA of 0.00016 U/mL.\nIt also exhibited a wide linear range from 0.005 to 80 U/mL. This research not\nonly provides a novel methodology for kinase activity analysis but also\npioneers the innovative strategy of magnetic modulation for enhanced PEC\nsensing. It opens new avenues for developing high-performance biosensing\nplatforms, holding significant promise for early disease diagnosis and drug\nscreening applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u78c1\u573a\u8c03\u5236\u7b56\u7565\uff0c\u5229\u7528MXenes/Co-TiO2\u5f02\u8d28\u7ed3\u6784\u663e\u8457\u63d0\u9ad8\u4e86PEC\u751f\u7269\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9PKA\u7684\u8d85\u7075\u654f\u68c0\u6d4b\uff0c\u4e3a\u751f\u7269\u4f20\u611f\u548c\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5149\u7535\u5316\u5b66\uff08PEC\uff09\u751f\u7269\u4f20\u611f\u5668\u867d\u7136\u5177\u6709\u9ad8\u7075\u654f\u5ea6\u548c\u4f4e\u80cc\u666f\u566a\u58f0\u7684\u4f18\u70b9\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u5149\u751f\u8f7d\u6d41\u5b50\u590d\u5408\u7684\u4e25\u91cd\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u63a5\u89e6\u5f0f\u78c1\u573a\u8c03\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u64cd\u7eb5\u8f7d\u6d41\u5b50\u81ea\u65cb\u6001\u6765\u6291\u5236\u7535\u5b50-\u7a7a\u7a74\u590d\u5408\uff0c\u5e76\u5728\u6b64\u673a\u5236\u4e0a\u5f00\u53d1\u4e86\u57fa\u4e8eMXenes/\u94b4\u63ba\u6742\u4e8c\u6c27\u5316\u949b\uff08Co-TiO2\uff09\u5f02\u8d28\u7ed3\u6784\u7684\u65b0\u578b\u78c1\u573a\u8c03\u5236\u5149\u7535\u5316\u5b66\uff08PEC\uff09\u751f\u7269\u4f20\u611f\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u5bf9\u86cb\u767d\u6fc0\u9176A\uff08PKA\uff09\u6d3b\u6027\u7684\u8d85\u7075\u654f\u68c0\u6d4b\u3002", "result": "\u4e0e\u672a\u65bd\u52a0\u78c1\u573a\u7684\u63a2\u9488\u4fee\u9970\u751f\u7269\u4f20\u611f\u5668\u76f8\u6bd4\uff0c\u6240\u5f00\u53d1\u7684\u5e73\u53f0\u68c0\u6d4b\u7075\u654f\u5ea6\u63d0\u9ad8\u4e8668.75%\uff0cPKA\u7684\u8d85\u4f4e\u68c0\u6d4b\u9650\u4e3a0.00016 U/mL\uff0c\u7ebf\u6027\u8303\u56f4\u4e3a0.005\u81f380 U/mL\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u78c1\u573a\u8c03\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u64cd\u7eb5\u8f7d\u6d41\u5b50\u81ea\u65cb\u6001\u6765\u6291\u5236\u7535\u5b50-\u7a7a\u7a74\u590d\u5408\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u5149\u7535\u8f6c\u6362\u6548\u7387\u3002\u8be5\u7b56\u7565\u5728MXenes/\u94b4\u63ba\u6742\u4e8c\u6c27\u5316\u949b\uff08Co-TiO2\uff09\u5f02\u8d28\u7ed3\u6784\u7684\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u78c1\u573a\u8c03\u5236\u7684\u86cb\u767d\u6fc0\u9176A\uff08PKA\uff09\u6d3b\u6027\u68c0\u6d4b\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u5bf9PKA\u7684\u8d85\u7075\u654f\u68c0\u6d4b\uff0c\u68c0\u6d4b\u7075\u654f\u5ea6\u63d0\u9ad8\u4e8668.75%\uff0cPKA\u7684\u8d85\u4f4e\u68c0\u6d4b\u9650\u4e3a0.00016 U/mL\uff0c\u7ebf\u6027\u8303\u56f4\u5bbd\uff0c\u4e3a\u6fc0\u9176\u6d3b\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u4e3a\u9ad8\u6027\u80fd\u751f\u7269\u4f20\u611f\u5e73\u53f0\u7684\u5f00\u53d1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6709\u671b\u7528\u4e8e\u65e9\u671f\u75be\u75c5\u8bca\u65ad\u548c\u836f\u7269\u7b5b\u9009\u3002"}}
{"id": "2507.12028", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.12028", "abs": "https://arxiv.org/abs/2507.12028", "authors": ["Soheil Mahdizadeh", "Elyas Oustad", "Mohsen Ansari"], "title": "MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments", "comment": null, "summary": "Task offloading in three-layer fog computing environments presents a critical\nchallenge due to user equipment (UE) mobility, which frequently triggers costly\nservice migrations and degrades overall system performance. This paper\naddresses this problem by proposing MOFCO, a novel Mobility- and\nMigration-aware Task Offloading algorithm for Fog Computing environments. The\nproposed method formulates task offloading and resource allocation as a\nMixed-Integer Nonlinear Programming (MINLP) problem and employs a\nheuristic-aided evolutionary game theory approach to solve it efficiently. To\nevaluate MOFCO, we simulate mobile users using SUMO, providing realistic\nmobility patterns. Experimental results show that MOFCO reduces system cost,\ndefined as a combination of latency and energy consumption, by an average of\n19% and up to 43% in certain scenarios compared to state-of-the-art methods.", "AI": {"tldr": "MOFCO \u662f\u4e00\u79cd\u65b0\u7684\u96fe\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u7b97\u6cd5\uff0c\u53ef\u611f\u77e5\u79fb\u52a8\u6027\u548c\u8fc1\u79fb\uff0c\u901a\u8fc7\u8fdb\u5316\u535a\u5f08\u8bba\u89e3\u51b3 MINLP \u95ee\u9898\uff0c\u5728\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u597d 19%\u3002", "motivation": "\u79fb\u52a8\u6027\u7ed9\u4e09\u5c42\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u5e26\u6765\u4e86\u6311\u6218\uff0c\u4f1a\u5bfc\u81f4\u670d\u52a1\u8fc1\u79fb\u6210\u672c\u9ad8\u6602\u5e76\u964d\u4f4e\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002", "method": "MOFCO \u7b97\u6cd5\u5c06\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u5236\u5b9a\u4e3a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u53d7\u542f\u53d1\u4e8e\u8fdb\u5316\u535a\u5f08\u8bba\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u6709\u6548\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMOFCO \u5c06\u7cfb\u7edf\u6210\u672c\uff08\u5ef6\u8fdf\u548c\u80fd\u8017\uff09\u5e73\u5747\u964d\u4f4e\u4e86 19%\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6700\u9ad8\u53ef\u8fbe 43%\u3002", "conclusion": "MOFCO \u901a\u8fc7\u7ed3\u5408\u8fc1\u79fb\u611f\u77e5\u548c\u79fb\u52a8\u6027\uff0c\u5728\u4e09\u5c42\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u5c06\u7cfb\u7edf\u6210\u672c\uff08\u5ef6\u8fdf\u548c\u80fd\u8017\uff09\u5e73\u5747\u964d\u4f4e 19%\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u9ad8\u8fbe 43%\u3002"}}
{"id": "2507.12063", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.12063", "abs": "https://arxiv.org/abs/2507.12063", "authors": ["Naoki Shibao", "Sho Tsugawa"], "title": "Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns", "comment": null, "summary": "A wide variety of information is disseminated through social media, and\ncontent that spreads at scale can have tangible effects on the real world. To\ncurb the spread of harmful content and promote the dissemination of reliable\ninformation, research on cascade graph mining has attracted increasing\nattention. A promising approach in this area is Contrastive Cascade Graph\nLearning (CCGL). One important task in cascade graph mining is cascade\nclassification, which involves categorizing cascade graphs based on their\nstructural characteristics. Although CCGL is expected to be effective for this\ntask, its performance has not yet been thoroughly evaluated. This study aims to\ninvestigate the effectiveness of CCGL for cascade classification. Our findings\ndemonstrate the strong performance of CCGL in capturing platform- and\nmodel-specific structural patterns in cascade graphs, highlighting its\npotential for a range of downstream information diffusion analysis tasks.", "AI": {"tldr": "CCGL\u662f\u4e00\u79cd\u7528\u4e8e\u7ea7\u8054\u56fe\u5206\u7c7b\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6355\u6349\u7ed3\u6784\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u904f\u5236\u6709\u5bb3\u5185\u5bb9\u7684\u4f20\u64ad\u548c\u4fc3\u8fdb\u53ef\u9760\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u5bf9\u7ea7\u8054\u56fe\u6316\u6398\u7684\u7814\u7a76\u5438\u5f15\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u5176\u4e2d\uff0c\u5bf9\u6bd4\u7ea7\u8054\u56fe\u5b66\u4e60\uff08CCGL\uff09\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7ea7\u8054\u56fe\u6316\u6398\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u4efb\u52a1\u662f\u7ea7\u8054\u5206\u7c7b\uff0c\u5373\u6839\u636e\u7ea7\u8054\u56fe\u7684\u7ed3\u6784\u7279\u5f81\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\u3002\u5c3d\u7ba1CCGL\u6709\u671b\u5728\u6b64\u4efb\u52a1\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u5bf9\u6bd4\u7ea7\u8054\u56fe\u5b66\u4e60\uff08CCGL\uff09", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cCCGL\u5728\u6355\u6349\u5e73\u53f0\u548c\u6a21\u578b\u7279\u5b9a\u7684\u7ea7\u8054\u56fe\u7ed3\u6784\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CCGL\u5728\u6355\u6349\u5e73\u53f0\u548c\u6a21\u578b\u7279\u5b9a\u7684\u7ea7\u8054\u56fe\u7ed3\u6784\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u4fe1\u606f\u4f20\u64ad\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2507.11560", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11560", "abs": "https://arxiv.org/abs/2507.11560", "authors": ["Xin Wang", "Xiao Huan Li", "Xun Wang"], "title": "A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing", "comment": "6 pages, 4 figures, accepted by ICCC 2025", "summary": "The integration of the Industrial Internet of Things (IIoT) with Artificial\nIntelligence-Generated Content (AIGC) offers new opportunities for smart\nmanufacturing, but it also introduces challenges related to\ncomputation-intensive tasks and low-latency demands. Traditional generative\nmodels based on cloud computing are difficult to meet the real-time\nrequirements of AIGC tasks in IIoT environments, and edge computing can\neffectively reduce latency through task offloading. However, the dynamic nature\nof AIGC tasks, model switching delays, and resource constraints impose higher\ndemands on edge computing environments. To address these challenges, this paper\nproposes an AIGC task offloading framework tailored for IIoT edge computing\nenvironments, considering the latency and energy consumption caused by AIGC\nmodel switching for the first time. IIoT devices acted as multi-agent\ncollaboratively offload their dynamic AIGC tasks to the most appropriate edge\nservers deployed with different generative models. A model aware AIGC task\noffloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG-MATO) is devised to minimize the latency and energy. Experimental\nresults show that MADDPG-MATO outperforms baseline algorithms, achieving an\naverage reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%\nincrease in task completion rate across four sets of experiments with model\nnumbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is\nrobust and efficient in dynamic, high-load IIoT environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9IIoT\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684AIGC\u4efb\u52a1\u5378\u8f7d\u6846\u67b6\u548cMADDPG-MATO\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u73af\u5883\u4e0b\u7684AIGC\u4efb\u52a1\u9762\u4e34\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\u6311\u6218\uff0c\u4ee5\u53ca\u4f20\u7edf\u4e91\u5e73\u53f0\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u7684\u95ee\u9898\uff0c\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u5206\u8f7d\u4efb\u52a1\u4ee5\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u8003\u8651AIGC\u6a21\u578b\u5207\u6362\u5e26\u6765\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdAIGC\u4efb\u52a1\u5378\u8f7d\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08MADDPG-MATO\uff09\u7684\u611f\u77e5\u6a21\u578b\u5378\u8f7d\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMADDPG-MATO\u7b97\u6cd5\u7684\u5ef6\u8fdf\u5e73\u5747\u964d\u4f4e6.98%\uff0c\u80fd\u8017\u5e73\u5747\u964d\u4f4e7.12%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u5e73\u5747\u63d0\u9ad83.72%\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u52a8\u6001\u3001\u9ad8\u8d1f\u8f7d\u7684IIoT\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MADDPG-MATO\u7b97\u6cd5\u5728\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5728\u52a8\u6001\u3001\u9ad8\u8d1f\u8f7d\u7684IIoT\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.11623", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11623", "abs": "https://arxiv.org/abs/2507.11623", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "title": "A Roadmap for Climate-Relevant Robotics Research", "comment": null, "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.", "AI": {"tldr": "\u673a\u5668\u4eba\u5b66\u53ef\u4ee5\u4e3a\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u505a\u51fa\u8d21\u732e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8def\u7ebf\u56fe\uff0c\u5217\u51fa\u4e86\u673a\u5668\u4eba\u6280\u672f\u5728\u80fd\u6e90\u3001\u5efa\u7b51\u3001\u4ea4\u901a\u3001\u5de5\u4e1a\u3001\u571f\u5730\u5229\u7528\u548c\u5730\u7403\u79d1\u5b66\u7b49\u9886\u57df\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u7684\u5e94\u7528\u548c\u5408\u4f5c\u673a\u4f1a\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u662f21\u4e16\u7eaa\u9762\u4e34\u7684\u91cd\u5927\u6311\u6218\uff0c\u673a\u5668\u4eba\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u6b63\u5728\u5bfb\u6c42\u4e3a\u5176\u505a\u51fa\u8d21\u732e\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u9ad8\u5f71\u54cd\u529b\u9886\u57df\u548c\u5177\u4f53\u53ef\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u6765\u6fc0\u53d1\u673a\u5668\u4eba\u5b66\u548c\u6c14\u5019\u79d1\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u65b0\u7814\u7a76\u65b9\u5411\u548c\u5408\u4f5c\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u80fd\u6e90\u7cfb\u7edf\u4f18\u5316\u3001\u5efa\u7b51\u3001\u7cbe\u51c6\u519c\u4e1a\u3001\u5efa\u7b51\u56f4\u62a4\u7ed3\u6784\u6539\u9020\u3001\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u548c\u5927\u89c4\u6a21\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u7684\u8def\u7ebf\u56fe\u3002\u5b83\u5f3a\u8c03\u4e0d\u4ec5\u8981\u5e94\u7528\u5b9e\u4f53\u673a\u5668\u4eba\uff0c\u8fd8\u8981\u5e94\u7528\u673a\u5668\u4eba\u5b66\u7684\u89c4\u5212\u3001\u611f\u77e5\u3001\u63a7\u5236\u548c\u4f30\u8ba1\u7b97\u6cd5\u7b49\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u6c14\u5019\u76f8\u5173\u7684\u673a\u5668\u4eba\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u8def\u7ebf\u56fe\uff0c\u660e\u786e\u4e86\u5728\u80fd\u6e90\u3001\u5efa\u7b51\u73af\u5883\u3001\u4ea4\u901a\u3001\u5de5\u4e1a\u3001\u571f\u5730\u5229\u7528\u548c\u5730\u7403\u79d1\u5b66\u7b49\u9886\u57df\uff0c\u673a\u5668\u4eba\u4e13\u5bb6\u4e0e\u6c14\u5019\u4e13\u5bb6\u7684\u5408\u4f5c\u673a\u4f1a\u3002"}}
{"id": "2507.11883", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.11883", "abs": "https://arxiv.org/abs/2507.11883", "authors": ["Yao Zhang", "Indrajit Saha", "Zhaohong Sun", "Makoto Yokoo"], "title": "Coalitions on the Fly in Cooperative Games", "comment": "Full version with all detailed proofs of ECAI 2025 accepted paper\n  with the same title", "summary": "In this work, we examine a sequential setting of a cooperative game in which\nplayers arrive dynamically to form coalitions and complete tasks either\ntogether or individually, depending on the value created. Upon arrival, a new\nplayer as a decision maker faces two options: forming a new coalition or\njoining an existing one. We assume that players are greedy, i.e., they aim to\nmaximize their rewards based on the information available at their arrival. The\nobjective is to design an online value distribution policy that incentivizes\nplayers to form a coalition structure that maximizes social welfare. We focus\non monotone and bounded cooperative games. Our main result establishes an upper\nbound of $\\frac{3\\mathsf{min}}{\\mathsf{max}}$ on the competitive ratio for any\nirrevocable policy (i.e., one without redistribution), and proposes a policy\nthat achieves a near-optimal competitive ratio of $\\min\\left\\{\\frac{1}{2},\n\\frac{3\\mathsf{min}}{\\mathsf{max}}\\right\\}$, where $\\mathsf{min}$ and\n$\\mathsf{max}$ denote the smallest and largest marginal contribution of any\nsub-coalition of players respectively. Finally, we also consider\nnon-irrevocable policies, with alternative bounds only when the number of\nplayers is limited.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u5408\u4f5c\u535a\u5f08\u7684\u5728\u7ebf\u4ef7\u503c\u5206\u914d\u7b56\u7565\uff0c\u65e8\u5728\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\uff0c\u5e76\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u7b56\u7565\u63d0\u4f9b\u4e86\u7ade\u4e89\u6bd4\u7387\u5206\u6790\u3002", "motivation": "\u5728\u73a9\u5bb6\u52a8\u6001\u5230\u8fbe\u548c\u5f62\u6210\u8054\u76df\u4ee5\u5b8c\u6210\u4efb\u52a1\u7684\u5408\u4f5c\u535a\u5f08\u987a\u5e8f\u8bbe\u5b9a\u4e2d\uff0c\u8bbe\u8ba1\u4e00\u79cd\u80fd\u6fc0\u52b1\u73a9\u5bb6\u5f62\u6210\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u8054\u76df\u7ed3\u6784\u7684\u5728\u7ebf\u4ef7\u503c\u5206\u914d\u7b56\u7565\u3002", "method": "\u7814\u7a76\u4e86\u5408\u4f5c\u535a\u5f08\u4e2d\u7684\u987a\u5e8f\u8bbe\u5b9a\uff0c\u73a9\u5bb6\u52a8\u6001\u5730\u5f62\u6210\u8054\u76df\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u7ebf\u4ef7\u503c\u5206\u914d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u65e8\u5728\u6fc0\u52b1\u73a9\u5bb6\u5f62\u6210\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u8054\u76df\u7ed3\u6784\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u5355\u8c03\u548c\u6709\u754c\u5408\u4f5c\u535a\u5f08\u3002", "result": "\u5728\u5355\u8c03\u548c\u6709\u754c\u5408\u4f5c\u535a\u5f08\u4e2d\uff0c\u4efb\u4f55\u4e0d\u53ef\u64a4\u9500\u7b56\u7565\u7684\u7ade\u4e89\u6bd4\u7387\u4e0a\u9650\u4e3a $\\frac{3\\mathsf{min}}{\\mathsf{max}}$\u3002\u6240\u63d0\u51fa\u7684\u7b56\u7565\u7ade\u4e89\u6bd4\u7387\u63a5\u8fd1\u6700\u4f18\uff0c\u4e3a $\\min\\left\\{\\frac{1}{2}, \\frac{3\\mathsf{min}}{\\mathsf{max}}\\right\\}$\u3002", "conclusion": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u7ebf\u4ef7\u503c\u5206\u914d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7ade\u4e89\u6bd4\u7387\uff0c\u5e76\u5728\u6709\u9650\u73a9\u5bb6\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u8003\u8651\u4e86\u975e\u53ef\u64a4\u9500\u7b56\u7565\u3002"}}
{"id": "2507.11857", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11857", "abs": "https://arxiv.org/abs/2507.11857", "authors": ["Benjamin Watson", "Alinda Friedman", "Aaron McGaffey"], "title": "Measuring and predicting visual fidelity", "comment": null, "summary": "This paper is a study of techniques for measuring and predicting visual\nfidelity. As visual stimuli we use polygonal models, and vary their fidelity\nwith two different model simplification algorithms. We also group the stimuli\ninto two object types: animals and man made artifacts. We examine three\ndifferent experimental techniques for measuring these fidelity changes: naming\ntimes, ratings, and preferences. All the measures were sensitive to the type of\nsimplification and level of simplification. However, the measures differed from\none another in their response to object type. We also examine several automatic\ntechniques for predicting these experimental measures, including techniques\nbased on images and on the models themselves. Automatic measures of fidelity\nwere successful at predicting experimental ratings, less successful at\npredicting preferences, and largely failures at predicting naming times. We\nconclude with suggestions for use and improvement of the experimental and\nautomatic measures of visual fidelity.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u6d4b\u91cf\u548c\u9884\u6d4b\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u5b9e\u9a8c\u548c\u81ea\u52a8\u6280\u672f\u3002\u53d1\u73b0\u5b9e\u9a8c\u6280\u672f\u5bf9\u7b80\u5316\u654f\u611f\u4f46\u5bf9\u7269\u4f53\u7c7b\u578b\u53cd\u5e94\u4e0d\u540c\u3002\u81ea\u52a8\u6280\u672f\u64c5\u957f\u9884\u6d4b\u8bc4\u5206\uff0c\u4f46\u5728\u9884\u6d4b\u504f\u597d\u548c\u547d\u540d\u65f6\u95f4\u65b9\u9762\u6548\u679c\u4e0d\u4f73\u3002\u5efa\u8bae\u6539\u8fdb\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u548c\u8bc4\u4f30\u6d4b\u91cf\u548c\u9884\u6d4b\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u4e0d\u540c\u6280\u672f\uff0c\u5e76\u4e86\u89e3\u8fd9\u4e9b\u6280\u672f\u5bf9\u4e0d\u540c\u7269\u4f53\u7c7b\u578b\u548c\u7b80\u5316\u7ea7\u522b\u7684\u54cd\u5e94\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u8fb9\u5f62\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u523a\u6fc0\uff0c\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u7684\u6a21\u578b\u7b80\u5316\u7b97\u6cd5\uff08\u672a\u5177\u4f53\u8bf4\u660e\uff09\u6539\u53d8\u5176\u4fdd\u771f\u5ea6\u3002\u7814\u7a76\u5c06\u523a\u6fc0\u5206\u4e3a\u52a8\u7269\u548c\u4eba\u9020\u7269\u54c1\u4e24\u7c7b\u3002\u5b9e\u9a8c\u6280\u672f\u5305\u62ec\u547d\u540d\u65f6\u95f4\u3001\u8bc4\u5206\u548c\u504f\u597d\u3002\u81ea\u52a8\u9884\u6d4b\u6280\u672f\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u548c\u6a21\u578b\u672c\u8eab\u7684\u6280\u672f\u3002", "result": "\u6240\u6709\u6d4b\u91cf\u65b9\u6cd5\uff08\u547d\u540d\u65f6\u95f4\u3001\u8bc4\u5206\u3001\u504f\u597d\uff09\u90fd\u5bf9\u7b80\u5316\u7c7b\u578b\u548c\u7b80\u5316\u7ea7\u522b\u654f\u611f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6d4b\u91cf\u65b9\u6cd5\u5728\u5bf9\u52a8\u7269\u548c\u4eba\u9020\u7269\u54c1\u8fd9\u4e24\u79cd\u7269\u4f53\u7c7b\u578b\u7684\u54cd\u5e94\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u81ea\u52a8\u6d4b\u91cf\u6280\u672f\u5728\u9884\u6d4b\u5b9e\u9a8c\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5728\u9884\u6d4b\u504f\u597d\u65b9\u9762\u6548\u679c\u4e00\u822c\uff0c\u5728\u9884\u6d4b\u547d\u540d\u65f6\u95f4\u65b9\u9762\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u7528\u4e8e\u6d4b\u91cf\u548c\u9884\u6d4b\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u6280\u672f\u8fdb\u884c\u4e86\u7814\u7a76\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u591a\u8fb9\u5f62\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u523a\u6fc0\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u7684\u6a21\u578b\u7b80\u5316\u7b97\u6cd5\u6765\u6539\u53d8\u5176\u4fdd\u771f\u5ea6\u3002\u7814\u7a76\u8fd8\u5c06\u523a\u6fc0\u5206\u4e3a\u4e24\u7c7b\uff1a\u52a8\u7269\u548c\u4eba\u9020\u7269\u54c1\u3002\u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5b9e\u9a8c\u6280\u672f\u6765\u6d4b\u91cf\u8fd9\u4e9b\u4fdd\u771f\u5ea6\u53d8\u5316\uff1a\u547d\u540d\u65f6\u95f4\u3001\u8bc4\u5206\u548c\u504f\u597d\u3002\u6240\u6709\u6d4b\u91cf\u65b9\u6cd5\u90fd\u5bf9\u7b80\u5316\u7c7b\u578b\u548c\u7b80\u5316\u7ea7\u522b\u654f\u611f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6d4b\u91cf\u65b9\u6cd5\u5728\u5bf9\u7269\u4f53\u7c7b\u578b\u7684\u54cd\u5e94\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u7814\u7a76\u8fd8\u68c0\u67e5\u4e86\u51e0\u79cd\u81ea\u52a8\u6280\u672f\u6765\u9884\u6d4b\u8fd9\u4e9b\u5b9e\u9a8c\u6d4b\u91cf\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u548c\u6a21\u578b\u672c\u8eab\u7684\u6280\u672f\u3002\u81ea\u52a8\u4fdd\u771f\u5ea6\u6d4b\u91cf\u5728\u9884\u6d4b\u5b9e\u9a8c\u8bc4\u5206\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5728\u9884\u6d4b\u504f\u597d\u65b9\u9762\u4e0d\u592a\u6210\u529f\uff0c\u5728\u9884\u6d4b\u547d\u540d\u65f6\u95f4\u65b9\u9762\u57fa\u672c\u5931\u8d25\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u5b9e\u9a8c\u548c\u81ea\u52a8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6d4b\u91cf\u7684\u4f7f\u7528\u548c\u6539\u8fdb\u63d0\u51fa\u4e86\u5efa\u8bae\u3002"}}
{"id": "2507.11625", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MapIQ\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u5730\u56fe\u4e0a\u7684\u89c6\u89c9\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u7684\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\uff08Map-VQA\uff09\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7ea7\u7edf\u8ba1\u5730\u56fe\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8303\u56f4\u548c\u5bf9\u5730\u56fe\u7c7b\u578b\u7684\u8986\u76d6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5305\u542b\u591a\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u4e3b\u9898\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u5730\u56fe\u4e0a\u7684\u8868\u73b0\uff0c\u6765\u63a8\u52a8Map-VQA\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u63a2\u7a76\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u654f\u611f\u6027\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b14,706\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u7684MapIQ\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u5206\u7ea7\u7edf\u8ba1\u5730\u56fe\u3001\u5730\u56fe\u586b\u5145\u56fe\u548c\u6bd4\u4f8b\u7b26\u53f7\u5730\u56fe\u4e09\u79cd\u5730\u56fe\u7c7b\u578b\uff0c\u4ee5\u53ca\u4f4f\u623f\u3001\u72af\u7f6a\u7b49\u516d\u4e2a\u4e0d\u540c\u4e3b\u9898\u3002\u8bc4\u4f30\u4e86\u591a\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u516d\u79cd\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u901a\u8fc7\u989d\u5916\u7684\u5b9e\u9a8c\u7814\u7a76\u4e86\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\uff08\u5982\u989c\u8272\u65b9\u6848\u3001\u56fe\u4f8b\u8bbe\u8ba1\u3001\u5730\u56fe\u5143\u7d20\u79fb\u9664\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728MapIQ\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5730\u56fe\u7c7b\u578b\u548c\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5404\u5f02\u3002\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5904\u7406\u7279\u5b9a\u5730\u56fe\u7c7b\u578b\uff08\u5982\u6bd4\u4f8b\u7b26\u53f7\u5730\u56fe\uff09\u548c\u6267\u884c\u67d0\u4e9b\u5206\u6790\u4efb\u52a1\uff08\u5982\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\uff09\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u793a\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002", "conclusion": "MapIQ\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u540c\u5730\u56fe\u7c7b\u578b\u548c\u5e94\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u65b9\u9762\u3002"}}
{"id": "2507.11751", "categories": ["cs.NE", "cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11751", "abs": "https://arxiv.org/abs/2507.11751", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Identifying similar documents within extensive volumes of data poses a\nsignificant challenge. To tackle this issue, researchers have developed a\nvariety of effective distributed computing techniques. With the advancement of\ncomputing power and the rise of big data, deep neural networks and evolutionary\ncomputing algorithms such as genetic algorithms and differential evolution\nalgorithms have achieved greater success. This survey will explore the most\nrecent advancements in the search for documents based on their semantic text\nsimilarity, focusing on genetic and differential evolutionary computing\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u4e3b\u8981\u63a2\u8ba8\u4e86\u5728\u6587\u6863\u68c0\u7d22\u9886\u57df\uff0c\u5229\u7528\u9057\u4f20\u7b97\u6cd5\u548c\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u8fdb\u884c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u6587\u6863\u8bc6\u522b\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u8bc6\u522b\u76f8\u4f3c\u6587\u6863\u7684\u6311\u6218\u3002", "method": "\u5bf9\u6587\u6863\u7684\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u7814\u7a76\uff0c\u4fa7\u91cd\u4e8e\u9057\u4f20\u7b97\u6cd5\u548c\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u3002", "result": "\u6df1\u5165\u63a2\u8ba8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u8fdb\u5316\u8ba1\u7b97\u7b97\u6cd5\uff08\u5982\u9057\u4f20\u7b97\u6cd5\u548c\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\uff09\u5728\u6587\u6863\u76f8\u4f3c\u6027\u641c\u7d22\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u5bf9\u57fa\u4e8e\u9057\u4f20\u548c\u5dee\u5206\u8fdb\u5316\u8ba1\u7b97\u7b97\u6cd5\u7684\u6587\u6863\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002"}}
{"id": "2507.11550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11550", "abs": "https://arxiv.org/abs/2507.11550", "authors": ["Hyeonseok Jin", "Geonmin Kim", "Kyungbaek Kim"], "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction", "comment": "7 pages", "summary": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.", "AI": {"tldr": "DDCN\u662f\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u65b0\u578b\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u4ea4\u901a\u9884\u6d4b\u4e2d\u9700\u8981\u9884\u5b9a\u4e49\u90bb\u63a5\u77e9\u9635\u3001\u96be\u4ee5\u6355\u6349\u533a\u57df\u548c\u65f6\u95f4\u6bb5\u53d8\u5316\u7684\u4ea4\u901a\u6a21\u5f0f\u5f02\u8d28\u6027\u4ee5\u53ca\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u5b58\u5728\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aDeformable Dynamic Convolution Network (DDCN)\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u91c7\u7528\u7c7b\u4f3cTransformer\u7684CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5e94\u7528\u57fa\u4e8e\u504f\u79fb\u91cf\u7684\u53ef\u53d8\u5f62\u6ee4\u6ce2\u5668\u6765\u89e3\u51b3\u4f20\u7edfCNN\u5728\u5efa\u6a21\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7ed3\u6784\u548c\u65f6\u7a7a\u5f02\u8d28\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002DDCN\u901a\u8fc7\u5728\u7f16\u7801\u5668\u7684\u7a7a\u95f4\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u5e94\u7528\u63d0\u51fa\u7684\u65b9\u6cd5\u6765\u5f3a\u8c03\u91cd\u8981\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u524d\u9988\u6a21\u5757\u7684\u89e3\u7801\u5668\u6765\u8865\u5145\u7f16\u7801\u5668\u7684\u8f93\u51fa\u3002", "result": "DDCN\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "DDCN\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u5728\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.11906", "categories": ["cs.MA", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11906", "abs": "https://arxiv.org/abs/2507.11906", "authors": ["Tadahiro Taniguchi", "Masatoshi Nagano", "Haruumi Omoto", "Yoshiki Hayashi"], "title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models", "comment": null, "summary": "Collective human activities like using an Ouija board (or Kokkuri-san) often\nproduce emergent, coherent linguistic outputs unintended by any single\nparticipant. While psychological explanations such as the ideomotor effect\nexist, a computational understanding of how decentralized, implicit linguistic\nknowledge fuses through shared physical interaction remains elusive. We\nintroduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this\nphenomenon as collective Langevin dynamics sampling from implicitly fused\nlanguage models. Each participant is represented as an agent associated with an\nenergy landscape derived from an internal language model reflecting linguistic\npriors, and agents exert stochastic forces based on local energy gradients. We\ntheoretically prove that the collective motion of the shared pointer\n(planchette) corresponds to Langevin MCMC sampling from the sum of individual\nenergy landscapes, representing fused collective knowledge. Simulations\nvalidate that CoCre-Sam dynamics effectively fuse different models and generate\nmeaningful character sequences, while ablation studies confirm the essential\nroles of collective interaction and stochasticity. Altogether, CoCre-Sam\nprovides a novel computational mechanism linking individual implicit knowledge,\nembodied collective action, and emergent linguistic phenomena, grounding these\ncomplex interactions in the principles of probabilistic sampling.", "AI": {"tldr": "CoCre-Sam \u6846\u67b6\u5c06\u96c6\u4f53\u4eba\u7c7b\u8bed\u8a00\u73b0\u8c61\uff08\u5982\u4f7f\u7528\u901a\u7075\u677f\uff09\u5efa\u6a21\u4e3a\u96c6\u4f53 Langevin \u52a8\u529b\u5b66\u91c7\u6837\uff0c\u6709\u6548\u5730\u878d\u5408\u4e86\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4ece\u6982\u7387\u91c7\u6837\u539f\u7406\u4e2d\u4ea7\u751f\u4e86\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002", "motivation": "\u867d\u7136\u5b58\u5728\u5fc3\u7406\u5b66\u89e3\u91ca\uff08\u5982\u610f\u5ff5\u8fd0\u52a8\u6548\u5e94\uff09\uff0c\u4f46\u53bb\u4e2d\u5fc3\u5316\u3001\u9690\u5f0f\u8bed\u8a00\u77e5\u8bc6\u5982\u4f55\u901a\u8fc7\u5171\u4eab\u7269\u7406\u4ea4\u4e92\u878d\u5408\u7684\u8ba1\u7b97\u7406\u89e3\u4ecd\u7136\u96be\u4ee5\u6349\u6478\u3002", "method": " CoCre-Sam\uff08\u96c6\u4f53\u751f\u7269\u91c7\u6837\uff09\u6846\u67b6\u5c06\u6b64\u73b0\u8c61\u5efa\u6a21\u4e3a\u4ece\u9690\u5f0f\u878d\u5408\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96c6\u4f53 Langevin \u52a8\u529b\u5b66\u91c7\u6837\u3002\u6bcf\u4e2a\u53c2\u4e0e\u8005\u90fd\u8868\u793a\u4e3a\u4e0e\u80fd\u91cf\u666f\u89c2\u76f8\u5173\u7684\u4ee3\u7406\uff0c\u8be5\u666f\u89c2\u6e90\u81ea\u53cd\u6620\u8bed\u8a00\u5148\u9a8c\u7684\u5185\u90e8\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e14\u4ee3\u7406\u6839\u636e\u5c40\u90e8\u80fd\u91cf\u68af\u5ea6\u65bd\u52a0\u968f\u673a\u529b\u3002", "result": "\u6a21\u62df\u9a8c\u8bc1\u4e86 CoCre-Sam \u52a8\u529b\u5b66\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u4e0d\u540c\u7684\u6a21\u578b\u5e76\u751f\u6210\u6709\u610f\u4e49\u7684\u5b57\u7b26\u5e8f\u5217\uff0c\u800c\u90e8\u5206\u79fb\u9664\u7814\u7a76\u5219\u8bc1\u5b9e\u4e86\u96c6\u4f53\u4e92\u52a8\u548c\u968f\u673a\u6027\u7684\u57fa\u672c\u4f5c\u7528\u3002", "conclusion": "CoCre-Sam \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u5c06\u4e2a\u4f53\u9690\u6027\u77e5\u8bc6\u3001\u5177\u8eab\u96c6\u4f53\u884c\u4e3a\u548c\u6d8c\u73b0\u7684\u8bed\u8a00\u73b0\u8c61\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5c06\u8fd9\u4e9b\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\u4e0e\u6982\u7387\u91c7\u6837\u539f\u7406\u76f8\u7ed3\u5408\u3002"}}
{"id": "2507.12142", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "RiemannLoRA\u901a\u8fc7\u5c06LoRA\u9002\u914d\u5668\u89c6\u4e3a\u6d41\u5f62\u5e76\u5229\u7528\u9ece\u66fc\u4f18\u5316\u6765\u6539\u8fdb\u521d\u59cb\u5316\u548c\u5904\u7406\u8fc7\u53c2\u6570\u5316\uff0c\u4ece\u800c\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3LoRA\u5728\u521d\u59cb\u5316\u548c\u8fc7\u53c2\u6570\u5316\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u5c06LoRA\u77e9\u9635\u89c6\u4e3a\u6d41\u5f62\uff0c\u5e76\u901a\u8fc7\u9ece\u66fc\u4f18\u5316\u8fdb\u884c\u521d\u59cb\u5316\u548c\u66f4\u65b0\u3002", "result": "RiemannLoRA\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u4e0a\u5747\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "RiemannLoRA\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u4e0a\u5747\u4f18\u4e8eLoRA\u53ca\u5176\u53d8\u4f53\u3002"}}
{"id": "2507.11577", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11577", "abs": "https://arxiv.org/abs/2507.11577", "authors": ["Tom Scruby"], "title": "Visualising Quantum Product Codes", "comment": "20 pages, mostly pictures", "summary": "The hypergraph product (HGP) is a famous code construction technique with an\nequally famous canonical visualisation. This visual perspective provides much\nmore than simply a way to build intuition: HGP codes can be defined\ngraphically, properties can be demonstrated graphically, and approaches to\nfault-tolerant logic have been developed graphically. In recent years two\npowerful generalisations of this product -- the lifted and balanced products --\nhave been proposed and employed to great success, but a unified graphical\napproach to describing these codes has been absent. In these notes I review the\ncanonical approach to visualising HGP codes and then show how, via the addition\nof a third dimension, it can be generalised to apply to both the lifted and the\nbalanced product. In the process we obtain clear intuition into various\nproperties of these codes such as i) why it is hard to bound $k$ and $d$ ii)\nthe issues with finding a canonical logical basis in the general case and iii)\nhow the two products are related, and how they differ. I have attempted to\nstructure these notes plainly and directly, so that the visual intuition can be\neasily obtained by those who want it while the rigorous justification is still\navailable to those who demand it.", "AI": {"tldr": "\u672c\u6587\u5c06\u8d85\u56fe\u4e58\u79ef\uff08HGP\uff09\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u63a8\u5e7f\u5230the lifted\u548cthe balanced\u4e58\u79ef\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56fe\u5f62\u5316\u63cf\u8ff0\uff0c\u5e76\u9610\u91ca\u4e86\u5b83\u4eec\u7684\u5404\u79cd\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709\u4e24\u79cd\u5f3a\u5927\u7684\u8d85\u56fe\u4e58\u79ef\uff08HGP\uff09\u7684\u6cdb\u5316\u2014\u2014the lifted\u548cthe balanced\u4e58\u79ef\uff0c\u867d\u7136\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u56fe\u5f62\u5316\u63cf\u8ff0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u53ef\u89c6\u5316HGP\u7801\u7684\u7ecf\u5178\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u589e\u52a0\u7b2c\u4e09\u7ef4\u5ea6\u5c06\u5176\u63a8\u5e7f\u5230the lifted\u548cthe balanced\u4e58\u79ef\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56fe\u5f62\u5316\u65b9\u6cd5\u6765\u63cf\u8ff0the lifted\u548cthe balanced\u4e58\u79ef\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u5404\u79cd\u5c5e\u6027\uff0c\u4f8b\u5982\u754c\u5b9a$k$\u548c$d$\u7684\u96be\u5ea6\u3001\u5bfb\u627e\u7ecf\u5178\u903b\u8f91\u57fa\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u8fd9\u4e24\u79cd\u4e58\u79ef\u4e4b\u95f4\u7684\u8054\u7cfb\u548c\u5dee\u5f02\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u6dfb\u52a0\u7b2c\u4e09\u7ef4\u5ea6\uff0c\u5c06\u8d85\u56fe\u4e58\u79ef\uff08HGP\uff09\u7684\u7ecf\u5178\u53ef\u89c6\u5316\u65b9\u6cd5\u63a8\u5e7f\u5230 the lifted \u548c the balanced \u4e58\u79ef\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56fe\u5f62\u5316\u65b9\u6cd5\u6765\u63cf\u8ff0\u8fd9\u4e9b\u4ee3\u7801\u3002\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u5b83\u4eec\u7684\u5c5e\u6027\uff0c\u4f8b\u5982 i) \u4e3a\u4ec0\u4e48\u96be\u4ee5\u754c\u5b9a $k$ \u548c $d$\uff1bii) \u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u5bfb\u627e\u7ecf\u5178\u903b\u8f91\u57fa\u7684\u56f0\u96be\uff1b\u4ee5\u53ca iii) \u8fd9\u4e24\u79cd\u4e58\u79ef\u7684\u5173\u7cfb\u548c\u533a\u522b\u3002"}}
{"id": "2507.11570", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11570", "abs": "https://arxiv.org/abs/2507.11570", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "comment": null, "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "AI": {"tldr": "SurgeryLSTM \u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u5efa\u6a21\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u9884\u6d4b\u62e9\u671f\u810a\u67f1\u624b\u672f\u60a3\u8005\u7684\u505c\u7559\u65f6\u95f4\u65b9\u9762\uff0c\u5c55\u73b0\u4e86\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u548c\u8bc4\u4f30\u9884\u6d4b\u62e9\u671f\u810a\u67f1\u624b\u672f\u60a3\u8005\u505c\u7559\u65f6\u95f4\uff08LOS\uff09\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\uff0c\u5e76\u7740\u91cd\u4e8e\u65f6\u95f4\u5efa\u6a21\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5c06\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u548c XGBoost\uff09\u4e0e\u4ed6\u4eec\u5f00\u53d1\u7684\u6a21\u578b SurgeryLSTM \u8fdb\u884c\u4e86\u6bd4\u8f83\u3002SurgeryLSTM \u662f\u4e00\u4e2a\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u63a9\u7801\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\uff08BiLSTM\uff09\u6a21\u578b\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u7ed3\u6784\u5316\u7684\u56f4\u624b\u672f\u671f\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u3002\u901a\u8fc7\u51b3\u5b9a\u7cfb\u6570\uff08R2\uff09\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u8bc6\u522b\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002", "result": "SurgeryLSTM \u6a21\u578b\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff08R2=0.86\uff09\uff0c\u4f18\u4e8e XGBoost\uff08R2=0.85\uff09\u548c\u57fa\u7ebf\u6a21\u578b\u3002\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u672f\u524d\u4e34\u5e8a\u5e8f\u5217\u4e2d\u6709\u5f71\u54cd\u529b\u7684\u65f6\u95f4\u6bb5\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u8ffd\u8e2a\u54ea\u4e9b\u4e8b\u4ef6\u6216\u7279\u5f81\u5bf9\u6bcf\u4e2a LOS \u9884\u6d4b\u7684\u8d21\u732e\u6700\u5927\u3002\u9aa8\u9abc\u75be\u75c5\u3001\u6162\u6027\u80be\u75c5\u548c\u8170\u690e\u878d\u5408\u672f\u88ab\u786e\u5b9a\u4e3a\u5f71\u54cd LOS \u7684\u6700\u91cd\u8981\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "SurgeryLSTM \u6a21\u578b\u4e3a\u62e9\u671f\u810a\u67f1\u624b\u672f\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684 AI \u89e3\u51b3\u65b9\u6848\u3002\u5c06\u57fa\u4e8e\u65f6\u95f4\u7684\u3001\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6574\u5408\u5230\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u51fa\u9662\u51c6\u5907\u5ea6\u548c\u4e2a\u4f53\u5316\u60a3\u8005\u62a4\u7406\u3002"}}
{"id": "2507.11750", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.11750", "abs": "https://arxiv.org/abs/2507.11750", "authors": ["Zhi Li", "Huiju Lee", "Chris Wolverton", "Yi Xia"], "title": "High-throughput computational framework for lattice dynamics and thermal transport including high-order anharmonicity: an application to cubic and tetragonal inorganic compounds", "comment": "38 pages, 5 figures", "summary": "Accurately predicting lattice thermal conductivity (kL) from first principles\nremains a challenge in identifying materials with extreme thermal behavior.\nWhile modern lattice dynamics methods enable routine predictions of kL within\nthe harmonic approximation and three-phonon scattering framework (HA+3ph),\nreliable results, especially for low-kL compounds, require higher-order\nanharmonic effects, including self-consistent phonon renormalization,\nfour-phonon scattering, and off-diagonal heat flux (SCPH+3,4ph+OD). We present\na high-throughput workflow integrating these effects into a unified framework.\nUsing this, we compute kL for 773 cubic and tetragonal inorganic compounds\nacross diverse chemistries and structures. From 562 dynamically stable\ncompounds, we assess the hierarchical effects of higher-order anharmonicity.\nFor about 60% of materials, HA+3ph predictions closely match those from\nSCPH+3,4ph+OD. However, SCPH corrections often increase kL, sometimes by over 8\ntimes, while four-phonon scattering universally reduces it, occasionally to 15%\nof the HA+3ph value. Off-diagonal contributions are minor in high-kL systems\nbut can exceed 50% of total kL in highly anharmonic, low-kL compounds. We\nhighlight four cases-Rb2TlAlH6, Cu3VS4, CuBr, and Cl2O-exhibiting distinct\nanharmonic behaviors. This work delivers not only a robust workflow for\nhigh-fidelity kL dataset but also a quantitative framework to determine when\nhigher-order effects are essential. The hierarchy of kL results, from the\nHA+3ph to SCPH+3,4ph+OD level, offers a scalable, interpretable route to\ndiscovering next-generation extreme thermal materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u901a\u91cf\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u6750\u6599\u7684\u6676\u683c\u70ed\u5bfc\u7387 (kL)\uff0c\u65b9\u6cd5\u662f\u5c06\u66f4\u9ad8\u7ea7\u522b\u7684\u4e0d\u8c10\u6ce2\u6548\u5e94\uff08\u5982\u56db\u58f0\u5b50\u6563\u5c04\uff09\u7eb3\u5165\u8ba1\u7b97\u4e2d\u3002\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u8fd9\u4e9b\u6548\u5e94\u5982\u4f55\u663e\u8457\u5f71\u54cd kL\uff0c\u5e76\u4e3a\u53d1\u73b0\u5177\u6709\u6781\u7aef\u70ed\u5b66\u7279\u6027\u7684\u65b0\u6750\u6599\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "motivation": "\u51c6\u786e\u5730\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u9884\u6d4b kL \u5bf9\u4e8e\u8bc6\u522b\u5177\u6709\u6781\u7aef\u70ed\u5b66\u884c\u4e3a\u7684\u6750\u6599\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u53ef\u9760\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f4e kL \u5316\u5408\u7269\uff0c\u9700\u8981\u66f4\u9ad8\u7ea7\u522b\u7684\u4e0d\u8c10\u6ce2\u6548\u5e94\u3002", "method": "\u8be5\u5de5\u4f5c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u901a\u91cf\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5c06\u81ea\u6d3d\u58f0\u5b50\u91cd\u6784\u3001\u56db\u58f0\u5b50\u6563\u5c04\u548c\u79bb\u9762\u70ed\u6d41\u7b49\u66f4\u9ad8\u7ea7\u522b\u7684\u4e0d\u8c10\u6ce2\u6548\u5e94\u96c6\u6210\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u8ba1\u7b97 kL\u3002", "result": "\u8be5\u5de5\u4f5c\u8ba1\u7b97\u4e86 773 \u79cd\u7acb\u65b9\u548c\u56db\u65b9\u65e0\u673a\u5316\u5408\u7269\u7684 kL\u3002\u5728 562 \u79cd\u52a8\u6001\u7a33\u5b9a\u7684\u5316\u5408\u7269\u4e2d\uff0c\u7ea6 60% \u7684\u6750\u6599\u7684 HA+3ph \u9884\u6d4b\u4e0e SCPH+3,4ph+OD \u9884\u6d4b\u975e\u5e38\u5339\u914d\u3002\u7136\u800c\uff0cSCPH \u6821\u6b63\u901a\u5e38\u4f1a\u589e\u52a0 kL\uff0c\u6709\u65f6\u4f1a\u589e\u52a0 8 \u500d\u4ee5\u4e0a\uff0c\u800c\u56db\u58f0\u5b50\u6563\u5c04\u666e\u904d\u964d\u4f4e\u4e86 kL\uff0c\u6709\u65f6\u4f1a\u964d\u4f4e\u5230 HA+3ph \u503c\u7684 15%\u3002\u79bb\u9762\u8d21\u732e\u5728\u9ad8 kL \u7cfb\u7edf\u4e2d\u5f88\u5c0f\uff0c\u4f46\u5728\u9ad8\u5ea6\u4e0d\u8c10\u6ce2\u3001\u4f4e kL \u7684\u5316\u5408\u7269\u4e2d\u53ef\u80fd\u8d85\u8fc7\u603b kL \u7684 50%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u901a\u91cf\u7684 \u0926\u0941\u0938\u0930\u0940\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u96c6\u6210\u66f4\u9ad8\u7ea7\u522b\u7684\u4e0d\u8c10\u6ce2\u6548\u5e94\uff0c\u4ee5\u8fdb\u884c\u7cbe\u786e\u7684\u6676\u683c\u70ed\u5bfc\u7387\uff08kL\uff09\u9884\u6d4b\u3002\u5b83\u91cf\u5316\u4e86\u8fd9\u4e9b\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u8bc6\u522b\u5177\u6709\u6781\u7aef\u70ed\u5b66\u884c\u4e3a\u7684\u65b0\u578b\u6750\u6599\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\u3002"}}
{"id": "2507.11846", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11846", "abs": "https://arxiv.org/abs/2507.11846", "authors": ["Yulu Guo", "Tongjia Zhang", "Xiangwen Gu", "Shu Sun", "Meixia Tao", "Ruifeng Gao"], "title": "Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment", "comment": null, "summary": "In this paper, we present detailed low-altitude channel measurements at the\nFR3 band in an outdoor campus environment. Using a time-domain channel sounder\nsystem, we conduct two types of measurements: path loss measurements by moving\nthe transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and\ndirectional power angular spectrum measurements through antenna scanning at\nhalf-power beam width intervals. The path loss analysis across different Rx\nshows that the close-in model outperforms conventional 3GPP models and\nheight-corrected variants, with path loss exponents close to free space values\nindicating line-of-sight dominance. The power angular spectrum measurements\nshow that propagation behavior varies significantly with environmental\nconditions. Closer Rx exhibit stronger sensitivity to ground reflections during\ndownward Tx tilting, while obstructed links display uniform angular\ncharacteristics due to dominant scattering effects, and corridor environments\nproduce asymmetric power distributions. These results indicate that\nlow-altitude propagation is characterized by complex interactions between Tx\nheight and ground scattering mechanisms, providing fundamental insights for\nchannel modeling in emerging mid-band communication systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u5ba4\u5916\u6821\u56ed\u73af\u5883\u4e2d\u8fdb\u884c\u4f4e\u9ad8\u5ea6\u4fe1\u9053\u6d4b\u91cf\uff0c\u5c55\u793a\u4e86\u8def\u5f84\u635f\u8017\u548c\u529f\u7387\u89d2\u8c31\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u7ed3\u679c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u65e8\u5728\u4e3a\u4e2d\u9891\u6bb5\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u6e20\u9053\u6a21\u578b\u7684\u57fa\u672c\u89c1\u89e3\u3002", "motivation": "\u672c\u8bba\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5173\u4e8eFR3\u9891\u6bb5\u5728\u5ba4\u5916\u6821\u56ed\u73af\u5883\u4e2d\u4f4e\u9ad8\u5ea6\u4fe1\u9053\u6d4b\u91cf\u7684\u8be6\u7ec6\u4fe1\u606f\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u65f6\u57df\u4fe1\u9053\u63a2\u6d4b\u7cfb\u7edf\uff0c\u8fdb\u884c\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u6d4b\u91cf\uff1a\u901a\u8fc7\u6cbf26\u4e2a\u70b9\u7684\u5c4b\u9876\u8def\u5f84\u4ee51\u7c73\u95f4\u9694\u79fb\u52a8\u53d1\u5c04\u5668\uff08Tx\uff09\u8fdb\u884c\u7684\u8def\u5f84\u635f\u8017\u6d4b\u91cf\uff0c\u4ee5\u53ca\u901a\u8fc7\u5728\u534a\u529f\u7387\u6ce2\u675f\u5bbd\u5ea6\u95f4\u9694\u5185\u8fdb\u884c\u5929\u7ebf\u626b\u63cf\u800c\u8fdb\u884c\u7684\u5b9a\u5411\u529f\u7387\u89d2\u8c31\u6d4b\u91cf\u3002", "result": "\u8def\u5f84\u635f\u8017\u5206\u6790\u8868\u660e\uff0c\u8fd1\u70b9\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u76843GPP\u6a21\u578b\u53ca\u5176\u9ad8\u5ea6\u6821\u6b63\u7684\u53d8\u4f53\uff0c\u8def\u5f84\u635f\u8017\u6307\u6570\u63a5\u8fd1\u81ea\u7531\u7a7a\u95f4\u503c\uff0c\u8868\u660e\u89c6\u8ddd\u4f20\u64ad\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002\u529f\u7387\u89d2\u8c31\u6d4b\u91cf\u8868\u660e\uff0c\u4f20\u64ad\u884c\u4e3a\u968f\u73af\u5883\u6761\u4ef6\u53d8\u5316\u663e\u8457\uff0c\u9760\u8fd1\u63a5\u6536\u5668\u7684\u63a5\u6536\u5668\u5bf9\u5730\u9762\u53cd\u5c04\u66f4\u654f\u611f\uff0c\u800c\u906e\u6321\u94fe\u8def\u7531\u4e8e\u4e3b\u8981\u7684\u6563\u5c04\u6548\u5e94\u800c\u8868\u73b0\u51fa\u5747\u5300\u7684\u89d2\u5ea6\u7279\u6027\uff0c\u8d70\u5eca\u73af\u5883\u5219\u4ea7\u751f\u4e0d\u5bf9\u79f0\u7684\u529f\u7387\u5206\u5e03\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f4e\u9ad8\u5ea6\u4f20\u64ad\u7684\u7279\u70b9\u662f\u53d1\u5c04\u5668\u9ad8\u5ea6\u4e0e\u5730\u9762\u6563\u5c04\u673a\u5236\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u4e86\u65b0\u5174\u7684\u4e2d\u9891\u6bb5\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6e20\u9053\u6a21\u578b\u7684\u57fa\u672c\u89c1\u89e3\u3002"}}
{"id": "2507.12047", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.12047", "abs": "https://arxiv.org/abs/2507.12047", "authors": ["Michal Dvo\u0159\u00e1k", "Du\u0161an Knop", "Michal Opler", "Jan Pokorn\u00fd", "Ond\u0159ej Such\u00fd", "Krisztina Szil\u00e1gyi"], "title": "Pathfinding in Self-Deleting Graphs", "comment": null, "summary": "In this paper, we study the problem of pathfinding on traversal-dependent\ngraphs, i.e., graphs whose edges change depending on the previously visited\nvertices. In particular, we study \\emph{self-deleting graphs}, introduced by\nCarmesin et al. (Sarah Carmesin, David Woller, David Parker, Miroslav Kulich,\nand Masoumeh Mansouri. The Hamiltonian cycle and travelling salesperson\nproblems with traversal-dependent edge deletion. J. Comput. Sci.), which\nconsist of a graph $G=(V, E)$ and a function $f\\colon V\\rightarrow 2^E$, where\n$f(v)$ is the set of edges that will be deleted after visiting the vertex $v$.\nIn the \\textsc{(Shortest) Self-Deleting $s$-$t$-path} problem we are given a\nself-deleting graph and its vertices $s$ and $t$, and we are asked to find a\n(shortest) path from $s$ to $t$, such that it does not traverse an edge in\n$f(v)$ after visiting $v$ for any vertex $v$.\n  We prove that \\textsc{Self-Deleting $s$-$t$-path} is NP-hard even if the\ngiven graph is outerplanar, bipartite, has maximum degree $3$, bandwidth $2$\nand $|f(v)|\\leq 1$ for each vertex $v$. We show that \\textsc{Shortest\nSelf-Deleting $s$-$t$-path} is W[1]-complete parameterized by the length of the\nsought path and that \\textsc{Self-Deleting $s$-$t$-path} is \\W{1}-complete\nparameterized by the vertex cover number, feedback vertex set number and\ntreedepth. We also show that the problem becomes FPT when we parameterize by\nthe maximum size of $f(v)$ and several structural parameters. Lastly, we show\nthat the problem does not admit a polynomial kernel even for parameterization\nby the vertex cover number and the maximum size of $f(v)$ combined already on\n2-outerplanar graphs.", "AI": {"tldr": "\u5728\u904d\u5386\u4f9d\u8d56\u56fe\uff08\u7279\u522b\u662f\u81ea\u5220\u9664\u56fe\uff09\u4e0a\u5bfb\u627e\u8def\u5f84\u7684\u95ee\u9898\u662fNP\u96be\u7684\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u53c2\u6570\u5316\u4e0b\u662fW[1]-\u5b8c\u5168\u7684\u3002\u4f46\u5f53\u53c2\u6570\u4e3af(v)\u7684\u6700\u5927\u5c3a\u5bf8\u548c\u67d0\u4e9b\u7ed3\u6784\u53c2\u6570\u65f6\uff0c\u95ee\u9898\u662fFPT\u7684\u3002\u8be5\u95ee\u9898\u4e5f\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u6838\u3002  ", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u5373\u5728\u201c\u904d\u5386\u4f9d\u8d56\u56fe\u201d\u4e0a\u8fdb\u884c\u8def\u5f84\u5bfb\u627e\u3002\u8fd9\u7c7b\u56fe\u7684\u7279\u70b9\u662f\u8fb9\u7684\u5b58\u5728\u4e0e\u5426\u4f1a\u968f\u7740\u8bbf\u95ee\u8fc7\u7684\u9876\u70b9\u7684\u53d8\u5316\u800c\u53d8\u5316\uff0c\u8fd9\u4e0e\u4f20\u7edf\u56fe\u641c\u7d22\u95ee\u9898\u6709\u663e\u8457\u533a\u522b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u5173\u6ce8\u7684\u662f\u201c\u81ea\u5220\u9664\u56fe\u201d\uff0c\u5176\u4e2d\u6bcf\u4e2a\u9876\u70b9\u90fd\u5173\u8054\u4e00\u4e2a\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u6307\u5b9a\u4e86\u8bbf\u95ee\u8be5\u9876\u70b9\u540e\u5c06\u88ab\u5220\u9664\u7684\u8fb9\u96c6\u3002\u8fd9\u79cd\u904d\u5386\u4f9d\u8d56\u6027\u4f7f\u5f97\u8def\u5f84\u5bfb\u627e\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u3002\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u8fd9\u7c7b\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u63a2\u7d22\u5728\u4e0d\u540c\u53c2\u6570\u5316\u4e0b\u7684\u53ef\u5904\u7406\u6027\uff0c\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u6d89\u53ca\u52a8\u6001\u6216\u72b6\u6001\u4f9d\u8d56\u6027\u641c\u7d22\u95ee\u9898\u7684\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5b9a\u4e49\u4e86\u201c\u81ea\u5220\u9664\u56fe\u201d\u7684\u6982\u5ff5\uff0c\u5373\u56fe\u4e2d\u8fb9\u7684\u5220\u9664\u4e0e\u5426\u53d6\u51b3\u4e8e\u4e4b\u524d\u8bbf\u95ee\u8fc7\u7684\u9876\u70b9\u3002\u7136\u540e\uff0c\u9488\u5bf9\u201c\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u56fe\u662f\u5916\u5e73\u9762\u3001\u4e8c\u5206\u56fe\u3001\u6700\u5927\u5ea6\u4e3a3\u3001\u5e26\u5bbd\u4e3a2\u4e14\u6bcf\u4e2a\u9876\u70b9\u7684f(v)\u5927\u5c0f\u4e0d\u8d85\u8fc71\u7684\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8be5\u95ee\u9898\u4e5f\u662fNP\u96be\u7684\u3002\u5bf9\u4e8e\u201c\u6700\u77ed\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5f53\u4ee5\u8def\u5f84\u957f\u5ea6\u4e3a\u53c2\u6570\u65f6\uff0c\u8be5\u95ee\u9898\u662fW[1]-\u5b8c\u5168\u7684\u3002\u5bf9\u4e8e\u201c\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5f53\u4ee5\u9876\u70b9\u8986\u76d6\u6570\u3001\u53cd\u9988\u9876\u70b9\u96c6\u6570\u548c\u6811\u6df1\u5ea6\u4e3a\u53c2\u6570\u65f6\uff0c\u8be5\u95ee\u9898\u4e5f\u662fW[1]-\u5b8c\u5168\u7684\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5f53\u4ee5f(v)\u7684\u6700\u5927\u5c3a\u5bf8\u4ee5\u53ca\u51e0\u4e2a\u7ed3\u6784\u53c2\u6570\u4e3a\u53c2\u6570\u65f6\uff0c\u8be5\u95ee\u9898\u662f\u53ef\u5904\u7406\u7684\uff08FPT\uff09\u3002\u6700\u540e\uff0c\u7814\u7a76\u901a\u8fc7\u8bc1\u660e\u5373\u4f7f\u57282-\u5916\u5e73\u9762\u56fe\u4e0a\uff0c\u7ed3\u5408\u9876\u70b9\u8986\u76d6\u6570\u548cf(v)\u7684\u6700\u5927\u5c3a\u5bf8\u4f5c\u4e3a\u53c2\u6570\u65f6\uff0c\u8be5\u95ee\u9898\u4e5f\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u6838\uff0c\u4ece\u800c\u5bf9\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u8fdb\u884c\u4e86\u66f4\u6df1\u5165\u7684\u63a2\u8ba8\u3002", "result": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u201c\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\u5728\u591a\u79cd\u9650\u5236\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u56fe\u4e3a\u5916\u5e73\u9762\u3001\u4e8c\u5206\u56fe\u3001\u6700\u5927\u5ea6\u4e3a3\u3001\u5e26\u5bbd\u4e3a2\u4e14|f(v)|\u22641\uff09\u4ecd\u7136\u662fNP\u96be\u7684\u3002\u8fdb\u4e00\u6b65\u5730\uff0c\u201c\u6700\u77ed\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\u5728\u4ee5\u8def\u5f84\u957f\u5ea6\u4e3a\u53c2\u6570\u65f6\u662fW[1]-\u5b8c\u5168\u7684\uff0c\u800c\u201c\u81ea\u5220\u9664s-t\u8def\u5f84\u201d\u95ee\u9898\u5728\u4ee5\u9876\u70b9\u8986\u76d6\u6570\u3001\u53cd\u9988\u9876\u70b9\u96c6\u6570\u548c\u6811\u6df1\u5ea6\u4e3a\u53c2\u6570\u65f6\u4e5f\u662fW[1]-\u5b8c\u5168\u7684\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u8be5\u95ee\u9898\u5728\u4ee5f(v)\u7684\u6700\u5927\u5c3a\u5bf8\u548c\u51e0\u4e2a\u7ed3\u6784\u53c2\u6570\u4e3a\u53c2\u6570\u65f6\u662fFPT\u7684\u3002\u6700\u540e\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u4ec5\u8003\u86512-\u5916\u5e73\u9762\u56fe\uff0c\u5e76\u4e14\u53c2\u6570\u4e3a\u9876\u70b9\u8986\u76d6\u6570\u548cf(v)\u7684\u6700\u5927\u5c3a\u5bf8\u7684\u7ec4\u5408\uff0c\u8be5\u95ee\u9898\u4e5f\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u6838\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u81ea\u5220\u9664\u56fe\u4e0a\u5bfb\u627es-t\u8def\u5f84\u7684\u95ee\u9898\u662fNP\u96be\u7684\uff0c\u5373\u4f7f\u5728\u7279\u5b9a\u9650\u5236\u6761\u4ef6\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u8868\u660e\u8be5\u95ee\u9898\u5728\u4ee5\u8def\u5f84\u957f\u5ea6\u3001\u9876\u70b9\u8986\u76d6\u6570\u3001\u53cd\u9988\u9876\u70b9\u96c6\u6570\u548c\u6811\u6df1\u5ea6\u4e3a\u53c2\u6570\u65f6\u662fW[1]-\u5b8c\u5168\u7684\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5f53\u4ee5f(v)\u7684\u6700\u5927\u5c3a\u5bf8\u548c\u51e0\u4e2a\u7ed3\u6784\u53c2\u6570\u4e3a\u53c2\u6570\u65f6\uff0c\u8be5\u95ee\u9898\u662fFPT\uff08\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7684\uff09\u3002\u6700\u540e\uff0c\u7814\u7a76\u6307\u51fa\uff0c\u5373\u4f7f\u7ed3\u5408\u4e86\u9876\u70b9\u8986\u76d6\u6570\u548cf(v)\u7684\u6700\u5927\u5c3a\u5bf8\u4f5c\u4e3a\u53c2\u6570\uff0c\u8be5\u95ee\u9898\u57282-\u5916\u5e73\u9762\u56fe\u4e0a\u4e5f\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u6838\u3002"}}
{"id": "2507.11704", "categories": ["cs.LO", "I.2.3"], "pdf": "https://arxiv.org/pdf/2507.11704", "abs": "https://arxiv.org/abs/2507.11704", "authors": ["Jorge Fandinno", "Christoph Glinzer", "Zachary Hansen", "Jan Heuer", "Yuliya Lierler", "Vladimir Lifschitz", "Torsten Schaub", "Tobias Stolzmann"], "title": "Anthem 2.0: Automated Reasoning for Answer Set Programming", "comment": "Accepted to Theory and Practice of Logic Programming (ICLP 2025)", "summary": "Anthem 2.0 is a tool to aid in the verification of logic programs written in\nan expressive fragment of Clingo's input language named mini-gringo, which\nincludes arithmetic operations and simple choice rules but not aggregates. It\ncan translate logic programs into formula representations in the logic of\nhere-and-there, and analyze properties of logic programs such as tightness.\nMost importantly, Anthem 2.0 can support program verification by invoking\nfirst-order theorem provers to confirm that a program adheres to a first-order\nspecification, or to establish strong and external equivalence of programs.\nThis paper serves as an overview of the system's capabilities. We demonstrate\nhow to use Anthem 2.0 effectively and interpret its results.", "AI": {"tldr": "Anthem 2.0 \u662f\u4e00\u4e2a\u7528\u4e8e\u9a8c\u8bc1 mini-gringo \u903b\u8f91\u7a0b\u5e8f\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u7ffb\u8bd1\u7a0b\u5e8f\u3001\u5206\u6790\u5c5e\u6027\u5e76\u4f7f\u7528\u4e00\u9636\u5b9a\u7406\u8bc1\u660e\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "Anthem 2.0 \u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5e2e\u52a9\u9a8c\u8bc1\u4f7f\u7528 mini-gringo \u8bed\u8a00\u7f16\u5199\u7684\u903b\u8f91\u7a0b\u5e8f\uff0c\u8be5\u8bed\u8a00\u5305\u542b\u7b97\u672f\u8fd0\u7b97\u548c\u9009\u62e9\u89c4\u5219\u3002", "method": "Anthem 2.0 \u5c06\u903b\u8f91\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u201chere-and-there\u201d\u903b\u8f91\u4e2d\u7684\u516c\u5f0f\u8868\u793a\uff0c\u5e76\u5206\u6790\u5982\u7d27\u6027\u7b49\u903b\u8f91\u7a0b\u5e8f\u7684\u5c5e\u6027\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8c03\u7528\u4e00\u9636\u5b9a\u7406\u8bc1\u660e\u5668\u6765\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u5c5e\u6027\u3002", "result": "Anthem 2.0 \u80fd\u591f\u652f\u6301\u7a0b\u5e8f\u9a8c\u8bc1\uff0c\u5e76\u80fd\u6f14\u793a\u5982\u4f55\u6709\u6548\u5730\u4f7f\u7528\u8be5\u5de5\u5177\u5e76\u89e3\u91ca\u5176\u7ed3\u679c\u3002", "conclusion": "Anthem 2.0 \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u9a8c\u8bc1 mini-gringo \u8bed\u8a00\u7f16\u5199\u7684\u903b\u8f91\u7a0b\u5e8f\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528\u4e00\u9636\u5b9a\u7406\u8bc1\u660e\u5668\u6765\u652f\u6301\u7a0b\u5e8f\u9a8c\u8bc1\uff0c\u4f8b\u5982\u786e\u8ba4\u7a0b\u5e8f\u662f\u5426\u7b26\u5408\u4e00\u9636\u89c4\u8303\uff0c\u6216\u5efa\u7acb\u7a0b\u5e8f\u7684\u5f3a\u7b49\u4ef7\u548c\u5916\u90e8\u7b49\u4ef7\u3002"}}
{"id": "2507.11722", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.11722", "abs": "https://arxiv.org/abs/2507.11722", "authors": ["Richard Monge", "Yuki Nakamura", "Olaf Bach", "Jason Shao", "Artur Lozovoi", "Alexander A. Wood", "Kento Sasaki", "Kensuke Kobayashi", "Tom Delord", "Carlos A. Meriles"], "title": "Beyond ensemble averaging: Parallelized single-shot readout of hole capture in diamond", "comment": null, "summary": "Understanding the generation, transport and capture of charge carriers in\nsemiconductors is of fundamental technological importance. However, the\nensemble measurement techniques ubiquitous in electronics offer limited insight\ninto the nanoscale environment that is crucial to the operation of modern\nquantum-electronic devices. Here, we combine widefield optical microscopy with\nprecision spectroscopy to examine the capture of photogenerated holes by\nnegatively charged nitrogen vacancy (NV-) centers in diamond. Simultaneous\nsingle-shot charge readout over hundreds of individual NVs allows us to resolve\nthe roles of ionized impurities, reveal the formation of space charges fields,\nand monitor the thermalization of hot photo-carriers during diffusion. We\nmeasure effective NV- hole capture radii in excess of 0.2 um, a value\napproaching the Onsager limit and made possible here thanks to the\nnear-complete neutralization of coexisting charge traps. These results\nestablish a new platform for resolving charge dynamics beyond ensemble\naverages, with direct relevance to nanoscale electronics and quantum devices.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5229\u7528\u5148\u8fdb\u7684\u5149\u5b66\u6280\u672f\uff0c\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u63ed\u793a\u4e86\u534a\u5bfc\u4f53\u4e2d\u7684\u7535\u8377\u8f7d\u6d41\u5b50\u884c\u4e3a\uff0c\u4e3a\u91cf\u5b50\u8bbe\u5907\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u7406\u89e3\u534a\u5bfc\u4f53\u4e2d\u7535\u8377\u8f7d\u6d41\u5b50\u7684\u4ea7\u751f\u3001\u4f20\u8f93\u548c\u6355\u83b7\u5177\u6709\u6839\u672c\u7684\u6280\u672f\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u7535\u5b50\u5b66\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7cfb\u7efc\u6d4b\u91cf\u6280\u672f\u5bf9\u73b0\u4ee3\u91cf\u5b50\u7535\u5b50\u8bbe\u5907\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u7684\u7eb3\u7c73\u7ea7\u73af\u5883\u7684\u6d1e\u5bdf\u529b\u6709\u9650\u3002", "method": "\u7ed3\u5408\u5bbd\u573a\u5149\u5b66\u663e\u5fae\u955c\u548c\u7cbe\u5bc6\u5149\u8c31\u5b66\uff0c\u7814\u7a76\u5149\u751f\u7a7a\u7a74\u88ab\u91d1\u521a\u77f3\u4e2d\u5e26\u8d1f\u7535\u7684\u6c2e\u7a7a\u4f4d\uff08NV-\uff09\u4e2d\u5fc3\u6355\u83b7\u7684\u60c5\u51b5\u3002", "result": "\u540c\u65f6\u5bf9\u6570\u767e\u4e2a\u72ec\u7acb\u7684NV\u8fdb\u884c\u5355\u6b21\u7535\u8377\u8bfb\u6570\uff0c\u53ef\u4ee5\u89e3\u6790\u7535\u79bb\u6742\u8d28\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u7a7a\u95f4\u7535\u8377\u573a\u7684\u5f62\u6210\uff0c\u5e76\u76d1\u6d4b\u6269\u6563\u8fc7\u7a0b\u4e2d\u70ed\u8f7d\u6d41\u5b50\u7684\u70ed\u5316\u3002\u6d4b\u91cf\u5230\u7684NV-\u7a7a\u7a74\u6355\u83b7\u534a\u5f84\u8d85\u8fc70.2\u5fae\u7c73\uff0c\u63a5\u8fd1Onsager\u6781\u9650\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5171\u5b58\u7535\u8377\u9677\u9631\u7684\u8fd1\u4e4e\u5b8c\u5168\u4e2d\u548c\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u89e3\u51b3\u8d85\u8d8a\u6574\u4f53\u5e73\u5747\u503c\u7684\u7535\u8377\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5e76\u4e0e\u7eb3\u7c73\u7535\u5b50\u5b66\u548c\u91cf\u5b50\u8bbe\u5907\u76f4\u63a5\u76f8\u5173\u3002"}}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "MLLM \u9a8c\u8bc1\u5668\u5b58\u5728\u201c\u4e00\u81f4\u6027\u504f\u5dee\u201d\u95ee\u9898\uff0c\u5373\u503e\u5411\u4e8e\u504f\u7231\u4e0a\u4e0b\u6587\u4e2d\u7684\u4fe1\u606f\uff0c\u5373\u4f7f\u8fd9\u4e9b\u4fe1\u606f\u4e0d\u51c6\u786e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SGV \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9 MLLM \u5148\u751f\u6210\u901a\u7528\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u518d\u57fa\u4e8e\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u6765\u8bc4\u4f30\u5177\u4f53\u8f68\u8ff9\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5956\u52b1\u51fd\u6570\u5728\u6570\u5b66\u548c\u68cb\u76d8\u6e38\u620f\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u6ca1\u6709\u660e\u786e\u6210\u529f\u6807\u51c6\uff08\u5982\u8ba1\u7b97\u673a\u4f7f\u7528\uff09\u7684\u9886\u57df\uff0c\u5c06\u5176\u6269\u5c55\u5b58\u5728\u6311\u6218\u3002MLLM \u56e0\u5176\u4e16\u754c\u77e5\u8bc6\u3001\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u548c\u63a8\u7406\u80fd\u529b\u800c\u6210\u4e3a\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u4e00\u81f4\u6027\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u81ea\u6211\u63a5\u5730\u9a8c\u8bc1\u201d\uff08Self-Grounded Verification, SGV\uff09\u7684\u65b0\u65b9\u6cd5\u3002SGV \u901a\u8fc7\u4e24\u4e2a\u6b65\u9aa4\u6765\u5229\u7528 MLLM \u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff1a1. \u68c0\u7d22\u4e0e\u8bc4\u4f30\u6570\u636e\u65e0\u5173\u7684\u3001\u5173\u4e8e\u4efb\u52a1\u5b8c\u6210\u7684\u5e7f\u6cdb\u5148\u9a8c\u77e5\u8bc6\u30022. \u5728\u81ea\u6211\u751f\u6210\u7684\u5148\u9a8c\u77e5\u8bc6\u7684\u6761\u4ef6\u4e0b\uff0c\u5bf9\u5019\u9009\u8f68\u8ff9\u8fdb\u884c\u63a8\u7406\u548c\u8bc4\u4f30\u3002", "result": "SGV \u65b9\u6cd5\u63d0\u9ad8\u4e86 MLLM \u9a8c\u8bc1\u5668\u7684\u51c6\u786e\u6027\u548c\u6545\u969c\u68c0\u6d4b\u7387\uff0c\u6700\u9ad8\u53ef\u63d0\u9ad8 20 \u4e2a\u767e\u5206\u70b9\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5bf9\u5f02\u6784\u4ee3\u7406\u8fdb\u884c\u5b9e\u65f6\u76d1\u7763\uff0c\u63d0\u9ad8\u4e86 GUI \u4e13\u5bb6\u5728 OSWorld \u4e2d\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u6269\u6563\u7b56\u7565\u5728 robomimic \u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca ReAct \u4ee3\u7406\u5728 VisualWebArena \u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u7684\u6700\u5148\u8fdb\u8bb0\u5f55\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u8bb0\u5f55 48%\u3002", "conclusion": "MLLM \u9a8c\u8bc1\u5668\u5b58\u5728\u4e00\u81f4\u6027\u504f\u5dee\uff0c\u4f1a\u504f\u7231\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u4fe1\u606f\uff0c\u5373\u4f7f\u8fd9\u4e9b\u4fe1\u606f\u5e76\u4e0d\u7406\u60f3\u3002SGV \u65b9\u6cd5\u901a\u8fc7\u5229\u7528 MLLM \u7684\u91c7\u6837\u673a\u5236\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6545\u969c\u68c0\u6d4b\u7387\uff0c\u5e76\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.11849", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11849", "abs": "https://arxiv.org/abs/2507.11849", "authors": ["Tanjim Rahman"], "title": "Mobility Extraction and Analysis of GaN HEMTs for RF Applications Using TCAD and Experimental Data", "comment": "5 pages, 7 figures", "summary": "This paper presents an analysis of GaN high-electron-mobility transistors\n(HEMTs) using both TCAD simulation and experimental characterization. The\nenergy band structure was studied using Nextnano simulation software to observe\ntwo-dimensional electron gas (2DEG) formation and carrier confinement under\nequilibrium conditions. Additionally, I-V and C-V data from fabricated\nresearch-grade GaN HEMTs were analyzed to extract key electrical parameters.\nThe device demonstrated an ON current of 1.9 mA and an OFF current of 0.01 mA,\nindicating a strong ON/OFF current ratio. A subthreshold swing of 80 mV/decade\nand a DIBL of 5 mV/V were observed, confirming good gate control and\nshort-channel suppression. The ON-resistance was 22.72 ohm per micron, with a\nsaturation voltage of 1 V . The peak transconductance was extracted as 0.18 mS\nin the linear region and 0.5 mS in saturation. Field-effect mobility was\ncalculated using the transconductance method, with a maximum value of\napproximately 1200 cm2/V.s at low drain bias. The combined simulation and\nexperimental approach provided comprehensive insight into GaN HEMT behavior,\nenabling a deeper understanding of structure-performance relationships critical\nto advanced transistor design.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7TCAD\u4eff\u771f\u548c\u5b9e\u9a8c\u5206\u6790\u4e86GaN HEMT\u5668\u4ef6\uff0c\u5c55\u793a\u4e86\u5176\u826f\u597d\u7684\u7535\u5b66\u6027\u80fd\u548c\u9ad8\u8fc1\u79fb\u7387\uff0c\u4e3a\u5668\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u4e3a\u4e86\u6df1\u5165\u7406\u89e3GaN HEMT\u5668\u4ef6\u7684\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\uff0c\u4e3a\u5148\u8fdb\u7684\u6676\u4f53\u7ba1\u8bbe\u8ba1\u63d0\u4f9b\u5173\u952e\u4f9d\u636e\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528Nextnano\u4eff\u771f\u8f6f\u4ef6\u8fdb\u884cTCAD\u6a21\u62df\uff0c\u4ee5\u7814\u7a762DEG\u5f62\u6210\u548c\u8f7d\u6d41\u5b50\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u5f81\u5206\u6790\u4e86\u5236\u9020\u51fa\u7684GaN HEMT\u5668\u4ef6\u7684I-V\u548cC-V\u6570\u636e\uff0c\u63d0\u53d6\u4e86\u5173\u952e\u7535\u5b66\u53c2\u6570\uff0c\u5982ON/OFF\u7535\u6d41\u6bd4\u3001\u4e9a\u9608\u503c\u6446\u5e45\u3001DIBL\u3001\u5bfc\u901a\u7535\u963b\u3001\u9971\u548c\u7535\u538b\u3001\u8de8\u5bfc\u548c\u573a\u6548\u5e94\u8fc1\u79fb\u7387\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5668\u4ef6\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5305\u62ec\u9ad8ON/OFF\u7535\u6d41\u6bd4\u3001\u826f\u597d\u7684\u6805\u6781\u63a7\u5236\u3001\u77ed\u6c9f\u9053\u6291\u5236\u4ee5\u53ca\u9ad8\u8fbe1200 cm2/V.s\u7684\u5cf0\u503c\u573a\u6548\u5e94\u8fc1\u79fb\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86TCAD\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u5f81\uff0c\u5bf9GaN HEMT\u5668\u4ef6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u80fd\u5e26\u7ed3\u6784\u3001\u8f7d\u6d41\u5b50\u884c\u4e3a\u4ee5\u53ca\u5173\u952e\u7684\u7535\u5b66\u53c2\u6570\uff0c\u4e3a\u5148\u8fdbGaN\u5668\u4ef6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.12418", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.12418", "abs": "https://arxiv.org/abs/2507.12418", "authors": ["George Alexakis", "Dimitrios Schoinianakis", "Giorgos Dimitrakopoulos"], "title": "High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic", "comment": "28th Euromicro Conference Series on Digital System Design (DSD 2025)", "summary": "The Number Theoretic Transform (NTT) is a fundamental operation in\nprivacy-preserving technologies, particularly within fully homomorphic\nencryption (FHE). The efficiency of NTT computation directly impacts the\noverall performance of FHE, making hardware acceleration a critical technology\nthat will enable realistic FHE applications. Custom accelerators, in FPGAs or\nASICs, offer significant performance advantages due to their ability to exploit\nmassive parallelism and specialized optimizations. However, the operation of\nNTT over large moduli requires large word-length modulo arithmetic that limits\nachievable clock frequencies in hardware and increases hardware area costs. To\novercome such deficits, digit-serial arithmetic has been explored for modular\nmultiplication and addition independently. The goal of this work is to leverage\ndigit-serial modulo arithmetic combined with appropriate redundant data\nrepresentation to design modular pipelined NTT accelerators that operate\nuniformly on arbitrary small digits, without the need for intermediate\n(de)serialization. The proposed architecture enables high clock frequencies\nthrough regular pipelining while maintaining parallelism. Experimental results\ndemonstrate that the proposed approach outperforms state-of-the-art\nimplementations and reduces hardware complexity under equal performance and\ninput-output bandwidth constraints.", "AI": {"tldr": "\u901a\u8fc7\u6570\u5b57\u4e32\u884c\u7b97\u672f\u548c\u5197\u4f59\u6570\u636e\u8868\u793a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684NTT\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u786c\u4ef6\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709NTT\u786c\u4ef6\u52a0\u901f\u5668\u5728\u5927\u6570\u6a21\u6570\u8fd0\u7b97\u4e2d\u9047\u5230\u7684\u786c\u4ef6\u9762\u79ef\u548c\u65f6\u949f\u9891\u7387\u9650\u5236\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684NTT\u786c\u4ef6\u52a0\u901f\u5668\u3002", "method": "\u672c\u6587\u5229\u7528\u6570\u5b57\u4e32\u884c\u6a21\u8fd0\u7b97\u7ed3\u5408\u5197\u4f59\u6570\u636e\u8868\u793a\uff0c\u8bbe\u8ba1\u4e86\u6a21\u5757\u5316\u6d41\u6c34\u7ebfNTT\u52a0\u901f\u5668\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4efb\u610f\u5c0f\u7684\u6570\u5b57\u4e0a\u7edf\u4e00\u8fd0\u884c\uff0c\u65e0\u9700\u4e2d\u95f4\u7684\uff08\u53cd\uff09\u5e8f\u5217\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u540c\u7b49\u6027\u80fd\u548c\u8f93\u5165\u8f93\u51fa\u5e26\u5bbd\u7ea6\u675f\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5b9e\u73b0\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u7684\u65f6\u949f\u9891\u7387\u5e76\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6570\u5b57\u4e32\u884c\u7b97\u672f\u548c\u5197\u4f59\u6570\u636e\u8868\u793a\u7684\u6a21\u5757\u5316\u6d41\u6c34\u7ebfNTT\u52a0\u901f\u5668\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u5e76\u884c\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u89c4\u6574\u7684\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u65f6\u949f\u9891\u7387\uff0c\u5e76\u4e14\u5728\u540c\u7b49\u6027\u80fd\u548c\u8f93\u5165\u8f93\u51fa\u5e26\u5bbd\u9650\u5236\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5b9e\u73b0\u5177\u6709\u66f4\u4f4e\u7684\u786c\u4ef6\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.12108", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12108", "abs": "https://arxiv.org/abs/2507.12108", "authors": ["Lorenzo Mannocci", "Stefano Cresci", "Matteo Magnani", "Anna Monreale", "Maurizio Tesconi"], "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies", "comment": null, "summary": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.", "AI": {"tldr": "This paper explores multimodal approaches for detecting coordinated online behavior, finding that combining different data types offers a richer understanding than single-data methods, despite variations in individual data source utility.", "motivation": "Traditional methods focusing on single interaction types or independently analyzing multiple modalities may miss complex dynamics in multimodal coordinated behavior. This research aims to improve the detection and analysis of such behavior.", "method": "The study compares different operationalizations for detecting multimodal coordinated behavior, evaluating weakly and strongly integrated multimodal models against monomodal approaches. It assesses the impact of various implementations of multimodality on detection outcomes.", "result": "Multimodal approaches provide a more comprehensive understanding of coordination dynamics compared to monomodal ones, even though some modalities do not contribute distinct information. The findings help in understanding the trade-offs in integrating different data modalities.", "conclusion": "Coordinated online behavior analysis benefits from multimodal approaches, although not all modalities offer unique insights. The study highlights the trade-offs between weakly and strongly integrated multimodal models for capturing different types of coordination."}}
{"id": "2507.11563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11563", "abs": "https://arxiv.org/abs/2507.11563", "authors": ["Giulio Attenni", "Novella Bartolini"], "title": "Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "summary": "This paper presents a theoretical discussion for environmentally-conscious\njob deployment and migration in cloud environments, aiming to minimize the\nenvironmental impact of resource provisioning while incorporating\nsustainability requirements. As the demand for sustainable cloud services\ngrows, it is crucial for cloud customers to select data center operators based\non sustainability metrics and to accurately report the ecological footprint of\ntheir services. To this end, we analyze sustainability reports and define\ncomprehensive environmental impact profiles for data centers, incorporating key\nsustainability indicators. We formalize the problem as an optimization model,\nbalancing multiple environmental factors while respecting user preferences. A\nsimulative case study demonstrates the {potential} of our approach compared to\nbaseline strategies that optimize for single sustainability factors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u4e91\u73af\u5883\u4e2d\u8fdb\u884c\u73af\u5883\u610f\u8bc6\u7684\u4f5c\u4e1a\u90e8\u7f72\u548c\u8fc1\u79fb\uff0c\u901a\u8fc7\u5e73\u8861\u591a\u4e2a\u73af\u5883\u56e0\u7d20\u6765\u6700\u5c0f\u5316\u73af\u5883\u5f71\u54cd\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u8003\u8651\u5355\u4e00\u73af\u5883\u56e0\u7d20\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5bf9\u53ef\u6301\u7eed\u4e91\u670d\u52a1\u7684\u9700\u6c42\u589e\u957f\uff0c\u4e91\u5ba2\u6237\u9700\u8981\u6839\u636e\u53ef\u6301\u7eed\u6027\u6307\u6807\u9009\u62e9\u6570\u636e\u4e2d\u5fc3\u8fd0\u8425\u5546\uff0c\u5e76\u51c6\u786e\u62a5\u544a\u5176\u670d\u52a1\u7684\u751f\u6001\u8db3\u8ff9\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e91\u73af\u5883\u4e2d\u4f5c\u4e1a\u90e8\u7f72\u548c\u8fc1\u79fb\u7684\u73af\u5883\u5f71\u54cd\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u5e73\u8861\u591a\u4e2a\u73af\u5883\u56e0\u7d20\uff0c\u540c\u65f6\u5c0a\u91cd\u7528\u6237\u504f\u597d\uff0c\u4ee5\u5b9e\u73b0\u73af\u5883\u610f\u8bc6\u7684\u4f5c\u4e1a\u90e8\u7f72\u548c\u8fc1\u79fb\u3002\u901a\u8fc7\u5206\u6790\u53ef\u6301\u7eed\u6027\u62a5\u544a\uff0c\u5b9a\u4e49\u4e86\u5305\u542b\u5173\u952e\u53ef\u6301\u7eed\u6027\u6307\u6807\u7684\u6570\u636e\u4e2d\u5fc3\u73af\u5883\u5f71\u54cd\u6863\u6848\u3002", "result": "\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4ec5\u4f18\u5316\u5355\u4e00\u53ef\u6301\u7eed\u6027\u56e0\u7d20\u7684\u57fa\u7ebf\u7b56\u7565\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e91\u73af\u5883\u4e2d\u8fdb\u884c\u6ee1\u8db3\u73af\u5883\u610f\u8bc6\u7684\u4f5c\u4e1a\u90e8\u7f72\u548c\u8fc1\u79fb\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u8d44\u6e90\u914d\u7f6e\u5bf9\u73af\u5883\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u7eb3\u5165\u53ef\u6301\u7eed\u6027\u8981\u6c42\u3002\u968f\u7740\u5bf9\u53ef\u6301\u7eed\u4e91\u670d\u52a1\u7684\u9700\u6c42\u589e\u957f\uff0c\u4e91\u5ba2\u6237\u6839\u636e\u53ef\u6301\u7eed\u6027\u6307\u6807\u9009\u62e9\u6570\u636e\u4e2d\u5fc3\u8fd0\u8425\u5546\u4ee5\u53ca\u51c6\u786e\u62a5\u544a\u5176\u670d\u52a1\u7684\u751f\u6001\u8db3\u8ff9\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5206\u6790\u4e86\u53ef\u6301\u7eed\u6027\u62a5\u544a\uff0c\u5e76\u5b9a\u4e49\u4e86\u5168\u9762\u7684\u6570\u636e\u4e2d\u5fc3\u73af\u5883\u5f71\u54cd\u6863\u6848\uff0c\u7eb3\u5165\u4e86\u5173\u952e\u7684\u53ef\u6301\u7eed\u6027\u6307\u6807\u3002\u6211\u4eec\u5c06\u6b64\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u4f18\u5316\u6a21\u578b\uff0c\u5728\u5c0a\u91cd\u7528\u6237\u504f\u597d\u7684\u540c\u65f6\u5e73\u8861\u591a\u4e2a\u73af\u5883\u56e0\u7d20\u3002\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u4e0e\u4ec5\u9488\u5bf9\u5355\u4e00\u53ef\u6301\u7eed\u6027\u56e0\u7d20\u8fdb\u884c\u4f18\u5316\u7684\u57fa\u7ebf\u7b56\u7565\u76f8\u6bd4\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11716", "abs": "https://arxiv.org/abs/2507.11716", "authors": ["Yifan Xu", "Qianwei Wang", "Jordan Lillie", "Vineet Kamat", "Carol Menassa", "Clive D'Souza"], "title": "CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment", "comment": "13 pages, 10 figures", "summary": "As the global population of people with disabilities (PWD) continues to grow,\nso will the need for mobility solutions that promote independent living and\nsocial integration. Wheelchairs are vital for the mobility of PWD in both\nindoor and outdoor environments. The current SOTA in powered wheelchairs is\nbased on either manually controlled or fully autonomous modes of operation,\noffering limited flexibility and often proving difficult to navigate in\nspatially constrained environments. Moreover, research on robotic wheelchairs\nhas focused predominantly on complete autonomy or improved manual control;\napproaches that can compromise efficiency and user trust. To overcome these\nchallenges, this paper introduces the CoNav Chair, a smart wheelchair based on\nthe Robot Operating System (ROS) and featuring shared control navigation and\nobstacle avoidance capabilities that are intended to enhance navigational\nefficiency, safety, and ease of use for the user. The paper outlines the CoNav\nChair's design and presents a preliminary usability evaluation comparing three\ndistinct navigation modes, namely, manual, shared, and fully autonomous,\nconducted with 21 healthy, unimpaired participants traversing an indoor\nbuilding environment. Study findings indicated that the shared control\nnavigation framework had significantly fewer collisions and performed\ncomparably, if not superior to the autonomous and manual modes, on task\ncompletion time, trajectory length, and smoothness; and was perceived as being\nsafer and more efficient based on user reported subjective assessments of\nusability. Overall, the CoNav system demonstrated acceptable safety and\nperformance, laying the foundation for subsequent usability testing with end\nusers, namely, PWDs who rely on a powered wheelchair for mobility.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12054", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12054", "abs": "https://arxiv.org/abs/2507.12054", "authors": ["Tian Bai", "Yiding Feng", "Yaohao Liu", "Mengfan Ma", "Mingyu Xiao"], "title": "Contracting with a Mechanism Designer", "comment": null, "summary": "This paper explores the economic interactions within modern crowdsourcing\nmarkets. In these markets, employers issue requests for tasks, platforms\nfacilitate the recruitment of crowd workers, and workers complete tasks for\nmonetary rewards. Recognizing that these roles serve distinct functions within\nthe ecosystem, we introduce a three-party model that distinguishes among the\nprincipal (the requester), the intermediary (the platform), and the pool of\nagents (the workers). The principal, unable to directly engage with agents,\nrelies on the intermediary to recruit and incentivize them. This interaction\nunfolds in two stages: first, the principal designs a profit-sharing contract\nwith the intermediary; second, the intermediary implements a mechanism to\nselect an agent to complete the delegated task.\n  We analyze the proposed model as an extensive-form Stackelberg game. Our\ncontributions are fourfold: (1) We fully characterize the subgame perfect\nequilibrium. In particular, we reduce the principal's contract design problem\nto a novel auction-theoretic formulation we term virtual value pricing, and\nreveals that linear contracts are optimal even when the task have multiple\noutcomes and agents' cost distributions are asymmetric. (2) To quantify the\nprincipal's utility loss from delegation and information asymmetry, we\nintroduce the price of double marginalization (PoDM) and the classical price of\nanarchy (PoA), and derive tight or nearly tight bounds on both ratios under\nregular and monotone hazard rate (MHR) distributions. (3) We further examine\nthese two ratios in a natural setting where the intermediary is restricted to\nanonymous pricing mechanisms, and show that similar qualitative insights\ncontinue to hold. (4) Finally, we extend our results on both ratios to a robust\nframework that accommodates scenarios in which the principal lacks precise\ninformation about the market size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e09\u65b9\u6a21\u578b\u6765\u5206\u6790\u4f17\u5305\u5e02\u573a\u7684\u7ecf\u6d4e\u4e92\u52a8\uff0c\u4f7f\u7528\u65af\u5854\u514b\u5c14\u4f2f\u683c\u535a\u5f08\u548c\u865a\u62df\u4ef7\u503c\u5b9a\u4ef7\u6765\u4f18\u5316\u59d4\u6258\u65b9\u4e0e\u5e73\u53f0\u4e4b\u95f4\u7684\u5408\u540c\u8bbe\u8ba1\uff0c\u5e76\u91cf\u5316\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6548\u7387\u635f\u5931\u3002", "motivation": "\u73b0\u4ee3\u4f17\u5305\u5e02\u573a\u4e2d\uff0c\u96c7\u4e3b\u3001\u5e73\u53f0\u548c\u5de5\u4eba\u4e4b\u95f4\u7684\u7ecf\u6d4e\u4e92\u52a8\u65e5\u76ca\u590d\u6742\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5145\u5206\u89e3\u91ca\u8fd9\u79cd\u591a\u65b9\u5173\u7cfb\u4e2d\u7684\u6fc0\u52b1\u673a\u5236\u548c\u6548\u7387\u635f\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u6765\u7406\u89e3\u548c\u4f18\u5316\u8fd9\u4e9b\u5e02\u573a\u3002", "method": "\u672c\u6587\u5c06\u4f17\u5305\u5e02\u573a\u4e2d\u7684\u7ecf\u6d4e\u4e92\u52a8\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4e09\u65b9\u535a\u5f08\uff0c\u5373\u59d4\u6258\u65b9\uff08\u96c7\u4e3b\uff09\u3001\u4e2d\u95f4\u65b9\uff08\u5e73\u53f0\uff09\u548c\u4ee3\u7406\u65b9\uff08\u5de5\u4eba\uff09\u3002\u8be5\u6a21\u578b\u88ab\u5206\u6790\u4e3a\u4e00\u4e2a\u6269\u5c55\u578b\u65af\u5854\u514b\u5c14\u4f2f\u683c\u535a\u5f08\u3002\u7814\u7a76\u4e2d\u5f15\u5165\u4e86\u865a\u62df\u4ef7\u503c\u5b9a\u4ef7\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u59d4\u6258\u65b9\u7684\u6548\u7528\u635f\u5931\u91cf\u5316\u4e3a\u53cc\u91cd\u8fb9\u9645\u5316\u4ef7\u683c\uff08PoDM\uff09\u548c\u65e0\u653f\u5e9c\u4ef7\u683c\uff08PoA\uff09\u3002", "result": "\u7814\u7a76\u6210\u529f\u5730\u5bf9\u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861\u8fdb\u884c\u4e86\u5b8c\u5168\u523b\u753b\uff0c\u63ed\u793a\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7ebf\u6027\u5408\u540c\u662f\u6700\u4f18\u7684\u3002\u7814\u7a76\u8fd8\u91cf\u5316\u4e86\u59d4\u6258\u65b9\u56e0\u59d4\u6258\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u9020\u6210\u7684\u6548\u7528\u635f\u5931\uff08PoDM\u548cPoA\uff09\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u6bd4\u7387\u5728\u4e0d\u540c\u5206\u5e03\u5047\u8bbe\u548c\u533f\u540d\u5b9a\u4ef7\u673a\u5236\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u754c\u9650\u3002\u6700\u540e\uff0c\u7814\u7a76\u5c06\u8fd9\u4e9b\u7ed3\u679c\u6269\u5c55\u5230\u4e00\u4e2a\u8003\u8651\u5e02\u573a\u89c4\u6a21\u4e0d\u786e\u5b9a\u6027\u7684\u7a33\u5065\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u4f17\u5305\u5e02\u573a\u7684\u7ecf\u6d4e\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u4ee3\u7406\u6210\u672c\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bbe\u8ba1\u6700\u4f18\u59d4\u6258\u5408\u540c\u3002"}}
{"id": "2507.11949", "categories": ["cs.GR", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11949", "abs": "https://arxiv.org/abs/2507.11949", "authors": ["Shuyang Xu", "Zhiyang Dou", "Mingyi Shi", "Liang Pan", "Leo Ho", "Jingbo Wang", "Yuan Liu", "Cheng Lin", "Yuexin Ma", "Wenping Wang", "Taku Komura"], "title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "comment": null, "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.", "AI": {"tldr": "This paper introduces the first Spatial Audio-Driven Human Motion (SAM) dataset and a diffusion-based framework (MOSPA) to generate human motion realistically responding to spatial audio, achieving state-of-the-art results.", "motivation": "Existing models overlook the impact of spatial features in spatial audio signals on human motion. This work aims to bridge this gap by enabling high-quality modeling of human movements in response to spatial audio.", "method": "A diffusion-based generative framework called MOSPA is developed to generate human motion from spatial audio. It utilizes an effective fusion mechanism to capture the relationship between body motion and spatial audio.", "result": "MOSPA can generate diverse and realistic human motions conditioned on varying spatial audio inputs. The SAM dataset provides diverse and high-quality spatial audio and motion data for benchmarking.", "conclusion": "The proposed MOSPA framework and SAM dataset achieve state-of-the-art performance in generating human motion driven by spatial audio, establishing a new benchmark for this task."}}
{"id": "2507.11634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u3001\u589e\u91cf\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982mDeBERTa\u548cXLM-RoBERTa\uff09\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6ce2\u65af\u8bed\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe96%.", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5229\u7528\u6709\u9650\u6570\u636e\u5e76\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4e86XLM-RoBERTa\u3001mDeBERTa\u548cDistilBERT\u4e09\u4e2a\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6765\u81eaX\u3001Instagram\u3001Digikala\u3001Snappfood\u548cTaaghche\u7b49\u591a\u79cd\u6765\u6e90\u7684\u6ce2\u65af\u8bed\u5c0f\u6837\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cmDeBERTa\u548cXLM-RoBERTa\u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7ee9\u6548\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u4e8696%\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u5728\u6ce2\u65af\u8bed\u4e2d\u4f7f\u7528\u5c11\u6837\u672c\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\u7684\u6a21\u578b\uff0cmDeBERTa\u548cXLM-RoBERTa\u6a21\u578b\u8fbe\u5230\u4e8696%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u3001\u589e\u91cf\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.11788", "categories": ["cs.NE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11788", "abs": "https://arxiv.org/abs/2507.11788", "authors": ["Daniel Mitropolsky", "Christos Papadimitriou"], "title": "Simulated Language Acquisition in a Biologically Realistic Model of the Brain", "comment": "13 pages, 6 figures", "summary": "Despite tremendous progress in neuroscience, we do not have a compelling\nnarrative for the precise way whereby the spiking of neurons in our brain\nresults in high-level cognitive phenomena such as planning and language. We\nintroduce a simple mathematical formulation of six basic and broadly accepted\nprinciples of neuroscience: excitatory neurons, brain areas, random synapses,\nHebbian plasticity, local inhibition, and inter-area inhibition. We implement a\nsimulated neuromorphic system based on this formalism, which is capable of\nbasic language acquisition: Starting from a tabula rasa, the system learns, in\nany language, the semantics of words, their syntactic role (verb versus noun),\nand the word order of the language, including the ability to generate novel\nsentences, through the exposure to a modest number of grounded sentences in the\nsame language. We discuss several possible extensions and implications of this\nresult.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6570\u5b66\u6a21\u578b\u548c\u6a21\u62df\u7cfb\u7edf\uff0c\u80fd\u591f\u5b66\u4e60\u8bed\u8a00\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u7684\u53e5\u5b50\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u79d1\u5b66\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u6b65\uff0c\u4f46\u6211\u4eec\u4ecd\u7136\u7f3a\u4e4f\u5173\u4e8e\u5927\u8111\u795e\u7ecf\u5143\u653e\u7535\u5982\u4f55\u4ea7\u751f\u9ad8\u7ea7\u8ba4\u77e5\u73b0\u8c61\uff08\u5982\u89c4\u5212\u548c\u8bed\u8a00\uff09\u7684\u4ee4\u4eba\u4fe1\u670d\u7684\u53d9\u8ff0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516d\u4e2a\u57fa\u672c\u795e\u7ecf\u79d1\u5b66\u539f\u7406\uff08\u5174\u594b\u6027\u795e\u7ecf\u5143\u3001\u5927\u8111\u533a\u57df\u3001\u968f\u673a\u7a81\u89e6\u3001\u8d6b\u5e03\u53ef\u5851\u6027\u3001\u5c40\u90e8\u6291\u5236\u548c\u533a\u57df\u95f4\u6291\u5236\uff09\u7684\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u3002", "result": "\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\uff0c\u901a\u8fc7\u63a5\u89e6\u9002\u91cf\u7684\u57fa\u7840\u53e5\u5b50\uff0c\u5b66\u4e60\u4efb\u4f55\u8bed\u8a00\u7684\u8bed\u4e49\u3001\u8bcd\u6027\uff08\u52a8\u8bcd\u6216\u540d\u8bcd\uff09\u4ee5\u53ca\u8bcd\u5e8f\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u7684\u53e5\u5b50\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u4efb\u4f55\u8bed\u8a00\u7684\u8bed\u4e49\u3001\u8bcd\u6027\uff08\u52a8\u8bcd\u6216\u540d\u8bcd\uff09\u4ee5\u53ca\u8bcd\u5e8f\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u7684\u53e5\u5b50\u3002"}}
{"id": "2507.11554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "AI": {"tldr": "Inversion-DPO \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408 DPO \u548c DDIM \u53cd\u6f14\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7684\u5bf9\u9f50\uff0c\u5e76\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u57fa\u7840\u6a21\u578b\u548c\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff0c\u4ee5\u53ca\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u7684\u9650\u5236\u3002", "method": "Inversion-DPO \u6846\u67b6\uff0c\u5b83\u5c06 DPO \u4e0e DDIM \u53cd\u6f14\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u53cd\u6f14\u5c06\u83b7\u80dc\u548c\u5931\u8d25\u6837\u672c\u8f6c\u6362\u4e3a\u566a\u58f0\uff0c\u4ece\u800c\u8fdb\u884c\u540e\u9a8c\u91c7\u6837\u3002", "result": "Inversion-DPO \u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u548c\u7ec4\u5408\u4e00\u81f4\u6027\u7684\u56fe\u50cf\u3002", "conclusion": "Inversion-DPO \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5c06 DPO \u4e0e DDIM \u53cd\u6f14\u76f8\u7ed3\u5408\uff0c\u6d88\u9664\u4e86\u5bf9\u5956\u52b1\u5efa\u6a21\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u5bf9\u9f50\u3002"}}
{"id": "2507.12400", "categories": ["cs.MA", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.12400", "abs": "https://arxiv.org/abs/2507.12400", "authors": ["Noble Harasha", "Cristina Gava", "Nancy Lynch", "Claudia Contini", "Frederik Mallmann-Trenn"], "title": "Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment", "comment": null, "summary": "Deploying motile nanosized particles, also known as ``nanobots'', in the\nhuman body promises to improve selectivity in drug delivery and reduce side\neffects. We consider a swarm of nanobots locating a single cancerous region and\ntreating it by releasing an onboard payload of drugs at the site. At nanoscale,\nthe computation, communication, sensing, and locomotion capabilities of\nindividual agents are extremely limited, noisy, and/or nonexistent.\n  We present a general model to formally describe the individual and collective\nbehavior of agents in a colloidal environment, such as the bloodstream, for\ncancer detection and treatment by nanobots. This includes a feasible and\nprecise model of agent locomotion, inspired by actual nanoparticles that, in\nthe presence of an external chemical gradient, move towards areas of higher\nconcentration by means of self-propulsion. We present two variants of our\ngeneral model: The first assumes an endogenous chemical gradient that is fixed\nover time and centered at the targeted cancer site; the second is a more\nspeculative and dynamic variant in which agents themselves create and amplify a\nchemical gradient centered at the cancer site. In both settings, agents can\nsense the gradient and ascend it noisily, locating the cancer site more quickly\nthan via simple Brownian motion.\n  For the first variant of the model, we present simulation results to show the\nbehavior of agents under our locomotion model, as well as {analytical results}\nto bound the time it takes for the agents to reach the cancer site. For the\nsecond variant, simulation results highlight the collective benefit in having\nagents issue their own chemical signal. While arguably more speculative in its\nagent capability assumptions, this variant shows a significant improvement in\nruntime performance over the first variant, resulting from its chemical signal\namplification mechanism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u764c\u75c7\u6cbb\u7597\u7684\u7eb3\u7c73\u673a\u5668\u4eba\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u4e24\u79cd\u53d8\u4f53\uff1a\u4e00\u79cd\u4f7f\u7528\u56fa\u5b9a\u7684\u5316\u5b66\u68af\u5ea6\uff0c\u53e6\u4e00\u79cd\u4f7f\u7528\u7531\u7eb3\u7c73\u673a\u5668\u4eba\u81ea\u8eab\u4ea7\u751f\u7684\u52a8\u6001\u5316\u5b66\u68af\u5ea6\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u7b2c\u4e8c\u79cd\u6a21\u578b\u53d8\u4f53\u7531\u4e8e\u5176\u5316\u5b66\u4fe1\u53f7\u653e\u5927\u673a\u5236\uff0c\u5728\u5b9a\u4f4d\u764c\u53d8\u7ec4\u7ec7\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5c06\u8fd0\u52a8\u7eb3\u7c73\u9897\u7c92\uff08\u201c\u7eb3\u7c73\u673a\u5668\u4eba\u201d\uff09\u5f15\u5165\u4eba\u4f53\uff0c\u6709\u6f5c\u529b\u63d0\u9ad8\u836f\u7269\u9012\u9001\u7684\u9009\u62e9\u6027\u5e76\u51cf\u5c11\u526f\u4f5c\u7528\u3002\u672c\u6587\u8003\u8651\u4e86\u4e00\u4e2a\u7eb3\u7c73\u673a\u5668\u4eba\u8702\u7fa4\uff0c\u7528\u4e8e\u5b9a\u4f4d\u5355\u4e00\u764c\u53d8\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5728\u764c\u53d8\u90e8\u4f4d\u91ca\u653e\u643a\u5e26\u7684\u836f\u7269\u6765\u6cbb\u7597\u3002\u7136\u800c\uff0c\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\uff0c\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u8ba1\u7b97\u3001\u901a\u4fe1\u3001\u4f20\u611f\u548c\u8fd0\u52a8\u80fd\u529b\u90fd\u6781\u5176\u6709\u9650\u3001\u4e0d\u7a33\u5b9a\u4e14/\u6216\u4e0d\u5b58\u5728\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6a21\u578b\u6765\u6b63\u5f0f\u63cf\u8ff0\u7eb3\u7c73\u673a\u5668\u4eba\uff08\u5728\u8840\u6db2\u7b49\u80f6\u4f53\u73af\u5883\u4e2d\uff09\u7684\u4e2a\u4f53\u548c\u96c6\u4f53\u884c\u4e3a\uff0c\u7528\u4e8e\u764c\u75c7\u7684\u68c0\u6d4b\u548c\u6cbb\u7597\u3002\u8be5\u6a21\u578b\u5305\u62ec\u4e00\u4e2a\u53d7\u5b9e\u9645\u7eb3\u7c73\u9897\u7c92\u542f\u53d1\u7684\u3001\u53ef\u884c\u7684\u3001\u7cbe\u786e\u7684\u667a\u80fd\u4f53\u8fd0\u52a8\u6a21\u578b\uff0c\u8fd9\u4e9b\u9897\u7c92\u5728\u5916\u90e8\u5316\u5b66\u68af\u5ea6\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u63a8\u4f5c\u7528\u79fb\u5411\u6d53\u5ea6\u8f83\u9ad8\u533a\u57df\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u8be5\u901a\u7528\u6a21\u578b\u7684\u4e24\u79cd\u53d8\u4f53\uff1a\u7b2c\u4e00\u79cd\u5047\u8bbe\u5b58\u5728\u4e00\u4e2a\u968f\u65f6\u95f4\u56fa\u5b9a\u7684\u3001\u4ee5\u764c\u53d8\u90e8\u4f4d\u4e3a\u4e2d\u5fc3\u7684\u5185\u6e90\u6027\u5316\u5b66\u68af\u5ea6\uff1b\u7b2c\u4e8c\u79cd\u662f\u4e00\u79cd\u66f4\u5177\u63a8\u6d4b\u6027\u548c\u52a8\u6001\u6027\u7684\u53d8\u4f53\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u81ea\u8eab\u4ea7\u751f\u5e76\u653e\u5927\u4e00\u4e2a\u4ee5\u764c\u53d8\u90e8\u4f4d\u4e3a\u4e2d\u5fc3\u7684\u5316\u5b66\u68af\u5ea6\u3002", "result": "\u5bf9\u4e8e\u7b2c\u4e00\u79cd\u6a21\u578b\u53d8\u4f53\uff0c\u6587\u7ae0\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5728\u6240\u63d0\u51fa\u7684\u8fd0\u52a8\u6a21\u578b\u4e0b\u7684\u884c\u4e3a\u7684\u6a21\u62df\u7ed3\u679c\uff0c\u4ee5\u53ca\u5206\u6790\u7ed3\u679c\uff0c\u7528\u4e8e\u9650\u5236\u667a\u80fd\u4f53\u5230\u8fbe\u764c\u53d8\u90e8\u4f4d\u6240\u9700\u7684\u65f6\u95f4\u3002\u5bf9\u4e8e\u7b2c\u4e8c\u79cd\u6a21\u578b\u53d8\u4f53\uff0c\u6a21\u62df\u7ed3\u679c\u7a81\u663e\u4e86\u667a\u80fd\u4f53\u53d1\u51fa\u81ea\u8eab\u5316\u5b66\u4fe1\u53f7\u7684\u96c6\u4f53\u6548\u76ca\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u8fd0\u884c\u65f6\u6027\u80fd\u7684\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5c3d\u7ba1\u7b2c\u4e8c\u79cd\u6a21\u578b\u5728\u667a\u80fd\u4f53\u80fd\u529b\u5047\u8bbe\u65b9\u9762\u66f4\u5177\u63a8\u6d4b\u6027\uff0c\u4f46\u5b83\u901a\u8fc7\u5316\u5b66\u4fe1\u53f7\u653e\u5927\u673a\u5236\u5728\u8fd0\u884c\u65f6\u6027\u80fd\u4e0a\u6709\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8fd9\u8868\u660e\u5176\u96c6\u4f53\u6548\u76ca\u3002"}}
{"id": "2507.11604", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11604", "abs": "https://arxiv.org/abs/2507.11604", "authors": ["Mariesa H. Teo", "Willers Yang", "James Sud", "Teague Tomesh", "Frederic T. Chong", "Eric R. Anschuetz"], "title": "k-Contextuality as a Heuristic for Memory Separations in Learning", "comment": null, "summary": "Classical machine learning models struggle with learning and prediction tasks\non data sets exhibiting long-range correlations. Previously, the existence of a\nlong-range correlational structure known as contextuality was shown to inhibit\nefficient classical machine learning representations of certain\nquantum-inspired sequential distributions. Here, we define a new quantifier of\ncontextuality we call strong k-contextuality, and prove that any translation\ntask exhibiting strong k-contextuality is unable to be represented to finite\nrelative entropy by a classical streaming model with fewer than k latent\nstates. Importantly, this correlation measure does not induce a similar\nresource lower bound for quantum generative models. Using this theory as\nmotivation, we develop efficient algorithms which estimate our new measure of\ncontextuality in sequential data, and empirically show that this estimate is a\ngood predictor for the difference in performance of resource-constrained\nclassical and quantum Bayesian networks in modeling the data. Strong\nk-contextuality thus emerges as a measure to help identify problems that are\ndifficult for classical computers, but may not be for quantum computers.", "AI": {"tldr": "\u63d0\u51fa\u5f3ak-\u60c5\u5883\u6027\u6765\u8bc6\u522b\u7ecf\u5178\u8ba1\u7b97\u673a\u96be\u4ee5\u5904\u7406\u4f46\u91cf\u5b50\u8ba1\u7b97\u673a\u53ef\u80fd\u64c5\u957f\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4f30\u8ba1\u8be5\u5ea6\u91cf\u548c\u9884\u6d4b\u7ecf\u5178\u4e0e\u91cf\u5b50\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u7684\u7b97\u6cd5\u3002", "motivation": "\u5728\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5904\u7406\u5177\u6709\u957f\u7a0b\u76f8\u5173\u6027\u7684\u6570\u636e\u96c6\u7684\u80cc\u666f\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u8861\u91cf\u60c5\u5883\u6027\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u7684\u52a8\u673a\uff0c\u4ee5\u4f30\u8ba1\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u60c5\u5883\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f3ak-\u60c5\u5883\u6027\u7684\u65b0\u60c5\u5883\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5177\u6709\u5f3ak-\u60c5\u5883\u6027\u7684\u7ffb\u8bd1\u4efb\u52a1\u65e0\u6cd5\u88ab\u5177\u6709\u5c11\u4e8ek\u4e2a\u6f5c\u5728\u72b6\u6001\u7684\u7ecf\u5178\u6d41\u6a21\u578b\u4ee5\u6709\u9650\u7684\u76f8\u5bf9\u71b5\u8868\u793a\u3002\u8be5\u76f8\u5173\u6027\u5ea6\u91cf\u4e0d\u4f1a\u5bf9\u91cf\u5b50\u751f\u6210\u6a21\u578b\u4ea7\u751f\u7c7b\u4f3c\u7684\u8d44\u6e90\u4e0b\u754c\u3002", "result": "\u7ecf\u9a8c\u8868\u660e\uff0c\u5f3ak-\u60c5\u5883\u6027\u7684\u4f30\u8ba1\u503c\u53ef\u4ee5\u9884\u6d4b\u8d44\u6e90\u53d7\u9650\u7684\u7ecf\u5178\u548c\u91cf\u5b50\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u5efa\u6a21\u6570\u636e\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5f3ak-\u60c5\u5883\u6027\u53ef\u4ee5\u4f5c\u4e3a\u8861\u91cf\u95ee\u9898\u7684\u96be\u5ea6\u7684\u6307\u6807\uff0c\u8fd9\u5bf9\u4e8e\u7ecf\u5178\u8ba1\u7b97\u673a\u6765\u8bf4\u53ef\u80fd\u5f88\u56f0\u96be\uff0c\u4f46\u5bf9\u4e8e\u91cf\u5b50\u8ba1\u7b97\u673a\u6765\u8bf4\u5219\u4e0d\u7136\u3002"}}
{"id": "2507.11574", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11574", "abs": "https://arxiv.org/abs/2507.11574", "authors": ["Kazuma Kobayashi", "Shailesh Garg", "Farid Ahmed", "Souvik Chakraborty", "Syed Bahauddin Alam"], "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators", "comment": null, "summary": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe\ndeployment of deep learning in real-time virtual sensing, particularly in\nhigh-stakes domains where sparse, noisy, or non-collocated sensor data are the\nnorm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework\nthat transforms neural operator-based virtual sensing with calibrated,\ndistribution-free prediction intervals. By unifying Monte Carlo dropout with\nsplit conformal prediction in a single DeepONet architecture, CMCO achieves\nspatially resolved uncertainty estimates without retraining, ensembling, or\ncustom loss design. Our method addresses a longstanding challenge: how to endow\noperator learning with efficient and reliable UQ across heterogeneous domains.\nThrough rigorous evaluation on three distinct applications: turbulent flow,\nelastoplastic deformation, and global cosmic radiation dose estimation-CMCO\nconsistently attains near-nominal empirical coverage, even in settings with\nstrong spatial gradients and proxy-based sensing. This breakthrough offers a\ngeneral-purpose, plug-and-play UQ solution for neural operators, unlocking\nreal-time, trustworthy inference in digital twins, sensor fusion, and\nsafety-critical monitoring. By bridging theory and deployment with minimal\ncomputational overhead, CMCO establishes a new foundation for scalable,\ngeneralizable, and uncertainty-aware scientific machine learning.", "AI": {"tldr": "\"CMCO \u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u865a\u62df\u4f20\u611f\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5b83\u5c06\u8499\u7279\u5361\u6d1b dropout \u548c\u5206\u88c2\u4fdd\u5f62\u9884\u6d4b\u7ed3\u5408\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u590d\u6742\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u3002\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u90fd\u663e\u793a\u51fa\u826f\u597d\u7684\u6548\u679c\u3002\"", "motivation": "\"\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u865a\u62df\u4f20\u611f\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\u3001\u5608\u6742\u6216\u975e\u5171\u7f6e\u7684\u9886\u57df\uff0cCMCO \u65e8\u5728\u63d0\u4f9b\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\"", "method": "\"CMCO \u6846\u67b6\u901a\u8fc7\u5c06\u8499\u7279\u5361\u6d1b dropout \u4e0e\u5206\u88c2\u4fdd\u5f62\u9884\u6d4b\u76f8\u7ed3\u5408\uff0c\u5728\u4e00\u4e2a DeepONet \u67b6\u6784\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ecf\u8fc7\u6821\u51c6\u7684\u3001\u65e0\u5206\u5e03\u9884\u6d4b\u533a\u95f4\u7684\u795e\u7ecf\u7b97\u5b50\u865a\u62df\u4f20\u611f\u3002\"", "result": "\"CMCO \u5728\u6e4d\u6d41\u3001\u5f39\u5851\u6027\u53d8\u5f62\u548c\u5168\u7403\u5b87\u5b99\u8f90\u5c04\u5242\u91cf\u4f30\u7b97\u4e09\u4e2a\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u5728\u5177\u6709\u5f3a\u7a7a\u95f4\u68af\u5ea6\u548c\u4ee3\u7406\u4f20\u611f\u7684\u8bbe\u7f6e\u4e2d\uff0c\u4e5f\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6807\u79f0\u7684\u7ecf\u9a8c\u8986\u76d6\u7387\u3002\"", "conclusion": "\"CMCO \u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u795e\u7ecf\u7b97\u5b50\uff0c\u5728\u6570\u5b57\u5b6a\u751f\u3001\u4f20\u611f\u5668\u878d\u5408\u548c\u5b89\u5168\u5173\u952e\u76d1\u63a7\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u53ef\u4fe1\u7684\u63a8\u7406\u3002\u901a\u8fc7\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u8fde\u63a5\u7406\u8bba\u548c\u90e8\u7f72\uff0cCMCO \u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u6cdb\u5316\u4e14\u80fd\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\u3002\""}}
{"id": "2507.11806", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.11806", "abs": "https://arxiv.org/abs/2507.11806", "authors": ["Hendrik Kra\u00df", "Ju Huang", "Seyed Mohamad Moosavi"], "title": "MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling", "comment": null, "summary": "Universal machine learning interatomic potentials (uMLIPs) have emerged as\npowerful tools for accelerating atomistic simulations, offering scalable and\nefficient modeling with accuracy close to quantum calculations. However, their\nreliability and effectiveness in practical, real-world applications remain an\nopen question. Metal-organic frameworks (MOFs) and related nanoporous materials\nare highly porous crystals with critical relevance in carbon capture, energy\nstorage, and catalysis applications. Modeling nanoporous materials presents\ndistinct challenges for uMLIPs due to their diverse chemistry, structural\ncomplexity, including porosity and coordination bonds, and the absence from\nexisting training datasets. Here, we introduce MOFSimBench, a benchmark to\nevaluate uMLIPs on key materials modeling tasks for nanoporous materials,\nincluding structural optimization, molecular dynamics (MD) stability, the\nprediction of bulk properties, such as bulk modulus and heat capacity, and\nguest-host interactions. Evaluating over 20 models from various architectures\non a chemically and structurally diverse materials set, we find that\ntop-performing uMLIPs consistently outperform classical force fields and\nfine-tuned machine learning potentials across all tasks, demonstrating their\nreadiness for deployment in nanoporous materials modeling. Our analysis\nhighlights that data quality, particularly the diversity of training sets and\ninclusion of out-of-equilibrium conformations, plays a more critical role than\nmodel architecture in determining performance across all evaluated uMLIPs. We\nrelease our modular and extendable benchmarking framework at\nhttps://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide\nthe adoption for nanoporous materials modeling and further development of\nuMLIPs.", "AI": {"tldr": "uMLIPs\u5728\u591a\u5b54\u6750\u6599\u5efa\u6a21\u65b9\u9762\u5df2\u51c6\u5907\u597d\u90e8\u7f72\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u529b\u573a\u548c\u5fae\u8c03\u7684\u673a\u5668\u5b66\u4e60\u52bf\u3002\u6a21\u578b\u67b6\u6784\u4e0d\u5982\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5305\u542b\u975e\u5e73\u8861\u6784\u8c61\u7684\u6570\u636e\u3002", "motivation": "\u91d1\u5c5e\u6709\u673a\u6846\u67b6\uff08MOFs\uff09\u548c\u76f8\u5173\u7eb3\u7c73\u591a\u5b54\u6750\u6599\u5728\u78b3\u6355\u83b7\u3001\u50a8\u80fd\u548c\u50ac\u5316\u5e94\u7528\u4e2d\u5177\u6709\u5173\u952e\u610f\u4e49\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u5316\u5b66\u6027\u8d28\u591a\u6837\u3001\u7ed3\u6784\u590d\u6742\uff08\u5305\u62ec\u5b54\u9699\u7387\u548c\u914d\u4f4d\u952e\uff09\u4ee5\u53ca\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u8fd9\u4e9b\u6750\u6599\uff0cuMLIPs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u8fd9\u7c7b\u6750\u6599\u7684\u5efa\u6a21\u4ecd\u5b58\u5728\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMOFSimBench\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30uMLIPs\u5728\u591a\u5b54\u6750\u6599\u5efa\u6a21\u7684\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u7ed3\u6784\u4f18\u5316\u3001\u5206\u5b50\u52a8\u529b\u5b66\u7a33\u5b9a\u6027\u3001\u4f53\u6750\u6599\u6027\u8d28\uff08\u5982\u4f53\u79ef\u6a21\u91cf\u548c\u70ed\u5bb9\uff09\u7684\u9884\u6d4b\u4ee5\u53ca\u5ba2\u4e3b\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5316\u5b66\u548c\u7ed3\u6784\u4e0a\u591a\u6837\u5316\u7684\u6750\u6599\u96c6\u4e0a\u5bf920\u591a\u4e2a\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u540e\uff0c\u53d1\u73b0\u6027\u80fd\u6700\u4f73\u7684uMLIPs\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u529b\u573a\u548c\u5fae\u8c03\u7684\u673a\u5668\u5b66\u4e60\u52bf\u3002\u6570\u636e\u8d28\u91cf\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\u548c\u5305\u542b\u975e\u5e73\u8861\u6784\u8c61\uff0c\u5bf9\u51b3\u5b9auMLIPs\u7684\u6027\u80fd\u8d77\u7740\u6bd4\u6a21\u578b\u67b6\u6784\u66f4\u91cd\u8981\u7684\u4f5c\u7528\u3002", "conclusion": "uMLIPs\u5728\u591a\u5b54\u6750\u6599\u5efa\u6a21\u65b9\u9762\u5df2\u51c6\u5907\u597d\u90e8\u7f72\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u529b\u573a\u548c\u5fae\u8c03\u7684\u673a\u5668\u5b66\u4e60\u52bf\u3002\u6a21\u578b\u67b6\u6784\u4e0d\u5982\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5305\u542b\u975e\u5e73\u8861\u6784\u8c61\u7684\u6570\u636e\u3002"}}
{"id": "2507.11912", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11912", "abs": "https://arxiv.org/abs/2507.11912", "authors": ["Tzu-Hsuan Chou", "Nicolo Michelusi", "David J. Love", "James V. Krogmeier"], "title": "Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks", "comment": "This paper is accepted for publication in IEEE Transactions on\n  Communications. 17 pages", "summary": "In this paper, a novel approach is proposed to improve the minimum\nsignal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal\nmulti-user wireless relay networks, by optimizing the placement of unmanned\naerial vehicle (UAV) relays, relay beamforming, and receive combining. The\ndesign is separated into two problems: beamforming-aware UAV placement\noptimization and transceiver design for minimum SINR maximization. A\nsignificant challenge in beamforming-aware UAV placement optimization is the\nlack of instantaneous channel state information (CSI) prior to deploying UAV\nrelays, making it difficult to derive the beamforming SINR in non-orthogonal\nmulti-user transmission. To address this issue, an approximation of the\nexpected beamforming SINR is derived using the narrow beam property of a\nmassive MIMO base station. Based on this, a UAV placement algorithm is proposed\nto provide UAV positions that improve the minimum expected beamforming SINR\namong users, using a difference-of-convex framework. Subsequently, after\ndeploying the UAV relays to the optimized positions, and with estimated CSI\navailable, a joint relay beamforming and receive combining (JRBC) algorithm is\nproposed to optimize the transceiver to improve the minimum beamforming SINR\namong users, using a block-coordinate descent approach. Numerical results show\nthat the UAV placement algorithm combined with the JRBC algorithm provides a\n4.6 dB SINR improvement over state-of-the-art schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65e0\u4eba\u673a\u4e2d\u7ee7\u4f4d\u7f6e\u548c\u6536\u53d1\u5668\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u975e\u6b63\u4ea4\u591a\u7528\u6237\u65e0\u7ebf\u4e2d\u7ee7\u7f51\u7edc\u4e2d\u7528\u6237\u7684\u6700\u5c0f SINR\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 SINR \u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6848\u6709\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u975e\u6b63\u4ea4\u591a\u7528\u6237\u65e0\u7ebf\u4e2d\u7ee7\u7f51\u7edc\u4e2d\u7528\u6237\u7684\u6700\u5c0f\u4fe1\u566a\u6bd4\uff08SINR\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u65e0\u4eba\u673a\uff08UAV\uff09\u4e2d\u7ee7\u7684\u4f4d\u7f6e\u3001\u4e2d\u7ee7\u6ce2\u675f\u5f62\u6210\u548c\u63a5\u6536\u7ec4\u5408\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u8bbe\u8ba1\u5206\u4e3a\u4e24\u4e2a\u95ee\u9898\uff1a\u6ce2\u675f\u5f62\u6210\u611f\u77e5\u65e0\u4eba\u673a\u4f4d\u7f6e\u4f18\u5316\u548c\u7528\u4e8e\u6700\u5c0f\u5316\u4fe1\u566a\u6bd4\u6700\u5927\u5316\u7684\u6536\u53d1\u5668\u8bbe\u8ba1\u3002\u5229\u7528\u5927\u89c4\u6a21 MIMO \u57fa\u7ad9\u7684\u7a84\u6ce2\u675f\u7279\u6027\uff0c\u63a8\u5bfc\u4e86\u671f\u671b\u6ce2\u675f\u5f62\u6210\u4fe1\u566a\u6bd4\u7684\u8fd1\u4f3c\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u4f4d\u7f6e\u7b97\u6cd5\uff0c\u4ee5\u4f7f\u7528\u5dee\u503c\u51f8\u6846\u67b6\u6765\u63d0\u4f9b\u6539\u8fdb\u6700\u5c0f\u671f\u671b\u6ce2\u675f\u5f62\u6210\u4fe1\u566a\u6bd4\u7684\u7528\u6237\u4f4d\u7f6e\u3002\u968f\u540e\uff0c\u5728\u5c06\u65e0\u4eba\u673a\u4e2d\u7ee7\u90e8\u7f72\u5230\u4f18\u5316\u4f4d\u7f6e\u5e76\u63d0\u4f9b\u4f30\u8ba1\u7684 CSI \u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4e2d\u7ee7\u6ce2\u675f\u5f62\u6210\u548c\u63a5\u6536\u7ec4\u5408\uff08JRBC\uff09\u7b97\u6cd5\uff0c\u4ee5\u4f7f\u7528\u5757\u5750\u6807\u4e0b\u964d\u65b9\u6cd5\u4f18\u5316\u6536\u53d1\u5668\u4ee5\u6539\u8fdb\u6700\u5c0f\u6ce2\u675f\u5f62\u6210\u4fe1\u566a\u6bd4\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u4f4d\u7f6e\u7b97\u6cd5\u7ed3\u5408 JRBC \u7b97\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6848\u76f8\u6bd4\uff0cSINR \u63d0\u9ad8\u4e86 4.6 dB\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u4f4d\u7f6e\u7b97\u6cd5\u7ed3\u5408\u8054\u5408\u4e2d\u7ee7\u6ce2\u675f\u5f62\u6210\u548c\u63a5\u6536\u7ec4\u5408\uff08JRBC\uff09\u7b97\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6848\u76f8\u6bd4\uff0cSINR \u63d0\u9ad8\u4e86 4.6 dB\u3002"}}
{"id": "2507.12130", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.12130", "abs": "https://arxiv.org/abs/2507.12130", "authors": ["Adithya Bijoy", "Ankit Mondal", "Ashish Chiplunkar"], "title": "Weighted $k$-Server Admits an Exponentially Competitive Algorithm", "comment": null, "summary": "The weighted $k$-server is a variant of the $k$-server problem, where the\ncost of moving a server is the server's weight times the distance through which\nit moves. The problem is famous for its intriguing properties and for evading\nstandard techniques for designing and analyzing online algorithms. Even on\nuniform metric spaces with sufficiently many points, the deterministic\ncompetitive ratio of weighted $k$-server is known to increase doubly\nexponentially with respect to $k$, while the behavior of its randomized\ncompetitive ratio is not fully understood. Specifically, no upper bound better\nthan doubly exponential is known, while the best known lower bound is singly\nexponential in $k$. In this paper, we close the exponential gap between these\nbounds by giving an $\\exp(O(k^2))$-competitive randomized online algorithm for\nthe weighted $k$-server problem on uniform metrics, thus breaking the doubly\nexponential barrier for deterministic algorithms for the first time. This is\nachieved by a recursively defined notion of a phase which, on the one hand,\nforces a lower bound on the cost of any offline solution, while, on the other\nhand, also admits a randomized online algorithm with bounded expected cost. The\nalgorithm is also recursive; it involves running several algorithms virtually\nand in parallel and following the decisions of one of them in a random order.\nWe also show that our techniques can be lifted to construct an\n$\\exp(O(k^2))$-competitive randomized online algorithm for the generalized\n$k$-server problem on weighted uniform metrics.", "AI": {"tldr": "\u672c\u6587\u4e3a\u52a0\u6743 k-\u670d\u52a1\u5668\u95ee\u9898\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ade\u4e89\u6bd4\u4e3a exp(O(k^2)) \u7684\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u6253\u7834\u4e86\u4e4b\u524d\u7684\u6307\u6570\u7ea7\u58c1\u5792\u3002", "motivation": "\u52a0\u6743 k-\u670d\u52a1\u5668\u95ee\u9898\u4ee5\u5176\u590d\u6742\u7684\u6027\u8d28\u548c\u9003\u907f\u6807\u51c6\u5728\u7ebf\u7b97\u6cd5\u6280\u672f\u800c\u95fb\u540d\uff0c\u5176\u786e\u5b9a\u6027\u7ade\u4e89\u6bd4\u968f k \u6307\u6570\u7ea7\u589e\u957f\uff0c\u800c\u968f\u673a\u7ade\u4e89\u6bd4\u7684\u884c\u4e3a\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\uff0c\u672c\u6587\u65e8\u5728\u7f29\u5c0f\u968f\u673a\u548c\u786e\u5b9a\u6027\u7ade\u4e89\u6bd4\u4e4b\u95f4\u7684\u6307\u6570\u7ea7\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u201c\u9636\u6bb5\u201d\u7684\u6982\u5ff5\uff0c\u5bf9\u4efb\u4f55\u79bb\u7ebf\u89e3\u65bd\u52a0\u6210\u672c\u4e0b\u754c\uff0c\u540c\u65f6\u5141\u8bb8\u5177\u6709\u6709\u754c\u671f\u671b\u6210\u672c\u7684\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u672c\u8eab\u4e5f\u662f\u9012\u5f52\u7684\uff0c\u6d89\u53ca\u865a\u62df\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u7b97\u6cd5\uff0c\u5e76\u4ee5\u968f\u673a\u987a\u5e8f\u9075\u5faa\u5176\u4e2d\u4e00\u4e2a\u7b97\u6cd5\u7684\u51b3\u7b56\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ade\u4e89\u6bd4\u4e3a exp(O(k^2)) \u7684\u52a0\u6743 k-\u670d\u52a1\u5668\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6253\u7834\u4e86\u786e\u5b9a\u6027\u7b97\u6cd5\u7684\u6307\u6570\u7ea7\u58c1\u5792\u3002\u8be5\u6280\u672f\u8fd8\u53ef\u4ee5\u63a8\u5e7f\u5230\u52a0\u6743\u7edf\u4e00\u5ea6\u91cf\u4e0a\u7684\u5e7f\u4e49 k-\u670d\u52a1\u5668\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3a\u52a0\u6743 k-\u670d\u52a1\u5668\u95ee\u9898\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709 exp(O(k^2)) \u7ade\u4e89\u6bd4\u7684\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u6253\u7834\u4e86\u786e\u5b9a\u6027\u7b97\u6cd5\u7684\u6307\u6570\u7ea7\u58c1\u5792\uff0c\u5e76\u5c06\u5176\u6280\u672f\u63a8\u5e7f\u5230\u52a0\u6743\u7edf\u4e00\u5ea6\u91cf\u4e0a\u7684\u5e7f\u4e49 k-\u670d\u52a1\u5668\u95ee\u9898\u3002"}}
{"id": "2507.11961", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.11961", "abs": "https://arxiv.org/abs/2507.11961", "authors": ["Pascal Kettmann", "Jesse Heyninck", "Hannes Strass"], "title": "Approximation Fixpoint Theory as a Unifying Framework for Fuzzy Logic Programming Semantics (Extended Version)", "comment": null, "summary": "Fuzzy logic programming is an established approach for reasoning under\nuncertainty. Several semantics from classical, two-valued logic programming\nhave been generalized to the case of fuzzy logic programs. In this paper, we\nshow that two of the most prominent classical semantics, namely the stable\nmodel and the well-founded semantics, can be reconstructed within the general\nframework of approximation fixpoint theory (AFT). This not only widens the\nscope of AFT from two- to many-valued logics, but allows a wide range of\nexisting AFT results to be applied to fuzzy logic programming. As first\nexamples of such applications, we clarify the formal relationship between\nexisting semantics, generalize the notion of stratification from classical to\nfuzzy logic programs, and devise \"more precise\" variants of the semantics.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\u7edf\u4e00\u4e86\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6a21\u578b\u548c\u6709\u6839\u636e\u6a21\u578b\u8bed\u4e49\uff0c\u5e76\u6269\u5c55\u4e86AFT\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u4e3a\u4e86\u5c06\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\u7684\u8303\u56f4\u4ece\u4e8c\u503c\u903b\u8f91\u6269\u5c55\u5230\u591a\u503c\u903b\u8f91\uff0c\u5e76\u4f7f\u73b0\u6709\u7684AFT\u7ed3\u679c\u80fd\u591f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\u3002", "method": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u7ecf\u5178\u903b\u8f91\u7f16\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6a21\u578b\u548c\u6709\u6839\u636e\u6a21\u578b\u8bed\u4e49\u91cd\u6784\u5230\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\u7684\u901a\u7528\u6846\u67b6\u4e2d\u3002", "result": "\u672c\u6587\u6f84\u6e05\u4e86\u73b0\u6709\u8bed\u4e49\u4e4b\u95f4\u7684\u5f62\u5f0f\u5173\u7cfb\uff0c\u5c06\u5206\u5c42\u7ed3\u6784\u7684\u6982\u5ff5\u4ece\u7ecf\u5178\u6a21\u7cca\u903b\u8f91\u7a0b\u5e8f\u63a8\u5e7f\u5230\u6a21\u7cca\u903b\u8f91\u7a0b\u5e8f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fd9\u4e9b\u8bed\u4e49\u7684\u201c\u66f4\u7cbe\u786e\u201d\u53d8\u4f53\u3002", "conclusion": "\u672c\u6587\u5c06\u4e24\u4e2a\u6700\u8457\u540d\u7684\u7ecf\u5178\u8bed\u4e49\uff0c\u5373\u7a33\u5b9a\u6a21\u578b\u548c\u6709\u6839\u636e\u6a21\u578b\uff0c\u5728\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\u7684\u901a\u7528\u6846\u67b6\u5185\u8fdb\u884c\u91cd\u5efa\u3002\u8fd9\u4e0d\u4ec5\u5c06AFT\u7684\u8303\u56f4\u4ece\u4e8c\u503c\u903b\u8f91\u6269\u5c55\u5230\u591a\u503c\u903b\u8f91\uff0c\u800c\u4e14\u5141\u8bb8\u5c06\u73b0\u6709\u7684AFT\u7ed3\u679c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\u3002"}}
{"id": "2507.11933", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.11933", "abs": "https://arxiv.org/abs/2507.11933", "authors": ["Zuxuan Ou", "Hui-Qiang Liang", "Guo-Fu Xu", "Linhu Li"], "title": "Anisotropic-scaling localization in higher-dimensional non-Hermitian systems", "comment": "26 pages, 13 figures", "summary": "Spatial localization of quantum states is one of the focal points in\ncondensed matter physics and quantum simulations, as it signatures profound\nphysics such as nontrivial band topology and non-reciprocal non-Hermiticity.\nYet, in higher dimensions, characterizing state localization becomes elusive\ndue to the sophisticated interplay between different localization mechanisms\nand spacial geometries. In this work, we unveil an exotic type of localization\nphenomenon in higher-dimensional non-Hermitian systems, termed\nanisotropic-scaling localization (ASL), where localization lengths follow\ndistinct size-dependent scaling rules in an anisotropic manner. Assisted with\nboth analytical solution and numerical simulation, we find that ASL can emerge\nfrom two different mechanisms of effective bulk couplings or one-dimensional\njunction between different 1D edges, depending on how non-reciprocity is\nintroduced to the system. The competition between ASL states and edge\nnon-Hermitian skin states are further identified by their complex and real\neigenenergies, respectively. Our results resolve the subtle co-existence of\nloop-like spectrum and skin-like localization of boundary states in\ncontemporary literature, and provide a framework to classify the intricate\nhigher-order non- Hermitian localization regarding their localization profiles.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u66f4\u9ad8\u7ef4\u5ea6\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u4e00\u79cd\u65b0\u5c40\u57df\u5316\u73b0\u8c61\u2014\u2014\u5404\u5411\u5f02\u6027\u5b9a\u6807\u5c40\u57df\u5316\uff08ASL\uff09\uff0c\u5b83\u5177\u6709\u5c3a\u5bf8\u4f9d\u8d56\u7684\u5404\u5411\u5f02\u6027\u5c40\u57df\u5316\u957f\u5ea6\u3002ASL\u7684\u4ea7\u751f\u673a\u5236\u4e0e\u975e\u5012\u6613\u6027\u7684\u5f15\u5165\u65b9\u5f0f\u6709\u5173\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7279\u5f81\u503c\u7684\u5b9e\u90e8\u6216\u865a\u90e8\u6765\u533a\u5206ASL\u6001\u4e0e\u975e\u5384\u7c73\u76ae\u80a4\u6001\u3002\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u5173\u4e8e\u73af\u72b6\u8c31\u548c\u8fb9\u754c\u6001\u76ae\u80a4\u72b6\u5c40\u57df\u5316\u5171\u5b58\u7684\u96be\u9898\uff0c\u5e76\u4e3a\u7406\u89e3\u548c\u5206\u7c7b\u590d\u6742\u7684\u66f4\u9ad8\u7ef4\u5ea6\u975e\u5384\u7c73\u5c40\u57df\u5316\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002", "motivation": "\u7a7a\u95f4\u91cf\u5b50\u6001\u5c40\u57df\u5316\u662f\u51dd\u805a\u6001\u7269\u7406\u548c\u91cf\u5b50\u6a21\u62df\u7684\u7126\u70b9\u4e4b\u4e00\uff0c\u56e0\u4e3a\u5b83\u6807\u5fd7\u7740\u975e\u5e73\u51e1\u7684\u80fd\u5e26\u62d3\u6251\u548c\u975e\u5012\u6613\u6027\u975e\u5384\u7c73\u6027\u7b49\u6df1\u523b\u7684\u7269\u7406\u73b0\u8c61\u3002\u7136\u800c\uff0c\u5728\u66f4\u9ad8\u7ef4\u5ea6\u4e2d\uff0c\u7531\u4e8e\u4e0d\u540c\u5c40\u57df\u5316\u673a\u5236\u548c\u7a7a\u95f4\u51e0\u4f55\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6001\u5c40\u57df\u5316\u7684\u8868\u5f81\u53d8\u5f97\u96be\u4ee5\u6349\u6478\u3002", "method": "\u901a\u8fc7\u89e3\u6790\u89e3\u548c\u6570\u503c\u6a21\u62df\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u5404\u5411\u5f02\u6027\u5b9a\u6807\u5c40\u57df\u5316\uff08ASL\uff09\u73b0\u8c61\uff0c\u5176\u4e2d\u5c40\u57df\u5316\u957f\u5ea6\u9075\u5faa\u4e0d\u540c\u7684\u5c3a\u5bf8\u4f9d\u8d56\u5b9a\u6807\u89c4\u5219\u3002ASL\u53ef\u4ee5\u6e90\u4e8e\u4e24\u79cd\u4e0d\u540c\u7684\u673a\u5236\uff1a\u6709\u6548\u7684\u4f53\u8026\u5408\u6216\u4e0d\u540c\u4e00\u7ef4\u8fb9\u7f18\u4e4b\u95f4\u7684\u7ed3\u3002\u901a\u8fc7\u590d\u548c\u5b9e\u7279\u5f81\u503c\u5206\u522b\u8bc6\u522b\u4e86ASL\u6001\u548c\u8fb9\u7f18\u975e\u5384\u7c73\u76ae\u80a4\u6001\u7684\u7ade\u4e89\uff0c\u89e3\u51b3\u4e86\u73af\u72b6\u8c31\u548c\u8fb9\u754c\u6001\u76ae\u80a4\u72b6\u5c40\u57df\u5316\u5171\u5b58\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u63ed\u793a\u4e86\u4e00\u79cd\u5728\u66f4\u9ad8\u7ef4\u5ea6\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u5916\u6765\u7684\u5c40\u57df\u5316\u73b0\u8c61\uff0c\u79f0\u4e3a\u5404\u5411\u5f02\u6027\u5b9a\u6807\u5c40\u57df\u5316\uff08ASL\uff09\uff0c\u5176\u4e2d\u5c40\u57df\u5316\u957f\u5ea6\u9075\u5faa\u4e0d\u540c\u7684\u5c3a\u5bf8\u4f9d\u8d56\u5b9a\u6807\u89c4\u5219\uff0c\u8868\u73b0\u51fa\u5404\u5411\u5f02\u6027\u3002ASL\u53ef\u4ee5\u6e90\u4e8e\u4e24\u79cd\u4e0d\u540c\u7684\u673a\u5236\uff1a\u6709\u6548\u7684\u4f53\u8026\u5408\u6216\u4e0d\u540c\u4e00\u7ef4\u8fb9\u7f18\u4e4b\u95f4\u7684\u7ed3\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u975e\u5012\u6613\u6027\u5f15\u5165\u7cfb\u7edf\u7684\u65b9\u5f0f\u3002\u901a\u8fc7\u590d\u548c\u5b9e\u7279\u5f81\u503c\u5206\u522b\u8bc6\u522b\u4e86ASL\u6001\u548c\u8fb9\u7f18\u975e\u5384\u7c73\u76ae\u80a4\u6001\u7684\u7ade\u4e89\u3002\u6211\u4eec\u7684\u7ed3\u679c\u89e3\u51b3\u4e86\u5f53\u4ee3\u6587\u732e\u4e2d\u73af\u72b6\u8c31\u548c\u8fb9\u754c\u6001\u76ae\u80a4\u72b6\u5c40\u57df\u5316\u8fd9\u79cd\u5fae\u5999\u7684\u5171\u5b58\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u5bf9\u5177\u6709\u590d\u6742\u66f4\u9ad8\u9636\u975e\u5384\u7c73\u5c40\u57df\u5316\u7684\u5c40\u57df\u5316\u8f6e\u5ed3\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2507.11733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11733", "abs": "https://arxiv.org/abs/2507.11733", "authors": ["Srikanth Vemula"], "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "comment": null, "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes.", "AI": {"tldr": "ClarifAI\u901a\u8fc7\u7ed3\u5408\u6848\u4f8b\u63a8\u7406\u548c\u672c\u4f53\u6765\u63d0\u9ad8AI\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4ee5\u6ee1\u8db3\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5e76\u53ef\u7528\u4e8e\u9ad8\u98ce\u9669\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8AI\u5728\u6539\u8fdb\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86ClarifAI\u3002", "method": "\u5229\u7528\u6848\u4f8b\u63a8\u7406\uff08CBR\uff09\u65b9\u6cd5\u5e76\u6574\u5408\u9762\u5411\u672c\u4f53\u7684\u65b9\u6cd5\uff0cClarifAI\u65e8\u5728\u6ee1\u8db3AI\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5404\u79cd\u5229\u76ca\u76f8\u5173\u8005\u7684\u590d\u6742\u89e3\u91ca\u9700\u6c42\u3002", "result": "ClarifAI\u7ed3\u5408\u4e86CBR\u548c\u672c\u4f53\uff0c\u63d0\u4f9b\u4e86\u8be6\u5c3d\u7684\u89e3\u91ca\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u8de8\u4e0d\u540c\u90e8\u95e8\u548c\u9ad8\u98ce\u9669\u73af\u5883\u7684AI\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86ClarifAI\u5728\u63d0\u9ad8AI\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u5728\u5173\u952e\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u90e8\u7f72AI\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.11872", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11872", "abs": "https://arxiv.org/abs/2507.11872", "authors": ["Wenhan Cao", "Tianyi Zhang", "Shengbo Eben Li"], "title": "Algorithm Design and Comparative Test of Natural Gradient Gaussian Approximation Filter", "comment": null, "summary": "Popular Bayes filters typically rely on linearization techniques such as\nTaylor series expansion and stochastic linear regression to use the structure\nof standard Kalman filter. These techniques may introduce large estimation\nerrors in nonlinear and non-Gaussian systems. This paper overviews a recent\nbreakthrough in filtering algorithm design called \\textit{N}atural\nGr\\textit{a}dient Gaussia\\textit{n} Appr\\textit{o}ximation (NANO) filter and\ncompare its performance over a large class of nonlinear filters. The NANO\nfilter interprets Bayesian filtering as solutions to two distinct optimization\nproblems, which allows to define optimal Gaussian approximation and derive its\ncorresponding extremum conditions. The algorithm design still follows the\ntwo-step structure of Bayes filters. In the prediction step, NANO filter\ncalculates the first two moments of the prior distribution, and this process is\nequivalent to a moment-matching filter. In the update step, natural gradient\ndescent is employed to directly minimize the objective of the update step,\nthereby avoiding errors caused by model linearization. Comparative tests are\nconducted on four classic systems, including the damped linear oscillator,\nsequence forecasting, modified growth model, and robot localization, under\nGaussian, Laplace, and Beta noise to evaluate the NANO filter's capability in\nhandling nonlinearity. Additionally, we validate the NANO filter's robustness\nto data outliers using a satellite attitude estimation example. It is observed\nthat the NANO filter outperforms popular Kalman filters family such as extended\nKalman filter (EKF), unscented Kalman filter (UKF), iterated extended Kalman\nfilter (IEKF) and posterior linearization filter (PLF), while having similar\ncomputational burden.", "AI": {"tldr": "NANO\u6ee4\u6ce2\u5668\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u907f\u514d\u7ebf\u6027\u5316\u8bef\u5dee\uff0c\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u4e0a\u4f18\u4e8eEKF\u3001UKF\u7b49\u6ee4\u6ce2\u5668\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edf\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u4e2d\u7ebf\u6027\u5316\u6280\u672f\uff08\u5982\u6cf0\u52d2\u5c55\u5f00\u548c\u968f\u673a\u7ebf\u6027\u56de\u5f52\uff09\u5728\u975e\u7ebf\u6027\u4e0e\u975e\u9ad8\u65af\u7cfb\u7edf\u4e0a\u5f15\u5165\u7684\u4f30\u8ba1\u8bef\u5dee\u3002", "method": "NANO\u6ee4\u6ce2\u5668\u5c06\u8d1d\u53f6\u65af\u6ee4\u6ce2\u89e3\u91ca\u4e3a\u4e24\u4e2a\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9a\u4e49\u4e86\u6700\u4f18\u9ad8\u65af\u8fd1\u4f3c\u53ca\u5176\u6781\u503c\u6761\u4ef6\u3002\u5728\u9884\u6d4b\u6b65\uff0c\u5b83\u8ba1\u7b97\u5148\u9a8c\u5206\u5e03\u7684\u524d\u4e24\u4e2a\u77e9\uff0c\u7b49\u540c\u4e8e\u77e9\u5339\u914d\u6ee4\u6ce2\u5668\uff1b\u5728\u66f4\u65b0\u6b65\uff0c\u91c7\u7528\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u6cd5\u76f4\u63a5\u6700\u5c0f\u5316\u66f4\u65b0\u76ee\u6807\uff0c\u907f\u514d\u4e86\u6a21\u578b\u7ebf\u6027\u5316\u8bef\u5dee\u3002", "result": "\u901a\u8fc7\u5728\u963b\u5c3c\u7ebf\u6027\u632f\u8361\u5668\u3001\u5e8f\u5217\u9884\u6d4b\u3001\u4fee\u6b63\u589e\u957f\u6a21\u578b\u548c\u673a\u5668\u4eba\u5b9a\u4f4d\u7b49\u56db\u4e2a\u7ecf\u5178\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u9ad8\u65af\u3001\u62c9\u666e\u62c9\u65af\u548cBeta\u566a\u58f0\uff0c\u9a8c\u8bc1\u4e86NANO\u6ee4\u6ce2\u5668\u5904\u7406\u975e\u7ebf\u6027\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u7684\u4f8b\u5b50\uff0c\u9a8c\u8bc1\u4e86NANO\u6ee4\u6ce2\u5668\u5bf9\u6570\u636e\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "NANO\u6ee4\u6ce2\u5668\u5728\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u65f6\uff0c\u76f8\u6bd4\u4e8eEKF\u3001UKF\u3001IEKF\u548cPLF\u7b49\u6ee4\u6ce2\u5668\uff0c\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u8ba1\u7b97\u8d1f\u62c5\u76f8\u4f3c\u3002"}}
{"id": "2507.12442", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12442", "abs": "https://arxiv.org/abs/2507.12442", "authors": ["Saptarshi Mitra", "Rachid Karami", "Haocheng Xu", "Sitao Huang", "Hyoukjun Kwon"], "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length", "comment": "12 pages, 7 figures", "summary": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.", "AI": {"tldr": "SSMs\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u6bd4Transformers\u66f4\u9002\u5408\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5904\u7406\u7684\u5e8f\u5217\u66f4\u957f\uff0c\u901f\u5ea6\u66f4\u5feb\u3002SSM\u7684\u5185\u6838\u662f\u6027\u80fd\u74f6\u9888\uff0c\u503c\u5f97\u5173\u6ce8\u3002", "motivation": "\u968f\u7740\u5bf9\u80fd\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u5904\u7406\u8fde\u7eed\u3001\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u7684\u673a\u5668\u667a\u80fd\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u4f46\u4f20\u7edf\u7684Transformer\u67b6\u6784\u56e0\u5176\u4e8c\u6b21\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u800c\u5728\u6548\u7387\u548c\u53ef\u7528\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5bf9SSM\u548c\u6df7\u5408\u6a21\u578b\u7b49\u65b0\u67b6\u6784\u5728\u5b9e\u9645\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u5316\u8868\u5f81\uff0c\u4ee5\u6307\u5bfc\u7cfb\u7edf\u7ea7\u4f18\u5316\u548c\u63a8\u52a8\u65b0\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5bf9Transformer\u3001SSM\u548c\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u6d88\u8d39\u7ea7\u548c\u5d4c\u5165\u5f0fGPU\u4e0a\u5bf9\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8fdb\u884c\u7cfb\u7edf\u5316\u6027\u80fd\u8868\u5f81\u3002\u5206\u6790\u4e86SSM\u7684\u786c\u4ef6\u611f\u77e5\u5185\u6838\u5bf9\u63a8\u7406\u5ef6\u8fdf\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bbe\u5907\u7279\u5b9a\u8868\u5f81\u3002", "result": "SSMs\u4e0d\u4ec5\u53ef\u884c\uff0c\u800c\u4e14\u5728\u8be5\u9886\u57df\u8868\u73b0\u66f4\u4f18\u8d8a\uff0c\u80fd\u591f\u572824GB\u6d88\u8d39\u7ea7GPU\u4e0a\u5904\u7406\u957f\u8fbe220K\u4e2atoken\u7684\u5e8f\u5217\uff0c\u8fd9\u5927\u7ea6\u662f\u540c\u7c7bTransformer\u76844\u500d\u3002\u867d\u7136Transformers\u5728\u77ed\u5e8f\u5217\u4e0a\u53ef\u80fd\u5feb1.8\u500d\uff0c\u4f46SSMs\u5728\u6781\u957f\u4e0a\u4e0b\u6587\uff08\u7ea657K token\uff09\u65f6\u901f\u5ea6\u53ef\u63d0\u53474\u500d\u3002SSM\u7684\u786c\u4ef6\u611f\u77e5\u5185\u6838\u5360\u7528\u4e86\u8d85\u8fc755%\u7684\u8fb9\u7f18\u5e73\u53f0\u63a8\u7406\u5ef6\u8fdf\uff0c\u662f\u672a\u6765\u786c\u4ef6\u52a0\u901f\u7684\u5173\u952e\u3002", "conclusion": "SSMs\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u4e8eTransformer\uff0c\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u80fd\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\uff08\u9ad8\u8fbe220K tokens\uff09\uff0c\u5e76\u4e14\u5728\u957f\u4e0a\u4e0b\u6587\u65f6\u901f\u5ea6\u66f4\u5feb\uff08\u9ad8\u8fbe4\u500d\uff09\u3002SSM\u7684\u786c\u4ef6\u611f\u77e5\u5185\u6838\u662f\u5f71\u54cd\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u662f\u672a\u6765\u786c\u4ef6\u52a0\u901f\u7684\u4e3b\u8981\u76ee\u6807\u3002"}}
{"id": "2507.11683", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11683", "abs": "https://arxiv.org/abs/2507.11683", "authors": ["Seth Ockerman", "Amal Gueroudji", "Tanwi Mallick", "Yixuan He", "Line Pouchard", "Robert Ross", "Shivaram Venkataraman"], "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training", "comment": "To be published in the 2025 International Conference for High\n  Performance Computing, Networking, Storage, and Analysis", "summary": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for\nmodeling spatial and temporal data dependencies. However, their applications\nhave been limited primarily to small-scale datasets because of memory\nconstraints. While distributed training offers a solution, current frameworks\nlack support for spatiotemporal models and overlook the properties of\nspatiotemporal data. Informed by a scaling study on a large-scale workload, we\npresent PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch\nGeometric Temporal that integrates distributed data parallel training and two\nnovel strategies: index-batching and distributed-index-batching. Our index\ntechniques exploit spatiotemporal structure to construct snapshots dynamically\nat runtime, significantly reducing memory overhead, while\ndistributed-index-batching extends this approach by enabling scalable\nprocessing across multiple GPUs. Our techniques enable the first-ever training\nof an ST-GNN on the entire PeMS dataset without graph partitioning, reducing\npeak memory usage by up to 89\\% and achieving up to a 13.1x speedup over\nstandard DDP with 128 GPUs.", "AI": {"tldr": "ST-GNNs \u5b58\u5728\u5185\u5b58\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u7684 PGT-I \u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u521b\u65b0\u7684\u7d22\u5f15\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u8bad\u7ec3\u3002", "motivation": "ST-GNNs \u5728\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u65b9\u9762\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u4e3b\u8981\u53d7\u9650\u4e8e\u5185\u5b58\uff0c\u5bfc\u81f4\u5176\u5e94\u7528\u89c4\u6a21\u53d7\u9650\u3002\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u7f3a\u4e4f\u5bf9 ST-GNNs \u7684\u652f\u6301\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u65f6\u7a7a\u6570\u636e\u7684\u7279\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PGT-I \u7684\u65b9\u6cd5\uff0c\u5b83\u6269\u5c55\u4e86 PyTorch Geometric Temporal\uff0c\u901a\u8fc7\u96c6\u6210\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u3001\u7d22\u5f15\u6279\u5904\u7406\u548c\u5206\u5e03\u5f0f\u7d22\u5f15\u6279\u5904\u7406\u7b56\u7565\u3002\u7d22\u5f15\u6280\u672f\u5229\u7528\u65f6\u7a7a\u7ed3\u6784\u52a8\u6001\u6784\u5efa\u5feb\u7167\uff0c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff0c\u800c\u5206\u5e03\u5f0f\u7d22\u5f15\u6279\u5904\u7406\u5219\u652f\u6301\u8de8\u591a\u4e2a GPU \u7684\u53ef\u6269\u5c55\u5904\u7406\u3002", "result": "PGT-I \u5b9e\u73b0\u4e86\u5728\u4e0d\u8fdb\u884c\u56fe\u5212\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u6574\u4e2a PeMS \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 ST-GNNs\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u91cf\u6700\u591a\u53ef\u51cf\u5c11 89%\uff0c\u5e76\u5728 128 \u4e2a GPU \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 13.1 \u500d\u7684\u6807\u51c6 DDP \u52a0\u901f\u3002", "conclusion": "PyTorch Geometric Temporal Index (PGT-I) \u6269\u5c55\u4e86 PyTorch Geometric Temporal\uff0c\u901a\u8fc7\u96c6\u6210\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u3001\u7d22\u5f15\u6279\u5904\u7406\u548c\u5206\u5e03\u5f0f\u7d22\u5f15\u6279\u5904\u7406\uff0c\u89e3\u51b3\u4e86 ST-GNNs \u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2507.11770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11770", "abs": "https://arxiv.org/abs/2507.11770", "authors": ["Giang Nguyen", "Mihai Pomarlan", "Sascha Jongebloed", "Nils Leusmann", "Minh Nhat Vu", "Michael Beetz"], "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies", "comment": "8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS2025)", "summary": "In robotics, the effective integration of environmental data into actionable\nknowledge remains a significant challenge due to the variety and\nincompatibility of data formats commonly used in scene descriptions, such as\nMJCF, URDF, and SDF. This paper presents a novel approach that addresses these\nchallenges by developing a unified scene graph model that standardizes these\nvaried formats into the Universal Scene Description (USD) format. This\nstandardization facilitates the integration of these scene graphs with robot\nontologies through semantic reporting, enabling the translation of complex\nenvironmental data into actionable knowledge essential for cognitive robotic\ncontrol. We evaluated our approach by converting procedural 3D environments\ninto USD format, which is then annotated semantically and translated into a\nknowledge graph to effectively answer competency questions, demonstrating its\nutility for real-time robotic decision-making. Additionally, we developed a\nweb-based visualization tool to support the semantic mapping process, providing\nusers with an intuitive interface to manage the 3D environment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11971", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11971", "abs": "https://arxiv.org/abs/2507.11971", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "comment": null, "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 3D \u5c42\u6b21\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 3D \u8868\u793a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u548c\u6613\u4e8e\u7f16\u8f91\u7684 3D \u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684 3D \u8868\u793a\uff08\u5982\u7f51\u683c\u3001\u4f53\u7d20\u3001\u70b9\u4e91\u548c NeRF\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u4efb\u52a1\u7279\u5b9a\u6027\u3001\u7f16\u8f91\u590d\u6742\u6027\u3001\u7ed3\u6784\u6a21\u7cca\u6027\u4ee5\u53ca\u6570\u636e\u590d\u6742\u6027\u4e0e\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 3D \u5c42\u6b21\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5bf9\u8c61\u7684\u8868\u9762\u548c\u5185\u90e8\u5206\u5e03\u7a00\u758f\u7684\u4ee3\u7406\u8282\u70b9\uff0c\u5e76\u901a\u8fc7\u5c42\u6b21\u7ed3\u6784\uff08\u6811\u72b6\u7ed3\u6784\uff09\u7ec4\u7ec7\u5b83\u4eec\u3002\u6bcf\u4e2a\u8282\u70b9\u90fd\u5b58\u50a8\u5176\u90bb\u57df\u5185\u7684\u5c40\u90e8\u5f62\u72b6\u548c\u7eb9\u7406\u4fe1\u606f\uff08\u7531\u5c0f\u578b MLP \u9690\u5f0f\u7f16\u7801\uff09\u3002\u67e5\u8be2\u4efb\u4f55 3D \u5750\u6807\u7684\u5c5e\u6027\u6d89\u53ca\u4ece\u9644\u8fd1\u548c\u7236\u8282\u70b9\u8fdb\u884c\u9ad8\u6548\u7684\u795e\u7ecf\u63d2\u503c\u548c\u8f7b\u91cf\u7ea7\u89e3\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 3D \u91cd\u5efa\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u8868\u8fbe\u6548\u7387\u3001\u9ad8\u4fdd\u771f\u6e32\u67d3\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u751f\u6210\u4e86\u7d27\u51d1\u7684\u8868\u793a\uff0c\u5176\u4e2d\u8282\u70b9\u4e0e\u5c40\u90e8\u8bed\u4e49\u5bf9\u9f50\uff0c\u80fd\u591f\u76f4\u63a5\u8fdb\u884c\u62d6\u653e\u7f16\u8f91\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8d28\u91cf-\u590d\u6742\u6027\u63a7\u5236\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u8868\u8fbe\u6548\u7387\u3001\u9ad8\u4fdd\u771f\u6e32\u67d3\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u53ef\u7f16\u8f91\u6027\u3002"}}
{"id": "2507.11661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "PgM\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u5212\u5206\u5668\u5c06\u591a\u6a21\u6001\u8868\u793a\u5212\u5206\u4e3a\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u5206\u522b\u8fdb\u884c\u5b66\u4e60\u548c\u91cd\u5efa\uff0c\u4ee5\u63d0\u9ad8\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6548\u679c\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4e3a\u4e86\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u80fd\u591f\u7075\u6d3b\u8c03\u6574\u4e0d\u540c\u6a21\u6001\u548c\u5206\u533a\u7684\u8868\u793a\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "PgM\u6846\u67b6\u7531\u6a21\u6001\u5212\u5206\u5668\u3001\u5355\u6a21\u6001\u5b66\u4e60\u5668\u3001\u914d\u5bf9\u6a21\u6001\u5b66\u4e60\u5668\u548c\u5355\u914d\u5bf9\u6a21\u6001\u89e3\u7801\u5668\u7ec4\u6210\uff0c\u5176\u4e2d\u6a21\u6001\u5212\u5206\u5668\u5c06\u5b66\u4e60\u5230\u7684\u6a21\u6001\u8868\u793a\u5206\u5272\u6210\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u6a21\u6001\u5b66\u4e60\u5668\u5305\u542b\u7528\u4e8e\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u5b66\u4e60\u7684\u4e13\u7528\u7ec4\u4ef6\uff0c\u800c\u5355\u914d\u5bf9\u6a21\u6001\u89e3\u7801\u5668\u5219\u57fa\u4e8e\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\u91cd\u5efa\u6a21\u6001\u8868\u793a\u3002", "result": "PgM\u6846\u67b6\u80fd\u591f\u5f7b\u5e95\u5b66\u4e60\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u5177\u6709\u7075\u6d3b\u8c03\u6574\u8868\u793a\u5206\u5e03\u7684\u80fd\u529b\uff0c\u540c\u65f6\u5141\u8bb8\u8de8\u6a21\u6001\u548c\u5206\u533a\u7684\u4e0d\u540c\u5b66\u4e60\u901f\u7387\uff0c\u5e76\u5728\u56db\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u548c\u73b0\u6709\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "conclusion": "PgM\u6846\u67b6\u5728\u56db\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fc1\u79fb\u5230\u73b0\u6709\u6a21\u578b\uff0c\u540c\u65f6\u53ef\u4ee5\u8c03\u6574\u6a21\u6001\u548c\u5206\u533a\u7684\u5206\u5e03\u4ee5\u53ca\u5b66\u4e60\u901f\u7387\u3002"}}
{"id": "2507.11558", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11558", "abs": "https://arxiv.org/abs/2507.11558", "authors": ["Changlu Chen", "Yanbin Liu", "Chaoxi Niu", "Ling Chen", "Tianqing Zhu"], "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting", "comment": null, "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 ST-VFM\uff0c\u4e00\u4e2a\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u4e13\u95e8\u7684\u91cd\u7f16\u7a0b\u9636\u6bb5\u6765\u9002\u914d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\uff0c\u89e3\u51b3\u4e86 VFMs \u5728\u65f6\u95f4\u5efa\u6a21\u548c\u6a21\u6001\u5dee\u8ddd\u65b9\u9762\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u8bc1\u660e ST-VFM \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u6355\u6349\u4e00\u7ef4\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\uff0c\u96be\u4ee5\u6a21\u62df\u51c6\u786e\u7684\u65f6\u7a7a\u9884\u6d4b\u6240\u5fc5\u9700\u7684\u66f4\u4e30\u5bcc\u7684\u65f6\u7a7a\uff08ST\uff09\u76f8\u5173\u6027\u3002\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7a7a\u95f4\u5148\u9a8c\uff0c\u4f46\u5728\u5e94\u7528\u4e8e ST \u4efb\u52a1\u65f6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1) \u7f3a\u4e4f\u56fa\u6709\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u548c (2) \u89c6\u89c9\u4e0e ST \u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528 VFMs \u8fdb\u884c\u65f6\u7a7a\u9884\u6d4b\u7684\u6846\u67b6\u3002", "method": "ST-VFM \u91c7\u7528\u4e00\u79cd\u53cc\u5206\u652f\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u539f\u59cb ST \u8f93\u5165\u548c\u8f85\u52a9 ST \u6d41\u8f93\u5165\uff0c\u5176\u4e2d\u6d41\u7f16\u7801\u4e86\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u5dee\u4fe1\u53f7\uff0c\u53ef\u89e3\u91ca\u4e3a\u52a8\u6001\u7a7a\u95f4\u7ebf\u7d22\u3002\u4e3a\u4e86\u6709\u6548\u5730\u5904\u7406\u8fd9\u4e9b\u53cc\u5206\u652f\u8f93\u5165\uff0cST-VFM \u5f15\u5165\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684\u91cd\u7f16\u7a0b\u9636\u6bb5\uff1a\u9884 VFM \u91cd\u7f16\u7a0b\u9636\u6bb5\u5e94\u7528\u65f6\u95f4\u611f\u77e5\u4ee4\u724c\u9002\u914d\u5668\u6765\u5d4c\u5165\u65f6\u95f4\u4e0a\u4e0b\u6587\u5e76\u5c06\u4e24\u4e2a\u5206\u652f\u5bf9\u9f50\u5230 VFM \u517c\u5bb9\u7684\u7279\u5f81\u7a7a\u95f4\uff1b\u540e VFM \u91cd\u7f16\u7a0b\u9636\u6bb5\u5f15\u5165\u53cc\u8fb9\u4ea4\u53c9\u63d0\u793a\u534f\u8c03\u6a21\u5757\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u6761\u4ef6\u52a8\u6001\u5730\u5b9e\u73b0\u5206\u652f\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u51bb\u7ed3\u7684 VFM \u4e3b\u5e72\u7684\u60c5\u51b5\u4e0b\u4e30\u5bcc\u8054\u5408\u8868\u793a\u5b66\u4e60\u3002", "result": "ST-VFM \u5728\u5341\u4e2a\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728 VFM \u4e3b\u5e72\uff08\u4f8b\u5982 DINO\u3001CLIP\u3001DEIT\uff09\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u4e86\u9a8c\u8bc1\u6027\u5206\u6790\u3002", "conclusion": "ST-VFM \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u7cfb\u7edf\u5730\u91cd\u65b0\u7f16\u7a0b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7528\u4e8e\u901a\u7528\u7684\u65f6\u7a7a\u9884\u6d4b\uff0c\u5e76\u5728\u5341\u4e2a\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u7684\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728 VFM \u4e3b\u5e72\uff08\u4f8b\u5982 DINO\u3001CLIP\u3001DEIT\uff09\u4e0a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u76f8\u5173\u7684\u9a8c\u8bc1\u6027\u5206\u6790\uff0c\u786e\u7acb\u4e86\u5176\u4f5c\u4e3a\u65f6\u7a7a\u9884\u6d4b\u7684\u5f3a\u5927\u901a\u7528\u6846\u67b6\u7684\u5730\u4f4d\u3002"}}
{"id": "2507.11660", "categories": ["cs.LG", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11660", "abs": "https://arxiv.org/abs/2507.11660", "authors": ["Joao F. Rocha", "Ke Xu", "Xingzhi Sun", "Ananya Krishna", "Dhananjay Bhaskar", "Blanche Mongeon", "Morgan Craig", "Mark Gerstein", "Smita Krishnaswamy"], "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics", "comment": null, "summary": "The advent of single-cell technology has significantly improved our\nunderstanding of cellular states and subpopulations in various tissues under\nnormal and diseased conditions by employing data-driven approaches such as\nclustering and trajectory inference. However, these methods consider cells as\nindependent data points of population distributions. With spatial\ntranscriptomics, we can represent cellular organization, along with dynamic\ncell-cell interactions that lead to changes in cell state. Still, key\ncomputational advances are necessary to enable the data-driven learning of such\ncomplex interactive cellular dynamics. While agent-based modeling (ABM)\nprovides a powerful framework, traditional approaches rely on handcrafted rules\nderived from domain knowledge rather than data-driven approaches. To address\nthis, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)\nintegrating ABM with deep learning to model intercellular communication, and\nits effect on the intracellular gene regulatory network. Using graph ODE\nnetworks (GDEs) with shared weights per cell type, our approach represents\ngenes as vertices and interactions as directed edges, dynamically learning\ntheir strengths through a designed attention mechanism. Trained to match\ncontinuous trajectories of simulated as well as inferred trajectories from\nspatial transcriptomics data, the model captures both intercellular and\nintracellular interactions, enabling a more adaptive and accurate\nrepresentation of cellular dynamics.", "AI": {"tldr": "STAGED\u901a\u8fc7\u7ed3\u5408ABM\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f7f\u7528\u56feODE\u7f51\u7edc\u5b66\u4e60\u7ec6\u80de\u95f4\u901a\u8baf\u548c\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6a21\u62df\u7ec6\u80de\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u7ec6\u80de\u6280\u672f\u548c\u8ba1\u7b97\u65b9\u6cd5\u5728\u5206\u6790\u7ec6\u80de\u52a8\u6001\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u5c06\u7ec6\u80de\u89c6\u4e3a\u72ec\u7acb\u6570\u636e\u70b9\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c4\u5219\u7684ABM\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u6027\u3002\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u5b66\u4e60\u590d\u6742\u7684\u7ec6\u80de\u76f8\u4e92\u4f5c\u7528\u52a8\u6001\u3002", "method": "STAGED\u6a21\u578b\uff0c\u4f7f\u7528\u56feODE\u7f51\u7edc\uff08GDEs\uff09\u8868\u793a\u57fa\u56e0\u548c\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\uff0c\u5e76\u8fdb\u884c\u8bad\u7ec3\u4ee5\u5339\u914d\u6a21\u62df\u548c\u6765\u81ea\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u7684\u63a8\u65ad\u8f68\u8ff9\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u6355\u6349\u7ec6\u80de\u95f4\u548c\u7ec6\u80de\u5185\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7ec6\u80de\u52a8\u6001\u66f4\u81ea\u9002\u5e94\u548c\u66f4\u51c6\u786e\u7684\u8868\u793a\u3002", "conclusion": "STAGED\u6574\u5408\u4e86ABM\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u901a\u8fc7\u56feODE\u7f51\u7edc\uff08GDEs\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u7ec6\u80de\u95f4\u901a\u8baf\u53ca\u5176\u5bf9\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u66f4\u81ea\u9002\u5e94\u3001\u66f4\u51c6\u786e\u5730\u8868\u793a\u7ec6\u80de\u52a8\u6001\u3002"}}
{"id": "2507.11619", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.11619", "abs": "https://arxiv.org/abs/2507.11619", "authors": ["Annarita Scocco", "Wai-Keong Mok", "Leandro Aolita", "Mario Collura", "Tobias Haug"], "title": "Rise and fall of nonstabilizerness via random measurements", "comment": null, "summary": "We investigate the dynamics of nonstabilizerness - also known as `magic' - in\nmonitored quantum circuits composed of random Clifford unitaries and local\nprojective measurements. For measurements in the computational basis, we derive\nan analytical model for dynamics of the stabilizer nullity, showing that it\ndecays in quantized steps and requires exponentially many measurements to\nvanish, which reveals the strong protection through Clifford scrambling. On the\nother hand, for measurements performed in rotated non-Clifford bases,\nmeasurements can both create and destroy nonstabilizerness. Here, the dynamics\nleads to a steady-state with non-trivial nonstabilizerness, independent of the\ninitial state. We find that Haar-random states equilibrate in constant time,\nwhereas stabilizer states exhibit linear-in-size relaxation time. While the\nstabilizer nullity is insensitive to the rotation angle, Stabilizer R\\'enyi\nEntropies expose a richer structure in their dynamics. Our results uncover\nsharp distinctions between coarse and fine-grained nonstabilizerness\ndiagnostics and demonstrate how measurements can both suppress and sustain\nquantum computational resources.", "AI": {"tldr": "\u7814\u7a76\u4e86\u76d1\u6d4b\u91cf\u5b50\u7535\u8def\u4e2d\u975e\u7a33\u5b9a\u6027\u7684\u52a8\u529b\u5b66\u3002\u8ba1\u7b97\u57fa\u6d4b\u91cf\u4f1a\u901a\u8fc7\u514b\u5229\u798f\u5fb7\u6253\u4e71\u6765\u4fdd\u62a4\u975e\u7a33\u5b9a\u6027\uff0c\u800c\u975e\u514b\u5229\u798f\u5fb7\u57fa\u6d4b\u91cf\u4f1a\u540c\u65f6\u4ea7\u751f\u548c\u7834\u574f\u975e\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u7a33\u5b9a\u7684\u975e\u7a33\u5b9a\u6027\u72b6\u6001\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u76d1\u6d4b\u91cf\u5b50\u7535\u8def\u4e2d\u975e\u7a33\u5b9a\uff08\u9b54\u6cd5\uff09\u7684\u52a8\u529b\u5b66\u3002", "method": "\u7814\u7a76\u4e86\u7531\u968f\u673a\u514b\u5229\u798f\u5fb7\u9149\u53d8\u6362\u548c\u5c40\u90e8\u5e7a\u6b63\u6d4b\u91cf\u7684\u76d1\u6d4b\u91cf\u5b50\u7535\u8def\u4e2d\u975e\u7a33\u5b9a\uff08\u4e5f\u79f0\u4e3a\u201c\u9b54\u6cd5\u201d\uff09\u7684\u52a8\u529b\u5b66\u3002\u5bf9\u4e8e\u8ba1\u7b97\u57fa\u4e2d\u7684\u6d4b\u91cf\uff0c\u63a8\u5bfc\u4e86\u7a33\u5b9a\u5668\u96f6\u5ea6\u52a8\u529b\u5b66\u7684\u89e3\u6790\u6a21\u578b\uff1b\u5bf9\u4e8e\u5728\u65cb\u8f6c\u7684\u975e\u514b\u5229\u798f\u5fb7\u57fa\u4e2d\u8fdb\u884c\u7684\u6d4b\u91cf\uff0c\u7814\u7a76\u4e86\u975e\u7a33\u5b9a\u6027\u7684\u52a8\u529b\u5b66\u3002", "result": "\u5bf9\u4e8e\u8ba1\u7b97\u57fa\u6d4b\u91cf\uff0c\u7a33\u5b9a\u5668\u96f6\u5ea6\u4ee5\u91cf\u5b50\u5316\u7684\u6b65\u957f\u8870\u51cf\uff0c\u9700\u8981\u6307\u6570\u6d4b\u91cf\u624d\u80fd\u6d88\u5931\u3002\u5bf9\u4e8e\u65cb\u8f6c\u975e\u514b\u5229\u798f\u5fb7\u57fa\u6d4b\u91cf\uff0c\u6d4b\u91cf\u4f1a\u540c\u65f6\u4ea7\u751f\u548c\u7834\u574f\u975e\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u975e\u5e73\u51e1\u7a33\u6001\u3002Haar\u968f\u673a\u72b6\u6001\u5728\u6052\u5b9a\u65f6\u95f4\u5185\u8fbe\u5230\u5e73\u8861\uff0c\u800c\u7a33\u5b9a\u5668\u72b6\u6001\u8868\u73b0\u51fa\u4e0e\u5927\u5c0f\u6210\u6b63\u6bd4\u7684\u5f1b\u8c6b\u65f6\u95f4\u3002\u7a33\u5b9a\u5668\u96f6\u5ea6\u5bf9\u65cb\u8f6c\u89d2\u4e0d\u654f\u611f\uff0c\u800c\u7a33\u5b9a\u5668R\u00e9nyi\u71b5\u63ed\u793a\u4e86\u66f4\u4e30\u5bcc\u7684\u52a8\u529b\u5b66\u7ed3\u6784\u3002", "conclusion": "\u6d4b\u91cf\u53ef\u4ee5\u540c\u65f6\u6291\u5236\u548c\u7ef4\u6301\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u975e\u7a33\u5b9a\u8bca\u65ad\u4e4b\u95f4\u7684\u663e\u8457\u533a\u522b\u3002"}}
{"id": "2507.11589", "categories": ["cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.11589", "abs": "https://arxiv.org/abs/2507.11589", "authors": ["Sandeep Suresh Cranganore", "Andrei Bodnar", "Arturs Berzins", "Johannes Brandstetter"], "title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "comment": "63 pages, 22 figures, 10 Tables, Github:\n  https://github.com/AndreiB137/EinFields", "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\n\\emph{metric}, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural\nTensor Fields} with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields", "AI": {"tldr": "Einstein Fields \u662f\u4e00\u79cd\u795e\u7ecf\u5f20\u91cf\u573a\uff0c\u80fd\u538b\u7f29\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u5bfc\u51fa\u7269\u7406\u91cf\uff0c\u5e76\u5728\u65f6\u7a7a\u51e0\u4f55\u7f16\u7801\u4e2d\u81ea\u7136\u4ea7\u751f\u52a8\u529b\u5b66\u3002", "motivation": "\u4e3a\u4e86\u538b\u7f29\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\uff0c\u5e76\u4e3a\u6570\u503c\u76f8\u5bf9\u8bba\u63d0\u4f9b\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u6027\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Einstein Fields \u7684\u795e\u7ecf\u8868\u5f81\u3002", "method": "Einstein Fields \u901a\u8fc7\u5c06\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u6838\u5fc3\u5f20\u91cf\u573a\u201c\u5ea6\u89c4\u201d\u5efa\u6a21\u4e3a\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u7684\u538b\u7f29\u3002\u8fd9\u79cd\u795e\u7ecf\u5f20\u91cf\u573a\u8868\u793a\u4f7f\u5f97\u80fd\u591f\u5229\u7528\u81ea\u52a8\u5fae\u5206\u6765\u63a8\u5bfc\u7269\u7406\u91cf\uff0c\u5e76\u4e14\u5728\u7f16\u7801\u65f6\u7a7a\u51e0\u4f55\u7684\u540c\u65f6\uff0c\u52a8\u529b\u5b66\u4f5c\u4e3a\u526f\u4ea7\u54c1\u81ea\u7136\u6d8c\u73b0\u3002", "result": "Einstein Fields \u5728\u591a\u4e2a\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u5178\u578b\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u5c55\u73b0\u4e86\u5176\u5728\u8fde\u7eed\u65f6\u7a7a\u5efa\u6a21\u3001\u7f51\u683c\u65e0\u5173\u6027\u3001\u5b58\u50a8\u6548\u7387\u3001\u5bfc\u6570\u7cbe\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "Einstein Fields \u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u8868\u5f81\uff0c\u80fd\u591f\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u538b\u7f29\u5230\u7d27\u51d1\u7684\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u4e2d\uff0c\u5e76\u80fd\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u5bfc\u51fa\u7269\u7406\u91cf\u3002\u4e0e\u4f20\u7edf\u7684\u795e\u7ecf\u573a\u4e0d\u540c\uff0cEinstein Fields \u662f\u795e\u7ecf\u5f20\u91cf\u573a\uff0c\u5176\u4f18\u52bf\u5728\u4e8e\u80fd\u591f\u5728\u65f6\u7a7a\u51e0\u4f55\u7f16\u7801\u7684\u540c\u65f6\u81ea\u7136\u5730\u4ea7\u751f\u52a8\u529b\u5b66\u3002\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u65f6\u7a7a\u5efa\u6a21\u3001\u7f51\u683c\u65e0\u5173\u6027\u3001\u5b58\u50a8\u6548\u7387\u3001\u5bfc\u6570\u7cbe\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u5728\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u5178\u578b\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u7814\u7a76\u56e2\u961f\u8fd8\u53d1\u5e03\u4e86\u57fa\u4e8e JAX \u7684\u5f00\u6e90\u5e93\uff0c\u4e3a\u6570\u503c\u76f8\u5bf9\u8bba\u7684\u53ef\u6269\u5c55\u548c\u8868\u8fbe\u6027\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.11863", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.11863", "abs": "https://arxiv.org/abs/2507.11863", "authors": ["Guo-Dong Zhao", "Ismaila Dabo", "Long-Qing Chen"], "title": "Thermodynamic stabilization and electronic effects of oxygen vacancies at BiFeO$_3$ neutral ferroelectric domain walls", "comment": "21 pages, 14 figures", "summary": "Enhanced conductivity at ferroelectric domain walls in BiFeO$_3$ has been\nwidely observed, yet the microscopic origins of this effect, including\nelectronic contributions from domain-wall defects, are incompletely understood\nat the atomistic level. Here, we carry out first-principles simulations to\nquantify the thermodynamic stability and electronic impact of oxygen vacancies\nat charge-neutral 71$^\\circ$, 109$^\\circ$, and 180$^\\circ$ domain walls of\nBiFeO$_3$. We find that vacancies are energetically favored at domain walls by\nup to 0.3 eV compared to the bulk, leading to orders-of-magnitude increase in\nvacancy equilibrium concentration. The corresponding formation energy\nlandscapes are discontinued and explained by local bond weakening. The\nvacancies induce localized electronic intragap states corresponding to small\npolarons, which promote thermally activated n-type conduction in the\nlow-current regime, and their tendency to aggregate facilitate Schottky\nemission in the high-current regime. Our results provide a quantitative\nfoundation for interpreting domain-wall conduction, offer guidance for defect\nengineering in ferroelectrics, and provide important information to phase-field\nsimulations of defect-domain wall interactions in a ferroelectric domain\nstructure.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\uff0c\u89e3\u91ca\u4e86BiFeO3\u94c1\u7535\u7574\u58c1\u4e2d\u6c27\u7a7a\u4f4d\u5bfc\u81f4\u7684\u5bfc\u7535\u6027\u589e\u5f3a\u673a\u5236\uff0c\u4e3a\u94c1\u7535\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "BiFeO3\u94c1\u7535\u6750\u6599\u4e2d\u7684\u7574\u58c1\u5bfc\u7535\u6027\u589e\u5f3a\u673a\u5236\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\uff0c\u7279\u522b\u662f\u7574\u58c1\u7f3a\u9677\u7684\u7535\u5b50\u8d21\u732e\uff0c\u56e0\u6b64\u9700\u8981\u539f\u5b50\u5c3a\u5ea6\u7684\u7406\u89e3\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u8ba1\u7b97\u4e86\u6c27\u7a7a\u4f4d\u5728BiFeO3\u94c1\u7535\u7574\u58c1\u4e2d\u7684\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u548c\u7535\u5b50\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u4f4e\u7535\u6d41\u548c\u9ad8\u7535\u6d41\u4e0b\u7684\u5bfc\u7535\u673a\u5236\u3002", "result": "\u6c27\u7a7a\u4f4d\u5728\u7574\u58c1\u4e2d\u7684\u80fd\u91cf\u6bd4\u5728\u4f53\u76f8\u4e2d\u9ad80.3 eV\uff0c\u8fd9\u4f7f\u5f97\u7a7a\u4f4d\u5728\u7574\u58c1\u4e2d\u7684\u6d53\u5ea6\u589e\u52a0\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002\u6c27\u7a7a\u4f4d\u5f15\u8d77\u7684\u7535\u5b50\u5e26\u9699\u6001\u4f1a\u4fc3\u8fdbn\u578b\u5bfc\u7535\uff0c\u5e76\u4e14\u7a7a\u4f4d\u7684\u805a\u96c6\u4f1a\u4fc3\u8fdb\u9ad8\u7535\u6d41\u4e0b\u7684\u8096\u7279\u57fa\u52bf\u5792\u53d1\u5c04\u3002", "conclusion": "\u9996\u5148\uff0c\u6c27\u7a7a\u4f4d\u5728\u94c1\u7535\u7574\u58c1\u4e2d\u7684\u80fd\u91cf\u7a33\u5b9a\u6027\u9ad8\u4e8e\u4f53\u76f8\uff0c\u4e0e\u5c40\u90e8\u952e\u7684\u51cf\u5f31\u6709\u5173\u3002\u5176\u6b21\uff0c\u6c27\u7a7a\u4f4d\u4f1a\u5728\u7574\u58c1\u4e2d\u8bf1\u5bfc\u5c40\u57df\u7684\u6781\u5316\u5b50\u7535\u5b50\u5e26\u9699\u6001\uff0c\u8fd9\u4f1a\u5728\u4f4e\u7535\u6d41\u4e0b\u4fc3\u8fdbn\u578b\u5bfc\u7535\uff0c\u5e76\u4e14\u7a7a\u4f4d\u7684\u805a\u96c6\u4f1a\u4fc3\u8fdb\u9ad8\u7535\u6d41\u4e0b\u7684\u8096\u7279\u57fa\u52bf\u5792\u53d1\u5c04\u3002\u6700\u540e\uff0c\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u7574\u58c1\u5bfc\u7535\u6027\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7840\uff0c\u5e76\u4e3a\u94c1\u7535\u6750\u6599\u4e2d\u7684\u7f3a\u9677\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.11913", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11913", "abs": "https://arxiv.org/abs/2507.11913", "authors": ["Chen Zhu", "Siyun Liang", "Zhouxiang Zhao", "Jianrong Bao", "Zhaohui Yang", "Zhaoyang Zhang", "Dusit Niyato"], "title": "Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission", "comment": null, "summary": "Semantic communication emphasizes the transmission of meaning rather than raw\nsymbols. It offers a promising solution to alleviate network congestion and\nimprove transmission efficiency. In this paper, we propose a wireless image\ncommunication framework that employs probability graphs as shared semantic\nknowledge base among distributed users. High-level image semantics are\nrepresented via scene graphs, and a two-stage compression algorithm is devised\nto remove predictable components based on learned conditional and co-occurrence\nprobabilities. At the transmitter, the algorithm filters redundant relations\nand entity pairs, while at the receiver, semantic recovery leverages the same\nprobability graphs to reconstruct omitted information. For further research, we\nalso put forward a multi-round semantic compression algorithm with its\ntheoretical performance analysis. Simulation results demonstrate that our\nsemantic-aware scheme achieves superior transmission throughput and satiable\nsemantic alignment, validating the efficacy of leveraging high-level semantics\nfor image communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u56fe\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u538b\u7f29\u548c\u591a\u8f6e\u8bed\u4e49\u538b\u7f29\u7b97\u6cd5\u63d0\u9ad8\u4f20\u8f93\u6548\u7387\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u8bed\u4e49\u901a\u4fe1\u65e8\u5728\u4f20\u8f93\u610f\u4e49\u800c\u975e\u539f\u59cb\u7b26\u53f7\uff0c\u4e3a\u7f13\u89e3\u7f51\u7edc\u62e5\u585e\u548c\u63d0\u9ad8\u4f20\u8f93\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7ebf\u56fe\u50cf\u901a\u4fe1\u6846\u67b6\uff0c\u4f7f\u7528\u6982\u7387\u56fe\u4f5c\u4e3a\u5206\u5e03\u5f0f\u7528\u6237\u7684\u5171\u4eab\u8bed\u4e49\u77e5\u8bc6\u5e93\u3002\u901a\u8fc7\u573a\u666f\u56fe\u8868\u793a\u9ad8\u5c42\u56fe\u50cf\u8bed\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u538b\u7f29\u7b97\u6cd5\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u6761\u4ef6\u548c\u5171\u73b0\u6982\u7387\u53bb\u9664\u53ef\u9884\u6d4b\u7684\u6210\u5206\u3002\u5728\u53d1\u5c04\u7aef\uff0c\u8be5\u7b97\u6cd5\u8fc7\u6ee4\u5197\u4f59\u7684\u5173\u7cfb\u548c\u5b9e\u4f53\u5bf9\uff1b\u5728\u63a5\u6536\u7aef\uff0c\u8bed\u4e49\u6062\u590d\u5229\u7528\u76f8\u540c\u7684\u6982\u7387\u56fe\u91cd\u5efa\u88ab\u7701\u7565\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7406\u8bba\u6027\u80fd\u5206\u6790\u7684\u591a\u8f6e\u8bed\u4e49\u538b\u7f29\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u65b9\u6848\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u4f20\u8f93\u541e\u5410\u91cf\u548c\u53ef\u89c2\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528\u9ad8\u5c42\u8bed\u4e49\u8fdb\u884c\u56fe\u50cf\u901a\u4fe1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u65b9\u6848\u901a\u8fc7\u5229\u7528\u9ad8\u5c42\u8bed\u4e49\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u4f20\u8f93\u541e\u5410\u91cf\u548c\u53ef\u89c2\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.12304", "categories": ["cs.DS", "cs.DM", "68W25, 68W40, 68Q25, 90C27", "F.2.2; G.2.1; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.12304", "abs": "https://arxiv.org/abs/2507.12304", "authors": ["Sophia Heimann", "Hung P. Hoang", "Stefan Hougardy"], "title": "A near-complete resolution of the exponential-time complexity of k-opt for the traveling salesman problem", "comment": "41 pages, 19 figures", "summary": "The $k$-opt algorithm is one of the simplest and most widely used heuristics\nfor solving the traveling salesman problem. Starting from an arbitrary tour,\nthe $k$-opt algorithm improves the current tour in each iteration by exchanging\nup to $k$ edges. The algorithm continues until no further improvement of this\nkind is possible. For a long time, it remained an open question how many\niterations the $k$-opt algorithm might require for small values of $k$,\nassuming the use of an optimal pivot rule. In this paper, we resolve this\nquestion for the cases $k = 3$ and $k = 4$ by proving that in both these cases\nan exponential number of iterations may be needed even if an optimal pivot rule\nis used. Combined with a recent result from Heimann, Hoang, and Hougardy (ICALP\n2024), this provides a complete answer for all $k \\geq 3$ regarding the number\nof iterations the $k$-opt algorithm may require under an optimal pivot rule. In\naddition we establish an analogous exponential lower bound for the 2.5-opt\nalgorithm, a variant that generalizes 2-opt and is a restricted version of\n3-opt. All our results hold for both the general and the metric traveling\nsalesman problem.", "AI": {"tldr": "$k$-opt \u7b97\u6cd5\u5728 $k=3,4$ \u65f6\u53ef\u80fd\u9700\u8981\u6307\u6570\u7ea7\u8fed\u4ee3\uff0c2.5-opt \u4e5f\u6709\u7c7b\u4f3c\u4e0b\u754c\u3002", "motivation": "\u89e3\u51b3 $k$-opt \u7b97\u6cd5\u5728 $k=3$ \u548c $k=4$ \u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u4f18\u67a2\u8f74\u9009\u62e9\u89c4\u5219\uff0c\u8fed\u4ee3\u6b21\u6570\u7684\u4e0a\u754c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u5728 $k=3$ \u548c $k=4$ \u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u4f18\u67a2\u8f74\u9009\u62e9\u89c4\u5219\uff0ck-opt \u7b97\u6cd5\u4e5f\u53ef\u80fd\u9700\u8981\u6307\u6570\u7ea7\u8fed\u4ee3\u6b21\u6570\uff0c\u6765\u89e3\u51b3 $k$-opt \u7b97\u6cd5\u8fed\u4ee3\u6b21\u6570\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86 $k$-opt \u7b97\u6cd5\u5728 $k=3$ \u548c $k=4$ \u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u6307\u6570\u7ea7\u8fed\u4ee3\u6b21\u6570\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u4f18\u67a2\u8f74\u9009\u62e9\u89c4\u5219\u3002\u540c\u65f6\uff0c\u4e5f\u4e3a 2.5-opt \u7b97\u6cd5\u5efa\u7acb\u4e86\u7c7b\u4f3c\u7684\u6307\u6570\u7ea7\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86 $k$-opt \u7b97\u6cd5\u5728 $k=3$ \u548c $k=4$ \u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u4f18\u67a2\u8f74\u9009\u62e9\u89c4\u5219\uff0c\u4e5f\u53ef\u80fd\u9700\u8981\u6307\u6570\u7ea7\u8fed\u4ee3\u6b21\u6570\u7684\u95ee\u9898\u3002\u7814\u7a76\u8fd8\u4e3a 2.5-opt \u7b97\u6cd5\u5efa\u7acb\u4e86\u7c7b\u4f3c\u7684\u6307\u6570\u7ea7\u4e0b\u754c\u3002\u6240\u6709\u7ed3\u679c\u5747\u9002\u7528\u4e8e\u4e00\u822c\u548c\u5ea6\u91cf\u65c5\u884c\u5546\u95ee\u9898\u3002"}}
{"id": "2507.12286", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.12286", "abs": "https://arxiv.org/abs/2507.12286", "authors": ["Anouk Oudshoorn", "Magdalena Ortiz", "Mantas Simkus"], "title": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques", "comment": "36 pages, 6 figures, submitted to the journal of Artificial\n  Intelligence (AIJ)", "summary": "SHACL and OWL are two prominent W3C standards for managing RDF data. These\nlanguages share many features, but they have one fundamental difference: OWL,\ndesigned for inferring facts from incomplete data, makes the open-world\nassumption, whereas SHACL is a constraint language that treats the data as\ncomplete and must be validated under the closed-world assumption. The\ncombination of both formalisms is very appealing and has been called for, but\ntheir semantic gap is a major challenge, semantically and computationally. In\nthis paper, we advocate a semantics for SHACL validation in the presence of\nontologies based on core universal models. We provide a technique for\nconstructing these models for ontologies in the rich data-tractable description\nlogic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to\ndevelop a rewriting technique that reduces SHACL validation in the presence of\nontologies to standard validation. Finally, we study the complexity of SHACL\nvalidation in the presence of ontologies, and show that even very simple\nontologies make the problem EXPTIME-complete, and PTIME-complete in data\ncomplexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5b58\u5728\u672c\u4f53\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1 SHACL \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06 SHACL \u9a8c\u8bc1\u8fd8\u539f\u4e3a\u6807\u51c6\u9a8c\u8bc1\u6765\u89e3\u51b3 SHACL \u548c OWL \u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u6709\u7b80\u5355\u7684\u672c\u4f53\uff0c\u9a8c\u8bc1\u7684\u590d\u6742\u5ea6\u4e5f\u5f88\u9ad8\u3002", "motivation": "SHACL \u548c OWL \u662f\u4e24\u4e2a\u91cd\u8981\u7684 W3C \u6807\u51c6\uff0c\u5728\u7ba1\u7406 RDF \u6570\u636e\u65b9\u9762\u6709\u8bb8\u591a\u5171\u540c\u70b9\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u6027\u7684\u5dee\u5f02\uff1aOWL \u57fa\u4e8e\u5f00\u653e\u4e16\u754c\u5047\u8bbe\uff0c\u800c SHACL \u57fa\u4e8e\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\u3002\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b83\u4eec\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u5728\u8bed\u4e49\u548c\u8ba1\u7b97\u4e0a\u90fd\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5fc3\u901a\u7528\u6a21\u578b\u7684 SHACL \u9a8c\u8bc1\u8bed\u4e49\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e3a Horn-ALCHIQ \u4e2d\u7684\u672c\u4f53\u6784\u9020\u8fd9\u4e9b\u6a21\u578b\u7684\u6280\u672f\u3002\u7136\u540e\uff0c\u4f7f\u7528\u8be5\u6a21\u578b\u7684\u6709\u9650\u8868\u793a\u6765\u5f00\u53d1\u4e00\u79cd\u5c06 SHACL \u9a8c\u8bc1\uff08\u5728\u5b58\u5728\u672c\u4f53\u7684\u60c5\u51b5\u4e0b\uff09\u8fd8\u539f\u4e3a\u6807\u51c6\u9a8c\u8bc1\u7684\u91cd\u5199\u6280\u672f\u3002", "result": "\u7814\u7a76\u4e86 SHACL \u9a8c\u8bc1\u5728\u5b58\u5728\u672c\u4f53\u65f6\u7684\u60c5\u51b5\u4e0b\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u8868\u660e\u5373\u4f7f\u662f\u5f88\u7b80\u5355\u7684\u672c\u4f53\u4e5f\u4f1a\u4f7f\u95ee\u9898\u8fbe\u5230 EXPTIME \u5b8c\u5168\uff0c\u800c\u5728\u6570\u636e\u590d\u6742\u5ea6\u4e0b\u8fbe\u5230 PTIME \u5b8c\u5168\u3002", "conclusion": "SHACL \u9a8c\u8bc1\u5728\u5b58\u5728\u672c\u4f53\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u88ab\u8fd8\u539f\u4e3a\u6807\u51c6\u9a8c\u8bc1\uff0c\u5e76\u4e14\u5373\u4f7f\u662f\u5f88\u7b80\u5355\u7684\u672c\u4f53\uff0c\u5176\u590d\u6742\u5ea6\u4e5f\u662f EXPTIME \u5b8c\u5168\u7684\uff0c\u800c\u5728\u6570\u636e\u590d\u6742\u5ea6\u4e0b\u662f PTIME \u5b8c\u5168\u7684\u3002"}}
{"id": "2507.11934", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.11934", "abs": "https://arxiv.org/abs/2507.11934", "authors": ["Xin Hu", "Yuya Ominato", "Mamoru Matsuo"], "title": "Quantum oscillations of valley current driven by microwave irradiation in transition-metal dichalcogenide/ferromagnet hybrids", "comment": "6 pages, 3 figures, one table", "summary": "We theoretically study spin and valley transport in a TMDC/ferromagnet\nheterostructure under a perpendicular magnetic field. We find that\nmicrowave-driven spin pumping induces a valley-selective spin excitation, a\ndirect consequence of the valley-asymmetric Landau levels in the TMDC\nconduction band. This process generates a pure valley current which, as our\ncentral finding, exhibits pronounced quantum oscillations as a function of\nchemical potential. These oscillations provide a definitive experimental\nsignature of the quantized valley states and establish a new pathway to\ninterface spintronics and valleytronics.", "AI": {"tldr": "Spin pumping in TMDC/ferromagnet heterostructures creates a valley current with quantum oscillations, linking spintronics and valleytronics.", "motivation": "To theoretically study spin and valley transport in a TMDC/ferromagnet heterostructure and to explore phenomena arising from the interplay between spintronics and valleytronics.", "method": "Theoretical study of spin and valley transport in a TMDC/ferromagnet heterostructure under a perpendicular magnetic field, focusing on microwave-driven spin pumping and its effect on valley-selective spin excitation.", "result": "Microwave-driven spin pumping induces a valley-selective spin excitation due to valley-asymmetric Landau levels, generating a pure valley current. This valley current exhibits pronounced quantum oscillations as a function of chemical potential, serving as an experimental signature of quantized valley states.", "conclusion": "The study establishes a new pathway to interface spintronics and valleytronics by demonstrating microwave-driven spin pumping in a TMDC/ferromagnet heterostructure, which induces a valley-selective spin excitation and generates a pure valley current exhibiting quantum oscillations."}}
{"id": "2507.11737", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11737", "abs": "https://arxiv.org/abs/2507.11737", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "comment": null, "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them.", "AI": {"tldr": "LLM\u5728\u81ea\u52a8\u6784\u5efa\u52a8\u6001\u89c4\u5212\uff08DP\uff09\u6a21\u578b\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46DP\u95ee\u9898\u7684\u7279\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86DP-Bench\u57fa\u51c6\u548cDPLM\u6a21\u578b\uff0c\u5e76\u901a\u8fc7DualReflect\u6570\u636e\u751f\u6210\u7ba1\u9053\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0eSOTA LLM\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u6570\u636e\u751f\u6210\u7b56\u7565\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u7684DP\u6a21\u578b\u6784\u5efa\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\uff0cLLM\u6709\u6f5c\u529b\u81ea\u52a8\u5316\u6b64\u8fc7\u7a0b\uff0c\u4f46DP\u95ee\u9898\u7684\u968f\u673a\u8f6c\u79fb\u548c\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u7ed9\u76f4\u63a5\u5e94\u7528LLM\u5e26\u6765\u4e86\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728DP\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "method": "DP-Bench\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u6db5\u76d6\u6559\u79d1\u4e66\u7ea7\u522bDP\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u3002DPLM\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u4e13\u7528\u6a21\u578b\uff0c\u5176\u6838\u5fc3\u662fDualReflect\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u524d\u5411\u751f\u6210\uff08\u589e\u52a0\u591a\u6837\u6027\uff09\u548c\u540e\u5411\u751f\u6210\uff08\u4fdd\u8bc1\u53ef\u9760\u6027\uff09\u6765\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u3002", "result": "DP-Bench\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cDPLM\u7684\u6027\u80fd\u53ef\u4e0eOpenAI\u7684o1\u548cDeepSeek-R1\u7b49SOTA LLM\u76f8\u5ab2\u7f8e\uff0c\u5e76\u5728\u96be\u9898\u4e0a\u8d85\u8d8a\u5b83\u4eec\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5411\u540e\u751f\u6210\u5728\u4f4e\u6570\u636e\u91cf\u65f6\u56e0\u5176\u5f3a\u6b63\u786e\u6027\u4fdd\u8bc1\u800c\u66f4\u53d7\u9752\u7750\uff0c\u800c\u5411\u524d\u751f\u6210\u5728\u5927\u89c4\u6a21\u65f6\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316\u8868\u8ff0\u53d8\u5f97\u66f4\u6709\u4ef7\u503c\uff0c\u4e24\u8005\u7ed3\u5408\u4f7f\u7528\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "DP-Bench\u548cDPLM\u7684\u5f15\u5165\u4e3aDP\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u6cd5\u3002DPLM\u901a\u8fc7DualReflect\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u4f18\u5148\u8003\u8651\u5411\u540e\u751f\u6210\u4ee5\u4fdd\u8bc1\u6b63\u786e\u6027\uff0c\u5728\u5927\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u5219\u7ed3\u5408\u5411\u524d\u751f\u6210\u4ee5\u589e\u52a0\u591a\u6837\u6027\uff0c\u4ece\u800c\u5728DP\u95ee\u9898\u4e0a\u8fbe\u5230\u4e0eSOTA LLM\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11924", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11924", "abs": "https://arxiv.org/abs/2507.11924", "authors": ["Hyeongmin Choe", "Soojean Han"], "title": "Advantages of Feedback in Distributed Data-Gathering for Accurate and Power-Efficient State-Estimation", "comment": "Archived version. Related work under further development", "summary": "In distributed target-tracking sensor networks, efficient data gathering\nmethods are necessary to save communication resources and assure information\naccuracy. This paper proposes a Feedback (FB) distributed data-gathering method\nwhich lets the central unit feed information back to the mobile sensors; each\nsensor then uses it to cancel redundant transmissions and reduce communication\ncongestion. We rigorously compare its performance, in terms of mean-squared\nerror (MSE) and cost of power per sensor, against more conventional\nNon-Feedback (NF) architectures by evaluating conditions of feasibility and\nadvantage under different architecture specifications (e.g., communication\ndelay rate, power cost rate, maximum back-off time, sampling period,\nobservation noise). Here, we defined the advantage as the performance gain\nachieved by FB over NF, while FB is said to be feasible if the advantage region\nis nonempty. Our theoretical analyses show that the feasibility of FB depends\nmore on the communication power cost, while the advantage depends on the\nsensors' propagation delay per transmission interval; we derive concrete\nconditions under which these outcomes hold. Using extensive numerical\nsimulations under a variety of settings, we confirm the accuracy of the derived\nconditions, and show that our theoretical results hold even for more complex\nscenarios where the simplifying assumptions no longer hold.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u6570\u636e\u6536\u96c6\u65b9\u6cd5FB\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u6bd4NF\u65b9\u6cd5\u66f4\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u5728\u5206\u5e03\u5f0f\u76ee\u6807\u8ddf\u8e2a\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u8282\u7701\u901a\u4fe1\u8d44\u6e90\u548c\u4fdd\u8bc1\u4fe1\u606f\u51c6\u786e\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cd\u9988\uff08FB\uff09\u7684\u5206\u5e03\u5f0f\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u4f20\u7edf\u7684\u975e\u53cd\u9988\uff08NF\uff09\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u5728\u4e0d\u540c\u67b6\u6784\u89c4\u8303\u4e0b\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u6570\u503c\u6a21\u62df\uff0c\u8bc1\u660e\u4e86FB\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f97\u51fa\u4e86FB\u53ef\u884c\u6027\u548c\u4f18\u52bf\u7684\u6761\u4ef6\u3002", "conclusion": "FB\u65b9\u6cd5\u5728\u901a\u4fe1\u80fd\u529b\u548c\u6570\u636e\u51c6\u786e\u6027\u65b9\u9762\u90fd\u4f18\u4e8eNF\u65b9\u6cd5\uff0c\u5e76\u4e14FB\u53ef\u884c\u6027\u53d6\u51b3\u4e8e\u901a\u4fe1\u6210\u672c\uff0c\u800cFB\u4f18\u52bf\u53d6\u51b3\u4e8e\u901a\u4fe1\u5ef6\u8fdf\u3002"}}
{"id": "2507.12308", "categories": ["cs.CL", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406VHDL\u4ee3\u7801\uff08\u4e00\u79cd\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Descriptions (CoDes)\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba9LLMs\u5148\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\uff0c\u7136\u540e\u518d\u751f\u6210\u6700\u7ec8\u7684VHDL\u4ee3\u7801\u6216\u6458\u8981\uff0c\u6765\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u6548\u679c\u66f4\u597d\uff0c\u4e3a\u6539\u8fdbLLMs\u5728VHDL\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u4ee3\u7801\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9488\u5bf9\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08HDL\uff09\uff0c\u7279\u522b\u662fVHDL\u7684\u7814\u7a76\u548c\u4f18\u5316\u4ecd\u7136\u4e0d\u8db3\u3002\u73b0\u6709\u4ee3\u7801LLMs\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86Chain-of-Descriptions (CoDes)\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\u6765\u589e\u5f3aLLMs\u5904\u7406VHDL\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u8f93\u5165\u7ed3\u5408\u3002\u5728VHDL-Eval\u548cVHDL-Xform\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u73b0\u6709\u4ee3\u7801LLMs\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002CoDes\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u63d0\u793a\u7b56\u7565\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Chain-of-Descriptions (CoDes)\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2507.11830", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11830", "abs": "https://arxiv.org/abs/2507.11830", "authors": ["Samyam Rajbhandari", "Mert Hidayetoglu", "Aurick Qiao", "Ye Wang", "Juncheng Yang", "Jeff Rasley", "Michael Wyatt", "Yuxiong He"], "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI", "comment": null, "summary": "Inference is now the dominant AI workload, yet existing systems force\ntrade-offs between latency, throughput, and cost. Arctic Inference, an\nopen-source vLLM plugin from Snowflake AI Research, introduces Shift\nParallelism, a dynamic parallelism strategy that adapts to real-world traffic\nwhile integrating speculative decoding, SwiftKV compute reduction, and\noptimized embedding inference. It achieves up to 3.4 times faster request\ncompletion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for\nembeddings, outperforming both latency- and throughput-optimized deployments.\nAlready powering Snowflake Cortex AI, Arctic Inference delivers\nstate-of-the-art, cost-effective inference for enterprise AI and is now\navailable to the community.", "AI": {"tldr": "Arctic Inference \u662f\u4e00\u4e2a\u5f00\u6e90 vLLM \u63d2\u4ef6\uff0c\u901a\u8fc7 Shift Parallelism \u7b49\u6280\u672f\u63d0\u9ad8\u4e86 AI \u63a8\u7406\u7684\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u95ee\u9898\u3002", "method": "Arctic Inference \u5f15\u5165\u4e86Shift Parallelism\uff0c\u4e00\u79cd\u52a8\u6001\u5e76\u884c\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u63a8\u6d4b\u89e3\u7801\u3001SwiftKV \u8ba1\u7b97\u51cf\u5c11\u548c\u4f18\u5316\u7684\u5d4c\u5165\u63a8\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8fbe 3.4 \u500d\u7684\u8bf7\u6c42\u5b8c\u6210\u901f\u5ea6\uff0c1.75 \u500d\u7684\u751f\u6210\u901f\u5ea6\uff0c\u4ee5\u53ca\u6bcf GPU 160 \u4e07\u4e2a token/\u79d2\u7684\u5d4c\u5165\u541e\u5410\u91cf\uff0c\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4f18\u5316\u90e8\u7f72\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Arctic Inference \u73b0\u5df2\u5f00\u6e90\uff0c\u4e3a\u4f01\u4e1a AI \u63d0\u4f9b\u6700\u5148\u8fdb\u3001\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u63a8\u7406\uff0c\u5e76\u5df2\u96c6\u6210\u5230 Snowflake Cortex AI \u4e2d\u3002"}}
{"id": "2507.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11840", "abs": "https://arxiv.org/abs/2507.11840", "authors": ["Gaofeng Li", "Ruize Wang", "Peisen Xu", "Qi Ye", "Jiming Chen"], "title": "The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey", "comment": null, "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.", "AI": {"tldr": "This survey covers the progress of robotic manipulation, from mechanical programming to embodied intelligence and from simple grippers to dexterous hands. It focuses on current advances in data collection and skill learning, and identifies key challenges in the field.", "motivation": "Achieving human-like dexterous robotic manipulation is a central goal and a pivotal challenge in robotics, with AI enabling rapid progress.", "method": "This survey summarizes the evolution of robotic manipulation from mechanical programming to embodied intelligence, focusing on the transition from simple grippers to multi-fingered dexterous hands. It highlights recent advances in dexterous manipulation data collection (simulation, human demonstrations, teleoperation) and skill-learning frameworks (imitation, reinforcement learning), and discusses three key challenges.", "result": "The survey outlines key characteristics and main challenges in robotic manipulation, highlights recent advances in data collection and skill-learning frameworks, and summarizes and discusses three key challenges restricting the development of dexterous robotic manipulation.", "conclusion": "The survey summarizes the evolution of robotic manipulation, focusing on current advances in data collection and skill-learning frameworks for dexterous manipulation, while also outlining key challenges."}}
{"id": "2507.12094", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12094", "abs": "https://arxiv.org/abs/2507.12094", "authors": ["Yiding Feng", "Wei Tang"], "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "comment": null, "summary": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "AI": {"tldr": "\u8861\u91cf\u9884\u6d4b\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u7528\u6027\uff0c\u63d0\u51fa\u4fe1\u606f\u5dee\u7684\u6982\u5ff5\u53ca\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u5728\u51b3\u7b56\u8005\u9700\u8981\u5728\u591a\u4e2a\u53ef\u80fd\u6821\u51c6\u9519\u8bef\u7684\u9884\u6d4b\u6a21\u578b\u4e2d\u8fdb\u884c\u9009\u62e9\u65f6\uff0c\u786e\u5b9a\u54ea\u4e2a\u6a21\u578b\u66f4\u6709\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4fe1\u606f\u5dee\u7684\u6982\u5ff5\uff0c\u5e76\u7ed9\u51fa\u4e86\u5176\u5bf9\u5076\u523b\u753b\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u4ee5\u89c6\u4e3aEMD\u677e\u5f1b\u53d8\u4f53\u7684\u4fe1\u606f\u5ea6\u91cf\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4fe1\u606f\u5dee\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u6982\u5ff5\uff08\u5982U-\u6821\u51c6\u548c\u6821\u51c6\u51b3\u7b56\u635f\u5931\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u540c\u65f6\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u5ea6\u91cf\uff0c\u8be5\u5ea6\u91cf\u6ee1\u8db3\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u6837\u672c\u9ad8\u6548\u5730\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4fe1\u606f\u5dee\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u91cf\u5316\u4e24\u4e2a\u9884\u6d4b\u6a21\u578b\u5728\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6548\u7528\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662fEMD\u7684\u677e\u5f1b\u53d8\u4f53\uff0c\u5177\u6709\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u4ec5\u80fd\u8bbf\u95ee\u9884\u6d4b\u503c\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6837\u672c\u9ad8\u6548\u4f30\u8ba1\u3002"}}
{"id": "2507.12156", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12156", "abs": "https://arxiv.org/abs/2507.12156", "authors": ["Chen Li", "Shanshan Dong", "Sheng Qiu", "Jianmin Han", "Zan Gao", "Kemeng Huang", "Taku Komura"], "title": "SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models", "comment": null, "summary": "Reconstructing dynamic fluids from sparse views is a long-standing and\nchallenging problem, due to the severe lack of 3D information from insufficient\nview coverage. While several pioneering approaches have attempted to address\nthis issue using differentiable rendering or novel view synthesis, they are\noften limited by time-consuming optimization and refinement processes under\nill-posed conditions. To tackle above challenges, we propose SmokeSVD, an\nefficient and effective framework to progressively generate and reconstruct\ndynamic smoke from a single video by integrating both the powerful generative\ncapabilities from diffusion models and physically guided consistency\noptimization towards realistic appearance and dynamic evolution. Specifically,\nwe first propose a physically guided side-view synthesizer based on diffusion\nmodels, which explicitly incorporates divergence and gradient guidance of\nvelocity fields to generate visually realistic and spatio-temporally consistent\nside-view images frame by frame, significantly alleviating the ill-posedness of\nsingle-view reconstruction without imposing additional constraints.\nSubsequently, we determine a rough estimation of density field from the pair of\nfront-view input and side-view synthetic image, and further refine 2D blurry\nnovel-view images and 3D coarse-grained density field through an iterative\nprocess that progressively renders and enhances the images from increasing\nnovel viewing angles, generating high-quality multi-view image sequences.\nFinally, we reconstruct and estimate the fine-grained density field, velocity\nfield, and smoke source via differentiable advection by leveraging the\nNavier-Stokes equations. Extensive quantitative and qualitative experiments\nshow that our approach achieves high-quality reconstruction and outperforms\nprevious state-of-the-art techniques.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno S\u00e1ez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "H\u00e9ctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\u3001\u9010\u6b65\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7b54\u6848\uff0c\u5e76\u5728TableVQA-Bench\u4e0a\u5c55\u793a\u4e86\u5176\u5728\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u8be5\u7cfb\u7edf\u65e8\u5728\u5f25\u5408\u7aef\u5230\u7aef\u8868\u683c\u89c6\u89c9\u95ee\u7b54\uff08TableVQA\uff09\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u5dee\u8ddd\uff0c\u63d0\u4f9b\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u5141\u8bb8\u68c0\u67e5\u6240\u6709\u4e2d\u95f4\u8f93\u51fa\uff0c\u5305\u62ec\u89e3\u6790\u7684\u8868\u683c\u3001\u63a8\u7406\u6b65\u9aa4\u3001\u751f\u6210\u7684\u4ee3\u7801\u548c\u6700\u7ec8\u7b54\u6848\u3002", "method": "ExpliCIT-QA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u95ee\u7b54\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\uff08\u4f7f\u7528\u94fe\u5f0f\u601d\u8003\u63d0\u53d6\u8868\u683c\u56fe\u50cf\u5185\u5bb9\uff09\u3001\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\uff08\u751f\u6210\u9010\u6b65\u89e3\u91ca\uff09\u3001\u81ea\u52a8\u4ee3\u7801\u751f\u6210\uff08\u521b\u5efaPython/Pandas\u811a\u672c\u5e76\u8fdb\u884c\u9519\u8bef\u5904\u7406\uff09\u3001\u4ee3\u7801\u6267\u884c\uff08\u8ba1\u7b97\u6700\u7ec8\u7b54\u6848\uff09\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08\u63cf\u8ff0\u7b54\u6848\u8ba1\u7b97\u8fc7\u7a0b\uff09\u3002", "result": "ExpliCIT-QA\u5728TableVQA-Bench\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u6539\u8fdb\u3002", "conclusion": "ExpliCIT-QA\u5728TableVQA-Bench\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u4e3a\u91d1\u878d\u548c\u533b\u7597\u4fdd\u5065\u7b49\u9700\u8981\u5ba1\u8ba1\u7ed3\u679c\u7684\u654f\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.12207", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.12207", "abs": "https://arxiv.org/abs/2507.12207", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "comment": "ICML 2025 CO-Build Workshop Poster", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "AI": {"tldr": "BuildEvo\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u8fc7\u7a0b\u6765\u81ea\u52a8\u5316\u8bbe\u8ba1\u5efa\u7b51\u80fd\u6e90\u9884\u6d4b\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u7269\u7406\u539f\u7406\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u5148\u8fdb\u7684\u6a21\u578b\u53ef\u80fd\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u63a8\u5e7f\uff0c\u5ffd\u7565\u4e86\u7269\u7406\u539f\u7406\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bbe\u8ba1\u51fa\u65e2\u6709\u6548\u53c8\u53ef\u89e3\u91ca\uff0c\u5e76\u8003\u8651\u7269\u7406\u539f\u7406\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "BuildEvo\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e00\u4e2a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u5efa\u7b51\u80fd\u6e90\u9884\u6d4b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8be5\u8fc7\u7a0b\u7cfb\u7edf\u5730\u878d\u5165\u4e86\u5efa\u7b51\u7279\u5f81\u548c\u8fd0\u884c\u6570\u636e\u7684\u7269\u7406\u89c1\u89e3\u3002", "result": "BuildEvo\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u9884\u6d4b\u903b\u8f91\u3002", "conclusion": "BuildEvo\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8fdb\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u8bbe\u8ba1\u6709\u6548\u7684\u3001\u53ef\u89e3\u91ca\u7684\u3001\u5e76\u878d\u5165\u4e86\u7269\u7406\u6d1e\u5bdf\u7684\u5efa\u7b51\u80fd\u6e90\u9884\u6d4b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9884\u6d4b\u903b\u8f91\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2507.11562", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11562", "abs": "https://arxiv.org/abs/2507.11562", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "comment": "6 pages", "summary": "The wide range of deformation artifacts that arise from complex light\npropagation, scattering, and depth-dependent attenuation makes the underwater\nimage restoration to remain a challenging problem. Like other single deep\nregressor networks, conventional GAN-based restoration methods struggle to\nperform well across this heterogeneous domain, since a single generator network\nis typically insufficient to capture the full range of visual degradations. In\norder to overcome this limitation, we propose xOp-GAN, a novel GAN model with\nseveral expert generator networks, each trained solely on a particular subset\nwith a certain image quality. Thus, each generator can learn to maximize its\nrestoration performance for a particular quality range. Once a xOp-GAN is\ntrained, each generator can restore the input image and the best restored image\ncan then be selected by the discriminator based on its perceptual confidence\nscore. As a result, xOP-GAN is the first GAN model with multiple generators\nwhere the discriminator is being used during the inference of the regression\ntask. Experimental results on benchmark Large Scale Underwater Image (LSUI)\ndataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,\nsurpassing all single-regressor models by a large margin even, with reduced\ncomplexity.", "AI": {"tldr": "xOp-GAN \u662f\u4e00\u79cd\u5305\u542b\u591a\u4e2a\u4e13\u5bb6\u751f\u6210\u5668\u7684\u65b0\u578b GAN \u6a21\u578b\uff0c\u53ef\u89e3\u51b3\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u5f02\u6784\u57df\u95ee\u9898\uff0c\u5e76\u5728 LSUI \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edf GAN \u6a21\u578b\u96be\u4ee5\u5904\u7406\u5f02\u6784\u57df\u4e2d\u5e7f\u6cdb\u7684\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u56e0\u4e3a\u5355\u4e00\u751f\u6210\u5668\u7f51\u7edc\u4e0d\u8db3\u4ee5\u6355\u6349\u5168\u90e8\u89c6\u89c9\u9000\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a xOp-GAN \u7684\u65b0\u578b GAN \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u591a\u4e2a\u4e13\u5bb6\u751f\u6210\u5668\u7f51\u7edc\uff0c\u6bcf\u4e2a\u7f51\u7edc\u4e13\u95e8\u9488\u5bf9\u7279\u5b9a\u56fe\u50cf\u8d28\u91cf\u5b50\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u5728\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6bcf\u4e2a\u751f\u6210\u5668\u90fd\u53ef\u4ee5\u6062\u590d\u8f93\u5165\u56fe\u50cf\uff0c\u7136\u540e\u7531\u5224\u522b\u5668\u6839\u636e\u5176\u611f\u77e5\u7f6e\u4fe1\u5ea6\u5206\u6570\u9009\u62e9\u6700\u4f73\u6062\u590d\u56fe\u50cf\u3002", "result": "xOp-GAN \u80fd\u591f\u5b9e\u73b0\u9ad8\u8fbe 25.16 dB \u7684 PSNR\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5355\u4e00\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "xOp-GAN \u5728 LSUI \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 25.16 dB \u7684 PSNR\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5355\u4e00\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002"}}
{"id": "2507.11675", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11675", "abs": "https://arxiv.org/abs/2507.11675", "authors": ["Xiaogang Li", "Qiming Ding", "Kecheng Liu", "Xiao Yuan"], "title": "Dynamics Simulation of Arbitrary Non-Hermitian Systems Based on Quantum Monte Carlo", "comment": null, "summary": "Non-Hermitian quantum systems exhibit unique properties and hold significant\npromise for diverse applications, yet their dynamical simulation poses a\nparticular challenge due to intrinsic openness and non-unitary evolution. Here,\nwe introduce a hybrid classical-quantum algorithm based on Quantum Monte Carlo\n(QMC) for simulating the dynamics of arbitrary time-dependent non-Hermitian\nsystems. Notably, this approach constitutes a natural extension of the quantum\nimaginary-time evolution (QITE) algorithm. This algorithm combines the\nadvantages of both classical and quantum computation and exhibits good\napplicability and adaptability, making it promising for simulating arbitrary\nnon-Hermitian systems such as PT-symmetric systems, non-physical processes, and\nopen quantum systems. To validate the algorithm, we applied it to the dynamic\nsimulation of open quantum systems and achieved the desired results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u7b97\u6cd5\uff08\u57fa\u4e8eQMC\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u975e\u5384\u7c73\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u6311\u6218\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002", "motivation": "\u975e\u5384\u7c73\u91cf\u5b50\u7cfb\u7edf\u867d\u7136\u5177\u6709\u72ec\u7279\u6027\u4e14\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u5f00\u653e\u6027\u548c\u975e\u5e7a\u6b63\u6f14\u5316\u7ed9\u52a8\u529b\u5b66\u6a21\u62df\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8499\u7279\u5361\u6d1b\uff08QMC\uff09\u7684\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u4efb\u610f\u65f6\u95f4\u4f9d\u8d56\u7684\u975e\u5384\u7c73\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u8be5\u7b97\u6cd5\u662f\u91cf\u5b50\u865a\u65f6\u6f14\u5316\uff08QITE\uff09\u7b97\u6cd5\u7684\u81ea\u7136\u6269\u5c55\u3002", "result": "\u8be5\u6df7\u5408\u7b97\u6cd5\u7ed3\u5408\u4e86\u7ecf\u5178\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u70b9\uff0c\u5177\u6709\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u52a8\u6001\u6a21\u62df\uff0c\u8fbe\u5230\u4e86\u9884\u671f\u6548\u679c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u6a21\u62df\u4efb\u610f\u65f6\u95f4\u4f9d\u8d56\u7684\u975e\u5384\u7c73\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u65b9\u6cd5\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002"}}
{"id": "2507.11590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11590", "abs": "https://arxiv.org/abs/2507.11590", "authors": ["Raju Challagundla", "Mohsen Dorodchi", "Pu Wang", "Minwoo Lee"], "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques", "comment": null, "summary": "As privacy regulations become more stringent and access to real-world data\nbecomes increasingly constrained, synthetic data generation has emerged as a\nvital solution, especially for tabular datasets, which are central to domains\nlike finance, healthcare and the social sciences. This survey presents a\ncomprehensive and focused review of recent advances in synthetic tabular data\ngeneration, emphasizing methods that preserve complex feature relationships,\nmaintain statistical fidelity, and satisfy privacy requirements. A key\ncontribution of this work is the introduction of a novel taxonomy based on\npractical generation objectives, including intended downstream applications,\nprivacy guarantees, and data utility, directly informing methodological design\nand evaluation strategies. Therefore, this review prioritizes the actionable\ngoals that drive synthetic data creation, including conditional generation and\nrisk-sensitive modeling. Additionally, the survey proposes a benchmark\nframework to align technical innovation with real-world demands. By bridging\ntheoretical foundations with practical deployment, this work serves as both a\nroadmap for future research and a guide for implementing synthetic tabular data\nin privacy-critical environments.", "AI": {"tldr": "\u6b64\u9879\u5de5\u4f5c\u5bf9\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4fdd\u6301\u6570\u636e\u590d\u6742\u6027\u3001\u7edf\u8ba1\u51c6\u786e\u6027\u548c\u9690\u79c1\u6027\u7684\u65b9\u6cd5\u3002\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u6cd5\u89c4\u7684\u6536\u7d27\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u8bbf\u95ee\u7684\u9650\u5236\uff0c\u5408\u6210\u6570\u636e\uff08\u5c24\u5176\u662f\u8868\u683c\u6570\u636e\uff09\u5df2\u6210\u4e3a\u91d1\u878d\u3001\u533b\u7597\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u7684\u5173\u952e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5bf9\u4fdd\u7559\u590d\u6742\u7279\u5f81\u5173\u7cfb\u3001\u7edf\u8ba1\u4fdd\u771f\u5ea6\u548c\u9690\u79c1\u8981\u6c42\u7684\u65b0\u5174\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\u548c\u5206\u7c7b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u9645\u751f\u6210\u76ee\u6807\uff08\u5305\u62ec\u9884\u671f\u4e0b\u6e38\u5e94\u7528\u3001\u9690\u79c1\u4fdd\u8bc1\u548c\u6570\u636e\u6548\u7528\uff09\u7684\u65b0\u578b\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u65b9\u6cd5\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u6280\u672f\u521b\u65b0\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u4e3a\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u7279\u522b\u5173\u6ce8\u9690\u79c1\u548c\u6570\u636e\u6548\u7528\u3002"}}
{"id": "2507.11864", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.11864", "abs": "https://arxiv.org/abs/2507.11864", "authors": ["P K Shihabudeen", "Alex Sam", "Shih-Wen Chiu", "Ta-Jen Yen", "Kea-Tiong Tang"], "title": "Ultrasensitive Room-Temperature NO2 Gas Sensor Based on In2O3-NbS2 Heterojunction", "comment": null, "summary": "Niobium disulfide (NbS2), a two-dimensional transition metal dichalcogenide\nwith semi metallic conductivity and high surface activity, offers promising\nproperties for electronic and sensing applications. In this study, we report a\nhigh-performance NO2 gas sensor based on a heterostructure comprising a\nspin-coated In2O3 film on a semi-metallic NbS2 film.", "AI": {"tldr": "A new NO2 gas sensor was created using NbS2 and In2O3 films, showing high performance.", "motivation": "Investigating the potential of NbS2, a 2D transition metal dichalcogenide with semi-metallic conductivity and high surface activity, for electronic and sensing applications.", "method": "Fabrication of a heterostructure comprising a spin-coated In2O3 film on a semi-metallic NbS2 film.", "result": "A high-performance NO2 gas sensor was developed.", "conclusion": "The study reports a high-performance NO2 gas sensor based on a heterostructure comprising a spin-coated In2O3 film on a semi-metallic NbS2 film."}}
{"id": "2507.11919", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11919", "abs": "https://arxiv.org/abs/2507.11919", "authors": ["Wei Zhou", "Wei-Jian Li", "Wei-Xin Ren"], "title": "STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis", "comment": null, "summary": "The decomposition of complex, multicomponent, and non-stationary signals into\ntheir constituent modes is a fundamental yet significant challenge in science\nand engineering. Existing methods often struggle with a trade-off among\naccuracy, computational cost, and the need for prior information such as the\nnumber of modes. This paper introduces time-frequency mode decomposition\n(TFMD), a novel framework for the fast, robust, and adaptive decomposition of\nsuch signals. TFMD operates on the principle that modes form contiguous\nhigh-energy regions in the time-frequency domain. Its non-iterative pipeline\nreframes signal decomposition as an image segmentation task: a signal is\ntransformed into a spectrogram, which is then smoothed to enhance the\ncontinuity of these high-energy regions. A sequence of adaptive thresholding\nand connected-component labeling with size-based filtering is then employed to\nautomatically segment the spectrogram and generate a mask for each mode. The\nmodes are finally reconstructed via the inverse short-time Fourier transform.\nValidation on diverse synthetic signals demonstrates that TFMD accurately\ndetermines the number of modes and reconstructs them with high fidelity. Its\nperformance is particularly strong in high-noise conditions. A comparative\nanalysis confirms that TFMD provides robust, competitive performance across a\nwider variety of signal types, while a theoretical complexity analysis reveals\nits superior computational efficiency stemming from its non-iterative design.\nThe method's practical utility is further demonstrated by successfully\nextracting modal responses from a real-world footbridge vibration signal. TFMD\nprovides a computationally efficient and powerful paradigm for multicomponent\nsignal analysis, offering a compelling balance of accuracy, versatility, and\nefficiency for large-scale or time-sensitive applications.", "AI": {"tldr": "TFMD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4fe1\u53f7\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fe1\u53f7\u5206\u89e3\u89c6\u4e3a\u65f6\u9891\u57df\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u5206\u89e3\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u4e0d\u540c\u4fe1\u53f7\u7c7b\u578b\u7684\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4fe1\u53f7\u5206\u89e3\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5148\u9a8c\u4fe1\u606f\uff08\u5982\u6a21\u5f0f\u6570\u91cf\uff09\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u800cTFMD\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u5206\u89e3\u65b9\u6cd5\u3002", "method": "TFMD\u5c06\u4fe1\u53f7\u5206\u89e3\u91cd\u6784\u4e3a\u65f6\u9891\u57df\u4e2d\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002\u9996\u5148\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u9891\u8c31\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u5e73\u6ed1\u5904\u7406\u6765\u589e\u5f3a\u9ad8\u80fd\u533a\u57df\u7684\u8fde\u7eed\u6027\u3002\u63a5\u7740\u91c7\u7528\u4e00\u7cfb\u5217\u81ea\u9002\u5e94\u9608\u503c\u5904\u7406\u548c\u57fa\u4e8e\u8fde\u901a\u7ec4\u4ef6\u6807\u8bb0\u7684\u5c3a\u5bf8\u8fc7\u6ee4\u6765\u81ea\u52a8\u5206\u5272\u9891\u8c31\u56fe\u5e76\u4e3a\u6bcf\u4e2a\u6a21\u5f0f\u751f\u6210\u63a9\u7801\u3002\u6700\u540e\u901a\u8fc7\u77ed\u65f6\u5085\u91cc\u53f6\u9006\u53d8\u6362\u6765\u91cd\u6784\u6a21\u5f0f\u3002", "result": "TFMD\u80fd\u591f\u51c6\u786e\u786e\u5b9a\u6a21\u5f0f\u6570\u91cf\u5e76\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u91cd\u6784\u6a21\u5f0f\uff0c\u5728\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cTFMD\u5728\u66f4\u5e7f\u6cdb\u7684\u4fe1\u53f7\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u4e14\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u975e\u8fed\u4ee3\u8bbe\u8ba1\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u6865\u6881\u632f\u52a8\u4fe1\u53f7\u5206\u6790\u3002", "conclusion": "TFMD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u591a\u7ec4\u5206\u4fe1\u53f7\u5206\u6790\u8303\u5f0f\uff0c\u5728\u51c6\u786e\u6027\u3001\u591a\u529f\u80fd\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6216\u65f6\u95f4\u654f\u611f\u7684\u5e94\u7528\u3002"}}
{"id": "2507.12357", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12357", "abs": "https://arxiv.org/abs/2507.12357", "authors": ["Ariel Ben Eliezer", "Noam Nisan"], "title": "Online Block Packing", "comment": null, "summary": "We consider the algorithmic challenge that is faced by blockchains that have\nmultidimensional block constraints and serve quasi-patient bidders. We provide\nonline approximation algorithms for this problem, thus solving open problems\nleft by [Babaioff and Nisan, EC 2025].", "AI": {"tldr": "\u4e3a\u5177\u6709\u591a\u7ef4\u5757\u7ea6\u675f\u548c\u8fd1\u4f3c\u8010\u5fc3\u7ade\u6807\u8005\u7684\u533a\u5757\u94fe\u63d0\u4f9b\u5728\u7ebf\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3 Babaioff \u548c Nisan\uff082025 \u5e74 EC\uff09\u63d0\u51fa\u7684\u516c\u5f00\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3 Babaioff \u548c Nisan\uff082025 \u5e74 EC\uff09\u7559\u4e0b\u7684\u516c\u5f00\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u6d89\u53ca\u5177\u6709\u591a\u7ef4\u5757\u7ea6\u675f\u548c\u8fd1\u4f3c\u8010\u5fc3\u7ade\u6807\u8005\u7684\u533a\u5757\u94fe\u7684\u7b97\u6cd5\u6311\u6218\u3002", "method": "\u5728\u7ebf\u8fd1\u4f3c\u7b97\u6cd5", "result": "\u63d0\u4f9b\u5728\u7ebf\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5177\u6709\u591a\u7ef4\u5757\u7ea6\u675f\u548c\u8fd1\u4f3c\u8010\u5fc3\u7ade\u6807\u8005\u7684\u533a\u5757\u94fe\u7684\u7b97\u6cd5\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u6709\u591a\u7ef4\u5757\u7ea6\u675f\u4e14\u670d\u52a1\u4e8e\u8fd1\u4f3c\u8010\u5fc3\u7ade\u6807\u8005\u7684\u533a\u5757\u94fe\u7b97\u6cd5\u6311\u6218\u63d0\u4f9b\u4e86\u5728\u7ebf\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2507.12034", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12034", "abs": "https://arxiv.org/abs/2507.12034", "authors": ["Alexander V. Savin"], "title": "Shock absorption by multilayer carbon nanotube packings", "comment": "10 pages, 9 figures", "summary": "The propagation of transverse impact energy in a multilayer packing (in an\narray) of parallel single-walled carbon nanotubes has been simulated. It has\nbeen shown that such nanotube arrays are effective shock absorbers. The\ndepreciation effect is most pronounced for packings of nanotubes with a\ndiameter of 2.7-3.9 nm. Here, part of the impact energy is absorbed due to the\ntransfer of the packing to a higher energy stationary state, in which part of\nthe nanotubes is in a collapsed state. The impact impulse reaches the other\nedge of the packing most weakened and distributed over time. For nanotubes with\na smaller diameter, the compression of the array occurs elastically without\nenergy accumulation, and for nanotubes with a larger diameter - with energy\nrelease.", "AI": {"tldr": "\u78b3\u7eb3\u7c73\u7ba1\u9635\u5217\u662f\u6709\u6548\u7684\u51cf\u9707\u5668\u3002", "motivation": "\u7814\u7a76\u78b3\u7eb3\u7c73\u7ba1\u9635\u5217\u4f5c\u4e3a\u51cf\u9707\u5668\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u4e86\u6a2a\u5411\u51b2\u51fb\u80fd\u91cf\u5728\u5e73\u884c\u5355\u58c1\u78b3\u7eb3\u7c73\u7ba1\u7684\u591a\u5c42\u5806\u79ef\uff08\u9635\u5217\uff09\u4e2d\u7684\u4f20\u64ad\u3002", "result": "\u76f4\u5f84\u4e3a 2.7-3.9 nm \u7684\u78b3\u7eb3\u7c73\u7ba1\u9635\u5217\u6700\u80fd\u5438\u6536\u51b2\u51fb\u80fd\u91cf\uff0c\u90e8\u5206\u78b3\u7eb3\u7c73\u7ba1\u5728\u5438\u6536\u80fd\u91cf\u540e\u5904\u4e8e\u5d29\u6e83\u72b6\u6001\u3002\u8f83\u5c0f\u76f4\u5f84\u78b3\u7eb3\u7c73\u7ba1\u7684\u538b\u7f29\u662f\u5f39\u6027\u7684\uff0c\u800c\u8f83\u5927\u76f4\u5f84\u78b3\u7eb3\u7c73\u7ba1\u7684\u538b\u7f29\u4f1a\u91ca\u653e\u80fd\u91cf\u3002", "conclusion": "\u78b3\u7eb3\u7c73\u7ba1\u9635\u5217\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u51cf\u9707\u5668\uff0c\u7279\u522b\u662f\u76f4\u5f84\u4e3a 2.7-3.9 nm \u7684\u78b3\u7eb3\u7c73\u7ba1\u9635\u5217\u6548\u679c\u6700\u663e\u8457\u3002\u51b2\u51fb\u80fd\u91cf\u7684\u4e00\u90e8\u5206\u88ab\u5438\u6536\u5230\u6253\u5305\u7684\u80fd\u91cf\u8f6c\u79fb\u5230\u4e00\u4e2a\u66f4\u9ad8\u7684\u56fa\u5b9a\u72b6\u6001\uff0c\u5176\u4e2d\u4e00\u90e8\u5206\u78b3\u7eb3\u7c73\u7ba1\u5904\u4e8e\u5d29\u6e83\u72b6\u6001\u3002"}}
{"id": "2507.11787", "categories": ["cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11787", "abs": "https://arxiv.org/abs/2507.11787", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86 Swarm Intelligence \u7b97\u6cd5\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u6587\u6863\u641c\u7d22\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "Swarm Intelligence (SI) \u4f5c\u4e3a\u4e00\u79cd\u6a21\u62df\u81ea\u7136\u754c\u751f\u7269\u7fa4\u4f53\u884c\u4e3a\u7684\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff0c\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u673a\u4f18\u5316\u95ee\u9898\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u8ba8 SI \u7b97\u6cd5\u5728\u6587\u6863\u641c\u7d22\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8c03\u7814\u548c\u5206\u6790\u6700\u65b0\u7684 Swarm Intelligence \u7b97\u6cd5\u5728\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u6587\u6863\u641c\u7d22\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002", "result": "\u672c\u6b21\u8c03\u67e5\u56de\u987e\u4e86 Swarm Intelligence \u7b97\u6cd5\u5728\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u6587\u6863\u641c\u7d22\u4e2d\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "Swarm Intelligence (SI) \u7b97\u6cd5\u5728\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u6587\u6863\u641c\u7d22\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u91cd\u4e8e\u7b97\u6cd5\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5728\u5177\u4f53\u5e94\u7528\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.12031", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12031", "abs": "https://arxiv.org/abs/2507.12031", "authors": ["Fateme Salehi", "Aamir Mahmood", "Sarder Fakhrul Abedin", "Kyi Thar", "Mikael Gidlund"], "title": "Towards Ultra-Reliable 6G in-X Subnetworks: Dynamic Link Adaptation by Deep Reinforcement Learning", "comment": null, "summary": "6G networks are composed of subnetworks expected to meet ultra-reliable\nlow-latency communication (URLLC) requirements for mission-critical\napplications such as industrial control and automation. An often-ignored aspect\nin URLLC is consecutive packet outages, which can destabilize control loops and\ncompromise safety in in-factory environments. Hence, the current work proposes\na link adaptation framework to support extreme reliability requirements using\nthe soft actor-critic (SAC)-based deep reinforcement learning (DRL) algorithm\nthat jointly optimizes energy efficiency (EE) and reliability under dynamic\nchannel and interference conditions. Unlike prior work focusing on average\nreliability, our method explicitly targets reducing burst/consecutive outages\nthrough adaptive control of transmit power and blocklength based solely on the\nobserved signal-to-interference-plus-noise ratio (SINR). The joint optimization\nproblem is formulated under finite blocklength and quality of service\nconstraints, balancing reliability and EE. Simulation results show that the\nproposed method significantly outperforms the baseline algorithms, reducing\noutage bursts while consuming only 18\\% of the transmission cost required by a\nfull/maximum resource allocation policy in the evaluated scenario. The\nframework also supports flexible trade-off tuning between EE and reliability by\nadjusting reward weights, making it adaptable to diverse industrial\nrequirements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDRL\u7684\u94fe\u8def\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u529f\u7387\u548c\u5757\u957f\u6765\u51cf\u5c116G\u7f51\u7edc\u4e2d\u7684\u8fde\u7eed\u4e22\u5305\uff0c\u4ece\u800c\u63d0\u9ad8URLLC\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u89e3\u51b36G\u7f51\u7edc\u4e2dURLLC\uff08\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff09\u9762\u4e34\u7684\u8fde\u7eed\u4e22\u5305\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u53ef\u80fd\u7834\u574f\u5de5\u4e1a\u63a7\u5236\u548c\u81ea\u52a8\u5316\u7b49\u5173\u952e\u4efb\u52a1\u7684\u5b89\u5168\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8f6fActor-Critic\uff08SAC\uff09\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\uff0c\u9488\u5bf9\u52a8\u6001\u4fe1\u9053\u548c\u5e72\u6270\u6761\u4ef6\uff0c\u8054\u5408\u4f18\u5316\u80fd\u6e90\u6548\u7387\uff08EE\uff09\u548c\u53ef\u9760\u6027\uff0c\u4ee5\u51cf\u5c11\u8fde\u7eed\u4e22\u5305\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\uff0c\u51cf\u5c11\u4e86\u4e22\u5305\u7a81\u53d1\uff0c\u5e76\u4e14\u5728\u8bc4\u4f30\u573a\u666f\u4e2d\u4ec5\u6d88\u8017\u4e86\u5168\u8d44\u6e90\u5206\u914d\u7b56\u756518%\u7684\u4f20\u8f93\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSAC\u7684DRL\u65b9\u6cd5\u901a\u8fc7\u8c03\u6574\u4f20\u8f93\u529f\u7387\u548c\u5757\u957f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fde\u7eed\u4e22\u5305\u4e8b\u4ef6\uff0c\u540c\u65f6\u4ec5\u6d88\u8017\u4e86\u6700\u5927\u8d44\u6e90\u5206\u914d\u7b56\u756518%\u7684\u4f20\u8f93\u6210\u672c\uff0c\u5e76\u5728\u53ef\u9760\u6027\u548c\u80fd\u6e90\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u6743\u8861\u3002"}}
{"id": "2507.11899", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11899", "abs": "https://arxiv.org/abs/2507.11899", "authors": ["Saeid Aghasoleymani Najafabadi"], "title": "Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst", "comment": null, "summary": "Load balancing plays a pivotal role in cloud computing, ensuring that\nresources are optimally allocated to maintain high service quality and\noperational efficiency. As workloads in cloud environments become increasingly\ndynamic and unpredictable, load balancing strategies are evolving from\ntraditional static methods to more adaptive and intelligent approaches. In this\nstudy, the Cloud Analyst simulation tool was used to evaluate the performance\nof different load balancing algorithms under various scenarios, including both\ncentralized and distributed resource setups. The results highlight that while\nthe Round Robin algorithm yields slightly better processing times within a\nsingle data center, Equally Spread and Throttled techniques perform\ncompetitively, especially when network latency is considered. More importantly,\nwhen resources are distributed across multiple data centers, response times are\nsignificantly reduced, emphasizing the value of proximity and efficient load\ndistribution. In these distributed environments, Equally Spread and Throttled\nalgorithms not only maintain quick response times but also contribute to lower\noperational costs. These findings demonstrate the necessity of strategic\nresource placement and proactive infrastructure planning to balance performance\nand cost. Adopting intelligent, dynamic load balancing and resource management\npractices can help organizations meet evolving cloud demands, optimize costs,\nand maintain a competitive advantage. Continuous evaluation and integration of\nemerging technologies are crucial for sustaining effective and scalable cloud\noperations.", "AI": {"tldr": "\u5728\u4e91\u73af\u5883\u4e2d\uff0c\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5faa\u73af\u7b97\u6cd5\u5728\u5355\u6570\u636e\u4e2d\u5fc3\u5185\u7565\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\uff0c\u5747\u8861\u548c\u9650\u6d41\u7b97\u6cd5\u80fd\u66f4\u597d\u5730\u51cf\u5c11\u54cd\u5e94\u65f6\u95f4\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u968f\u7740\u4e91\u73af\u5883\u4e2d\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5f97\u8d8a\u6765\u8d8a\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\uff0c\u9700\u8981\u4ece\u4f20\u7edf\u7684\u9759\u6001\u65b9\u6cd5\u6f14\u53d8\u5230\u66f4\u5177\u9002\u5e94\u6027\u548c\u667a\u80fd\u5316\u7684\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3001\u4fdd\u6301\u9ad8\u8d28\u91cf\u670d\u52a1\u548c\u8fd0\u8425\u6548\u7387\u3002", "method": "\u4f7f\u7528Cloud Analyst\u6a21\u62df\u5de5\u5177\u8bc4\u4f30\u4e86\u5728\u4e2d\u5fc3\u5316\u548c\u5206\u5e03\u5f0f\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u4e0d\u540c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u5355\u6570\u636e\u4e2d\u5fc3\u5185\uff0c\u5faa\u73af\u7b97\u6cd5\u7684\u5904\u7406\u65f6\u95f4\u7565\u597d\uff1b\u4f46\u8003\u8651\u5230\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5747\u8861\u548c\u9650\u6d41\u6280\u672f\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\u3002\u5728\u5206\u5e03\u5f0f\u73af\u5883\uff08\u8de8\u591a\u4e2a\u6570\u636e\u4e2d\u5fc3\uff09\u4e2d\uff0c\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff0c\u5747\u8861\u548c\u9650\u6d41\u7b97\u6cd5\u5728\u4fdd\u6301\u5feb\u901f\u54cd\u5e94\u65f6\u95f4\u548c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u4e2d\u91c7\u7528\u667a\u80fd\u3001\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u548c\u8d44\u6e90\u7ba1\u7406\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u65ad\u53d8\u5316\u7684\u4e91\u9700\u6c42\u3001\u4f18\u5316\u6210\u672c\u5e76\u4fdd\u6301\u7ade\u4e89\u4f18\u52bf\u3002\u6301\u7eed\u8bc4\u4f30\u548c\u6574\u5408\u65b0\u5174\u6280\u672f\u5bf9\u4e8e\u7ef4\u6301\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u4e91\u8fd0\u8425\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.11852", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11852", "abs": "https://arxiv.org/abs/2507.11852", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "comment": "17 pages", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "AI": {"tldr": "\u5bf9\u81ea\u52a8\u9a91\u884c (AR) \u6280\u672f\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5176\u5728\u57ce\u5e02\u4ea4\u901a\u4e2d\u7684\u5e94\u7528\u3002\u5ba1\u67e5\u5f3a\u8c03\u4e86 AR \u76f8\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76 (AD) \u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u786e\u5b9a\u4e86\u7814\u7a76\u4e2d\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u4f20\u611f\u548c\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u7535\u52a8\u8e0f\u677f\u8f66\u548c\u7535\u52a8\u81ea\u884c\u8f66\u7b49\u4e24\u8f6e\u8f66\u8f86\u7684\u5feb\u901f\u666e\u53ca\uff0c\u5bf9\u53ef\u9760\u7684\u81ea\u52a8\u9a91\u884c (AR) \u6280\u672f\u4ea7\u751f\u4e86\u8feb\u5207\u9700\u6c42\u3002\u7136\u800c\uff0c\u4e0e\u6210\u719f\u7684\u81ea\u52a8\u9a7e\u9a76 (AD) \u7cfb\u7edf\u76f8\u6bd4\uff0cAR \u7531\u4e8e\u4e24\u8f6e\u5e73\u53f0\u7684\u56fa\u6709\u4e0d\u7a33\u5b9a\u6027\u3001\u5c3a\u5bf8\u548c\u529f\u7387\u9650\u5236\u4ee5\u53ca\u4e0d\u53ef\u9884\u6d4b\u7684\u73af\u5883\uff0c\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u5e76\u5bf9\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u5ba1\u67e5\u4e86\u81ea\u52a8\u9a91\u884c (AR) \u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u5305\u62ec\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\uff0c\u5e76\u501f\u9274\u4e86\u81ea\u52a8\u9a7e\u9a76 (AD) \u6280\u672f\u3002", "result": "\u5ba1\u67e5\u786e\u5b9a\u4e86\u5f53\u524d AR \u7814\u7a76\u4e2d\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5305\u62ec\u7f3a\u4e4f\u5168\u9762\u7684 AR \u611f\u77e5\u7cfb\u7edf\u3001\u884c\u4e1a\u548c\u653f\u5e9c\u652f\u6301\u6709\u9650\uff0c\u4ee5\u53ca\u7814\u7a76\u754c\u5173\u6ce8\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u4ece AD \u7684\u89d2\u5ea6\u5206\u6790\u4e86 AR \u7684\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u6280\u672f\u548c\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7b49\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u901a\u8fc7\u6574\u5408\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u7684\u89c1\u89e3\u548c\u81ea\u52a8\u9a91\u884c (AR) \u7684\u5177\u4f53\u8981\u6c42\uff0c\u52a0\u901f\u672a\u6765\u57ce\u5e02\u4ea4\u901a\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u9a91\u884c\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.12168", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12168", "abs": "https://arxiv.org/abs/2507.12168", "authors": ["Lu Yu", "Zhong Ren", "Youyi Zheng", "Xiang Chen", "Kun Zhou"], "title": "Shape Adaptation for 3D Hairstyle Retargeting", "comment": null, "summary": "It is demanding to author an existing hairstyle for novel characters in games\nand VR applications. However, it is a non-trivial task for artists due to the\ncomplicated hair geometries and spatial interactions to preserve. In this\npaper, we present an automatic shape adaptation method to retarget 3D\nhairstyles. We formulate the adaptation process as a constrained optimization\nproblem, where all the shape properties and spatial relationships are converted\ninto individual objectives and constraints. To make such an optimization on\nhigh-resolution hairstyles tractable, we adopt a multi-scale strategy to\ncompute the target positions of the hair strands in a coarse-to-fine manner.\nThe global solving for the inter-strands coupling is restricted to the coarse\nlevel, and the solving for fine details is made local and parallel. In\naddition, we present a novel hairline edit tool to allow for user customization\nduring retargeting. We achieve it by solving physics-based deformations of an\nembedded membrane to redistribute the hair roots with minimal distortion. We\ndemonstrate the efficacy of our method through quantitative and qualitative\nexperiments on various hairstyles and characters.", "AI": {"tldr": "An automatic method for adapting 3D hairstyles to new characters in games/VR, using multi-scale optimization and a new hairline editing tool, making it easier for artists.", "motivation": "Retargeting existing 3D hairstyles for new characters in games and VR is challenging for artists due to complex hair geometries and spatial interactions that need to be preserved.", "method": "The method formulates hair retargeting as a constrained optimization problem, using a multi-scale strategy for efficient computation. It addresses global coupling at a coarse level and local details in parallel. A novel hairline edit tool is introduced, utilizing physics-based membrane deformation to redistribute hair roots with minimal distortion.", "result": "The method effectively retargets 3D hairstyles, demonstrated through quantitative and qualitative experiments on various hairstyles and characters, preserving shape properties and spatial relationships.", "conclusion": "This paper presents an automatic shape adaptation method to retarget 3D hairstyles, achieving efficient and high-quality results through a multi-scale optimization strategy and offering user customization with a novel hairline edit tool."}}
{"id": "2507.11742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Lud\u00e4scher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "CRABS\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u6cd5\u5206\u6790\u548cLLM\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u51c6\u786e\u7406\u89e3Python\u7b14\u8bb0\u672c\u7684\u6570\u636e\u6d41\u548c\u6267\u884c\u4f9d\u8d56\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u7406\u89e3\u5305\u542b\u590d\u6742\u6570\u636e\u548c\u8f6f\u4ef6\u4f9d\u8d56\u7684Python\u7b14\u8bb0\u672c\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e38\u51fa\u73b0\u5e7b\u89c9\u548c\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u4f30\u3001\u590d\u7528\u548c\u6539\u7f16\u7b14\u8bb0\u672c\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u7b14\u8bb0\u672c\u7684\u7ed3\u6784\u548c\u6570\u636e\u6d41\u3002", "method": "CRABS\u7b56\u7565\uff1a\u7ed3\u5408\u6d45\u5c42\u8bed\u6cd5\u5206\u6790\uff08AST\uff09\u548cLLM\u7684\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u8bc6\u522b\u5355\u5143\u683c\u7684\u8f93\u5165\u8f93\u51fa\u96c6\u6765\u89e3\u51b3\u6b67\u4e49\uff0c\u4ece\u800c\u6784\u5efa\u4fe1\u606f\u6d41\u56fe\u548c\u5355\u5143\u683c\u6267\u884c\u4f9d\u8d56\u56fe\u3002", "result": "CRABS\u5728\u5305\u542b3454\u4e2a\u5355\u5143\u683c\u8f93\u5165\u8f93\u51fa\u768450\u4e2aKaggle\u7b14\u8bb0\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cLLM\u5728\u89e3\u51b3\u8bed\u6cd5\u5206\u6790\u9057\u7559\u7684\u6b67\u4e49\u65b9\u9762\u8fbe\u5230\u4e8698%\u7684\u51c6\u786e\u7387\u3002CRABS\u5728\u8bc6\u522b\u5355\u5143\u683c\u95f4\u4fe1\u606f\u6d41\u65b9\u9762\u53d6\u5f97\u4e8698%\u7684\u5e73\u5747F1\u5206\u6570\uff0c\u5728\u8bc6\u522b\u4f20\u9012\u6027\u5355\u5143\u683c\u6267\u884c\u4f9d\u8d56\u65b9\u9762\u53d6\u5f97\u4e8699%\u7684\u5e73\u5747F1\u5206\u6570\u3002", "conclusion": "CRABS\u901a\u8fc7\u7ed3\u5408\u6d45\u5c42\u8bed\u6cd5\u5206\u6790\u548cLLM\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u8bc6\u522b\u7b14\u8bb0\u672c\u4e2d\u7684\u4fe1\u606f\u6d41\u548c\u5355\u5143\u683c\u6267\u884c\u4f9d\u8d56\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523098%\u548c99%\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u7406\u89e3\u590d\u6742\u7b14\u8bb0\u672c\u65f6\u9047\u5230\u7684\u6311\u6218\u3002"}}
{"id": "2507.11571", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11571", "abs": "https://arxiv.org/abs/2507.11571", "authors": ["Varun Velankar"], "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "comment": null, "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5143\u5206\u6790\u548c\u5b9e\u9a8c\uff0c\u4e3a\u4ece\u6b65\u6001\u4f30\u8ba1\u5e74\u9f84\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u7ebf\u548c\u5b9e\u7528\u6307\u5357\uff0c\u65e8\u5728\u5c06\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5e74\u9f84\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u5230\u4e09\u5e74\u4ee5\u4e0b\u3002", "motivation": "\u4ece\u6b65\u6001\u4f30\u8ba1\u4e00\u4e2a\u4eba\u7684\u5e74\u9f84\u5728\u533b\u7597\u4fdd\u5065\u3001\u5b89\u5168\u548c\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5bf9\u6d89\u53ca\u89c6\u9891\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u96f7\u8fbe\u4f20\u611f\u5668\u8bb0\u5f55\u7684\u4e03\u4e07\u4e94\u5343\u591a\u540d\u53d7\u8bd5\u8005\u7684\u4e94\u5341\u4e5d\u9879\u7814\u7a76\u8fdb\u884c\u4e86\u56de\u987e\uff0c\u7136\u540e\u5206\u6790\u4e86 OU-ISIR \u5927\u89c4\u6a21\u4eba\u53e3\u6570\u636e\u96c6\u4e2d\u516d\u4e07\u4e09\u5343\u516b\u767e\u56db\u5341\u516d\u4e2a\u6b65\u6001\u5468\u671f\uff0c\u91cf\u5316\u4e86\u5e74\u9f84\u4e0e\u6b65\u957f\u3001\u6b65\u884c\u901f\u5ea6\u3001\u6b65\u9891\u3001\u6b65\u957f\u65f6\u95f4\u53d8\u5f02\u6027\u548c\u5173\u8282\u89d2\u5ea6\u71b5\u8fd9\u4e94\u4e2a\u5173\u952e\u6307\u6807\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6700\u540e\uff0c\u7814\u7a76\u5bf9 VersatileGait \u6570\u636e\u5e93\u7684\u5341\u4e07\u4e2a\u6837\u672c\u5b50\u96c6\u8fdb\u884c\u4e86\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u3001\u591a\u5c42\u611f\u77e5\u5668\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6bd4\u8f83\u3002", "result": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5e73\u5747\u8bef\u5dee\u7ea6\u4e3a 4.2 \u5e74\uff0c\u60ef\u6027\u4f20\u611f\u5668\u6a21\u578b\u7684\u5e73\u5747\u8bef\u5dee\u7ea6\u4e3a 4.5 \u5e74\uff0c\u800c\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u8bef\u5dee\u4f4e\u81f3 3.4 \u5e74\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u9ad8\u8fbe 96% \u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5904\u7406\u6bcf\u4e2a\u6837\u672c\u7684\u65f6\u95f4\u4e0d\u5230 0.1 \u79d2\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u5e7f\u6cdb\u7684\u5143\u5206\u6790\u548c\u65b0\u8fdb\u884c\u7684\u3001\u5927\u89c4\u6a21\u7684\u5b9e\u9a8c\u4ee5\u53ca\u53ef\u89e3\u91ca\u7684\u53ef\u89c6\u5316\uff0c\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u6027\u80fd\u57fa\u7ebf\uff0c\u5e76\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5c06\u6b65\u6001\u5e74\u9f84\u8bef\u5dee\u51cf\u5c11\u5230\u4e09\u5e74\u4ee5\u4e0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u3002"}}
{"id": "2507.11979", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11979", "abs": "https://arxiv.org/abs/2507.11979", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11696", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.11696", "abs": "https://arxiv.org/abs/2507.11696", "authors": ["Alice C. Quillen", "Nathan Skerrett", "Damian R. Sowinski", "Abobakar Sediq Miakhel"], "title": "Notions of Adiabatic Drift in the Quantized Harper model", "comment": null, "summary": "We study a quantized, discrete and drifting version of the Harper\nHamiltonian, also called the finite almost Mathieu operator, which resembles\nthe pendulum Hamiltonian but in phase space is confined to a torus. Spacing\nbetween pairs of eigenvalues of the operator spans many orders of magnitude,\nwith nearly degenerate pairs of states at energies that are associated with\ncirculating orbits in the associated classical system. When parameters of the\nsystem slowly vary, both adiabatic and diabatic transitions can take place at\ndrift rates that span many orders of magnitude. Only under an extremely\nnegligible drift rate would all transitions into superposition states be\nsuppressed. The wide range of energy level spacings could be a common property\nof quantum systems with non-local potentials that are related to resonant\nclassical dynamical systems. Notions for adiabatic drift are discussed for\nquantum systems that are associated with classical ones with divided phase\nspace.", "AI": {"tldr": "\u7814\u7a76\u4e86\u54c8\u73c0\u54c8\u5bc6\u987f\u91cf\u7684\u91cf\u5316\u3001\u79bb\u6563\u548c\u6f02\u79fb\u7248\u672c\uff08\u6709\u9650\u51e0\u4e4e\u9a6c\u4fee\u7b97\u5b50\uff09\uff0c\u8be5\u7b97\u5b50\u7c7b\u4f3c\u4e8e\u949f\u6446\u54c8\u5bc6\u987f\u91cf\uff0c\u4f46\u53d7\u9650\u4e8e\u73af\u9762\u3002\u8be5\u7b97\u5b50\u5177\u6709\u8de8\u8d8a\u591a\u4e2a\u6570\u91cf\u7ea7\u7684\u80fd\u7ea7\u95f4\u8ddd\uff0c\u5e76\u4e0e\u5faa\u73af\u8f68\u9053\u76f8\u5173\u3002\u7cfb\u7edf\u53c2\u6570\u7f13\u6162\u53d8\u5316\u65f6\uff0c\u4f1a\u53d1\u751f\u7edd\u70ed\u548c\u975e\u7edd\u70ed\u8dc3\u8fc1\u3002\u53ea\u6709\u5728\u6f02\u79fb\u901f\u7387\u6781\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u80fd\u6291\u5236\u6240\u6709\u5411\u53e0\u52a0\u6001\u7684\u8dc3\u8fc1\u3002\u8be5\u6a21\u578b\u8868\u660e\uff0c\u5177\u6709\u975e\u5c40\u57df\u52bf\u4e14\u4e0e\u5171\u632f\u7ecf\u5178\u52a8\u529b\u7cfb\u7edf\u76f8\u5173\u7684\u91cf\u5b50\u7cfb\u7edf\u53ef\u80fd\u5177\u6709\u5e7f\u6cdb\u7684\u80fd\u7ea7\u95f4\u8ddd\u3002\u8ba8\u8bba\u4e86\u4e0e\u5177\u6709\u5206\u88c2\u76f8\u7a7a\u95f4\u7684\u7ecf\u5178\u7cfb\u7edf\u76f8\u5173\u8054\u7684\u91cf\u5b50\u7cfb\u7edf\u7684\u7edd\u70ed\u6f02\u79fb\u6982\u5ff5\u3002", "motivation": "\u7814\u7a76\u91cf\u5316\u3001\u79bb\u6563\u548c\u6f02\u79fb\u7684\u54c8\u73c0\u54c8\u5bc6\u987f\u91cf\uff0c\u8fd9\u79cd\u54c8\u5bc6\u987f\u91cf\u5728\u76f8\u7a7a\u95f4\u4e2d\u9650\u5236\u5728\u73af\u9762\u4e0a\uff0c\u7c7b\u4f3c\u4e8e\u949f\u6446\u54c8\u5bc6\u987f\u91cf\u3002", "method": "\u7814\u7a76\u4e86\u54c8\u73c0\u54c8\u5bc6\u987f\u91cf\u7684\u91cf\u5316\u3001\u79bb\u6563\u548c\u6f02\u79fb\u7248\u672c\uff0c\u4e5f\u79f0\u4e3a\u6709\u9650\u51e0\u4e4e\u9a6c\u4fee\u7b97\u5b50\u3002", "result": "\u89c2\u5bdf\u5230\u80fd\u7ea7\u95f4\u8ddd\u8de8\u8d8a\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u5177\u6709\u63a5\u8fd1\u7b80\u5e76\u7684\u80fd\u7ea7\u5bf9\uff0c\u8fd9\u4e9b\u80fd\u7ea7\u5bf9\u4e0e\u76f8\u5173\u7684\u7ecf\u5178\u7cfb\u7edf\u4e2d\u7684\u5faa\u73af\u8f68\u9053\u76f8\u5173\u3002\u5f53\u7cfb\u7edf\u53c2\u6570\u7f13\u6162\u53d8\u5316\u65f6\uff0c\u7edd\u70ed\u548c\u975e\u7edd\u70ed\u8dc3\u8fc1\u53ef\u80fd\u53d1\u751f\u5728\u8de8\u8d8a\u591a\u4e2a\u6570\u91cf\u7ea7\u7684\u6f02\u79fb\u901f\u7387\u4e0b\u3002\u53ea\u6709\u5728\u6f02\u79fb\u901f\u7387\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u80fd\u6291\u5236\u6240\u6709\u5411\u53e0\u52a0\u6001\u7684\u8dc3\u8fc1\u3002", "conclusion": "\u8be5\u6a21\u578b\u8868\u660e\uff0c\u5177\u6709\u975e\u5c40\u57df\u52bf\u4e14\u4e0e\u5171\u632f\u7ecf\u5178\u52a8\u529b\u7cfb\u7edf\u76f8\u5173\u7684\u91cf\u5b50\u7cfb\u7edf\u53ef\u80fd\u5177\u6709\u5e7f\u6cdb\u7684\u80fd\u7ea7\u95f4\u8ddd\u3002"}}
{"id": "2507.11620", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11620", "abs": "https://arxiv.org/abs/2507.11620", "authors": ["Steven Dillmann", "Juan Rafael Mart\u00ednez-Galarza"], "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u7684\u65b0\u578b\u5f20\u91cf\u8868\u793a\u548c\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u5728X\u5c04\u7ebf\u5929\u6587\u5b66\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u56e0\u5176\u975e\u7ed3\u6784\u5316\u548c\u4e0d\u89c4\u5219\u7684\u7ed3\u6784\uff0c\u5728\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u548c\u8bc6\u522b\u663e\u8457\u73b0\u8c61\u65b9\u9762\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u4ee5\u5b66\u4e60\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u8fd9\u4e9b\u5d4c\u5165\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u5f02\u5e38\u68c0\u6d4b\u3001\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u65e0\u76d1\u7763\u5206\u7c7b\u3002\u5728X\u5c04\u7ebf\u5929\u6587\u5b66\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u8868\u660e\u8fd9\u4e9b\u8868\u793a\u6210\u529f\u5730\u6355\u83b7\u4e86\u65f6\u95f4\u548c\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u5206\u79bb\u4e86\u4e0d\u540c\u7c7b\u522b\u7684X\u5c04\u7ebf\u77ac\u53d8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790\u8de8\u79d1\u5b66\u548c\u5de5\u4e1a\u9886\u57df\u7684\u590d\u6742\u3001\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12018", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12018", "abs": "https://arxiv.org/abs/2507.12018", "authors": ["M. Zhezhu", "Y. Melikyan", "V. Hayrapetyan", "A. Vasilev", "H. Gharagulyan"], "title": "Dual Effect of L-Cysteine on the Reorientation and Relaxation of Fe3O4-Decorated Graphene Oxide Liquid Crystals", "comment": null, "summary": "Here, we study the dynamics of Fe3O4-decorated L-Cysteine-functionalized GOLC\ndirector under an external magnetic field and analyze L-Cysteine's influence on\nthe reorientation and relaxation time of the director. In particular, Fe3O4\nnanoparticles were synthesized by solution-combustion method and added for\naltering orientational properties of GO which we synthesized electrochemically\nand functionalized by L-Cysteine. In addition, a comprehensive comparison of\nthe director behaviour of GOLC and Fe3O4-decorated L-Cysteine-GOLC was\nundertaken to verify the tunability of the aforementioned systems. Furthermore,\nwe demonstrate dual-effect of L-Cysteine on the magnetic field-induced\nalignment and relaxation time of GOLC systems, namely decrease in reorientation\ntime and at the same time increase in relaxation time. Besides, micropattern\ncreation and controlling in the drying drops of GOLC (net- and knit-like,\nflower-like, radial- and parallel-strip etc.) using a magnetic field were\nshown. The results of our studies could facilitate the fabrication of ordered\nand patterned tunable GOLC assemblies for a range of advanced applications.\nGOLC was undertaken to verify the tunability of the aforementioned systems.\nFurthermore, we demonstrate dual-effect of L-Cysteine on the magnetic\nfield-induced alignment and relaxation time of GOLC systems, namely decrease in\nreorientation time and at the same time increase in relaxation time. Besides,\nmicropattern creation and controlling in the drying drops of GOLC (net- and\nknit-like, flower-like, radial- and parallel-strip etc.) using a magnetic field\nwere shown. The results of our studies could facilitate the fabrication of\nordered and patterned tunable GOLC assemblies for a range of advanced\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u78c1\u573a\u63a7\u5236Fe3O4\u4fee\u9970\u7684L-\u534a\u80f1\u6c28\u9178\u529f\u80fd\u5316GOLC\u7684\u53d6\u5411\u548c\u5f1b\u8c6b\u65f6\u95f4\uff0c\u5e76\u5b9e\u73b0\u4e86\u5fae\u56fe\u6848\u7684\u521b\u5efa\uff0c\u4e3a\u9ad8\u7ea7\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u8c03\u8c10GOLC\u7ec4\u4ef6\u7684\u5236\u9020\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76Fe3O4\u4fee\u9970\u7684L-\u534a\u80f1\u6c28\u9178\u529f\u80fd\u5316GOLC\u5728\u5916\u90e8\u78c1\u573a\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5206\u6790L-\u534a\u80f1\u6c28\u9178\u5bf9GOLC\u53d6\u5411\u548c\u5f1b\u8c6b\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5fae\u56fe\u6848\u521b\u5efa\u548c\u63a7\u5236\u65b9\u9762\u7684\u5e94\u7528\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u8c03\u8c10GOLC\u7ec4\u4ef6\u7684\u5236\u9020\u3002", "method": "\u901a\u8fc7\u6eb6\u6db2\u71c3\u70e7\u6cd5\u5408\u6210Fe3O4\u7eb3\u7c73\u9897\u7c92\uff0c\u5e76\u7528L-\u534a\u80f1\u6c28\u9178\u5bf9\u7535\u5316\u5b66\u5408\u6210\u7684GO\u8fdb\u884c\u529f\u80fd\u5316\uff0c\u7136\u540e\u7814\u7a76Fe3O4\u4fee\u9970\u7684L-\u534a\u80f1\u6c28\u9178\u529f\u80fd\u5316GOLC\u5728\u5916\u90e8\u78c1\u573a\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u4e0eGOLC\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u7814\u7a76L-\u534a\u80f1\u6c28\u9178\u5bf9GOLC\u7cfb\u7edf\u78c1\u573a\u8bf1\u5bfc\u7684\u53d6\u5411\u548c\u5f1b\u8c6b\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "Fe3O4\u4fee\u9970\u7684L-\u534a\u80f1\u6c28\u9178\u529f\u80fd\u5316GOLC\u5728\u78c1\u573a\u4f5c\u7528\u4e0b\u8868\u73b0\u51fa\u53ef\u8c03\u8c10\u7684\u53d6\u5411\u884c\u4e3a\u3002L-\u534a\u80f1\u6c28\u9178\u80fd\u591f\u7f29\u77edGOLC\u7684\u53d6\u5411\u65f6\u95f4\u5e76\u5ef6\u957f\u5176\u5f1b\u8c6b\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u78c1\u573a\u53ef\u4ee5\u63a7\u5236GOLC\u5e72\u71e5\u6db2\u6ef4\u7684\u5fae\u56fe\u6848\u521b\u5efa\uff0c\u5f62\u6210\u7f51\u72b6\u3001\u9488\u7ec7\u72b6\u3001\u82b1\u72b6\u3001\u8f90\u5c04\u72b6\u548c\u5e73\u884c\u6761\u72b6\u7b49\u56fe\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0cL-\u534a\u80f1\u6c28\u9178\u5bf9Fe3O4\u4fee\u9970\u7684L-\u534a\u80f1\u6c28\u9178\u529f\u80fd\u5316GOLC\u7684\u78c1\u573a\u8bf1\u5bfc\u53d6\u5411\u548c\u5f1b\u8c6b\u65f6\u95f4\u5177\u6709\u53cc\u91cd\u5f71\u54cd\uff0c\u53ef\u7f29\u77ed\u53d6\u5411\u65f6\u95f4\u5e76\u5ef6\u957f\u5f1b\u8c6b\u65f6\u95f4\uff0c\u8fd9\u4e3a\u53ef\u8c03\u8c10GOLC\u7ec4\u4ef6\u7684\u5236\u9020\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.12000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12000", "abs": "https://arxiv.org/abs/2507.12000", "authors": ["Jiahong Ning", "Ce Zheng", "Tingting Yang"], "title": "DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "comment": "ICML 2025", "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "AI": {"tldr": "DSSD\u662f\u4e00\u79cd\u5728\u8bbe\u5907\u8fb9\u7f18\u7cfb\u7edf\u90e8\u7f72LLM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bbe\u5907\u548c\u8fb9\u7f18\u4e4b\u95f4\u5206\u914d\u9a8c\u8bc1\u4efb\u52a1\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bbe\u5907\u8fb9\u7f18\u7cfb\u7edf\u90e8\u7f72\u7684\u8d44\u6e90\u9650\u5236\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u6216\u5019\u9009\u8bcd\u5143\u9a8c\u8bc1\u7684\u4e0a\u884c\u4f20\u8f93\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u7f3a\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5e03\u5f0f\u5206\u88c2\u63a8\u6d4b\u89e3\u7801\uff08DSSD\uff09\u7684\u65b0\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4fdd\u6301\u4e86SLM-LLM\u7684\u5212\u5206\uff0c\u5e76\u5c06\u9a8c\u8bc1\u9636\u6bb5\u5728\u8bbe\u5907\u548c\u8fb9\u7f18\u4e4b\u95f4\u8fdb\u884c\u4e86\u5212\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDSSD\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "DSSD\u901a\u8fc7\u5728\u8bbe\u5907\u548c\u8fb9\u7f18\u4e4b\u95f4\u5212\u5206\u9a8c\u8bc1\u9636\u6bb5\uff0c\u7528\u5355\u4e00\u4e0b\u884c\u4f20\u8f93\u66ff\u6362\u4e86\u591a\u4e2a\u8bcd\u6c47\u5206\u5e03\u7684\u4e0a\u884c\u4f20\u8f93\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12115", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12115", "abs": "https://arxiv.org/abs/2507.12115", "authors": ["Jan Karthein", "Jonas Buchhorn", "Kaycee Underwood", "Abdur Rehman Jalil", "Max Va\u00dfen-Carl", "Peter Sch\u00fcffelgen", "Detlev Gr\u00fctzmacher", "Thomas Sch\u00e4pers"], "title": "Controlling the magneto-transport properties of magnetic topological insulator thin films from Cr$_x$(Bi$_y\\,$Sb$_{1-y}$)$_{2-x}$Te$_3$ via molecular beam epitaxy", "comment": "15 pages, 8 figures", "summary": "In this work we present a systematic in-depth study of how we can alter the\nmagneto-transport properties of magnetic topological insulator thin films by\ntuning the parameters of the molecular beam epitaxy. First, we show how a\nvarying substrate temperature changes the surface morphology and when chosen\nproperly leads to a high crystal quality. Next, the effect of the chromium\nconcentration on the film roughness and crystal quality is investigated.\nFinally, both the substrate temperature and the chromium concentration are\ninvestigated with respect to their effect on the magneto-transport properties\nof the magnetic topological insulator thin films. It becomes apparent that the\nsubstrate temperature and the chromium concentration can be used to tune the\nFermi level of the film which allows to make the material intrinsically charge\nneutral. A very low chromium concentration furthermore allows to tune the\nmagnetic topological insulator into a regime where strong superconducting\ncorrelations can be expected when combining the material with a superconductor.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u5206\u5b50\u675f\u5916\u5ef6\u7684\u886c\u5e95\u6e29\u5ea6\u548c\u94ec\u6d53\u5ea6\uff0c\u53ef\u4ee5\u4f18\u5316\u78c1\u6027\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u7684\u8d28\u91cf\uff0c\u8c03\u63a7\u5176\u8f7d\u6d41\u5b50\u6d53\u5ea6\uff0c\u5e76\u6709\u671b\u5b9e\u73b0\u8d85\u5bfc\u7535\u6027\u3002", "motivation": "\u63a2\u7a76\u901a\u8fc7\u8c03\u6574\u5206\u5b50\u675f\u5916\u5ef6\u53c2\u6570\u6765\u8c03\u63a7\u78c1\u6027\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u78c1\u8f93\u8fd0\u6027\u8d28\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u8f7d\u6d41\u5b50\u6d53\u5ea6\u7684\u8c03\u63a7\u548c\u6f5c\u5728\u7684\u8d85\u5bfc\u7535\u6027\u3002", "method": "\u7814\u7a76\u4e86\u5206\u5b50\u675f\u5916\u5ef6\u8fc7\u7a0b\u4e2d\u886c\u5e95\u6e29\u5ea6\u548c\u94ec\u6d53\u5ea6\u5bf9\u78c1\u6027\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u7684\u8868\u9762\u5f62\u8c8c\u3001\u6676\u4f53\u8d28\u91cf\u3001\u7c97\u7cd9\u5ea6\u548c\u78c1\u8f93\u8fd0\u6027\u8d28\u7684\u5f71\u54cd\u3002", "result": "\u886c\u5e95\u6e29\u5ea6\u548c\u94ec\u6d53\u5ea6\u7684\u7cbe\u786e\u63a7\u5236\u53ef\u4ee5\u4f18\u5316\u8584\u819c\u7684\u8d28\u91cf\uff0c\u5e76\u8c03\u63a7\u8d39\u7c73\u80fd\u7ea7\uff0c\u4f7f\u5176\u63a5\u8fd1\u672c\u5f81\u7535\u8377\u4e2d\u6027\u3002\u8f83\u4f4e\u7684\u94ec\u6d53\u5ea6\u6709\u52a9\u4e8e\u5c06\u6750\u6599\u63a8\u5411\u53ef\u80fd\u51fa\u73b0\u5f3a\u8d85\u5bfc\u5173\u8054\u7684\u533a\u57df\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u5206\u5b50\u675f\u5916\u5ef6\u7684\u53c2\u6570\uff0c\u53ef\u4ee5\u4f18\u5316\u78c1\u6027\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u7684\u8f7d\u6d41\u5b50\u6d53\u5ea6\uff0c\u5e76\u53ef\u80fd\u5b9e\u73b0\u8d85\u5bfc\u7535\u6027\u3002"}}
{"id": "2507.11916", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528CPU\u548cGPU\u5e76\u884c\u6027\u7684\u6210\u672c\u53d7\u9650\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08CB-DFS\uff09\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86Batch IDA*\u548cBatch BTS\u7b49\u7b97\u6cd5\uff0c\u5728\u9b54\u65b9\u548c\u6ed1\u52a8\u62fc\u56fe\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528GPU\u7684\u5e76\u884c\u5904\u7406\u80fd\u529b\u6765\u589e\u5f3a\u7ecf\u5178\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u7684\u6279\u91cfGPU\u8ba1\u7b97\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6210\u672c\u53d7\u9650\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08CB-DFS\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u6b64\u65b9\u6cd5\u7684\u7b97\u6cd5\uff0c\u5982Batch IDA*\u548cBatch BTS\u3002", "result": "GPU\u64cd\u4f5c\u53ef\u4ee5\u6709\u6548\u5730\u6279\u91cf\u5904\u7406DFS\uff0c\u5e76\u4e14\u57283x3\u9b54\u65b9\u548c4x4\u6ed1\u52a8\u62fc\u56fe\uff08STP\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8fd8\u5206\u6790\u4e86\u8d85\u53c2\u6570\u3001\u795e\u7ecf\u7f51\u7edc\u542f\u53d1\u5f0f\u5927\u5c0f\u548c\u786c\u4ef6\u8d44\u6e90\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "GPU\u64cd\u4f5c\u53ef\u4ee5\u6709\u6548\u5730\u6279\u91cf\u5904\u7406DFS\uff0c\u5e76\u4e14\u6211\u4eec\u57283x3\u9b54\u65b9\u548c4x4\u6ed1\u52a8\u62fc\u56fe\uff08STP\uff09\u4e0a\u8bc4\u4f30\u4e86CB-DFS\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.12052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12052", "abs": "https://arxiv.org/abs/2507.12052", "authors": ["Takumi Shinohara", "Karl H. Johansson", "Henrik Sandberg"], "title": "Distributed Resilient State Estimation and Control with Strategically Implemented Security Measures", "comment": null, "summary": "This paper addresses the problem of distributed resilient state estimation\nand control for linear time-invariant systems in the presence of malicious\nfalse data injection sensor attacks and bounded noise. We consider a system\noperator (defender) capable of deploying cybersecurity measures to counteract\nthe sensor compromises. Although such measures enhance resilience against\nadversarial attacks, they may incur substantial costs; hence, it is crucial to\nselect countermeasures to balance resilience gains and cost efficiency\nstrategically. We first demonstrate that the system's resilience against\nattacks is maximized through the appropriate implementation of security\nmeasures, implying that no attacker can execute undetectable sensor attacks.\nBuilding on this analysis, we propose an algorithm that identifies the optimal\nsecurity measure. While determining this measure is NP-hard in general, we also\nderive sufficient conditions under which efficient computation is feasible.\nFurthermore, we develop a distributed resilient state estimation and control\nscheme informed by the optimal security measure and establish conditions that\nguarantee bounded estimation and control errors. Finally, we validate the\nefficacy of our approach via numerical simulations of a vehicle platooning\nscenario.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9762\u5bf9\u6076\u610f\u653b\u51fb\u548c\u566a\u58f0\u65f6\uff0c\u80fd\u591f\u5e73\u8861\u6210\u672c\u548c\u5f39\u6027\u7684\u5206\u5e03\u5f0f\u72b6\u6001\u4f30\u8ba1\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8f66\u8f86\u7f16\u961f\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5b58\u5728\u6076\u610f\u865a\u5047\u6570\u636e\u6ce8\u5165\u4f20\u611f\u5668\u653b\u51fb\u548c\u6709\u754c\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u7814\u7a76\u5206\u5e03\u5f0f\u5f39\u6027\u72b6\u6001\u4f30\u8ba1\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u8003\u8651\u4e86\u7cfb\u7edf\u8fd0\u8425\u5546\uff08\u9632\u5fa1\u8005\uff09\u4e3a\u62b5\u5fa1\u4f20\u611f\u5668\u653b\u51fb\u800c\u90e8\u7f72\u7f51\u7edc\u5b89\u5168\u63aa\u65bd\u7684\u6210\u672c\u6548\u76ca\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\u6765\u8bc6\u522b\u6700\u4f18\u5b89\u5168\u63aa\u65bd\uff0c\u5e76\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u63a8\u5bfc\u51fa\u53ef\u9ad8\u6548\u8ba1\u7b97\u7684\u5145\u5206\u6761\u4ef6\uff0c\u6700\u7ec8\u5f00\u53d1\u4e86\u4e00\u4e2a\u8003\u8651\u6700\u4f18\u5b89\u5168\u63aa\u65bd\u7684\u5206\u5e03\u5f0f\u5f39\u6027\u72b6\u6001\u4f30\u8ba1\u548c\u63a7\u5236\u65b9\u6848\u3002", "result": "\u8bc1\u660e\u4e86\u901a\u8fc7\u9002\u5f53\u5b9e\u65bd\u5b89\u5168\u63aa\u65bd\u53ef\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u7684\u5f39\u6027\uff0c\u4ece\u800c\u4f7f\u5f97\u653b\u51fb\u8005\u65e0\u6cd5\u6267\u884c\u4e0d\u53ef\u68c0\u6d4b\u7684\u4f20\u611f\u5668\u653b\u51fb\uff0c\u5e76\u7ed9\u51fa\u4e86\u4fdd\u8bc1\u4f30\u8ba1\u548c\u63a7\u5236\u8bef\u5dee\u7684\u6709\u754c\u6761\u4ef6\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u5f39\u6027\u72b6\u6001\u4f30\u8ba1\u4e0e\u63a7\u5236\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u5e73\u8861\u5f39\u6027\u6536\u76ca\u4e0e\u6210\u672c\u6548\u76ca\uff0c\u5e76\u5728\u8f66\u8f86\u7f16\u961f\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2507.11929", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11929", "abs": "https://arxiv.org/abs/2507.11929", "authors": ["Minchen Yu", "Yinghao Ren", "Jiamu Zhao", "Jiaqi Li"], "title": "Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics", "comment": null, "summary": "Serverless computing has attracted a broad range of applications due to its\nease of use and resource elasticity. However, developing serverless\napplications often poses a dilemma -- relying on general-purpose serverless\nplatforms can fall short of delivering satisfactory performance for complex\nworkloads, whereas building application-specific serverless systems undermines\nthe simplicity and generality. In this paper, we propose an extensible design\nprinciple for serverless computing. We argue that a platform should enable\ndevelopers to extend system behaviors for domain-specialized optimizations\nwhile retaining a shared, easy-to-use serverless environment. We take data\nanalytics as a representative serverless use case and realize this design\nprinciple in Proteus. Proteus introduces a novel abstraction of decision\nworkflows, allowing developers to customize control-plane behaviors for\nimproved application performance. Preliminary results show that Proteus's\nprototype effectively optimizes analytical query execution and supports\nfine-grained resource sharing across diverse applications.", "AI": {"tldr": "Serverless computing often forces a choice between performance and simplicity. This paper proposes an extensible design principle for serverless platforms, realized in Proteus. Proteus uses decision workflows to let developers optimize for specific needs (like data analytics) while keeping the platform simple and shareable, showing improved performance and resource sharing.", "motivation": "Serverless applications face a dilemma between using general-purpose platforms that may lack performance for complex workloads and building application-specific systems that sacrifice simplicity and generality. The paper aims to address this by enabling domain-specialized optimizations within a shared, easy-to-use serverless environment.", "method": "The paper proposes an extensible design principle for serverless computing and demonstrates its application through a system called Proteus. Proteus introduces a novel abstraction of decision workflows to allow developers to customize control-plane behaviors for performance improvements, using data analytics as a representative use case.", "result": "Preliminary results show that Proteus", "conclusion": "The proposed extensible design principle and Proteus platform enable developers to customize serverless platform behaviors for domain-specific optimizations, retaining a shared and easy-to-use environment. Proteus, with its decision workflows abstraction, effectively optimizes analytical query execution and supports fine-grained resource sharing."}}
{"id": "2507.11880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11880", "abs": "https://arxiv.org/abs/2507.11880", "authors": ["Jinyuan Liu", "Minglei Fu", "Ling Shi", "Chenguang Yang", "Wenan Zhang"], "title": "A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications", "comment": "37 pages, 33 figures", "summary": "Tethered robots play a pivotal role in specialized environments such as\ndisaster response and underground exploration, where their stable power supply\nand reliable communication offer unparalleled advantages. However, their motion\nplanning is severely constrained by tether length limitations and entanglement\nrisks, posing significant challenges to achieving optimal path planning. To\naddress these challenges, this study introduces CDT-TCS (Convex Dissection\nTopology-based Tethered Configuration Search), a novel algorithm that leverages\nCDT Encoding as a homotopy invariant to represent topological states of paths.\nBy integrating algebraic topology with geometric optimization, CDT-TCS\nefficiently computes the complete set of optimal feasible configurations for\ntethered robots at all positions in 2D environments through a single\ncomputation. Building on this foundation, we further propose three\napplication-specific algorithms: i) CDT-TPP for optimal tethered path planning,\nii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for\ndistance-optimal path planning of untethered robots. All theoretical results\nand propositions underlying these algorithms are rigorously proven and\nthoroughly discussed in this paper. Extensive simulations demonstrate that the\nproposed algorithms significantly outperform state-of-the-art methods in their\nrespective problem domains. Furthermore, real-world experiments on robotic\nplatforms validate the practicality and engineering value of the proposed\nframework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDT-TCS\u7684\u65b0\u578b\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6709\u7ef3\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u9762\u4e34\u7684\u7ebf\u675f\u957f\u5ea6\u9650\u5236\u548c\u7f20\u7ed5\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u4ee3\u6570\u62d3\u6251\u548c\u51e0\u4f55\u4f18\u5316\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u8ba1\u7b97\u51fa\u673a\u5668\u4eba\u7684\u6700\u4f18\u53ef\u884c\u914d\u7f6e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e09\u4e2a\u5177\u4f53\u7684\u5e94\u7528\u7b97\u6cd5\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u6709\u7ef3\u673a\u5668\u4eba\u5728\u707e\u96be\u54cd\u5e94\u548c\u5730\u4e0b\u63a2\u7d22\u7b49\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u8fd0\u52a8\u89c4\u5212\u53d7\u5230\u7ebf\u675f\u957f\u5ea6\u9650\u5236\u548c\u7f20\u7ed5\u98ce\u9669\u7684\u4e25\u91cd\u5236\u7ea6\u3002", "method": " CDT-TCS\u7b97\u6cd5\u5229\u7528CDT\u7f16\u7801\u4f5c\u4e3a\u540c\u4f26\u4e0d\u53d8\u91cf\u6765\u8868\u793a\u8def\u5f84\u7684\u62d3\u6251\u72b6\u6001\uff0c\u7ed3\u5408\u4ee3\u6570\u62d3\u6251\u548c\u51e0\u4f55\u4f18\u5316\uff0c\u80fd\u591f\u4e00\u6b21\u6027\u8ba1\u7b97\u51fa\u4e8c\u7ef4\u73af\u5883\u4e2d\u6240\u6709\u4f4d\u7f6e\u7684\u6709\u7ef3\u673a\u5668\u4eba\u7684\u6700\u4f18\u53ef\u884c\u914d\u7f6e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86CDT-TPP\uff08\u6700\u4f18\u6709\u7ef3\u8def\u5f84\u89c4\u5212\uff09\u3001CDT-TMV\uff08\u8003\u8651\u7ebf\u675f\u7ea6\u675f\u7684\u591a\u76ee\u6807\u8bbf\u95ee\uff09\u548cCDT-UTPP\uff08\u65e0\u7ef3\u673a\u5668\u4eba\u6700\u4f18\u8ddd\u79bb\u8def\u5f84\u89c4\u5212\uff09\u4e09\u4e2a\u7279\u5b9a\u5e94\u7528\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5404\u81ea\u7684\u95ee\u9898\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9645\u9a8c\u8bc1\u3002", "conclusion": " CDT-TCS\u7b97\u6cd5\u53ca\u5176\u884d\u751f\u7b97\u6cd5\u5728\u6709\u7ef3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3001\u591a\u76ee\u6807\u8bbf\u95ee\u548c\u65e0\u7ef3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.12439", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12439", "abs": "https://arxiv.org/abs/2507.12439", "authors": ["Daniel Commey", "Rebecca A. Sarpong", "Griffith S. Klogo", "Winful Bagyl-Bac", "Garth V. Crosby"], "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u7684\u3001\u7ecf\u6d4e\u4e0a\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6fc0\u52b1\u673a\u5236\u4f7f\u6076\u610f\u884c\u4e3a\u5728\u7ecf\u6d4e\u4e0a\u4e0d\u5408\u7406\uff0c\u4ee5\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u673a\u5236\u901a\u5e38\u662f\u53cd\u5e94\u6027\u7684\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u7edf\u8ba1\u805a\u5408\u89c4\u5219\uff0c\u540c\u65f6\u901a\u5e38\u5047\u8bbe\u6709\u8bda\u5b9e\u591a\u6570\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u7684\u3001\u7ecf\u6d4e\u4e0a\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5f00\u653e\u53c2\u4e0e\u7684\u8054\u90a6\u5b66\u4e60\u6240\u9762\u4e34\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8d1d\u53f6\u65af\u6fc0\u52b1\u673a\u5236\uff0c\u5c06\u6bcf\u6b21\u8bad\u7ec3\u8f6e\u6b21\u5efa\u6a21\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u7684\u8d1d\u53f6\u65af\u535a\u5f08\uff0c\u670d\u52a1\u5668\u5229\u7528\u4e00\u5c0f\u90e8\u5206\u79c1\u6709\u7684\u9a8c\u8bc1\u6570\u636e\u96c6\u5728\u652f\u4ed8\u524d\u9a8c\u8bc1\u66f4\u65b0\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u673a\u5236\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u9762\u5bf950%\u7684\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\u8005\uff0c\u8be5\u673a\u5236\u4ecd\u80fd\u4fdd\u630196.7%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u6bd4\u9762\u5bf930%\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\u8005\u7684\u573a\u666f\u4f4e0.3\u4e2a\u767e\u5206\u70b9\u3002\u8fd9\u6bd4\u6807\u51c6\u7684FedAvg\uff08\u5728\u76f8\u540c50%\u653b\u51fb\u4e0b\u51c6\u786e\u7387\u4f1a\u5d29\u6e83\uff09\u9ad8\u51fa51.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u673a\u5236\u8ba1\u7b97\u91cf\u8f7b\u3001\u9884\u7b97\u6709\u9650\uff0c\u5e76\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4e3a\u7ecf\u6d4e\u4e0a\u7a33\u5065\u548c\u53ef\u6301\u7eed\u7684\u8054\u90a6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9014\u5f84\u3002"}}
{"id": "2507.11764", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "AI Wizards improved news subjectivity detection by adding sentiment scores to transformer models, achieving top rankings, including 1st in Greek.", "motivation": "To improve subjectivity detection in news articles by exploring the integration of sentiment scores into transformer-based models and assessing generalization capabilities across various languages, including unseen ones.", "method": "The study enhanced transformer-based classifiers (mDeBERTaV3-base, ModernBERT-base, Llama3.2-1B) by integrating sentiment scores from an auxiliary model with sentence representations. Decision threshold calibration was used to handle class imbalance.", "result": "Sentiment feature integration significantly boosted performance, especially the subjective F1 score. The framework achieved high rankings, including 1st place for Greek with a Macro F1 score of 0.51.", "conclusion": "AI Wizards' sentiment-augmented transformer models achieved strong performance in the CLEF 2025 CheckThat! Task 1, demonstrating significant improvements, particularly in subjective F1 score, and ranking first for Greek."}}
{"id": "2507.11575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11575", "abs": "https://arxiv.org/abs/2507.11575", "authors": ["Victor Caquilpan"], "title": "What cat is that? A re-id model for feral cats", "comment": "Master's project", "summary": "Feral cats exert a substantial and detrimental impact on Australian wildlife,\nplacing them among the most dangerous invasive species worldwide. Therefore,\nclosely monitoring these cats is essential labour in minimising their effects.\nIn this context, the potential application of Re-Identification (re-ID) emerges\nto enhance monitoring activities for these animals, utilising images captured\nby camera traps. This project explores different CV approaches to create a\nre-ID model able to identify individual feral cats in the wild. The main\napproach consists of modifying a part-pose guided network (PPGNet) model,\ninitially used in the re-ID of Amur tigers, to be applicable for feral cats.\nThis adaptation, resulting in PPGNet-Cat, which incorporates specific\nmodifications to suit the characteristics of feral cats images. Additionally,\nvarious experiments were conducted, particularly exploring contrastive learning\napproaches such as ArcFace loss. The main results indicate that PPGNet-Cat\nexcels in identifying feral cats, achieving high performance with a mean\nAverage Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes\nestablish PPGNet-Cat as a competitive model within the realm of re-ID.", "AI": {"tldr": "\u91ce\u732b\u5bf9\u6fb3\u5927\u5229\u4e9a\u91ce\u751f\u52a8\u7269\u6784\u6210\u5a01\u80c1\uff0c\u672c\u7814\u7a76\u4f7f\u7528\u6539\u8fdb\u7684PPGNet\u6a21\u578b\uff08PPGNet-Cat\uff09\u901a\u8fc7\u6444\u50cf\u5934\u9677\u9631\u56fe\u50cf\u8bc6\u522b\u91ce\u732b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3002", "motivation": "\u7531\u4e8e\u91ce\u732b\u5bf9\u6fb3\u5927\u5229\u4e9a\u91ce\u751f\u52a8\u7269\u6784\u6210\u91cd\u5927\u4e14\u6709\u5bb3\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u5bf9\u5176\u8fdb\u884c\u5bc6\u5207\u76d1\u6d4b\u5bf9\u4e8e\u51cf\u8f7b\u5176\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u8fd9\u4e9b\u52a8\u7269\u7684\u76d1\u6d4b\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u4fee\u6539\u4e86\u7528\u4e8e\u963f\u7a46\u5c14\u864e\u91cd\u65b0\u8bc6\u522b\u7684\u90e8\u5206\u59ff\u52bf\u5f15\u5bfc\u7f51\u7edc\uff08PPGNet\uff09\u6a21\u578b\uff0c\u4ee5\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u8bc6\u522b\u91ce\u5916\u5355\u72ec\u91ce\u732b\u7684\u91cd\u65b0\u8bc6\u522b\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u5305\u62ec\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08\u5982ArcFace\u635f\u5931\uff09\u5728\u5185\u7684\u5404\u79cd\u5b9e\u9a8c\u3002", "result": "PPGNet-Cat\u5728\u8bc6\u522b\u91ce\u732b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u4e3a0.86\uff0c\u6392\u540d\u7b2c\u4e00\u7684\u51c6\u786e\u7387\u4e3a0.95\u3002", "conclusion": "PPGNet-Cat\u5728\u91ce\u5916\u8bc6\u522b\u732b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u91cd\u65b0\u8bc6\u522b\u9886\u57df\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.12174", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12174", "abs": "https://arxiv.org/abs/2507.12174", "authors": ["Zhenmin Huang", "Yusen Xie", "Benshan Ma", "Shaojie Shen", "Jun Ma"], "title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "comment": null, "summary": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u535a\u5f08\u8bba\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u535a\u5f08\u5e76\u5229\u7528ADMM\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u535a\u5f08\u8bba\u65b9\u6cd5\u5728\u5904\u7406\u667a\u80fd\u4f53\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3001\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u89c4\u5212\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u535a\u5f08\u8bba\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u5177\u6709\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u7684\u667a\u80fd\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u822c\u8d1d\u53f6\u65af\u535a\u5f08\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u7684\u7b49\u4ef7\u5f62\u5f0f\u4e3a\u52bf\u535a\u5f08\u3002\u901a\u8fc7\u4f18\u5316\u7edf\u4e00\u7684\u4f18\u5316\u95ee\u9898\u6765\u83b7\u5f97\u6700\u4f18\u4ea4\u4e92\u8f68\u8ff9\u7684\u8d1d\u53f6\u65af\u7eb3\u4ec0\u5747\u8861\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u5076\u5171\u8bc6\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u5e76\u884c\u6c42\u89e3\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u5177\u6709\u4e00\u822c\u5f62\u5f0f\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u7684\u573a\u666f\u4e0b\u5747\u6709\u6548\uff0c\u5e76\u4e14\u5176\u53ef\u6269\u5c55\u6027\u4f18\u4e8e\u73b0\u6709\u7684\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5177\u6709\u4e00\u822c\u5f62\u5f0f\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0d\u786e\u5b9a\u535a\u5f08\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2507.11707", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11707", "abs": "https://arxiv.org/abs/2507.11707", "authors": ["Leo S\u00fcnkel", "Jonas Stein", "Maximilian Zorn", "Thomas Gabor", "Claudia Linnhoff-Popien"], "title": "Time-Aware Qubit Assignment and Circuit Optimization for Distributed Quantum Computing", "comment": "Accepted at QCE 2025", "summary": "The emerging paradigm of distributed quantum computing promises a potential\nsolution to scaling quantum computing to currently unfeasible dimensions. While\nthis approach itself is still in its infancy, and many obstacles must still be\novercome before its physical implementation, challenges from the software and\nalgorithmic side must also be identified and addressed. For instance, this\nparadigm shift requires a new form of compiler that considers the network\nconstraints in general as well as phenomena arising due to the nature of\nquantum communication. In distributed quantum computing, large circuits are\ndivided into smaller subcircuits such that they can be executed individually\nand simultaneously on multiple QPUs that are connected through quantum\nchannels. As quantum communication, for example, in the form of teleportation,\nis expensive, it must be used sparingly. We address the problem of assigning\nqubits to QPUs to minimize communication costs in two different ways. First by\napplying time-aware algorithms that take into account the changing connectivity\nof a given circuit as well as the underlying network topology. We define the\noptimization problem, use simulated annealing and an evolutionary algorithm and\ncompare the results to graph partitioning and sequential qubit assignment\nbaselines. In another approach, we propose an evolutionary-based quantum\ncircuit optimization algorithm that adjusts the circuit itself rather than the\nschedule to reduce the overall communication cost. We evaluate the techniques\nagainst random circuits and different network topologies. Both evolutionary\nalgorithms outperform the baseline in terms of communication cost reduction. We\ngive an outlook on how the approaches can be integrated into a compilation\nframework for distributed quantum computing.", "AI": {"tldr": "\u4e3a\u51cf\u5c11\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u6210\u672c\uff0c\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1. \u91c7\u7528\u65f6\u95f4\u611f\u77e5\u7b97\u6cd5\u4f18\u5316\u91cf\u5b50\u6bd4\u7279\u5206\u914d\uff1b2. \u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u91cf\u5b50\u7535\u8def\u672c\u8eab\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u3002", "motivation": "\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u901a\u8fc7\u5c06\u5927\u578b\u91cf\u5b50\u7535\u8def\u5206\u89e3\u4e3a\u53ef\u5728\u591a\u4e2a\u91cf\u5b50\u5904\u7406\u5355\u5143\uff08QPU\uff09\u4e0a\u5e76\u884c\u6267\u884c\u7684\u5b50\u7535\u8def\uff0c\u4e3a\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u9014\u5f84\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u65b0\u7684\u7f16\u8bd1\u5668\u6765\u5904\u7406\u7f51\u7edc\u7ea6\u675f\u548c\u91cf\u5b50\u901a\u4fe1\u7684\u72ec\u7279\u6027\u8d28\uff0c\u7279\u522b\u662f\u91cf\u5b50\u901a\u4fe1\uff08\u5982\u91cf\u5b50\u9690\u5f62\u4f20\u6001\uff09\u7684\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u5176\u4f7f\u7528\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u9000\u706b\u548c\u8fdb\u5316\u7b97\u6cd5\u6765\u89e3\u51b3\u91cf\u5b50\u6bd4\u7279\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u6700\u5c0f\u5316\u901a\u4fe1\u6210\u672c\uff0c\u5e76\u5c06\u5176\u4e0e\u56fe\u5212\u5206\u548c\u987a\u5e8f\u91cf\u5b50\u6bd4\u7279\u5206\u914d\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7684\u91cf\u5b50\u7535\u8def\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u7535\u8def\u672c\u8eab\u6765\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\uff08\u65f6\u95f4\u611f\u77e5\u7b97\u6cd5\u548c\u8fdb\u5316\u7b97\u6cd5\uff09\u5728\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u56fe\u5212\u5206\u548c\u987a\u5e8f\u91cf\u5b50\u6bd4\u7279\u5206\u914d\u65b9\u6cd5\u3002\u8fdb\u5316\u7b97\u6cd5\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u6280\u672f\u4ee5\u51cf\u5c11\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u6210\u672c\uff1a\u4e00\u79cd\u662f\u57fa\u4e8e\u65f6\u95f4\u611f\u77e5\u7684\u7b97\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u8fdb\u5316\u7684\u91cf\u5b50\u7535\u8def\u4f18\u5316\u7b97\u6cd5\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u901a\u8fc7\u4f18\u5316\u5b50\u7535\u8def\u7684\u5206\u914d\u548c\u8c03\u6574\u7535\u8def\u672c\u8eab\u6765\u6700\u5c0f\u5316\u91cf\u5b50\u901a\u4fe1\u7684\u5f00\u9500\u3002"}}
{"id": "2507.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11639", "abs": "https://arxiv.org/abs/2507.11639", "authors": ["Fouad Oubari", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Deep Generative Methods and Tire Architecture Design", "comment": null, "summary": "As deep generative models proliferate across the AI landscape, industrial\npractitioners still face critical yet unanswered questions about which deep\ngenerative models best suit complex manufacturing design tasks. This work\naddresses this question through a complete study of five representative models\n(Variational Autoencoder, Generative Adversarial Network, multimodal\nVariational Autoencoder, Denoising Diffusion Probabilistic Model, and\nMultinomial Diffusion Model) on industrial tire architecture generation. Our\nevaluation spans three key industrial scenarios: (i) unconditional generation\nof complete multi-component designs, (ii) component-conditioned generation\n(reconstructing architectures from partial observations), and (iii)\ndimension-constrained generation (creating designs that satisfy specific\ndimensional requirements). To enable discrete diffusion models to handle\nconditional scenarios, we introduce categorical inpainting, a mask-aware\nreverse diffusion process that preserves known labels without requiring\nadditional training. Our evaluation employs geometry-aware metrics specifically\ncalibrated for industrial requirements, quantifying spatial coherence,\ncomponent interaction, structural connectivity, and perceptual fidelity. Our\nfindings reveal that diffusion models achieve the strongest overall\nperformance; a masking-trained VAE nonetheless outperforms the multimodal\nvariant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics,\nand within the diffusion family MDM leads in-distribution whereas DDPM\ngeneralises better to out-of-distribution dimensional constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5de5\u4e1a\u8f6e\u80ce\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6269\u6563\u6a21\u578b\u6574\u4f53\u6027\u80fd\u6700\u4f18\uff0cMDM\u548cDDPM\u5404\u6709\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684\u201c\u5206\u7c7b\u586b\u5145\u201d\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u6761\u4ef6\u751f\u6210\u573a\u666f\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5de5\u4e1a\u754c\u5728\u9009\u62e9\u6700\u9002\u5408\u590d\u6742\u5236\u9020\u8bbe\u8ba1\u4efb\u52a1\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u65f6\u9762\u4e34\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5de5\u4e1a\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5206\u7c7b\u586b\u5145\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u63a9\u7801\u611f\u77e5\u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u53ef\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u5df2\u77e5\u6807\u7b7e\uff0c\u4ece\u800c\u4f7f\u79bb\u6563\u6269\u6563\u6a21\u578b\u80fd\u591f\u5904\u7406\u6761\u4ef6\u573a\u666f\u3002\u6211\u4eec\u91c7\u7528\u4e86\u4e13\u95e8\u9488\u5bf9\u5de5\u4e1a\u8981\u6c42\u6821\u51c6\u7684\u3001\u51e0\u4f55\u611f\u77e5\u7684\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u4e9b\u6307\u6807\u91cf\u5316\u4e86\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u7ec4\u4ef6\u4ea4\u4e92\u3001\u7ed3\u6784\u8fde\u901a\u6027\u548c\u611f\u77e5\u4fdd\u771f\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6269\u6563\u6a21\u578b\u5728\u5de5\u4e1a\u8f6e\u80ce\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u6574\u4f53\u6027\u80fd\u3002\u5176\u4e2d\uff0cMDM\u5728\u5206\u5e03\u5185\u751f\u6210\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cDDPM\u5728\u5904\u7406\u5206\u5e03\u5916\u7ef4\u5ea6\u7ea6\u675f\u65b9\u9762\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u6b64\u5916\uff0c\u63a9\u7801\u8bad\u7ec3\u7684VAE\u5728\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8eMMVAE\textsuperscript{+}\u3002", "conclusion": "\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5bf9\u4e94\u79cd\u4ee3\u8868\u6027\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3001\u591a\u6a21\u6001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u548c\u591a\u9879\u5f0f\u6269\u6563\u6a21\u578b\uff09\u5728\u5de5\u4e1a\u8f6e\u80ce\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u5177\u4f53\u800c\u8a00\uff0cMDM\u5728\u5206\u5e03\u5185\u751f\u6210\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u800cDDPM\u5728\u5904\u7406\u5206\u5e03\u5916\u7ef4\u5ea6\u7ea6\u675f\u65b9\u9762\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7ecf\u8fc7\u63a9\u7801\u8bad\u7ec3\u7684VAE\u5728\u51e0\u4e4e\u6240\u6709\u6761\u4ef6\u751f\u6210\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u591a\u6a21\u6001\u53d8\u5206\u81ea\u7f16\u7801\u5668MMVAE\textsuperscript{+}\u3002"}}
{"id": "2507.12085", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12085", "abs": "https://arxiv.org/abs/2507.12085", "authors": ["L. Plucinski", "G. Bihlmayer", "Y. Mokrousov", "Yishui Zhou", "Yixi Su", "A. Bostwick", "C. Jozwiak", "E. Rotenberg", "D. Usachov", "C. M. Schneider"], "title": "Single domain spectroscopic signatures of a magnetic Kagome metal", "comment": "7 pages, 4 figures", "summary": "Spin- and orbital-resolved access to the electronic bands is necessary to\nestablish key properties of quantum materials such as the quantum-geometric\ntensor. Despite recent revival on magnetic Kagome compounds, no spectroscopic\naccess to their magnetic properties has been available so far due to small\ndomain sizes and lack of appropriate techniques. Furthermore, their real space\nmagnetic texture is often complex and temperature-dependent. We investigate the\nmagnetic Kagome metal DyMn$_6$Sn$_6$ using high-resolution micro-focused\ncircular-dichroic angle-resolved photoemission ($\\mu$-CD-ARPES) to probe its\nmagnetic and electronic properties. By tuning the kinetic energy to various\nfeatures of the Dy $4f$ multiplet, we resolve magnetic domains in samples\ncryo-cooled down to 20 K. Smaller, but clear signatures are detected in the Mn\n$3p$ levels. The behavior of both Dy $4f$ and Mn $3p$ features are in\nremarkable agreement with our modeling based on the Hartree-Fock method,\nrevealing ferrimagnetic alignment of Dy and Mn local moments, and further\nstrengthening our interpretation. Adjusting the energy to the Mn $3d$-dominated\nvalence bands reveals signatures which we relate to the orbital magnetization\nthrough a comparison to {\\it ab initio} electronic structure calculations. Our\nstudy establishes the spectroscopic access to a single magnetic domain in a\nKagome metal, paving the way for further research into imaging magnetic phases\nof novel magnetic materials using $\\mu$-CD-ARPES.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u03bc-CD-ARPES\u6280\u672f\u6210\u529f\u83b7\u53d6\u4e86Kagome\u78c1\u6027\u91d1\u5c5eDyMn6Sn6\u7684\u5355\u78c1\u7574\u7535\u5b50\u7ed3\u6784\u548c\u78c1\u6027\u4fe1\u606f\uff0c\u63ed\u793a\u4e86Dy\u548cMn\u7684\u78c1\u77e9\u6392\u5217\u65b9\u5f0f\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u5176\u8f68\u9053\u78c1\u5316\u7684\u5149\u8c31\u5b66\u7814\u7a76\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u65b0\u578b\u78c1\u6027\u6750\u6599\u7684\u78c1\u76f8\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u83b7\u5f97\u91cf\u5b50\u6750\u6599\uff08\u5982\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\uff09\u7684\u5173\u952e\u6027\u8d28\uff0c\u9700\u8981\u80fd\u591f\u5206\u8fa8\u81ea\u65cb\u548c\u8f68\u9053\u7684\u7535\u5b50\u80fd\u5e26\u3002\u5c3d\u7ba1\u8fd1\u671f\u5bf9\u78c1\u6027Kagome\u5316\u5408\u7269\u7684\u7814\u7a76\u6709\u6240\u590d\u82cf\uff0c\u4f46\u7531\u4e8e\u7574\u5c3a\u5bf8\u5c0f\u548c\u7f3a\u4e4f\u5408\u9002\u7684\u6280\u672f\uff0c\u4e00\u76f4\u65e0\u6cd5\u5bf9\u5176\u78c1\u6027\u8fdb\u884c\u5149\u8c31\u5b66\u6d4b\u91cf\u3002\u6b64\u5916\uff0c\u5176\u771f\u5b9e\u7a7a\u95f4\u7684\u78c1\u6784\u578b\u590d\u6742\u4e14\u4f9d\u8d56\u4e8e\u6e29\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u5fae\u805a\u7126\u5706\u4e8c\u5411\u8272\u6027\u89d2\u5206\u8fa8\u5149\u7535\u5b50\u80fd\u8c31\uff08\u03bc-CD-ARPES\uff09\u6280\u672f\u3002", "result": "\u7814\u7a76\u6210\u529f\u5206\u8fa8\u4e86DyMn6Sn6\u7684\u78c1\u7574\uff0c\u5e76\u83b7\u5f97\u4e86Dy 4f\u548cMn 3p\u80fd\u7ea7\u7684\u78c1\u6027\u4fe1\u606f\u3002\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u57fa\u4e8eHartree-Fock\u65b9\u6cd5\u7684\u6a21\u578b\u8ba1\u7b97\u7ed3\u679c\u543b\u5408\uff0c\u63ed\u793a\u4e86Dy\u548cMn\u5c40\u57df\u78c1\u77e9\u7684 the ferrimagnetic alignment\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0eab initio\u7535\u5b50\u7ed3\u6784\u8ba1\u7b97\u7684\u6bd4\u8f83\uff0c\u7814\u7a76\u5c06Mn 3d\u4e3b\u5bfc\u7684\u4ef7\u5e26\u7684\u7279\u5f81\u4e0e\u8f68\u9053\u78c1\u5316\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u03bc-CD-ARPES\u6280\u672f\u5b9e\u73b0\u4e86\u5bf9Kagome\u78c1\u6027\u91d1\u5c5eDyMn6Sn6\u7684\u5355\u78c1\u7574\uff08\u5305\u62ecDy 4f\u548cMn 3p\u80fd\u7ea7\uff09\u7684\u5149\u8c31\u5b66\u7814\u7a76\uff0c\u5e76\u9996\u6b21\u83b7\u5f97\u4e86\u5176\u7535\u5b50\u5e26\u7684\u81ea\u65cb\u548c\u8f68\u9053\u5206\u8fa8\u4fe1\u606f\u3002"}}
{"id": "2507.12010", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12010", "abs": "https://arxiv.org/abs/2507.12010", "authors": ["Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Marc Miranda", "Lorenz Mohr", "Dieter Novotny", "Tom Kaufmann", "Christian Schneider", "Reiner Thom\u00e4"], "title": "Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set", "comment": null, "summary": "With the upcoming capabilities of integrated sensing and communication (ISAC)\nand the incorporation of user equipment (UE) like unmanned aerial vehicles\n(UAVs) in 6G mobile networks, there is a significant opportunity to enhance\nsituational awareness through multi-static radar sensing in meshed ISAC\nnetworks. This paper presents a real-world channel sounding data set acquired\nusing a testbed with synchronized, distributed ground-based sensor nodes and\nflying sensor nodes within a swarm of up to four drones. The conducted\nmeasurement campaign is designed to sense the bi-static reflectivity of objects\nsuch as parking cars, vertical take-off and landing (VTOL) aircraft, and small\ndrones in multi-path environments. We detail the rationale behind the selection\nof the included scenarios and the configuration of the participating nodesand\npresent exemplary results to demonstrate the potential of using collaborating\ndrone swarms for multi-static radar tracking and localization in air-to-air\n(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available\nto support the development and validation of future ISAC algorithms in\nreal-world environments rather than relying solely on simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e6G ISAC\u7f51\u7edc\u7684\u65e0\u4eba\u673a\u7fa4\u591a\u9759\u6001\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7269\u4f53\u611f\u77e5\u548c\u8ddf\u8e2a\uff0c\u652f\u6301\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e0b\u7684\u7b97\u6cd5\u5f00\u53d1\u3002", "motivation": "\u968f\u77406G\u79fb\u52a8\u7f51\u7edc\u4e2d\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u548c\u65e0\u4eba\u673a\uff08UAV\uff09\u7b49\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u80fd\u529b\u7684\u63d0\u5347\uff0c\u901a\u8fc7\u591a\u9759\u6001\u96f7\u8fbe\u4f20\u611f\u5728\u7f51\u72b6ISAC\u7f51\u7edc\u4e2d\u589e\u5f3a\u6001\u52bf\u611f\u77e5\u5177\u6709\u91cd\u5927\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5305\u542b\u591a\u8fbe\u56db\u67b6\u65e0\u4eba\u673a\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5229\u7528\u540c\u6b65\u7684\u5206\u5e03\u5f0f\u5730\u9762\u4f20\u611f\u5668\u8282\u70b9\u548c\u98de\u884c\u4f20\u611f\u5668\u8282\u70b9\uff0c\u91c7\u96c6\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u4fe1\u9053\u63a2\u6d4b\u6570\u636e\uff0c\u4ee5\u611f\u77e5\u591a\u5f84\u73af\u5883\u4e2d\u7684\u5404\u79cd\u7269\u4f53\uff08\u5982\u505c\u653e\u7684\u6c7d\u8f66\u3001\u5782\u76f4\u8d77\u964d\u98de\u884c\u5668\u548c\u5c0f\u578b\u65e0\u4eba\u673a\uff09\u7684\u53cc\u9759\u6001\u53cd\u5c04\u7387\u3002", "result": "\u5c55\u793a\u4e86\u5229\u7528\u534f\u540c\u65e0\u4eba\u673a\u7fa4\u5728\u7a7a\u5bf9\u7a7a\uff08A2A\uff09\u548c\u7a7a\u5bf9\u5730\uff08A2G\uff09\u573a\u666f\u4e2d\u8fdb\u884c\u591a\u9759\u6001\u96f7\u8fbe\u8ddf\u8e2a\u548c\u5b9a\u4f4d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765ISAC\u7b97\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u65e0\u4eba\u673a\u7fa4\u7684\u591a\u9759\u6001\u96f7\u8fbe\u4f20\u611f\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a86G\u7f516G\u7f51\u7edc\u4e2d\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u901a\u8fc7\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u63d0\u5347\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2507.12150", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12150", "abs": "https://arxiv.org/abs/2507.12150", "authors": ["Stefano Trivini", "Jon Ortuzar", "Katerina Vaxevani", "Beatriz Vi\u00f1a-Baus\u00e1", "F. Sebastian Bergeret", "Jose Ignacio Pascual"], "title": "Local control of parity and charge in nanoscale superconducting lead islands", "comment": null, "summary": "Small superconducting islands can exhibit charge quantization, where Coulomb\ninteractions compete with Cooper pairing. Using scanning tunneling\nspectroscopy, we probe this interplay by measuring the charging energy ($E_C$)\nand the pairing energy ($\\Delta$) of individual nano-islands. Below a critical\nisland size, where $E_C > \\Delta$, we observe a crossover between even and odd\nparity ground states. By applying controlled voltage pulses, we continuously\ntune the island's electrostatic potential and map the full charge-parity\nlandscape. These results demonstrate tunable superconducting ground states,\noffering a potential platform for qubit design and control.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5c0f\u8d85\u5bfc\u5c9b\u4e2d\u7684\u7535\u8377\u91cf\u5b50\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u901a\u8fc7\u7535\u538b\u8109\u51b2\u53ef\u4ee5\u8c03\u8c10\u5176\u8d85\u5bfc\u57fa\u6001\uff0c\u6709\u671b\u7528\u4e8e\u91cf\u5b50\u6bd4\u7279\u8bbe\u8ba1\u3002", "motivation": "\u63a2\u7d22\u5c0f\u8d85\u5bfc\u5c9b\u4e2d\u7535\u8377\u91cf\u5b50\u5316\u4e0e\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u548c\u5e93\u73c0\u5bf9\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u626b\u63cf\u96a7\u9053\u5149\u8c31\u6280\u672f\uff0c\u6d4b\u91cf\u4e86\u5355\u4e2a\u7eb3\u7c73\u5c9b\u7684\u5145\u7535\u80fd\u91cf\uff08EC\uff09\u548c\u914d\u5bf9\u80fd\u91cf\uff08\u0394\uff09\uff0c\u5e76\u901a\u8fc7\u65bd\u52a0\u53ef\u63a7\u7535\u538b\u8109\u51b2\u6765\u8c03\u8282\u5c9b\u7684\u9759\u7535\u52bf\uff0c\u7ed8\u5236\u4e86\u5b8c\u6574\u7684\u7535\u8377-\u5947\u5076\u6821\u9a8c\u56fe\u8c31\u3002", "result": "\u5728\u4e34\u754c\u5c9b\u5c3a\u5bf8\u4ee5\u4e0b\uff08EC > \u0394\uff09\uff0c\u89c2\u5bdf\u5230\u4e86\u57fa\u6001\u5728\u5076\u6570\u548c\u5947\u6570\u5947\u5076\u6821\u9a8c\u4e4b\u95f4\u7684\u4ea4\u53c9\u3002\u901a\u8fc7\u7535\u538b\u8109\u51b2\u53ef\u4ee5\u8fde\u7eed\u8c03\u8c10\u5c9b\u7684\u9759\u7535\u52bf\uff0c\u5e76\u7ed8\u5236\u51fa\u5b8c\u6574\u7684\u7535\u8377-\u5947\u5076\u6821\u9a8c\u56fe\u8c31\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u53ef\u8c03\u7684\u8d85\u5bfc\u57fa\u6001\uff0c\u4e3a\u91cf\u5b50\u6bd4\u7279\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.11988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11988", "abs": "https://arxiv.org/abs/2507.11988", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "comment": "14 pages, 1 figures,", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12082", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12082", "abs": "https://arxiv.org/abs/2507.12082", "authors": ["Theofilos Papadopoulos", "Antonios Antonopoulos"], "title": "Inductance Estimation for High-Power Multilayer Rectangle Planar Windings", "comment": "7 pages, 7 figures, IEEE Journal of Emerging and Selected Topics in\n  Industrial Electronics, 2025", "summary": "This paper proposes a simple and accurate monomial-like equation for\nestimating the inductance of Multilayer Rectangle-shaped Planar Windings\n(MLRPWs) for high-frequency, high-power applications. The equation consists of\nthe power product of the geometrical dimensions, raised at individual power\ncoefficients. The coefficients are generated via Multiple Linear Regression\n(MLR), based on a large set of approximately 6,000 simulated windings, with an\n80/20 training/evaluation sample ratio. The resulting mean error value is 0%,\nwith a standard deviation below 1.8%. The accuracy of the inductance estimation\nis confirmed on several experimental samples, with dimensions both within and\noutside the initial training dataset.", "AI": {"tldr": "A new equation estimates inductance for planar windings with high accuracy, validated by simulations and experiments.", "motivation": "To develop a simple and accurate equation for estimating the inductance of Multilayer Rectangle-shaped Planar Windings (MLRPWs) for high-frequency, high-power applications.", "method": "Multiple Linear Regression (MLR) was used to generate coefficients for a monomial-like equation based on approximately 6,000 simulated windings, using an 80/20 training/evaluation split.", "result": "The equation achieved a mean error of 0% and a standard deviation below 1.8%, with accuracy confirmed on experimental samples both within and outside the initial training dataset.", "conclusion": "The proposed monomial-like equation provides a simple and accurate method for estimating the inductance of MLRPWs for high-frequency, high-power applications, with low mean error and standard deviation, confirmed by experimental validation."}}
{"id": "2507.11978", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11978", "abs": "https://arxiv.org/abs/2507.11978", "authors": ["Jiacheng Huang", "Zimin Li", "Yinghui Li", "Haojie Wang"], "title": "NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning", "comment": null, "summary": "The emergence of deep learning domain-specific languages (DSLs) has\nsubstantially reduced the obstacles in developing high-performance,\ncross-platform compute kernels. However, current DSLs, such as Triton, still\ndemand that developers possess expertise in parallel programming and expose\nthem to many low-level details. This requirement complicates the development\nprocess and adds to the difficulty of maintaining compute kernels.\nConsequently, developing a new programming model that supports serial\nprogramming for deep learning workloads is crucial.\n  This paper introduces NineToothed, a domain-specific language that offers\nserial semantics for machine learning programming. Through the automatic\ntransformation of serial code into parallel code, NineToothed significantly\nstreamlines the development process while causing minimal performance\ndegradation. NineToothed encompasses (1) a language with tensor-oriented\nmetaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the\nexpression of tiled computations without the need to manage low-level details\nand (2) a code generator for generating high-performance parallel code. Our\nevaluation results indicate that NineToothed can greatly simplify compute\nkernel development while maintaining performance comparable to that of Triton.", "AI": {"tldr": "NineToothed\u662f\u4e00\u4e2a\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u5b83\u5141\u8bb8\u5f00\u53d1\u8005\u4f7f\u7528\u4e32\u884c\u7f16\u7a0b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5c06\u4e32\u884c\u4ee3\u7801\u8f6c\u6362\u4e3a\u5e76\u884c\u4ee3\u7801\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eTriton\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u5982Triton\uff0c\u8981\u6c42\u5f00\u53d1\u8005\u5177\u5907\u5e76\u884c\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u5e76\u66b4\u9732\u8bb8\u591a\u5e95\u5c42\u7ec6\u8282\uff0c\u8fd9\u589e\u52a0\u4e86\u5f00\u53d1\u548c\u7ef4\u62a4\u7684\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u652f\u6301\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e32\u884c\u7f16\u7a0b\u7684\u65b0\u7f16\u7a0b\u6a21\u578b\u3002", "method": "NineToothed\u901a\u8fc7\uff081\uff09\u4e00\u79cd\u5177\u6709\u5f20\u91cf\u5bfc\u5411\u5143\u7f16\u7a0b\uff08TOM\uff09\u7684\u8bed\u8a00\uff0c\u91c7\u7528\u6392\u5217\u548c\u5e94\u7528\u8303\u5f0f\uff0c\u652f\u6301\u8868\u8fbe\u5e73\u94fa\u8ba1\u7b97\u800c\u65e0\u9700\u7ba1\u7406\u4f4e\u7ea7\u7ec6\u8282\uff0c\u4ee5\u53ca\uff082\uff09\u4e00\u4e2a\u751f\u6210\u9ad8\u6027\u80fd\u5e76\u884c\u4ee3\u7801\u7684\u4ee3\u7801\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4e86\u5e8f\u5217\u4ee3\u7801\u5230\u5e76\u884c\u4ee3\u7801\u7684\u81ea\u52a8\u8f6c\u6362\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cNineToothed\u80fd\u591f\u6781\u5927\u5730\u7b80\u5316\u8ba1\u7b97\u5185\u6838\u7684\u5f00\u53d1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eTriton\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "NineToothed\u901a\u8fc7\u5176\u7279\u5b9a\u9886\u57df\u8bed\u8a00\uff08DSL\uff09\u548c\u4ee3\u7801\u751f\u6210\u5668\uff0c\u80fd\u591f\u663e\u8457\u7b80\u5316\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u5185\u6838\u7684\u5f00\u53d1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eTriton\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11889", "abs": "https://arxiv.org/abs/2507.11889", "authors": ["Adnan Abdullah", "Alankrit Gupta", "Vaishnav Ramesh", "Shivali Patel", "Md Jahidul Islam"], "title": "NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy", "comment": "10 pages, V1", "summary": "Adaptive mission control and dynamic parameter reconfiguration are essential\nfor autonomous underwater vehicles (AUVs) operating in GPS-denied,\ncommunication-limited marine environments. However, most current AUV platforms\nexecute static, pre-programmed missions or rely on tethered connections and\nhigh-latency acoustic channels for mid-mission updates, significantly limiting\ntheir adaptability and responsiveness. In this paper, we introduce NemeSys, a\nnovel AUV system designed to support real-time mission reconfiguration through\ncompact optical and magnetoelectric (OME) signaling facilitated by floating\nbuoys. We present the full system design, control architecture, and a semantic\nmission encoding framework that enables interactive exploration and task\nadaptation via low-bandwidth communication. The proposed system is validated\nthrough analytical modeling, controlled experimental evaluations, and\nopen-water trials. Results confirm the feasibility of online mission adaptation\nand semantic task updates, highlighting NemeSys as an online AUV platform for\ngoal-driven adaptive autonomy in dynamic and uncertain underwater environments.", "AI": {"tldr": "NemeSys AUV\u7cfb\u7edf\u901a\u8fc7\u6d6e\u6807\u5229\u7528\u5149\u5b66\u548c\u78c1\u7535\u4fe1\u53f7\u5b9e\u73b0\u4f4e\u5e26\u5bbd\u901a\u4fe1\uff0c\u53ef\u5728\u6c34\u4e0b\u5b9e\u65f6\u91cd\u6784\u4efb\u52a1\uff0c\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAUV\u5e73\u53f0\u5728GPS\u62d2\u6b62\u3001\u901a\u4fe1\u53d7\u9650\u7684\u6d77 marine\u73af\u5883\u4e2d\uff0c\u5927\u591a\u6267\u884c\u9759\u6001\u3001\u9884\u7f16\u7a0b\u4efb\u52a1\uff0c\u6216\u4f9d\u8d56\u7cfb\u7ef3\u8fde\u63a5\u548c\u9ad8\u5ef6\u8fdf\u58f0\u5b66\u4fe1\u9053\u8fdb\u884c\u4e2d\u9014\u4efb\u52a1\u66f4\u65b0\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9002\u5e94\u6027\u548c\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNemeSys\u7684\u65b0\u578bAUV\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u6d6e\u6807\u8fdb\u884c\u7d27\u51d1\u7684\u5149\u5b66\u548c\u78c1\u7535\uff08OME\uff09\u4fe1\u53f7\u4f20\u8f93\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4efb\u52a1\u91cd\u6784\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u5b8c\u6574\u7684\u7cfb\u7edf\u8bbe\u8ba1\u3001\u63a7\u5236\u67b6\u6784\u548c\u8bed\u4e49\u4efb\u52a1\u7f16\u7801\u6846\u67b6\uff0c\u652f\u6301\u901a\u8fc7\u4f4e\u5e26\u5bbd\u901a\u4fe1\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u4efb\u52a1\u9002\u5e94\u3002", "result": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u3001\u53d7\u63a7\u5b9e\u9a8c\u8bc4\u4f30\u548c\u5f00\u9614\u6c34\u57df\u8bd5\u9a8c\u9a8c\u8bc1\u4e86NemeSys\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5728\u7ebf\u4efb\u52a1\u9002\u5e94\u548c\u8bed\u4e49\u4efb\u52a1\u66f4\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "NemeSys\u662f\u4e00\u4e2a\u652f\u6301\u901a\u8fc7\u6d6e\u6807\u8fdb\u884c\u5b9e\u65f6\u4efb\u52a1\u91cd\u6784\u7684AUV\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u5149\u5b66\u548c\u78c1\u7535\uff08OME\uff09\u4fe1\u53f7\u4f20\u8f93\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5e26\u5bbd\u901a\u4fe1\u4e0b\u7684\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u9a8c\u8bc1\u4e86\u5728\u7ebfAUV\u5e73\u53f0\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6c34\u4e0b\u73af\u5883\u4e2d\u5b9e\u73b0\u76ee\u6807\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u81ea\u4e3b\u6027\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.11809", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11809", "abs": "https://arxiv.org/abs/2507.11809", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u53ef\u590d\u73b0\u6027\u7814\u7a76\uff0c\u5229\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5904\u7406\u5305\u542b\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6ce8\u610f\u529b\u5934\u901a\u8fc7\u901a\u7528\u590d\u5236\u6291\u5236\u800c\u975e\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u6765\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\uff0c\u4e14\u8fd9\u79cd\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff0c\u66f4\u5927\u6a21\u578b\u7684\u6a21\u5f0f\u66f4\u4e13\u4e1a\u5316\u548c\u7c7b\u522b\u654f\u611f\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9762\u5bf9\u76f8\u4e92\u7ade\u4e89\u7684\u4e8b\u5b9e\u4fe1\u606f\u548c\u53cd\u4e8b\u5b9e\u4fe1\u606f\u65f6\u5982\u4f55\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u7740\u91cd\u7814\u7a76\u6ce8\u610f\u529b\u5934\u5728\u6b64\u8fc7\u7a0b\u4e2d\u626e\u6f14\u7684\u89d2\u8272\u3002\u7814\u7a76\u8bd5\u56fe\u91cd\u73b0\u5e76\u89e3\u91ca\u4e09\u9879\u8fd1\u671f\u7814\u7a76\uff08Ortu et al., Yu, Merullo, and Pavlick\u4ee5\u53caMcDougall et al.\uff09\u5173\u4e8e\u6a21\u578b\u5b66\u4e60\u7684\u4e8b\u5b9e\u4e0e\u77db\u76fe\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7ade\u4e89\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u8fd9\u4e9b\u7814\u7a76\u5229\u7528\u4e86\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u53ef\u590d\u73b0\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u6765\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u7ade\u4e89\u6027\u4e8b\u5b9e\u4fe1\u606f\u548c\u53cd\u4e8b\u5b9e\u4fe1\u606f\u7684\u80fd\u529b\u3002\u7814\u7a76\u91cd\u70b9\u5173\u6ce8\u6ce8\u610f\u529b\u5934\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u8bd5\u56fe\u91cd\u73b0\u548c\u8c03\u548c\u4e09\u9879\u8fd1\u671f\u7814\u7a76\u7684\u53d1\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u672c\u7814\u7a76\u68c0\u67e5\u4e86\u6ce8\u610f\u529b\u5934\u5f3a\u5ea6\u4e0e\u4e8b\u5b9e\u8f93\u51fa\u6bd4\u4f8b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc4\u4f30\u4e86\u5173\u4e8e\u6ce8\u610f\u529b\u5934\u6291\u5236\u673a\u5236\u7684\u7ade\u4e89\u6027\u5047\u8bbe\uff0c\u5e76\u8c03\u67e5\u4e86\u8fd9\u4e9b\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u9886\u57df\u7279\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\u7684\u6ce8\u610f\u529b\u5934\u901a\u8fc7\u901a\u7528\u7684\u590d\u5236\u6291\u5236\u673a\u5236\uff08\u800c\u975e\u9009\u62e9\u6027\u7684\u53cd\u4e8b\u5b9e\u6291\u5236\uff09\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u56e0\u4e3a\u589e\u5f3a\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u4e5f\u4f1a\u6291\u5236\u6b63\u786e\u4e8b\u5b9e\u7684\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u662f\u9886\u57df\u4f9d\u8d56\u7684\uff0c\u66f4\u5927\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u4e13\u4e1a\u5316\u548c\u66f4\u5177\u7c7b\u522b\u654f\u611f\u6027\u7684\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5904\u7406\u7ade\u4e89\u6027\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u4fe1\u606f\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u901a\u7528\u7684\u590d\u5236\u6291\u5236\u673a\u5236\u800c\u975e\u9009\u62e9\u6027\u7684\u53cd\u4e8b\u5b9e\u6291\u5236\u6765\u4fc3\u8fdb\u4e8b\u5b9e\u7684\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u6ce8\u610f\u529b\u5934\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff0c\u66f4\u5927\u6a21\u578b\u7684\u6a21\u5f0f\u66f4\u52a0\u4e13\u4e1a\u5316\u548c\u7c7b\u522b\u654f\u611f\u3002"}}
{"id": "2507.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11579", "abs": "https://arxiv.org/abs/2507.11579", "authors": ["Sathvik Chereddy", "John Femiani"], "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation", "comment": "17 pages, 63 figures, Proceedings of the 42nd International\n  Conference on Machine Learning (ICML2025)", "summary": "We present SketchDNN, a generative model for synthesizing CAD sketches that\njointly models both continuous parameters and discrete class labels through a\nunified continuous-discrete diffusion process. Our core innovation is\nGaussian-Softmax diffusion, where logits perturbed with Gaussian noise are\nprojected onto the probability simplex via a softmax transformation,\nfacilitating blended class labels for discrete variables. This formulation\naddresses 2 key challenges, namely, the heterogeneity of primitive\nparameterizations and the permutation invariance of primitives in CAD sketches.\nOur approach significantly improves generation quality, reducing Fr\\'echet\nInception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)\nfrom 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch\ngeneration on the SketchGraphs dataset.", "AI": {"tldr": "SketchDNN\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u65af-softmax\u6269\u6563\u8054\u5408\u5904\u7406\u8fde\u7eed\u548c\u79bb\u6563\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86CAD\u8349\u56fe\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u6392\u5217\u4e0d\u53d8\u6027\u95ee\u9898\uff0c\u5e76\u5728SketchGraphs\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3CAD\u8349\u56fe\u751f\u6210\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u539f\u59cb\u53c2\u6570\u5316\u7684\u5f02\u8d28\u6027\u548c\u539f\u59cb\u7684\u6392\u5217\u4e0d\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9ad8\u65af-softmax\u6269\u6563\u7684\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u8fde\u7eed-\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\u8054\u5408\u5efa\u6a21\u8fde\u7eed\u53c2\u6570\u548c\u79bb\u6563\u7c7b\u6807\u7b7e\u3002", "result": "FID\u4ece16.04\u964d\u4f4e\u52307.80\uff0cNLL\u4ece84.8\u964d\u4f4e\u523081.33\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SketchDNN\u5728SketchGraphs\u6570\u636e\u96c6\u4e0a\u5728CAD\u8349\u56fe\u751f\u6210\u65b9\u9762\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002"}}
{"id": "2507.11712", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.11712", "abs": "https://arxiv.org/abs/2507.11712", "authors": ["Brett Min", "Matthew Gerry", "Dvira Segal"], "title": "Separation of relaxation timescales via strong system-bath coupling: Dissipative three-level system as a case study", "comment": "15 pages, 8 figures", "summary": "We analytically demonstrate that strong system-bath coupling separates the\nrelaxation dynamics of a dissipative quantum system into two distinct regimes:\na short-time dynamics that, as expected, accelerates with increasing coupling\nto the environment, and a slow dynamics that, counterintuitively, becomes\nincreasingly prolonged at sufficiently strong coupling. Using the\nreaction-coordinate polaron-transform mapping, we uncover the general mechanism\nbehind this effect and derive accurate expressions for both relaxation\ntimescales. Numerical simulations confirm our analytical predictions. From a\npractical perspective, our results suggest that strong coupling to a\ndissipative bath can autonomously generate and sustain long-lived quantum\ncoherences, offering a promising strategy for bath-engineered quantum state\npreparation.", "AI": {"tldr": "\u5f3a\u7cfb\u7edf-\u6d74\u8026\u5408\u5bf9\u91cf\u5b50\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\u4ea7\u751f\u5f71\u54cd\uff0c\u5bfc\u81f4\u77ed\u671f\u52a8\u529b\u5b66\u52a0\u901f\uff0c\u800c\u957f\u671f\u52a8\u529b\u5b66\u5219\u5ef6\u957f\u3002", "motivation": "\u5f3a\u7cfb\u7edf-\u6d74\u8026\u5408\u5c06\u8017\u6563\u91cf\u5b50\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\u5206\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u72b6\u6001\uff1a\u77ed\u671f\u52a8\u529b\u5b66\uff0c\u5982\u9884\u671f\u7684\u90a3\u6837\uff0c\u968f\u7740\u4e0e\u73af\u5883\u7684\u8026\u5408\u589e\u52a0\u800c\u52a0\u901f\uff1b\u4ee5\u53ca\u7f13\u6162\u52a8\u529b\u5b66\uff0c\u4e0e\u76f4\u89c9\u76f8\u53cd\uff0c\u5728\u8db3\u591f\u5f3a\u7684\u8026\u5408\u4e0b\u4f1a\u8d8a\u6765\u8d8a\u5ef6\u957f\u3002", "method": "\u4f7f\u7528\u53cd\u5e94\u5750\u6807\u6781\u5316\u5b50\u53d8\u6362\u6620\u5c04\uff0c\u63ed\u793a\u4e86\u8be5\u6548\u5e94\u7684\u666e\u904d\u673a\u5236\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e24\u79cd\u5f1b\u8c6b\u65f6\u95f4\u5c3a\u5ea6\u7684\u7cbe\u786e\u8868\u8fbe\u5f0f\u3002\u6570\u503c\u6a21\u62df\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u5206\u6790\u9884\u6d4b\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5f3a\u7cfb\u7edf-\u6d74\u8026\u5408\u5c06\u8017\u6563\u91cf\u5b50\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\u5206\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u72b6\u6001\uff1a\u77ed\u671f\u52a8\u529b\u5b66\uff0c\u5982\u9884\u671f\u7684\u90a3\u6837\uff0c\u968f\u7740\u4e0e\u73af\u5883\u7684\u8026\u5408\u589e\u52a0\u800c\u52a0\u901f\uff1b\u4ee5\u53ca\u7f13\u6162\u52a8\u529b\u5b66\uff0c\u4e0e\u76f4\u89c9\u76f8\u53cd\uff0c\u5728\u8db3\u591f\u5f3a\u7684\u8026\u5408\u4e0b\u4f1a\u8d8a\u6765\u8d8a\u5ef6\u957f\u3002", "conclusion": "\u5f3a\u8026\u5408\u5230\u8017\u6563\u6d74\u53ef\u4ee5\u81ea\u53d1\u5730\u4ea7\u751f\u548c\u7ef4\u6301\u957f\u5bff\u547d\u7684\u91cf\u5b50\u76f8\u5e72\u6027\uff0c\u4e3a\u901a\u8fc7\u6d74\u5de5\u7a0b\u5236\u5907\u91cf\u5b50\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u7b56\u7565\u3002"}}
{"id": "2507.11645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11645", "abs": "https://arxiv.org/abs/2507.11645", "authors": ["Ahmed Salah", "David Yevick"], "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation", "comment": "15 pages, 11 figures", "summary": "Grokking refers to delayed generalization in which the increase in test\naccuracy of a neural network occurs appreciably after the improvement in\ntraining accuracy This paper introduces several practical metrics including\nvariance under dropout, robustness, embedding similarity, and sparsity\nmeasures, that can forecast grokking behavior. Specifically, the resilience of\nneural networks to noise during inference is estimated from a Dropout\nRobustness Curve (DRC) obtained from the variation of the accuracy with the\ndropout rate as the model transitions from memorization to generalization. The\nvariance of the test accuracy under stochastic dropout across training\ncheckpoints further exhibits a local maximum during the grokking. Additionally,\nthe percentage of inactive neurons decreases during generalization, while the\nembeddings tend to a bimodal distribution independent of initialization that\ncorrelates with the observed cosine similarity patterns and dataset symmetries.\nThese metrics additionally provide valuable insight into the origin and\nbehaviour of grokking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u53ef\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc grokking \u73b0\u8c61\uff08\u5ef6\u8fdf\u6cdb\u5316\uff09\u7684\u6307\u6807\uff0c\u5305\u62ec dropout \u9c81\u68d2\u6027\u66f2\u7ebf\u3001dropout \u4e0b\u7684\u51c6\u786e\u6027\u65b9\u5dee\u3001\u4e0d\u6d3b\u8dc3\u795e\u7ecf\u5143\u7684\u767e\u5206\u6bd4\u4ee5\u53ca\u5d4c\u5165\u7684\u5206\u5e03\u548c\u76f8\u4f3c\u6027\u3002\u8fd9\u4e9b\u6307\u6807\u6709\u52a9\u4e8e\u7406\u89e3 grokking \u7684\u8d77\u6e90\u548c\u884c\u4e3a\u3002", "motivation": "Grokking \u662f\u4e00\u79cd\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\uff0c\u5176\u4e2d\u795e\u7ecf\u7f51\u7edc\u7684\u6d4b\u8bd5\u51c6\u786e\u6027\u5728\u8bad\u7ec3\u51c6\u786e\u6027\u63d0\u9ad8\u540e\u624d\u660e\u663e\u63d0\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u53ef\u9884\u6d4b grokking \u884c\u4e3a\u7684\u5b9e\u7528\u6307\u6807\u3002", "method": "\u901a\u8fc7 Dropout \u9c81\u68d2\u6027\u66f2\u7ebf (DRC) \u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u566a\u58f0\u7684\u97e7\u6027\uff0c\u8be5\u66f2\u7ebf\u662f\u901a\u8fc7\u5728\u6a21\u578b\u4ece\u8bb0\u5fc6\u5316\u5230\u6cdb\u5316\u7684\u8fc7\u7a0b\u4e2d\uff0c\u51c6\u786e\u6027\u968f dropout \u7387\u53d8\u5316\u7684\u5f97\u5230\u7684\u3002\u6b64\u5916\uff0c\u5728\u8bad\u7ec3\u68c0\u67e5\u70b9\u4e2d\uff0c\u6d4b\u8bd5\u51c6\u786e\u6027\u5728\u968f\u673a dropout \u4e0b\u7684\u65b9\u5dee\u5728 grokking \u671f\u95f4\u8868\u73b0\u51fa\u5c40\u90e8\u6700\u5927\u503c\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e0d\u6d3b\u8dc3\u795e\u7ecf\u5143\u7684\u767e\u5206\u6bd4\u5728\u6cdb\u5316\u671f\u95f4\u4f1a\u51cf\u5c11\uff0c\u800c\u5d4c\u5165\u5219\u8d8b\u5411\u4e8e\u53cc\u5cf0\u5206\u5e03\uff0c\u4e14\u4e0e\u89c2\u5bdf\u5230\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u6a21\u5f0f\u548c\u6570\u636e\u96c6\u5bf9\u79f0\u6027\u76f8\u5173\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u7684\u6307\u6807\uff08dropout \u4e0b\u7684\u65b9\u5dee\u3001\u9c81\u68d2\u6027\u3001\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u7a00\u758f\u6027\u5ea6\u91cf\uff09\u80fd\u591f\u9884\u6d4b grokking \u884c\u4e3a\u3002\u5177\u4f53\u6765\u8bf4\uff0cDRC \u66f2\u7ebf\u3001\u6d4b\u8bd5\u51c6\u786e\u6027\u5728\u968f\u673a dropout \u4e0b\u7684\u65b9\u5dee\u3001\u4e0d\u6d3b\u8dc3\u795e\u7ecf\u5143\u7684\u767e\u5206\u6bd4\u4ee5\u53ca\u5d4c\u5165\u7684\u53cc\u5cf0\u5206\u5e03\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u6a21\u5f0f\u90fd\u4e0e grokking \u73b0\u8c61\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5b9e\u7528\u7684\u6307\u6807\uff0c\u5982 dropout \u4e0b\u7684\u65b9\u5dee\u3001\u9c81\u68d2\u6027\u3001\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u7a00\u758f\u6027\u5ea6\u91cf\uff0c\u53ef\u4ee5\u9884\u6d4b grokking \u884c\u4e3a\u3002\u8fd9\u4e9b\u6307\u6807\u4e3a\u7406\u89e3 grokking \u7684\u8d77\u6e90\u548c\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.12113", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12113", "abs": "https://arxiv.org/abs/2507.12113", "authors": ["Christian S. Kern", "Xiaosheng Yang", "Giovanni Zamborlini", "Simone Mearini", "Matteo Jugovac", "Vitaliy Feyer", "Umberto De Giovannini", "Angel Rubio", "Serguei Soubatch", "Michael G. Ramsey", "F. Stefan Tautz", "Peter Puschnig"], "title": "Circular dichroism in the photoelectron angular distribution of achiral molecules", "comment": null, "summary": "Circular dichroism in the angular distribution (CDAD) is the effect that the\nangular intensity distribution of photoemitted electrons depends on the\nhandedness of the incident circularly polarized light. A CDAD may arise from\nintrinsic material properties like chirality, spin-orbit interaction, or\nquantum-geometrical effects on the electronic structure. In addition, CDAD has\nalso been reported for achiral organic molecules at the interface to metallic\nsubstrates. For this latter case, we investigate two prototypical\n$\\pi$-conjugated molecules, namely tetracene and pentacene, whose frontier\norbitals have a similar shape but exhibit distinctly different symmetries. By\ncomparing experimental CDAD momentum maps with simulations within\ntime-dependent density functional theory, we show how the final state of the\nphotoelectron must be regarded as the source of the CDAD in such otherwise\nachiral systems. We gain additional insight into the mechanism by employing a\nsimple scattering model for the final state, which allows us to decompose the\nCDAD signal into partial wave contributions.", "AI": {"tldr": "CDAD\u5728\u975e\u624b\u6027\u4f53\u7cfb\u4e2d\u6e90\u4e8e\u5149\u7535\u5b50\u7ec8\u6001\uff0c\u53ef\u901a\u8fc7\u6563\u5c04\u6a21\u578b\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u975e\u624b\u6027\u6709\u673a\u5206\u5b50\u5728\u91d1\u5c5e\u886c\u5e95\u754c\u9762\u5904CDAD\u7684\u4ea7\u751f\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8cCDAD\u52a8\u91cf\u56fe\u4e0e\u4f9d\u8d56\u65f6\u7a7a\u8c03\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7684\u6a21\u62df\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u91c7\u7528\u7b80\u5355\u7684\u6563\u5c04\u6a21\u578b\u6765\u5206\u6790\u7ec8\u6001\u3002", "result": "\u5c55\u793a\u4e86\u975e\u624b\u6027\u4f53\u7cfb\u5149\u7535\u5b50\u7684\u7ec8\u6001\u662fCDAD\u7684\u6765\u6e90\uff0c\u5e76\u80fd\u5206\u89e3CDAD\u4fe1\u53f7\u4e3a\u90e8\u5206\u6ce2\u7684\u8d21\u732e\u3002", "conclusion": "CDAD\u4fe1\u53f7\u6e90\u4e8e\u975e\u624b\u6027\u4f53\u7cfb\u5149\u7535\u5b50\u7684\u7ec8\u6001\uff0c\u5176CDAD\u4fe1\u53f7\u53ef\u5206\u89e3\u4e3a\u90e8\u5206\u6ce2\u7684\u8d21\u732e\u3002"}}
{"id": "2507.12132", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12132", "abs": "https://arxiv.org/abs/2507.12132", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "comment": null, "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e NeRF \u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528 Wi-Fi CSI \u6765\u521b\u5efa\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\uff0c\u4ee5\u63d0\u9ad8\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u57fa\u4e8e Wi-Fi CSI \u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u65b9\u6cd5\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4ece\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u4e2d\u91cd\u5efa\u4e09\u7ef4\u8fd0\u52a8\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\uff0c\u4ee5\u63d0\u4f9b\u6d3b\u52a8\u7684\u5168\u8c8c\u5e76\u63d0\u9ad8\u5bf9\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86 Wi-Fi HAR \u7684\u6cdb\u5316\u51c6\u786e\u6027\u3002", "conclusion": "Wi-Fi CSI \u7684\u65b0\u65b9\u6cd5 DoRFs \u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e Wi-Fi \u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u4f20\u611f\u5e94\u7528\u5c55\u793a\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12249", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12249", "abs": "https://arxiv.org/abs/2507.12249", "authors": ["Agnieszka Mi\u0119tkiewicz", "Jakub Ziembicki", "Krzysztof Gawarecki"], "title": "Light-hole states and hyperfine interaction in electrically-defined Ge/GeSn quantum dots", "comment": null, "summary": "We theoretically investigate hole spins confined in a gate-defined quantum\ndot (QD) embedded in GeSn/Ge/GeSn quantum well (QW) structure. Owing to the\ntensile strain in the Ge layer, the system effectively realizes a light-hole\nqubit. We systematically study how various morphological parameters influence\nthe energy spectrum, g-factors, and the hyperfine coupling to the nuclear spin\nbath. The simulations are carried out using a realistic, fully atomic\nsp$^3$d$^5$s$^*$ tight-binding model. We also perform complementary DFT\ncalculations of wave functions near the atomic cores and use them to\nparameterize the hyperfine-interaction Hamiltonian. We evaluate the Overhauser\nfield fluctuations and demonstrate that the strength of the hyperfine coupling\nfor the lowest hole doublet crucially depends on the Sn content in the barrier.\nWe highlight the conduction-valence band mixing, which leads to considerable\n$s$-type admixtures to the hole states, providing the dominant channel of\nhyperfine coupling due to the Fermi contact interaction.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u6d41\u4fe1\u606f\uff0c\u6210\u529f\u8bad\u7ec3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\uff0c\u5176\u65b9\u6cd5\u548c\u7ed3\u679c\u7c7b\u4f3c\u4e8e\u98de\u884c\u6606\u866b\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u65e0\u4eba\u673a\u63a7\u5236\u5f8b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "motivation": "\u7531\u4e8e\u751f\u7269\u7cfb\u7edf\u5728\u98de\u884c\u548c\u907f\u969c\u65b9\u9762\u5177\u6709\u51fa\u8272\u7684\u80fd\u529b\uff0c\u5c3d\u7ba1\u5176\u611f\u77e5\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u56e0\u6b64\u5728\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u4e86\u53d7\u751f\u7269\u542f\u53d1\u7684 \u8bbe\u8ba1\u3002\u7279\u522b\u662f\uff0c\u871c\u8702\u4e3b\u8981\u5229\u7528\u5176\u89c6\u89c9\u573a\u4e2d\u7269\u4f53\u7684\u89c6\u6d41\uff08\u89c6\u8fd0\u52a8\uff09\u6765\u5bfc\u822a\u62e5\u6324\u7684\u73af\u5883\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u89c6\u6d41\u4f5c\u4e3a\u4f20\u611f\u8f93\u5165\uff0c\u8bad\u7ec3\u65e0\u4eba\u673a\u5728\u5145\u6ee1\u969c\u788d\u7269\u7684\u96a7\u9053\u4e2d\u8fdb\u884c\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u6240\u5f97\u667a\u80fd\u4f53\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u63a2\u7a76\u5176\u5bf9\u89c6\u6d41\u7684\u54ea\u4e9b\u533a\u57df\u8fdb\u884c\u91cd\u70b9\u5173\u6ce8\u4ee5\u505a\u51fa\u8fd0\u52a8\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u667a\u80fd\u4f53\u4e3b\u8981\u5173\u6ce8\u89c6\u6d41\u4e0d\u8fde\u7eed\u548c\u89c6\u6d41\u5927\u7684\u533a\u57df\u3002\u8fd9\u4e9b\u8bad\u7ec3\u597d\u7684\u667a\u80fd\u4f53\u901a\u8fc7\u907f\u5f00\u4ea7\u751f\u5927\u89c6\u6d41\u7684\u969c\u788d\u7269\u5e76\u5728\u73af\u5883\u4e2d\u4fdd\u6301\u5c45\u4e2d\u4f4d\u7f6e\u6765\u5bfc\u822a\uff0c\u8fd9\u4e0e\u98de\u884c\u6606\u866b\u7684\u884c\u4e3a\u76f8\u4f3c\u3002\u8fd9\u79cd\u6a21\u5f0f\u5728\u72ec\u7acb\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u8868\u660e\u8fd9\u53ef\u80fd\u662f\u4e00\u79cd\u5f00\u53d1\u7269\u7406\u65e0\u4eba\u673a\u7684\u7b80\u5355\u663e\u5f0f\u63a7\u5236\u5f8b\u7684\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u89c6\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8bad\u7ec3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53ef\u4e3a\u5f00\u53d1\u7b80\u5355\u7684\u663e\u5f0f\u63a7\u5236\u5f8b\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.12163", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12163", "abs": "https://arxiv.org/abs/2507.12163", "authors": ["Pramit Karmakar", "Siddharth B", "Chinmay Murlidhar Kadnur Rao"], "title": "Integrated Switched Capacitor Array and Synchronous Charge Extraction with Adaptive Hybrid MPPT for Piezoelectric Harvesters", "comment": null, "summary": "Energy Harvesting technologies will play a fundamental role in the\ndevelopment of the next generation of electronic systems as well as in\nadvancing the development of sustainable infrastructure. One of the critical\nchallenges in EH is utilizing ambient vibrations to harvest energy. Piezo\nEnergy Harvesting, which uses ambient vibrations, is a promising technology in\nenergy harvesting and a self-powered technology. However, it suffers from\nseveral practical challenges. Some of these challenges include narrow\nbandwidth, non-linearity, and impedance mismatch, among others. This paper\npresents a novel, simulated Piezo Energy Harvesting (PEH) framework that\naddresses some of these challenges. The proposed model is designed to be\nadaptive and effective against the inherent non-linearity of PEH. This detailed\nmodel covers a non-linear piezo, Synchronous Electric Charge Extraction (SECE),\nHybrid Maximum Power Point Tracking (MPPT) and a Switched Capacitor Array\n(SCA). The SECE extracts the maximum charge accumulated on the piezo every time\nthe piezo reaches the mechanical extremum. The Bouc-Wen model has been used to\nestablish nonlinearity in the system. The hybrid MPPT exhibits significant\nimprovement over conventional P&O, while the SCA-tuned system demonstrates\nresilience against variable frequency input.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u975e\u7ebf\u6027\u538b\u7535\u6a21\u578b\u3001\u540c\u6b65\u7535\u8377\u63d0\u53d6\uff08SECE\uff09\u3001\u6df7\u5408\u6700\u5927\u529f\u7387\u70b9\u8ddf\u8e2a\uff08MPPT\uff09\u548c\u5f00\u5173\u7535\u5bb9\u9635\u5217\uff08SCA\uff09\u7684\u538b\u7535\u80fd\u91cf\u6536\u96c6\uff08PEH\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709PEH\u6280\u672f\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u5728\u5904\u7406\u975e\u7ebf\u6027\u3001\u63d0\u9ad8\u529f\u7387\u8f93\u51fa\u548c\u9002\u5e94\u53ef\u53d8\u9891\u7387\u8f93\u5165\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u538b\u7535\u80fd\u91cf\u6536\u96c6\uff08PEH\uff09\u4f5c\u4e3a\u4e00\u79cd\u5229\u7528\u73af\u5883\u632f\u52a8\u6765\u6536\u96c6\u80fd\u91cf\u7684\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u5728\u81ea\u4f9b\u7535\u6280\u672f\u548c\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u7136\u800c\uff0cPEH\u6280\u672f\u9762\u4e34\u7740\u8bf8\u5982\u5e26\u5bbd\u7a84\u3001\u975e\u7ebf\u6027\u5f3a\u548c\u963b\u6297\u5931\u914d\u7b49\u5b9e\u9645\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u62dfPEH\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u7684\u538b\u7535\u80fd\u91cf\u6536\u96c6\uff08PEH\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u975e\u7ebf\u6027\u538b\u7535\u6a21\u578b\u3001\u540c\u6b65\u7535\u8377\u63d0\u53d6\uff08SECE\uff09\u3001\u6df7\u5408\u6700\u5927\u529f\u7387\u70b9\u8ddf\u8e2a\uff08MPPT\uff09\u548c\u5f00\u5173\u7535\u5bb9\u9635\u5217\uff08SCA\uff09\u3002\u5176\u4e2d\uff0cBouc-Wen\u6a21\u578b\u7528\u4e8e\u5efa\u7acb\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\uff0cSECE\u7528\u4e8e\u5728\u538b\u7535\u8fbe\u5230\u673a\u68b0\u6781\u503c\u65f6\u63d0\u53d6\u6700\u5927\u7535\u8377\uff0c\u6df7\u5408MPPT\u7528\u4e8e\u6539\u8fdb\u6027\u80fd\uff0cSCA\u7528\u4e8e\u63d0\u9ad8\u7cfb\u7edf\u5bf9\u53ef\u53d8\u9891\u7387\u8f93\u5165\u7684\u5f39\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6df7\u5408MPPT\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684P&O\uff08Perturb and Observe\uff09\u7b97\u6cd5\u6709\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u7ecf\u8fc7SCA\u8c03\u6574\u7684\u7cfb\u7edf\u8bc1\u660e\u4e86\u5176\u5728\u9762\u5bf9\u53ef\u53d8\u9891\u7387\u8f93\u5165\u65f6\u5177\u6709\u826f\u597d\u7684\u5f39\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u6a21\u62df\u7684\u538b\u7535\u80fd\u91cf\u6536\u96c6\uff08PEH\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u538b\u7535\u80fd\u91cf\u6536\u96c6\uff08PEH\uff09\u4e2d\u7684\u975e\u7ebf\u6027\u3001\u7a84\u5e26\u5bbd\u548c\u963b\u6297\u4e0d\u5339\u914d\u7b49\u6311\u6218\u3002\u901a\u8fc7\u91c7\u7528\u975e\u7ebf\u6027\u538b\u7535\u6a21\u578b\u3001\u540c\u6b65\u7535\u8377\u63d0\u53d6\uff08SECE\uff09\u3001\u6df7\u5408\u6700\u5927\u529f\u7387\u70b9\u8ddf\u8e2a\uff08MPPT\uff09\u548c\u5f00\u5173\u7535\u5bb9\u9635\u5217\uff08SCA\uff09\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u9002\u5e94PEH\u7684\u975e\u7ebf\u6027\u7279\u6027\uff0c\u5e76\u5728\u53ef\u53d8\u9891\u7387\u8f93\u5165\u4e0b\u8868\u73b0\u51fa\u5f39\u6027\u3002"}}
{"id": "2507.12032", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12032", "abs": "https://arxiv.org/abs/2507.12032", "authors": ["Brian-Frederik Jahnke", "Ren\u00e9 Brinkhege", "Jan Peter Meyer", "Daniel Tebernum", "Falk Howar"], "title": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum", "comment": null, "summary": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms.", "AI": {"tldr": "ARRC\u901a\u8fc7\u5c06\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u63a8\u8350\u7cfb\u7edf\u96c6\u6210\u5230\u8d44\u6e90\u7ba1\u7406\u4e2d\uff0c\u4ee5\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\u5b9e\u73b0\u4e86\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u53ef\u6301\u7eed\u3001\u900f\u660e\u4e14\u4f4e\u6210\u672c\u7684\u8d44\u6e90\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u5b9e\u73b0\u53ef\u6301\u7eed\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u7ef4\u62a4\u7684\u8d44\u6e90\u4f18\u5316\u81ea\u52a8\u5316\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e94\u5bf9\u7531\u5f02\u6784\u5e73\u53f0\u548c\u5206\u5c42\u62bd\u8c61\u5f15\u8d77\u7684\u6301\u7eed\u8fc7\u5ea6\u914d\u7f6e\u548c\u64cd\u4f5c\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7684\u7cfb\u7edf\u5e26\u6765\u7684\u8106\u5f31\u6027\u3001\u6062\u590d\u56f0\u96be\u548c\u6280\u672f\u503a\u52a1\u7d2f\u79ef\u95ee\u9898\u3002", "method": "ARRC\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u8bbe\u8ba1\u539f\u5219\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c01\u88c5\u4f18\u5316\u903b\u8f91\u5230\u4e13\u95e8\u7684\u3001\u53ef\u5ba1\u8ba1\u7684\u4ee3\u7406\u4e2d\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u63a5\u53e3\u8fdb\u884c\u534f\u8c03\uff0c\u5c06\u53ef\u89e3\u91ca\u7684\u3001\u8de8\u5c42\u7ea7\u7684\u8d44\u6e90\u5efa\u8bae\u76f4\u63a5\u6ce8\u5165\u5230\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u6d41\u4e2d\uff08\u4f8b\u5982\uff0c\u5de5\u5355\u548cGitOps\u62c9\u53d6\u8bf7\u6c42\uff09\u3002", "result": "ARRC\u5728\u591a\u533a\u57df\u5de5\u4e1a\u90e8\u7f72\u4e2d\u7684\u7ecf\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8fd0\u8425\u5546\u5de5\u4f5c\u91cf\u51cf\u5c11\u8d85\u8fc750%\uff0c\u8ba1\u7b97\u5229\u7528\u7387\u63d0\u9ad8\u9ad8\u8fbe7.7\u500d\uff0c\u9519\u8bef\u7387\u4fdd\u6301\u57285%\u4ee5\u4e0b\uff0c\u4e14\u5927\u90e8\u5206\u6536\u76ca\u6765\u81ea\u4e8e\u64cd\u4f5c\u5458\u6279\u51c6\u7684\u589e\u91cf\u66f4\u6539\u3002", "conclusion": "ARRC\u662f\u4e00\u4e2a\u7ecf\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u53ef\u89e3\u91ca\u7684\u3001\u5de5\u4f5c\u6d41\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u96c6\u6210\u5230\u8d44\u6e90\u7ba1\u7406\u4e2d\uff0c\u65e8\u5728\u63a8\u8fdb\u5065\u58ee\u3001\u53ef\u7ef4\u62a4\u548c\u900f\u660e\u7684\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u5e73\u53f0\u7684\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2507.11920", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11920", "abs": "https://arxiv.org/abs/2507.11920", "authors": ["Jeongyong Yang", "KwangBin Lee", "SooJean Han"], "title": "Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments", "comment": null, "summary": "Real-time path planning in dense, uncertain environments remains a\nchallenging problem, as predicting the future motions of numerous dynamic\nobstacles is computationally burdensome and unrealistic. To address this, we\nintroduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a\nprediction-based risk-aware path-planning framework which uses a hybrid\ncombination of models to predict local obstacle movement. HyPRAP uses a novel\nPrediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by\neach obstacle, enabling the selective use of predictors based on whether the\nagent prioritizes high predictive accuracy or low computational prediction\noverhead. This selective routing enables the agent to focus on high-risk\nobstacles while ignoring or simplifying low-risk ones, making it suitable for\nenvironments with a large number of obstacles. Moreover, HyPRAP incorporates\nuncertainty quantification through hybrid conformal prediction by deriving\nconfidence bounds simultaneously achieved by multiple predictions across\ndifferent models. Theoretical analysis demonstrates that HyPRAP effectively\nbalances safety and computational efficiency by leveraging the diversity of\nprediction models. Extensive simulations validate these insights for more\ngeneral settings, confirming that HyPRAP performs better compared to single\npredictor methods, and P-CRI performs better over naive proximity-based risk\nassessment.", "AI": {"tldr": "HyPRAP \u662f\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u548c\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u5728\u590d\u6742\u7684\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u548c\u5b89\u5168\u7684\u5bfc\u822a\u3002", "motivation": "\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u5728\u5bc6\u96c6\u4e14\u5145\u6ee1\u4e0d\u786e\u5b9a\u6027\u7684\u73af\u5883\u4e2d\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u9884\u6d4b\u5927\u91cf\u52a8\u6001\u969c\u788d\u7269\u7684\u672a\u6765\u8fd0\u52a8\u5728\u8ba1\u7b97\u4e0a\u65e2\u7e41\u91cd\u53c8\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HyPRAP \u7684\u6df7\u5408\u9884\u6d4b\u98ce\u9669\u611f\u77e5\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u6765\u9884\u6d4b\u5c40\u90e8\u969c\u788d\u7269\u7684\u8fd0\u52a8\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u9884\u6d4b\u7684\u78b0\u649e\u98ce\u9669\u6307\u6570\uff08P-CRI\uff09\u6765\u8bc4\u4f30\u98ce\u9669\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u98ce\u9669\u969c\u788d\u7269\u7684\u91cd\u70b9\u5173\u6ce8\u548c\u5bf9\u4f4e\u98ce\u9669\u969c\u788d\u7269\u7684\u7b80\u5316\u5904\u7406\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u6df7\u5408\u5171\u5f62\u9884\u6d4b\u5b9e\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "HyPRAP \u6846\u67b6\u80fd\u591f\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e14\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u9884\u6d4b\u5668\u65b9\u6cd5\uff0cP-CRI \u6307\u6570\u4e5f\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u90bb\u8fd1\u5ea6\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "HyPRAP\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u9884\u6d4b\u6a21\u578b\u548c P-CRI \u6307\u6570\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u969c\u788d\u7269\u5bc6\u96c6\u7684\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.11832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11832", "abs": "https://arxiv.org/abs/2507.11832", "authors": ["Yash Ingle", "Pruthwik Mishra"], "title": "ILID: Native Script Language Identification for Indian Languages", "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b230K\u4e2a\u53e5\u5b50\uff08\u5305\u62ec\u82f1\u8bed\u548c\u6240\u670922\u79cd\u5b98\u65b9\u5370\u5ea6\u8bed\u8a00\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u5370\u5ea6\u8bed\u8a00\u8bc6\u522b\u7684\u6311\u6218\u3002", "motivation": "\u8bed\u8a00\u8bc6\u522b\u5728NLP\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5608\u6742\u3001\u77ed\u5c0f\u548c\u6df7\u5408\u8bed\u8a00\u7684\u73af\u5883\u65f6\uff0c\u800c\u5370\u5ea6\u8bed\u8a00\u4e4b\u95f4\u7531\u4e8e\u8bcd\u6c47\u548c\u8bed\u97f3\u76f8\u4f3c\u6027\u4ee5\u53ca\u5171\u4eab\u76f8\u540c\u811a\u672c\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u6700\u65b0\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5f00\u53d1\u548c\u53d1\u5e03\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b230K\u4e2a\u53e5\u5b50\uff08\u5305\u62ec\u82f1\u8bed\u548c\u6240\u670922\u79cd\u5b98\u65b9\u5370\u5ea6\u8bed\u8a00\uff09\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u6570\u636e\u662f\u65b0\u521b\u5efa\u7684\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b230K\u4e2a\u53e5\u5b50\uff08\u5305\u62ec\u82f1\u8bed\u548c\u6240\u670922\u79cd\u5b98\u65b9\u5370\u5ea6\u8bed\u8a00\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e0a\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.11638", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11638", "abs": "https://arxiv.org/abs/2507.11638", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Effective treatment for rectal cancer relies on accurate lymph node\nmetastasis (LNM) staging. However, radiological criteria based on lymph node\n(LN) size, shape and texture morphology have limited diagnostic accuracy. In\nthis work, we investigate applying a Variational Autoencoder (VAE) as a feature\nencoder model to replace the large pre-trained Convolutional Neural Network\n(CNN) used in existing approaches. The motivation for using a VAE is that the\ngenerative model aims to reconstruct the images, so it directly encodes visual\nfeatures and meaningful patterns across the data. This leads to a disentangled\nand structured latent space which can be more interpretable than a CNN. Models\nare deployed on an in-house MRI dataset with 168 patients who did not undergo\nneo-adjuvant treatment. The post-operative pathological N stage was used as the\nground truth to evaluate model predictions. Our proposed model 'VAE-MLP'\nachieved state-of-the-art performance on the MRI dataset, with cross-validated\nmetrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85\n+/- 0.05. Code is available at:\nhttps://github.com/benkeel/Lymph_Node_Classification_MIUA.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u66ff\u4ee3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u63d0\u9ad8\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u7684MRI\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u7684VAE-MLP\u6a21\u578b\u5728168\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.86\u7684AUC\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u57fa\u4e8e\u6dcb\u5df4\u7ed3\uff08LN\uff09\u7684\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u7eb9\u7406\u5f62\u6001\u7684\u653e\u5c04\u5b66\u6807\u51c6\u5728\u6dcb\u5df4\u7ed3\u8f6c\u79fb\uff08LNM\uff09\u5206\u671f\u4e2d\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5229\u7528VAE\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u56e0\u4e3a\u5176\u751f\u6210\u6a21\u578b\u65e8\u5728\u91cd\u5efa\u56fe\u50cf\uff0c\u80fd\u591f\u76f4\u63a5\u7f16\u7801\u6709\u610f\u4e49\u7684\u89c6\u89c9\u7279\u5f81\u548c\u6a21\u5f0f\uff0c\u4ece\u800c\u4ea7\u751f\u6bd4CNN\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u3001\u89e3\u7f20\u7ed3\u7684\u3001\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u7279\u5f81\u7f16\u7801\u6a21\u578b\uff0c\u4ee5\u66ff\u4ee3\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u5bb9\u91cf\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\uff08LNM\uff09\u7684\u653e\u5c04\u5b66\u5206\u671f\u3002", "result": "\u5728\u5305\u542b168\u540d\u672a\u63a5\u53d7\u65b0\u8f85\u52a9\u6cbb\u7597\u7684\u60a3\u8005\u7684\u5185\u90e8MRI\u6570\u636e\u96c6\u4e0a\uff0cVAE-MLP\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u7684\u8bc4\u4f30\u6307\u6807\u5305\u62ecAUC 0.86 +/- 0.05\uff0c\u654f\u611f\u60270.79 +/- 0.06\uff0c\u7279\u5f02\u60270.85 +/- 0.05\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684VAE-MLP\u6a21\u578b\u5728\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u7684MRI\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAUC\u8fbe\u52300.86 +/- 0.05\uff0c\u663e\u793a\u51fa\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11713", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11713", "abs": "https://arxiv.org/abs/2507.11713", "authors": ["Chiara Marletto", "Vlatko Vedral"], "title": "Can entanglement be mediated by a Koopmanian system?", "comment": "4 pages, no figures", "summary": "We present a method for coupling a Koopmanian classical system to two quantum\nbits to mediate an interaction between them. We then prove that the resulting\ndynamics can never lead to entanglement between the two qubits. Even though the\ntotal system of two qubits and the Koopmanian classical system are described\nwith the full quantum formalism, we show that their composite system violates\nexact conservation laws as expected for a hybrid quantum-classical system. We\nfinally discuss the implications for semi-classical treatments of quantum\ngravity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7ecf\u5178\u7cfb\u7edf\u4e0e\u91cf\u5b50\u6bd4\u7279\u8026\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u4e0d\u4f1a\u4ea7\u751f\u91cf\u5b50\u7ea0\u7f20\uff0c\u5c3d\u7ba1\u5b83\u8fdd\u53cd\u4e86\u5b88\u6052\u5b9a\u5f8b\uff0c\u8fd9\u53ef\u80fd\u5bf9\u91cf\u5b50\u5f15\u529b\u6709\u5f71\u54cd\u3002", "motivation": "\u8ba8\u8bba\u4e86\u5176\u5bf9\u91cf\u5b50\u5f15\u529b\u534a\u7ecf\u5178\u5904\u7406\u7684\u542f\u793a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5e93\u666e\u66fc\u7ecf\u5178\u7cfb\u7edf\u4e0e\u4e24\u4e2a\u91cf\u5b50\u6bd4\u7279\u8026\u5408\u4ee5\u4ecb\u5bfc\u5b83\u4eec\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u52a8\u529b\u5b66\u6c38\u8fdc\u4e0d\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u7684\u7ea0\u7f20\u3002", "conclusion": "\u8be5\u590d\u5408\u7cfb\u7edf\u8fdd\u53cd\u4e86\u4e0e\u9884\u671f\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7cfb\u7edf\u76f8\u7b26\u7684\u7cbe\u786e\u5b88\u6052\u5b9a\u5f8b\u3002"}}
{"id": "2507.11649", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11649", "abs": "https://arxiv.org/abs/2507.11649", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u6765\u4fdd\u62a4\u8bc4\u4f30\u9636\u6bb5\u9690\u79c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc1\u660e\u672c\u5730\u635f\u5931\u4f4e\u4e8e\u9608\u503c\u6765\u66ff\u4ee3\u76f4\u63a5\u66b4\u9732\u635f\u5931\u503c\uff0c\u5e76\u5728MNIST\u548cHAR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5141\u8bb8\u5728\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5206\u6563\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002\u7136\u800c\uff0cFL\u4e2d\u7684\u8bc4\u4f30\u9636\u6bb5\u53ef\u80fd\u901a\u8fc7\u5171\u4eab\u7684\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKP\uff09\u7684\u65b0\u578b\u534f\u8bae\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\u3002\u5ba2\u6237\u7aef\u751f\u6210\u7b80\u6d01\u7684\u8bc1\u660e\uff0c\u65ad\u8a00\u5176\u672c\u5730\u635f\u5931\u4f4e\u4e8e\u9884\u5b9a\u9608\u503c\uff0c\u800c\u4e0d\u662f\u66b4\u9732\u539f\u59cb\u635f\u5931\u503c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5916\u90e8API\uff0c\u5e76\u5305\u542b\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u6a21\u62df\u3001ZKP\u7535\u8def\u8bbe\u8ba1\u548c\u5728MNIST\u53ca\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u7684\u81ea\u5305\u542b\u6a21\u5757\u3002", "result": "\u7814\u7a76\u5728MNIST\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u9608\u503c\u7684\u8bc1\u660e\u3002\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u901a\u4fe1\u6210\u672c\u548c\u53ef\u9a8c\u8bc1\u6027\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKP\uff09\u7684\u65b0\u578b\u534f\u8bae\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\uff0c\u4ee5\u89e3\u51b3\u8bc4\u4f30\u9636\u6bb5\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u95ee\u9898\u3002"}}
{"id": "2507.12279", "categories": ["cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.12279", "abs": "https://arxiv.org/abs/2507.12279", "authors": ["Ritik R. Modi", "Karen L. Sauer"], "title": "Cryogenic spin 3/2 nuclear quadrupole resonance: Spin relaxation and electric field gradient via Rabi frequency goniometry", "comment": "10 pages, 12 figures", "summary": "A straightforward way to determine the electric field gradient $-$ principal\naxes frame (EFG-PAF) on single crystal samples with spin 3/2 nuclei is\ndemonstrated. Nuclear quadrupole resonance (NQR) spectroscopy is used to\ndetermine the Rabi frequency for $^{35}$Cl in a single crystal of potassium\nchlorate (KClO$_3$) by comparing the NQR signal for powder and single crystal\nsamples. By exploiting the geometrical dependence of the Rabi frequency with\nrespect to the excitation direction, EFG-PAF is readily determined.\nFurthermore, relaxation times $T_1$ and $T_2^*$ were measured at multiple\ntemperatures ranging from $17~\\textrm{K}$ to $200 ~$K, extending the results of\nprevious works to colder temperatures where new relaxation mechanisms become\ndominant. The experiments were performed in a cryogen-free cryostat, which\nposed distinct challenges compared to a conventional cryogenic cooling setup.\nThe successful operation of the NQR probe within a cryogen-free cryostat has\nthe potential to make the technique more accessible and widen applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528NQR\u5149\u8c31\u786e\u5b9a\u5355\u6676\u6837\u54c1\u4e0a\u81ea\u65cb\u4e3a3/2\u7684\u6838\u7684\u7535\u573a\u68af\u5ea6\u4e3b\u8f74\u5750\u6807\u7cfb\u7684\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e35Cl\u7684\u6c2f\u5316\u94be\u5355\u6676\u6837\u54c1\u3002\u7814\u7a76\u8fd8\u8fdb\u884c\u4e86\u4f4e\u6e29\u4e0b\u7684\u5f1b\u8c6b\u65f6\u95f4\u6d4b\u91cf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65e0\u4f4e\u6e29\u6052\u6e29\u5668\u4e2d\u8fdb\u884cNQR\u5b9e\u9a8c\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u627e\u5230\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u786e\u5b9a\u5355\u6676\u6837\u54c1\u4e0a\u81ea\u65cb\u4e3a3/2\u7684\u6838\u7684\u7535\u573a\u68af\u5ea6\u4e3b\u8f74\u5750\u6807\u7cfb\uff0c\u5e76\u63a2\u7d22\u5728\u65e0\u4f4e\u6e29\u6052\u6e29\u5668\u4e2d\u8fdb\u884cNQR\u5b9e\u9a8c\u7684\u53ef\u884c\u6027\u3002", "method": "\u5229\u7528\u6838\u56db\u6781\u5171\u632f(NQR)\u5149\u8c31\u5b66\uff0c\u901a\u8fc7\u6bd4\u8f83\u7c89\u672b\u548c\u5355\u6676\u6837\u54c1\u5728\u4e0d\u540c\u6fc0\u53d1\u65b9\u5411\u4e0b\u7684NQR\u4fe1\u53f7\uff0c\u6765\u786e\u5b9a35Cl\u7684\u62c9\u6bd4\u9891\u7387\uff0c\u5e76\u636e\u6b64\u786e\u5b9aEFG-PAF\u3002", "result": "\u6210\u529f\u786e\u5b9a\u4e8635Cl\u5728KClO3\u5355\u6676\u6837\u54c1\u4e0a\u7684EFG-PAF\uff0c\u5e76\u6d4b\u91cf\u4e86\u572817K\u81f3200K\u6e29\u5ea6\u8303\u56f4\u5185\u4ee5\u53ca\u5728\u65e0\u4f4e\u6e29\u6052\u6e29\u5668\u4e2d\u8fdb\u884c\u7684T1\u548cT2*\u5f1b\u8c6b\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u81ea\u65cb\u4e3a3/2\u7684\u6838\u5728\u5355\u6676\u6837\u54c1\u4e0a\u7535\u573a\u68af\u5ea6\u4e3b\u8f74\u5750\u6807\u7cfb(EFG-PAF)\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e35Cl\u7684\u6c2f\u5316\u94be(KClO3)\u5355\u6676\u6837\u54c1\u3002\u7814\u7a76\u8fd8\u6d4b\u91cf\u4e86\u572817K\u81f3200K\u6e29\u5ea6\u8303\u56f4\u5185\uff0c\u4ee5\u53ca\u5728\u65e0\u4f4e\u6e29\u6052\u6e29\u5668\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u65f6\uff0cT1\u548cT2*\u5f1b\u8c6b\u65f6\u95f4\u3002"}}
{"id": "2507.12146", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12146", "abs": "https://arxiv.org/abs/2507.12146", "authors": ["Carl Collmann", "Bitan Banerjee", "Ahmad Nimr", "Gerhard Fettweis"], "title": "A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops", "comment": "14 Pages", "summary": "In MIMO systems, the presence of phase noise is a significant factor that can\ndegrade performance. For MIMO testbeds build from SDR devices, phase noise\ncannot be ignored, particular in applications that require phase\nsynchronization. This is especially relevant in MIMO systems that employ\ndigital beamforming, where precise phase alignment is crucial. Accordingly,\naccurate phase noise modelling of SDR devices is essential. However, the\ninformation provided in data sheets for different SDR models varies widely and\nis often insufficient for comprehensive characterization of their phase noise\nperformance. While numerical simulations of PLL phase noise behavior are\ndocumented in the literature, there is a lack of extensive measurements\nsupported by appropriate system modelling. In this work, we present a practical\nphase noise modeling methodology applied to an SDR from the USRP X310 series.\nBased on measurement data, we derive estimates of key PLL performance\nindicators such as cycle-to-cycle jitter, oscillator constants, and PLL\nbandwidth. Furthermore, we propose a parametric model for the phase noise PSD\nof the PLL circuit and provide corresponding parameter estimates. This model\ncan be used for further investigation into the impact of phase noise on MIMO\nsystem performance implemented by similar SDR devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9SDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u4ee5USRPX310\u4e3a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d4b\u91cf\u6570\u636e\u63d0\u53d6\u5173\u952e\u7684\u9501\u76f8\u73af\u6027\u80fd\u53c2\u6570\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8e\u5206\u6790\u76f8\u4f4d\u566a\u58f0\u5bf9MIMO\u7cfb\u7edf\u5f71\u54cd\u7684\u53c2\u6570\u5316\u6a21\u578b\u3002", "motivation": "MIMO\u7cfb\u7edf\u4e2d\u7684\u76f8\u4f4d\u566a\u58f0\u4f1a\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u76f8\u4f4d\u540c\u6b65\u7684\u5e94\u7528\u4e2d\u3002\u5bf9\u4e8e\u4f7f\u7528SDR\u8bbe\u5907\u6784\u5efa\u7684MIMO\u6d4b\u8bd5\u5e73\u53f0\uff0c\u76f8\u4f4d\u566a\u58f0\u7684\u5f71\u54cd\u4e0d\u5bb9\u5ffd\u89c6\uff0c\u7279\u522b\u662f\u5728\u91c7\u7528\u6570\u5b57\u6ce2\u675f\u5f62\u6210\u7684MIMO\u7cfb\u7edf\u4e2d\uff0c\u7cbe\u786e\u7684\u76f8\u4f4d\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u5bf9SDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u8fdb\u884c\u7cbe\u786e\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709SDR\u8bbe\u5907\u7684\u6570\u636e\u624b\u518c\u4fe1\u606f\u4e0d\u8db3\u4ee5\u8fdb\u884c\u5168\u9762\u7684\u76f8\u4f4d\u566a\u58f0\u6027\u80fd\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6d4b\u91cf\u6570\u636e\uff0c\u63a8\u5bfc\u4e86\u5173\u952e\u7684\u9501\u76f8\u73af\uff08PLL\uff09\u6027\u80fd\u6307\u6807\uff08\u5982\u5468\u671f\u6296\u52a8\u3001\u632f\u8361\u5668\u5e38\u6570\u548cPLL\u5e26\u5bbd\uff09\u7684\u4f30\u8ba1\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2aPLL\u7535\u8def\u76f8\u4f4d\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u7684\u53c2\u6570\u5316\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u57fa\u4e8e\u6d4b\u91cf\u6570\u636e\uff0c\u63a8\u5bfc\u4e86\u5173\u952e\u7684\u9501\u76f8\u73af\uff08PLL\uff09\u6027\u80fd\u6307\u6807\uff0c\u5982\u5468\u671f\u6296\u52a8\u3001\u632f\u8361\u5668\u5e38\u6570\u548cPLL\u5e26\u5bbd\u7684\u4f30\u8ba1\u503c\u3002\u63d0\u51fa\u4e86\u4e00\u4e2aPLL\u7535\u8def\u76f8\u4f4d\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u7684\u53c2\u6570\u5316\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u53c2\u6570\u4f30\u8ba1\u3002\u8be5\u6a21\u578b\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u76f8\u4f4d\u566a\u58f0\u5bf9\u4f7f\u7528\u7c7b\u4f3cSDR\u8bbe\u5907\u5b9e\u73b0\u7684MIMO\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e94\u7528\u4e8eUSRPX310\u7cfb\u5217SDR\u7684\u5b9e\u7528\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\u3002\u57fa\u4e8e\u6d4b\u91cf\u6570\u636e\uff0c\u7814\u7a76\u63a8\u5bfc\u4e86\u8bf8\u5982\u5468\u671f\u6296\u52a8\u3001\u632f\u8361\u5668\u5e38\u6570\u548c\u9501\u76f8\u73af\u5e26\u5bbd\u7b49\u5173\u952e\u9501\u76f8\u73af\u6027\u80fd\u6307\u6807\u7684\u4f30\u8ba1\u503c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9501\u76f8\u73af\u7535\u8def\u76f8\u4f4d\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u7684\u53c2\u6570\u5316\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2507.12254", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.12254", "abs": "https://arxiv.org/abs/2507.12254", "authors": ["Le Zhang", "Wenqiang Zhou", "Xinjie Fang", "Zhen Zhan", "Kenji Watanabe", "Takashi Taniguchi", "Yi-feng Yang", "Shuigang Xu"], "title": "Electrically tunable heavy fermion and quantum criticality in magic-angle twisted trilayer graphene", "comment": null, "summary": "The interplay between localized magnetic moments and itinerant electrons\ngives rise to exotic quantum states in condensed matter systems.\nTwo-dimensional moire superlattices offer a powerful platform for engineering\nheavy fermion states beyond conventional rare-earth intermetallic compounds.\nWhile localized and itinerant carriers have been observed in twisted graphene\nmoire systems, direct evidence of their strong coupling--leading to artificial\nheavy fermion states--has remained elusive. Here, we demonstrate electrically\ntunable heavy fermion in magic-angle twisted trilayer graphene, achieved by\ncontrolling the Kondo hybridization between localized flatband electrons and\nitinerant Dirac electrons via a displacement field. Our results reveal a\ncontinuous quantum phase transition from an antiferromagnetic semimetal to a\nparamagnetic heavy fermion metal, evidenced by a crossover from logarithmic to\nquadratic temperature-dependent resistivity, a dramatic enhancement of\nquasiparticle effective mass, and Fermi surface reconstruction near quantum\ncritical point. This highly tunable platform offers unprecedented control over\nheavy fermion physics, establishing moire heterostructures as a versatile arena\nfor exploring correlated quantum phases--including potential unconventional\nsuperconductivity--in two-dimensional materials.", "AI": {"tldr": "\u5728\u9b54\u89d2\u626d\u66f2\u4e09\u5c42\u77f3\u58a8\u70ef\u4e2d\uff0c\u901a\u8fc7\u8c03\u63a7Kondo\u6742\u5316\uff0c\u5b9e\u73b0\u4e86\u7535\u53ef\u8c03\u7684\u91cd\u8d39\u7c73\u5b50\u6001\uff0c\u5e76\u89c2\u5bdf\u5230\u91cf\u5b50\u76f8\u53d8\u3002", "motivation": "\u63a2\u7d22\u5728\u975e\u7a00\u571f\u4f53\u7cfb\u4e2d\u5de5\u7a0b\u5316\u91cd\u8d39\u7c73\u5b50\u6001\uff0c\u4ee5\u53ca\u5728\u626d\u66f2\u77f3\u58a8\u70ef\u4f53\u7cfb\u4e2d\u5bfb\u627e\u5c40\u57df\u7535\u5b50\u4e0e\u5de1\u6e38\u7535\u5b50\u5f3a\u8026\u5408\u7684\u76f4\u63a5\u8bc1\u636e\u3002", "method": "\u5229\u7528\u9b54\u89d2\u626d\u66f2\u4e09\u5c42\u77f3\u58a8\u70ef\uff0c\u901a\u8fc7\u4f4d\u79fb\u573a\u8c03\u63a7\u5c40\u57df\u5e73\u5e26\u7535\u5b50\u4e0e\u5de1\u6e38\u72c4\u62c9\u514b\u7535\u5b50\u4e4b\u95f4\u7684Kondo\u6742\u5316\u4f5c\u7528\uff0c\u5b9e\u73b0\u4e86\u91cd\u8d39\u7c73\u5b50\u6001\u3002", "result": "\u89c2\u5bdf\u5230\u4ece\u53cd\u94c1\u78c1\u534a\u91d1\u5c5e\u5230\u987a\u78c1\u91cd\u8d39\u7c73\u5b50\u91d1\u5c5e\u7684\u8fde\u7eed\u91cf\u5b50\u76f8\u53d8\uff0c\u8868\u73b0\u4e3a\u7535\u963b\u7387\u4ece\u5bf9\u6570\u6e29\u5ea6\u4f9d\u8d56\u5230\u4e8c\u6b21\u65b9\u6e29\u5ea6\u4f9d\u8d56\u7684\u4ea4\u53c9\uff0c\u4ee5\u53ca\u5728\u91cf\u5b50\u4e34\u754c\u70b9\u9644\u8fd1\u51c6\u7c92\u5b50\u6709\u6548\u8d28\u91cf\u7684\u663e\u8457\u589e\u52a0\u548c\u8d39\u7c73\u9762\u91cd\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u9b54\u89d2\u626d\u66f2\u4e09\u5c42\u77f3\u58a8\u70ef\u4e2d\u7684Kondo\u6742\u5316\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u7535\u8c03\u8c10\u7684\u91cd\u8d39\u7c73\u5b50\u6001\uff0c\u63ed\u793a\u4e86\u4ece\u53cd\u94c1\u78c1\u534a\u91d1\u5c5e\u5230\u987a\u78c1\u91cd\u8d39\u7c73\u5b50\u91d1\u5c5e\u7684\u8fde\u7eed\u91cf\u5b50\u76f8\u53d8\uff0c\u4e3a\u91cd\u8d39\u7c73\u5b50\u7269\u7406\u548c\u975e\u5e38\u89c4\u8d85\u5bfc\u7b49\u76f8\u5173\u91cf\u5b50\u73b0\u8c61\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "AI": {"tldr": "TPE-MARL \u901a\u8fc7\u6784\u5efa\u535a\u5f08\u62d3\u6251\u5f20\u91cf\u548c\u878d\u5408\u8bbf\u95ee\u8ba1\u6570\u4e0e\u4e92\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5f00\u53d1\u96be\u9898\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u6df7\u5408\u4ea4\u901a\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u56fa\u6709\u7684\u63a2\u7d22-\u5f00\u53d1\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4ea4\u901a\u6d41\u7b49\u590d\u6742\u573a\u666f\u4e0b\uff0c\u667a\u80fd\u4f53\u6570\u91cf\u7684\u589e\u52a0\u5bfc\u81f4\u8054\u5408\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f7f\u5f97\u4f18\u5316\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08TPE-MARL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u52a8\u6001\u4ea4\u901a\u6d41\u535a\u5f08\u62d3\u6251\u5f20\u91cf\uff0c\u7528\u4e8e\u538b\u7f29\u9ad8\u7ef4\u4ea4\u901a\u72b6\u6001\u4fe1\u606f\u5e76\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5229\u7528 QMIX \u4f5c\u4e3a\u57fa\u7840\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u8bbf\u95ee\u8ba1\u6570\u548c\u667a\u80fd\u4f53\u4e92\u4fe1\u606f\uff0c\u6784\u5efa\u4e86\u62d3\u6251\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "result": "TPE-MARL \u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u548c\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u6e17\u900f\u7387\u4e0b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6a21\u62df\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u52a8\u6001\u3001\u63a2\u7d22\u6a21\u5f0f\u3001\u5b8f\u89c2\u4ea4\u901a\u6027\u80fd\u6307\u6807\u548c\u5fae\u89c2\u8f66\u8f86\u884c\u4e3a\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\uff0c\u5e76\u5728\u4ea4\u901a\u6548\u7387\u3001\u5b89\u5168\u3001\u51b3\u7b56\u5e73\u6ed1\u5ea6\u548c\u4efb\u52a1\u5b8c\u6210\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "TPE-MARL \u7b97\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u5f00\u53d1\uff0c\u5728\u4ea4\u901a\u6548\u7387\u3001\u5b89\u5168\u3001\u51b3\u7b56\u5e73\u6ed1\u5ea6\u548c\u4efb\u52a1\u5b8c\u6210\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5176\u51b3\u7b56\u7684\u5408\u7406\u6027\u53ef\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8d8a\u3002"}}
{"id": "2507.12187", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12187", "abs": "https://arxiv.org/abs/2507.12187", "authors": ["Laura Boca de Giuli", "Alessio La Bella", "Riccardo Scattolini"], "title": "Learning, fast and slow: a two-fold algorithm for data-based model adaptation", "comment": null, "summary": "This article addresses the challenge of adapting data-based models over time.\nWe propose a novel two-fold modelling architecture designed to correct\nplant-model mismatch caused by two types of uncertainty. Out-of-domain\nuncertainty arises when the system operates under conditions not represented in\nthe initial training dataset, while in-domain uncertainty results from\nreal-world variability and flaws in the model structure or training process. To\nhandle out-of-domain uncertainty, a slow learning component, inspired by the\nhuman brain's slow thinking process, learns system dynamics under unexplored\noperating conditions, and it is activated only when a monitoring strategy deems\nit necessary. This component consists of an ensemble of models, featuring (i) a\ncombination rule that weights individual models based on the statistical\nproximity between their training data and the current operating condition, and\n(ii) a monitoring algorithm based on statistical control charts that supervises\nthe ensemble's reliability and triggers the offline training and integration of\na new model when a new operating condition is detected. To address in-domain\nuncertainty, a fast learning component, inspired by the human brain's fast\nthinking process, continuously compensates in real time for the mismatch of the\nslow learning model. This component is implemented as a Gaussian process (GP)\nmodel, trained online at each iteration using recent data while discarding\nolder samples. The proposed methodology is tested on a benchmark energy system\nreferenced in the literature, demonstrating that the combined use of slow and\nfast learning components improves model accuracy compared to standard\nadaptation approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u7ea7\u5efa\u6a21\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u6162\u901f\uff08\u57fa\u4e8e\u96c6\u5408\u7684\u6a21\u578b\uff0c\u5177\u6709\u76d1\u6d4b\u548c\u5728\u7ebf\u96c6\u6210\uff09\u548c\u5feb\u901f\uff08\u5728\u7ebf\u8bad\u7ec3\u7684\u9ad8\u65af\u8fc7\u7a0b\uff09\u5b66\u4e60\u7ec4\u4ef6\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u9a71\u52a8\u6a21\u578b\u968f\u65f6\u95f4\u63a8\u79fb\u548c\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u81ea\u9002\u5e94\u6311\u6218\uff0c\u5e76\u5728\u57fa\u51c6\u80fd\u6e90\u7cfb\u7edf\u4e0a\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u6a21\u578b\u968f\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u6311\u6218\uff0c\u5904\u7406\u7531\u8d85\u51fa\u8303\u56f4\u548c\u8303\u56f4\u5185\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u8d77Plant-model mismatch\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u7ea7\u5efa\u6a21\u67b6\u6784\uff1a1. \u6162\u901f\u5b66\u4e60\u7ec4\u4ef6\uff1a\u5f53\u76d1\u6d4b\u7b56\u7565\u8ba4\u4e3a\u6709\u5fc5\u8981\u65f6\uff0c\u4f1a\u5b66\u4e60\u7cfb\u7edf\u5728\u672a\u63a2\u7d22\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u52a8\u6001\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u6a21\u578b\u96c6\u5408\uff0c\u5176\u4e2d\u7ec4\u5408\u89c4\u5219\u6839\u636e\u5176\u8bad\u7ec3\u6570\u636e\u4e0e\u5f53\u524d\u64cd\u4f5c\u6761\u4ef6\u4e4b\u95f4\u7684\u7edf\u8ba1\u63a5\u8fd1\u5ea6\u6765\u4e3a\u5355\u4e2a\u6a21\u578b\u8bbe\u7f6e\u6743\u91cd\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u7edf\u8ba1\u63a7\u5236\u56fe\u7684\u76d1\u6d4b\u7b97\u6cd5\u6765\u76d1\u7763\u96c6\u5408\u7684\u53ef\u9760\u6027\u5e76\u5728\u68c0\u6d4b\u5230\u65b0\u64cd\u4f5c\u6761\u4ef6\u65f6\u89e6\u53d1\u65b0\u6a21\u578b\u7684\u79bb\u7ebf\u8bad\u7ec3\u548c\u96c6\u6210\u30022. \u5feb\u901f\u5b66\u4e60\u7ec4\u4ef6\uff1a\u4f5c\u4e3a\u4e00\u4e2a\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u6a21\u578b\uff0c\u5b83\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u4f7f\u7528\u6700\u8fd1\u7684\u6570\u636e\u8fdb\u884c\u5728\u7ebf\u8bad\u7ec3\uff0c\u540c\u65f6\u4e22\u5f03\u65e7\u6837\u672c\uff0c\u4ece\u800c\u5b9e\u65f6\u8865\u507f\u6162\u901f\u5b66\u4e60\u6a21\u578b\u7684\u5931\u914d\u3002", "result": "\u5728\u6587\u732e\u4e2d\u5f15\u7528\u7684\u57fa\u51c6\u80fd\u6e90\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6162\u901f\u548c\u5feb\u901f\u5b66\u4e60\u7ec4\u4ef6\u7684\u7ec4\u5408\u4f7f\u7528\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6162\u901f\u548c\u5feb\u901f\u5b66\u4e60\u7ec4\u4ef6\uff0c\u5728\u5904\u7406\u8d85\u51fa\u8303\u56f4\u548c\u8303\u56f4\u5185\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\uff0c\u6bd4\u6807\u51c6\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2507.12038", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12038", "abs": "https://arxiv.org/abs/2507.12038", "authors": ["Alkida Balliu", "Thomas Boudier", "Francesco d'Amore", "Dennis Olivetti", "Gustav Schmid", "Jukka Suomela"], "title": "Distributed Algorithms for Potential Problems", "comment": "28 pages, 4 figures", "summary": "In this work we present a fast distributed algorithm for local potential\nproblems: these are graph problems where the task is to find a locally optimal\nsolution where no node can unilaterally improve the utility in its local\nneighborhood by changing its own label. A simple example of such a problem is\nthe task of finding a locally optimal cut, i.e., a cut where for each node at\nleast half of its incident edges are cut edges. The distributed round\ncomplexity of locally optimal cut has been wide open; the problem is known to\nrequire $\\Omega(\\log n)$ rounds in the deterministic LOCAL model and\n$\\Omega(\\log \\log n)$ rounds in the randomized LOCAL model, but the only known\nupper bound is the trivial brute-force solution of $O(n)$ rounds. Locally\noptimal cut in bounded-degree graphs is perhaps the simplest example of a\nlocally checkable labeling problem for which there is still such a large gap\nbetween current upper and lower bounds. We show that in bounded-degree graphs,\nall local potential problems, including locally optimal cut, can be solved in\n$\\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.\nIn particular, the deterministic round complexity of the locally optimal cut\nproblem is now settled to $\\log^{\\Theta(1)} n$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u53ef\u5c06\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff08\u5982\u5c40\u90e8\u6700\u4f18\u5272\uff09\u5728\u6709\u754c\u5ea6\u56fe\u4e2d\u7684\u8f6e\u6570\u590d\u6742\u5ea6\u4ece $O(n)$ \u964d\u4f4e\u5230 $\\log^{O(1)} n$\uff0c\u5e76\u5c06\u786e\u5b9a\u6027\u8f6e\u6570\u590d\u6742\u5ea6\u7cbe\u786e\u5230 $\\Theta(\\log n)$\u3002", "motivation": "\u89e3\u51b3\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff08\u5982\u5c40\u90e8\u6700\u4f18\u5272\uff09\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5c40\u90e8\u6700\u4f18\u5272\u95ee\u9898\uff0c\u5176\u786e\u5b9a\u6027\u548c\u968f\u673a\u5316\u6a21\u578b\u4e2d\u7684\u8f6e\u6570\u590d\u6742\u5ea6\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u7684\u5feb\u901f\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u6709\u754c\u5ea6\u56fe\u3002", "result": "\u5728\u6709\u754c\u5ea6\u56fe\u4e2d\uff0c\u6240\u6709\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff08\u5305\u62ec\u5c40\u90e8\u6700\u4f18\u5272\uff09\u90fd\u53ef\u4ee5\u5728\u786e\u5b9a\u6027\u548c\u968f\u673a\u5316\u5c40\u90e8\u6a21\u578b\u4e2d\u4ee5 $\\log^{O(1)} n$ \u8f6e\u6570\u89e3\u51b3\u3002\u7279\u522b\u662f\uff0c\u5c40\u90e8\u6700\u4f18\u5272\u95ee\u9898\u7684\u786e\u5b9a\u6027\u8f6e\u6570\u590d\u6742\u5ea6\u88ab\u786e\u5b9a\u4e3a $\\Theta(\\log n)$\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff08\u5305\u62ec\u5c40\u90e8\u6700\u4f18\u5272\uff09\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u786e\u5b9a\u6027\u548c\u968f\u673a\u5316\u5c40\u90e8\u6a21\u578b\u4e2d\u90fd\u53ea\u9700\u8981 $\\log^{O(1)} n$ \u7684\u8f6e\u6570\uff0c\u4ece\u800c\u7f29\u5c0f\u4e86\u5c40\u90e8\u6700\u4f18\u5272\u95ee\u9898\u7684\u5df2\u77e5\u4e0a\u754c\u548c\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5c06\u8be5\u95ee\u9898\u7684\u786e\u5b9a\u6027\u8f6e\u6570\u590d\u6742\u5ea6\u786e\u5b9a\u4e3a $\\Theta(\\log n)$\u3002"}}
{"id": "2507.11938", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11938", "abs": "https://arxiv.org/abs/2507.11938", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "comment": "Accepted by IEEE T-RO", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "AI": {"tldr": "Grasping unknown objects from a single view is hard due to partial observations. Existing methods are sensitive to noise. This paper proposes a new method using similarity matching: find similar known objects in a database, imitate their grasps, and fine-tune. It uses multiple features (semantic, geometric, dimensional) and a new descriptor (C-FPFH) for better matching, especially with noisy, partial data. Large language models and a new registration method also help.", "motivation": "Traditional learning-based grasping approaches face limitations in performance robustness due to sensitivity to sensing noise and environmental changes. This paper aims to address this bottleneck by introducing a new perspective of similarity matching to achieve highly generalized grasping of unknown objects from a single view, overcoming the uncertainty caused by partial observations.", "method": "The paper proposes a novel method for grasping unknown objects from a single view based on similarity matching. It involves three steps: 1) identifying candidate objects with high similarity using visual features and a database; 2) planning grasps for the unknown object based on the candidate models' pre-existing grasping knowledge; 3) optimizing grasp quality via local fine-tuning. A multi-level similarity matching framework integrating semantic, geometric, and dimensional features is introduced to handle uncertainty. A new point cloud geometric descriptor, C-FPFH, is proposed for accurate similarity assessment between partial and complete point clouds. Large language models, semi-oriented bounding boxes, and a novel plane detection-based point cloud registration approach are also incorporated to enhance matching accuracy.", "result": "The method robustly achieves unknown-object grasping from a single viewpoint. It utilizes visual features for similarity matching with a database, plans imitative grasps using candidate models, and optimizes grasp quality through local fine-tuning. The multi-level similarity matching framework and the novel C-FPFH descriptor improve accuracy under single-view conditions, enhanced further by LLMs and a new registration approach.", "conclusion": "The proposed similarity matching method robustly achieves unknown-object grasping from a single viewpoint by leveraging visual features for similarity matching with a database, planning imitative grasps using candidate models, and optimizing grasp quality through local fine-tuning. The multi-level similarity matching framework integrates semantic, geometric, and dimensional features, and a novel C-FPFH descriptor enhances assessment between partial and complete point clouds. LLMs, semi-oriented bounding boxes, and a novel point cloud registration approach further improve matching accuracy."}}
{"id": "2507.11851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11851", "abs": "https://arxiv.org/abs/2507.11851", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u672a\u6765\u6807\u8bb0\u7684\u77e5\u8bc6\u5e76\u5f15\u5165\u5c4f\u853d\u8f93\u5165\u3001\u95e8\u63a7 LoRA\u3001\u91c7\u6837\u5668\u6a21\u5757\u3001\u8f85\u52a9\u635f\u5931\u548c\u6295\u673a\u751f\u6210\u7b49\u521b\u65b0\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u635f\u5931\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86\u591a\u8fbe5\u500d\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u56fa\u6709\u7684\u987a\u5e8f\u751f\u6210\u6a21\u5f0f\u7684\u9650\u5236\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4e86\u6807\u51c6\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5173\u4e8e\u672a\u6765\u6807\u8bb0\u7684\u5185\u5728\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u79cd\u6280\u672f\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\uff0c\u4ece\u800c\u80fd\u591f\u540c\u6b65\u9884\u6d4b\u591a\u4e2a\u540e\u7eed\u6807\u8bb0\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u5173\u952e\u521b\u65b0\u5b9e\u73b0\u591a\u4e8e\u4e00\u4e2a\u7684\u6807\u8bb0\u7684\u540c\u6b65\u9884\u6d4b\uff1a(1) \u5c4f\u853d\u8f93\u5165\u5f62\u5f0f\uff0c\u5176\u4e2d\u591a\u4e2a\u672a\u6765\u6807\u8bb0\u4ece\u5171\u540c\u524d\u7f00\u8054\u5408\u9884\u6d4b\uff1b(2) \u95e8\u63a7 LoRA \u5f62\u5f0f\uff0c\u4fdd\u7559\u4e86\u539f\u59cb LLM \u7684\u529f\u80fd\uff0c\u540c\u65f6\u652f\u6301\u591a\u6807\u8bb0\u9884\u6d4b\uff1b(3) \u8f7b\u91cf\u7ea7\u3001\u53ef\u5b66\u4e60\u7684\u91c7\u6837\u5668\u6a21\u5757\uff0c\u4ece\u9884\u6d4b\u7684\u672a\u6765\u6807\u8bb0\u751f\u6210\u8fde\u8d2f\u5e8f\u5217\uff1b(4) \u4e00\u7ec4\u8f85\u52a9\u8bad\u7ec3\u635f\u5931\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u8054\u5408\u9884\u6d4b\u6807\u8bb0\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\uff1b(5) \u6295\u673a\u751f\u6210\u7b56\u7565\uff0c\u5728\u672a\u6765\u4e8c\u6b21\u65b9\u6269\u5c55\u6807\u8bb0\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u5728\u751f\u6210\u4ee3\u7801\u548c\u6570\u5b66\u65b9\u9762\u901f\u5ea6\u63d0\u9ad8\u4e86\u8fd15\u500d\uff0c\u5728\u901a\u7528\u804a\u5929\u548c\u77e5\u8bc6\u4efb\u52a1\u65b9\u9762\u901f\u5ea6\u63d0\u9ad8\u4e86\u8fd12.5\u500d\uff0c\u540c\u65f6\u6ca1\u6709\u635f\u5931\u4efb\u4f55\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u5728\u751f\u6210\u4ee3\u7801\u548c\u6570\u5b66\u65b9\u9762\u901f\u5ea6\u63d0\u9ad8\u4e86\u8fd15\u500d\uff0c\u5728\u901a\u7528\u804a\u5929\u548c\u77e5\u8bc6\u4efb\u52a1\u65b9\u9762\u901f\u5ea6\u63d0\u9ad8\u4e86\u8fd12.5\u500d\uff0c\u5e76\u4e14\u6ca1\u6709\u635f\u5931\u4efb\u4f55\u8d28\u91cf\u3002"}}
{"id": "2507.11642", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11642", "abs": "https://arxiv.org/abs/2507.11642", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "comment": null, "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u52bf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u8fd0\u52a8\u6765\u8bc6\u522b\u677f\u7403\u6bd4\u8d5b\u4e2d\u7684\u4eba\u7c7b\u610f\u56fe\uff0c\u5e76\u5728\u533a\u5206\u653b\u51fb\u6027\u548c\u9632\u5fa1\u6027\u5c04\u95e8\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u59ff\u52bf\u53ef\u4ee5\u7528\u4e8e\u63a8\u65ad\u5fc3\u7406\u72b6\u6001\uff0c\u4f8b\u5982\u75b2\u52b3\u548c\u8fd0\u52a8\u635f\u4f24\uff0c\u4f46\u7531\u4e8e\u5bf9\u4eba\u7c7b\u53d7\u8bd5\u8005\u6570\u636e\u7684\u654f\u611f\u6027\uff0c\u8fd9\u79cd\u89c6\u89c9\u8bca\u65ad\u9762\u4e34\u7740\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u5728\u677f\u7403\u6bd4\u8d5b\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u4ee5\u6d4b\u8bd5\u4ed6\u4eec\u5173\u4e8e\u59ff\u52bf\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u7684\u5047\u8bbe\uff0c\u4ee5\u79ef\u7d2f\u6765\u81ea\u7ecf\u5386\u5404\u79cd\u60c5\u7eea\u72b6\u6001\u7684\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u6570\u636e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u52bf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u6790\u6765\u8bc6\u522b\u4eba\u7c7b\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u73b0\u6709\u6570\u636e\u7edf\u8ba1\u4f5c\u4e3a\u5f31\u76d1\u7763\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u53d1\u73b0\u3002", "result": "\u5728\u533a\u5206\u653b\u51fb\u6027\u548c\u9632\u5fa1\u6027\u5c04\u95e8\u610f\u56fe\u65b9\u9762\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc7 75% \u7684 F1 \u5206\u6570\u548c\u8d85\u8fc7 80% \u7684 AUC-ROC\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u59ff\u52bf\u53ef\u4ee5\u63d0\u4f9b\u5f3a\u5927\u7684\u610f\u56fe\u63a8\u65ad\u4fe1\u53f7\uff0c\u5373\u4f7f\u5728\u6570\u636e\u7ba1\u9053\u5b58\u5728\u56fa\u6709\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u73b0\u6709\u7684\u6570\u636e\u7edf\u8ba1\u4f5c\u4e3a\u5f31\u76d1\u7763\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u53d1\u73b0\uff0c\u4e3a\u514b\u670d\u6570\u636e\u6807\u8bb0\u9650\u5236\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u8fd0\u52a8\u5206\u6790\u7684\u901a\u7528\u6280\u672f\uff0c\u5e76\u4e3a\u8de8\u4e0d\u540c\u9886\u57df\u7684\u884c\u4e3a\u5206\u6790\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.11735", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.11735", "abs": "https://arxiv.org/abs/2507.11735", "authors": ["Gabriele Carcassi", "Christine A. Aidala"], "title": "Non-additive measures for quantum probability?", "comment": "5 pages, prepared for the 20th International Conference on\n  Information Processing and Management of Uncertainty in Knowledge-Based\n  Systems, Lisbon, Portugal, July 22-26, 2024", "summary": "It is well-established that quantum probability does not follow classical\nKolmogorov probability calculus. Various approaches have been developed to\nloosen the axioms, of which the use of signed measures is the most successful\n(e.g. the Wigner quasiprobability distribution). As part of our larger effort\nAssumptions of Physics, we have been considering the various roles of measures,\nwhich are used in physics not only for probability, but also to quantify the\ncount of possible states and configurations. These measures play a crucial role\nin classical mechanics, as they effectively define its geometric structure. If\none tries to construct a parallel in quantum mechanics, the measure to quantify\nthe count of states turns out to be non-additive. The proper extension of\nprobability calculus may require the use of non-additive measures, which is\nsomething that, to our knowledge, has not yet been explored. The purpose of\nthis paper is to present the general idea and the open problems to an audience\nthat is knowledgeable of the subject of non-additive set functions, though not\nnecessarily in quantum physics, in the hope that it will spark helpful\ndiscussions. We go through the motivation in simple terms, which stems from the\nlink between the entropy of a uniform distribution and the logarithm of the\nmeasure associated to its support. If one extends this notion to quantum\nmechanics, the associated measure is non-additive. We will explore some\nproperties of this \"quantum measure\", its reasonableness in terms of the\nphysics, but its peculiarity on the math side. We will explore the need for a\nset of properties that can properly characterize the measure and a\ngeneralization of the Radon-Nikodym derivative to define a properly extended\nprobability calculus that reduces to the standard additive one on sets of\nphysically distinguishable cases (i.e. orthogonal measurement outcomes).", "AI": {"tldr": "Quantum probability is not classical. This paper explores using non-additive measures, a concept from classical physics, to quantify quantum states. It introduces a \"quantum measure\" and discusses its properties and potential for a new probability calculus, aiming to bridge classical and quantum physics.", "motivation": "The motivation stems from the fundamental difference between quantum and classical probability, where quantum probability does not follow the classical Kolmogorov calculus. While signed measures have been successful, this paper explores the less-trodden path of non-additive measures to quantify the count of quantum states, motivated by the non-additive nature of measures in quantum mechanics, unlike their additive counterparts in classical mechanics.", "method": "The paper proposes exploring non-additive measures in quantum mechanics, drawing parallels with classical mechanics where measures define geometric structure. It links the concept to the entropy of uniform distributions and the logarithm of the measure of their support. The study will investigate the properties of this \"quantum measure\" and explore generalizations of the Radon-Nikodym derivative to develop an extended probability calculus.", "result": "The paper aims to present the general idea and open problems related to non-additive measures in quantum mechanics. It will explore the properties of the proposed \"quantum measure\", discuss its physical reasonableness and mathematical peculiarities, and investigate the need for characterizing properties and a generalization of the Radon-Nikodym derivative for an extended probability calculus.", "conclusion": "This paper explores the potential of non-additive measures in quantum mechanics, proposing a generalization of probability calculus that reduces to the standard additive form for distinguishable cases. It aims to bridge the gap between classical and quantum probability by investigating the properties of a \"quantum measure\" and its mathematical and physical implications."}}
{"id": "2507.12293", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12293", "abs": "https://arxiv.org/abs/2507.12293", "authors": ["Shashikant Kumar", "Zixi Zhang", "Phanish Suryanarayana"], "title": "Ab initio study of flexoelectricity in MXene monolayers", "comment": "8 pages, 4 figures, 1 table", "summary": "We investigate flexoelectricity in MXene monolayers from first principles.\nSpecifically, we compute the transverse flexoelectric coefficients of 126 MXene\nmonolayers along their two principal directions using Kohn-Sham density\nfunctional theory. The values span a wide range from 0.19$e$ to 1.3$e$ and are\nnearly isotropic with respect to bending direction. The transition metal is\nfound to play a significant role in the flexoelectric response, with\nnitride-based MXenes consistently displaying larger coefficients than their\ncarbide counterparts. Moreover, the coefficients increase with structural\nthickness, but when normalized by the bending modulus, which is also computed\nfor all 126 monolayers, they exhibit the opposite trend.", "AI": {"tldr": "MXene\u5355\u5c42\u6750\u6599\u5177\u6709\u663e\u8457\u7684\u538b\u7535\u6548\u5e94\uff0c\u6c2e\u5316\u7269\u57faMXene\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u7814\u7a76MXene\u5355\u5c42\u6750\u6599\u7684\u538b\u7535\u6548\u5e94\u3002", "method": "\u4f7f\u7528Kohn-Sham\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u4e86126\u79cdMXene\u5355\u5c42\u6750\u6599\u6cbf\u4e24\u4e2a\u4e3b\u65b9\u5411\u7684\u6a2a\u5411\u538b\u7535\u7cfb\u6570\u3002", "result": "\u8ba1\u7b97\u5f97\u5230\u7684\u538b\u7535\u7cfb\u6570\u8303\u56f4\u4e3a0.19e\u81f31.3e\uff0c\u5e76\u4e14\u5728\u4e24\u4e2a\u4e3b\u65b9\u5411\u4e0a\u51e0\u4e4e\u5404\u5411\u540c\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fc7\u6e21\u91d1\u5c5e\u5bf9\u538b\u7535\u54cd\u5e94\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6c2e\u5316\u7269\u57faMXene\u7684\u538b\u7535\u7cfb\u6570\u666e\u904d\u9ad8\u4e8e\u78b3\u5316\u7269\u57faMXene\u3002\u6b64\u5916\uff0c\u538b\u7535\u7cfb\u6570\u968f\u7ed3\u6784\u539a\u5ea6\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u4f46\u7ecf\u8fc7\u5f2f\u66f2\u6a21\u91cf\u5f52\u4e00\u5316\u540e\uff0c\u8d8b\u52bf\u5219\u76f8\u53cd\u3002", "conclusion": "MXene\u5355\u5c42\u6750\u6599\u7684\u538b\u7535\u6548\u5e94\u5177\u6709\u5f88\u5927\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6c2e\u5316\u7269\u57faMXene\u4e2d\uff0c\u5176\u7cfb\u6570\u4e0e\u7ed3\u6784\u539a\u5ea6\u6210\u6b63\u6bd4\uff0c\u4f46\u4e0e\u5f2f\u66f2\u6a21\u91cf\u6210\u53cd\u6bd4\u3002"}}
{"id": "2507.12210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12210", "abs": "https://arxiv.org/abs/2507.12210", "authors": ["Jialiang Zhu", "Sanoopkumar P. S.", "Arman Farhang"], "title": "PAPR of DFT-s-OTFS with Pulse Shaping", "comment": null, "summary": "Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average\npower ratio (PAPR) when the number of Doppler bins is large. To address this\nissue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed\nby applying DFT spreading across the Doppler dimension. This paper presents a\nthorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different\npulse shaping filters and resource allocation strategies. Specifically, we\nderive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler\nresource allocation schemes. Our analysis reveals that DFT-s-OTFS with\ninterleaved allocation yields a lower PAPR than that of block allocation.\nFurthermore, we show that interleaved allocation produces a periodic\ntime-domain signal composed of repeated quadrature amplitude modulated (QAM)\nsymbols which simplifies the transmitter design. Based on our analytical\nresults, the root raised cosine (RRC) pulse generally results in a higher\nmaximum PAPR compared to the rectangular pulse. Simulation results confirm the\nvalidity of the derived PAPR upper bounds. Furthermore, we also demonstrate\nthrough BER simulation analysis that the DFT-s-OTFS gives the same performance\nas OTFS without DFT spreading.", "AI": {"tldr": "DFT-s-OTFS\u901a\u8fc7DFT\u6269\u5c55\u7f13\u89e3\u4e86OTFS\u5728\u9ad8 dolay\u0131\u60c5\u51b5\u4e0b\u7684PAPR\u95ee\u9898\u3002\u4ea4\u7ec7\u5206\u914d\u6bd4\u5757\u5206\u914d\u6548\u679c\u66f4\u597d\uff0cRRC\u8109\u51b2\u6bd4\u77e9\u5f62\u8109\u51b2PAPR\u66f4\u9ad8\u3002DFT-s-OTFS\u7684BER\u6027\u80fd\u4e0eOTFS\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3OTFS\u5728\u5927\u591a\u666e\u52d2\u5355\u5143\u6570\u4e0bPAPR\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a8\u5bfcDFT-s-OTFS\u5728\u4e0d\u540c\u8109\u51b2\u6574\u5f62\u6ee4\u6ce2\u5668\u548c\u8d44\u6e90\u5206\u914d\u7b56\u7565\u4e0b\u7684PAPR\u4e0a\u9650\uff0c\u5e76\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u63a8\u5bfc\u4e86\u4ea4\u7ec7\u548c\u5757\u4e24\u79cd\u90fd\u535c\u52d2\u8d44\u6e90\u5206\u914d\u65b9\u6848\u4e0b\u7684DFT-s-OTFS\u7684PAPR\u4e0a\u9650\u3002\u4ea4\u7ec7\u5206\u914d\u65b9\u6848\u6bd4\u5757\u5206\u914d\u65b9\u6848\u5177\u6709\u66f4\u4f4e\u7684PAPR\u3002", "conclusion": "DFT-s-OTFS\u5728\u4ea4\u7ec7\u5206\u914d\u65b9\u6848\u4e0b\u53ef\u4ee5\u83b7\u5f97\u6bd4\u5757\u5206\u914d\u65b9\u6848\u66f4\u4f4e\u7684PAPR\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ea7\u751f\u5468\u671f\u6027\u7684\u65f6\u57df\u4fe1\u53f7\uff0c\u4ece\u800c\u7b80\u5316\u53d1\u5c04\u673a\u8bbe\u8ba1\u3002\u4e0e\u77e9\u5f62\u8109\u51b2\u76f8\u6bd4\uff0cRRC\u8109\u51b2\u901a\u5e38\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u6700\u5927PAPR\u3002DFT-s-OTFS\u5728BER\u6027\u80fd\u4e0a\u4e0e\u672a\u8fdb\u884cDFT\u6269\u5c55\u7684OTFS\u76f8\u5f53\u3002"}}
{"id": "2507.11918", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.11918", "abs": "https://arxiv.org/abs/2507.11918", "authors": ["Yi-Hsien Wu", "Leon C. Camenzind", "Patrick B\u00fctler", "Ik Kyeong Jin", "Akito Noiri", "Kenta Takeda", "Takashi Nakajima", "Takashi Kobayashi", "Giordano Scappucci", "Hsi-Sheng Goan", "Seigo Tarucha"], "title": "Simultaneous High-Fidelity Single-Qubit Gates in a Spin Qubit Array", "comment": "36 pages (including Extended Data), 5 figures, 10 Extended Data\n  Figures, 1 Extended Data Table, 8 pages Supplementary Information", "summary": "Silicon spin qubits are a promising platform for scalable quantum computing\ndue to their compatibility with industrial semiconductor fabrication and the\nrecent scaling to multi-qubit devices. Control fidelities above the 99%\nfault-tolerant threshold are routinely achieved, but extending high-fidelity\ncontrol to simultaneous multi-qubit operation remains a major challenge. We\ndemonstrate high-fidelity, fully parallel control of five silicon spin qubits\nusing a single shared microwave line. Using tailored control pulses, all qubits\nachieve primitive $\\pi/2$ gate fidelities well above 99.99%, with some\napproaching 99.999%, exceeding previously reported fidelities in silicon spin\nqubits. These fidelities are mostly preserved during simultaneous operation of\nup to three qubits, and remain at the practical fault-tolerant threshold of\n99.9% even during fully parallel five-qubit operation. This performance is\nenabled by a calibration scheme that compensates drive-induced phase shifts\nusing only pairwise calibrations, scaling quadratically with qubit number and\navoiding exponential overhead. By reducing the number of impedance-controlled\nmicrowave lines, our approach addresses a key architectural bottleneck and\noffers a scalable control strategy for high-fidelity operation in large spin\nqubit arrays.", "AI": {"tldr": "\u901a\u8fc7\u5355\u4e2a\u5fae\u6ce2\u7ebf\u5b9e\u73b0\u4e86\u4e94\u4e2a\u7845\u91cf\u5b50\u6bd4\u7279\u7684\u9ad8\u4fdd\u771f\u5ea6\u5e76\u884c\u63a7\u5236\uff0c\u514b\u670d\u4e86\u6269\u5c55\u74f6\u9888\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u53ef\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u4e2d\uff0c\u7845\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u5728\u6269\u5c55\u5230\u591a\u91cf\u5b50\u6bd4\u7279\u5668\u4ef6\u65f6\uff0c\u5b9e\u73b0\u540c\u65f6\u591a\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u7684\u9ad8\u4fdd\u771f\u5ea6\u63a7\u5236\u8fd9\u4e00\u4e3b\u8981\u6311\u6218\u3002", "method": "\u5229\u7528\u5355\u4e2a\u5171\u4eab\u5fae\u6ce2\u7ebf\u548c\u5b9a\u5236\u63a7\u5236\u8109\u51b2\uff0c\u7ed3\u5408\u4ec5\u5305\u542b\u6210\u5bf9\u6821\u51c6\u7684\u6821\u51c6\u65b9\u6848\uff0c\u4ee5\u8865\u507f\u9a71\u52a8\u5f15\u8d77\u7684\u76f8\u4f4d\u504f\u79fb\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e94\u4e2a\u7845\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u5168\u90e8\u5e76\u884c\u9ad8\u4fdd\u771f\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u5bf9\u4e94\u4e2a\u7845\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u5168\u90e8\u5e76\u884c\u63a7\u5236\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4e8e99.99%\u7684\u539f\u59cb\u03c0/2\u95e8\u4fdd\u771f\u5ea6\uff0c\u90e8\u5206\u91cf\u5b50\u6bd4\u7279\u63a5\u8fd199.999%\u3002\u5373\u4f7f\u5728\u540c\u65f6\u64cd\u4f5c\u591a\u8fbe\u4e09\u4e2a\u91cf\u5b50\u6bd4\u7279\u65f6\uff0c\u8fd9\u4e9b\u4fdd\u771f\u5ea6\u4e5f\u57fa\u672c\u5f97\u4ee5\u4fdd\u6301\uff0c\u5e76\u4e14\u5728\u5168\u90e8\u5e76\u884c\u64cd\u4f5c\u4e94\u4e2a\u91cf\u5b50\u6bd4\u7279\u65f6\uff0c\u4fdd\u771f\u5ea6\u4ecd\u4fdd\u6301\u572899.9%\u7684\u5b9e\u7528\u5bb9\u9519\u9608\u503c\u4e4b\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5355\u4e2a\u5171\u4eab\u5fae\u6ce2\u7ebf\u548c\u5b9a\u5236\u63a7\u5236\u8109\u51b2\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u4e94\u4e2a\u7845\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u5168\u90e8\u5e76\u884c\u9ad8\u4fdd\u771f\u5ea6\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u786c\u4ef6\u590d\u6742\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u6bd4\u7279\u7684\u63a7\u5236\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21\u91cf\u5b50\u8ba1\u7b97\u9635\u5217\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2507.12186", "categories": ["cs.AI", "I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12186", "abs": "https://arxiv.org/abs/2507.12186", "authors": ["Edward Kim", "Hanna Kurniawati"], "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "AI": {"tldr": "PORP \u662f\u4e00\u79cd\u521b\u65b0\u7684\u5728\u7ebf POMDP \u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u91c7\u6837\u548c\u6e10\u8fdb\u5f0f\u7b56\u7565\u66f4\u65b0\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u7ebf\u89c4\u5212\u4e2d\u7684\u91c7\u6837\u7a00\u758f\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c06\u6027\u80fd\u635f\u5931\u754c\u5b9a\u4e3a\u91c7\u6837\u8fd1\u4f3c\u8bef\u5dee\u7684\u5e73\u5747\u503c\uff0c\u800c\u4e0d\u662f\u901a\u5e38\u7684\u6700\u5927\u503c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u610f\u5728\u7ebf\u8fd1\u4f3c POMDP \u6c42\u89e3\u5668\u2014\u2014\u90e8\u5206\u53ef\u89c2\u5bdf\u53c2\u8003\u7b56\u7565\u7f16\u7a0b (PORP)\uff0c\u8be5\u6c42\u89e3\u5668\u80fd\u591f\u6df1\u5165\u91c7\u6837\u6709\u610f\u4e49\u7684\u672a\u6765\u5386\u53f2\uff0c\u540c\u65f6\u5f3a\u5236\u6267\u884c\u6e10\u8fdb\u5f0f\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u8be5\u7b97\u6cd5\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u8868\u660e\u8be5\u6c42\u89e3\u5668\u5728\u52a8\u6001\u6f14\u5316\u73af\u5883\u4e2d\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u5728\u7ebf\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6c42\u89e3\u5668\u5728 Corsica \u5730\u533a\u7684\u76f4\u5347\u673a\u7d27\u6025\u60c5\u51b5\u7b49\u5927\u89c4\u6a21\u52a8\u6001\u6f14\u5316\u73af\u5883\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u5728\u7ebf\u57fa\u51c6\u3002"}}
{"id": "2507.12259", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12259", "abs": "https://arxiv.org/abs/2507.12259", "authors": ["Lihan Lian", "Yuxin Tong", "Uduak Inyang-Udoh"], "title": "Neural Co-state Regulator: A Data-Driven Paradigm for Real-time Optimal Control with Input Constraints", "comment": null, "summary": "We propose a novel unsupervised learning framework for solving nonlinear\noptimal control problems (OCPs) with input constraints in real-time. In this\nframework, a neural network (NN) learns to predict the optimal co-state\ntrajectory that minimizes the control Hamiltonian for a given system, at any\nsystem's state, based on the Pontryagin's Minimum Principle (PMP).\nSpecifically, the NN is trained to find the norm-optimal co-state solution that\nsimultaneously satisfies the nonlinear system dynamics and minimizes a\nquadratic regulation cost. The control input is then extracted from the\npredicted optimal co-state trajectory by solving a quadratic program (QP) to\nsatisfy input constraints and optimality conditions. We coin the term neural\nco-state regulator (NCR) to describe the combination of the co-state NN and\ncontrol input QP solver. To demonstrate the effectiveness of the NCR, we\ncompare its feedback control performance with that of an expert nonlinear model\npredictive control (MPC) solver on a unicycle model. Because the NCR's training\ndoes not rely on expert nonlinear control solvers which are often suboptimal,\nthe NCR is able to produce solutions that outperform the nonlinear MPC solver\nin terms of convergence error and input trajectory smoothness even for system\nconditions that are outside its original training domain. At the same time, the\nNCR offers two orders of magnitude less computational time than the nonlinear\nMPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u534f\u540c\u72b6\u6001\u8c03\u8282\u5668\uff08NCR\uff09\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u89e3\u51b3\u975e\u7ebf\u6027\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6700\u4f18\u534f\u540c\u72b6\u6001\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u4e8c\u6b21\u89c4\u5212\u5904\u7406\u8f93\u5165\u7ea6\u675f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cNCR\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u975e\u7ebf\u6027MPC\u3002", "motivation": "\u4e3a\u4e86\u5728\u5b9e\u65f6\u60c5\u51b5\u4e0b\u89e3\u51b3\u5177\u6709\u8f93\u5165\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff08OCPs\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u6765\u9884\u6d4b\u6700\u4f18\u534f\u540c\u72b6\u6001\u8f68\u8ff9\uff0c\u4ee5\u6700\u5c0f\u5316\u63a7\u5236\u54c8\u5bc6\u987f\u91cf\uff0c\u5e76\u7ed3\u5408\u4e8c\u6b21\u89c4\u5212\uff08QP\uff09\u6c42\u89e3\u5668\u6765\u6ee1\u8db3\u8f93\u5165\u7ea6\u675f\u548c\u6700\u4f18\u6027\u6761\u4ef6\u3002", "result": "NCR\u5728\u6536\u655b\u8bef\u5dee\u548c\u8f93\u5165\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u65b9\u9762\u4f18\u4e8e\u975e\u7ebf\u6027MPC\u6c42\u89e3\u5668\uff0c\u5e76\u4e14\u8ba1\u7b97\u65f6\u95f4\u6bd4\u975e\u7ebf\u6027MPC\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u534f\u540c\u72b6\u6001\u8c03\u8282\u5668\uff08NCR\uff09\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u975e\u7ebf\u6027\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6536\u655b\u8bef\u5dee\u548c\u8f93\u5165\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u65b9\u9762\u4f18\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u4e5f\u5927\u5927\u51cf\u5c11\u3002"}}
{"id": "2507.12106", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.12106", "abs": "https://arxiv.org/abs/2507.12106", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "comment": "18 pages, 6 Figures", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "AI": {"tldr": "\u8be5\u9879\u76ee\u901a\u8fc7\u96c6\u6210\u7269\u8054\u7f51\u548c\u6570\u636e\u9a71\u52a8\u5e73\u53f0\uff0c\u5229\u7528\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u4f18\u5316\u57ce\u5e02\u7eff\u5730\u7ba1\u7406\uff0c\u63d0\u9ad8\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u57ce\u5e02\u516c\u5171\u7eff\u5730\u7684\u6709\u6548\u8bbe\u8ba1\u548c\u7ba1\u7406\u5bf9\u4fc3\u8fdb\u57ce\u5e02\u4eba\u53e3\u7684\u5065\u5eb7\u548c\u798f\u7949\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u533a\u57df\u662f\u57ce\u5e02\u751f\u6001\u7cfb\u7edf\u7684\u201c\u7eff\u8272\u80ba\u201d\uff0c\u901a\u8fc7\u63d0\u4f9b\u751f\u6001\u7cfb\u7edf\u670d\u52a1\u6765\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u7269\u8054\u7f51\u7cfb\u7edf\u548c\u6570\u636e\u9a71\u52a8\u7684\u6cbb\u7406\u5e73\u53f0\uff0c\u5229\u7528\u6811\u8bed\u8005\u4f20\u611f\u5668\u3001\u571f\u58e4\u6e7f\u5ea6\u548c\u6c34\u5206\u6f5c\u529b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u9884\u6d4b\u6a21\u578b\u548c\u7269\u8054\u7f51\u4f20\u611f\u5668\u7684\u5b9e\u65f6\u6570\u636e\uff0c\u5bf9\u57ce\u5e02\u7eff\u5730\u8fdb\u884c\u5b9e\u65f6\u76d1\u63a7\u548c\u667a\u80fd\u7ba1\u7406\uff0c\u4f18\u5316\u704c\u6e89\uff0c\u5e76\u63d0\u4f9b\u81ea\u5b9a\u4e49\u8b66\u62a5\u3002", "result": "\u90e8\u7f72\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u5e73\u53f0\uff0c\u652f\u6301\u57ce\u5e02\u7eff\u5730\u7ba1\u7406\u8005\u3001\u6280\u672f\u4e13\u5bb6\u548c\u8fd0\u8425\u4eba\u5458\u8fdb\u884c\u6574\u4f53\u7684\u5b9e\u65f6\u51b3\u7b56\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6811\u6728\u548c\u7eff\u5730\u5065\u5eb7\u72b6\u51b5\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u5e76\u4f18\u5316\u4e86\u516c\u56ed\u704c\u6e89\u3002", "conclusion": "\u8be5\u9879\u76ee\u5c55\u793a\u4e86\u6570\u5b57\u5316\u3001\u7269\u8054\u7f51\u4f20\u611f\u5668\u878d\u5408\u548c\u6280\u672f\u521b\u65b0\u5982\u4f55\u652f\u6301\u53ef\u6301\u7eed\u57ce\u5e02\u6cbb\u7406\uff0c\u589e\u5f3a\u73af\u5883\u97e7\u6027\u5e76\u63d0\u9ad8\u5e02\u6c11\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2507.11940", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11940", "abs": "https://arxiv.org/abs/2507.11940", "authors": ["Kanghyun Ryu", "Minjun Sung", "Piyush Gupta", "Jovin D'sa", "Faizan M. Tariq", "David Isele", "Sangjae Bae"], "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving", "comment": "To be published in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Motion planning for autonomous vehicles (AVs) in dense traffic is\nchallenging, often leading to overly conservative behavior and unmet planning\nobjectives. This challenge stems from the AVs' limited ability to anticipate\nand respond to the interactive behavior of surrounding agents. Traditional\ndecoupled prediction and planning pipelines rely on non-interactive predictions\nthat overlook the fact that agents often adapt their behavior in response to\nthe AV's actions. To address this, we propose Interaction-Aware Neural\nNetwork-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which\nenables interactive trajectory planning by predicting how surrounding agents\nmay react to each control sequence sampled by MPPI. To improve performance in\nstructured lane environments, we introduce a spline-based prior for the MPPI\nsampling distribution, enabling efficient lane-changing behavior. We evaluate\nIANN-MPPI in a dense traffic merging scenario, demonstrating its ability to\nperform efficient merging maneuvers. Our project website is available at\nhttps://sites.google.com/berkeley.edu/iann-mppi", "AI": {"tldr": "A new control method, IANN-MPPI, allows autonomous vehicles to plan trajectories interactively by predicting how other vehicles react to the AV's potential actions, leading to more efficient maneuvers in dense traffic.", "motivation": "Autonomous vehicles (AVs) in dense traffic exhibit overly conservative behavior due to limited ability to anticipate and respond to interactive behavior of surrounding agents. Traditional methods overlook agents adapting their behavior in response to AV actions.", "method": "The proposed method is Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which incorporates a spline-based prior for the MPPI sampling distribution to enhance lane-changing behavior. It addresses limitations of traditional methods by predicting interactive behavior, unlike non-interactive predictions.", "result": "IANN-MPPI was evaluated in a dense traffic merging scenario, demonstrating its ability to perform efficient merging maneuvers.", "conclusion": "IANN-MPPI enables interactive trajectory planning by predicting surrounding agents' reactions to control sequences, improving performance in dense traffic merging scenarios and allowing efficient merging maneuvers."}}
{"id": "2507.11862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11862", "abs": "https://arxiv.org/abs/2507.11862", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u9886\u57df\u8fc1\u79fb\u3001\u6570\u636e\u878d\u5408\u548c\u5c11\u6837\u672c\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6cd5\u5f8b-\u4f20\u8bb0\u8fc1\u79fb\u6548\u679c\u597d\uff0c\u533b\u7597\u8fc1\u79fb\u6548\u679c\u5dee\uff0c\u878d\u5408\u6536\u76ca\u56e0\u9886\u57df\u800c\u5f02\uff0c\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u5c11\u6837\u672c\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8bc6\u522b\u7387\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u662f\u81ea\u52a8\u5316\u6587\u672c\u533f\u540d\u5316\u7684\u6838\u5fc3\u3002\u672c\u6587\u7814\u7a76\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u533b\u7597\uff08I2B2\uff09\u3001\u6cd5\u5f8b\uff08TAB\uff09\u548c\u4f20\u8bb0\uff08Wikipedia\uff09\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff1a\u9886\u57df\u5185\u6027\u80fd\u3001\u8de8\u9886\u57df\u8fc1\u79fb\u6027\u3001\u878d\u5408\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u80fd\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u4f20\u8bb0\u6587\u672c\uff0c\u800c\u533b\u7597\u9886\u57df\u6570\u636e\u5219\u96be\u4ee5\u63a5\u53d7\u5916\u90e8\u8fc1\u79fb\u3002\u878d\u5408\u7684\u6536\u76ca\u56e0\u9886\u57df\u800c\u5f02\uff0c\u5728\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\uff0c\u4ec5\u970010%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8bc6\u522b\u7387\u3002", "conclusion": "\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u53ef\u7528\u4e8ePII\u8bc6\u522b\uff0c\u5176\u4e2d\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u5bf9\u4f20\u8bb0\u6587\u672c\u7684\u8fc1\u79fb\u6548\u679c\u826f\u597d\uff0c\u800c\u533b\u7597\u9886\u57df\u6570\u636e\u7684\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\u3002\u6570\u636e\u878d\u5408\u7684\u6536\u76ca\u5177\u6709\u9886\u57df\u7279\u5f02\u6027\uff0c\u5e76\u4e14\u5728\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\uff0c\u4ec5\u4f7f\u752810%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8bc6\u522b\u3002"}}
{"id": "2507.11653", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11653", "abs": "https://arxiv.org/abs/2507.11653", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "AI": {"tldr": "VISTA \u662f\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5bf9\u8c61\u7684\u5206\u5272\u548c\u8ddf\u8e2a\u4ee5\u53ca\u5b50\u56fe\u5bf9\u5e94\u641c\u7d22\u6765\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6311\u6218\u3002\u5b83\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u90fd\u80fd\u5b9e\u73b0\u4e00\u81f4\u7684\u5b9a\u4f4d\uff0c\u5e76\u5728\u6027\u80fd\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5168\u5c40\u5b9a\u4f4d\u5bf9\u4e8e\u81ea\u4e3b\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4ee3\u7406\u5fc5\u987b\u5728\u7531\u4e0d\u540c\u4f1a\u8bdd\u6216\u53e6\u4e00\u4e2a\u4ee3\u7406\u751f\u6210\u7684\u5730\u56fe\u4e2d\u8fdb\u884c\u5b9a\u4f4d\u7684\u60c5\u51b5\u4e0b\uff0c\u56e0\u4e3a\u4ee3\u7406\u901a\u5e38\u4e0d\u77e5\u9053\u53c2\u8003\u7cfb\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f20\u7edf\u573a\u666f\u8bc6\u522b\u65b9\u6cd5\u7684\u5df2\u77e5\u5931\u6548\u6a21\u5f0f\uff08\u5982\u89c6\u70b9\u53d8\u5316\u3001\u5b63\u8282\u53d8\u5316\u3001\u7a7a\u95f4\u6df7\u6dc6\u548c\u906e\u6321\uff09\u5f15\u8d77\u7684\u89c6\u89c9\u53d8\u5316\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u6b64\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "VISTA (View-Invariant Segmentation-Based Tracking for Frame Alignment) \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u5f00\u653e\u96c6\u3001\u5355\u76ee\u7684\u5168\u5c40\u5b9a\u4f4d\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\uff1a1) \u4e00\u4e2a\u524d\u7aef\u3001\u57fa\u4e8e\u5bf9\u8c61\u7684\u5206\u5272\u548c\u8ddf\u8e2a\u6d41\u7a0b\uff0c\u4ee5\u53ca 2) \u4e00\u4e2a\u5b50\u56fe\u5bf9\u5e94\u641c\u7d22\uff0c\u5229\u7528\u73af\u5883\u5730\u56fe\u4e4b\u95f4\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u6765\u5bf9\u9f50\u8f66\u8f86\u5750\u6807\u7cfb\u3002", "result": "VISTA \u5728\u5b63\u8282\u6027\u548c\u659c\u89d2\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u53ec\u56de\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86 69%\u3002\u6b64\u5916\uff0cVISTA \u7ef4\u62a4\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u4ec5\u5305\u542b 0.6% \u5185\u5b58\u7684\u57fa\u4e8e\u5bf9\u8c61\u7684\u5730\u56fe\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u65f6\u5b9e\u73b0\u3002", "conclusion": "VISTA \u6846\u67b6\u80fd\u591f\u5728\u4e0d\u540c\u7684\u6444\u50cf\u5934\u89c6\u89d2\u548c\u5b63\u8282\u53d8\u5316\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684\u5b9a\u4f4d\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u7279\u5b9a\u9886\u57df\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u5728\u5b63\u8282\u6027\u548c\u659c\u89d2\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u53ec\u56de\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86 69%\u3002\u6b64\u5916\uff0cVISTA \u7ef4\u62a4\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u4ec5\u5305\u542b 0.6% \u5185\u5b58\u7684\u57fa\u4e8e\u5bf9\u8c61\u7684\u5730\u56fe\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u65f6\u5b9e\u73b0\u3002"}}
{"id": "2507.11766", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.11766", "abs": "https://arxiv.org/abs/2507.11766", "authors": ["Paul E. Lammert"], "title": "The Gorini-Kossakowski-Sudarshan-Lindblad generation theorem", "comment": "14 pates", "summary": "The Lindblad equation embodies a fundamental paradigm of the quantum theory\nof open systems, and the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL)\ngeneration theorem says precisely which superoperators can appear on its\nright-hand side. These are the generators of completely positive\ntrace-preserving (or nonincreasing) semigroups. A complete exposition of this\ntheorem is given. The finite-dimensional case is handled using a form of\nJamio\\l{}kowski transform. The treatment requires no previous knowledge of\ncomplete positivity and obtains the Choi-Kraus presentation along the way. The\n(separable) infinite-dimensional case is handled by means of a sequence of\nfinite-dimensional approximations, using the finite-dimensional case as a\ncrucial tool. An extension to time-dependent generator is also given.", "AI": {"tldr": "The paper explains the GKSL theorem for quantum systems, showing how to generate certain types of evolution (semigroups) using specific mathematical tools, including an extension for systems that change over time.", "motivation": "The paper aims to explain the fundamental paradigm of the Lindblad equation in quantum theory, specifically focusing on the GKSL generation theorem which defines the generators of completely positive trace-preserving semigroups.", "method": "The finite-dimensional case is addressed using a Jamio\nl{}kowski transform, while the infinite-dimensional case is handled through finite-dimensional approximations. An extension to time-dependent generators is also included.", "result": "The paper successfully demonstrates the GKSL generation theorem for both finite and infinite-dimensional cases, deriving the Choi-Kraus presentation and extending the results to time-dependent generators.", "conclusion": "This paper provides a comprehensive exposition of the GKSL generation theorem, detailing the conditions under which superoperators can generate completely positive trace-preserving semigroups for open quantum systems."}}
{"id": "2507.11688", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11688", "abs": "https://arxiv.org/abs/2507.11688", "authors": ["Travis Pence", "Daisuke Yamada", "Vikas Singh"], "title": "Composing Linear Layers from Irreducibles", "comment": "27 Pages, 13 Tables, 8 Figures", "summary": "Contemporary large models often exhibit behaviors suggesting the presence of\nlow-level primitives that compose into modules with richer functionality, but\nthese fundamental building blocks remain poorly understood. We investigate this\ncompositional structure in linear layers by asking: can we identify/synthesize\nlinear transformations from a minimal set of geometric primitives? Using\nClifford algebra, we show that linear layers can be expressed as compositions\nof bivectors -- geometric objects encoding oriented planes -- and introduce a\ndifferentiable algorithm that decomposes them into products of rotors. This\nconstruction uses only O(log^2 d) parameters, versus O(d^2) required by dense\nmatrices. Applied to the key, query, and value projections in LLM attention\nlayers, our rotor-based layers match the performance of strong baselines such\nas block-Hadamard and low-rank approximations. Our findings provide an\nalgebraic perspective on how these geometric primitives can compose into\nhigher-level functions within deep models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528 Clifford \u4ee3\u6570\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528 O(log^2 d) \u53c2\u6570\u5206\u89e3\u7ebf\u6027\u5c42\u7684\u65b9\u6cd5\uff0c\u4e0e O(d^2) \u7684\u5bc6\u96c6\u77e9\u9635\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u867d\u7136\u5927\u578b\u6a21\u578b\u8868\u73b0\u51fa\u4f3c\u4e4e\u662f\u7531\u4f4e\u7ea7\u539f\u8bed\u7ec4\u6210\u7684\u6a21\u5757\u5316\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u57fa\u672c\u6784\u5efa\u5757\u7684\u7406\u89e3\u7a0b\u5ea6\u5f88\u5dee\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bc6\u522b/\u5408\u6210\u7531\u4e00\u7ec4\u6700\u5c0f\u7684\u51e0\u4f55\u539f\u8bed\u7ec4\u6210\u7684\u7ebf\u6027\u53d8\u6362\u6765\u63a2\u7d22\u7ebf\u6027\u5c42\u4e2d\u7684\u7ec4\u5408\u7ed3\u6784\u3002", "method": "\u4f7f\u7528 Clifford \u4ee3\u6570\uff0c\u7814\u7a76\u8868\u660e\u7ebf\u6027\u5c42\u53ef\u4ee5\u8868\u793a\u4e3a\u53cc\u5411\u91cf\uff08\u7f16\u7801\u5b9a\u5411\u5e73\u9762\u7684\u51e0\u4f55\u5bf9\u8c61\uff09\u7684\u7ec4\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7b97\u6cd5\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u65cb\u8f6c\u5411\u91cf\u7684\u4e58\u79ef\u3002", "result": "\u4e0e\u5757\u54c8\u8fbe\u739b\u548c\u4f4e\u79e9\u8fd1\u4f3c\u7b49\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u57fa\u4e8e\u65cb\u8f6c\u5411\u91cf\u7684\u5c42\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u5c42\u7684\u952e\u3001\u67e5\u8be2\u548c\u503c\u6295\u5f71\u4e2d\u5339\u914d\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u51e0\u4f55\u539f\u8bed\u5982\u4f55\u7ec4\u5408\u6210\u66f4\u9ad8\u7ea7\u522b\u7684\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ee3\u6570\u89c6\u89d2\u3002"}}
{"id": "2507.12324", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.12324", "abs": "https://arxiv.org/abs/2507.12324", "authors": ["Haoshen Ye", "Shuai Dong"], "title": "Magnetic and ferroelectric phase diagram of twisted CrI$_3$ layers", "comment": "9 pages, 9 figures", "summary": "Twisting layers provide a rich ore for exotic physics in low dimensions.\nDespite the abundant discoveries of twistronics from the aspect of electronic\nstructures, ferroic moir\\'e textures are more plain and thus less concerned.\nRigid lattice models are straightforward which can give a rough but intuitional\ndescription in most cases. However, taking CrI$_3$ as a model system, here we\nwill demonstrate that the interlayer stacking potential can spontaneously lead\nto structural relaxation, which plays a vital role to understand the ferroicity\nin the twisted superlattices. The magnetic ground state is sensitive to the\nstacking mode and twisting angles, which can be seriously affected by the\nstructural relaxation. In particular, the expected magnetic bubbles are\nannihilated in its bilayer. In contrast, due to topological protection, the\nferroelectric vortices are more robust to structural relaxation, as well as\ntwisting angle and thickness.Due to the universal existence of spontaneous\nstructural relaxation in twisted superlattices, our work may lead to a general\nrevisitation of emerging physics of twistronics.", "AI": {"tldr": "CrI3 \u7684\u626d\u8f6c\u8d85\u6676\u683c\u4e2d\u7684\u7ed3\u6784\u5f1b\u8c6b\u4f1a\u5f71\u54cd\u78c1\u6027\uff0c\u4f46\u5bf9\u94c1\u7535\u6027\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u7535\u5b50\u7ed3\u6784\u65b9\u9762\u7684\u626d\u8f6c\u5b50\u5b66\u5df2\u6709\u5927\u91cf\u53d1\u73b0\uff0c\u4f46\u94c1\u6027\u83ab\u5c14\u7eb9\u7406\u8f83\u4e3a\u5e73\u6de1\uff0c\u56e0\u6b64\u53d7\u5230\u7684\u5173\u6ce8\u8f83\u5c11\u3002", "method": "\u901a\u8fc7 CrI3 \u6a21\u578b\u7cfb\u7edf\uff0c\u8bc1\u660e\u5c42\u95f4\u5806\u53e0\u52bf\u80fd\u81ea\u53d1\u5bfc\u81f4\u7ed3\u6784\u5f1b\u8c6b\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u626d\u8f6c\u8d85\u6676\u683c\u4e2d\u7684\u94c1\u6027\u81f3\u5173\u91cd\u8981\u3002", "result": "\u5c42\u95f4\u5806\u53e0\u52bf\u80fd\u81ea\u53d1\u5bfc\u81f4\u7ed3\u6784\u5f1b\u8c6b\uff0c\u5f71\u54cd\u626d\u8f6c\u8d85\u6676\u683c\u4e2d\u7684\u78c1\u57fa\u6001\u3002 \u626d\u8f6c\u53cc\u5c42 CrI3 \u4e2d\u7684\u78c1\u6ce1\u88ab\u6d88\u9664\uff0c\u800c\u94c1\u7535\u6da1\u65cb\u5219\u56e0\u62d3\u6251\u4fdd\u62a4\u800c\u66f4\u52a0\u9c81\u68d2\u3002", "conclusion": "\u5ffd\u7565\u626d\u8f6c\u53cc\u5c42 CrI3 \u4e2d\u78c1\u6ce1\u7684\u9884\u671f\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u62d3\u6251\u4fdd\u62a4\uff0c\u94c1\u7535\u6da1\u65cb\u5bf9\u7ed3\u6784\u5f1b\u8c6b\u3001\u626d\u8f6c\u89d2\u548c\u539a\u5ea6\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.12211", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12211", "abs": "https://arxiv.org/abs/2507.12211", "authors": ["Sa\u00fal Fenollosa"], "title": "Cell Sensing: Traffic detection", "comment": "39 pages, 16 figures. Student project report at the\n  Telecommunications Circuits Laboratory (TCL), EPFL. Supervised by Prof.\n  Andreas Burg and Sitian Li", "summary": "This work presents a passive sensing system for traffic monitoring using\nambient Long Term Evolution (LTE) signals as a non-intrusive and scalable\nalternative to traditional surveillance methods. The approach employs a\ndual-receiver architecture analyzing Channel State Information (CSI) to isolate\ndifferential Doppler shifts induced by moving targets, effectively mitigating\nhardware-induced phase impairments. Implemented with a Software Defined Radio\n(SDR) platform and srsRAN software, the system demonstrated over 90% detection\naccuracy for speeds above 6000 mm/min in controlled indoor tests, and provided\nreliable speed estimations for pedestrians and vehicles in outdoor evaluations.\nDespite challenges at low speeds, directional ambiguity, and multipath fading\nin urban settings, the results validate LTE-based passive sensing as a feasible\ntraffic monitoring method, identifying critical areas for future research such\nas angle-of-arrival (AoA) integration, machine learning, and real-time embedded\nsystem development.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LTE\u4fe1\u53f7\u8fdb\u884c\u4ea4\u901a\u76d1\u63a7\u7684\u88ab\u52a8\u4f20\u611f\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u73af\u5883LTE\u4fe1\u53f7\u7684\u88ab\u52a8\u4f20\u611f\u7cfb\u7edf\uff0c\u4f5c\u4e3a\u4f20\u7edf\u76d1\u63a7\u65b9\u6cd5\u7684\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u73af\u5883LTE\u4fe1\u53f7\u7684\u88ab\u52a8\u4f20\u611f\u7cfb\u7edf\uff0c\u91c7\u7528\u53cc\u63a5\u6536\u5668\u67b6\u6784\u5206\u6790CSI\u4ee5\u9694\u79bb\u7531\u79fb\u52a8\u76ee\u6807\u5f15\u8d77\u7684\u5dee\u5206\u591a\u666e\u52d2\u9891\u79fb\uff0c\u4ece\u800c\u514b\u670d\u786c\u4ef6\u76f8\u4f4d\u635f\u4f24\u3002", "result": "\u5728\u5ba4\u5185\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u57286000\u6beb\u7c73/\u5206\u949f\u4ee5\u4e0a\u7684\u901f\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5ba4\u5916\u8bc4\u4f30\u4e2d\u53ef\u9760\u5730\u4f30\u8ba1\u4e86\u884c\u4eba\u548c\u8f66\u8f86\u7684\u901f\u5ea6\u3002", "conclusion": "LTE\u4fe1\u53f7\u53ef\u4f5c\u4e3a\u4e00\u79cd\u53ef\u884c\u7684\u4ea4\u901a\u76d1\u63a7\u65b9\u6cd5\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ecAoA\u96c6\u6210\u3001\u673a\u5668\u5b66\u4e60\u548c\u5b9e\u65f6\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2507.12065", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.12065", "abs": "https://arxiv.org/abs/2507.12065", "authors": ["Zi-Xu Lu", "Xuan Zuo", "Zhi-Yuan Fan", "Jie Li"], "title": "Optomagnonic continuous-variable quantum teleportation enhanced by non-Gaussian distillation", "comment": null, "summary": "The capability of magnons to coherently couple with various quantum systems\nmakes them an ideal candidate to build hybrid quantum systems. The optomagnonic\ncoupling is essential for constructing a hybrid magnonic quantum network, as\nthe transmission of quantum information among remote quantum nodes must be\naccomplished using light rather than microwave field. Here we provide an\noptomagnonic continuous-variable quantum teleportation protocol, which enables\nthe transfer of an input optical state to a remote magnon mode. To overcome the\ncurrently relatively weak coupling in the experiment, we introduce non-Gaussian\ndistillation operations to enhance the optomagnonic entanglement and thus the\nfidelity of the teleportation. An auxiliary microwave cavity is adopted to\nrealize the non-Gaussian and displacement operations on magnons. We show that a\nseries of optical states, such as coherent, single-photon, squeezed and cat\nstates, can be teleported to the magnon mode. The work provides guidance for\nthe experimental realization of magnonic quantum repeaters and quantum networks\nand a new route to prepare diverse magnonic quantum states exploiting the\nphoton-to-magnon quantum teleportation.", "AI": {"tldr": "\u5229\u7528\u5149\u9541\u590d\u5408\u91cf\u5b50\u4f20\u9001\u534f\u8bae\uff0c\u901a\u8fc7\u975e\u9ad8\u65af\u64cd\u4f5c\u514b\u670d\u5f31\u8026\u5408\uff0c\u5b9e\u73b0\u5149\u6001\u5230\u9541\u590d\u5408\u6001\u7684\u4f20\u9001\u3002", "motivation": "\u4e3a\u4e86\u5728\u8fdc\u8ddd\u79bb\u91cf\u5b50\u8282\u70b9\u4e4b\u95f4\u5229\u7528\u5149\u800c\u975e\u5fae\u6ce2\u573a\u4f20\u8f93\u91cf\u5b50\u4fe1\u606f\uff0c\u6784\u5efa\u6df7\u5408\u9541\u590d\u5408\u91cf\u5b50\u7f51\u7edc\u9700\u8981\u5149\u9541\u590d\u5408\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u975e\u9ad8\u65af\u64cd\u4f5c\uff08\u4f8b\u5982\uff0c\u4f7f\u7528\u8f85\u52a9\u5fae\u6ce2\u8154\u5b9e\u73b0\u5bf9\u9541\u590d\u5408\u7684\u65b0\u9ad8\u65af\u548c\u4f4d\u79fb\u64cd\u4f5c\uff09\u6765\u589e\u5f3a\u5149\u9541\u590d\u5408\u7ea0\u7f20\uff0c\u8fdb\u800c\u63d0\u9ad8\u4f20\u9001\u4fdd\u771f\u5ea6\u3002", "result": "\u6210\u529f\u5c06\u76f8\u5e72\u6001\u3001\u5355\u5149\u5b50\u6001\u3001\u538b\u7f29\u6001\u548c\u732b\u6001\u7b49\u4e00\u7cfb\u5217\u5149\u6001\u4f20\u9001\u5230\u9541\u590d\u5408\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9541\u590d\u5408\u91cf\u5b50\u4e2d\u7ee7\u548c\u91cf\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u5b9e\u73b0\u6307\u5357\uff0c\u5e76\u5f00\u8f9f\u4e86\u4e00\u6761\u5229\u7528\u5149\u5b50\u5230\u9541\u590d\u5408\u65b0\u8def\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u9541\u590d\u5408\u91cf\u5b50\u6001\u7684\u5236\u5907\u3002"}}
{"id": "2507.12327", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12327", "abs": "https://arxiv.org/abs/2507.12327", "authors": ["Mohamad Charara", "Martin De Montigny", "Nivine Abou Daher", "Hanane Dagdougui", "Antoine Lesage-Landry"], "title": "Mixed-integer Second-Order Cone Programming for Multi-period Scheduling of Flexible AC Transmission System Devices", "comment": "10 pages, 1 figure, submitted to CIGR\\'E 2025 International\n  Symposium, Paper 10998, PS1: System Enhancement, Markets and Regulation", "summary": "With the increasing energy demand and the growing integration of renewable\nsources of energy, power systems face operational challenges such as overloads,\nlosses, and stability concerns, particularly as networks operate near their\ncapacity limits. Flexible alternating current transmission system (FACTS)\ndevices are essential to ensure reliable grid operations and enable the\nefficient integration of renewable energy. This work introduces a mixed-integer\nsecond-order cone programming (MISOCP) model for the multi-period scheduling of\nkey FACTS devices in electric transmission systems. The proposed model\nintegrates four key control mechanisms: (i) on-load tap changers (OLTCs) for\nvoltage regulation via discrete taps; (ii) static synchronous compensators\n(STATCOMs) and (iii) shunt reactors for reactive power compensation; and (iv)\nthyristor-controlled series capacitors (TCSCs) for adjustable impedance and\nflow control. The objective is to minimize active power losses using a limited\nnumber of control actions while meeting physical and operational constraints at\nall times throughout the defined time horizon. To ensure tractability, the\nmodel employs a second-order cone relaxation of the power flow. Device-specific\nconstraints are handled via binary expansion and linearization: OLTCs and shunt\nreactors are modelled with discrete variables, STATCOMs through reactive power\nbounds, and TCSCs using a reformulation-linearization technique (RLT). A\nmulti-period formulation captures the sequential nature of decision making,\nensuring consistency across time steps. The model is evaluated on the IEEE\n9-bus, 30-bus, and RTS96 test systems, demonstrating its ability to reduce\nlosses, with potential applicability to larger-scale grids.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdMISOCP\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316FACTS\u8bbe\u5907\uff08OLTC\u3001STATCOM\u3001\u5e76\u8054\u7535\u6297\u5668\u3001TCSC\uff09\u7684\u591a\u5468\u671f\u8c03\u5ea6\uff0c\u4ee5\u6700\u5c0f\u5316\u7535\u529b\u635f\u8017\u3002\u6a21\u578b\u901a\u8fc7\u4e8c\u9636\u9525\u677e\u5f1b\u548c\u7ebf\u6027\u5316\u5904\u7406\u8bbe\u5907\u7ea6\u675f\uff0c\u5e76\u5728\u6807\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u80fd\u6e90\u9700\u6c42\u7684\u589e\u957f\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u7684\u589e\u52a0\uff0c\u7535\u529b\u7cfb\u7edf\u9762\u4e34\u7740\u8fd0\u884c\u6311\u6218\uff0c\u5982\u8fc7\u8f7d\u3001\u635f\u8017\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002FACTS\u8bbe\u5907\u5bf9\u4e8e\u786e\u4fdd\u7535\u7f51\u53ef\u9760\u8fd0\u884c\u548c\u9ad8\u6548\u6574\u5408\u53ef\u518d\u751f\u80fd\u6e90\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6574\u6570\u4e8c\u9636\u9525\u89c4\u5212\uff08MISOCP\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u7535\u7f51\u8f93\u7535\u7cfb\u7edf\u4e2d\u5173\u952e\u67d4\u6027\u4ea4\u6d41\u8f93\u7535\u7cfb\u7edf\uff08FACTS\uff09\u8bbe\u5907\u7684\u5236\u5b9a\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u56db\u79cd\u5173\u952e\u63a7\u5236\u673a\u5236\uff1a\u5229\u7528\u79bb\u6563\u62bd\u5934\u7684\u5728\u7ebf\u8d1f\u8377\u5206\u63a5\u5934\u6539\u53d8\u5668\uff08OLTC\uff09\u3001\u9759\u6b62\u540c\u6b65\u8865\u507f\u5668\uff08STATCOM\uff09\u3001\u5e76\u8054\u7535\u6297\u5668\u548c\u6676\u95f8\u7ba1\u63a7\u5236\u4e32\u8054\u7535\u5bb9\u5668\uff08TCSC\uff09\u3002\u4e3a\u4e86\u4fdd\u8bc1\u53ef\u5904\u7406\u6027\uff0c\u6a21\u578b\u91c7\u7528\u4e86\u4e8c\u9636\u9525\u677e\u5f1b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e8c\u8fdb\u5236\u6269\u5c55\u548c\u7ebf\u6027\u5316\u5904\u7406\u4e86\u8bbe\u5907\u7279\u5b9a\u7684\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5728IEEE 9\u8282\u70b9\u300130\u8282\u70b9\u548cRTS96\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684MISOCP\u6a21\u578b\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6709\u529f\u529f\u7387\u635f\u8017\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728IEEE 9\u8282\u70b9\u300130\u8282\u70b9\u548cRTS96\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u51cf\u5c11\u635f\u8017\u7684\u80fd\u529b\uff0c\u5e76\u6709\u53ef\u80fd\u5e94\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u7535\u7f51\u3002"}}
{"id": "2507.12205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12205", "abs": "https://arxiv.org/abs/2507.12205", "authors": ["Junqing Lin", "Jingwei Sun", "Mingge Lu", "Guangzhong Sun"], "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage", "comment": "11 pages", "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.", "AI": {"tldr": "EC-SpMV\u901a\u8fc7\u5206\u5c42\u5757\u63d0\u53d6\u548cEC-CSR\u683c\u5f0f\uff0c\u52a0\u901f\u4e86\u7a00\u758fLLM\u63a8\u7406\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe6.44\u500d\uff0c\u5b58\u50a8\u5f00\u9500\u51cf\u5c11\u9ad8\u8fbe55.4%\u3002", "motivation": "\u73b0\u6709\u7684SpMV\u5185\u6838\u548c\u7a00\u758f\u77e9\u9635\u683c\u5f0f\uff08\u6e90\u4e8e\u79d1\u5b66\u8ba1\u7b97\uff09\u672a\u80fd\u5145\u5206\u5229\u7528\u7a00\u758fLLM\uff08\u5c24\u5176\u662f\u5728\u6279\u5904\u7406\u5927\u5c0f\u4e3a1\u7684\u89e3\u7801\u5668\u9636\u6bb5\uff09\u7684\u72ec\u7279\u7ed3\u6784\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u5b58\u50a8\u5f00\u9500\u8fc7\u5927\u3002", "method": "EC-SpMV\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GPU\u4f18\u5316\u7a00\u758f\u77e9\u9635-\u5411\u91cf\u4e58\uff08SpMV\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\uff081\uff09\u6355\u6349\u7a00\u758fLLM\u4e2d\u591a\u7c92\u5ea6\u5757\u7ed3\u6784\u7684\u5206\u5c42\u5757\u63d0\u53d6\u7b97\u6cd5\uff0c\u4ee5\u53ca\uff082\uff09\u4f7f\u7528\u589e\u91cf\u7d22\u5f15\u7684EC-CSR\u65b0\u538b\u7f29\u7a00\u758f\u683c\u5f0f\uff0c\u4ee5\u964d\u4f4e\u5b58\u50a8\u5f00\u9500\u548c\u63d0\u9ad8\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u3002", "result": "EC-SpMV\u5728LLaMA\u548cOPT\u6a21\u578b\u7684\u7a00\u758f\u6743\u91cd\u77e9\u9635\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u6700\u5148\u8fdb\u7684SpMV\u5e93\u76f8\u6bd4\uff0c\u901f\u5ea6\u6700\u9ad8\u63d0\u9ad8\u4e866.44\u500d\uff0c\u5b58\u50a8\u5f00\u9500\u6bd4CSR\u683c\u5f0f\u51cf\u5c11\u4e86\u9ad8\u8fbe55.4%\u3002", "conclusion": "EC-SpMV\u901a\u8fc7\u5176\u5206\u5c42\u5757\u63d0\u53d6\u7b97\u6cd5\u548cEC-CSR\u538b\u7f29\u683c\u5f0f\uff0c\u5728\u7a00\u758fLLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u5b58\u50a8\u4f18\u5316\u3002"}}
{"id": "2507.11974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11974", "abs": "https://arxiv.org/abs/2507.11974", "authors": ["Waseem Akram", "Muhayy Ud Din", "Lyes Saad Soud", "Irfan Hussain"], "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming", "comment": null, "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.", "AI": {"tldr": "GAI\u6b63\u6210\u4e3a\u6c34\u4ea7\u517b\u6b96\u4e1a\u53d1\u5c55\u7684\u5173\u952e\u9a71\u52a8\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u6765\u4f18\u5316\u51b3\u7b56\u548c\u8fd0\u8425\uff0c\u5c3d\u7ba1\u4ecd\u5b58\u5728\u6570\u636e\u53ef\u7528\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6c34\u4ea7\u517b\u6b96\u4e1a\u5411\u6570\u636e\u9a71\u52a8\u3001\u81ea\u52a8\u5316\u548c\u6570\u5b57\u5316\u96c6\u6210\u8fd0\u8425\uff08\u6c34\u4ea7\u517b\u6b964.0\uff09\u7684\u8303\u5f0f\u8f6c\u53d8\uff0cGAI\u6a21\u578b\u4e3a\u884c\u4e1a\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u9047\u3002", "method": "\u672c\u7efc\u8ff0\u5168\u9762\u7efc\u5408\u4e86GAI\u5728\u6c34\u4ea7\u517b\u6b96\u9886\u57df\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u57fa\u7840\u67b6\u6784\u3001\u5b9e\u9a8c\u7cfb\u7edf\u3001\u8bd5\u70b9\u90e8\u7f72\u548c\u5b9e\u9645\u7528\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u8d8a\u611f\u77e5\u3001\u63a7\u5236\u3001\u4f18\u5316\u3001\u901a\u4fe1\u548c\u76d1\u7ba1\u5408\u89c4\u7684\u5e94\u7528\u5206\u7c7b\u3002", "result": "GAI\u5728\u5b9e\u73b0\u6c34\u4e0b\u611f\u77e5\u3001\u6570\u5b57\u5b6a\u751f\u5efa\u6a21\u548c\u81ea\u4e3b\u89c4\u5212\uff08\u5982ROV\u4efb\u52a1\uff09\u65b9\u9762\u53d1\u6325\u7740\u65e5\u76ca\u91cd\u8981\u7684\u4f5c\u7528\u3002", "conclusion": "GAI\u662f\u6c34\u4ea7\u517b\u6b96\u4e1a\u5411\u667a\u80fd\u5316\u3001\u9002\u5e94\u6027\u51b3\u7b56\u53d1\u5c55\u7684\u5173\u952e\u63a8\u52a8\u8005\uff0c\u5b83\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5728\u73af\u5883\u76d1\u6d4b\u3001\u673a\u5668\u4eba\u6280\u672f\u3001\u75be\u75c5\u8bca\u65ad\u3001\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3001\u62a5\u544a\u548c\u5e02\u573a\u5206\u6790\u7b49\u9886\u57df\u63d0\u4f9b\u65b0\u7684\u673a\u9047\u3002"}}
{"id": "2507.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11867", "abs": "https://arxiv.org/abs/2507.11867", "authors": ["Xiangyu Yang", "Xinying Qiu"], "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11730", "abs": "https://arxiv.org/abs/2507.11730", "authors": ["Maciej Szankin", "Vidhyananth Venkatasamy", "Lihang Ying"], "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis", "comment": null, "summary": "Outdoor advertisements remain a critical medium for modern marketing, yet\naccurately verifying billboard text visibility under real-world conditions is\nstill challenging. Traditional Optical Character Recognition (OCR) pipelines\nexcel at cropped text recognition but often struggle with complex outdoor\nscenes, varying fonts, and weather-induced visual noise. Recently, multimodal\nVision-Language Models (VLMs) have emerged as promising alternatives, offering\nend-to-end scene understanding with no explicit detection step. This work\nsystematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,\nInternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline\n(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with\nsynthetic weather distortions to simulate realistic degradation. Our results\nreveal that while selected VLMs excel at holistic scene reasoning, lightweight\nCNN pipelines still achieve competitive accuracy for cropped text at a fraction\nof the computational cost-an important consideration for edge deployment. To\nfoster future research, we release our weather-augmented benchmark and\nevaluation code publicly.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9VLM\u548cCNN\u5728\u6237\u5916\u5e7f\u544a\u6587\u672c\u8bc6\u522b\u65b9\u9762\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u51c6\u786e\u9a8c\u8bc1\u6237\u5916\u5e7f\u544a\u724c\u6587\u672c\u53ef\u89c1\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u7684OCR\u65b9\u6cd5\u5728\u590d\u6742\u6237\u5916\u573a\u666f\u3001\u591a\u53d8\u5b57\u4f53\u548c\u5929\u6c14\u5f15\u8d77\u7684\u89c6\u89c9\u566a\u58f0\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u50cfVLM\u8fd9\u6837\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5bf9\u7cbe\u9009\u7684VLM\uff08\u5305\u62ecQwen 2.5 VL 3B\u3001InternVL3\u548cSmolVLM2\uff09\u4e0e\u7d27\u51d1\u578bCNN\uff08PaddleOCRv4\uff09\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08ICDAR 2015\u548cSVT\uff09\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u5408\u6210\u5929\u6c14\u5931\u771f\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u3002", "result": "\u6240\u9009\u7684VLM\u5728\u6574\u4f53\u573a\u666f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8f7b\u91cf\u7ea7CNN\u7ba1\u9053\u5728\u88c1\u526a\u6587\u672c\u8bc6\u522b\u65b9\u9762\u4ecd\u7136\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8ba1\u7b97\u6210\u672c\u5374\u4f4e\u5f97\u591a\uff0c\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u90e8\u7f72\u800c\u8a00\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8003\u91cf\u56e0\u7d20\u3002", "conclusion": "\u8f7b\u91cf\u7ea7CNN\u7ba1\u9053\u5728\u8ba1\u7b97\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\u5373\u53ef\u5b9e\u73b0\u88c1\u526a\u6587\u672c\u7684\u7ade\u4e89\u529b\uff0c\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u90e8\u7f72\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2507.11793", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11793", "abs": "https://arxiv.org/abs/2507.11793", "authors": ["Andrew E. Deneris", "Paolo Braccia", "Pablo Bermejo", "N. L. Diaz", "Antonio A. Mele", "M. Cerezo"], "title": "Analyzing the free states of one quantum resource theory as resource states of another", "comment": "14+22 pages, 4 figures, 1 table", "summary": "In the context of quantum resource theories (QRTs), free states are defined\nas those which can be obtained at no cost under a certain restricted set of\nconditions. However, when taking a free state from one QRT and evaluating it\nthrough the optics of another QRT, it might well turn out that the state is now\nextremely resourceful. Such realization has recently prompted numerous works\ncharacterizing states across several QRTs. In this work we contribute to this\nbody of knowledge by analyzing the resourcefulness in free states for--and\nacross witnesses of--the QRTs of multipartite entanglement, fermionic\nnon-Gaussianity, imaginarity, realness, spin coherence, Clifford\nnon-stabilizerness, $S_n$-equivariance and non-uniform entanglement. We provide\nrigorous theoretical results as well as present numerical studies that showcase\nthe rich and complex behavior that arises in this type of cross-examination.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u81ea\u7531\u6001\u5728\u591a\u65b9\u7ea0\u7f20\u3001\u8d39\u7c73\u5b50\u975e\u9ad8\u65af\u6027\u7b49\u591a\u4e2a\u7406\u8bba\u4e2d\u7684\u8d44\u6e90\u6027\uff0c\u63ed\u793a\u4e86\u8de8\u7406\u8bba\u5206\u6790\u7684\u590d\u6742\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\uff08QRTs\uff09\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u5f00\u59cb\u5173\u6ce8\u5c06\u4e00\u4e2aQRT\u4e2d\u7684\u81ea\u7531\u6001\u7f6e\u4e8e\u53e6\u4e00\u4e2aQRT\u7684\u6846\u67b6\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u53ef\u80fd\u63ed\u793a\u5176\u6f5c\u5728\u7684\u8d44\u6e90\u5c5e\u6027\u3002\u8fd9\u79cd\u8de8\u7406\u8bba\u7684\u89c6\u89d2\u4fc3\u4f7f\u4e86\u5bf9\u4e0d\u540cQRTs\u4e4b\u95f4\u72b6\u6001\u8fdb\u884c\u8868\u5f81\u7684\u7814\u7a76\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u4e3a\u8fd9\u4e00\u9886\u57df\u8d21\u732e\u529b\u91cf\uff0c\u901a\u8fc7\u5206\u6790\u81ea\u7531\u6001\u5728\u591a\u4e2a\u7279\u5b9aQRTs\u4e2d\u7684\u8d44\u6e90\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u81ea\u7531\u6001\u5728\u591a\u4e2a\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u7684\u8868\u73b0\uff0c\u8de8\u8d8a\u4e86\u591a\u65b9\u7ea0\u7f20\u3001\u8d39\u7c73\u5b50\u975e\u9ad8\u65af\u6027\u3001\u865a\u6570\u6027\u3001\u5b9e\u6570\u6027\u3001\u81ea\u65cb\u76f8\u5e72\u6027\u3001Clifford\u975e\u7a33\u5b9a\u592a\u6027\u3001$S_n$-\u7b49\u53d8\u6027\u4ee5\u53ca\u975e\u5747\u5300\u7ea0\u7f20\u7b49\u7406\u8bba\u3002\u7814\u7a76\u7ed3\u5408\u4e86\u4e25\u683c\u7684\u7406\u8bba\u63a8\u5bfc\u548c\u6570\u503c\u6a21\u62df\uff0c\u4ee5\u63ed\u793a\u8de8\u7406\u8bba\u5206\u6790\u7684\u73b0\u8c61\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8de8\u7406\u8bba\u5206\u6790\u4e2d\uff0c\u81ea\u7531\u6001\u7684\u884c\u4e3a\u8868\u73b0\u51fa\u4e30\u5bcc\u4e14\u590d\u6742\u7684\u7279\u6027\u3002\u7406\u8bba\u7814\u7a76\u548c\u6570\u503c\u6a21\u62df\u5747\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u70b9\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\uff08QRTs\uff09\u4e2d\u7684\u81ea\u7531\u6001\u5728\u591a\u4e2a\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\uff08\u5305\u62ec\u591a\u65b9\u7ea0\u7f20\u3001\u8d39\u7c73\u5b50\u975e\u9ad8\u65af\u6027\u3001\u865a\u6570\u6027\u3001\u5b9e\u6570\u6027\u3001\u81ea\u65cb\u76f8\u5e72\u6027\u3001Clifford\u975e\u7a33\u5b9a\u592a\u6027\u3001$S_n$-\u7b49\u53d8\u6027\u4ee5\u53ca\u975e\u5747\u5300\u7ea0\u7f20\uff09\u7684\u89c1\u8bc1\u4e4b\u95f4\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u7ed3\u679c\u548c\u6570\u503c\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u8de8\u7406\u8bba\u5206\u6790\u7684\u4e30\u5bcc\u590d\u6742\u884c\u4e3a\u3002"}}
{"id": "2507.11690", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11690", "abs": "https://arxiv.org/abs/2507.11690", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "comment": "10 pages, 9 additional pages for Appendix", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "AI": {"tldr": "\u7814\u7a76\u4e86 coreset \u9009\u62e9\u5bf9\u6570\u636e\u96c6\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u65b9\u6cd5\u66f4\u4e0d\u5bb9\u6613\u52a0\u5267\u504f\u5dee\uff0c\u5e76\u4e14\u964d\u4f4e\u504f\u5dee\u5e76\u4e0d\u76f4\u63a5\u7b49\u540c\u4e8e\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u7ea6\u7b80\u65b9\u6cd5\uff08\u5982 coreset \u9009\u62e9\uff09\u5bf9\u6570\u636e\u96c6\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5b83\u4eec\u662f\u5426\u4f1a\u5ef6\u7eed\u3001\u653e\u5927\u6216\u51cf\u8f7b\u8fd9\u4e9b\u504f\u5dee\u3002", "method": "\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u865a\u5047\u76f8\u5173\u6027\u57fa\u51c6\u3001\u4e94\u4e2a\u6837\u672c\u91cd\u8981\u6027/\u96be\u6613\u5ea6\u8bc4\u5206\u6307\u6807\u548c\u4e94\u4e2a\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u5206\u6790\u4e86 coreset \u9009\u62e9\u5bf9\u865a\u5047\u504f\u5dee\u6c34\u5e73\u548c\u4e0b\u6e38\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684\u6837\u672c\u8868\u5f81\u9009\u62e9 coreset \u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u8868\u5f81\u9009\u62e9\uff0c\u610f\u5916\u52a0\u5267\u504f\u5dee\u7684\u98ce\u9669\u76f8\u5bf9\u8f83\u4f4e\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u4f18\u5148\u9009\u62e9\u96be\u6837\u672c\u53ef\u4ee5\u964d\u4f4e\u504f\u5dee\u6c34\u5e73\uff0c\u4f46\u4e0d\u80fd\u4fdd\u8bc1\u4e0b\u6e38\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u67d0\u4e9b\u4ee5\u96be\u6837\u672c\u4e3a\u5bfc\u5411\u7684 coreset \u9009\u62e9\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u964d\u4f4e\u504f\u5dee\u6c34\u5e73\uff0c\u4f46\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u4e0b\u6e38\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.12353", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.12353", "abs": "https://arxiv.org/abs/2507.12353", "authors": ["Tony Chiang", "John J. Plombon", "Megan K. Lenox", "Ian Mercer", "Punyashloka Debashis", "Mahendra DC", "Susan Trolier-McKinstry", "Jon-Paul Maria", "Jon F. Ihlefeld", "Ian A. Young", "John T. Heron"], "title": "Material-Limited Switching in Nanoscale Ferroelectrics", "comment": "19 pages, 4 figures", "summary": "The ferroelectric switching speed has been experimentally obfuscated by the\ninteraction between the measurement circuit and the ferroelectric switching\nitself. This has prohibited the observation of real material responses at\nnanosecond timescales and lower. Here, fundamental polarization switching\nspeeds in ferroelectric materials with the perovskite, fluorite, and wurtzite\nstructures are reported. Upon lateral scaling of island capacitors from micron\nto nanoscales, a clear transition from circuit-limited switching to a\nmaterial-limited switching regime is observed. In\nLa$_{0.15}$Bi$_{0.85}$FeO$_{3}$ capacitors, switching is as fast as ~150 ps,\nthe fastest switching time reported. For polycrystalline\nHf$_{0.5}$Zr$_{0.5}$O$_{2}$ capacitors, a fundamental switching limit of ~210\nps is observed. Switching times for Al$_{0.92}$B$_{0.08}$N are near 20 ns,\nlimited by the coercive and breakdown electric fields. The activation field,\ninstantaneous pseudo-resistivity, and energy-delay are reported in this\nmaterial-limited regime. Lastly, a criterion for reaching the material-limited\nregime is provided. This regime enables observation of intrinsic material\nproperties and favorable scaling trends for high-performance computing.", "AI": {"tldr": "Ferroelectric switching speed is no longer obfuscated by measurement circuits, achieving speeds as fast as ~150 ps in La$_{0.15}$Bi$_{0.85}$FeO$_{3}$ capacitors. This advancement allows for the observation of intrinsic material properties and favorable scaling trends for high-performance computing.", "motivation": "The motivation is to overcome the limitations imposed by the interaction between measurement circuits and ferroelectric switching, which has historically obscured the real material responses at nanosecond timescales and lower.", "method": "The paper reports fundamental polarization switching speeds in ferroelectric materials by experimentally distinguishing between circuit-limited and material-limited switching regimes through lateral scaling of island capacitors from micron to nanoscales.", "result": "The study reports switching speeds as fast as ~150 ps in La$_{0.15}$Bi$_{0.85}$FeO$_{3}$ capacitors, a fundamental switching limit of ~210 ps in polycrystalline Hf$_{0.5}$Zr$_{0.5}$O$_{2}$ capacitors, and switching times near 20 ns for Al$_{0.92}$B$_{0.08}$N, limited by coercive and breakdown electric fields. It also reports the activation field, instantaneous pseudo-resistivity, and energy-delay in the material-limited regime.", "conclusion": "The study establishes a criterion for achieving the material-limited regime in ferroelectric switching, enabling the observation of intrinsic material properties and favorable scaling trends for high-performance computing."}}
{"id": "2507.12221", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12221", "abs": "https://arxiv.org/abs/2507.12221", "authors": ["Alejandro Castilla", "Sa\u00fal Fenollosa", "Monika Drozdowska", "Alejandro Lopez-Escudero", "Sergio Mic\u00f2-Rosa", "Narcis Cardona"], "title": "Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G", "comment": "6 pages, 13 figures. Accepted for publication at the 2024 IEEE 35th\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC)", "summary": "Integrated Sensing and Communication (ISAC) design is crucial for 6G and\nharmonizes environmental data sensing with communication, emphasizing the need\nto understand and model these elements. This paper delves into dual-channel\nmodels for ISAC, employing channel extraction techniques to validate and\nenhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the\nextraction of the bistatic sensing channel from monostatic measurements and\nsubsequent communication channel estimation. The proposed methods involve\ninterference extraction, module and phase correlation analyses, chirp\nclustering, and auto-clutter reduction. A comprehensive set-up in an anechoic\nchamber with controlled scenarios evaluates the proposed techniques,\ndemonstrating successful channel extraction and validation through Root Mean\nSquare Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival\n(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the\neffectiveness of the proposed approach, presenting an innovative stride towards\nfully integrated sensing and communication in future networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbeISAC\u7684\u53cc\u901a\u9053\u6a21\u578b\uff0c\u901a\u8fc7\u5e72\u6270\u63d0\u53d6\u3001\u4fe1\u9053\u5206\u6790\u7b49\u65b9\u6cd5\u63d0\u53d6\u611f\u77e5\u548c\u901a\u4fe1\u4fe1\u9053\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e3a6G ISAC\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db36G\u5bf9\u73af\u5883\u6570\u636e\u611f\u77e5\u4e0e\u901a\u4fe1\u878d\u5408\u7684\u9700\u6c42\uff0c\u7406\u89e3\u548c\u5efa\u6a21ISAC\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u53cc\u901a\u9053\u6a21\u578b\u5728\u6beb\u7c73\u6ce2\u96f7\u8fbeISAC\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7528\u4e8eISAC\u53cc\u901a\u9053\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5e72\u6270\u63d0\u53d6\u3001\u6a21\u5757\u548c\u76f8\u4f4d\u76f8\u5173\u6027\u5206\u6790\u3001chirp\u805a\u7c7b\u4ee5\u53ca\u81ea\u6742\u6ce2\u7f29\u51cf\uff0c\u4ee5\u4ece\u5355\u7ad9\u6d4b\u91cf\u4e2d\u63d0\u53d6\u53cc\u7ad9\u611f\u77e5\u4fe1\u9053\u5e76\u8fdb\u884c\u901a\u4fe1\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6280\u672f\u80fd\u591f\u6210\u529f\u63d0\u53d6\u548c\u9a8c\u8bc1\u611f\u77e5\u548c\u901a\u4fe1\u4fe1\u9053\uff0c\u901a\u8fc7\u5747\u65b9\u6839\u5ef6\u8fdf\u6269\u5c55\uff08RMS DS\uff09\u3001\u529f\u7387\u5ef6\u8fdf\u5256\u9762\uff08PDP\uff09\u548c\u5230\u8fbe\u89d2\uff08AoA\uff09\u5206\u6790\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u4e0eRT\u6a21\u62df\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u6d88\u58f0\u5ba4\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u5c04\u7ebf\u8ffd\u8e2a\uff08RT\uff09\u6a21\u62df\u8fdb\u884c\u6bd4\u8f83\uff0c\u6210\u529f\u5730\u63d0\u53d6\u548c\u9a8c\u8bc1\u4e86\u53cc\u901a\u9053\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6beb\u7c73\u6ce2\u96f7\u8fbeISAC\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u7684\u5b8c\u5168\u96c6\u6210\u611f\u77e5\u548c\u901a\u4fe1\u63d0\u4f9b\u4e86\u521b\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.12176", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2507.12176", "abs": "https://arxiv.org/abs/2507.12176", "authors": ["Rui-Jie Chen", "Guo-Qing Zhang", "Zhi Li", "Dan-Wei Zhang"], "title": "Mobility rings in a non-Hermitian non-Abelian quasiperiodic lattice", "comment": "9 pages, 6 figures; version accepted by PRA", "summary": "We study localization and topological properties in spin-1/2 non-reciprocal\nAubry-Andr\\'{e} chain with SU(2) non-Abelian artificial gauge fields. The\nresults reveal that, different from the Abelian case, mobility rings, will\nemerge in the non-Abelian case accompanied by the non-Hermitian topological\nphase transition. As the non-Hermitian extension of mobility edges, such\nmobility rings separate Anderson localized eigenstates from extended\neigenstates in the complex energy plane under the periodic boundary condition.\nBased on the topological properties, we obtain the exact expression of the\nmobility rings. Furthermore, the corresponding indicators such as inverse\nparticipation rate, normalized participation ratio, winding number,\nnon-Hermitian spectral structures and wave functions are numerically studied.\nThe numerical results are in good agreement with the analytical expression,\nwhich confirms the emergence of mobility rings.", "AI": {"tldr": "\u7814\u7a76\u4e86\u81ea\u65cb1/2\u975e\u4e92\u6613Aubry-Andr'e\u94fe\u4e2d\u7684\u975e\u5384\u7c73\u62d3\u6251\u76f8\u53d8\u548c\u8fc1\u79fb\u7387\u73af\u3002", "motivation": "\u7814\u7a76\u81ea\u65cb1/2\u975e\u4e92\u6613Aubry-Andr'e\u94fe\u5728SU(2)\u975e\u963f\u8d1d\u5c14\u4eba\u5de5\u89c4\u8303\u573a\u4e0b\u7684\u5c40\u57df\u5316\u548c\u62d3\u6251\u6027\u8d28\u3002", "method": "\u57fa\u4e8e\u62d3\u6251\u6027\u8d28\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u8fc1\u79fb\u7387\u73af\u7684\u7cbe\u786e\u8868\u8fbe\u5f0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6570\u503c\u7814\u7a76\u4e86\u76f8\u5e94\u7684\u6307\u6807\uff0c\u5982\u53cd\u5411\u53c2\u4e0e\u7387\u3001\u5f52\u4e00\u5316\u53c2\u4e0e\u7387\u3001\u7f20\u7ed5\u6570\u3001\u975e\u5384\u7c73\u8c31\u7ed3\u6784\u548c\u6ce2\u51fd\u6570\u3002", "result": "\u6570\u503c\u7ed3\u679c\u4e0e\u89e3\u6790\u8868\u8fbe\u5f0f\u543b\u5408\u826f\u597d\uff0c\u8bc1\u5b9e\u4e86\u8fc1\u79fb\u7387\u73af\u7684\u51fa\u73b0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728SU(2)\u975e\u963f\u8d1d\u5c14\u4eba\u5de5\u89c4\u8303\u573a\u4e0b\u7684\u81ea\u65cb1/2\u975e\u4e92\u6613Aubry-Andr'e\u94fe\u4e2d\uff0c\u4e0e\u963f\u8d1d\u5c14\u60c5\u51b5\u4e0d\u540c\uff0c\u975e\u963f\u8d1d\u5c14\u60c5\u51b5\u4f1a\u51fa\u73b0\u8fc1\u79fb\u7387\u73af\uff0c\u5e76\u4f34\u968f\u7740\u975e\u5384\u7c73\u62d3\u6251\u76f8\u53d8\u3002\u8fd9\u4e9b\u8fc1\u79fb\u7387\u73af\u662f\u8fc1\u79fb\u7387\u8fb9\u7684\u975e\u5384\u7c73\u63a8\u5e7f\uff0c\u5728\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u4e0b\uff0c\u5b83\u4eec\u5c06\u5b89\u5fb7\u68ee\u5c40\u57df\u5316\u6001\u4e0e\u6269\u5c55\u6001\u5728\u590d\u80fd\u91cf\u5e73\u9762\u4e0a\u5206\u79bb\u5f00\u6765\u3002"}}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u4e2d\u56fd\u8c61\u68cb\u8fd9\u4e00\u590d\u6742\u68cb\u76d8\u6e38\u620f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aXiangqi-R1\u7684LLM\u8bad\u7ec3\u6846\u67b6\u548c\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347LLM\u7684\u7a7a\u95f4\u7b56\u7565\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cXiangqi-R1\u76f8\u6bd4\u901a\u7528LLM\u5728\u8d70\u6cd5\u5408\u6cd5\u6027\u548c\u5206\u6790\u51c6\u786e\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u53d1\u5c55\u901a\u7528\u7b56\u7565\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u7a7a\u95f4\u7b56\u7565\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u5bf9\u4e8e\u590d\u6742\u4e14\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u68cb\u76d8\u6e38\u620f\u81f3\u5173\u91cd\u8981\uff0c\u5c3d\u7ba1LLM\u5728\u4e00\u822c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e2d\u56fd\u8c61\u68cb\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u5305\u542b\u4e13\u5bb6\u6ce8\u91ca\u548c\u5f15\u64ce\u8bc4\u4f30\u7684\u4e94\u767e\u4e07\u4e2a\u68cb\u76d8-\u8d70\u6cd5\u5bf9\u7684\u5927\u578b\u6570\u636e\u96c6\u3002\u5f15\u5165\u4e86\u4e00\u4e2a7B\u53c2\u6570\u6a21\u578bXiangqi-R1\uff0c\u8be5\u6a21\u578b\u7ecf\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff1a(1)\u9488\u5bf9\u5408\u6cd5\u8d70\u6cd5\u9884\u6d4b\u8fdb\u884c\u5fae\u8c03\uff0c(2)\u7ed3\u5408\u7b56\u7565\u6ce8\u91ca\u4ee5\u6539\u8fdb\u51b3\u7b56\uff0c(3)\u901a\u8fc7\u5177\u6709\u591a\u7ef4\u5ea6\u5956\u52b1\u4fe1\u53f7\u7684\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4ee5\u589e\u5f3a\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "result": "\u4e0e\u901a\u7528LLM\u76f8\u6bd4\uff0cXiangqi-R1\u5728\u8d70\u6cd5\u5408\u6cd5\u6027\u65b9\u9762\u63d0\u9ad8\u4e8618%\uff0c\u5206\u6790\u51c6\u786e\u7387\u63d0\u9ad8\u4e8622%\u3002", "conclusion": " Xiangqi-R1\u5728\u63d0\u9ad8\u4e2d\u56fd\u8c61\u68cb\u7b49\u7a7a\u95f4\u590d\u6742\u9886\u57df\u901a\u7528\u7b56\u7565\u667a\u80fd\u65b9\u9762\u5c55\u793a\u4e86\u6f5c\u529b\uff0c\u8868\u660e\u901a\u7528LLM\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2507.12339", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12339", "abs": "https://arxiv.org/abs/2507.12339", "authors": ["Youssef Ait Si", "Antoine Girard", "Adnane Saoud"], "title": "Symbolic Control: Unveiling Free Robustness Margins", "comment": "9", "summary": "This paper addresses the challenge of ensuring robustness in the presence of\nsystem perturbations for symbolic control techniques. Given a discrete-time\ncontrol system that is related to its symbolic model by an alternating\nsimulation relation. In this paper, we focus on computing the maximum\nrobustness margin under which the symbolic model remains valid for a\nperturbed-version of the discrete-time control system. We first show that\nsymbolic models are inherently equipped with a certain free robustness margins.\nWe then provide constructive procedures to compute uniform and non-uniform\n(state and input dependent) robustness margins. We also show that the tightness\nof the robustness margin depends on the tightness of the reachability technique\nused to compute the symbolic model. We then explain how the computed robustness\nmargin can be used for the sake of controller synthesis. Finally, we present\ntwo illustrative examples to demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8ba1\u7b97\u7b26\u53f7\u63a7\u5236\u9c81\u68d2\u6027\u88d5\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u63a7\u5236\u5668\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u63a7\u5236\u6280\u672f\u5728\u7cfb\u7edf\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6700\u5927\u9c81\u68d2\u6027\u88d5\u503c\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5747\u5300\u88d5\u503c\u548c\u975e\u5747\u5300\u88d5\u503c\uff08\u72b6\u6001\u548c\u8f93\u5165\u76f8\u5173\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u9c81\u68d2\u6027\u88d5\u503c\u7684\u7d27\u5bc6\u5ea6\u4f9d\u8d56\u4e8e\u7528\u4e8e\u8ba1\u7b97\u7b26\u53f7\u6a21\u578b\u7684\u53ef\u8fbe\u6027\u6280\u672f\u7684\u7d27\u5bc6\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86\u7b26\u53f7\u6a21\u578b\u5177\u6709\u56fa\u6709\u7684\u9c81\u68d2\u6027\u88d5\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba1\u7b97\u8fd9\u4e9b\u88d5\u503c\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u5728\u63a7\u5236\u5668\u7efc\u5408\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6700\u5927\u9c81\u68d2\u6027\u88d5\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63a7\u5236\u5668\u7efc\u5408\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.11991", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11991", "abs": "https://arxiv.org/abs/2507.11991", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "comment": null, "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff09\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u5341\u5b57\u8def\u53e3\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u751f\u6210\u548c\u5229\u7528\u78b0\u649e\u8bf1\u56e0\u7684\u4f20\u611f\u5668\u566a\u58f0\u5e8f\u5217\u6765\u6784\u5efa\u66f4\u4f18\u7684\u89c4\u5212\u5668\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u9ad8\u98ce\u9669\u4ea4\u901a\u533a\u57df\uff08\u5982\u5341\u5b57\u8def\u53e3\uff09\u662f\u9020\u6210\u78b0\u649e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u5341\u5b57\u8def\u53e3\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u5177\u4f53\u4e3a1000\u6b65\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u4ee5\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u56db\u5411\u5341\u5b57\u8def\u53e3\u5bfc\u822a\u65f6\uff0c\u57fa\u4e8e\u5165\u4fb5\u8005\u76f8\u5bf9\u4f4d\u7f6e\u548c\u901f\u5ea6\u7684\u78b0\u649e\u8bf1\u56e0\u4f20\u611f\u5668\u566a\u58f0\u5e8f\u5217\u3002\u4e4b\u540e\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\u5c061000\u6b65\u6a21\u578b\u63d0\u70bc\u4e3a\u5355\u6b65\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u5e76\u4fdd\u6301\u91c7\u6837\u8d28\u91cf\u3002", "result": "\u4e0e\u57fa\u7ebf\u667a\u80fd\u9a7e\u9a76\u6a21\u578b\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7a33\u5065\u89c4\u5212\u5668\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u663e\u8457\u66f4\u4f4e\u7684\u5931\u8d25\u7387\u548c\u5ef6\u8fdf\u7387\u3002", "conclusion": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u5341\u5b57\u8def\u53e3\u7b49\u9ad8\u98ce\u9669\u4ea4\u901a\u533a\u57df\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u751f\u6210\u78b0\u649e\u8bf1\u56e0\u7684\u4f20\u611f\u5668\u566a\u58f0\u5e8f\u5217\uff0c\u5e76\u53ef\u4ee5\u5c06\u5176\u63d0\u70bc\u4e3a\u5355\u6b65\u6a21\u578b\u4ee5\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u4ece\u800c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u6784\u5efa\u7a33\u5065\u7684\u89c4\u5212\u5668\u3002"}}
{"id": "2507.11875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "DualReward\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u673a\u5236\u548c\u81ea\u9002\u5e94\u7f29\u653e\u6765\u751f\u6210\u586b\u7a7a\u6d4b\u8bd5\u5e72\u6270\u9879\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u6216\u9759\u6001\u751f\u6210\u6a21\u578b\u7684\u586b\u7a7a\u6d4b\u8bd5\u5e72\u6270\u9879\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6DualReward\uff0c\u5176\u6838\u5fc3\u5728\u4e8e\u4f7f\u7528\u4e00\u79cd\u5305\u542b\u81ea\u9002\u5e94\u7f29\u653e\u7684\u53cc\u5956\u52b1\u7ed3\u6784\uff0c\u4ee5\u533a\u5206\u4eba\u7c7b\u521b\u5efa\u7684\u4f18\u8d28\u5e72\u6270\u9879\u548c\u6a21\u578b\u751f\u6210\u7684\u5019\u9009\u5e72\u6270\u9879\uff0c\u5e76\u6839\u636e\u6a21\u578b\u7684\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4fe1\u53f7\u7684\u5f3a\u5ea6\u3002", "result": "\u5728CLOTH-F\uff08\u6bb5\u843d\u7ea7\uff09\u548cMCQ\uff08\u53e5\u5b50\u7ea7\uff09\u4e24\u4e2a\u586b\u7a7a\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDualReward\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5728CLOTH-F\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7a33\u6b65\u63d0\u5347\uff0c\u5728MCQ\u6570\u636e\u96c6\u4e0a\u5219\u5e26\u6765\u4e86\u66f4\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08P@1\u63d0\u53473.48-3.86%\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u591a\u6837\u5316\u95ee\u9898\u7c7b\u578b\u548c\u9886\u57df\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "DualReward\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5305\u542b\u81ea\u9002\u5e94\u7f29\u653e\u7684\u53cc\u5956\u52b1\u7ed3\u6784\uff0c\u5728\u81ea\u52a8\u751f\u6210\u586b\u7a7a\u6d4b\u8bd5\u5e72\u6270\u9879\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5c24\u5176\u5728\u5904\u7406\u591a\u6837\u5316\u8de8\u9886\u57df\u6570\u636e\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.11761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11761", "abs": "https://arxiv.org/abs/2507.11761", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning", "comment": null, "summary": "Abstract visual reasoning (AVR) enables humans to quickly discover and\ngeneralize abstract rules to new scenarios. Designing intelligent systems with\nhuman-like AVR abilities has been a long-standing topic in the artificial\nintelligence community. Deep AVR solvers have recently achieved remarkable\nsuccess in various AVR tasks. However, they usually use task-specific designs\nor parameters in different tasks. In such a paradigm, solving new tasks often\nmeans retraining the model, and sometimes retuning the model architectures,\nwhich increases the cost of solving AVR problems. In contrast to task-specific\napproaches, this paper proposes a novel Unified Conditional Generative Solver\n(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we\nprove that some well-known AVR tasks can be reformulated as the problem of\nestimating the predictability of target images in problem panels. Then, we\nillustrate that, under the proposed framework, training one conditional\ngenerative model can solve various AVR tasks. The experiments show that with a\nsingle round of multi-task training, UCGS demonstrates abstract reasoning\nability across various AVR tasks. Especially, UCGS exhibits the ability of\nzero-shot reasoning, enabling it to perform abstract reasoning on problems from\nunseen AVR tasks in the testing phase.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUCGS\u7684\u7edf\u4e00\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u5b83\u80fd\u5728\u4e00\u4e2a\u6846\u67b6\u5185\u89e3\u51b3\u591a\u4e2a\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u80fd\u5728\u672a\u89c1\u8fc7\u7684\u65b0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u7f3a\u70b9\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6AVR\u6c42\u89e3\u5668\u901a\u5e38\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8fdb\u884c\u7279\u5b9a\u8bbe\u8ba1\u6216\u53c2\u6570\u8c03\u6574\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b0\u4efb\u52a1\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u751a\u81f3\u8c03\u6574\u6a21\u578b\u67b6\u6784\uff0c\u589e\u52a0\u4e86\u6210\u672c\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff08UCGS\uff09\uff0c\u5c06\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff08AVR\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u4f30\u8ba1\u95ee\u9898\u9762\u677f\u4e2d\u76ee\u6807\u56fe\u50cf\u7684\u53ef\u9884\u6d4b\u6027\u3002\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u5185\u89e3\u51b3\u591a\u4e2aAVR\u4efb\u52a1\u3002", "result": "UCGS\u5728\u5355\u4e00\u8f6e\u6b21\u7684\u591a\u4efb\u52a1\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u8de8\u8d8a\u591a\u4e2aAVR\u4efb\u52a1\u5c55\u73b0\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002\u7279\u522b\u5730\uff0cUCGS\u8868\u73b0\u51fa\u96f6\u6837\u672c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u5728\u6d4b\u8bd5\u9636\u6bb5\u5904\u7406\u672a\u89c1\u8fc7\u7684AVR\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff08UCGS\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u5355\u4e00\u6846\u67b6\u89e3\u51b3\u591a\u4e2a\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002UCGS\u901a\u8fc7\u5c06AVR\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u4f30\u8ba1\u95ee\u9898\u9762\u677f\u4e2d\u76ee\u6807\u56fe\u50cf\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u6761\u4ef6\u751f\u6210\u6a21\u578b\u6765\u5904\u7406\u4e0d\u540c\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u751a\u81f3\u5728\u672a\u89c1\u8fc7\u7684AVR\u4efb\u52a1\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406\u3002"}}
{"id": "2507.11885", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11885", "abs": "https://arxiv.org/abs/2507.11885", "authors": ["Nishan Amgain", "Mahir Rahman", "Umar Arshad", "Fernando Romero Consuegra", "Emil Sayahi", "Imran M. Mirza"], "title": "Tripartite Entanglement in Multimode Cavity Quantum Electrodynamics", "comment": "11 pages, 5 figures", "summary": "We numerically investigate the generation and dynamics of tripartite\nentanglement among qubits (quantum emitters or atoms) in multimode cavity\nquantum electrodynamics (cQED). Our cQED architecture features three initially\nunentangled excited two-level quantum emitters confined within a\ntriangle-shaped multimode optical cavity, which later become entangled due to a\nJaynes-Cummings-like interaction. Using the tripartite negativity measure of\nentanglement and fidelity with respect to the genuine tripartite entangled\nstate (Greenberger-Horne-Zeilinger (or GHZ) state, to be precise), we analyze\nthe impact of the number of cavity modes, qubit locations, and losses\n(spontaneous emission from qubits and photon leakage from the cavity mirrors)\non the generated entanglement. Our key results include the presence of two\nkinds of retardation effects: one resulting from the time it takes for photons\nto propagate from one qubit to another, and the other to complete one round\ntrip in the cavity. We observed these retardation effects only in multimode\ncavities, with the exciting possibility of controlling the collapse and revival\npatterns of tripartite entanglement by altering the qubit locations in the\ncavity. Furthermore, the impact of losses on the generated entanglement and the\ndependence of maximum entanglement on the total number of modes yield results\nthat surpass those reported for single and two excitations. With recent\nadvances in circuit quantum electrodynamics, these findings hold promise for\nthe development of entanglement-based quantum networking protocols and quantum\nmemories.", "AI": {"tldr": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u591a\u6a21\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u4e09\u65b9\u7ea0\u7f20\u7684\u751f\u6210\u3001\u52a8\u529b\u5b66\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff08\u6a21\u5f0f\u6570\u91cf\u3001\u6bd4\u7279\u4f4d\u7f6e\u3001\u635f\u8017\uff09\uff0c\u5e76\u53d1\u73b0\u4e86\u53ef\u63a7\u7684\u7ea0\u7f20\u52a8\u529b\u5b66\u548c\u4f18\u4e8e\u5355/\u53cc\u6fc0\u53d1\u7684\u7ea0\u7f20\u6027\u80fd\uff0c\u4e3a\u91cf\u5b50\u7f51\u7edc\u548c\u91cf\u5b50\u5b58\u50a8\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u4e3a\u4e86\u5728\u591a\u6a21\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\uff08cQED\uff09\u67b6\u6784\u4e2d\uff0c\u7814\u7a76\u4e09\u65b9\u7ea0\u7f20\u7684\u751f\u6210\u548c\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\u7ea0\u7f20\u4e0e\u8154\u6a21\u5f0f\u6570\u91cf\u3001\u6bd4\u7279\u4f4d\u7f6e\u4ee5\u53ca\u635f\u8017\uff08\u81ea\u53d1\u8f90\u5c04\u548c\u5149\u5b50\u6cc4\u6f0f\uff09\u7b49\u56e0\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u6570\u503c\u6a21\u62df\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u591a\u6a21\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\uff08cQED\uff09\u4e2d\u6bd4\u7279\uff08\u91cf\u5b50\u53d1\u5c04\u4f53\u6216\u539f\u5b50\uff09\u4e09\u65b9\u7ea0\u7f20\u7684\u751f\u6210\u548c\u52a8\u529b\u5b66\u3002\u5229\u7528\u4e09\u65b9\u8d1f\u71b5\u548c\u4e0e\u771f\u5b9e\u4e09\u65b9\u7ea0\u7f20\u6001\uff08\u7effberger-Horne-Zeilinger\u6001\uff09\u7684\u4fdd\u771f\u5ea6\u6765\u5206\u6790\u6a21\u5f0f\u6570\u91cf\u3001\u6bd4\u7279\u4f4d\u7f6e\u548c\u635f\u8017\uff08\u81ea\u53d1\u8f90\u5c04\u548c\u5149\u5b50\u6cc4\u6f0f\uff09\u7684\u5f71\u54cd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u63ed\u793a\u4e86\u4e24\u79cd\u5ef6\u8fdf\u6548\u5e94\uff1a\u5149\u5b50\u5728\u6bd4\u7279\u95f4\u4f20\u64ad\u7684\u65f6\u95f4\u548c\u5149\u5b50\u5728\u8154\u5185\u5b8c\u6210\u4e00\u4e2a\u6765\u56de\u7684\u65f6\u95f4\u3002\u8fd9\u4e9b\u6548\u5e94\u4ec5\u5728\u591a\u6a21\u8154\u4e2d\u89c2\u5bdf\u5230\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u6539\u53d8\u8154\u5185\u6bd4\u7279\u7684\u4f4d\u7f6e\u6765\u63a7\u5236\u4e09\u65b9\u7ea0\u7f20\u7684\u584c\u7f29\u548c\u590d\u5174\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u635f\u8017\u5bf9\u751f\u6210\u7ea0\u7f20\u7684\u5f71\u54cd\u4ee5\u53ca\u6700\u5927\u7ea0\u7f20\u4e0e\u603b\u6a21\u5f0f\u6570\u91cf\u7684\u4f9d\u8d56\u6027\uff0c\u5176\u7ed3\u679c\u4f18\u4e8e\u5355\u6fc0\u53d1\u548c\u53cc\u6fc0\u53d1\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u7ea0\u7f20\u7684\u91cf\u5b50\u7f51\u7edc\u534f\u8bae\u548c\u91cf\u5b50\u5b58\u50a8\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5e0c\u671b\uff0c\u7279\u522b\u662f\u5728\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\uff08cQED\uff09\u9886\u57df\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u64cd\u7eb5\u8154\u6a21\u5f0f\u3001\u6bd4\u7279\u4f4d\u7f6e\u548c\u635f\u8017\u6765\u63a7\u5236\u4e09\u65b9\u7ea0\u7f20\u52a8\u529b\u5b66\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11702", "abs": "https://arxiv.org/abs/2507.11702", "authors": ["Hein de Wilde", "Ali Mohammed Mansoor Alsahag", "Pierre Blanchet"], "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption", "comment": null, "summary": "Railroad traffic disruption as a result of leaf-fall cost the UK rail\nindustry over 300 million per year and measures to mitigate such disruptions\nare employed on a large scale, with 1.67 million kilometers of track being\ntreated in the UK in 2021 alone. Therefore, the ability to anticipate the\ntiming of leaf-fall would offer substantial benefits for rail network\noperators, enabling the efficient scheduling of such mitigation measures.\nHowever, current methodologies for predicting leaf-fall exhibit considerable\nlimitations in terms of scalability and reliability. This study endeavors to\ndevise a prediction system that leverages specialized prediction methods and\nthe latest satellite data sources to generate both scalable and reliable\ninsights into leaf-fall timings. An LSTM network trained on ground-truth\nleaf-falling data combined with multispectral and meteorological satellite data\ndemonstrated a root-mean-square error of 6.32 days for predicting the start of\nleaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which\nimproves upon previous work on the topic, offers promising opportunities for\nthe optimization of leaf mitigation measures in the railway industry and the\nimprovement of our understanding of complex ecological systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528LSTM\u548c\u536b\u661f\u6570\u636e\u9884\u6d4b\u94c1\u8def\u53f6\u843d\u65f6\u95f4\uff0c\u4ee5\u51cf\u5c11\u4ea4\u901a\u4e2d\u65ad\u635f\u5931\u3002", "motivation": "\u6bcf\u5e74\u7531\u4e8e\u843d\u53f6\u9020\u6210\u7684\u94c1\u8def\u4ea4\u901a\u4e2d\u65ad\u7ed9\u82f1\u56fd\u94c1\u8def\u884c\u4e1a\u5e26\u6765\u8d85\u8fc73\u4ebf\u82f1\u9551\u7684\u635f\u5931\uff0c\u56e0\u6b64\u9884\u6d4b\u843d\u53f6\u65f6\u95f4\u5bf9\u4e8e\u9ad8\u6548\u5b89\u6392\u7f13\u89e3\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u9884\u6d4b\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e00\u79cd\u7ed3\u5408\u4e86\u5730\u9762\u771f\u5b9e\u53f6\u843d\u6570\u636e\u3001\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u548c\u6c14\u8c61\u536b\u661f\u6570\u636e\u7684LSTM\uff08\u957f\u77ed\u671f\u8bb0\u5fc6\uff09\u7f51\u7edc\u6a21\u578b\uff0c\u6765\u9884\u6d4b\u53f6\u843d\u7684\u65f6\u95f4\u3002", "result": "LSTM\u6a21\u578b\u5728\u9884\u6d4b\u53f6\u843d\u5f00\u59cb\u65f6\u95f4\u65b9\u9762\u8fbe\u5230\u4e866.32\u5929\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff0c\u5728\u9884\u6d4b\u53f6\u843d\u7ed3\u675f\u65f6\u95f4\u65b9\u9762\u8fbe\u5230\u4e869.31\u5929\u7684\u5747\u65b9\u6839\u8bef\u5dee\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eLSTM\u548c\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u7684\u53f6\u843d\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u53f6\u843d\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u94c1\u8def\u884c\u4e1a\u7684\u53f6\u843d\u9632\u62a4\u63aa\u65bd\u4f18\u5316\u548c\u751f\u6001\u7cfb\u7edf\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2507.12387", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12387", "abs": "https://arxiv.org/abs/2507.12387", "authors": ["Cunzhi Zhang", "Victor Wen-zhe Yu", "Yu Jin", "Jonah Nagura", "Sevim Polat Genlik", "Maryam Ghazisaeidi", "Giulia Galli"], "title": "Towards dislocation-driven quantum interconnects", "comment": null, "summary": "A central problem in the deployment of quantum technologies is the\nrealization of robust architectures for quantum interconnects. We propose to\nengineer interconnects in semiconductors and insulators by patterning spin\nqubits at dislocations, thus forming quasi one-dimensional lines of entangled\npoint defects. To gain insight into the feasibility and control of\ndislocation-driven interconnects, we investigate the optical cycle and\ncoherence properties of nitrogen-vacancy (NV) centers in diamond, in proximity\nof dislocations, using a combination of advanced first-principles calculations.\nWe show that one can engineer spin defects with properties similar to those of\ntheir bulk counterparts, including charge stability and a favorable optical\ncycle, and that NV centers close to dislocations have much improved coherence\nproperties. Finally, we predict optically detected magnetic resonance spectra\nthat may facilitate the experimental identification of specific defect\nconfigurations. Our results provide a theoretical foundation for the\nengineering of one-dimensional arrays of spin defects in the solid state.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5728\u534a\u5bfc\u4f53\u548c\u7edd\u7f18\u4f53\u7684\u4f4d\u9519\u5904\u523b\u8680\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\uff0c\u4ee5\u5b9e\u73b0\u91cf\u5b50\u4e92\u8fde\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5de5\u7a0b\u5316\u5177\u6709\u826f\u597d\u5149\u5b66\u548c\u7535\u8377\u7279\u6027\u7684\u81ea\u65cb\u7f3a\u9677\uff0c\u5e76\u6539\u5584NV\u4e2d\u5fc3\u7684\u76f8\u5e72\u6027\u3002\u6700\u540e\uff0c\u9884\u6d4b\u7684\u5149\u8c31\u6709\u52a9\u4e8e\u8bc6\u522b\u7f3a\u9677\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u91cf\u5b50\u4e92\u8fde\u7684\u9c81\u68d2\u67b6\u6784\uff0c\u5373\u91cf\u5b50\u6280\u672f\u90e8\u7f72\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u534a\u5bfc\u4f53\u548c\u7edd\u7f18\u4f53\u4e2d\u901a\u8fc7\u5728\u4f4d\u9519\u5904\u523b\u8680\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u6765\u6784\u5efa\u4e92\u8fde\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u5f62\u6210\u51c6\u4e00\u7ef4\u7ea0\u7f20\u70b9\u7f3a\u9677\u7ebf\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u91d1\u521a\u77f3\u4e2d\u6c2e-\u7a7a\u4f4d\uff08NV\uff09\u4e2d\u5fc3\u5728\u4f4d\u9519\u9644\u8fd1\u7684\u5149\u5b66\u5468\u671f\u548c\u76f8\u5e72\u7279\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u5de5\u7a0b\u5316\u5177\u6709\u7c7b\u4f3c\u4e8e\u5176\u5757\u72b6\u5bf9\u5e94\u7269\u7684\u7279\u6027\u7684\u81ea\u65cb\u7f3a\u9677\uff0c\u5305\u62ec\u7535\u8377\u7a33\u5b9a\u6027\u548c\u6709\u5229\u7684\u5149\u5b66\u5468\u671f\u3002\u6b64\u5916\uff0c\u9760\u8fd1\u4f4d\u9519\u7684NV\u4e2d\u5fc3\u7684\u76f8\u5e72\u7279\u6027\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\u3002\u6700\u540e\uff0c\u7814\u7a76\u9884\u6d4b\u4e86\u53ef\u80fd\u6709\u52a9\u4e8e\u5b9e\u9a8c\u8bc6\u522b\u7279\u5b9a\u7f3a\u9677\u6784\u578b\u7684\u5149\u5b66\u63a2\u6d4b\u78c1\u5171\u632f\u5149\u8c31\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56fa\u6001\u4e2d\u4e00\u7ef4\u81ea\u65cb\u7f3a\u9677\u9635\u5217\u7684\u5de5\u7a0b\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.12235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12235", "abs": "https://arxiv.org/abs/2507.12235", "authors": ["Sa\u00fal Fenollosa", "Monika Drozdowska", "Wenfei Yang", "Sergio Mic\u00f3-Rosa", "Alejandro Castilla", "Alejandro Lopez-Escudero", "Jian Li", "Narcis Cardona"], "title": "Frequency-responsive RCS characteristics and scaling implications for ISAC development", "comment": "6 pages, 12 figures, 3 tables. Accepted for publication at the 2024\n  IEEE Global Communications Conference (GLOBECOM), WS-02: Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "This paper presents an investigation on the Radar Cross-Section (RCS) of\nvarious targets, with the objective of analysing how RCS properties vary with\nfrequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and\na full-scale car were measured in the frequency bands referred to in industry\nstandards as FR2 and FR3. Measurements were taken in diverse environments,\nindoors and outdoors, to ensure comprehensive scenario coverage. The\nmethodology employed in RCS extraction performs background subtraction,\nfollowed by time-domain gating to isolate the influence of the target. This\nanalysis compares the RCS values and how the points of greatest contribution\nare distributed across different bands based on the range response of the RCS.\nAnalysis of the results demonstrated how RCS values change with frequency and\ntarget shape, providing insights into the electromagnetic behaviour of these\ntargets. Key findings highlight how much scaling RCS values based on frequency\nand geometry is complex and varies among different types of materials and\nshapes. These insights are instrumental for advancing sensing systems and\nenhancing 3GPP channel models, particularly for Integrated Sensing and\nCommunications (ISAC) techniques proposed for 6G standards.", "AI": {"tldr": "This paper analyzes how Radar Cross-Section (RCS) changes with frequency for different targets (AGV, pedestrian, car) in FR2/FR3 bands. It uses background subtraction and time-domain gating for measurement. Findings show RCS varies with frequency, shape, material, and geometry, which is important for 6G sensing and communication systems.", "motivation": "The objective was to analyze how Radar Cross-Section (RCS) properties vary with frequency for targets such as an AGV, a pedestrian, and a car, providing insights for advancing sensing systems and 3GPP channel models, particularly for Integrated Sensing and Communications (ISAC) techniques proposed for 6G standards.", "method": "The methodology involved background subtraction and time-domain gating to extract RCS, measuring targets like an AGV, pedestrian, and car in FR2 and FR3 frequency bands across indoor and outdoor environments. The analysis compared RCS values and the distribution of greatest contribution points based on range response across different bands.", "result": "Measurements were taken in FR2 and FR3 bands for various targets in diverse environments. The analysis compared RCS values and their distribution across different bands based on range response. Results showed how RCS values change with frequency and target shape, indicating that scaling RCS values based on frequency and geometry is complex and varies among different materials and shapes.", "conclusion": "The paper demonstrates that RCS values change with frequency and target shape, highlighting the complexity of scaling RCS values based on frequency and geometry across different materials and shapes. These findings are crucial for improving sensing systems and 3GPP channel models for ISAC in 6G."}}
{"id": "2507.12444", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12444", "abs": "https://arxiv.org/abs/2507.12444", "authors": ["Man Shi", "Vikram Jain", "Antony Joseph", "Maurice Meijer", "Marian Verhelst"], "title": "BitWave: Exploiting Column-Based Bit-Level Sparsity for Deep Learning Acceleration", "comment": "15 pages, 18 figures, 2024 IEEE International Symposium on\n  High-Performance Computer Architecture (HPCA)", "summary": "Bit-serial computation facilitates bit-wise sequential data processing,\noffering numerous benefits, such as a reduced area footprint and\ndynamically-adaptive computational precision. It has emerged as a prominent\napproach, particularly in leveraging bit-level sparsity in Deep Neural Networks\n(DNNs). However, existing bit-serial accelerators exploit bit-level sparsity to\nreduce computations by skipping zero bits, but they suffer from inefficient\nmemory accesses due to the irregular indices of the non-zero bits.\n  As memory accesses typically are the dominant contributor to DNN accelerator\nperformance, this paper introduces a novel computing approach called\n\"bit-column-serial\" and a compatible architecture design named \"BitWave.\"\nBitWave harnesses the advantages of the \"bit-column-serial\" approach,\nleveraging structured bit-level sparsity in combination with dynamic dataflow\ntechniques. This achieves a reduction in computations and memory footprints\nthrough redundant computation skipping and weight compression. BitWave is able\nto mitigate the performance drop or the need for retraining that is typically\nassociated with sparsity-enhancing techniques using a post-training\noptimization involving selected weight bit-flips. Empirical studies conducted\non four deep-learning benchmarks demonstrate the achievements of BitWave: (1)\nMaximally realize 13.25x higher speedup, 7.71x efficiency compared to\nstate-of-the-art sparsity-aware accelerators. (2) Occupying 1.138 mm2 area and\nconsuming 17.56 mW power in 16nm FinFet process node.", "AI": {"tldr": "BitWave\u662f\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u548c\u67b6\u6784\uff0c\u901a\u8fc7\u4f4d\u5217\u4e32\u884c\u548c\u52a8\u6001\u6570\u636e\u6d41\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f4d\u4e32\u884c\u52a0\u901f\u5668\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u63d0\u5347\u901f\u5ea6\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u4f4d\u4e32\u884c\u52a0\u901f\u5668\u5728\u5229\u7528\u4f4d\u7ea7\u7a00\u758f\u6027\u65f6\uff0c\u7531\u4e8e\u975e\u96f6\u4f4d\u7684\u7d22\u5f15\u4e0d\u89c4\u5219\uff0c\u5bfc\u81f4\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u5185\u5b58\u8bbf\u95ee\u901a\u5e38\u662fDNN\u52a0\u901f\u5668\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4f4d\u5217\u4e32\u884c\u201d\u7684\u65b0\u578b\u8ba1\u7b97\u65b9\u6cd5\u548c\u517c\u5bb9\u7684\u201cBitWave\u201d\u67b6\u6784\u8bbe\u8ba1\uff0c\u5229\u7528\u7ed3\u6784\u5316\u4f4d\u7ea7\u7a00\u758f\u6027\uff0c\u7ed3\u5408\u52a8\u6001\u6570\u636e\u6d41\u6280\u672f\uff0c\u901a\u8fc7\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\u548c\u6743\u91cd\u538b\u7f29\u6765\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "BitWave\u572816nm FinFet\u5de5\u827a\u8282\u70b9\u4e0b\uff0c\u5b9e\u73b0\u4e861.138 mm2\u7684\u9762\u79ef\u548c17.56 mW\u7684\u529f\u8017\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u7a00\u758f\u611f\u77e5\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u6700\u9ad8\u53ef\u5b9e\u73b013.25\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c7.71\u500d\u7684\u6548\u7387\u63d0\u5347\u3002", "conclusion": "BitWave\u901a\u8fc7\u4f4d\u5217\u4e32\u884c\u65b9\u6cd5\u548cBitWave\u67b6\u6784\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u4f4d\u7ea7\u7a00\u758f\u6027\u548c\u52a8\u6001\u6570\u636e\u6d41\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5360\u7528\u7684\u51cf\u5c11\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u540e\u4f18\u5316\u7f13\u89e3\u4e86\u7a00\u758f\u6027\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u56db\u4e2a\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12067", "abs": "https://arxiv.org/abs/2507.12067", "authors": ["Xing Tong", "Michele D. Simoni"], "title": "Robust Route Planning for Sidewalk Delivery Robots", "comment": null, "summary": "Sidewalk delivery robots are a promising solution for urban freight\ndistribution, reducing congestion compared to trucks and providing a safer,\nhigher-capacity alternative to drones. However, unreliable travel times on\nsidewalks due to pedestrian density, obstacles, and varying infrastructure\nconditions can significantly affect their efficiency. This study addresses the\nrobust route planning problem for sidewalk robots, explicitly accounting for\ntravel time uncertainty due to varying sidewalk conditions. Optimization is\nintegrated with simulation to reproduce the effect of obstacles and pedestrian\nflows and generate realistic travel times. The study investigates three\ndifferent approaches to derive uncertainty sets, including budgeted,\nellipsoidal, and support vector clustering (SVC)-based methods, along with a\ndistributionally robust method to solve the shortest path (SP) problem. A\nrealistic case study reproducing pedestrian patterns in Stockholm's city center\nis used to evaluate the efficiency of robust routing across various robot\ndesigns and environmental conditions. The results show that, when compared to a\nconventional SP, robust routing significantly enhances operational reliability\nunder variable sidewalk conditions. The Ellipsoidal and DRSP approaches\noutperform the other methods, yielding the most efficient paths in terms of\naverage and worst-case delay. Sensitivity analyses reveal that robust\napproaches consistently outperform the conventional SP, particularly for\nsidewalk delivery robots that are wider, slower, and have more conservative\nnavigation behaviors. These benefits are even more pronounced in adverse\nweather conditions and high pedestrian congestion scenarios.", "AI": {"tldr": "\u4eba\u884c\u9053\u914d\u9001\u673a\u5668\u4eba\u7684\u51fa\u884c\u65f6\u95f4\u56e0\u884c\u4eba\u3001\u969c\u788d\u7269\u548c\u8def\u51b5\u800c\u5f02\u3002\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u548c\u6a21\u62df\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u9884\u7b97\u3001\u692d\u5706\u548c\u652f\u6301\u5411\u91cf\u805a\u7c7b\uff08SVC\uff09\u7b49\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u692d\u5706\u548cDRSP\u65b9\u6cd5\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u8f83\u5bbd\u3001\u8f83\u6162\u6216\u5728\u6076\u52a3\u5929\u6c14\u548c\u9ad8\u62e5\u6324\u73af\u5883\u4e0b\u8fd0\u884c\u65f6\u3002", "motivation": "\u4eba\u884c\u9053\u914d\u9001\u673a\u5668\u4eba\u662f\u57ce\u5e02\u8d27\u8fd0\u5206\u9500\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0e\u5361\u8f66\u76f8\u6bd4\u53ef\u51cf\u5c11\u62e5\u5835\uff0c\u5e76\u4e3a\u65e0\u4eba\u673a\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u7684\u5bb9\u91cf\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7531\u4e8e\u884c\u4eba\u5bc6\u5ea6\u3001\u969c\u788d\u7269\u548c\u57fa\u7840\u8bbe\u65bd\u6761\u4ef6\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u4eba\u884c\u9053\u51fa\u884c\u65f6\u95f4\u4e0d\u53ef\u9760\uff0c\u53ef\u80fd\u4f1a\u4e25\u91cd\u5f71\u54cd\u5176\u6548\u7387\u3002\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u4eba\u884c\u9053\u673a\u5668\u4eba\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u660e\u786e\u8003\u8651\u4e86\u7531\u4e8e\u4eba\u884c\u9053\u6761\u4ef6\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u6574\u5408\u4e86\u4f18\u5316\u548c\u6a21\u62df\uff0c\u4ee5\u91cd\u73b0\u969c\u788d\u7269\u548c\u884c\u4eba\u6d41\u91cf\u7684\u5f71\u54cd\u5e76\u751f\u6210\u771f\u5b9e\u7684\u51fa\u884c\u65f6\u95f4\u3002\u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u63a8\u5bfc\u4e0d\u786e\u5b9a\u6027\u96c6\uff0c\u5305\u62ec\u9884\u7b97\u3001\u692d\u5706\u548c\u652f\u6301\u5411\u91cf\u805a\u7c7b\uff08SVC\uff09\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\u6765\u89e3\u51b3\u6700\u77ed\u8def\u5f84\uff08SP\uff09\u95ee\u9898\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u8def\u5f84\uff08SP\uff09\u76f8\u6bd4\uff0c\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u5728\u4e0d\u540c\u7684\u8def\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u8425\u53ef\u9760\u6027\u3002\u692d\u5706\u548c\u5206\u5e03\u9c81\u68d2\u8def\u5f84\uff08DRSP\uff09\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728\u5e73\u5747\u548c\u6700\u5dee\u60c5\u51b5\u5ef6\u8fdf\u65b9\u9762\u4ea7\u751f\u4e86\u6700\u9ad8\u6548\u7684\u8def\u5f84\u3002\u654f\u611f\u6027\u5206\u6790\u8868\u660e\uff0c\u4e0e\u4f20\u7edfSP\u76f8\u6bd4\uff0c\u9c81\u68d2\u65b9\u6cd5\u59cb\u7ec8\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u66f4\u5bbd\u3001\u66f4\u6162\u3001\u5bfc\u822a\u884c\u4e3a\u66f4\u4fdd\u5b88\u7684\u8def\u8fb9\u914d\u9001\u673a\u5668\u4eba\u3002\u5728\u6076\u52a3\u5929\u6c14\u548c\u9ad8\u884c\u4eba\u62e5\u6324\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u5728\u5404\u79cd\u673a\u5668\u4eba\u8bbe\u8ba1\u548c\u73af\u5883\u6761\u4ef6\u4e0b\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u8def\u5f84\uff08SP\uff09\u76f8\u6bd4\uff0c\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u5728\u4e0d\u540c\u7684\u8def\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u8425\u53ef\u9760\u6027\u3002\u5728\u8003\u8651\u7684\u5404\u79cd\u65b9\u6cd5\u4e2d\uff0c\u692d\u5706\u548c\u5206\u5e03\u9c81\u68d2\u8def\u5f84\uff08DRSP\uff09\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728\u5e73\u5747\u548c\u6700\u5dee\u60c5\u51b5\u5ef6\u8fdf\u65b9\u9762\u4ea7\u751f\u4e86\u6700\u9ad8\u6548\u7684\u8def\u5f84\u3002\u654f\u611f\u6027\u5206\u6790\u8868\u660e\uff0c\u4e0e\u4f20\u7edfSP\u76f8\u6bd4\uff0c\u9c81\u68d2\u65b9\u6cd5\u59cb\u7ec8\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u66f4\u5bbd\u3001\u66f4\u6162\u3001\u5bfc\u822a\u884c\u4e3a\u66f4\u4fdd\u5b88\u7684\u8def\u8fb9\u914d\u9001\u673a\u5668\u4eba\u3002\u5728\u6076\u52a3\u5929\u6c14\u548c\u9ad8\u884c\u4eba\u62e5\u6324\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002"}}
{"id": "2507.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11878", "abs": "https://arxiv.org/abs/2507.11878", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "title": "LLMs Encode Harmfulness and Refusal Separately", "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "LLM\u7684\u5b89\u5168\u673a\u5236\u6bd4\u5176\u62d2\u7edd\u884c\u4e3a\u66f4\u80fd\u53cd\u6620\u5176\u5bf9\u6709\u5bb3\u6027\u7684\u7406\u89e3\u3002\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u4e2a\u72ec\u7acb\u7684\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\uff0c\u53ef\u7528\u4e8e\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f93\u5165\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatent Guard\u7684\u5185\u7f6e\u5b89\u5168\u673a\u5236\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u62d2\u7edd\u6709\u5bb3\u6307\u4ee4\u65f6\uff0c\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e86\u201c\u6709\u5bb3\u6027\u201d\u672c\u8eab\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8868\u73b0\u51fa\u62d2\u7edd\u884c\u4e3a\u3002\u7814\u7a76\u73b0\u6709\u5de5\u4f5c\u63ed\u793a\u7684\u201c\u62d2\u7edd\u65b9\u5411\u201d\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5168\u9762\u89e3\u91caLLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u56e0\u6b64\u63d0\u51fa\u8bc6\u522b\u5e76\u5206\u6790\u72ec\u7acb\u7684\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u5e76\u64cd\u7eb5LLM\u5185\u90e8\u7f16\u7801\u7684\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\u6765\u5206\u6790\u5176\u5b89\u5168\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6cbf\u7740\u6709\u5bb3\u6027\u65b9\u5411\u8fdb\u884c\u5e72\u9884\u4f1a\u6539\u53d8\u6a21\u578b\u5bf9\u6307\u4ee4\u6709\u5bb3\u6027\u7684\u5224\u65ad\uff0c\u800c\u6cbf\u7740\u62d2\u7edd\u65b9\u5411\u5219\u76f4\u63a5\u5f15\u53d1\u62d2\u7edd\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u67d0\u4e9b\u8d8a\u72f1\u65b9\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\u4ee5\u53ca\u5bf9\u6297\u6027\u5fae\u8c03\u5bf9\u6a21\u578b\u5185\u90e8\u6709\u5bb3\u6027\u7406\u89e3\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e86\u4e0e\u62d2\u7edd\u65b9\u5411\u4e0d\u540c\u7684\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\u3002\u6cbf\u6709\u5bb3\u6027\u65b9\u5411\u64cd\u7eb5\u53ef\u4f7fLLM\u5c06\u65e0\u5bb3\u6307\u4ee4\u8bef\u5224\u4e3a\u6709\u5bb3\uff1b\u800c\u6cbf\u62d2\u7edd\u65b9\u5411\u64cd\u7eb5\u5219\u76f4\u63a5\u5bfc\u81f4\u62d2\u7edd\uff0c\u4f46\u4e0d\u4f1a\u6539\u53d8\u6a21\u578b\u5bf9\u6709\u5bb3\u6027\u7684\u5224\u65ad\u3002\u90e8\u5206\u8d8a\u72f1\u65b9\u6cd5\u901a\u8fc7\u964d\u4f4e\u62d2\u7edd\u4fe1\u53f7\u800c\u975e\u6539\u53d8\u5185\u90e8\u6709\u5bb3\u6027\u5224\u65ad\u6765\u594f\u6548\u3002\u5bf9\u6297\u6027\u5fae\u8c03\u5bf9\u6709\u5bb3\u6027\u7406\u89e3\u5f71\u54cd\u751a\u5fae\u3002\u63d0\u51fa\u7684Latent Guard\u4f5c\u4e3a\u4e00\u79cd\u5185\u7f6e\u5b89\u5168\u673a\u5236\uff0c\u5728\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f93\u5165\u548c\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u5ab2\u7f8e\u751a\u81f3\u4f18\u4e8eLlama Guard 3 8B\u3002", "conclusion": "LLMs\u5bf9\u6709\u5bb3\u6307\u4ee4\u7684\u62d2\u7edd\u884c\u4e3a\u53ef\u80fd\u4e0e\u5176\u5bf9\u6709\u5bb3\u6027\u7684\u5185\u90e8\u7406\u89e3\u662f\u5206\u79bb\u7684\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\uff0c\u72ec\u7acb\u4e8e\u201c\u62d2\u7edd\u65b9\u5411\u201d\uff0c\u53ef\u4ee5\u64cd\u7eb5\u6a21\u578b\u5c06\u65e0\u5bb3\u6307\u4ee4\u8bef\u5224\u4e3a\u6709\u5bb3\u3002\u8fd9\u79cd\u6709\u5bb3\u6027\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5185\u7f6e\u7684\u5b89\u5168\u673a\u5236\uff08Latent Guard\uff09\uff0c\u6709\u6548\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f93\u5165\u5e76\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\uff0c\u5e76\u4e14\u80fd\u62b5\u6297\u5fae\u8c03\u653b\u51fb\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u4e13\u7528\u5b89\u5168\u6a21\u578b\u5ab2\u7f8e\u3002"}}
{"id": "2507.11834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11834", "abs": "https://arxiv.org/abs/2507.11834", "authors": ["Peiwen Xia", "Tangfei Liao", "Wei Zhu", "Danhuai Zhao", "Jianjun Ke", "Kaihao Zhang", "Tong Lu", "Tao Wang"], "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "comment": "Accepted by ECAI 2025", "summary": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11890", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.11890", "abs": "https://arxiv.org/abs/2507.11890", "authors": ["Jianmin Wang", "Rong Zhu", "Yue Li", "Z. Y. Ou"], "title": "Observation of quantum noise reduction in a Raman amplifier via quantum correlation between atom and light", "comment": "5 pages,5 figures", "summary": "Any amplifier requires coupling to its internal degrees of freedom for energy\ngain. This coupling introduces extra quantum noise to the output. On the other\nhand, if the internal degree of the amplifier can be accessed and manipulated,\nwe can manage and even reduce the quantum noise of the amplifier's output. In\nthis paper, we present an experiment to reduce the quantum noise of a Raman\namplifier by preparing the atomic medium in a correlated state with the Stokes\nlight field. We report an observation of quantum noise reduction of more than\n3.5 dB in the atomic Raman amplification process. From another perspective, the\nRaman amplifier at high gain in turn serves as a measurement tool for the\nquantum correlation between the atom and light. Furthermore, such a scheme,\nwhen viewed as a whole, also forms a quantum-entangled atom-light hybrid\ninterferometer that can lead to quantum-enhanced sensors.", "AI": {"tldr": "\u5b9e\u9a8c\u901a\u8fc7\u5236\u5907\u539f\u5b50\u4e0e\u5149\u573a\u7684\u5173\u8054\u6001\uff0c\u5b9e\u73b0\u4e86\u62c9\u66fc\u653e\u5927\u5668\u8d85\u8fc73.5dB\u7684\u91cf\u5b50\u566a\u58f0\u6291\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u91cf\u5b50\u4f20\u611f\u548c\u6d4b\u91cf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u653e\u5927\u5668\u5728\u653e\u5927\u80fd\u91cf\u7684\u540c\u65f6\u4f1a\u5f15\u5165\u91cf\u5b50\u566a\u58f0\uff0c\u4f46\u82e5\u80fd\u64cd\u63a7\u653e\u5927\u5668\u7684\u5185\u90e8\u81ea\u7531\u5ea6\uff0c\u5219\u53ef\u4ee5\u7ba1\u7406\u751a\u81f3\u964d\u4f4e\u8f93\u51fa\u7aef\u7684\u91cf\u5b50\u566a\u58f0\u3002", "method": "\u901a\u8fc7\u5236\u5907\u539f\u5b50\u4ecb\u8d28\u4e0e\u65af\u6258\u514b\u65af\u5149\u573a\u7684\u5173\u8054\u6001\uff0c\u5b9e\u9a8c\u6f14\u793a\u4e86\u964d\u4f4e\u62c9\u66fc\u653e\u5927\u5668\u7684\u91cf\u5b50\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u8d85\u8fc73.5dB\u7684\u91cf\u5b50\u566a\u58f0\u6291\u5236\uff0c\u5e76\u5c06\u62c9\u66fc\u653e\u5927\u5668\u4f5c\u4e3a\u6d4b\u91cf\u539f\u5b50\u4e0e\u5149\u91cf\u5b50\u5173\u8054\u7684\u5de5\u5177\uff0c\u540c\u65f6\u8be5\u65b9\u6848\u6784\u6210\u4e86\u4e00\u4e2a\u91cf\u5b50\u7ea0\u7f20\u539f\u5b50-\u5149\u6df7\u5408\u5e72\u6d89\u4eea\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u91cf\u5b50\u589e\u5f3a\u578b\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u53ef\u5b9e\u73b0\u8d85\u8fc73.5dB\u7684\u91cf\u5b50\u566a\u58f0\u6291\u5236\u3002"}}
{"id": "2507.11706", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11706", "abs": "https://arxiv.org/abs/2507.11706", "authors": ["Taira Tsuchiya", "Shinji Ito", "Haipeng Luo"], "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs", "comment": "40 pages", "summary": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u504f\u597d\u5bfc\u5411\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PbMDPs\uff09\uff0c\u5e76\u4e3a\u57fa\u4e8eBorda\u5f97\u5206\u7684PbMDPs\u5efa\u7acb\u4e86\u9057\u61be\u4e0b\u754c\u3002\u7814\u7a76\u63d0\u51fa\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5728\u5df2\u77e5\u548c\u672a\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\u5747\u80fd\u8fbe\u5230\u63a5\u8fd1\u7406\u8bba\u4e0b\u754c\u7684\u9057\u61be\u754c\u9650\u3002", "motivation": "\u5728\u6807\u51c6\u7247\u6bb5\u5f0f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u667a\u80fd\u4f53\u76f4\u63a5\u89c2\u6d4b\u5230\u635f\u5931\u7684\u6570\u503c\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u53ea\u80fd\u83b7\u5f97\u5173\u4e8e\u4e24\u4e2a\u9009\u9879\u4e4b\u95f4\u504f\u597d\u7684\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u504f\u597d\u5bfc\u5411\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PbMDPs\uff09\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u5019\u9009\u52a8\u4f5c\u7684\u504f\u597d\u6765\u5b66\u4e60\u3002", "method": "\u672c\u6587\u9996\u5148\u4e3a\u5177\u6709\u5bf9\u6297\u6027\u635f\u5931\u7684\u7247\u6bb5\u5f0fMDPs\u5efa\u7acb\u4e86\u4e00\u4e2a$\r\n Omega(\\sqrt{HSAT})$\u7684\u9057\u61be\u4e0b\u754c\uff0c\u7136\u540e\u5229\u7528\u8be5\u6784\u9020\u63a8\u5bfc\u51fa\u4e86\u57fa\u4e8eBorda\u5f97\u5206\u7684PbMDPs\u7684\u9057\u61be\u4e0b\u754c\u4e3a $\r\n Omega( (H^2 S K)^{1/3} T^{2/3} )$\u3002\u63a5\u7740\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u7ebf\u6027\u4f18\u5316\u7684\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5df2\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u9057\u61be\u754c\u9650\u4e3a$\r\n \tilde{O}((H^2 S^2 K)^{1/3} T^{2/3} )$\u3002\u4e3a\u4e86\u514b\u670d\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5176\u9057\u61be\u754c\u9650\u5728\u5df2\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\u7ea6\u4e3a$\r\n \tilde{O}( (H^6 S K^5)^{1/3} T^{2/3} )$\uff0c\u5e76\u8fdb\u4e00\u6b65\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u672a\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u3002", "result": "\u672c\u7814\u7a76\u4e3a\u57fa\u4e8eBorda\u5f97\u5206\u7684PbMDPs\u5efa\u7acb\u4e86\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u5176\u4e2d\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5728\u5df2\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u9057\u61be\u754c\u9650\u7ea6\u4e3a$\r\n \tilde{O}( (H^6 S K^5)^{1/3} T^{2/3} )$\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u4e86\u672a\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u504f\u597d\u5bfc\u5411\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PbMDPs\uff09\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u5019\u9009\u52a8\u4f5c\u7684\u504f\u597d\u6765\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u89c2\u6d4b\u635f\u5931\u503c\u3002\u7814\u7a76\u4e86\u57fa\u4e8eBorda\u5f97\u5206\u7684PbMDPs\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u9057\u61be\u754c\u9650\u3002"}}
{"id": "2507.12388", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12388", "abs": "https://arxiv.org/abs/2507.12388", "authors": ["Jiahui Liu", "Shuo Cao", "Yanzhou Wang", "Zheyong Fan", "Guocai Lv", "Ping Qian", "Yanjing Su"], "title": "Revealing the impact of chemical short-range order on radiation damage in MoNbTaVW high-entropy alloys using a machine-learning potential", "comment": null, "summary": "The effect of chemical short-range order (CSRO) on primary radiation damage\nin MoNbTaVW high-entropy alloys is investigated using hybrid Monte\nCarlo/molecular dynamics simulations with a machine-learned potential. We show\nthat CSRO enhances radiation tolerance by promoting interstitial diffusion\nwhile suppressing vacancy migration, thereby increasing defect recombination\nefficiency during recovery stage. However, CSRO is rapidly degraded under\ncumulative irradiation, with Warren-Cowley parameters dropping below 0.3 at a\ndose of only 0.03~dpa. This loss of ordering reduces the long-term enhancement\nof CSRO on radiation resistance. Our results highlight that while CSRO can\neffectively improve the radiation tolerance of MoNbTaVW, its stability under\nirradiation is critical to realizing and sustaining this benefit.", "AI": {"tldr": "CSRO\u5728MoNbTaVW\u5408\u91d1\u4e2d\u80fd\u63d0\u5347\u6297\u8f90\u7167\u80fd\u529b\uff0c\u4f46\u5176\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u5728\u8f90\u7167\u4e0b\u4f1a\u9000\u5316\uff0c\u5f71\u54cd\u957f\u671f\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u5316\u5b66\u77ed\u7a0b\u6709\u5e8f\uff08CSRO\uff09\u5bf9MoNbTaVW\u9ad8\u71b5\u5408\u91d1\u4e2d\u4e3b\u8981\u8f90\u5c04\u635f\u4f24\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u8f90\u5c04\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5bf9\u8f90\u5c04\u6297\u6027\u7684\u957f\u671f\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u8499\u7279\u5361\u6d1b/\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u52bf\u80fd\uff0c\u7814\u7a76\u4e86\u5316\u5b66\u77ed\u7a0b\u6709\u5e8f\uff08CSRO\uff09\u5bf9MoNbTaVW\u9ad8\u71b5\u5408\u91d1\u4e2d\u4e3b\u8981\u8f90\u7167\u635f\u4f24\u7684\u5f71\u54cd\u3002", "result": "CSRO\u901a\u8fc7\u4fc3\u8fdb\u95f4\u9699\u6269\u6563\u548c\u6291\u5236\u7a7a\u4f4d\u8fc1\u79fb\u6765\u589e\u5f3a\u6297\u8f90\u7167\u80fd\u529b\uff0c\u4ece\u800c\u5728\u6062\u590d\u9636\u6bb5\u63d0\u9ad8\u7f3a\u9677\u590d\u5408\u6548\u7387\u3002\u7136\u800c\uff0cCSRO\u5728\u7d2f\u79ef\u8f90\u7167\u4e0b\u4f1a\u8fc5\u901f\u9000\u5316\uff0c\u57280.03 dpa\u7684\u5242\u91cf\u4e0b\uff0cWarren-Cowley\u53c2\u6570\u4f1a\u964d\u81f30.3\u4ee5\u4e0b\u3002\u8fd9\u79cd\u6709\u5e8f\u6027\u7684\u4e27\u5931\u964d\u4f4e\u4e86CSRO\u5bf9\u8f90\u5c04\u6297\u6027\u7684\u957f\u671f\u589e\u5f3a\u4f5c\u7528\u3002", "conclusion": "CSRO\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8MoNbTaVW\u7684\u6297\u8f90\u7167\u80fd\u529b\uff0c\u4f46\u5176\u5728\u8f90\u7167\u4e0b\u7684\u7a33\u5b9a\u6027\u5bf9\u4e8e\u5b9e\u73b0\u548c\u7ef4\u6301\u8fd9\u79cd\u76ca\u5904\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.12301", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.12301", "abs": "https://arxiv.org/abs/2507.12301", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning", "comment": null, "summary": "Deep learning-based implicit channel state information (CSI) feedback has\nbeen introduced to enhance spectral efficiency in massive MIMO systems.\nExisting methods often show performance degradation in ultra-low-rate scenarios\nand inadaptability across diverse environments. In this paper, we propose\nDual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework\nincorporating two novel plug-in preprocessing modules to achieve ultra-low\nfeedback rates while maintaining high environmental robustness. First, a novel\nbi-directional correlation enhancement module is proposed to strengthen the\ncorrelation between uplink and downlink CSI eigenvector matrices. This module\nprojects highly correlated uplink and downlink channel matrices into their\nrespective eigenspaces, effectively reducing redundancy for ultra-low-rate\nfeedback. Second, an innovative input format alignment module is designed to\nmaintain consistent data distributions at both encoder and decoder sides\nwithout extra transmission overhead, thereby enhancing robustness against\nenvironmental variations. Finally, we develop an efficient transformer-based\nimplicit CSI feedback network to exploit angular-delay domain sparsity and\nbi-directional correlation for ultra-low-rate CSI compression. Simulation\nresults demonstrate successful reduction of the feedback overhead by 85%\ncompared with the state-of-the-art method and robustness against unseen\nenvironments.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u53cc\u5411\u76f8\u5173\u6027\u589e\u5f3a\u548c\u8f93\u5165\u683c\u5f0f\u5bf9\u9f50\u6a21\u5757\uff0cDual-ImRUNet\u6846\u67b6\u5b9e\u73b0\u4e86\u8d85\u4f4e\u6bd4\u7279\u7387\u7684\u9690\u5f0fCSI\u53cd\u9988\uff0c\u5e76\u63d0\u9ad8\u4e86\u73af\u5883\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53cd\u9988\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9690\u5f0fCSI\u53cd\u9988\u65b9\u6cd5\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u573a\u666f\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u73af\u5883\u4e4b\u95f4\u9002\u5e94\u6027\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-ImRUNet\u7684\u53cc\u5411\u9690\u5f0f\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u53cd\u9988\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u65b0\u7684\u5373\u63d2\u5373\u7528\u9884\u5904\u7406\u6a21\u5757\uff1a1.\u53cc\u5411\u76f8\u5173\u6027\u589e\u5f3a\u6a21\u5757\uff0c\u7528\u4e8e\u589e\u5f3a\u4e0a\u4e0b\u884c\u94fe\u8defCSI\u7279\u5f81\u5411\u91cf\u77e9\u9635\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u5c06\u9ad8\u5ea6\u76f8\u5173\u7684\u4e0a\u4e0b\u884c\u94fe\u8def\u4fe1\u9053\u77e9\u9635\u6295\u5f71\u5230\u5404\u81ea\u7684\u7279\u5f81\u7a7a\u95f4\u6765\u51cf\u5c11\u5197\u4f59\uff0c\u4ee5\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u7387\u53cd\u9988\u30022.\u8f93\u5165\u683c\u5f0f\u5bf9\u9f50\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u4e0d\u589e\u52a0\u989d\u5916\u4f20\u8f93\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u7f16\u89e3\u7801\u5668\u4e24\u4fa7\u6570\u636e\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u5bf9\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u9690\u5f0fCSI\u53cd\u9988\u7f51\u7edc\uff0c\u5229\u7528\u89d2\u5ea6-\u5ef6\u8fdf\u57df\u7a00\u758f\u6027\u548c\u53cc\u5411\u76f8\u5173\u6027\u8fdb\u884c\u8d85\u4f4e\u6bd4\u7279\u7387CSI\u538b\u7f29\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53cd\u9988\u5f00\u9500\u51cf\u5c11\u4e8685%\uff0c\u5e76\u4e14\u5728\u672a\u77e5\u7684\u73af\u5883\u4e2d\u4e5f\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684Dual-ImRUNet\u6846\u67b6\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u548c\u73af\u5883\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5c06\u53cd\u9988\u5f00\u9500\u51cf\u5c1185%\uff0c\u5e76\u80fd\u9002\u5e94\u672a\u77e5\u7684\u73af\u5883\u3002"}}
{"id": "2507.12319", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2507.12319", "abs": "https://arxiv.org/abs/2507.12319", "authors": ["Maritza Ahumada", "Natalia Valderrama-Quinteros", "Diego Tancara", "Guillermo Romero"], "title": "Hybrid quantum lattice model: Polaritons, photons, and spin waves propagation", "comment": "11 pages, 8 figures", "summary": "Controlling the propagation of quantum excitations in low-dimensional quantum\nsystems is pivotal for advancing quantum technologies, including communication\nnetworks and quantum simulators. We propose a one-dimensional hybrid quantum\nlattice model comprising coupled cavity quantum electrodynamics (QED) units.\nEach unit integrates a single-mode cavity that interacts with a two-level\nsystem (TLS), featuring direct coupling between adjacent TLLs. This\nconfiguration enables the coherent propagation of polaritons, spin waves, and\nphotons, depending on the interplay between light-matter coupling and spin-spin\ninteractions. Employing the time-evolving block decimation (TEBD) algorithm, we\nsimulate the dynamics of various excitation configurations and analyze their\ntransport characteristics using local observables. Our analysis reveals the\nimportance of matching impedance and resonance conditions via system parameters\nfor the propagation of different types of excitations or swapping the nature of\nexcitations along the hybrid lattice. These findings offer insight into\ndesigning controllable quantum links and single-excitation swaps in\nlow-dimensional quantum systems.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u63a7\u5236\u91cf\u5b50\u94fe\u4e2d\u6fc0\u53d1\u7684\u4f20\u64ad\u548c\u4ea4\u6362\u3002", "motivation": "\u63a7\u5236\u4f4e\u7ef4\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u6fc0\u53d1\u4f20\u64ad\u5bf9\u4e8e\u63a8\u8fdb\u91cf\u5b50\u901a\u4fe1\u7f51\u7edc\u548c\u91cf\u5b50\u6a21\u62df\u5668\u7b49\u91cf\u5b50\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u65f6\u95f4\u6f14\u5316\u5757 \u091c\u0947\u0935\u094d\u0939\u093e (TEBD) \u7b97\u6cd5\u6a21\u62df\u6df7\u5408\u91cf\u5b50\u94fe\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u53ef\u89c2\u6d4b\u91cf\u5206\u6790\u6fc0\u53d1\u4f20\u8f93\u7279\u6027\u3002", "result": "\u901a\u8fc7 TEBD \u6a21\u62df\uff0c\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u5339\u914d\u963b\u6297\u548c\u5171\u632f\u6761\u4ef6\u53ef\u4ee5\u63a7\u5236\u6781\u5316\u6fc0\u5143\u3001\u81ea\u65cb\u6ce2\u548c\u5149\u5b50\u7b49\u4e0d\u540c\u7c7b\u578b\u6fc0\u53d1\u7684\u4f20\u64ad\uff0c\u6216\u6cbf\u7740\u6df7\u5408\u94fe\u4ea4\u6362\u6fc0\u53d1\u6027\u8d28\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u53c2\u6570\uff08\u5982\u963b\u6297\u5339\u914d\u548c\u5171\u632f\u6761\u4ef6\uff09\u6765\u63a7\u5236\u4e0d\u540c\u7c7b\u578b\u6fc0\u53d1\u5728\u6df7\u5408\u91cf\u5b50\u94fe\u4e2d\u7684\u4f20\u64ad\uff0c\u6216\u5b9e\u73b0\u6fc0\u53d1\u6027\u8d28\u4ea4\u6362\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u63a7\u7684\u91cf\u5b50\u94fe\u8def\u548c\u5355\u6fc0\u53d1\u4ea4\u6362\u63d0\u4f9b\u4e86\u601d\u8def\u3002"}}
{"id": "2507.11941", "categories": ["cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE \u662f\u4e00\u4e2a\u5728 GPU \u4e0a\u5e76\u884c\u5316 BPE \u7684\u65b0\u65b9\u6cd5\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u541e\u5410\u91cf\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u8bcd\u5668\uff08\u5982 HuggingFace Tokenizers \u548c OpenAI \u7684 tiktoken\uff09\u5728 GPU \u6279\u91cf\u63a8\u7406\u5de5\u4f5c\u6d41\u4e2d\u662f CPU \u5bc6\u96c6\u578b\u7684\uff0c\u5e76\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "BlockBPE \u901a\u8fc7\u5e76\u884c\u5316 BPE \u7b97\u6cd5\uff0c\u5728 GPU \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5206\u8bcd\uff0c\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4ece $O(n \text{ log } n) $ \u964d\u4f4e\u5230 $O(nd)$\uff0c\u5176\u4e2d $d \text{ \u00ab } n$\u3002\u5b83\u901a\u8fc7\u6d88\u9664\u6b63\u5219\u8868\u8fbe\u5f0f\u9884\u5206\u8bcd\u5e76\u4f18\u5316\u7ebf\u7a0b\u5757\u5185\u7684\u5206\u8bcd\u5408\u5e76\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002", "result": "\u4e0e HuggingFace Tokenizers \u548c tiktoken \u76f8\u6bd4\uff0cBlockBPE \u5728\u9ad8\u6279\u91cf\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe 2 \u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff08\u76f8\u6bd4 tiktoken\uff09\u548c 2.5 \u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff08\u76f8\u6bd4 HuggingFace Tokenizers\uff09\u3002", "conclusion": "BlockBPE \u662f\u4e00\u79cd\u521b\u65b0\u7684\u5e76\u884c GPU \u5b9e\u73b0\uff0c\u7528\u4e8e\u5b57\u8282\u5bf9\u7f16\u7801 (BPE)\uff0c\u5b83\u901a\u8fc7\u6d88\u9664\u6b63\u5219\u8868\u8fbe\u5f0f\u9884\u5206\u8bcd\u5e76\u5229\u7528\u7ebf\u7a0b\u5757\u5185\u7684\u5e76\u884c\u5206\u8bcd\u5408\u5e76\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 GPU \u4e0a\u7684\u6279\u91cf\u63a8\u7406\u541e\u5410\u91cf\u3002"}}
{"id": "2507.12093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12093", "abs": "https://arxiv.org/abs/2507.12093", "authors": ["David Rapado-Rincon", "Gert Kootstra"], "title": "Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards", "comment": "Paper submitted to Smart Agricultural Technology", "summary": "Accurate mapping of individual trees is an important component for precision\nagriculture in orchards, as it allows autonomous robots to perform tasks like\ntargeted operations or individual tree monitoring. However, creating these maps\nis challenging because GPS signals are often unreliable under dense tree\ncanopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)\napproaches struggle in orchards because the repetitive appearance of trees can\nconfuse the system, leading to mapping errors. To address this, we introduce\nTree-SLAM, a semantic SLAM approach tailored for creating maps of individual\ntrees in orchards. Utilizing RGB-D images, our method detects tree trunks with\nan instance segmentation model, estimates their location and re-identifies them\nusing a cascade-graph-based data association algorithm. These re-identified\ntrunks serve as landmarks in a factor graph framework that integrates noisy GPS\nsignals, odometry, and trunk observations. The system produces maps of\nindividual trees with a geo-localization error as low as 18 cm, which is less\nthan 20\\% of the planting distance. The proposed method was validated on\ndiverse datasets from apple and pear orchards across different seasons,\ndemonstrating high mapping accuracy and robustness in scenarios with unreliable\nGPS signals.", "AI": {"tldr": "Tree-SLAM\u662f\u4e00\u79cd\u7528\u4e8e\u679c\u56ed\u7684\u8bed\u4e49SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u91cd\u8bc6\u522b\u6811\u5e72\u4f5c\u4e3a\u8def\u6807\uff0c\u89e3\u51b3\u4e86GPS\u4fe1\u53f7\u5dee\u548c\u73af\u5883\u91cd\u590d\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6811\u6728\u5730\u56fe\u7ed8\u5236\u3002", "motivation": "\u5728\u679c\u56ed\u7b49\u5bc6\u96c6\u6811\u51a0\u4e0b\uff0cGPS\u4fe1\u53f7\u5e38\u5e38\u4e0d\u53ef\u9760\uff0c\u7ed9\u7cbe\u786e\u7ed8\u5236\u5355\u68f5\u6811\u6728\u5e26\u6765\u6311\u6218\u3002\u540c\u65f6\uff0c\u6807\u51c6\u7684SLAM\u65b9\u6cd5\u4f1a\u56e0\u4e3a\u6811\u6728\u5916\u89c2\u7684\u91cd\u590d\u6027\u800c\u4ea7\u751f\u6df7\u6dc6\uff0c\u5bfc\u81f4\u5efa\u56fe\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u679c\u56ed\u73af\u5883\u7684SLAM\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Tree-SLAM\u662f\u4e00\u79cd\u8bed\u4e49SLAM\u65b9\u6cd5\uff0c\u5229\u7528RGB-D\u56fe\u50cf\u8fdb\u884c\u6811\u6728\u6620\u5c04\u3002\u5b83\u9996\u5148\u4f7f\u7528\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u68c0\u6d4b\u6811\u5e72\uff0c\u7136\u540e\u901a\u8fc7\u7ea7\u8054\u56fe\u7684\u6570\u636e\u5173\u8054\u7b97\u6cd5\u4f30\u8ba1\u6811\u5e72\u4f4d\u7f6e\u5e76\u8fdb\u884c\u91cd\u8bc6\u522b\u3002\u91cd\u8bc6\u522b\u7684\u6811\u5e72\u88ab\u7528\u4f5c\u56e0\u5b50\u56fe\u6846\u67b6\u4e2d\u7684\u8def\u6807\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86GPS\u4fe1\u53f7\u3001\u91cc\u7a0b\u8ba1\u548c\u6811\u5e72\u89c2\u6d4b\u6570\u636e\u3002", "result": "Tree-SLAM\u751f\u6210\u7684\u5355\u68f5\u6811\u6728\u5730\u56fe\u5177\u6709\u9ad8\u8fbe18\u5398\u7c73\u7684\u5730\u7406\u5b9a\u4f4d\u8bef\u5dee\uff0c\u8fd9\u4e00\u8bef\u5dee\u5c0f\u4e8e\u79cd\u690d\u8ddd\u79bb\u768420%\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5b63\u8282\u7684\u82f9\u679c\u548c\u68a8\u56ed\u7684\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u90fd\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u5efa\u56fe\u548c\u5728GPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Tree-SLAM\u5229\u7528RGB-D\u56fe\u50cf\uff0c\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u68c0\u6d4b\u6811\u5e72\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u7ea7\u8054\u56fe\u7684\u6570\u636e\u5173\u8054\u7b97\u6cd5\u8fdb\u884c\u5b9a\u4f4d\u548c\u91cd\u65b0\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u5c06\u91cd\u65b0\u8bc6\u522b\u7684\u6811\u5e72\u4f5c\u4e3a\u8def\u6807\uff0c\u7ed3\u5408\u4e86\u6709\u566a\u58f0\u7684GPS\u4fe1\u53f7\u3001\u91cc\u7a0b\u8ba1\u548c\u6811\u5e72\u89c2\u6d4b\u4fe1\u606f\uff0c\u5728\u4e00\u4e2a\u56e0\u5b50\u56fe\u6846\u67b6\u5185\u8fdb\u884cSLAM\uff0c\u5b9e\u73b0\u4e86\u4f4e\u81f318\u5398\u7c73\u7684\u6811\u6728\u5730\u56fe\u5730\u7406\u5b9a\u4f4d\u8bef\u5dee\uff0c\u4f18\u4e8e20%\u7684\u79cd\u690d\u8ddd\u79bb\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5b63\u8282\u7684\u82f9\u679c\u548c\u68a8\u56ed\u7684\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u5efa\u56fe\u548c\u5728GPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11882", "abs": "https://arxiv.org/abs/2507.11882", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709LLM\u8bc4\u4f30\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u8bed\u8a00\u548c\u672c\u5730\u5316\u7684\u95ee\u9898\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b30\u79cd\u8bed\u8a00\u7684Marco-Bench-MIF\u57fa\u51c6\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8bed\u8a00\u8d44\u6e90\u3001\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u672c\u5730\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u4e86\u8de8\u8bed\u8a00\u6307\u4ee4\u9075\u5faa\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc4\u4f30\u6570\u636e\u96c6\u5728\u591a\u8bed\u8a00\u548c\u672c\u5730\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u6216\u4f7f\u7528\u673a\u5668\u7ffb\u8bd1\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7ffb\u8bd1\u548c\u9a8c\u8bc1\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b30\u79cd\u8bed\u8a00\u7684\u672c\u5730\u5316\u591a\u8bed\u8a00\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u6570\u636e\u96c6Marco-Bench-MIF\uff0c\u8be5\u6570\u636e\u96c6\u8003\u8651\u4e86\u8bed\u8a00\u7ea6\u675f\u548c\u6587\u5316\u53c2\u8003\u3002", "result": "\u5728Marco-Bench-MIF\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1. \u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u5b58\u572825-35%\u7684\u51c6\u786e\u6027\u5dee\u8ddd\uff1b2. \u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u670945-60%\u7684\u5f71\u54cd\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u7279\u5b9a\u811a\u672c\u7684\u6311\u6218\uff1b3. \u673a\u5668\u7ffb\u8bd1\u6570\u636e\u76f8\u6bd4\u672c\u5730\u5316\u6570\u636e\u4f4e\u4f30\u4e867-22%\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMarco-Bench-MIF\u7684\u591a\u8bed\u8a00\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\uff0c\u5e76\u5bf920\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e86\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u51c6\u786e\u6027\u5dee\u8ddd\u3001\u6a21\u578b\u89c4\u6a21\u7684\u5f71\u54cd\u4ee5\u53ca\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f4e\u4f30\u51c6\u786e\u6027\u7b49\u95ee\u9898\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u8de8\u8bed\u8a00\u5173\u952e\u8bcd\u4e00\u81f4\u6027\u548c\u7ec4\u5408\u7ea6\u675f\u9075\u5b88\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2507.11845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11845", "abs": "https://arxiv.org/abs/2507.11845", "authors": ["Kexuan Shi", "Zhuang Qi", "Jingjing Zhu", "Lei Meng", "Yaochen Zhang", "Haibei Huang", "Xiangxu Meng"], "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "comment": "Accepted in ChinaMM and recommended to Displays", "summary": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "AI": {"tldr": "ProtoConNet\u901a\u8fc7\u6574\u5408\u80cc\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u539f\u578b\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u56fe\u50cf\u7684\u89c6\u89c9\u4fe1\u606f\u6765\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\uff0c\u4f46\u5ffd\u7565\u4e86\u6574\u5408\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6240\u5e26\u6765\u7684\u4f18\u52bf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u578b\u589e\u5f3a\u4e0e\u5bf9\u9f50\u65b9\u6cd5\uff08ProtoConNet\uff09\uff0c\u5305\u62ec\u57fa\u4e8e\u805a\u7c7b\u7684\u7c7b\u522b\u9009\u62e9\uff08CDS\uff09\u6a21\u5757\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49\u7ec6\u5316\uff08CSR\uff09\u6a21\u5757\u548c\u539f\u578b\u5bf9\u9f50\uff08PA\uff09\u6a21\u5757\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u6837\u672c\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\uff0c\u6253\u7834\u4e0a\u4e0b\u6587\u4e0e\u56fe\u50cf\u4e3b\u4f53\u7684\u865a\u5047\u5173\u8054\uff0c\u5e76\u7f29\u5c0f\u56fe\u50cf\u8868\u793a\u4e0e\u7c7b\u522b\u539f\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProtoConNet\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u589e\u5f3a\u4e86\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u5f00\u653e\u96c6\u6837\u672c\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ProtoConNet\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u80cc\u666f\u4fe1\u606f\u548c\u539f\u578b\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.11915", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.11915", "abs": "https://arxiv.org/abs/2507.11915", "authors": ["Mingrui Zhao", "Zhonghuai Hou"], "title": "Noise-induced Quantum Mpemba effect", "comment": null, "summary": "The quantum Mpemba effect (QMPE), an intriguing anomalous relaxation\nphenomenon, has recently attracted significant attention. However, how various\ntypes of noise, which are ubiquitous in real systems, may affect the QMPE\nremains unknown. Here, we address this gap by constructing a general dynamical\nframework for d level open quantum systems under random telegraph noise. By\ninvestigating the dynamics of an extended system and then projecting back, we\nfind that noise can induce additional modes and strongly influence the\nrelaxation dynamics of the original system. Specially, in the limit of long\ncorrelation time of noise, these modes cause anomalous slowdown for certain\ninitial states, thereby inducing or eliminating QMPE, illustrated by a\nthree-level example system. Interestingly, this mechanism leads to a counter\nintuitive effect:the decoherence rate may be slowed down by noise.", "AI": {"tldr": "\u566a\u58f0\u4f1a\u5f71\u54cd\u91cf\u5b50\u9a6c\u59c6\u5df4\u6548\u5e94\uff0c\u53ef\u4ee5\u8bf1\u53d1\u6216\u6d88\u9664\u5b83\uff0c\u5e76\u53ef\u80fd\u51cf\u6162\u9000\u76f8\u5e72\u901f\u7387\u3002", "motivation": "\u89e3\u51b3\u5404\u79cd\u566a\u58f0\u5982\u4f55\u5f71\u54cd\u91cf\u5b50\u9a6c\u59c6\u5df4\u6548\u5e94\u8fd9\u4e00\u672a\u77e5\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u9002\u7528\u4e8e\u968f\u673a\u7535\u62a5\u566a\u58f0\u4e0bd\u5c42\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u901a\u7528\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u6269\u5c55\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u7136\u540e\u8fdb\u884c\u6295\u5f71\u3002", "result": "\u566a\u58f0\u53ef\u4ee5\u8bf1\u5bfc\u989d\u5916\u7684\u6a21\u5f0f\u5e76\u5f3a\u70c8\u5f71\u54cd\u539f\u59cb\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\uff0c\u5728\u67d0\u4e9b\u521d\u59cb\u72b6\u6001\u4e0b\u4f1a\u5bfc\u81f4\u5f02\u5e38\u51cf\u6162\uff0c\u4ece\u800c\u8bf1\u53d1\u6216\u6d88\u9664\u91cf\u5b50\u9a6c\u59c6\u5df4\u6548\u5e94\u3002\u5728\u566a\u58f0\u76f8\u5173\u65f6\u95f4\u957f\u7684\u6781\u9650\u4e0b\uff0c\u4f1a\u4ea7\u751f\u53cd\u76f4\u89c9\u7684\u6548\u5e94\uff1a\u566a\u58f0\u53ef\u80fd\u51cf\u6162\u9000\u76f8\u5e72\u901f\u7387\u3002", "conclusion": "\u566a\u58f0\u53ef\u4ee5\u8bf1\u5bfc\u989d\u5916\u7684\u6a21\u5f0f\u5e76\u5f3a\u70c8\u5f71\u54cd\u539f\u59cb\u7cfb\u7edf\u7684\u5f1b\u8c6b\u52a8\u529b\u5b66\uff0c\u5728\u67d0\u4e9b\u521d\u59cb\u72b6\u6001\u4e0b\u4f1a\u5bfc\u81f4\u5f02\u5e38\u51cf\u6162\uff0c\u4ece\u800c\u8bf1\u53d1\u6216\u6d88\u9664\u91cf\u5b50\u9a6c\u59c6\u5df4\u6548\u5e94\u3002\u7279\u522b\u5730\uff0c\u5728\u566a\u58f0\u76f8\u5173\u65f6\u95f4\u957f\u7684\u6781\u9650\u4e0b\uff0c\u4f1a\u4ea7\u751f\u53cd\u76f4\u89c9\u7684\u6548\u5e94\uff1a\u566a\u58f0\u53ef\u80fd\u51cf\u6162\u9000\u76f8\u5e72\u901f\u7387\u3002"}}
{"id": "2507.11710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11710", "abs": "https://arxiv.org/abs/2507.11710", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links", "comment": "18 pages, 7 figures, preprint", "summary": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link\nprediction (LP) task. However, these models often rely on all dataset samples\nbeing drawn from the same distribution. In addition, graph generative models\n(GGMs) show a pronounced ability to generate novel output graphs. Despite this,\nGGM applications remain largely limited to domain-specific tasks. To bridge\nthis gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)\nstructurally-conditioned graph generation, and (2) adversarial co-training\nbetween an auto-encoder and GNN. As such, FLEX ensures structural-alignment\nbetween sample distributions to enhance link-prediction performance in\nout-of-distribution (OOD) scenarios. Notably, FLEX does not require expert\nknowledge to function in different OOD scenarios. Numerous experiments are\nconducted in synthetic and real-world OOD settings to demonstrate FLEX's\nperformance-enhancing ability, with further analysis for understanding the\neffects of graph data augmentation on link structures. The source code is\navailable here: https://github.com/revolins/FlexOOD.", "AI": {"tldr": "FLEX\u662f\u4e00\u4e2a\u56fe\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6761\u4ef6\u56fe\u751f\u6210\u548c\u5bf9\u6297\u6027\u534f\u540c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5904\u7406\u5206\u5e03\u5185\u6837\u672c\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u56fe\u751f\u6210\u6a21\u578b\uff08GGMs\uff09\u7684\u5e94\u7528\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u95ee\u9898\uff0c\u63d0\u51faFLEX\u6846\u67b6\u4ee5\u6865\u63a5\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "FLEX\u6846\u67b6\u7ed3\u5408\u4e86\u7ed3\u6784\u6761\u4ef6\u56fe\u751f\u6210\u548c\u81ea\u7f16\u7801\u5668\u4e0eGNN\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u534f\u540c\u8bad\u7ec3\u4e24\u79cd\u673a\u5236\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eFLEX\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684OOD\u73af\u5883\u4e2d\u80fd\u591f\u63d0\u5347\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u7406\u89e3\u56fe\u6570\u636e\u589e\u5f3a\u5bf9\u94fe\u63a5\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "conclusion": "FLEX\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u6761\u4ef6\u56fe\u751f\u6210\u548c\u81ea\u7f16\u7801\u5668\u4e0eGNN\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u534f\u540c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u5206\u5e03\u7684\u7ed3\u6784\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u63d0\u5347\u4e86\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002\u8be5\u6846\u67b6\u65e0\u9700\u4e13\u5bb6\u77e5\u8bc6\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u7684OOD\u573a\u666f\u3002"}}
{"id": "2507.12410", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12410", "abs": "https://arxiv.org/abs/2507.12410", "authors": ["Avdhesh K. Sharma", "Snehashish Chatterjee", "Premakumar Yanda", "Claudia Felser", "Chandra Shekhar"], "title": "Topological quantum materials: kagome, chiral, and square-net frameworks", "comment": null, "summary": "Topological quantum materials have emerged as a frontier in condensed matter\nphysics as well as in materials science, with intriguing electronic states that\nare robust to perturbations. Among the diverse structural motifs, kagome,\nchiral, and square-net structures offer a wide range of topological phases and\nphysical phenomena. These include Dirac and Weyl fermions, nodal-line\nsemimetals, flat bands, van Hove singularities, charge density waves,\nsuperconductivity, nontrivial Berry phase, nonlinear electrical and thermal\ntransports. This review explores the distinct roles of geometry, symmetry,\nspin-orbit coupling, and electron correlations in these three classes of\nmaterials. It also highlights how their crystallographic features give rise to\nunique electronic band structures, topologically protected states and different\nphysical properties, which require high-quality-single crystals. The present\ndiscussion comprises recent experimental discoveries and identification of the\nsynthesis routes of key materials within each framework. Finally, the review\noutlines the current challenges and future directions in the design and\nexploration of topological quantum materials.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86kagome\u3001\u624b\u5f81\u548c\u65b9\u7f51\u72b6\u7ed3\u6784\u62d3\u6251\u91cf\u5b50\u6750\u6599\u7684\u51e0\u4f55\u3001\u5bf9\u79f0\u6027\u3001\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u7535\u5b50\u5173\u8054\u7b49\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u5176\u72ec\u7279\u7684\u7535\u5b50\u80fd\u5e26\u7ed3\u6784\u548c\u7269\u7406\u6027\u8d28\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u6700\u65b0\u7684\u5b9e\u9a8c\u53d1\u73b0\u3001\u5408\u6210\u8def\u7ebf\u4ee5\u53ca\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u62d3\u6251\u91cf\u5b50\u6750\u6599\uff0c\u7279\u522b\u662f\u5177\u6709\u72c4\u62c9\u514b\u548c\u5916\u5c14\u8d39\u7c73\u5b50\u3001\u8282\u70b9\u7ebf\u534a\u91d1\u5c5e\u3001\u5e73\u5766\u80fd\u5e26\u3001\u8303\u970d\u592b\u5947\u70b9\u3001\u7535\u8377\u5bc6\u5ea6\u6ce2\u3001\u8d85\u5bfc\u7535\u6027\u3001\u975e\u5e73\u51e1\u8d1d\u91cc\u76f8\u4f4d\u4ee5\u53ca\u975e\u7ebf\u6027\u7535\u548c\u70ed\u8f93\u8fd0\u7b49\u7279\u6027\u7684kagome\u3001\u624b\u5f81\u548c\u65b9\u7f51\u72b6\u7ed3\u6784\u6750\u6599\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u51e0\u4f55\u3001\u5bf9\u79f0\u6027\u3001\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u7535\u5b50\u5173\u8054\u5728\u8fd9\u4e9b\u6750\u6599\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5b83\u4eec\u7684\u6676\u4f53\u7ed3\u6784\u5982\u4f55\u4ea7\u751f\u72ec\u7279\u7684\u7535\u5b50\u80fd\u5e26\u7ed3\u6784\u3001\u53d7\u62d3\u6251\u4fdd\u62a4\u7684\u6001\u4ee5\u53ca\u4e0d\u540c\u7684\u7269\u7406\u6027\u8d28\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u5355\u6676\u7684\u5236\u5907\u662f\u7814\u7a76\u8fd9\u4e9b\u6027\u8d28\u7684\u5173\u952e\u3002", "result": "\u6587\u7ae0\u56de\u987e\u4e86\u8fd9\u4e09\u7c7b\u6750\u6599\u7684\u6700\u65b0\u5b9e\u9a8c\u53d1\u73b0\u548c\u5173\u952e\u6750\u6599\u7684\u5408\u6210\u8def\u7ebf\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5728\u8bbe\u8ba1\u548c\u63a2\u7d22\u62d3\u6251\u91cf\u5b50\u6750\u6599\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u62d3\u6251\u91cf\u5b50\u6750\u6599\u5728\u51dd\u805a\u6001\u7269\u7406\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u524d\u6cbf\u5730\u4f4d\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86kagome\u3001\u624b\u5f81\u548c\u65b9\u7f51\u72b6\u7ed3\u6784\u8fd9\u4e09\u7c7b\u6750\u6599\u7684\u72ec\u7279\u4e4b\u5904\u3002"}}
{"id": "2507.12317", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12317", "abs": "https://arxiv.org/abs/2507.12317", "authors": ["Martin Agebj\u00e4r", "Gustav Zetterqvist", "Fredrik Gustafsson", "Johan Wahlstr\u00f6m", "Gustaf Hendeby"], "title": "Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors", "comment": "Accepted for publication in FUSION 2025 - 28th International\n  Conference on Information Fusion (FUSION), IEEE (2025)", "summary": "Road roughness significantly affects vehicle vibrations and ride quality. We\nintroduce a Kalman filter (KF)-based method for estimating road roughness in\nterms of the international roughness index (IRI) by fusing inertial and speed\nmeasurements, offering a cost-effective solution for pavement monitoring. The\nmethod involves system identification on a physical vehicle to estimate\nrealistic model parameters, followed by KF-based reconstruction of the\nlongitudinal road profile to compute IRI values. It explores IRI estimation\nusing vertical and lateral vibrations, the latter more common in modern\nvehicles. Validation on 230 km of real-world data shows promising results, with\nIRI estimation errors ranging from 1% to 10% of the reference values. However,\naccuracy deteriorates significantly when using only lateral vibrations,\nhighlighting their limitations. These findings demonstrate the potential of\nKF-based estimation for efficient road roughness monitoring.", "AI": {"tldr": "A KF-based method estimates road roughness (IRI) using inertial and speed data. It\u2019s cost-effective but less accurate with only lateral vibrations.", "motivation": "To provide a cost-effective solution for pavement monitoring by estimating road roughness using vehicle vibrations and speed measurements.", "method": "A Kalman filter (KF)-based method is introduced to estimate road roughness (IRI) by fusing inertial and speed measurements. The method involves system identification on a physical vehicle to estimate model parameters, followed by KF-based reconstruction of the longitudinal road profile to compute IRI values.", "result": "Validation on 230 km of real-world data shows IRI estimation errors between 1% and 10% of reference values. Accuracy significantly decreases when using only lateral vibrations.", "conclusion": "KF-based estimation shows potential for efficient road roughness monitoring, but accuracy is limited when using only lateral vibrations."}}
{"id": "2507.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12148", "abs": "https://arxiv.org/abs/2507.12148", "authors": ["Xing Tong", "Michele D. Simoni", "Kaj Munhoz Arfvidsson", "Jonas M\u00e5rtensson"], "title": "Leveraging Sidewalk Robots for Walkability-Related Analyses", "comment": null, "summary": "Walkability is a key component of sustainable urban development, while\ncollecting detailed data on its related features remains challenging due to the\nhigh costs and limited scalability of traditional methods. Sidewalk delivery\nrobots, increasingly deployed in urban environments, offer a promising solution\nto these limitations. This paper explores how these robots can serve as mobile\ndata collection platforms, capturing sidewalk-level features related to\nwalkability in a scalable, automated, and real-time manner. A sensor-equipped\nrobot was deployed on a sidewalk network at KTH in Stockholm, completing 101\ntrips covering 900 segments. From the collected data, different typologies of\nfeatures are derived, including robot trip characteristics (e.g., speed,\nduration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk\nutilization (e.g., pedestrian density). Their walkability-related implications\nwere investigated with a series of analyses. The results demonstrate that\npedestrian movement patterns are strongly influenced by sidewalk\ncharacteristics, with higher density, reduced width, and surface irregularity\nassociated with slower and more variable trajectories. Notably, robot speed\nclosely mirrors pedestrian behavior, highlighting its potential as a proxy for\nassessing pedestrian dynamics. The proposed framework enables continuous\nmonitoring of sidewalk conditions and pedestrian behavior, contributing to the\ndevelopment of more walkable, inclusive, and responsive urban environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8857\u9053\u914d\u9001\u673a\u5668\u4eba\u4f5c\u4e3a\u79fb\u52a8\u6570\u636e\u6536\u96c6\u5e73\u53f0\u6765\u8bc4\u4f30\u6b65\u884c\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6b65\u884c\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u8857\u9053\u914d\u9001\u673a\u5668\u4eba\u53ef\u4ee5\u4f5c\u4e3a\u79fb\u52a8\u6570\u636e\u6536\u96c6\u5e73\u53f0\uff0c\u4ee5\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u548c\u5b9e\u65f6\u7684\u65b9\u5f0f\u6355\u83b7\u6b65\u884c\u76f8\u5173\u7279\u5f81\u3002", "method": "\u5229\u7528\u914d\u5907\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u5728\u65af\u5fb7\u54e5\u5c14\u6469KTH\u7684\u8857\u9053\u7f51\u7edc\u4e0a\u8fdb\u884c\u90e8\u7f72\uff0c\u5b8c\u6210\u4e86101\u6b21\u884c\u7a0b\uff0c\u8986\u76d6\u4e86900\u4e2a\u8def\u6bb5\uff0c\u6536\u96c6\u4e86\u8857\u9053\u7ea7\u522b\u7684\u6b65\u884c\u76f8\u5173\u7279\u5f81\u6570\u636e\u3002", "result": "\u673a\u5668\u4eba\u901f\u5ea6\u4e0e\u884c\u4eba\u884c\u4e3a\u5bc6\u5207\u76f8\u5173\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u8bc4\u4f30\u884c\u4eba\u52a8\u6001\u7684\u4ee3\u7406\u3002\u884c\u4eba\u7684\u79fb\u52a8\u6a21\u5f0f\u53d7\u5230\u8857\u9053\u7279\u5f81\u7684\u663e\u8457\u5f71\u54cd\uff0c\u9ad8\u5bc6\u5ea6\u3001\u8f83\u7a84\u548c\u4e0d\u5e73\u6574\u7684\u8857\u9053\u4f1a\u5bfc\u81f4\u884c\u4eba\u79fb\u52a8\u8f68\u8ff9\u53d8\u6162\u548c\u53d8\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6301\u7eed\u76d1\u6d4b\u8857\u9053\u72b6\u51b5\u548c\u884c\u4eba\u884c\u4e3a\uff0c\u6709\u52a9\u4e8e\u521b\u5efa\u66f4\u5b9c\u884c\u3001\u5305\u5bb9\u548c\u54cd\u5e94\u8fc5\u901f\u7684\u57ce\u5e02\u73af\u5883\u3002"}}
{"id": "2507.11936", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11936", "abs": "https://arxiv.org/abs/2507.11936", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "title": "A Survey of Deep Learning for Geometry Problem Solving", "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "\u672c\u8bba\u6587\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u9886\u57df\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8c03\u7814\uff0c\u603b\u7ed3\u4e86\u76f8\u5173\u4efb\u52a1\u3001\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63a2\u8ba8\u4e86\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u662f\u6570\u5b66\u63a8\u7406\u7684\u5173\u952e\u9886\u57df\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6559\u80b2\u3001\u4eba\u5de5\u667a\u80fd\u7684\u6570\u5b66\u80fd\u529b\u8bc4\u4f30\u4ee5\u53ca\u591a\u6a21\u6001\u80fd\u529b\u8bc4\u4f30\u7b49\u91cd\u8981\u9886\u57df\u3002\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u5f15\u53d1\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u70ed\u6f6e\u3002", "method": "\u901a\u8fc7\uff08i\uff09\u5168\u9762\u603b\u7ed3\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u7684\u76f8\u5173\u4efb\u52a1\uff1b\uff08ii\uff09\u6df1\u5165\u56de\u987e\u76f8\u5173\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff1b\uff08iii\uff09\u8be6\u7ec6\u5206\u6790\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\uff1b\uff08iv\uff09\u6279\u5224\u6027\u5730\u8ba8\u8bba\u5f53\u524d\u7684\u6311\u6218\u548c\u672a\u6765\u53ef\u63a2\u7d22\u7684\u65b9\u5411\uff0c\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8c03\u7814\u3002", "result": "\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u603b\u7ed3\u3001\u56de\u987e\u548c\u5206\u6790\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u4e14\u5b9e\u7528\u7684\u53c2\u8003\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.11892", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "AI": {"tldr": "GRACE\u901a\u8fc7\u96c6\u6210\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u6587\u672c\u7cbe\u70bc\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u6539\u8fdb\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u6587\u672c\u7ebf\u7d22\u548c\u8fc7\u6ee4\u65e0\u5173\u9762\u90e8\u52a8\u6001\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u7684\u65b9\u6cd5\u5728\u5229\u7528\u751f\u6210\u6587\u672c\u4e2d\u8574\u542b\u7684\u7ec6\u5fae\u60c5\u611f\u7ebf\u7d22\u4ee5\u53ca\u8fc7\u6ee4\u6389\u4e0e\u60c5\u611f\u8868\u8fbe\u65e0\u5173\u7684\u9762\u90e8\u52a8\u6001\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "GRACE\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u8bed\u4e49\u6587\u672c\u7cbe\u70bc\u548c\u4ee4\u724c\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u6574\u5408\uff0c\u4ee5\u4fc3\u8fdb\u60c5\u611f\u7ebf\u7d22\u7684\u65f6\u7a7a\u7279\u5f81\u7684\u7cbe\u786e\u5b9a\u4f4d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7c97\u7ec6\u7c92\u5ea6\u60c5\u611f\u6587\u672c\u589e\u5f3a\uff08CATE\uff09\u6a21\u5757\u6784\u5efa\u60c5\u611f\u611f\u77e5\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u5dee\u5f02\u52a0\u6743\u673a\u5236\u7a81\u51fa\u4e0e\u8868\u60c5\u76f8\u5173\u7684\u4eba\u8138\u8fd0\u52a8\u3002\u8fd9\u4e9b\u7cbe\u70bc\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u4fe1\u53f7\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\u5728\u4ee4\u724c\u7ea7\u522b\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "GRACE\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6a21\u7cca\u6216\u4e0d\u5e73\u8861\u60c5\u611f\u7c7b\u522b\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0c\u5728UAR\u548cWAR\u65b9\u9762\u5747\u521b\u4e0b\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6210\u679c\u3002", "conclusion": "GRACE\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6a21\u7cca\u6216\u4e0d\u5e73\u8861\u60c5\u611f\u7c7b\u522b\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0c\u5728UAR\u548cWAR\u65b9\u9762\u5747\u521b\u4e0b\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6210\u679c\u3002"}}
{"id": "2507.11729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11729", "abs": "https://arxiv.org/abs/2507.11729", "authors": ["Amirhossein Ahmadi", "Hamidreza Zareipour", "Henry Leung"], "title": "Globalization for Scalable Short-term Load Forecasting", "comment": "63 pages with 22 figures", "summary": "Forecasting load in power transmission networks is essential across various\nhierarchical levels, from the system level down to individual points of\ndelivery (PoD). While intuitive and locally accurate, traditional local\nforecasting models (LFMs) face significant limitations, particularly in\nhandling generalizability, overfitting, data drift, and the cold start problem.\nThese methods also struggle with scalability, becoming computationally\nexpensive and less efficient as the network's size and data volume grow. In\ncontrast, global forecasting models (GFMs) offer a new approach to enhance\nprediction generalizability, scalability, accuracy, and robustness through\nglobalization and cross-learning. This paper investigates global load\nforecasting in the presence of data drifts, highlighting the impact of\ndifferent modeling techniques and data heterogeneity. We explore\nfeature-transforming and target-transforming models, demonstrating how\nglobalization, data heterogeneity, and data drift affect each differently. In\naddition, we examine the role of globalization in peak load forecasting and its\npotential for hierarchical forecasting. To address data heterogeneity and the\nbalance between globality and locality, we propose separate time series\nclustering (TSC) methods, introducing model-based TSC for feature-transforming\nmodels and new weighted instance-based TSC for target-transforming models.\nThrough extensive experiments on a real-world dataset of Alberta's electricity\nload, we demonstrate that global target-transforming models consistently\noutperform their local counterparts, especially when enriched with global\nfeatures and clustering techniques. In contrast, global feature-transforming\nmodels face challenges in balancing local and global dynamics, often requiring\nTSC to manage data heterogeneity effectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u5728\u6709\u5168\u5c40\u7279\u5f81\u548c\u805a\u7c7b\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u597d\u3002\u5168\u5c40\u7279\u5f81\u8f6c\u6362\u6a21\u578b\u5219\u5728\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u52a8\u6001\u65b9\u9762\u9047\u5230\u6311\u6218\uff0c\u9700\u8981\u805a\u7c7b\u6765\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u3002", "motivation": "\u4f20\u7edf\u5c40\u90e8\u9884\u6d4b\u6a21\u578b\uff08LFMs\uff09\u5728\u6cdb\u5316\u6027\u3001\u8fc7\u62df\u5408\u3001\u6570\u636e\u6f02\u79fb\u548c\u51b7\u542f\u52a8\u95ee\u9898\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5728\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u5168\u5c40\u9884\u6d4b\u6a21\u578b\uff08GFMs\uff09\u901a\u8fc7\u5168\u5c40\u5316\u548c\u4ea4\u53c9\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u9ad8\u9884\u6d4b\u7684\u6cdb\u5316\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u6f02\u79fb\u60c5\u51b5\u4e0b\u7684\u5168\u5c40\u8d1f\u8377\u9884\u6d4b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e0d\u540c\u5efa\u6a21\u6280\u672f\u548c\u6570\u636e\u5f02\u8d28\u6027\u7684\u5f71\u54cd\u3002\u6587\u7ae0\u63a2\u8ba8\u4e86\u7279\u5f81\u8f6c\u6362\u548c\u76ee\u6807\u8f6c\u6362\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u5168\u5c40\u5316\u3001\u6570\u636e\u5f02\u8d28\u6027\u548c\u6570\u636e\u6f02\u79fb\u7684\u4e0d\u540c\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8003\u5bdf\u4e86\u5168\u5c40\u5316\u5728\u5cf0\u503c\u8d1f\u8377\u9884\u6d4b\u548c\u5c42\u7ea7\u9884\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002\u4e3a\u4e86\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u5168\u5c40\u4e0e\u5c40\u90e8\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5355\u72ec\u7684\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\uff08TSC\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u7528\u4e8e\u7279\u5f81\u8f6c\u6362\u6a21\u578b\u7684\u57fa\u4e8e\u6a21\u578b\u7684TSC\u4ee5\u53ca\u65b0\u7684\u7528\u4e8e\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u7684\u52a0\u6743\u5b9e\u4f8bTSC\u3002", "result": "\u5728\u5bf9\u963f\u5c14\u4f2f\u5854\u7535\u529b\u8d1f\u8377\u7684\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u7ed3\u679c\u8868\u660e\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\uff08GTFM\uff09\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5176\u5c40\u90e8\u5bf9\u5e94\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u6574\u5408\u4e86\u5168\u5c40\u7279\u5f81\u548c\u805a\u7c7b\u6280\u672f\u4e4b\u540e\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5168\u5c40\u7279\u5f81\u8f6c\u6362\u6a21\u578b\uff08GFM\uff09\u5728\u534f\u8c03\u5c40\u90e8\u4e0e\u5168\u5c40\u52a8\u6001\u65b9\u9762\u9047\u5230\u56f0\u96be\uff0c\u901a\u5e38\u9700\u8981\u805a\u7c7b\u6280\u672f\uff08TSC\uff09\u6765\u6709\u6548\u5730\u7ba1\u7406\u6570\u636e\u5f02\u8d28\u6027\u3002", "conclusion": "\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\uff08GTFM\uff09\u5728\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u5c40\u90e8\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u5408\u5168\u5c40\u7279\u5f81\u548c\u805a\u7c7b\u6280\u672f\u65f6\u3002\u5168\u5c40\u7279\u5f81\u8f6c\u6362\u6a21\u578b\uff08GFM\uff09\u5728\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u52a8\u6001\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u805a\u7c7b\u6280\u672f\u6765\u6709\u6548\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u3002"}}
{"id": "2507.12446", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.12446", "abs": "https://arxiv.org/abs/2507.12446", "authors": ["John L. Lyons"], "title": "Alkali doping of Zn$_{\\rm x}$Mg$_{\\rm 1-x}$O alloys for $p$-type conductivity", "comment": null, "summary": "Nearly all ultrawide-bandgap oxides are affected by hole localization that\nlimits $p$-type conductivity and thus potential applications for these\nmaterials. Highly localized holes, also known as hole polarons, trap in the\nvicinity of acceptor dopants, giving rise to large ionization energies and\nseverely constraining free hole concentrations. Though this hole-trapping\nbehavior affects wurtzite zinc oxide, rocksalt zinc oxide was recently found to\nbe resistant to the formation of hole polarons. Moreover, $p$-type doping using\nlithium acceptors was predicted to be achievable. While rocksalt zinc oxide is\nmetastable and has a band gap near $\\sim$3 eV, here it is found that zinc\nmagnesium oxide (Zn$_{\\rm x}$Mg$_{\\rm 1-x}$O) alloys remain $p$-type dopable\nwithin the stable rocksalt crystal structure, in addition to exhibiting band\ngaps in excess of 4 eV. As in rocksalt zinc oxide, alkali acceptors are shallow\nin zinc magnesium oxide and do not appear to be affected by donor compensation.\nThese results indicate that alkali-doped Zn$_{\\rm x}$Mg$_{\\rm 1-x}$O alloys are\na promising system for achieving a $p$-type dopable ultrawide-bandgap oxide.", "AI": {"tldr": "Zn$_{x}$Mg$_{1-x}$O\u5408\u91d1\u662f\u5b9e\u73b0p\u578b\u8d85\u5bbd\u5e26\u9699\u6c27\u5316\u7269\u7684\u6709\u5e0c\u671b\u7684\u7cfb\u7edf\u3002", "motivation": "\u514b\u670d\u8d85\u5bbd\u5e26\u9699\u6c27\u5316\u7269\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7a7a\u7a74\u5c40\u57df\u5316\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0p\u578b\u5bfc\u7535\u6027", "method": "\u901a\u8fc7\u7814\u7a76\u6c27\u5316\u950c\u548c\u6c27\u5316\u9541\u7684\u5408\u91d1\u5728\u5ca9\u76d0\u7ed3\u6784\u4e2d\u7684\u6027\u8d28", "result": "Zn$_{x}$Mg$_{1-x}$O\u5408\u91d1\u5728\u7a33\u5b9a\u7684\u5ca9\u76d0\u6676\u4f53\u7ed3\u6784\u4e2d\u53ef\u4ee5\u5b9e\u73b0p\u578b\u63ba\u6742\uff0c\u5e76\u4e14\u5e26\u9699\u8d85\u8fc74 eV\uff0c\u8fd9\u4e0e\u4e4b\u524d\u7684\u7814\u7a76\u4e00\u81f4\uff0c\u5373\u5ca9\u76d0\u7ed3\u6784\u6c27\u5316\u950c\u5bf9\u7a7a\u7a74\u6781\u5316\u5b50\u4e0d\u654f\u611f\u3002", "conclusion": "\u78b1\u91d1\u5c5e\u63ba\u6742\u7684Zn$_{x}$Mg$_{1-x}$O\u5408\u91d1\u6709\u671b\u5b9e\u73b0p\u578b\u63ba\u6742\u7684\u8d85\u5bbd\u5e26\u9699\u6c27\u5316\u7269"}}
{"id": "2507.12133", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12133", "abs": "https://arxiv.org/abs/2507.12133", "authors": ["Hanwen Liu", "Yuhe Huang", "Yifeng Gong", "Yanjie Zhai", "Jiaxuan Lu"], "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "comment": null, "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "AI": {"tldr": "HyDRA\u662f\u4e00\u79cd\u6df7\u5408\u53cc\u6a21\u5c04\u9891\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u4f18\u5316\u7684VMD\u3001CNN\u3001Transformer\u548cMamba\uff0c\u7528\u4e8e\u8bbe\u5907\u8bc6\u522b\u3002\u5b83\u5728\u95ed\u96c6\u548c\u5f00\u96c6\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u65e0\u7ebf\u8eab\u4efd\u9a8c\u8bc1\u3002", "motivation": "\u8bbe\u5907\u8bc6\u522b\u5bf9\u4e8e\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u63a5\u5165\u63a7\u5236\u7b49\u5e94\u7528\u4e2d\u3002\u5c04\u9891\u6307\u7eb9\u8bc6\u522b(RFFI)\u901a\u8fc7\u5229\u7528\u786c\u4ef6\u5f15\u8d77\u7684\u4fe1\u53f7\u5931\u771f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u52a0\u5bc6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "HyDRA\u662f\u4e00\u79cd\u6df7\u5408\u53cc\u6a21\u5c04\u9891\u67b6\u6784\uff0c\u5b83\u5c06\u4f18\u5316\u7684\u53d8\u5206\u6a21\u6001\u5206\u89e3(VMD)\u4e0e\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u3001Transformer\u548cMamba\u7ec4\u4ef6\u878d\u5408\u7684\u65b0\u578b\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u95ed\u96c6\u548c\u5f00\u96c6\u5206\u7c7b\u4efb\u52a1\u3002\u4f18\u5316\u7684VMD\u901a\u8fc7\u56fa\u5b9a\u4e2d\u5fc3\u9891\u7387\u548c\u4f7f\u7528\u95ed\u5f0f\u89e3\u6765\u63d0\u9ad8\u9884\u5904\u7406\u6548\u7387\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002HyDRA\u91c7\u7528Transformer\u52a8\u6001\u5e8f\u5217\u7f16\u7801\u5668(TDSE)\u8fdb\u884c\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff0c\u5e76\u91c7\u7528Mamba\u7ebf\u6027\u6d41\u7f16\u7801\u5668(MLFE)\u8fdb\u884c\u7ebf\u6027\u590d\u6742\u5ea6\u5904\u7406\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6761\u4ef6\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cHyDRA\u5728\u95ed\u96c6\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb(SOTA)\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6211\u4eec\u63d0\u51fa\u7684\u5f00\u96c6\u5206\u7c7b\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u672a\u7ecf\u6388\u6743\u7684\u8bbe\u5907\u3002\u90e8\u7f72\u5728NVIDIA Jetson Xavier NX\u4e0a\uff0cHyDRA\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u7684\u63a8\u7406\u901f\u5ea6\u548c\u4f4e\u529f\u8017\u3002", "conclusion": "HyDRA\u5728\u95ed\u96c6\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb(SOTA)\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6211\u4eec\u63d0\u51fa\u7684\u5f00\u96c6\u5206\u7c7b\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u672a\u7ecf\u6388\u6743\u7684\u8bbe\u5907\u3002\u6b64\u5916\uff0cHyDRA\u90e8\u7f72\u5728NVIDIA Jetson Xavier NX\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u7684\u63a8\u7406\u901f\u5ea6\u548c\u4f4e\u529f\u8017\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b9e\u65f6\u65e0\u7ebf\u8eab\u4efd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12448", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.12448", "abs": "https://arxiv.org/abs/2507.12448", "authors": ["Jiahao Wu", "Guanjie He", "Wenyuan Zhuo", "Quan Fu", "Xin Wang"], "title": "Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for exchange-only qubits", "comment": "15+11 pages, 11 figures", "summary": "Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a\ncompelling platform for scalable semiconductor-based quantum computing by\nenabling universal control through purely exchange interactions. While\nhigh-fidelity single- and two-qubit gates have been demonstrated, the synthesis\nof efficient multi-qubit operations -- such as the Toffoli gate -- remains a\nkey bottleneck. Conventional gate decompositions into elementary operations\nlead to prohibitively long and error-prone pulse sequences, limiting practical\ndeployment. In this work, we introduce a gradient-based optimization algorithm,\nJenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate\nsequences. Applying JK to the Toffoli gate, we reduce the number of required\nexchange unitaries from 216 (in standard decomposition) to 92, and compress the\ntime steps required from 162 to 50, all while maintaining target fidelity.\nUnder realistic noise, the accumulated gate error from our optimized sequence\nis an order of magnitude lower than that of conventional approaches. These\nresults demonstrate that the JK algorithm is a general and scalable strategy\nfor multi-qubit gate synthesis in EO architectures, potentially facilitating\nrealization of multi-qubit algorithms on semiconductor platforms.", "AI": {"tldr": "\u4f7f\u7528 Jenga-Krotov (JK) \u4f18\u5316\u7b97\u6cd5\uff0c\u5c06\u5b9e\u73b0 Toffoli \u95e8\u6240\u9700\u7684\u4ea4\u6362\u64cd\u4f5c\u6570\u91cf\u51cf\u5c11\u4e86\u4e00\u534a\u4ee5\u4e0a\uff0c\u5e76\u5927\u5927\u7f29\u77ed\u4e86\u64cd\u4f5c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4e3a\u5728\u534a\u5bfc\u4f53\u5e73\u53f0\u4e0a\u5b9e\u73b0\u591a\u91cf\u5b50\u6bd4\u7279\u7b97\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u4f20\u7edf\u7684 Toffoli \u95e8\u7b49\u9ad8\u6548\u591a\u91cf\u5b50\u6bd4\u7279\u95e8\u64cd\u4f5c\u7684\u7efc\u5408\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u56e0\u4e3a\u5176\u8109\u51b2\u5e8f\u5217\u5197\u957f\u4e14\u6613\u51fa\u9519\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Jenga-Krotov (JK) \u7684\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u7d27\u51d1\u3001\u9ad8\u4fdd\u771f\u7684 EO \u91cf\u5b50\u6bd4\u7279\u95e8\u5e8f\u5217\u3002", "result": "\u5c06 Toffoli \u95e8\u7684\u5fc5\u9700\u4ea4\u6362\u5e7a\u6b63\u64cd\u4f5c\u4ece 216 \u4e2a\u51cf\u5c11\u5230 92 \u4e2a\uff0c\u5e76\u5c06\u6240\u9700\u65f6\u95f4\u6b65\u4ece 162 \u4e2a\u538b\u7f29\u5230 50 \u4e2a\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u4fdd\u771f\u5ea6\u3002\u5728\u5b9e\u9645\u566a\u58f0\u4e0b\uff0c\u4f18\u5316\u5e8f\u5217\u7684\u7d2f\u79ef\u95e8\u8bef\u5dee\u6bd4\u4f20\u7edf\u65b9\u6cd5\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86 Jenga-Krotov (JK) \u7b97\u6cd5\u662f\u4e00\u79cd\u7528\u4e8e EO \u4f53\u7cfb\u7ed3\u6784\u4e2d\u591a\u91cf\u5b50\u6bd4\u7279\u95e8\u7efc\u5408\u7684\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u6709\u671b\u5728\u534a\u5bfc\u4f53\u5e73\u53f0\u4e0a\u5b9e\u73b0\u591a\u91cf\u5b50\u6bd4\u7279\u7b97\u6cd5\u3002"}}
{"id": "2507.12158", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12158", "abs": "https://arxiv.org/abs/2507.12158", "authors": ["Nawshin Mannan Proma", "Gricel V\u00e1zquez", "Sepeedeh Shahbeigi", "Arjun Badyal", "Victoria Hodge"], "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach", "comment": "6 pages, 6 figures", "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u60c5\u51b5\u8986\u76d6\u7f51\u683c\u3001\u6982\u7387\u5efa\u6a21\u548c\u6a21\u578b\u68c0\u67e5\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u60c5\u51b5\u5e76\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u65e5\u76ca\u90e8\u7f72\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\uff0c\u786e\u4fdd\u5176\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u6027\u60c5\u51b5\u63d0\u53d6\u3001\u6982\u7387\u5efa\u6a21\u548c\u9a8c\u8bc1\u7684\u65b0\u578b\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\u3002\u7814\u7a76\u57fa\u4e8e\u60c5\u51b5\u8986\u76d6\u7f51\u683c\u7684\u6982\u5ff5\uff0c\u8be6\u5c3d\u679a\u4e3e\u4e86\u4e0e\u8f66\u8f86\u8fd0\u884c\u76f8\u5173\u7684\u73af\u5883\u914d\u7f6e\u3002\u8be5\u7f51\u683c\u901a\u8fc7\u57fa\u4e8e\u60c5\u51b5\u7684\u7cfb\u7edf\u6d4b\u8bd5\u6536\u96c6\u7684\u5b9a\u91cf\u6982\u7387\u6570\u636e\u8fdb\u884c\u589e\u5f3a\uff0c\u6355\u83b7\u4e86\u60c5\u51b5\u4e4b\u95f4\u7684\u6982\u7387\u8f6c\u6362\u3002\u7814\u7a76\u7136\u540e\u751f\u6210\u4e86\u4e00\u4e2a\u6982\u7387\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7f16\u7801\u4e86\u6b63\u5e38\u548c\u4e0d\u5b89\u5168\u7cfb\u7edf\u884c\u4e3a\u7684\u52a8\u6001\u3002\u901a\u8fc7\u9488\u5bf9\u8be5\u6a21\u578b\u7684\u6982\u7387\u6a21\u578b\u68c0\u67e5\u6765\u9a8c\u8bc1\u4ece\u5371\u9669\u5206\u6790\u4e2d\u63d0\u53d6\u5e76\u4ee5\u65f6\u5e8f\u903b\u8f91\u5f62\u5f0f\u5316\u7684\u5b89\u5168\u5c5e\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u9ad8\u98ce\u9669\u60c5\u51b5\uff0c\u63d0\u4f9b\u5b9a\u91cf\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u652f\u6301\u6ee1\u8db3\u76d1\u7ba1\u6807\u51c6\u7684\u8981\u6c42\uff0c\u4ece\u800c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u9760\u90e8\u7f72\u505a\u51fa\u8d21\u732e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u8bc6\u522b\u9ad8\u98ce\u9669\u60c5\u51b5\uff0c\u63d0\u4f9b\u5b9a\u91cf\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u652f\u6301\u7b26\u5408\u76d1\u7ba1\u6807\u51c6\uff0c\u4ece\u800c\u4fc3\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7a33\u5065\u90e8\u7f72\u3002"}}
{"id": "2507.11939", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b10\u79cd\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u82f1\u8bed\u4e2d\u5fc3\u5316\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002\u5b83\u901a\u8fc7\u89e3\u8026\u6570\u636e\u548c\u4ee3\u7801\u6765\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\uff0c\u5e76\u63ed\u793a\u4e86\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\u5728\u56fe\u8868\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u5168\u7403\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u5168\u7403\u53d7\u4f17\u7684\u53ef\u53ca\u6027\u548c\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u591a\u8bed\u8a00\u7684\u57fa\u51c6\u6765\u4fc3\u8fdb\u5bf9\u5168\u7403\u53d7\u4f17\u7684\u666e\u9002\u6027\u3002", "method": "PolyChartQA\u7684\u6784\u5efa\u91c7\u7528\u4e86\u89e3\u8026\u7ba1\u7ebf\uff0c\u5c06\u56fe\u8868\u6570\u636e\u4e0e\u6e32\u67d3\u4ee3\u7801\u5206\u79bb\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6570\u636e\u548c\u91cd\u7528\u4ee3\u7801\u6765\u7075\u6d3b\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u7ffb\u8bd1\uff0c\u5e76\u5b9e\u65bd\u4e86\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u7684\u591a\u8bed\u8a00\u56fe\u8868\u7684\u8bed\u8a00\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "PolyChartQA\u5305\u542b22,606\u5f20\u56fe\u8868\u548c26,151\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d610\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u5728\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\uff08\u5c24\u5176\u662f\u5177\u6709\u975e\u62c9\u4e01\u5b57\u7b26\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e4b\u95f4\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PolyChartQA\u7684\u53d1\u5e03\u4e3a\u63a8\u52a8\u5168\u7403\u5305\u5bb9\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u591a\u8bed\u8a00\u56fe\u8868\u7406\u89e3\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002"}}
{"id": "2507.11893", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11893", "abs": "https://arxiv.org/abs/2507.11893", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "title": "Spatial Frequency Modulation for Semantic Segmentation", "comment": "Accept by TPAMI 2025", "summary": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at\n\\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "AI": {"tldr": "SFM\u662f\u4e00\u79cd\u901a\u8fc7\u8c03\u5236\u548c\u89e3\u8c03\u9ad8\u9891\u7279\u5f81\u6765\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662f\u8bed\u4e49\u5206\u5272\uff09\u51c6\u786e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4fdd\u7559\u56fe\u50cf\u7ec6\u8282\u3002", "motivation": "\u89e3\u51b3\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\uff09\u7684\u4e0b\u91c7\u6837\u5c42\u4e2d\uff0c\u9ad8\u7a7a\u95f4\u9891\u7387\u4fe1\u606f\uff08\u5982\u7eb9\u7406\u7ec6\u8282\uff09\u5bb9\u6613\u56e0\u5948\u594e\u65af\u7279\u5b9a\u7406\u800c\u4ea7\u751f\u6df7\u53e0\u6216\u5931\u771f\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u9891\u7387\u8c03\u5236\uff08SFM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u91c7\u6837\uff08ARS\uff09\u5c06\u9ad8\u9891\u7279\u5f81\u8c03\u5236\u5230\u8f83\u4f4e\u9891\u7387\uff0c\u4ee5\u907f\u514d\u4e0b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u6df7\u53e0\u5931\u771f\uff1b\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff08MSAU\uff09\u5728\u6062\u590d\u9ad8\u9891\u4fe1\u606f\u65f6\u5229\u7528\u591a\u5c3a\u5ea6\u4fe1\u606f\u4ea4\u4e92\u3002", "result": "SFM\u6210\u529f\u7f13\u89e3\u4e86\u6df7\u53e0\u95ee\u9898\uff0c\u5e76\u5728\u4e0a\u91c7\u6837\u540e\u6210\u529f\u6062\u590d\u4e86\u9ad8\u9891\u7ec6\u8282\u3002\u8be5\u65b9\u6cd5\u5df2\u88ab\u9a8c\u8bc1\u53ef\u6269\u5c55\u5230\u56fe\u50cf\u5206\u7c7b\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u5b9e\u4f8b\u5206\u5272\u548c\u5168\u666f\u5206\u5272\u7b49\u4efb\u52a1\u3002", "conclusion": "SFM\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u91c7\u6837\uff08ARS\uff09\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff08MSAU\uff09\u6709\u6548\u7f13\u89e3\u4e86\u6df7\u53e0\u95ee\u9898\uff0c\u6210\u529f\u4fdd\u7559\u4e86\u9ad8\u9891\u7ec6\u8282\uff0c\u5e76\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u79cd\u5206\u5272\u548c\u8bc6\u522b\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.11957", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.11957", "abs": "https://arxiv.org/abs/2507.11957", "authors": ["Siddharth Tiwary", "Joel E. Moore"], "title": "Measurement-Induced Phase Transition in a Disordered XX Spin Chain: A Real-Space Renormalization Group Study", "comment": null, "summary": "Spin chains with quenched disorder exhibit rich critical behavior, often\ncaptured by real-space renormalization group (RSRG) techniques. However, the\nphysics of such systems in the presence of random measurements (i.e.,\nnon-Hermitian dephasing) remains largely unexplored. The interplay between\nmeasurements and unitary dynamics gives rise to novel phases and phase\ntransitions in monitored quantum systems. In this work, we investigate the\ndisordered XX spin chain subject to stochastic local measurements in the $X$\nand $Y$ bases. By mapping the monitored chain to a non-Hermitian spin ladder\nwith complex couplings, we propose an RSRG-for-excited-states (RSRG-X) approach\nfor this open-system setting. Our analysis reveals a new class of strongly\ndisordered fixed points that emerge due to non-unitarity, broadening the\nlandscape of critical phenomena accessible via RSRG.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e26\u968f\u673a\u6d4b\u91cf\u7684XX\u81ea\u65cb\u94fe\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RSRG-X\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e86\u7531\u975e\u5e7a\u6b63\u6027\u5f15\u8d77\u7684\u65b0\u578b\u5f3a\u65e0\u5e8f\u4e0d\u52a8\u70b9\u3002", "motivation": "\u7814\u7a76\u5728\u968f\u673a\u6d4b\u91cf\uff08\u975e\u5384\u7c73\u9000\u76f8\u5e72\uff09\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u6dec\u706d\u65e0\u5e8f\u81ea\u65cb\u94fe\u7684\u7269\u7406\u6027\u8d28\uff0c\u63a2\u7d22\u6d4b\u91cf\u4e0e\u5e7a\u6b63\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u4ea7\u751f\u65b0\u7684\u76f8\u548c\u76f8\u53d8\u3002", "method": "\u901a\u8fc7\u5c06\u53d7\u76d1\u6d4b\u94fe\u6620\u5c04\u5230\u5177\u6709\u590d\u6742\u8026\u5408\u7684\u975e\u5384\u7c73\u81ea\u65cb\u68af\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5f00\u653e\u7cfb\u7edf\u7684RSRG-X\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5728\u968f\u673a\u6d4b\u91cf\u4e0b\uff0cXX\u81ea\u65cb\u94fe\u4f1a\u51fa\u73b0\u4e00\u7c7b\u65b0\u7684\u5f3a\u65e0\u5e8f\u4e0d\u52a8\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u7531\u4e8e\u975e\u5e7a\u6b63\u6027\u51fa\u73b0\u7684\u4e00\u7c7b\u65b0\u7684\u5f3a\u65e0\u5e8f\u4e0d\u52a8\u70b9\uff0c\u62d3\u5bbd\u4e86RSRG\u53ef\u53ca\u7684\u4e34\u754c\u73b0\u8c61\u8303\u56f4\u3002"}}
{"id": "2507.11732", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11732", "abs": "https://arxiv.org/abs/2507.11732", "authors": ["Shiyu Chen", "Cencheng Shen", "Youngser Park", "Carey E. Priebe"], "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning", "comment": null, "summary": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide\nrange of node-level graph learning tasks. However, their performance is often\nconstrained by reliance on random or minimally informed initial feature\nrepresentations, which can lead to slow convergence and suboptimal solutions.\nIn this paper, we leverage a statistically grounded method, one-hot graph\nencoder embedding (GEE), to generate high-quality initial node features that\nenhance the end-to-end training of GNNs. We refer to this integrated framework\nas the GEE-powered GNN (GG), and demonstrate its effectiveness through\nextensive simulations and real-world experiments across both unsupervised and\nsupervised settings. In node clustering, GG consistently achieves\nstate-of-the-art performance, ranking first across all evaluated real-world\ndatasets, while exhibiting faster convergence compared to the standard GNN. For\nnode classification, we further propose an enhanced variant, GG-C, which\nconcatenates the outputs of GG and GEE and outperforms competing baselines.\nThese results confirm the importance of principled, structure-aware feature\ninitialization in realizing the full potential of GNNs.", "AI": {"tldr": "GG\u662f\u4e00\u79cd\u65b0\u7684GNN\u6846\u67b6\uff0c\u901a\u8fc7GEE\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u8282\u70b9\u7279\u5f81\uff0c\u5728\u8282\u70b9\u805a\u7c7b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GNN\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u4f9d\u8d56\u968f\u673a\u6216\u4fe1\u606f\u91cf\u5c11\u7684\u521d\u59cb\u7279\u5f81\u8868\u793a\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6536\u655b\u6162\u548c\u6b21\u4f18\u89e3\u3002\u800cGG\u5229\u7528GEE\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u8282\u70b9\u7279\u5f81\uff0c\u4ee5\u589e\u5f3aGNN\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGEE\uff08one-hot graph encoder embedding\uff09\u7684\u7edf\u8ba1\u5b66\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0eGNN\u76f8\u7ed3\u5408\uff0c\u5f62\u6210GEE\u9a71\u52a8\u7684GNN\uff08GG\uff09\uff0c\u5e76\u901a\u8fc7GG-C\uff08GG\u548cGEE\u7684\u8f93\u51fa\u8fde\u63a5\uff09\u6765\u6539\u8fdb\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u3002", "result": "GG\u5728\u8282\u70b9\u805a\u7c7b\u4efb\u52a1\u4e0a\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u4e14\u6bd4\u6807\u51c6GNN\u6536\u655b\u66f4\u5feb\u3002GG-C\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "GG-C\u901a\u8fc7\u8fde\u63a5GG\u548cGEE\u7684\u8f93\u51fa\u6765\u6539\u8fdbGNN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0cGG\u5728\u8282\u70b9\u805a\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6GNN\uff0c\u5e76\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u8fd9\u8868\u660e\u5728\u5b9e\u73b0GNN\u7684\u5168\u90e8\u6f5c\u529b\u65f6\uff0c\u57fa\u4e8e\u539f\u5219\u7684\u3001\u611f\u77e5\u7ed3\u6784\u7684\u7279\u5f81\u521d\u59cb\u5316\u5f88\u91cd\u8981\u3002"}}
{"id": "2507.11900", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11900", "abs": "https://arxiv.org/abs/2507.11900", "authors": ["Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai"], "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos", "comment": "CompressedVQA-HDR won first place in the FR track of the\n  Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE\n  ICME 2025", "summary": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.", "AI": {"tldr": "\u63d0\u51fa CompressedVQA-HDR \u6846\u67b6\uff0c\u5229\u7528 Swin Transformer (FR) \u548c SigLip 2 (NR) \u8bc4\u4f30 HDR \u89c6\u9891\u8d28\u91cf\uff0c\u901a\u8fc7\u5728 SDR \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u548c\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7b49\u7b56\u7565\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u538b\u7f29\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u65b9\u6cd5\u5728\u5904\u7406\u65e5\u76ca\u591a\u6837\u5316\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u7279\u522b\u662f\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u5185\u5bb9\u65f6\uff0c\u5f80\u5f80\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u8bc4\u4f30 HDR \u89c6\u9891\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CompressedVQA-HDR \u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6846\u67b6\uff0c\u5305\u542b\u5168\u53c2\u8003\uff08FR\uff09\u548c\u65e0\u53c2\u8003\uff08NR\uff09\u4e24\u79cd\u6a21\u578b\u3002FR \u6a21\u578b\u91c7\u7528 Swin Transformer \u63d0\u53d6\u53c2\u8003\u5e27\u548c\u5931\u771f\u5e27\u7684\u6df1\u5ea6\u7ed3\u6784\u548c\u7eb9\u7406\u76f8\u4f3c\u6027\u7279\u5f81\u3002NR \u6a21\u578b\u5229\u7528 SigLip 2 \u63d0\u53d6\u6700\u7ec8\u5c42\u7279\u5f81\u56fe\u7684\u5168\u5c40\u5747\u503c\u4f5c\u4e3a\u8d28\u91cf\u611f\u77e5\u8868\u793a\u3002\u4e3a\u4e86\u89e3\u51b3 HDR \u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0cFR \u6a21\u578b\u5728\u5927\u91cf SDR VQA \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u5728 HDRSDR-VQA \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff1bNR \u6a21\u578b\u5219\u91c7\u7528\u8de8\u591a\u4e2a\u538b\u7f29 VQA \u6570\u636e\u96c6\u7684\u8fed\u4ee3\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7b56\u7565\uff0c\u968f\u540e\u5728 HDRSDR-VQA \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 FR \u548c NR \u6a21\u578b\u76f8\u6bd4\u73b0\u6709\u7684 VQA \u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002CompressedVQA-HDR-FR \u6a21\u578b\u5728 IEEE ICME 2025 \u7684 Generalizable HDR & SDR Video Quality Measurement Grand Challenge \u7684 FR \u8d5b\u9053\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 CompressedVQA-HDR \u6846\u67b6\u5728\u8bc4\u4f30 HDR \u89c6\u9891\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176 FR \u548c NR \u6a21\u578b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14 FR \u6a21\u578b\u5728 IEEE ICME 2025 \u7684\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002"}}
{"id": "2507.11970", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.11970", "abs": "https://arxiv.org/abs/2507.11970", "authors": ["Mi-Ying Huang", "Er-Cheng Tang"], "title": "Obfuscation of Unitary Quantum Programs", "comment": null, "summary": "Program obfuscation aims to hide the inner workings of a program while\npreserving its functionality. In the quantum setting, recent works have\nobtained obfuscation schemes for specialized classes of quantum circuits. For\ninstance, Bartusek, Brakerski, and Vaikuntanathan (STOC 2024) constructed a\nquantum state obfuscation scheme, which supports the obfuscation of quantum\nprograms represented as quantum states for pseudo-deterministic quantum\nprograms with classical inputs and outputs in the classical oracle model.\n  In this work, we improve upon existing results by constructing the first\nquantum state obfuscation scheme for unitary (or approximately unitary) quantum\nprograms supporting quantum inputs and outputs in the classical oracle model.\nAt the core of our obfuscation scheme are two novel ingredients: a functional\nquantum authentication scheme that allows key holders to learn specific\nfunctions of the authenticated quantum state with simulation-based security,\nand a compiler that represents an arbitrary quantum circuit as a projective\nlinear-plus-measurement quantum program described by a sequence of non-adaptive\nClifford gates interleaved with adaptive and compatible measurements.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u6001\u6df7\u6dc6\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u91cf\u5b50\u8f93\u5165\u548c\u8f93\u51fa\u7684\u91cf\u5b50\u7a0b\u5e8f\u7684\u6df7\u6dc6\u3002", "motivation": "\u5728\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\uff0c\u7a0b\u5e8f\u6df7\u6dc6\u65e8\u5728\u9690\u85cf\u7a0b\u5e8f\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u540c\u65f6\u4fdd\u6301\u5176\u529f\u80fd\u3002\u867d\u7136\u73b0\u6709\u7814\u7a76\u5df2\u5728\u7279\u5b9a\u7c7b\u522b\u7684\u91cf\u5b50\u7535\u8def\u4e0a\u5b9e\u73b0\u4e86\u6df7\u6dc6\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u7ed3\u679c\uff0c\u63d0\u51fa\u9996\u4e2a\u652f\u6301\u91cf\u5b50\u8f93\u5165\u548c\u8f93\u51fa\u7684\u91cf\u5b50\u6001\u6df7\u6dc6\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u529f\u80fd\u6027\u91cf\u5b50\u8ba4\u8bc1\u65b9\u6848\u548c\u4e00\u4e2a\u7f16\u8bd1\u5668\u6765\u5b9e\u73b0\u6df7\u6dc6\u3002\u8be5\u8ba4\u8bc1\u65b9\u6848\u5141\u8bb8\u5bc6\u94a5\u6301\u6709\u8005\u5728\u6a21\u62df\u5b89\u5168\u4e0b\u5b66\u4e60\u8ba4\u8bc1\u91cf\u5b50\u6001\u7684\u7279\u5b9a\u51fd\u6570\u3002\u8be5\u7f16\u8bd1\u5668\u5c06\u4efb\u610f\u91cf\u5b50\u7535\u8def\u8868\u793a\u4e3a\u4e00\u79cd\u91cf\u5b50\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u7531\u975e\u81ea\u9002\u5e94\u514b\u5229\u798f\u5fb7\u95e8\u548c\u81ea\u9002\u5e94\u4e14\u517c\u5bb9\u7684\u6d4b\u91cf\u5e8f\u5217\u7ec4\u6210\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2a\u652f\u6301\u91cf\u5b50\u8f93\u5165\u548c\u8f93\u51fa\u7684\u91cf\u5b50\u6001\u6df7\u6dc6\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u529f\u80fd\u6027\u91cf\u5b50\u8ba4\u8bc1\u65b9\u6848\u548c\u4e00\u4e2a\u5c06\u4efb\u610f\u91cf\u5b50\u7535\u8def\u8868\u793a\u4e3a\u7279\u5b9a\u91cf\u5b50\u7a0b\u5e8f\u7684\u7f16\u8bd1\u5668\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u652f\u6301\u91cf\u5b50\u8f93\u5165\u548c\u8f93\u51fa\u7684\u91cf\u5b50\u6001\u6df7\u6dc6\u65b9\u6848\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u9488\u5bf9\u4f2a\u786e\u5b9a\u6027\u91cf\u5b50\u7a0b\u5e8f\u5728\u7ecf\u5178\u9884\u8a00\u673a\u6a21\u578b\u4e0b\u7684\u6df7\u6dc6\u65b9\u6848\u3002"}}
{"id": "2507.11739", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.11739", "abs": "https://arxiv.org/abs/2507.11739", "authors": ["Urban Fasel"], "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction", "comment": null, "summary": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for\ndiscovering nonlinear dynamical system models from data. Quantifying\nuncertainty in SINDy models is essential for assessing their reliability,\nparticularly in safety-critical applications. While various uncertainty\nquantification methods exist for SINDy, including Bayesian and ensemble\napproaches, this work explores the integration of Conformal Prediction, a\nframework that can provide valid prediction intervals with coverage guarantees\nbased on minimal assumptions like data exchangeability. We introduce three\napplications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)\nquantifying uncertainty in time series prediction, (2) model selection based on\nlibrary feature importance, and (3) quantifying the uncertainty of identified\nmodel coefficients using feature conformal prediction. We demonstrate the three\napplications on stochastic predator-prey dynamics and several chaotic dynamical\nsystems. We show that conformal prediction methods integrated with E-SINDy can\nreliably achieve desired target coverage for time series forecasting,\neffectively quantify feature importance, and produce more robust uncertainty\nintervals for model coefficients, even under non-Gaussian noise, compared to\nstandard E-SINDy coefficient estimates.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u4fdd\u5f62\u9884\u6d4b\u4e0e\u96c6\u6210SINDy\uff08E-SINDy\uff09\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u3001\u6a21\u578b\u9009\u62e9\u548c\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30SINDy\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u5b58\u5728\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\u4e0eSINDy\u7684\u7ed3\u5408\uff0c\u4ee5\u63d0\u4f9b\u5177\u6709\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\u3002", "method": "\u672c\u7814\u7a76\u5c06\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\u4e0e\u96c6\u6210SINDy\uff08E-SINDy\uff09\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5e94\u7528\uff1a1.\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff1b2.\u57fa\u4e8e\u5e93\u7279\u5f81\u91cd\u8981\u6027\u8fdb\u884c\u6a21\u578b\u9009\u62e9\uff1b3.\u901a\u8fc7\u7279\u5f81\u4fdd\u5f62\u9884\u6d4b\u91cf\u5316\u5df2\u8bc6\u522b\u6a21\u578b\u7cfb\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u4e0eE-SINDy\u96c6\u6210\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u80fd\u591f\u53ef\u9760\u5730\u5b9e\u73b0\u76ee\u6807\u8986\u76d6\u7387\uff0c\u5728\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u5e76\u4e14\u80fd\u591f\u4e3a\u6a21\u578b\u7cfb\u6570\u751f\u6210\u66f4\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\uff0c\u5373\u4f7f\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\uff0c\u5176\u8868\u73b0\u4e5f\u4f18\u4e8e\u6807\u51c6\u7684E-SINDy\u7cfb\u6570\u4f30\u8ba1\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u4fdd\u5f62\u9884\u6d4b\u4e0e\u96c6\u6210SINDy\uff08E-SINDy\uff09\u76f8\u7ed3\u5408\uff0c\u4e3aSINDy\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u6a21\u578b\u9009\u62e9\u548c\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6ee1\u8db3\u671f\u671b\u7684\u8986\u76d6\u7387\uff0c\u6709\u6548\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u751f\u6210\u66f4\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\uff0c\u5373\u4f7f\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u4e5f\u4f18\u4e8e\u6807\u51c6\u7684E-SINDy\u4f30\u8ba1\u3002"}}
{"id": "2507.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12194", "abs": "https://arxiv.org/abs/2507.12194", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "comment": null, "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniLGL\u7684\u7edf\u4e00\u5927\u5730\u6fc0\u5149\u96f7\u8fbe\uff08LGL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7f16\u7801\u51e0\u4f55\u548c\u6750\u8d28\u4fe1\u606f\u3001\u878d\u5408\u591aBEV\u56fe\u50cf\u4ee5\u53ca\u5f15\u5165\u89c6\u70b9\u4e0d\u53d8\u6027\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u7a7a\u95f4\u3001\u6750\u8d28\u548c\u4f20\u611f\u5668\u7c7b\u578b\u7684\u5747\u5300\u6027\uff0c\u5e76\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u548c\u5e73\u53f0\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u5730\u6fc0\u5149\u96f7\u8fbe\uff08LGL\uff09\u65b9\u6cd5\u901a\u5e38\u53ea\u8003\u8651\u6fc0\u5149\u96f7\u8fbe\u89c2\u6d4b\u7684\u90e8\u5206\u4fe1\u606f\uff08\u4f8b\u5982\u51e0\u4f55\u7279\u5f81\uff09\uff0c\u6216\u8005\u4ec5\u9002\u7528\u4e8e\u540c\u7c7b\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\uff0c\u5ffd\u7565\u4e86LGL\u7684\u7edf\u4e00\u6027\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684LGL\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u7a7a\u95f4\u3001\u6750\u8d28\u548c\u4f20\u611f\u5668\u7c7b\u578b\u7684\u5747\u5300\u6027\u3002", "method": "UniLGL\u65b9\u6cd5\u901a\u8fc7\u5c06\u5305\u542b\u51e0\u4f55\u548c\u6750\u8d28\u4fe1\u606f\u7684\u70b9\u4e91\u7f16\u7801\u4e3a\u4e00\u5bf9BEV\u56fe\u50cf\uff08\u7a7a\u95f4BEV\u56fe\u50cf\u548c\u5f3a\u5ea6BEV\u56fe\u50cf\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u878d\u5408\u7f51\u7edc\u6765\u63d0\u53d6\u5747\u5300\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u7a7a\u95f4\u548c\u6750\u8d28\u7684\u5747\u5300\u6027\u3002\u4e3a\u4e86\u5b9e\u73b0\u4f20\u611f\u5668\u7c7b\u578b\u5747\u5300\u6027\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u89c6\u70b9\u4e0d\u53d8\u6027\u5047\u8bbe\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u5e73\u79fb\u7b49\u53d8\u6027\u5047\u8bbe\uff0c\u5e76\u5728\u5168\u5c40\u63cf\u8ff0\u7b26\u548c\u5c40\u90e8\u7279\u5f81\u8868\u793a\u4e0a\u76d1\u7763UniLGL\u3002\u6700\u540e\uff0c\u901a\u8fc72D BEV\u56fe\u50cf\u4e0a\u7684\u5c40\u90e8\u7279\u5f81\u4e0e\u70b9\u4e91\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u9c81\u68d2\u7684\u5168\u5c40\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u5728SE(3)\u4e0a\u786e\u5b9a\u5168\u5c40\u59ff\u6001\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u65e0\u9700\u989d\u5916\u7684\u914d\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniLGL\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684LGL\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5361\u8f66\u548c\u5fae\u578b\u98de\u884c\u5668\u7b49\u4e0d\u540c\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u5efa\u56fe\u548c\u591aMAV\u534f\u540c\u63a2\u7d22\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5de5\u4e1a\u548c\u73b0\u573a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684UniLGL\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u901a\u8fc7\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u7a7a\u95f4\u548c\u6750\u8d28\u5747\u5300\u6027\u4ee5\u53ca\u4f20\u611f\u5668\u7c7b\u578b\u5747\u5300\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684LGL\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0cUniLGL\u5df2\u6210\u529f\u90e8\u7f72\u5728\u4e0d\u540c\u5e73\u53f0\uff08\u5982\u5361\u8f66\u548c\u5fae\u578b\u98de\u884c\u5668\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u5efa\u56fe\u548c\u591aMAV\u534f\u540c\u63a2\u7d22\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5de5\u4e1a\u548c\u73b0\u573a\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.11942", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11942", "abs": "https://arxiv.org/abs/2507.11942", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "DAC\u662f\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u4fe1\u606f\u5bc6\u5ea6\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4fe1\u606f\u71b5\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u7b97\u6cd5\u5c42\u9762\u7684\u6ce8\u610f\u529b\u5173\u952e\u6807\u8bb0\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u538b\u7f29\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u71b5\u7684\u53d8\u5316\uff0c\u800cDAC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7684\u3001\u8003\u8651\u6ce8\u610f\u529b\u7684\u4efb\u52a1\u65e0\u5173\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff08DAC\uff09\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u5728\u538b\u7f29\u8fc7\u7a0b\u4e2d\u52a8\u6001\u611f\u77e5\u71b5\u53d8\u5316\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63d0\u793a\u538b\u7f29\u3002", "result": "DAC\u5728\u5404\u79cd\u4efb\u52a1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "DAC\u5728LongBench\u3001GSM8K\u548cBBH\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u59cb\u7ec8\u80fd\u5e26\u6765\u7a33\u5065\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u6709\u6548\u8bc1\u660e\u4e86\u5176\u529f\u6548\u3002"}}
{"id": "2507.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11910", "abs": "https://arxiv.org/abs/2507.11910", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "AI": {"tldr": "SEPose is a synthetic dataset for event-based human pose estimation, useful for traffic and pedestrian monitoring, showing good sim-to-real transfer.", "motivation": "Addresses the limited availability of event-based human pose estimation data for challenging pedestrian and traffic monitoring conditions, despite the promise of event-based sensors in such scenarios.", "method": "Generated a synthetic event-based dataset (SEPose) using the CARLA simulator with dynamic vision sensors, featuring ~350K annotated pedestrians with body pose keypoints from fixed camera perspectives across diverse scenarios. Trained and evaluated state-of-the-art models (RVT, YOLOv8) on this dataset and real event-based data.", "result": "Trained existing state-of-the-art models on SEPose and evaluated them on real event-based data, demonstrating the dataset's effectiveness for sim-to-real generalization.", "conclusion": "The SEPose dataset enables training and evaluation of event-based human pose estimation models, demonstrating strong sim-to-real generalization capabilities."}}
{"id": "2507.12013", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12013", "abs": "https://arxiv.org/abs/2507.12013", "authors": ["Siddhant Dutta", "Nouhaila Innan", "Sadok Ben Yahia", "Muhammad Shafique"], "title": "QAS-QTNs: Curriculum Reinforcement Learning-Driven Quantum Architecture Search for Quantum Tensor Networks", "comment": "9 pages, 3 figures. Accepted at QCE 2025", "summary": "Quantum Architecture Search (QAS) is an emerging field aimed at automating\nthe design of quantum circuits for optimal performance. This paper introduces a\nnovel QAS framework employing hybrid quantum reinforcement learning with\nquantum curriculum learning strategies, enabling learning agents to tackle\nincreasingly complex quantum circuit design tasks. We benchmark four\nstate-of-the-art classical reinforcement learning algorithms (A2C, PPO, DDQN,\nTD3) against their quantum-enhanced counterparts (QA2C, QPPO, QDDQN, QTD3) for\noptimizing variational quantum circuits (VQCs). Our approach progressively\nincreases circuit depth and gate complexity during training, leveraging\nparameterized quantum circuits as function approximations. To improve learning\nefficiency and stability, all algorithms, both classical and quantum, are\naugmented with Prioritized Experience Replay (PER). Experimental results show\nthat quantum-enhanced RL significantly outperforms classical methods. In a\n2-qubit environment, PERQDDQN achieves a success probability of 0.46 with\n~3,000 optimal successes, surpassing classical PERDDQN (0.42, ~2,400). In the\nmore complex 3-qubit setting, PERQDDQN and PERQTD3 reach success probabilities\nof ~0.47, with optimal success counts of ~3,800 and ~3,600, respectively,\noutperforming their classical counterparts. Additionally, we apply our QAS-QTN\napproach to a classification problem, where the optimized quantum circuit\nachieves an accuracy of 90.33\\%, outperforming quantum models consisting of\nrandom ansatz. This hybrid classical-quantum approach leads to faster\nconvergence and more efficient quantum circuit designs, demonstrating its\npotential for advancing automated quantum architecture search.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u548c\u91cf\u5b50\u8bfe\u7a0b\u5b66\u4e60\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u4f18\u5316\u91cf\u5b50\u7535\u8def\u65b9\u9762\u76f8\u5bf9\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u66f4\u590d\u6742\u7684\u91cf\u5b50\u7cfb\u7edf\u65f6\u3002", "motivation": "\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u662f\u4e00\u4e2a\u65e8\u5728\u81ea\u52a8\u5316\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u7684\u65b0\u5174\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684QAS\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u4efb\u52a1\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6df7\u5408\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u548c\u91cf\u5b50\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e86\u56db\u79cd\u5148\u8fdb\u7684\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08A2C, PPO, DDQN, TD3\uff09\u53ca\u5176\u91cf\u5b50\u589e\u5f3a\u7248\u672c\uff08QA2C, QPPO, QDDQN, QTD3\uff09\uff0c\u4ee5\u4f18\u5316\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u7535\u8def\u6df1\u5ea6\u548c\u95e8\u590d\u6742\u5ea6\u6765\u8bad\u7ec3\u5b66\u4e60\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4f5c\u4e3a\u51fd\u6570\u903c\u8fd1\u5668\u3002\u4e3a\u4e86\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u6240\u6709\u7b97\u6cd5\uff08\u5305\u62ec\u7ecf\u5178\u548c\u91cf\u5b50\uff09\u90fd\u91c7\u7528\u4e86\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\uff08PER\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u53d8\u5206\u91cf\u5b50\u7535\u8def\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002\u57282\u91cf\u5b50\u6bd4\u7279\u73af\u5883\u4e2d\uff0cPERQDDQN\u7684\u6210\u529f\u6982\u7387\u4e3a0.46\uff0c\u6700\u4f18\u6210\u529f\u6b21\u6570\u7ea63000\u6b21\uff0c\u4f18\u4e8e\u7ecf\u5178PERDDQN\uff080.42\uff0c\u7ea62400\u6b21\uff09\u3002\u5728\u66f4\u590d\u6742\u76843\u91cf\u5b50\u6bd4\u7279\u73af\u5883\u4e2d\uff0cPERQDDQN\u548cPERQTD3\u7684\u6210\u529f\u6982\u7387\u5747\u8fbe\u5230\u7ea60.47\uff0c\u6700\u4f18\u6210\u529f\u6b21\u6570\u5206\u522b\u7ea6\u4e3a3800\u548c3600\u6b21\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u7ecf\u5178\u5bf9\u5e94\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5c06QAS-QTN\u65b9\u6cd5\u5e94\u7528\u4e8e\u5206\u7c7b\u95ee\u9898\u65f6\uff0c\u4f18\u5316\u540e\u7684\u91cf\u5b50\u7535\u8def\u51c6\u786e\u7387\u8fbe\u523090.33%\uff0c\u4f18\u4e8e\u968f\u673a\u9149\u91cf\u5b50\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6df7\u5408\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u4e0e\u91cf\u5b50\u8bfe\u7a0b\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u6846\u67b6\uff0c\u5728\u4f18\u5316\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7eaf\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u5730\u8bbe\u8ba1\u51fa\u6027\u80fd\u66f4\u4f18\u7684\u91cf\u5b50\u7535\u8def\u3002"}}
{"id": "2507.11757", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11757", "abs": "https://arxiv.org/abs/2507.11757", "authors": ["Yuehua Song", "Yong Gao"], "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction", "comment": null, "summary": "Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.", "AI": {"tldr": "A new Graph-in-Graph (GiG) model using both transductive and inductive learning improves drug-target interaction prediction by integrating molecular and network-level features, outperforming existing methods.", "motivation": "Accurately predicting drug-target interactions (DTIs) is crucial for advancing drug discovery and target validation. Existing machine learning approaches, including GNN-based methods, face difficulties in effectively integrating diverse features of drugs, targets, and their interactions. The proposed framework aims to address this limitation.", "method": "The study introduces a novel framework that leverages both transductive and inductive learning. This framework utilizes a GNN-based model called Graph-in-Graph (GiG), which represents drug and target molecular structures as meta-nodes in a drug-target interaction graph to explore intricate relationships. A special benchmark comprising drug SMILES, protein sequences, and their interaction data was compiled for evaluation.", "result": "The GiG model demonstrates significantly improved performance compared to existing approaches across all evaluation metrics on a specially compiled benchmark.", "conclusion": "The proposed GiG model significantly outperforms existing approaches across all evaluation metrics, highlighting the benefits of integrating different learning paradigms and interaction data."}}
{"id": "2507.12166", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12166", "abs": "https://arxiv.org/abs/2507.12166", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Junting Chen", "Zezhong Zhang", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "comment": null, "summary": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 UrbanRadio3D \u6570\u636e\u96c6\u548c RadioDiff-3D \u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709 3D \u65e0\u7ebf\u7535\u56fe\u6784\u5efa\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u7535\u56fe\uff08RM\uff09\u6784\u5efa\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u8def\u5f84\u635f\u8017\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u5230\u8fbe\u89d2\uff08DoA\uff09\u3001\u5230\u8fbe\u65f6\u95f4\uff08ToA\uff09\u548c\u5782\u76f4\u7a7a\u95f4\u53d8\u5316\u7b49\u5173\u952e\u53c2\u6570\u3002\u8fd9\u662f\u7531\u4e8e\u4f9d\u8d56\u9759\u6001\u5b66\u4e60\u8303\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a UrbanRadio3D \u7684\u5927\u89c4\u6a21\u3001\u9ad8\u5206\u8fa8\u7387 3D \u65e0\u7ebf\u7535\u56fe\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5c04\u7ebf\u8ffd\u8e2a\u6765\u6784\u5efa\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e 3D \u5377\u79ef UNet \u7684\u57fa\u51c6\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3a RadioDiff-3D \u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e 3D \u65e0\u7ebf\u7535\u56fe\u7684\u6784\u5efa\u3002", "result": "UrbanRadio3D \u6570\u636e\u96c6\u6bd4\u4e4b\u524d\u7684 3D RM \u6570\u636e\u96c6\u5927 37 \u500d\u4ee5\u4e0a\uff0c\u5305\u542b\u8def\u5f84\u635f\u8017\u3001DoA \u548c ToA \u4e09\u4e2a\u6307\u6807\uff0c\u4ee5\u53ca\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u591a 7 \u500d\u7684\u9ad8\u5ea6\u5c42\u3002RadioDiff-3D \u5728\u5404\u79cd\u73af\u5883\u52a8\u6001\u4e0b\uff0c\u5728\u6784\u5efa\u9ad8\u7ef4\u65e0\u7ebf\u7535\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u4e3a\u672a\u6765 3D \u73af\u5883\u611f\u77e5\u901a\u4fe1\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6240\u63d0\u51fa\u7684 RadioDiff-3D \u5728\u6784\u5efa\u9ad8\u7ef4\u65e0\u7ebf\u7535\u56fe\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12273", "abs": "https://arxiv.org/abs/2507.12273", "authors": ["Luca Garello", "Francesca Cocchella", "Alessandra Sciutti", "Manuel Catalano", "Francesco Rea"], "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot", "comment": null, "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.", "AI": {"tldr": "Alter-Ego \u673a\u5668\u4eba\u901a\u8fc7 LLM \u548c SLAM \u6280\u672f\u4e3a\u535a\u7269\u9986\u8bbf\u5ba2\u63d0\u4f9b\u4e92\u52a8\u5bfc\u89c8\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u53d7\u5230\u597d\u8bc4\uff0c\u4f46\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\u6709\u5f85\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u548c\u63d0\u5347\u516c\u5171\u573a\u6240\uff08\u7279\u522b\u662f\u6587\u5316\u548c\u6559\u80b2\u573a\u6240\uff09\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u7279\u522b\u662f\u5229\u7528\u81ea\u4e3b\u673a\u5668\u4eba\u3002", "method": "\u8bbe\u8ba1\u3001\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86 Alter-Ego \u673a\u5668\u4eba\uff0c\u5b83\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u3001\u60c5\u5883\u611f\u77e5\u7684\u95ee\u7b54\u4ea4\u4e92\uff0c\u5e76\u5229\u7528\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u6280\u672f\u8fdb\u884c\u65e0\u7f1d\u5bfc\u822a\u548c\u8def\u7ebf\u81ea\u9002\u5e94\u3002\u5728\u771f\u5b9e\u535a\u7269\u9986\u73af\u5883\u4e2d\u5bf9 34 \u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u5408\u4e86\u8bbf\u5ba2-\u673a\u5668\u4eba\u5bf9\u8bdd\u7684\u5b9a\u6027\u5206\u6790\u548c\u4ea4\u4e92\u524d\u540e\u95ee\u5377\u7684\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u673a\u5668\u4eba\u603b\u4f53\u4e0a\u53d7\u5230\u597d\u8bc4\uff0c\u5e76\u4e3a\u535a\u7269\u9986\u4f53\u9a8c\u589e\u6dfb\u4e86\u8da3\u5473\u6027\uff0c\u4f46\u5176\u5728\u7406\u89e3\u548c\u54cd\u5e94\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6587\u5316\u573a\u6240\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u7684\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5728\u652f\u6301\u53ef\u8fbe\u6027\u548c\u77e5\u8bc6\u83b7\u53d6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u6b64\u7c7b\u6280\u672f\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\u3002", "conclusion": "\u672c\u7814\u7a76\u9610\u8ff0\u4e86\u5728\u6587\u5316\u548c\u6559\u80b2\u573a\u6240\u90e8\u7f72\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff08\u5982 Alter-Ego \u535a\u7269\u9986\u5bfc\u89c8\u673a\u5668\u4eba\uff09\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u63d0\u4f9b\u5f15\u4eba\u5165\u80dc\u7684\u535a\u7269\u9986\u4f53\u9a8c\uff0c\u4f46\u5176\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002"}}
{"id": "2507.11953", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11953", "abs": "https://arxiv.org/abs/2507.11953", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "IAM\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u5927\u5c0f\u8bed\u8a00\u6a21\u578b\u95f4\u7684\u6ce8\u610f\u529b\u76f8\u4f3c\u6027\uff0c\u5b9e\u73b0\u5bf9Transformer\u7684\u4f18\u5316\uff0c\u4ece\u800c\u5728\u52a0\u901f\u8ba1\u7b97\u548c\u51cf\u5c11\u663e\u5b58\u5360\u7528\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5e76\u5229\u7528\u6a21\u578b\u95f4\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u7684\u7a00\u758f\u6027\u3002", "method": "IAM\u6846\u67b6\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5c3a\u5ea6\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u6ce8\u610f\u529b\u77e9\u9635\u7684\u76f8\u4f3c\u6027\uff0c\u9009\u62e9\u6620\u5c04\u5c42\u5e76\u8fdb\u884c\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u4ee5\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u5e76\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u3002", "result": "IAM\u6846\u67b6\u80fd\u591f\u52a0\u901f\u9884\u586b\u5145\uff08prefill\uff0915%\uff0c\u5e76\u51cf\u5c1122.1%\u7684KV\u7f13\u5b58\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u4e0d\u660e\u663e\u3002\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u7cfb\u5217\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5176\u4ed6KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\u517c\u5bb9\u3002", "conclusion": "IAM\u6846\u67b6\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u548c\u5927\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u7684\u53cc\u91cd\u6548\u76ca\uff0c\u5e76\u4e14\u5728\u4e0d\u660e\u663e\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u52a0\u901f\u9884\u586b\u5145\u5e76\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eIAM\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5176\u4ed6KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2507.11931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11931", "abs": "https://arxiv.org/abs/2507.11931", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "comment": null, "summary": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12016", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.12016", "abs": "https://arxiv.org/abs/2507.12016", "authors": ["Cong Fu", "Ousi Pan", "Zhiqiang Fan", "Yushun Tang", "Shanhe Su", "Youhui Lin", "Jincan Chen"], "title": "Extending the Limited Performance of the Quantum Refrigerator with Catalysts", "comment": "7+1 pages, 5 figures comments are welcome", "summary": "Quantum thermal machines offer promising platforms for exploring the\nfundamental limits of thermodynamics at the microscopic scale. Here, we study a\ntwo-stroke quantum refrigerator that extracts heat from a cold reservoir via\ndiscrete strokes powered by external work. The working medium consists of two\ntwo-level systems (TLSs) and two heat reservoirs at different temperatures and\nis assisted by an auxiliary system acting as a catalyst. Remarkably, the\ncatalyst remains unchanged after each cycle, ensuring that heat extraction is\ndriven entirely by the work input. We show that the presence of the catalyst\nleads to two significant enhancements: it enables the coefficient of\nperformance (COP) to exceed the Otto bound and allows the refrigerator to\noperate in frequency and temperature regimes that are inaccessible without a\ncatalyst. These results highlight the potential of catalytic mechanisms to\nbroaden the operational capabilities of quantum thermal devices and to surpass\nconventional thermodynamic performance limits.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u50ac\u5316\u5242\uff0c\u7814\u7a76\u4eba\u5458\u6f14\u793a\u4e86\u4e00\u79cd\u91cf\u5b50\u51b0\u7bb1\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u4f20\u7edf\u9650\u5236\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u6761\u4ef6\u4e0b\u8fd0\u884c\u3002", "motivation": "\u91cf\u5b50\u70ed\u529b\u5b66\u673a\u5668\u5728\u63a2\u7d22\u5fae\u89c2\u5c3a\u5ea6\u4e0a\u70ed\u529b\u5b66\u7684\u57fa\u672c\u6781\u9650\u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u5e73\u53f0\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u4e2a\u7531\u4e24\u4e2a\u53cc\u6001\u7cfb\u7edf\uff08TLS\uff09\u548c\u4e24\u4e2a\u4e0d\u540c\u6e29\u5ea6\u7684\u70ed\u5e93\u7ec4\u6210\u7684\u5de5\u4f5c\u4ecb\u8d28\uff0c\u5e76\u501f\u52a9\u5145\u5f53\u50ac\u5316\u5242\u7684\u8f85\u52a9\u7cfb\u7edf\u6765\u64cd\u4f5c\u7684\u4e8c\u51b2\u7a0b\u91cf\u5b50\u51b0\u7bb1\u3002", "result": "\u50ac\u5316\u5242\u7684\u5b58\u5728\u4f7f\u6027\u80fd\u7cfb\u6570\uff08COP\uff09\u8d85\u8d8a\u4e86\u5965\u6258\u754c\u9650\uff0c\u5e76\u5141\u8bb8\u5236\u51b7\u5668\u5728\u6ca1\u6709\u50ac\u5316\u5242\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u8fdb\u5165\u7684\u9891\u7387\u548c\u6e29\u5ea6\u72b6\u6001\u4e0b\u8fd0\u884c\u3002", "conclusion": "\u50ac\u5316\u5242\u80fd\u591f\u8d85\u8d8a\u5965\u6258\u754c\u9650\uff0c\u5e76\u5728\u6ca1\u6709\u50ac\u5316\u5242\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u8fdb\u5165\u7684\u9891\u7387\u548c\u6e29\u5ea6\u72b6\u6001\u4e0b\u8fd0\u884c\u5236\u51b7\u5668\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u50ac\u5316\u673a\u5236\u5728\u62d3\u5bbd\u91cf\u5b50\u70ed\u8bbe\u5907\u7684\u8fd0\u884c\u80fd\u529b\u548c\u8d85\u8d8a\u4f20\u7edf\u70ed\u529b\u5b66\u6027\u80fd\u6781\u9650\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11759", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11759", "abs": "https://arxiv.org/abs/2507.11759", "authors": ["Alexandra Volokhova", "L\u00e9na N\u00e9hale Ezzine", "Piotr Gai\u0144ski", "Luca Scimeca", "Emmanuel Bengio", "Prudencio Tossou", "Yoshua Bengio", "Alex Hernandez-Garcia"], "title": "Torsional-GFN: a conditional conformation generator for small molecules", "comment": "The two first authors are Alexandra Volokhova and L\\'ena N\\'ehale\n  Ezzine, with equal contribution", "summary": "Generating stable molecular conformations is crucial in several drug\ndiscovery applications, such as estimating the binding affinity of a molecule\nto a target. Recently, generative machine learning methods have emerged as a\npromising, more efficient method than molecular dynamics for sampling of\nconformations from the Boltzmann distribution. In this paper, we introduce\nTorsional-GFN, a conditional GFlowNet specifically designed to sample\nconformations of molecules proportionally to their Boltzmann distribution,\nusing only a reward function as training signal. Conditioned on a molecular\ngraph and its local structure (bond lengths and angles), Torsional-GFN samples\nrotations of its torsion angles. Our results demonstrate that Torsional-GFN is\nable to sample conformations approximately proportional to the Boltzmann\ndistribution for multiple molecules with a single model, and allows for\nzero-shot generalization to unseen bond lengths and angles coming from the MD\nsimulations for such molecules. Our work presents a promising avenue for\nscaling the proposed approach to larger molecular systems, achieving zero-shot\ngeneralization to unseen molecules, and including the generation of the local\nstructure into the GFlowNet model.", "AI": {"tldr": "Torsional-GFN \u662f\u4e00\u79cd GFlowNet \u6a21\u578b\uff0c\u53ef\u4ee5\u9ad8\u6548\u751f\u6210\u5206\u5b50\u6784\u8c61\uff0c\u5176\u91c7\u6837\u6bd4\u4f8b\u8fd1\u4f3c\u7b26\u5408\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u5e76\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728\u836f\u7269\u53d1\u73b0\u7b49\u5e94\u7528\u4e2d\u751f\u6210\u7a33\u5b9a\u7684\u5206\u5b50\u6784\u8c61\uff0c\u4ee5\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u66ff\u4ee3\u5206\u5b50\u52a8\u529b\u5b66\u6765\u4ece\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e2d\u91c7\u6837\u6784\u8c61\u3002", "method": "Torsional-GFN \u662f\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNet\uff09\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4ec5\u4f7f\u7528\u5956\u52b1\u51fd\u6570\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u901a\u8fc7\u91c7\u6837\u5176\u626d\u8f6c\u89d2\u6765\u751f\u6210\u4e0e\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u6210\u6bd4\u4f8b\u7684\u5206\u5b50\u6784\u8c61\u3002\u5b83\u4ee5\u5206\u5b50\u56fe\u548c\u5c40\u90e8\u7ed3\u6784\uff08\u952e\u957f\u3001\u952e\u89d2\uff09\u4f5c\u4e3a\u6761\u4ef6\u3002", "result": "Torsional-GFN \u80fd\u591f\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5bf9\u591a\u79cd\u5206\u5b50\u7684\u6784\u8c61\u8fdb\u884c\u91c7\u6837\uff0c\u91c7\u6837\u6bd4\u4f8b\u8fd1\u4f3c\u7b26\u5408\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u5e76\u80fd\u5bf9\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u51fa\u73b0\u7684\u672a\u89c1\u952e\u957f\u548c\u952e\u89d2\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "Torsional-GFN \u80fd\u591f\u4ee5\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5bf9\u591a\u79cd\u5206\u5b50\u7684\u6784\u8c61\u8fdb\u884c\u91c7\u6837\uff0c\u4e14\u91c7\u6837\u6bd4\u4f8b\u8fd1\u4f3c\u7b26\u5408\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u4e3a\u6269\u5c55\u5230\u66f4\u5927\u7684\u5206\u5b50\u7cfb\u7edf\u3001\u5b9e\u73b0\u5bf9\u672a\u89c1\u5206\u5b50\u7684\u96f6\u6837\u672c\u6cdb\u5316\u4ee5\u53ca\u5c06\u5c40\u90e8\u7ed3\u6784\u751f\u6210\u7eb3\u5165 GFlowNet \u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.12196", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12196", "abs": "https://arxiv.org/abs/2507.12196", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "title": "Selective Quantization Tuning for ONNX Models", "comment": "5 pages, 3 figures, 2 tables", "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "AI": {"tldr": "TuneQn \u662f\u4e00\u4e2a\u7528\u4e8e ONNX \u6a21\u578b\u7684\u9009\u62e9\u6027\u91cf\u5316\u3001\u90e8\u7f72\u548c\u4f18\u5316\u5957\u4ef6\uff0c\u53ef\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u3002", "motivation": "\u91cf\u5316\u4f1a\u964d\u4f4e\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f46\u53ef\u80fd\u4ee5\u727a\u7272\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\uff0c\u5e76\u4e14\u5b8c\u5168\u91cf\u5316\u6a21\u578b\u5728\u4f4e\u7aef\u786c\u4ef6\u4e0a\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9009\u62e9\u6027\u91cf\u5316\u65b9\u6cd5\u6765\u786e\u5b9a\u54ea\u4e9b\u5c42\u53ef\u4ee5\u88ab\u6392\u9664\u5728\u91cf\u5316\u4e4b\u5916\u3002", "method": "TuneQn \u662f\u4e00\u4e2a\u652f\u6301 ONNX \u6a21\u578b\u9009\u62e9\u6027\u91cf\u5316\u3001\u8de8\u591a\u79cd CPU \u548c GPU \u8bbe\u5907\u90e8\u7f72\u53ca\u6267\u884c\u7684\u5957\u4ef6\uff0c\u5e76\u7ed3\u5408\u4e86\u6027\u80fd\u5206\u6790\u548c\u591a\u76ee\u6807\u4f18\u5316\u3002TuneQn \u80fd\u751f\u6210\u9009\u62e9\u6027\u91cf\u5316\u7684 ONNX \u6a21\u578b\uff0c\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u90e8\u7f72\u5b83\u4eec\uff0c\u6d4b\u91cf\u51c6\u786e\u6027\u548c\u5927\u5c0f\u7b49\u6307\u6807\uff0c\u6267\u884c\u5e15\u7d2f\u6258\u524d\u6cbf\u6700\u5c0f\u5316\u4ee5\u8bc6\u522b\u6700\u4f73\u6a21\u578b\u5019\u9009\u8005\uff0c\u5e76\u53ef\u89c6\u5316\u7ed3\u679c\u3002", "result": "TuneQn \u5728\u56db\u4e2a ONNX \u6a21\u578b\u548c\u4e24\u4e2a\u91cf\u5316\u8bbe\u7f6e\u4e0a\uff0c\u8de8 CPU \u548c GPU \u8bbe\u5907\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TuneQn \u80fd\u591f\u6709\u6548\u5730\u6267\u884c\u9009\u62e9\u6027\u91cf\u5316\u548c\u8c03\u6574\uff0c\u9009\u62e9\u7684 ONNX \u6a21\u578b\u5019\u9009\u8005\u4e0e\u5b8c\u5168\u91cf\u5316\u6a21\u578b\u76f8\u6bd4\uff0c\u51c6\u786e\u6027\u635f\u5931\u6700\u591a\u53ef\u51cf\u5c11 54.14%\uff0c\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u6bd4\uff0c\u6a21\u578b\u5c3a\u5bf8\u6700\u591a\u53ef\u51cf\u5c11 72.9%\u3002"}}
{"id": "2507.12391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12391", "abs": "https://arxiv.org/abs/2507.12391", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.", "AI": {"tldr": "LLMs\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u73af\u5883\u65f6\u80fd\u529b\u6709\u9650\u3002\u89c6\u89c9\u8f93\u5165\u6709\u4e00\u5b9a\u5e2e\u52a9\uff0c\u4f46\u5e76\u975e\u603b\u662f\u6bd4\u6587\u672c\u63d0\u793a\u66f4\u4f18\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9\u8f93\u5165\u5bf9\u591a\u6a21\u6001LLM\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6548\u7528\u3002", "method": "\u901a\u8fc7\u8bc4\u4f3015\u79cd\u591a\u6a21\u6001LLMs\u57282D\u7f51\u683c\u73af\u5883\u4e2d\u751f\u6210\u8def\u5f84\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u4ec5\u6587\u672c\u8f93\u5165\u4e0e\u6587\u672c\u52a0\u89c6\u89c9\u8f93\u5165\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u7f51\u683c\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u7b80\u5355\u7684\u3001\u5c0f\u89c4\u6a21\u7684\u7f51\u683c\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u8f93\u5165\u6216\u5c11\u6837\u672c\u6587\u672c\u63d0\u793a\u80fd\u5e26\u6765\u4e00\u5b9a\u597d\u5904\uff0c\u6210\u529f\u7387\u4e2d\u7b49\u3002\u7136\u800c\uff0c\u5728\u8f83\u5927\u7684\u7f51\u683c\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u603b\u4f53\u800c\u8a00\uff0c\u66f4\u5927\u7684\u6a21\u578b\u901a\u5e38\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u4f46\u89c6\u89c9\u6a21\u6001\u5e76\u672a\u6bd4\u7ed3\u6784\u826f\u597d\u7684\u6587\u672c\u63d0\u793a\u663e\u793a\u51fa\u666e\u904d\u4f18\u52bf\u3002\u5728\u7b80\u5355\u7f51\u683c\u4e0a\u627e\u5230\u7684\u6210\u529f\u8def\u5f84\u8d28\u91cf\u666e\u904d\u8f83\u9ad8\u3002", "conclusion": "LLMs\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u5728\u5904\u7406\u590d\u6742\u73af\u5883\u548c\u5927\u89c4\u6a21\u4efb\u52a1\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u89c6\u89c9\u8f93\u5165\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u6709\u4e00\u5b9a\u5e2e\u52a9\uff0c\u4f46\u5e76\u975e\u603b\u662f\u4f18\u4e8e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8LLMs\u7684\u7a7a\u95f4\u63a8\u7406\u3001\u7ea6\u675f\u6ee1\u8db3\u548c\u591a\u6a21\u6001\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2507.11954", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11954", "abs": "https://arxiv.org/abs/2507.11954", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "A new multi-stage query-based framework for knowledge graph QA improves performance on complex reasoning and temporal questions, especially with smaller language models.", "motivation": "Large language models struggle with multi-hop reasoning and temporal questions in QA. Query-based KGQA offers a modular alternative by generating executable queries instead of direct answers.", "method": "We propose a multi-stage query-based framework for Wikidata QA, incorporating a novel entity linking and predicate matching method using CoT reasoning. We evaluate its robustness across multi-hop and temporal QA datasets through generalization and rejection studies.", "result": "Our proposed framework enhances performance on challenging multi-hop and temporal benchmarks, demonstrating the potential of query-based multi-stage KGQA for improving QA capabilities.", "conclusion": "We demonstrate that a query-based multi-stage KGQA framework can improve multi-hop and temporal QA performance, even with small language models."}}
{"id": "2507.11932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11932", "abs": "https://arxiv.org/abs/2507.11932", "authors": ["Mohammad Shahab Sepehri", "Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "comment": null, "summary": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.", "AI": {"tldr": "Hyperphantasia\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u53ef\u89c6\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u88ab\u52a8\u89c6\u89c9\u611f\u77e5\uff0c\u5bf9\u6a21\u578b\u5185\u90e8\u6784\u5efa\u89c6\u89c9\u6a21\u5f0f\u4ee5\u652f\u6301\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u4e86\u89e3\u6709\u9650\uff0c\u800c\u5fc3\u7406\u53ef\u89c6\u5316\u5bf9\u4eba\u7c7b\u6765\u8bf4\u662f\u4e00\u9879\u5173\u952e\u7684\u8ba4\u77e5\u6280\u80fd\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8c1c\u9898\uff08\u6bcf\u4e2a\u8c1c\u9898\u90fd\u6709\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff09\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aHyperphantasia\u7684\u5408\u6210\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u53ef\u89c6\u5316\u80fd\u529b\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u7684\u5168\u9762\u8bc4\u4f30\u63ed\u793a\u4e86\u4eba\u7c7b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u5e76\u63a2\u7d22\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u9ad8\u89c6\u89c9\u6a21\u62df\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u5c3d\u7ba1\u4e00\u4e9b\u6a21\u578b\u5728\u8bc6\u522b\u89c6\u89c9\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u90e8\u5206\u80fd\u529b\uff0c\u4f46\u5f3a\u5927\u7684\u5fc3\u7406\u53ef\u89c6\u5316\u80fd\u529b\u4ecd\u7136\u662f\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u6027\u6311\u6218\u3002"}}
{"id": "2507.12020", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.12020", "abs": "https://arxiv.org/abs/2507.12020", "authors": ["Salvatore Alex Cordovana", "Luigi Giannelli", "Nicola Macr\u00ec", "Giuliano Benenti", "Elisabetta Paladino", "Giuseppe A. Falci"], "title": "Channel capacity of small modular quantum networks in the ultrastrongly coupled regime", "comment": null, "summary": "We investigate state-transfer in modular quantum computer architectures\nexploiting the ultrastrong coupling regime of interaction between quantum\nprocessing units and ICs. We show that protocols based on adiabatic coherent\ntransport may achieve near-ideal single-letter quantum capacity and robustness\nagainst parametric fluctuations suppressing leakage induced by the dynamical\nCasimir effect.", "AI": {"tldr": "Modular quantum computers can achieve high capacity and robustness using ultrastrong coupling and adiabatic transport, mitigating leakage from the dynamical Casimir effect.", "motivation": "To investigate state-transfer in modular quantum computer architectures exploiting the ultrastrong coupling regime.", "method": "Investigate state-transfer in modular quantum computer architectures exploiting the ultrastrong coupling regime of interaction between quantum processing units and ICs.", "result": "Protocols based on adiabatic coherent transport may achieve near-ideal single-letter quantum capacity and robustness against parametric fluctuations suppressing leakage induced by the dynamical Casimir effect.", "conclusion": " Adiabatic coherent transport protocols achieve near-ideal single-letter quantum capacity and robustness against parametric fluctuations by suppressing leakage induced by the dynamical Casimir effect."}}
{"id": "2507.11771", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11771", "abs": "https://arxiv.org/abs/2507.11771", "authors": ["Sheikh Abdur Raheem Ali", "Justin Xu", "Ivory Yang", "Jasmine Xinze Li", "Ayse Arslan", "Clark Benham"], "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "comment": null, "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.", "AI": {"tldr": "\u5bf9\u6bd4\u6fc0\u6d3b\u589e\u52a0\uff08CAA\uff09\u6280\u672f\u5728Llama 2\u6a21\u578b\u4e0a\u7684\u6548\u679c\u968f\u5c42\u7ea7\u548c\u6a21\u578b\u89c4\u6a21\u53d8\u5316\uff0c\u8d1f\u9762\u5f15\u5bfc\u6548\u679c\u66f4\u5f3a\u3002", "motivation": "\u8bc4\u4f30\u5bf9\u6bd4\u6fc0\u6d3b\u589e\u52a0\uff08CAA\uff09\u6280\u672f\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u590d\u6742\u548c\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5bfb\u627e\u6a21\u578b\u6b8b\u5dee\u6d41\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u201c\u65b9\u5411\u201d\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u6765\u63a2\u7d22\u5bf9\u6bd4\u6fc0\u6d3b\u589e\u52a0\uff08CAA\uff09\u6280\u672f\u5728Llama 2\u6a21\u578b\uff087B\u300113B\u548c70B\uff09\u4e0a\u7684\u6709\u6548\u6027\u3002\u4f7f\u7528\u4ee5\u62d2\u7edd\u884c\u4e3a\u4e3a\u4e2d\u5fc3\u7684\u7b54\u6848\u5339\u914d\u95ee\u9898\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "1) CAA\u5728\u6a21\u578b\u65e9\u671f\u5230\u4e2d\u671f\u5c42\u7ea7\u6700\u4e3a\u6709\u6548\u3002 2) CAA\u7684\u6548\u679c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u800c\u51cf\u5f31\u3002 3) \u8d1f\u9762\u5f15\u5bfc\u6bd4\u6b63\u9762\u5f15\u5bfc\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u4ea7\u751f\u66f4\u660e\u663e\u7684\u6548\u679c\u3002", "conclusion": "\u5bf9\u6bd4\u6fc0\u6d3b\u589e\u52a0\uff08CAA\uff09\u6280\u672f\u5728\u4e0d\u540c\u89c4\u6a21\u7684Llama 2\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u63a2\u7d22\uff0c\u53d1\u73b0\u5176\u5728\u6a21\u578b\u65e9\u671f\u5230\u4e2d\u671f\u5c42\u7ea7\u6548\u679c\u6700\u4f73\uff0c\u4f46\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u6548\u679c\u4f1a\u51cf\u5f31\u3002\u6b64\u5916\uff0c\u8d1f\u9762\u5f15\u5bfc\u6bd4\u6b63\u9762\u5f15\u5bfc\u5177\u6709\u66f4\u663e\u8457\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.12433", "categories": ["cs.CV", "cs.SY", "eess.SY", "I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.12433", "abs": "https://arxiv.org/abs/2507.12433", "authors": ["Fahimeh Orvati Nia", "Hai Lin"], "title": "Traffic-Aware Pedestrian Intention Prediction", "comment": "6 pages, 4 figures. Accepted to the American Control Conference (ACC)\n  2025", "summary": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TA-STGCN\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u706f\u4fe1\u606f\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5bf9\u884c\u4eba\u610f\u56fe\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002", "motivation": "\u51c6\u786e\u7684\u884c\u4eba\u610f\u56fe\u4f30\u8ba1\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u7684\u5b89\u5168\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u672a\u80fd\u5145\u5206\u8003\u8651\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u573a\u666f\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u901a\u611f\u77e5\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08TA-STGCN\uff09\uff0c\u5c06\u4ea4\u901a\u4fe1\u53f7\u706f\u53ca\u5176\u72b6\u6001\uff08\u7ea2\u3001\u9ec4\u3001\u7eff\uff09\u6574\u5408\u5230\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u4e2d\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u72b6\u6001\u548c\u8fb9\u754c\u6846\u5927\u5c0f\u4f5c\u4e3a\u5173\u952e\u7279\u5f81\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "TA-STGCN\u5728PIE\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad84.75%\uff0c\u5728\u63d0\u9ad8\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "TA-STGCN\u6a21\u578b\u5728PIE\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad84.75%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6539\u8fdb\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12407", "abs": "https://arxiv.org/abs/2507.12407", "authors": ["Svetlana Levit", "Marc Toussaint"], "title": "Regrasp Maps for Sequential Manipulation Planning", "comment": null, "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.", "AI": {"tldr": "This paper presents a method to speed up TAMP solvers for regrasp manipulation by using a regrasp map to guide the search and handle mode switches and object placements.", "motivation": "We consider manipulation problems in constrained and cluttered settings, which require several regrasps at unknown locations.", "method": "We propose to inform an optimization-based task and motion planning (TAMP) solver with possible regrasp areas and grasp sequences to speed up the search. Our main idea is to use a state space abstraction, a regrasp map, capturing the combinations of available grasps in different parts of the configuration space, and allowing us to provide the solver with guesses for the mode switches and additional constraints for the object placements.", "result": "Our method is able to provide a robust search method for challenging regrasp manipulation problems.", "conclusion": "We propose a robust search method for challenging regrasp manipulation problems by interleaving the creation of regrasp maps, their adaptation based on failed refinements, and solving TAMP (sub)problems."}}
{"id": "2507.11959", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11959", "abs": "https://arxiv.org/abs/2507.11959", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "LLM\u96be\u4ee5\u90e8\u7f72\uff0c\u56e0\u4e3a\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002POT\u91cf\u5316\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728GPU\u4e0a\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684POT\u91cf\u5316\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u5728\u4f4e\u7cbe\u5ea6\u4e0b\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7684POT\u91cf\u5316\u65b9\u6cd5\u5728CPU\u4e0a\u9ad8\u6548\uff0c\u4f46\u5728GPU\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u539f\u56e0\u662f\u7b26\u53f7\u4f4d\u7ea0\u7f20\u548c\u53cd\u91cf\u5316\u6240\u9700\u7684\u987a\u5e8f\u4f4d\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u7684\u8bad\u7ec3\u540e\u7b97\u6cd5\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u9c81\u68d2\u7684\u8d77\u70b9\u521d\u59cb\u5316\u91cf\u5316\u5c3a\u5ea6\uff1b\u5176\u6b21\uff0c\u4f7f\u7528\u6700\u5c0f\u7684\u6821\u51c6\u96c6\u6765\u4f18\u5316\u8fd9\u4e9b\u5c3a\u5ea6\u3002", "result": "\u57282\u4f4d\u548c3\u4f4d\u683c\u5f0f\u7b49\u4f4e\u7cbe\u5ea6\u4e0b\uff0cPOT\u91cf\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6574\u6570\u91cf\u5316\u65b9\u6cd5\u3002\u5728NVIDIA V100\u4e0a\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e863.67\u500d\uff0c\u5728NVIDIA RTX 4090\u4e0a\u63d0\u9ad8\u4e861.63\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684POT\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8eLLM\u6743\u91cd\uff0c\u5728\u6781\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u66f4\u6709\u6548\u7684\u53cd\u91cf\u5316\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u3002"}}
{"id": "2507.11947", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11947", "abs": "https://arxiv.org/abs/2507.11947", "authors": ["Geon Park", "Seon Bin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "comment": "6 Pages", "summary": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.", "AI": {"tldr": "RaDL framework addresses challenges in text-to-image generation for multiple instances by improving positional accuracy and attribute consideration through relation-aware disentangled learning.", "motivation": "Existing text-to-image (T2I) methods struggle to account for relationship discrepancy and multiple attributes leakage when generating multiple instances within a single image prompt.", "method": "RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt.", "result": "RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances, as demonstrated through extensive evaluations on COCO-Position, COCO-MIG, and DrawBench.", "conclusion": "RaDL is a solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image."}}
{"id": "2507.12024", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12024", "abs": "https://arxiv.org/abs/2507.12024", "authors": ["Linghang Kong", "Fang Zhang", "Jianxin Chen"], "title": "Benchmarking fault-tolerant quantum computing hardware via QLOPS", "comment": null, "summary": "It is widely recognized that quantum computing has profound impacts on\nmultiple fields, including but not limited to cryptography, machine learning,\nmaterials science, etc. To run quantum algorithms, it is essential to develop\nscalable quantum hardware with low noise levels and to design efficient\nfault-tolerant quantum computing (FTQC) schemes. Currently, various FTQC\nschemes have been developed for different hardware platforms. However, a\ncomprehensive framework for the analysis and evaluation of these schemes is\nstill lacking. In this work, we propose Quantum Logical Operations Per Second\n(QLOPS) as a metric for assessing the performance of FTQC schemes on quantum\nhardware platforms. This benchmarking framework will integrate essential\nrelevant factors, e.g., the code rates of quantum error-correcting codes, the\naccuracy, throughput, and latency of the decoder, and reflect the practical\nrequirements of quantum algorithm execution. This framework will enable the\nidentification of bottlenecks in quantum hardware, providing potential\ndirections for their development. Moreover, our results will help establish a\ncomparative framework for evaluating FTQC designs. As this benchmarking\napproach considers practical applications, it may assist in estimating the\nhardware resources needed to implement quantum algorithms and offers\npreliminary insights into potential timelines.", "AI": {"tldr": "\u63d0\u51faQLOPS\u4f5c\u4e3a\u8bc4\u4f30FTQC\u65b9\u6848\u6027\u80fd\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u7801\u7387\u3001\u89e3\u7801\u5668\u51c6\u786e\u6027\u3001\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u7b49\u56e0\u7d20\uff0c\u5e76\u8003\u8651\u4e86\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u548c\u8bc4\u4f30\u4e0d\u540cFTQC\u65b9\u6848\u7684\u5168\u9762\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u91cf\u5b50\u903b\u8f91\u64cd\u4f5c\u6bcf\u79d2\u201d\uff08QLOPS\uff09\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u91cf\u5b50\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684FTQC\u65b9\u6848\u6027\u80fd\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6574\u5408\u4e86\u5173\u952e\u7684\u76f8\u5173\u56e0\u7d20\uff0c\u4f8b\u5982\u91cf\u5b50\u7ea0\u9519\u7801\u7684\u7801\u7387\u3001\u89e3\u7801\u5668\u7684\u51c6\u786e\u6027\u3001\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff0c\u5e76\u53cd\u6620\u4e86\u91cf\u5b50\u7b97\u6cd5\u6267\u884c\u7684\u5b9e\u9645\u9700\u6c42\u3002", "result": "QLOPS\u5ea6\u91cf\u6807\u51c6\u80fd\u591f\u8bc4\u4f30FTQC\u65b9\u6848\u5728\u91cf\u5b50\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8003\u8651\u4e86\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u91cf\u5b50\u786c\u4ef6\u4e2d\u7684\u74f6\u9888\uff0c\u4e3a\u786c\u4ef6\u5f00\u53d1\u63d0\u4f9b\u6f5c\u5728\u65b9\u5411\uff0c\u5e76\u4e3a\u8bc4\u4f30FTQC\u8bbe\u8ba1\u5efa\u7acb\u6bd4\u8f83\u6846\u67b6\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u8003\u8651\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u4f30\u7b97\u5b9e\u65bd\u91cf\u5b50\u7b97\u6cd5\u6240\u9700\u7684\u786c\u4ef6\u8d44\u6e90\uff0c\u5e76\u4e3a\u6f5c\u5728\u7684\u65f6\u95f4\u7ebf\u63d0\u4f9b\u521d\u6b65\u89c1\u89e3\u3002"}}
{"id": "2507.11776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11776", "abs": "https://arxiv.org/abs/2507.11776", "authors": ["Merel Kampere", "Ali Mohammed Mansoor Alsahag"], "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network", "comment": null, "summary": "The Dutch railway network is one of the busiest in the world, with delays\nbeing a prominent concern for the principal passenger railway operator NS. This\nresearch addresses a gap in delay prediction studies within the Dutch railway\nnetwork by employing an XGBoost Classifier with a focus on topological\nfeatures. Current research predominantly emphasizes short-term predictions and\nneglects the broader network-wide patterns essential for mitigating ripple\neffects. This research implements and improves an existing methodology,\noriginally designed to forecast the evolution of the fast-changing US air\nnetwork, to predict delays in the Dutch Railways. By integrating Node\nCentrality Measures and comparing multiple classifiers like RandomForest,\nDecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is\nto predict delayed trajectories. However, the results reveal limited\nperformance, especially in non-simultaneous testing scenarios, suggesting the\nnecessity for more context-specific adaptations. Regardless, this research\ncontributes to the understanding of transportation network evaluation and\nproposes future directions for developing more robust predictive models for\ndelays.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528XGBoost\u548c\u8282\u70b9\u4e2d\u5fc3\u6027\u6765\u9884\u6d4b\u8377\u5170\u94c1\u8def\u5ef6\u8bef\uff0c\u4f46\u6027\u80fd\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u4e2d\uff0c\u9700\u8981\u66f4\u591a\u7279\u5b9a\u60c5\u5883\u7684\u8c03\u6574\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8377\u5170\u94c1\u8def\u7f51\u7edc\u5ef6\u8bef\u9884\u6d4b\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u73b0\u6709\u7814\u7a76\u4fa7\u91cd\u4e8e\u77ed\u671f\u9884\u6d4b\u800c\u5ffd\u89c6\u4e86\u5bf9\u7f51\u7edc\u5927\u8303\u56f4\u6a21\u5f0f\u7684\u8003\u91cf\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9884\u6d4b\u5ef6\u8bef\u8f68\u8ff9\uff0c\u4ee5\u7f13\u89e3\u8fde\u9501\u6548\u5e94\u3002", "method": "\u91c7\u7528XGBoost\u5206\u7c7b\u5668\uff0c\u5e76\u7ed3\u5408\u8282\u70b9\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff0c\u5bf9\u8377\u5170\u94c1\u8def\u7f51\u7edc\u4e2d\u7684\u5ef6\u8bef\u8f68\u8ff9\u8fdb\u884c\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u8fd8\u5bf9\u6bd4\u4e86\u5305\u62ec\u968f\u673a\u68ee\u6797\u3001\u51b3\u7b56\u6811\u3001\u68af\u5ea6\u63d0\u5347\u3001AdaBoost\u548c\u903b\u8f91\u56de\u5f52\u5728\u5185\u7684\u591a\u79cd\u5206\u7c7b\u5668\u3002", "result": "XGBoost\u5206\u7c7b\u5668\u5728\u9884\u6d4b\u8377\u5170\u94c1\u8def\u5ef6\u8bef\u65b9\u9762\u8868\u73b0\u51fa\u6709\u9650\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u573a\u666f\u4e0b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9700\u8981\u9488\u5bf9\u5177\u4f53\u60c5\u5883\u8fdb\u884c\u66f4\u591a\u7684\u8c03\u6574\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u573a\u666f\u4e0b\u9884\u6d4b\u8377\u5170\u94c1\u8def\u5ef6\u8bef\u7684\u6027\u80fd\u6709\u9650\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u884c\u66f4\u7b26\u5408\u7279\u5b9a\u60c5\u5883\u7684\u8c03\u6574\u3002"}}
{"id": "2507.12431", "categories": ["cs.RO", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.12431", "abs": "https://arxiv.org/abs/2507.12431", "authors": ["Connor Burgess", "Kyle Douin", "Amir Kordijazi"], "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement", "comment": "14 pages, 4 figures", "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform.", "AI": {"tldr": "ACAT\u662f\u4e00\u79cd\u5168\u96c6\u6210\u7684\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\uff0c\u7528\u4e8e\u81ea\u52a8\u53163D\u6253\u5370\u6750\u6599\u7684\u8868\u9762\u6da6\u6e7f\u6027\u6d4b\u91cf\u3002", "motivation": "ACAT\u662f\u4e3a\u4e86\u89e3\u51b3\u624b\u52a8\u63a5\u89e6\u89d2\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u800c\u8bbe\u8ba1\u7684\uff0c\u65e8\u5728\u5b9e\u73b03D\u6253\u5370\u6750\u6599\u8868\u9762\u6da6\u6e7f\u6027\u6d4b\u91cf\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u7684\u7cbe\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "ACAT\u7cfb\u7edf\u7ed3\u5408\u4e86\u53ef\u7f16\u7a0b\u673a\u5668\u4eba\u6280\u672f\u3001\u7cbe\u786e\u7684\u6db2\u4f53\u5206\u914d\u4ee5\u53ca\u6a21\u5757\u5316\u7684\u8f6f\u4ef6-\u786c\u4ef6\u67b6\u6784\uff0c\u5305\u62ec\u7535\u6c14\u7cfb\u7edf\u3001\u57fa\u4e8eRaspberry Pi\u548cPython\u7684\u8f6f\u4ef6\u63a7\u5236\u7cfb\u7edf\u4ee5\u53ca\u5305\u542b3\u8f74\u7b1b\u5361\u5c14\u673a\u5668\u4eba\u3001\u6c14\u52a8\u9a71\u52a8\u548c\u7cbe\u5bc6\u6db2\u4f53\u5206\u914d\u5668\u7684\u673a\u68b0\u7cfb\u7edf\u3002", "result": "ACAT\u80fd\u591f\u5b9e\u73b0\u9ad8\u901a\u91cf\u7684\u81ea\u52a8\u5316\u8868\u9762\u8868\u5f81\uff0c\u5e76\u4e3a\u672a\u6765\u96c6\u6210\u5230\u667a\u80fd\u5236\u9020\u548c\u6750\u6599\u53d1\u73b0\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u5e73\u53f0\u3002", "conclusion": "ACAT\u662f\u4e00\u4e2a\u9ad8\u901a\u91cf\u3001\u81ea\u52a8\u5316\u7684\u8868\u9762\u8868\u5f81\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u96c6\u6210\u5230\u667a\u80fd\u5236\u9020\u548c\u6750\u6599\u53d1\u73b0\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.11966", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.11966", "abs": "https://arxiv.org/abs/2507.11966", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u6bd2\u6027\u5185\u5bb9\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u548c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u6a21\u578b\u4f18\u5316\uff0c\u4ee5\u5728\u65b0\u52a0\u5761\u5f0f\u82f1\u8bed\u7ffb\u8bd1\u4e2d\u4fdd\u7559\u6bd2\u6027\u548c\u7ec6\u5fae\u5dee\u522b\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u4ea4\u6d41\u8d8a\u6765\u8d8a\u591a\u5730\u5305\u542b\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u548c\u672c\u5730\u65b9\u8a00\uff0c\u6807\u51c6\u7684\u7ffb\u8bd1\u7cfb\u7edf\u5f80\u5f80\u65e0\u6cd5\u4fdd\u7559\u672c\u5730\u4fda\u8bed\u3001\u6df7\u5408\u8bed\u548c\u5305\u542b\u6709\u5bb3\u8a00\u8bba\u7684\u6587\u5316\u6807\u8bb0\u3002\u7531\u4e8e\u7f3a\u4e4f\u5e73\u884c\u6570\u636e\u548c\u7528\u4e8e\u51c0\u5316\u5192\u72af\u6027\u8868\u8fbe\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e4b\u95f4\u7ffb\u8bd1\u6709\u6bd2\u5185\u5bb9\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u3001\u4e24\u9636\u6bb5\u7684\u6bd2\u6027\u4fdd\u7559\u7ffb\u8bd1\u6846\u67b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u7684\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\uff0c\u8fed\u4ee3\u5730\u6574\u7406\u548c\u6392\u5e8f\u6807\u6ce8\u8005\u9009\u62e9\u7684\u65b0\u52a0\u5761\u5f0f\u82f1\u8bed-\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\uff0c\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u4fda\u8bed\u3001\u8bed\u6c14\u548c\u6bd2\u6027\u3002\u5176\u6b21\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u76f4\u63a5\u7ffb\u8bd1\u548c\u56de\u8bd1\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u5bf9\u51e0\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u4f18\u5316\u6a21\u578b-\u63d0\u793a\u5bf9\u3002", "result": "\u5b9a\u91cf\u7684\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6211\u4eec\u6d41\u7a0b\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u652f\u6301\u6587\u5316\u654f\u611f\u7684\u5ba1\u6838\u548c\u4f4e\u8d44\u6e90\u80cc\u666f\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u591a\u6587\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u505a\u51fa\u4e86\u8d21\u732e\u3002\u901a\u8fc7\u5c06\u65b0\u52a0\u5761\u5f0f\u82f1\u8bed\u5b9a\u4f4d\u4e3a\u5305\u5bb9\u6027\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u8bd5\u9a8c\u573a\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u5728\u5185\u5bb9\u5ba1\u6838\u548c\u533a\u57df\u5e73\u53f0\u6cbb\u7406\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u4fdd\u7559\u793e\u4f1a\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11955", "abs": "https://arxiv.org/abs/2507.11955", "authors": ["Yuhang Zhang", "Zhengyu Zhang", "Muxin Liao", "Shishun Tian", "Wenbin Zou", "Lu Zhang", "Chen Xu"], "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "comment": "This paper was accepted by IEEE Transactions on Intelligent\n  Transportation Systems", "summary": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12030", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12030", "abs": "https://arxiv.org/abs/2507.12030", "authors": ["Alexander Pirker", "Belen Munoz", "Wolfgang D\u00fcr"], "title": "A resource-centric, task-based approach to quantum network control", "comment": "14 pages, 10 figures", "summary": "Quantum networks exhibit fundamental differences from their classical\ncounterparts. These differences necessitate novel principles when organizing,\nmanaging, and operating them. Here we propose an unconventional approach to\norganize and manage the operations of quantum network devices. Instead of a\nhierarchical scheme using layers, like in classical networks and present\nquantum network stack models, we propose a resource-centric task-based scheme.\nIn this scheme, quantum applications pose objectives, initiated by a node, to a\nquantum network, such as sharing an entangled state or sending a qubit along a\npath. The quantum network node initiating the objective consequently derives a\ndistributed workflow, referred to as saga, comprising numerous tasks operating\non resources, which completes the objective. We identify three different kinds\nof resources with their own and independent topology, namely classical\nmessaging, quantum channels and entanglement. Sagas can either be centrally\norchestrated or performed in choreography by the network nodes. The tasks of a\nsaga originate from and operate on resources of the network, such as quantum\nchannels or entanglement, and they not only comprise operations and\nmeasurements, but potentially also include other tasks or even entire\nprotocols, such as sending a qubit, distributing entanglement or performing\nentanglement purification steps.", "AI": {"tldr": "\u91cf\u5b50\u7f51\u7edc\u9700\u8981\u65b0\u7684\u7ba1\u7406\u65b9\u5f0f\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u4e2d\u5fc3\u5316\u7684\u4efb\u52a1\u9a71\u52a8\u65b9\u6848\uff0c\u5c06\u76ee\u6807\u5206\u89e3\u4e3a\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\uff08saga\uff09\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u4f20\u7edf\u7684\u5c42\u6b21\u5316\u5206\u5c42\u65b9\u6848\u3002", "motivation": "\u91cf\u5b50\u7f51\u7edc\u4e0e\u7ecf\u5178\u7f51\u7edc\u5b58\u5728\u6839\u672c\u6027\u5dee\u5f02\uff0c\u9700\u8981\u65b0\u7684\u7ec4\u7ec7\u3001\u7ba1\u7406\u548c\u64cd\u4f5c\u539f\u5219\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u4e2d\u5fc3\u5316\u7684\u4efb\u52a1\u9a71\u52a8\u65b9\u6848\uff0c\u5c06\u91cf\u5b50\u5e94\u7528\u7684\u76ee\u6807\u5206\u89e3\u4e3a\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\uff08saga\uff09\uff0c\u5176\u4e2d\u5305\u542b\u5728\u91cf\u5b50\u7f51\u7edc\u8d44\u6e90\uff08\u5982\u7ecf\u5178\u6d88\u606f\u3001\u91cf\u5b50\u4fe1\u9053\u548c\u7ea0\u7f20\uff09\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6848\u5c06\u91cf\u5b50\u5e94\u7528\u7684\u76ee\u6807\u5206\u89e3\u4e3a\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\uff08saga\uff09\uff0c\u6bcf\u4e2a\u5de5\u4f5c\u6d41\u5305\u542b\u5728\u91cf\u5b50\u7f51\u7edc\u8d44\u6e90\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u53ef\u4ee5\u5305\u62ec\u64cd\u4f5c\u3001\u6d4b\u91cf\uff0c\u751a\u81f3\u5176\u4ed6\u534f\u8bae\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u4e2d\u5fc3\u5316\u7684\u4efb\u52a1\u9a71\u52a8\u65b9\u6848\uff0c\u7528\u4e8e\u7ec4\u7ec7\u548c\u7ba1\u7406\u91cf\u5b50\u7f51\u7edc\u8bbe\u5907\u7684\u64cd\u4f5c\uff0c\u8fd9\u4e0e\u4f20\u7edf\u7684\u5c42\u6b21\u5316\u5206\u5c42\u65b9\u6848\u4e0d\u540c\u3002"}}
{"id": "2507.11789", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11789", "abs": "https://arxiv.org/abs/2507.11789", "authors": ["Alessandro Palma", "Sergei Rybakov", "Leon Hetzel", "Stephan G\u00fcnnemann", "Fabian J. Theis"], "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "comment": "31 pages, 14 figures", "summary": "Latent space interpolations are a powerful tool for navigating deep\ngenerative models in applied settings. An example is single-cell RNA\nsequencing, where existing methods model cellular state transitions as latent\nspace interpolations with variational autoencoders, often assuming linear\nshifts and Euclidean geometry. However, unless explicitly enforced, linear\ninterpolations in the latent space may not correspond to geodesic paths on the\ndata manifold, limiting methods that assume Euclidean geometry in the data\nrepresentations. We introduce FlatVI, a novel training framework that\nregularises the latent manifold of discrete-likelihood variational autoencoders\ntowards Euclidean geometry, specifically tailored for modelling single-cell\ncount data. By encouraging straight lines in the latent space to approximate\ngeodesic interpolations on the decoded single-cell manifold, FlatVI enhances\ncompatibility with downstream approaches that assume Euclidean latent geometry.\nExperiments on synthetic data support the theoretical soundness of our\napproach, while applications to time-resolved single-cell RNA sequencing data\ndemonstrate improved trajectory reconstruction and manifold interpolation.", "AI": {"tldr": "FlatVI \u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6f5c\u5728\u6d41\u5f62\u6b63\u5219\u5316\u4e3a\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u6539\u8fdb\u4e86\u5355\u7ec6\u80de RNA \u6d4b\u5e8f\u6570\u636e\u7684\u8f68\u8ff9\u91cd\u5efa\u548c\u6d41\u5f62\u63d2\u503c\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u7ec6\u80de RNA \u6d4b\u5e8f\u65b9\u6cd5\u5728\u6a21\u62df\u7ec6\u80de\u72b6\u6001\u8f6c\u6362\u65f6\uff0c\u901a\u5e38\u5047\u8bbe\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u5e73\u79fb\u548c\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4f46\u8fd9\u53ef\u80fd\u4e0e\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u6d4b\u5730\u7ebf\u8def\u5f84\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u4e0b\u6e38\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a FlatVI \u7684\u65b0\u9896\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u79bb\u6563\u4f3c\u7136\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u6d41\u5f62\u6b63\u5219\u5316\u4e3a\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u652f\u6301\u4e86\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u5065\u5168\u6027\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u89e3\u6790\u5355\u7ec6\u80de RNA \u6d4b\u5e8f\u6570\u636e\u4e0a\u7684\u5e94\u7528\u8bc1\u660e\u4e86\u5176\u5728\u8f68\u8ff9\u91cd\u5efa\u548c\u6d41\u5f62\u63d2\u503c\u65b9\u9762\u7684\u6539\u8fdb\u3002", "conclusion": "FlatVI \u6846\u67b6\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6d41\u5f62\u4ee5\u5b9e\u73b0\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u5355\u7ec6\u80de RNA \u6d4b\u5e8f\u6570\u636e\u7684\u65f6\u95f4\u89e3\u6790\u8f68\u8ff9\u91cd\u5efa\u548c\u6d41\u5f62\u63d2\u503c\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12440", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "AI": {"tldr": "EgoVLA\u5229\u7528\u5927\u89c4\u6a21\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u8fdb\u884c\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u548c\u91cd\u5b9a\u5411\u6280\u672f\u8f6c\u6362\u52a8\u4f5c\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u7684\u89c4\u6a21\u9650\u5236\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u89c4\u6a21\u3001\u4e30\u5bcc\u573a\u666f\u548c\u4efb\u52a1\u7684\u4eba\u7c7b\u89c6\u9891\u6765\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u4ee5\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u3002", "method": "EgoVLA\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u4eba\u7c7b\u624b\u8155\u548c\u624b\u90e8\u52a8\u4f5c\uff0c\u5e76\u5229\u7528\u9006\u8fd0\u52a8\u5b66\u548c\u91cd\u5b9a\u5411\u6280\u672f\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u8be5\u6a21\u578b\u5728 Isaac Humanoid Manipulation Benchmark \u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u8bc4\u4f30\uff0c\u5e76\u5728\u5404\u79cd\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "result": "EgoVLA\u5728Isaac Humanoid Manipulation Benchmark\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4f7f\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u4eba\u7c7b\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0cEgoVLA\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u8bc1\u660e\u4e86\u4eba\u7c7b\u6570\u636e\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11972", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.11972", "abs": "https://arxiv.org/abs/2507.11972", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u56fe\u8868\u6587\u672c\u8868\u793a\uff0c\u53d1\u73b0LLM\u5728\u8bed\u8a00\u7406\u89e3\u7684\u62d3\u6251\u7ed3\u6784\u5c42\u9762\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e3a\u7406\u89e3\u4eba\u673a\u534f\u540c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7406\u89e3\u8bed\u8a00\u7684\u5dee\u5f02\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u63a8\u7406\u3001\u60c5\u611f\u89e3\u91ca\u548c\u4fe1\u606f\u68c0\u7d22\u7b49\u529f\u80fd\u6027\u4efb\u52a1\u3002\u5148\u524d\u7684\u7814\u7a76\u4f7f\u7528LLM\u548c\u4eba\u7c7b\u751f\u7269\u6807\u8bb0\u7814\u7a76\u9605\u8bfb\u7406\u89e3\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e0e\u63a8\u7406\u76ee\u6807\u76f8\u5173\u6027\u9ad8\u4f4e\u8bcd\u8bed\u7684\u751f\u7269\u6807\u8bb0\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u4f46\u4ec5\u5173\u6ce8\u5355\u4e2a\u5355\u8bcd\u9650\u5236\u4e86\u7406\u89e3\u7684\u6df1\u5ea6\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u5c06\u9605\u8bfb\u6750\u6599\u4e2d\u7684\u5355\u8bcd\u5206\u7ec4\u4e3a\u8282\u70b9\u548c\u8fb9\uff0c\u5f62\u6210\u57fa\u4e8e\u8bed\u4e49\u548c\u95ee\u9898\u5bfc\u5411\u63d0\u793a\u7684\u56fe\u8868\u6587\u672c\u8868\u793a\u3002\u7136\u540e\u6bd4\u8f83\u4e86\u91cd\u8981\u8282\u70b9\u548c\u8fb9\u4e0a\u7684\u773c\u52a8\u5206\u5e03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u5c42\u9762\uff0c\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u5728\u5148\u524d\u7684\u53d1\u73b0\u4e4b\u4e0a\uff0c\u5e76\u4e3a\u6709\u6548\u7684\u4eba\u673a\u534f\u540c\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "LLM\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u5c42\u9762\uff0c\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.11967", "categories": ["cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11967", "abs": "https://arxiv.org/abs/2507.11967", "authors": ["Yuchi Ishikawa", "Shota Nakada", "Hokuto Munakata", "Kazuhiro Saito", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "comment": "Interspeech 2025", "summary": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "AI": {"tldr": "\u63d0\u51faLG-CAV-MAE\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u6587\u672c\u7f16\u7801\u5668\u548c\u81ea\u52a8\u751f\u6210\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891-\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u5728\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u97f3\u9891-\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8bed\u8a00\u5f15\u5bfc\u5bf9\u6bd4\u97f3\u9891-\u89c6\u89c9\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08LG-CAV-MAE\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u9884\u8bad\u7ec3\u7684\u6587\u672c\u7f16\u7801\u5668\u96c6\u6210\u5230\u5bf9\u6bd4\u97f3\u9891-\u89c6\u89c9\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u4e2d\uff0c\u5b9e\u73b0\u4e86\u8de8\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u5b66\u4e60\u3002\u4e3a\u4e86\u8bad\u7ec3LG-CAV-MAE\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u751f\u6210\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u4e09\u5143\u7ec4\u7684\u81ea\u52a8\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u751f\u6210\u5e27\u7ea7\u5b57\u5e55\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eCLAP\u7684\u8fc7\u6ee4\u6765\u786e\u4fdd\u97f3\u9891\u548c\u5b57\u5e55\u4e4b\u95f4\u7684\u5f3a\u5bf9\u9f50\u3002", "result": "LG-CAV-MAE\u5728\u97f3\u9891-\u89c6\u89c9\u68c0\u7d22\u4efb\u52a1\u548c\u97f3\u9891-\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u97f3\u9891-\u89c6\u89c9\u68c0\u7d22\u548c\u97f3\u9891-\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u68c0\u7d22\u4efb\u52a1\u7684recall@10\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e865.6%\u548c3.2%\u3002"}}
{"id": "2507.12056", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.12056", "abs": "https://arxiv.org/abs/2507.12056", "authors": ["Giorgio Anfuso", "Giulia Piccitto", "Vittorio Romano", "Elisabetta Paladino", "Giuseppe Falci"], "title": "Selective decoupling in multi-level quantum systems by the SU(2) sign anomaly", "comment": "6 pages, 1 figure", "summary": "We investigate dynamical decoupling operated by $2\\pi$-pulses in a two-level\nsubspaces of a multilevel system showing that it may leads to selective\ndecoupling. This provides a flexible strategy for decoupling transitions in a\nquantum network, when control to directly address them is not available which\ncan be use to control internode interaction or actively suppress decoherence", "AI": {"tldr": "Selective decoupling in quantum networks is achieved using $2\text{\textpi}$-pulses, enabling control over interactions and decoherence when direct control is limited.", "motivation": "The motivation is to provide a flexible strategy for decoupling transitions in a quantum network, especially when direct control over these transitions is not available.", "method": "The study investigates dynamical decoupling using $2\text{\textpi}$-pulses within a two-level subspace of a multilevel system.", "result": "The research shows that selective decoupling can be achieved through this method, which can be used to control internode interactions or actively suppress decoherence.", "conclusion": "The paper concludes that dynamical decoupling operated by $2\text{\textpi}$-pulses in a two-level subspace of a multilevel system can lead to selective decoupling, offering a flexible strategy for controlling internode interactions or suppressing decoherence in quantum networks when direct control is unavailable."}}
{"id": "2507.11807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11807", "abs": "https://arxiv.org/abs/2507.11807", "authors": ["Ruofan Hu", "Dongyu Zhang", "Huayi Zhang", "Elke Rundensteiner"], "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "comment": "KDD 2025, 12 pages, 7 figures", "summary": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5143\u5b66\u4e60\u65b9\u6cd5CLID-MU\uff0c\u901a\u8fc7\u8de8\u5c42\u4fe1\u606f\u4e00\u81f4\u6027\u6307\u5bfc\u8bad\u7ec3\uff0c\u5728\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u6570\u636e\u65f6\u9700\u8981\u5e72\u51c0\u7684\u6807\u7b7e\u5143\u6570\u636e\u96c6\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u83b7\u5f97\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "CLID-MU\u65b9\u6cd5\u5229\u7528\u4e86\u6570\u636e\u672c\u8eab\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u901a\u8fc7\u8861\u91cf\u6a21\u578b\u6700\u540e\u9690\u85cf\u5c42\u4e0e\u6700\u7ec8\u5c42\u4e4b\u95f4\u6570\u636e\u7ed3\u6784\u7684\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u6307\u5bfc\u8bad\u7ec3\uff0c\u4ee5\u6b64\u89e3\u51b3\u4e86\u4f20\u7edf\u5143\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "result": "CLID-MU\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u4e0b\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5143\u5b66\u4e60\u65b9\u6cd5CLID-MU\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5904\u7406\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u6570\u636e\u65f6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.11968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11968", "abs": "https://arxiv.org/abs/2507.11968", "authors": ["Sahid Hossain Mustakim", "S M Jishanul Islam", "Ummay Maria Muna", "Montasir Chowdhury", "Mohammed Jawwadul Islam", "Sadia Ahmmed", "Tashfia Sikder", "Syed Tasdid Azam Dhrubo", "Swakkhar Shatabda"], "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "comment": "Accepted as long paper, SVU Workshop at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7SVMA\u6570\u636e\u96c6\u548cChimeraBreak\u653b\u51fb\u7b56\u7565\uff0c\u53d1\u73b0\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u4e09\u6a21\u6001\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u5168\u9762\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u65b9\u9762\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u653b\u51fb\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u7ec4\u5408\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e09\u6a21\u6001\u5b89\u5168\u6027\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b\u4eba\u7c7b\u6307\u5bfc\u7684\u5408\u6210\u5bf9\u6297\u6027\u653b\u51fb\u7684\u77ed\u89c6\u9891\u591a\u6a21\u6001\u5bf9\u6297\uff08SVMA\uff09\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u6311\u6218\u89c6\u89c9\u3001\u542c\u89c9\u548c\u8bed\u4e49\u63a8\u7406\u901a\u8def\u7684\u4e09\u6a21\u6001\u653b\u51fb\u7b56\u7565ChimeraBreak\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9ChimeraBreak\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u663e\u8457\u7684\u8106\u5f31\u6027\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u5e76\u66b4\u9732\u51fa\u6a21\u578b\u5728\u8bef\u5206\u7c7b\u826f\u6027\u6216\u8fdd\u89c4\u5185\u5bb9\u65b9\u9762\u7684\u504f\u89c1\u3002\u901a\u8fc7LLM-as-a-judge\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684SVMA\u6570\u636e\u96c6\u548cChimeraBreak\u653b\u51fb\u7b56\u7565\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u65b9\u9762\u5b58\u5728\u7684\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5b89\u5168\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.11818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11818", "abs": "https://arxiv.org/abs/2507.11818", "authors": ["Andrei Rekesh", "Miruna Cretu", "Dmytro Shevchuk", "Vignesh Ram Somnath", "Pietro Li\u00f2", "Robert A. Batey", "Mike Tyers", "Micha\u0142 Koziarski", "Cheng-Hao Liu"], "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "comment": null, "summary": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "AI": {"tldr": "SynCoGen \u662f\u4e00\u4e2a\u7528\u4e8e\u53ef\u5408\u6210 3D \u5206\u5b50\u751f\u6210\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff0c\u5e76\u4f7f\u7528\u5305\u542b 600K \u780c\u5757\u56fe\u548c 3.3M \u6784\u8c61\u7684 SynSpace \u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728 2D \u5206\u5b50\u56fe\u8868\u793a\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86 SynCoGen\uff0c\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u51e0\u4f55\u6761\u4ef6\u751f\u6210\u7684\u6846\u67b6\u3002", "method": "SynCoGen \u6846\u67b6\u7ed3\u5408\u4e86\u540c\u65f6\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\uff0c\u7528\u4e8e\u53ef\u5408\u6210\u7684 3D \u5206\u5b50\u751f\u6210\uff0c\u5e76\u4ece\u5206\u5b50\u780c\u5757\u3001\u5316\u5b66\u53cd\u5e94\u548c\u539f\u5b50\u5750\u6807\u7684\u8054\u5408\u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\u3002SynSpace \u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7 600K \u4e2a\u5408\u6210\u611f\u77e5\u780c\u5757\u56fe\u548c 3.3M \u4e2a\u6784\u8c61\u3002", "result": "SynCoGen \u5728\u65e0\u6761\u4ef6\u5c0f\u5206\u5b50\u56fe\u548c\u6784\u8c61\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u96f6\u6837\u672c\u5206\u5b50\u8fde\u63a5\u5668\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "SynCoGen \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u540c\u65f6\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u53ef\u5408\u6210\u7684 3D \u5206\u5b50\u751f\u6210\uff0c\u5e76\u5728\u65e0\u6761\u4ef6\u5c0f\u5206\u5b50\u56fe\u548c\u6784\u8c61\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u836f\u7269\u53d1\u73b0\u7684\u86cb\u767d\u8d28\u914d\u4f53\u751f\u6210\u65b9\u9762\uff0c\u5728\u96f6\u6837\u672c\u5206\u5b50\u8fde\u63a5\u5668\u8bbe\u8ba1\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.11981", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Ansch\u00fctz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "LLM \u5bf9\u591a\u4e49\u8bcd\u7684\u7b80\u5316\u5b9a\u4e49\u4f1a\u4e22\u5931\u4fe1\u606f\u5e76\u4ea7\u751f\u8bef\u5bfc\u3002\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5316\u4f1a\u964d\u4f4e\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u800c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9 Llama 3.1 8B \u8fdb\u884c\u5fae\u8c03\u53ef\u63d0\u9ad8\u5176\u540c\u4e49\u8bcd\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u63d0\u4f9b\u51c6\u786e\u7684\u5355\u8bcd\u5b9a\u4e49\u548c\u89e3\u91ca\uff0c\u4f46\u5bf9\u4e8e\u513f\u7ae5\u6216\u8bed\u8a00\u5b66\u4e60\u8005\u7b49\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\uff0c\u5b9a\u4e49\u7684\u8303\u56f4\u4e5f\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u5bf9\u4e8e\u540c\u4e49\u8bcd\u5c24\u5176\u91cd\u8981\uff0c\u56e0\u4e3a\u8fc7\u5ea6\u7b80\u5316\u53ef\u80fd\u4f1a\u901a\u8fc7\u7701\u7565\u5173\u952e\u8bcd\u4e49\u6765\u4e22\u5931\u4fe1\u606f\uff0c\u4ece\u800c\u53ef\u80fd\u8bef\u5bfc\u7528\u6237\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u7684\u65b0\u578b\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u901a\u8fc7 LLM-as-Judge \u548c\u4eba\u5de5\u6807\u6ce8\u6d4b\u8bd5\u4e86 DeepSeek v3\u3001Llama 4 Maverick\u3001Qwen3-30B A3B\u3001GPT-4o mini \u548c Llama 3.1 8B\u3002", "result": "\u7b80\u5316\u4f1a\u4e25\u91cd\u964d\u4f4e\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u5ffd\u7565\u8bcd\u8bed\u7684\u591a\u4e49\u6027\uff0c\u589e\u52a0\u8bef\u89e3\u7684\u98ce\u9669\u3002\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9 Llama 3.1 8B \u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6240\u6709\u63d0\u793a\u7c7b\u578b\u7684\u540c\u4e49\u8bcd\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u7b80\u5316\u4f1a\u4e25\u91cd\u5f71\u54cd\u540c\u4e49\u8bcd\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u5ffd\u7565\u591a\u4e49\u6027\uff0c\u589e\u52a0\u8bef\u89e3\u7684\u98ce\u9669\u3002\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03 Llama 3.1 8B \u53ef\u663e\u8457\u63d0\u9ad8\u6240\u6709\u7c7b\u578b\u63d0\u793a\u4e0b\u7684\u540c\u4e49\u8bcd\u54cd\u5e94\u8d28\u91cf\u3002\u8fd9\u8868\u660e\u5728\u6559\u80b2\u578b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u9700\u8981\u5e73\u8861\u7b80\u6d01\u6027\u548c\u5b8c\u6574\u6027\uff0c\u4ee5\u786e\u4fdd\u4e3a\u6240\u6709\u5b66\u4e60\u8005\u63d0\u4f9b\u53ef\u9760\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9a\u4e49\u3002"}}
{"id": "2507.11969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11969", "abs": "https://arxiv.org/abs/2507.11969", "authors": ["Zhaohong Huang", "Yuxin Zhang", "Jingjing Xie", "Fei Chao", "Rongrong Ji"], "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models", "comment": null, "summary": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.", "AI": {"tldr": "GS-Bias\u662f\u4e00\u79cd\u9ad8\u6548\u7684TTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40\u548c\u7a7a\u95f4\u504f\u7f6e\u6765\u63d0\u9ad8VLMs\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u672a\u80fd\u53d6\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u5e73\u8861\uff0c\u539f\u56e0\u5728\u4e8e\u8c03\u6574\u6587\u672c\u63d0\u793a\u7684\u5f00\u9500\u8fc7\u5927\uff0c\u6216\u624b\u5de5\u5236\u4f5c\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\u5e26\u6765\u7684\u6548\u76ca\u4e0d\u7a33\u5b9a\u3002", "method": "GS-Bias\u901a\u8fc7\u5b66\u4e60\u4e00\u81f4\u6027\u6765\u6355\u83b7\u6d4b\u8bd5\u56fe\u50cf\u7684\u5168\u5c40\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u5b66\u4e60\u56fe\u50cf\u7a7a\u95f4\u89c6\u89c9\u8868\u793a\u4e2d\u533a\u57df\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u8fd9\u4e24\u79cd\u504f\u7f6e\u76f4\u63a5\u6dfb\u52a0\u5230\u9884\u8bad\u7ec3VLMs\u7684logits\u8f93\u51fa\u4e0a\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9VLM\u8fdb\u884c\u5b8c\u5168\u53cd\u5411\u4f20\u64ad\u3002", "result": "GS-Bias\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u6bd4TPT\u63d0\u9ad8\u4e862.23%\uff0c\u5728\u9886\u57df\u6cdb\u5316\u65b9\u9762\u63d0\u9ad8\u4e862.72%\uff0c\u540c\u65f6\u5728ImageNet\u4e0a\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u4ec5\u4e3aTPT\u76846.5%\u3002", "conclusion": "GS-Bias\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40\u504f\u7f6e\u548c\u7a7a\u95f4\u504f\u7f6e\uff0c\u76f4\u63a5\u52a0\u5230\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8f93\u51falogits\u4e0a\uff0c\u907f\u514d\u4e86\u5bf9\u6574\u4e2aVLM\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u3002GS-Bias\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u6bd4TPT\u63d0\u9ad8\u4e862.23%\uff0c\u5728\u9886\u57df\u6cdb\u5316\u65b9\u9762\u63d0\u9ad8\u4e862.72%\uff0c\u540c\u65f6\u5728ImageNet\u4e0a\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u4ec5\u4e3aTPT\u76846.5%\u3002"}}
{"id": "2507.12066", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.12066", "abs": "https://arxiv.org/abs/2507.12066", "authors": ["Yue-Wei Song", "Ming-Yuan Gao", "Zhi-Cheng Guo", "Zheng-He Zhou", "Yin-Hai Li", "Guang-Can Guo", "Zhi-Yuan Zhou", "Bao-Sen Shi"], "title": "Generation of Near-ideal Indistinguishable Two-Photon State by Incoherent Light", "comment": null, "summary": "High-quality quantum states lie at the heart of advanced quantum information\nprocessing. The degree of photon indistinguishability is critical for\napplications from photonic quantum computation to precision metrology. The\ntwo-photon Hong-Ou-Mandel (HOM) interference effect provides a rigorous\nquantification method, with its visibility serving as the ultimate benchmark\nfor source quality. Generally, the coherent pumping is widely regarded as\nindispensable for the preparation of quantum sources. As a result, incoherent\nlight sources have seen limited applications in the current quantum\ntechnologies. In this work, we generate an indistinguishable two-photon state\nby incoherent light generated by frequency doubling of Amplified Spontaneous\nEmission light. The theoretical analysis indicates that phase randomization of\nthe pumping does not affect the coincidence visibility in two-photon intensity\ninterference. Moreover, temporal incoherence further enhances the symmetry of\nthe generated spectrum in second-harmonic generation. In the experiment, the\nincoherently pumped photon sources exhibit a heralding efficiency of\napproximately 60\\% and a coincidence-to-accidental ratio exceeding 15000. The\nobserved HOM interference fringes show the visibility of 99.1\\% without any\nspectrum filtering, confirming the near-ideal indistinguishability of the\nphotons. Our study reveals the role of temporal coherence in second-order\nnonlinear interactions, it provide a potential approach to use an easily\naccessible incoherent light for engineering high-quality quantum sources.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u4e0d\u76f8\u5e72\u5149\u6e90\uff08\u901a\u8fc7\u653e\u5927\u81ea\u53d1\u8f90\u5c04\u5149\u7684\u9891\u7387\u500d\u589e\uff09\u53ef\u4ee5\u5236\u5907\u51fa\u9ad8\u5ea6\u4e0d\u53ef\u533a\u5206\u7684\u5149\u5b50\u91cf\u5b50\u6001\uff0c\u5176Hong-Ou-Mandel\u5e72\u6d89\u53ef\u89c1\u5ea6\u9ad8\u8fbe99.1%\uff0c\u8fd9\u6311\u6218\u4e86\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u76f8\u5e72\u6cf5\u6d66\u662f\u5fc5\u9700\u7684\u89c2\u70b9\uff0c\u5e76\u4e3a\u5229\u7528\u6613\u4e8e\u83b7\u5f97\u7684\u4e0d\u76f8\u5e72\u5149\u6e90\u5de5\u7a0b\u5316\u9ad8\u8d28\u91cf\u91cf\u5b50\u6e90\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6cd5\u3002", "motivation": "\u5149\u5b50\u4e0d\u53ef\u533a\u5206\u6027\u662f\u5b9e\u73b0\u4ece\u5149\u5b50\u91cf\u5b50\u8ba1\u7b97\u5230\u7cbe\u5bc6\u8ba1\u91cf\u7b49\u5148\u8fdb\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5e94\u7528\u7684\u5173\u952e\u3002Hong-Ou-Mandel\uff08HOM\uff09\u5e72\u6d89\u6548\u5e94\u662f\u91cf\u5316\u5149\u5b50\u4e0d\u53ef\u533a\u5206\u6027\u7684\u6709\u529b\u65b9\u6cd5\uff0c\u5176\u53ef\u89c1\u5ea6\u662f\u8bc4\u4ef7\u5149\u6e90\u8d28\u91cf\u7684\u7ec8\u6781\u6807\u51c6\u3002\u7136\u800c\uff0c\u76ee\u524d\u91cf\u5b50\u6280\u672f\u666e\u904d\u8ba4\u4e3a\u76f8\u5e72\u6cf5\u6d66\u662f\u5236\u5907\u91cf\u5b50\u5149\u6e90\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u8fd9\u9650\u5236\u4e86\u4e0d\u76f8\u5e72\u5149\u6e90\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u4e0d\u76f8\u5e72\u5149\u6e90\u5236\u5907\u9ad8\u8d28\u91cf\u91cf\u5b50\u6001\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u653e\u5927\u81ea\u53d1\u8f90\u5c04\u5149\u8fdb\u884c\u9891\u7387\u500d\u589e\u6765\u4ea7\u751f\u4e0d\u76f8\u5e72\u5149\uff0c\u5e76\u5229\u7528\u5176\u5236\u5907\u4e86\u53ef\u533a\u5206\u7684\u4e8c\u5149\u5b50\u91cf\u5b50\u6001\u3002\u5bf9\u7406\u8bba\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u53d1\u73b0\u5728\u4e8c\u5149\u5b50\u5f3a\u5ea6\u5e72\u6d89\u4e2d\uff0c\u6cf5\u6d66\u76f8\u4f4d\u968f\u673a\u5316\u4e0d\u5f71\u54cd\u7b26\u5408\u53ef\u89c1\u5ea6\uff0c\u800c\u65f6\u95f4\u76f8\u5e72\u6027\u5219\u589e\u5f3a\u4e86\u4e8c\u6b21\u8c10\u6ce2\u4ea7\u751f\u4e2d\u5149\u8c31\u7684\u5bf9\u79f0\u6027\u3002\u5b9e\u9a8c\u4e2d\uff0c\u6d4b\u91cf\u4e86\u7531\u4e0d\u76f8\u5e72\u5149\u6e90\u4ea7\u751f\u7684\u4e8c\u5149\u5b50\u6001\u7684\u6d8c\u73b0\u6548\u7387\uff08\u7ea660%\uff09\u548c\u7b26\u5408-\u610f\u5916\u6bd4\uff08\u8d85\u8fc715000\uff09\uff0c\u5e76\u89c2\u5bdf\u5230Hong-Ou-Mandel\uff08HOM\uff09\u5e72\u6d89\u6761\u7eb9\uff0c\u53ef\u89c1\u5ea6\u9ad8\u8fbe99.1%\uff0c\u65e0\u9700\u8fdb\u884c\u5149\u8c31\u6ee4\u6ce2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u4e0d\u76f8\u5e72\u6cf5\u6d66\u5149\u5b50\u6e90\u5177\u6709\u7ea660%\u7684\u6d8c\u73b0\u6548\u7387\u548c\u8d85\u8fc715000\u7684\u7b26\u5408-\u610f\u5916\u6bd4\u3002\u5728\u6ca1\u6709\u8fdb\u884c\u5149\u8c31\u6ee4\u6ce2\u7684\u60c5\u51b5\u4e0b\uff0c\u89c2\u5bdf\u5230\u7684HOM\u5e72\u6d89\u6761\u7eb9\u53ef\u89c1\u5ea6\u8fbe\u5230\u4e8699.1%\uff0c\u8bc1\u660e\u4e86\u5149\u5b50\u63a5\u8fd1\u7406\u60f3\u7684\u4e0d\u53ef\u533a\u5206\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u76f8\u5e72\u6cf5\u6d66\u5bf9\u4e8e\u5236\u5907\u9ad8\u8d28\u91cf\u91cf\u5b50\u6e90\u5e76\u975e\u5fc5\u9700\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6613\u4e8e\u83b7\u53d6\u7684\u4e0d\u76f8\u5e72\u5149\u6e90\u6765\u5236\u5907\u9ad8\u8d28\u91cf\u91cf\u5b50\u6001\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.11821", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen\uff1a\u81ea\u52a8\u751f\u6210\u7279\u5b9a\u4efb\u52a1\u7684MNIST\u98ce\u683c\u6570\u636e\u96c6\uff0c\u5229\u7528CLIP\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u6811\u6728\u548c\u98df\u7269\u5206\u7c7b\u3002", "motivation": "\u6807\u51c6MNIST\u6570\u636e\u96c6\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u6811\u6728\u3001\u98df\u7269\u5206\u7c7b\uff09\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u548c\u4e0d\u8db3\u3002\u624b\u52a8\u521b\u5efa\u548c\u53d1\u5e03\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8017\u65f6\u4e14\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\u3002", "method": "MNIST-Gen\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u7406\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988\uff0c\u91c7\u7528\u5206\u5c42\u8bed\u4e49\u5206\u7c7b\u65b9\u6cd5\uff0c\u652f\u6301\u590d\u6742\u7684\u7c7b\u522b\u7ed3\u6784\u548c\u591a\u79cd\u5904\u7406\u6a21\u5f0f\uff08\u5355\u72ec\u5ba1\u67e5\u3001\u667a\u80fd\u6279\u91cf\u5904\u7406\u3001\u5feb\u901f\u6279\u91cf\u5904\u7406\uff09\u3002\u501f\u9274\u8303\u7574\u8bba\u7684\u601d\u60f3\uff0c\u5c06\u6570\u636e\u8f6c\u6362\u9636\u6bb5\u5efa\u6a21\u4e3a\u53ef\u7ec4\u5408\u7684\u6001\u5c04\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u201cTree-MNIST\u201d\u548c\u201cFood-MNIST\u201d\u4e24\u4e2a\u65b0\u7684\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u96c6\u3002MNIST-Gen\u5b9e\u73b0\u4e8685%\u7684\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u4e0e\u624b\u52a8\u65b9\u6cd5\u76f8\u6bd4\u8282\u7701\u4e8680%\u7684\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u7279\u5b9a\u4efb\u52a1\u8bc4\u4f30\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MNIST-Gen\u6846\u67b6\uff0c\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7279\u5b9a\u7c7b\u522bMNIST\u98ce\u683c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528CLIP\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u667a\u80fd\u7684\u6570\u636e\u96c6\u521b\u5efa\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMNIST-Gen\u5728\u751f\u6210\u201cTree-MNIST\u201d\u548c\u201cFood-MNIST\u201d\u6570\u636e\u96c6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523085%\uff0c\u5e76\u8282\u7701\u4e8680%\u7684\u65f6\u95f4\u3002"}}
{"id": "2507.12004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12004", "abs": "https://arxiv.org/abs/2507.12004", "authors": ["Josip Juki\u0107"], "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u8868\u793a\u5e73\u6ed1\u5ea6\u3001\u4e3b\u52a8\u5b66\u4e60\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5f31\u76d1\u7763\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u672c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8868\u793a\u5206\u6790\u548c\u4f18\u5316\u6280\u672f\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\uff1a1. \u57fa\u4e8e\u8868\u793a\u5e73\u6ed1\u5ea6\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5229\u7528\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6d77\u68ee\u77e9\u9635\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\u30022. \u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5229\u7528\u8868\u793a\u5e73\u6ed1\u5ea6\u6307\u5bfc\u65e9\u671f\u505c\u6b62\uff0c\u65e0\u9700\u6807\u8bb0\u9a8c\u8bc1\u96c6\uff0c\u4ece\u800c\u964d\u4f4e\u6807\u8bb0\u5de5\u4f5c\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u30023. \u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u589e\u5f3a\u5f31\u76d1\u7763\u6280\u672f\uff0c\u66f4\u6709\u6548\u5730\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u548c\u52a8\u6001\u6570\u636e\u73af\u5883\u4e0b\u7684\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u672c\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u548c\u52a8\u6001\u6570\u636e\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8868\u793a\u5e73\u6ed1\u5ea6\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4ee5\u53ca\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u5f31\u76d1\u7763\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u548c\u52a8\u6001\u6570\u636e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.11980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11980", "abs": "https://arxiv.org/abs/2507.11980", "authors": ["Jiajian Xie", "Shengyu Zhang", "Zhou Zhao", "Fan Wu", "Fei Wu"], "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "comment": "21 pages, 8 figures", "summary": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.", "AI": {"tldr": "EC-Diff\u901a\u8fc7\u68af\u5ea6\u4f30\u8ba1\u548c\u4f18\u5316\u7684\u4e91\u8fb9\u5207\u6362\u7b56\u7565\uff0c\u52a0\u901f\u4e86\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6df7\u5408\u8fb9\u7f18-\u4e91\u534f\u4f5c\u6846\u67b6\u4e2d\uff0c\u4e91\u7aef\u6a21\u578b\u8fc7\u5ea6\u53bb\u566a\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u8fc7\u957f\uff0c\u6216\u53bb\u566a\u4e0d\u8db3\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u548c\u8fb9\u7f18\u6a21\u578b\u8f93\u51fa\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEC-Diff\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u7684\u566a\u58f0\u4f30\u8ba1\u6765\u52a0\u901f\u4e91\u7aef\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u53cc\u9636\u6bb5\u8d2a\u5a6a\u641c\u7d22\u7b97\u6cd5\u6765\u4f18\u5316\u566a\u58f0\u4f30\u8ba1\u548c\u8fb9\u7f18\u6a21\u578b\u5207\u6362\u7684\u53c2\u6570\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cdK\u6b65\u566a\u58f0\u8fd1\u4f3c\u7b56\u7565\uff0c\u5229\u7528\u566a\u58f0\u68af\u5ea6\u6765\u51cf\u5c11\u4e91\u7aef\u63a8\u7406\u7684\u9891\u7387\uff0c\u5e76\u5468\u671f\u6027\u5730\u8fdb\u884c\u4e91\u7aef\u63a8\u7406\u4ee5\u7ea0\u6b63\u8bef\u5dee\u3002", "result": "EC-Diff\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "EC-Diff\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u566a\u58f0\u4f30\u8ba1\u52a0\u901f\u4e91\u63a8\u7406\uff0c\u5e76\u786e\u5b9a\u4e86\u4e91-\u8fb9\u5207\u6362\u7684\u6700\u4f73\u70b9\uff0c\u4ee5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u4ec5\u5728\u8fb9\u7f18\u8fdb\u884c\u63a8\u7406\u76f8\u6bd4\uff0cEC-Diff\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e0e\u4ec5\u5728\u4e91\u7aef\u63a8\u7406\u76f8\u6bd4\uff0c\u63a8\u7406\u901f\u5ea6\u5e73\u5747\u63d0\u9ad8\u4e862\u500d\u3002"}}
{"id": "2507.12071", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.12071", "abs": "https://arxiv.org/abs/2507.12071", "authors": ["P\u00e9ter L\u00e9vay", "Csaba Velich"], "title": "A thermofield-double model of Uhlmann anholonomy", "comment": "48 pages, 2 figures", "summary": "A simple parametrized family of quantum systems consisting of two entangled\nsubsystems, dubbed left and right ones, both of them featuring N qubits is\nconsidered in the thermofield double formalism. We assume that the system\nevolves in a purely geometric manner based on the parallel transport condition\ndue to Uhlmann. We explore the different interpretations of this evolution\nrelative to observers either coupled to the left or to the right subsystems.\nThe Uhlmann condition breaks the symmetry between left and right by regarding\none of the two possible sets of local unitary operations as gauge degrees of\nfreedom. Then gauging the right side we show that the geometric evolution on\nthe left manifests itself via certain local operations reminiscent of\nnon-unitary filtering measurements. On the other hand on the right the basic\nevolutionary steps are organized into a sequence of unitary operations of a\nholonomic quantum computation. We calculate the Uhlmann connection governing\nthe transport for our model which turns out to be related to higher dimensional\ninstantons. Then we evaluate the anholonomy of the connection for geodesic\ntriangles with geodesic segments defined with respect to the Bures metric. By\nanalysing the explicit form of the local filtering measurements showing up on\nthe left side we realize that they are also optimal measurements for\ndistinguishing two given mixed states in the statistical sense. We also point\nout that by conducting an interference experiment on the right side one can\nobserve the physical effects of the anholonomic quantum computation. We\ndemonstrate this by calculating explicit examples for phase shifts and\nvisibility patterns arising in such interference experiments. Finally a\nsequence of geodesic triangles producing the iSWAP gate via anholonomy needed\nfor computational universality is presented.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u7cfb\u7edf\u7684\u51e0\u4f55\u6f14\u5316\uff0c\u5229\u7528 Uhlmann \u6761\u4ef6\u548c\u70ed\u573a\u53cc\u91cd\u5f62\u5f0f\u4e3b\u4e49\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u89c2\u6d4b\u8005\u89c6\u89d2\u4e0b\u7684\u6f14\u5316\u884c\u4e3a\uff0c\u5e76\u5c06\u51e0\u4f55\u6f14\u5316\u4e0e\u91cf\u5b50\u8ba1\u7b97\uff08\u5982\u5168\u606f\u91cf\u5b50\u8ba1\u7b97\u548c iSWAP \u95e8\uff09\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u91cf\u5b50\u7cfb\u7edf\u7684\u51e0\u4f55\u6f14\u5316\u53ca\u5176\u4e0e\u4e0d\u540c\u89c2\u6d4b\u8005\u7684\u5173\u7cfb\u3002", "method": "\u672c\u6587\u8003\u8651\u4e86\u4e00\u4e2a\u7531\u4e24\u4e2a\u7ea0\u7f20\u7684\u5b50\u7cfb\u7edf\uff08\u79f0\u4e3a\u5de6\u548c\u53f3\u5b50\u7cfb\u7edf\uff09\u7ec4\u6210\u7684\u7b80\u5355\u53c2\u6570\u5316\u91cf\u5b50\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u5b50\u7cfb\u7edf\u90fd\u5177\u6709 N \u4e2a\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u7f6e\u4e8e\u70ed\u573a\u53cc\u91cd\u5f62\u5f0f\u4e3b\u4e49\u4e2d\u3002\u6211\u4eec\u5047\u8bbe\u7cfb\u7edf\u662f\u6839\u636e Uhlmann \u7684\u5e73\u884c\u4f20\u8f93\u6761\u4ef6\u4ee5\u7eaf\u7cb9\u7684\u51e0\u4f55\u65b9\u5f0f\u6f14\u5316\u7684\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u76f8\u5bf9\u4e8e\u8026\u5408\u5230\u5de6\u5b50\u7cfb\u7edf\u6216\u53f3\u5b50\u7cfb\u7edf\u7684\u89c2\u6d4b\u8005\u800c\u8a00\uff0c\u8be5\u6f14\u5316\u7684\u4e0d\u540c\u89e3\u91ca\u3002Uhlmann \u6761\u4ef6\u901a\u8fc7\u5c06\u4e24\u4e2a\u53ef\u80fd\u7684\u5c40\u90e8\u9149\u64cd\u4f5c\u96c6\u89c6\u4e3a\u89c4\u8303\u81ea\u7531\u5ea6\u6765\u6253\u7834\u5de6\u53f3\u5b50\u7cfb\u7edf\u4e4b\u95f4\u7684\u5bf9\u79f0\u6027\u3002", "result": "\u901a\u8fc7\u89c4\u8303\u53f3\u4fa7\uff0c\u6211\u4eec\u8868\u660e\u5de6\u4fa7\u7684\u51e0\u4f55\u6f14\u5316\u901a\u8fc7\u7c7b\u4f3c\u4e8e\u975e\u9149\u6ee4\u6ce2\u6d4b\u91cf\u7684\u5c40\u90e8\u64cd\u4f5c\u663e\u73b0\u51fa\u6765\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u53f3\u4fa7\uff0c\u57fa\u672c\u7684\u6f14\u5316\u6b65\u9aa4\u88ab\u7ec4\u7ec7\u6210\u4e00\u7cfb\u5217\u5168\u606f\u91cf\u5b50\u8ba1\u7b97\u7684\u9149\u64cd\u4f5c\u3002\u6211\u4eec\u8ba1\u7b97\u4e86\u63a7\u5236\u6211\u4eec\u6a21\u578b\u4f20\u8f93\u7684 Uhlmann \u8fde\u63a5\uff0c\u4e8b\u5b9e\u8bc1\u660e\u5b83\u4e0e\u9ad8\u7ef4\u77ac\u5b50\u6709\u5173\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5bf9\u4e8e\u4ee5 Bures \u5ea6\u91cf\u5b9a\u4e49\u7684\u6d4b\u5730\u7ebf\u6bb5\u7684\u6d4b\u5730\u7ebf\u4e09\u89d2\u5f62\u7684\u8fde\u63a5\u7684\u5355\u8282\u6027\u3002\u901a\u8fc7\u5206\u6790\u5de6\u4fa7\u51fa\u73b0\u7684\u5c40\u90e8\u6ee4\u6ce2\u6d4b\u91cf\u7684\u663e\u5f0f\u5f62\u5f0f\uff0c\u6211\u4eec\u610f\u8bc6\u5230\u5b83\u4eec\u5728\u7edf\u8ba1\u610f\u4e49\u4e0a\u4e5f\u662f\u533a\u5206\u4e24\u4e2a\u7ed9\u5b9a\u6df7\u5408\u6001\u7684\u6700\u4f73\u6d4b\u91cf\u3002\u6211\u4eec\u8fd8\u6307\u51fa\uff0c\u901a\u8fc7\u5728\u53f3\u4fa7\u8fdb\u884c\u5e72\u6d89\u5b9e\u9a8c\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u5355\u8282\u6027\u91cf\u5b50\u8ba1\u7b97\u7684\u7269\u7406\u6548\u5e94\u3002\u6211\u4eec\u901a\u8fc7\u8ba1\u7b97\u76f8\u79fb\u548c\u7531\u6b64\u4ea7\u751f\u7684\u5e72\u6d89\u5b9e\u9a8c\u4e2d\u7684\u53ef\u89c1\u5ea6\u6a21\u5f0f\u7684\u663e\u5f0f\u793a\u4f8b\u6765\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u6d4b\u5730\u7ebf\u4e09\u89d2\u5f62\u7ec4\u6210\u7684\u5e8f\u5217\uff0c\u901a\u8fc7\u5355\u8282\u6027\uff08anholonomy\uff09\u4ea7\u751f iSWAP \u95e8\uff0c\u8fd9\u662f\u8ba1\u7b97\u901a\u7528\u6027\u6240\u5fc5\u9700\u7684\u3002"}}
{"id": "2507.11836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11836", "abs": "https://arxiv.org/abs/2507.11836", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "comment": null, "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "AI": {"tldr": "HyperEvent \u901a\u8fc7\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u89c6\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\uff0c\u5e76\u4f7f\u7528\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u6709\u6548\u6355\u6349\u4e86\u4e8b\u4ef6\u95f4\u7684\u7ed3\u6784\u8054\u7cfb\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u65f6\uff0c\u4fa7\u91cd\u4e8e\u5355\u4e2a\u4ea4\u4e92\u6216\u539f\u5b50\u72b6\u6001\uff0c\u672a\u80fd\u6355\u6349\u590d\u5408\u8d85\u4e8b\u4ef6\uff08\u5373\u56e0\u679c\u76f8\u5173\u4e8b\u4ef6\u7ec4\uff09\u7684\u7ed3\u6784\u5185\u805a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HyperEvent \u7684\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u91cd\u6784\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\u6765\u91cf\u5316\u67e5\u8be2\u4e8b\u4ef6\u4e0e\u5386\u53f2\u4e8b\u4ef6\u4e4b\u95f4\u7684\u6210\u5bf9\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u6355\u6349\u8d85\u4e8b\u4ef6\u7684\u7ed3\u6784\u5185\u805a\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u4e8b\u4ef6\u6d41\u6765\u5b9e\u73b0\u5927\u89c4\u6a21\u56fe\u7684\u5e76\u53d1\u8bad\u7ec3\u3002", "result": "HyperEvent \u5728 5 \u4e2a\u6570\u636e\u96c6\u4e2d\u7684 4 \u4e2a\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u5728 Flight \u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86 6.95%\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u4ec5\u5360\u57fa\u7ebf\u7684 10.17%\u3002", "conclusion": "HyperEvent \u901a\u8fc7\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u91cd\u65b0\u6784\u5efa\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u6784\u5efa\u7684\u5173\u8054\u5e8f\u5217\uff0c\u6210\u529f\u6355\u6349\u4e86\u590d\u5408\u8d85\u4e8b\u4ef6\u7684\u7ed3\u6784\u5185\u805a\u6027\u3002\u8be5\u6846\u67b6\u5728 5 \u4e2a\u6570\u636e\u96c6\u4e2d\u7684 4 \u4e2a\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u5c55\u793a\u4e86\u51fa\u8272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f8b\u5982\u5728 Flight \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 6.95% \u7684\u5e73\u5747\u5012\u6570\u6392\u540d\u63d0\u5347\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684 10.17%\u3002"}}
{"id": "2507.12039", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12039", "abs": "https://arxiv.org/abs/2507.12039", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u521b\u9020\u529b\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u4eba\u7c7b\uff0c\u4f46\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u6269\u5c55\u6027\u521b\u9020\u529b\uff0c\u800cLLMs\u66f4\u503e\u5411\u4e8e\u56fa\u5b9a\u6027\u521b\u9020\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bed\u8a00\u521b\u9020\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u65bd\u4e86\u4e00\u9879\u5305\u542b\u8bcd\u8bed\u6784\u6210\uff08\u6d3e\u751f\u548c\u590d\u5408\uff09\u53ca\u9690\u55bb\u8bed\u8a00\u4f7f\u7528\u7684\u4efb\u52a1\u7684\u8bed\u8a00\u521b\u9020\u529b\u6d4b\u8bd5\uff0c\u5bf924\u540d\u4eba\u7c7b\u548c\u7b49\u91cfLLM\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528OCSAI\u5de5\u5177\u4ece\u539f\u521b\u6027\u3001\u9610\u8ff0\u6027\u548c\u7075\u6d3b\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e0a\u5747\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5e76\u5728\u516b\u9879\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u516d\u9879\u8868\u73b0\u66f4\u597d\u3002LLMs\u548c\u4eba\u7c7b\u5728\u4e2a\u4f53\u7b54\u6848\u7684\u72ec\u7279\u6027\u4e0a\u5b58\u5728\u7ec6\u5fae\u5dee\u5f02\u3002", "conclusion": "LLMs\u5728\u8bed\u8a00\u521b\u9020\u529b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u5728\u539f\u521b\u6027\u3001\u9610\u8ff0\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u3002\u4eba\u7c7b\u503e\u5411\u4e8e\u6269\u5c55\u6027\u521b\u9020\u529b\uff08E-creativity\uff09\uff0c\u800cLLMs\u503e\u5411\u4e8e\u56fa\u5b9a\u6027\u521b\u9020\u529b\uff08F-creativity\uff09\u3002"}}
{"id": "2507.11985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11985", "abs": "https://arxiv.org/abs/2507.11985", "authors": ["Jiahao Xia", "Yike Wu", "Wenjian Huang", "Jianguo Zhang", "Jian Zhang"], "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "comment": "Accepted to ICCV 2025", "summary": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12117", "categories": ["quant-ph", "cs.AI", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.12117", "abs": "https://arxiv.org/abs/2507.12117", "authors": ["Timothy Heightman", "Edward Jiang", "Ruth Mora-Soto", "Maciej Lewenstein", "Marcin P\u0142odzie\u0144"], "title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations", "comment": null, "summary": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of\nquantum mechanical systems, including superposition, coherence, and quantum\nentanglement for classical data processing. However, due to the exponential\ngrowth of the Hilbert space, QML faces practical limits in classical\nsimulations with the state-vector representation of quantum system. On the\nother hand, phase-space methods offer an alternative by encoding quantum states\nas quasi-probability functions. Building on prior work in qubit phase-space and\nthe Stratonovich-Weyl (SW) correspondence, we construct a closed, composable\ndynamical formalism for one- and many-qubit systems in phase-space. This\nformalism replaces the operator algebra of the Pauli group with function\ndynamics on symplectic manifolds, and recasts the curse of dimensionality in\nterms of harmonic support on a domain that scales linearly with the number of\nqubits. It opens a new route for QML based on variational modelling over\nphase-space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u76f8\u4f4d\u7a7a\u95f4\u91cf\u5b50\u6001\u52a8\u529b\u5b66\u5f62\u5f0f\uff0c\u514b\u670d\u4e86\u7ecf\u5178\u6a21\u62df\u7684\u9650\u5236\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u7ecf\u5178\u6a21\u62df\u4e2d\u7531\u4e8e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6307\u6570\u589e\u957f\u800c\u9762\u4e34\u7684\u5b9e\u9645\u9650\u5236\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u91cf\u5b50\u6001\u7684\u76f8\u4f4d\u7a7a\u95f4\u8868\u793a\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u95ed\u5408\u7684\u3001\u53ef\u7ec4\u5408\u7684\u52a8\u529b\u5b66\u5f62\u5f0f\uff0c\u7528\u4e8e\u5904\u7406\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u6001\u3002\u8be5\u65b9\u6cd5\u5c06 Pauli \u7fa4\u7684\u7b97\u5b50\u4ee3\u6570\u66ff\u6362\u4e3a\u8f9b\u6d41\u5f62\u4e0a\u7684\u51fd\u6570\u52a8\u529b\u5b66\uff0c\u5e76\u7528\u4e0e\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u6210\u7ebf\u6027\u5173\u7cfb\u7684\u57df\u4e0a\u7684\u8c10\u6ce2\u652f\u6491\u6765\u91cd\u6784\u7ef4\u5ea6\u707e\u96be\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u7ef4\u5ea6\u707e\u96be\u7684\u590d\u6742\u6027\u8f6c\u5316\u4e3a\u4e0e\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u6210\u7ebf\u6027\u5173\u7cfb\u7684\u8c10\u6ce2\u652f\u6491\uff0c\u4e3a\u57fa\u4e8e\u76f8\u4f4d\u7a7a\u95f4\u7684\u53d8\u5206\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684QML\u8def\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u6001\u76f8\u4f4d\u7a7a\u95f4\u7684\u52a8\u529b\u5b66\u5f62\u5f0f\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u5176\u7279\u70b9\u662f\u53d8\u5206\u5efa\u6a21\u3002"}}
{"id": "2507.11839", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11839", "abs": "https://arxiv.org/abs/2507.11839", "authors": ["Chengyue Gong", "Xinshi Chen", "Yuxuan Zhang", "Yuxuan Song", "Hao Zhou", "Wenzhi Xiao"], "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "comment": null, "summary": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "AI": {"tldr": "Protenix-Mini optimizes protein structure prediction by using a faster ODE sampler, pruning network components, and replacing the MSA module, resulting in a more efficient model with minimal accuracy loss.", "motivation": "The motivation is to address the challenge of balancing model efficiency and prediction accuracy in biomolecular structure prediction for efficient real-world deployment and inference-time scaling, especially for large-scale applications with limited computational resources.", "method": "The study introduces Protenix-Mini, a lightweight protein structure prediction model, by replacing the multi-step AF3 sampler with a few-step ODE sampler, pruning redundant blocks in the Protenix framework, and substituting the MSA module with an ESM module. It utilizes a two-step ODE sampling strategy and a streamlined architecture.", "result": "Protenix-Mini achieves high-fidelity predictions with only a negligible 1 to 5 percent decrease in performance compared to its full-scale counterpart on benchmark datasets, demonstrating significant reduction in model complexity and computational overhead.", "conclusion": "Protenix-Mini is an efficient protein structure prediction model that significantly reduces computational overhead with a slight decrease in accuracy, making it suitable for resource-limited applications."}}
{"id": "2507.11975", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11975", "abs": "https://arxiv.org/abs/2507.11975", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "comment": "25 pages, 5 figures, 4 tables", "summary": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u548c\u526a\u679d\u7ed3\u5408\u7684\u65b9\u6cd5\uff08XiNet\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u538b\u7f29\u5927\u578b\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u8868\u73b0\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u5728\u5229\u7528\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u65f6\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u6027\u4e5f\u663e\u8457\u589e\u52a0\u3002\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5728RL\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u5728RL\u4e2d\u6709\u6548\u5e94\u7528\u526a\u679d\u6280\u672f\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8bad\u7ec3\u548c\u526a\u679d\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4f7f\u7528\u5728\u7ebf\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff08OFENet\uff09\u7684\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u7f51\u7edc\u6765\u89e3\u51b3\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u786e\u5b9a\u7f51\u7edc\u6743\u91cd\u548c\u7528\u4e8e\u7f29\u653e\u6bcf\u4e2a\u5355\u5143\u76840/1\u968f\u673a\u53d8\u91cf\uff08\u03be\uff09\u7684\u53d8\u5206\u4f2f\u52aa\u5229\u5206\u5e03\u7684\u53c2\u6570\u3002\u5f53\u67d0\u4e2a\u5355\u5143\u5bf9\u6027\u80fd\u8d21\u732e\u5f88\u5c0f\u65f6\uff0c\u5176\u5bf9\u5e94\u7684\u53d8\u5206\u53c2\u6570\u4f1a\u8d8b\u5411\u4e8e0\uff0c\u4ece\u800c\u5c06\u8be5\u5355\u5143\u6c38\u4e45\u6027\u5730\u526a\u679d\u3002\u7814\u7a76\u4e2d\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u611f\u77e5\u3001\u4fc3\u8fdb\u7a00\u758f\u6027\u7684\u6b63\u5219\u5316\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u9488\u5bf9OFENet\u7684DenseNet\u67b6\u6784\uff0c\u5e76\u81ea\u52a8\u9009\u62e9\u76f8\u5173\u8d85\u53c2\u6570\uff0c\u5c06RL\u76ee\u6807\u4e0e\u7f51\u7edc\u538b\u7f29\u76f8\u7ed3\u5408\u3002", "result": "\u901a\u8fc7\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\uff08MuJoCo\uff09\u548cSoft Actor-Critic RL\u4ee3\u7406\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86OFENet\u53ef\u4ee5\u88ab\u5927\u5e45\u5ea6\u526a\u679d\uff0c\u540c\u65f6\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u526a\u679d\u5927\u578b\u7f51\u7edc\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5c0f\u578b\u7f51\u7edc\u80fd\u4ea7\u751f\u66f4\u6709\u6548\u3001\u6027\u80fd\u66f4\u9ad8\u7684RL\u4ee3\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5927\u5e45\u5ea6\u4fee\u526a\u5927\u578b\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fee\u526a\u5927\u578b\u7f51\u7edc\u53ef\u4ee5\u4ea7\u751f\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u5c0f\u578b\u7f51\u7edc\u66f4\u6709\u6548\u3001\u6027\u80fd\u66f4\u9ad8\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002"}}
{"id": "2507.12059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12059", "abs": "https://arxiv.org/abs/2507.12059", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "LLM\u5728\u63a8\u65ad\u57fa\u6570\u65b9\u5411\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76LLM\u5728\u63a8\u65ad\u57fa\u6570\u65b9\u5411\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u7531\u6a21\u677f\u96c6\u751f\u6210\u7684\u57fa\u51c6\u6765\u6d4b\u8bd528\u4e2aLLM\u63a8\u65ad\u57fa\u6570\u65b9\u5411\u7684\u80fd\u529b\uff0c\u8be5\u57fa\u51c6\u5141\u8bb8\u5728\u79fb\u52a8\u5de5\u5177\u3001\u7b2c\u4e00\u4eba\u79f0\u3001\u7b2c\u4e8c\u4eba\u79f0\u6216\u7b2c\u4e09\u4eba\u79f0\u7b49\u65b9\u9762\u8fdb\u884c\u53d8\u5316\u3002", "result": "\u5373\u4f7f\u662f\u8f83\u65b0\u7684LLM\u4e5f\u65e0\u6cd5\u53ef\u9760\u5730\u786e\u5b9a\u6240\u6709\u95ee\u9898\u7684\u6b63\u786e\u57fa\u6570\u65b9\u5411\u3002", "conclusion": "LLM\u5728\u63a8\u65ad\u57fa\u6570\u65b9\u5411\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u5373\u4f7f\u662f\u8f83\u65b0\u7684LLM\u4e5f\u65e0\u6cd5\u53ef\u9760\u5730\u786e\u5b9a\u6240\u6709\u95ee\u9898\u7684\u6b63\u786e\u57fa\u6570\u65b9\u5411\u3002"}}
{"id": "2507.11986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11986", "abs": "https://arxiv.org/abs/2507.11986", "authors": ["Jaehyun Lee", "Wonhark Park", "Wonsik Shin", "Hyunho Lee", "Hyoung Min Na", "Nojun Kwak"], "title": "Style Composition within Distinct LoRA modules for Traditional Art", "comment": null, "summary": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u6269\u6563\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u7a7a\u95f4\u63a9\u7801\u548c\u6df1\u5ea6\u56fe\u63a7\u5236\uff0c\u5728\u4e0d\u540c\u98ce\u683c\u7684\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u533a\u57df\u6027\u98ce\u683c\u6df7\u5408\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u548c\u98ce\u683c\u4e2a\u6027\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7ea0\u7f20\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u7f3a\u4e4f\u5e73\u6ed1\u63d2\u503c\u4f7f\u5f97\u96be\u4ee5\u4ee5\u53ef\u63a7\u7684\u3001\u533a\u57df\u6027\u7684\u65b9\u5f0f\u5e94\u7528\u4e0d\u540c\u7684\u7ed8\u753b\u6280\u672f\uff0c\u5e76\u4e14\u5e38\u5e38\u5bfc\u81f4\u4e00\u79cd\u98ce\u683c\u538b\u5012\u5176\u4ed6\u98ce\u683c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u7ba1\u7ebf\uff0c\u901a\u8fc7\u5728\u5355\u72ec\u8bad\u7ec3\u7684\u3001\u98ce\u683c\u4e13\u95e8\u5316\u7684\u6a21\u578b\u8fdb\u884c\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u5728\u53bb\u566a\u540e\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u98ce\u683c\u5408\u6210\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4\u63a9\u7801\u8de8\u5f02\u6784\u6269\u6563\u7ba1\u7ebf\u878d\u5408\u4f4e\u566a\u58f0\u6f5c\u5728\u4fe1\u606f\u3002\u5f15\u5165\u4e86\u901a\u8fc7ControlNet\u7684\u6df1\u5ea6\u56fe\u6761\u4ef6\u6765\u786e\u4fdd\u8de8\u4e0d\u540c\u6a21\u578b\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u7684\u63a9\u7801\u6210\u529f\u5b9e\u73b0\u7279\u5b9a\u533a\u57df\u7684\u98ce\u683c\u6df7\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u63a9\u7801\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5b9e\u73b0\u7279\u5b9a\u533a\u57df\u7684\u98ce\u683c\u6df7\u5408\uff0c\u5e76\u4fdd\u6301\u6bcf\u79cd\u98ce\u683c\u7684\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.12159", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12159", "abs": "https://arxiv.org/abs/2507.12159", "authors": ["Monit Sharma", "Hoong Chuin Lau"], "title": "Cutting Slack: Quantum Optimization with Slack-Free Methods for Combinatorial Benchmarks", "comment": "20 pages, 22 figures ,8 tables", "summary": "Constraint handling remains a key bottleneck in quantum combinatorial\noptimization. While slack-variable-based encodings are straightforward, they\nsignificantly increase qubit counts and circuit depth, challenging the\nscalability of quantum solvers. In this work, we investigate a suite of\nLagrangian-based optimization techniques including dual ascent, bundle methods,\ncutting plane approaches, and augmented Lagrangian formulations for solving\nconstrained combinatorial problems on quantum simulators and hardware. Our\nframework is applied to three representative NP-hard problems: the Travelling\nSalesman Problem (TSP), the Multi-Dimensional Knapsack Problem (MDKP), and the\nMaximum Independent Set (MIS).\n  We demonstrate that MDKP and TSP, with their inequality-based or\ndegree-constrained structures, allow for slack-free reformulations, leading to\nsignificant qubit savings without compromising performance. In contrast, MIS\ndoes not inherently benefit from slack elimination but still gains in\nfeasibility and objective quality from principled Lagrangian updates. We\nbenchmark these methods across classically hard instances, analyzing trade-offs\nin qubit usage, feasibility, and optimality gaps. Our results highlight the\nflexibility of Lagrangian formulations as a scalable alternative to naive QUBO\npenalization, even when qubit savings are not always achievable. This work\nprovides practical insights for deploying constraint-aware quantum optimization\npipelines, with applications in logistics, network design, and resource\nallocation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u7684\u4f18\u5316\u6280\u672f\uff0c\u5982\u5bf9\u5076\u4e0a\u5347\u3001\u6346\u7ed1\u65b9\u6cd5\u3001\u5272\u5e73\u9762\u65b9\u6cd5\u548c\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u516c\u5f0f\uff0c\u4ee5\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u89e3\u51b3\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u7814\u7a76\u4eba\u5458\u53d1\u73b0\uff0c\u5bf9\u4e8e TSP \u548c MDKP \u7b49\u95ee\u9898\uff0c\u53ef\u4ee5\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u7684\u4f7f\u7528\uff0c\u800c MIS \u95ee\u9898\u53ef\u4ee5\u4ece\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\u53d7\u76ca\uff0c\u4ee5\u63d0\u9ad8\u53ef\u884c\u6027\u548c\u7ed3\u679c\u8d28\u91cf\u3002\u603b\u4f53\u800c\u8a00\uff0c\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u4e3a\u7ea6\u675f\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0e\u4f20\u7edf\u7684 QUBO \u60e9\u7f5a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u80fd\u591f\u8282\u7701\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u4e3a\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u7ec4\u5408\u4f18\u5316\u4e2d\u7ea6\u675f\u5904\u7406\u7684\u74f6\u9888\uff0c\u7279\u522b\u662f\u4e0e\u677e\u5f1b\u53d8\u91cf\u7f16\u7801\u76f8\u5173\u7684\u6311\u6218\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u7684\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u6c42\u89e3\u5668\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5305\u62ec\u5bf9\u5076\u4e0a\u5347\u3001\u6346\u7ed1\u65b9\u6cd5\u3001\u5272\u5e73\u9762\u65b9\u6cd5\u548c\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u516c\u5f0f\u5728\u5185\u7684\u4e00\u7cfb\u5217\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u7684\u4f18\u5316\u6280\u672f\uff0c\u7528\u4e8e\u5728\u91cf\u5b50\u6a21\u62df\u5668\u548c\u786c\u4ef6\u4e0a\u6c42\u89e3\u7ea6\u675f\u7ec4\u5408\u95ee\u9898\u3002", "result": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u5177\u6709\u4e0d\u7b49\u5f0f\u7ea6\u675f\u6216\u5ea6\u7ea6\u675f\u7ed3\u6784\u7684\u591a\u7ef4\u80cc\u5305\u95ee\u9898\uff08MDKP\uff09\u548c\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65e0\u677e\u5f1b\u7684\u91cd\u65b0\u8868\u8ff0\uff0c\u4ece\u800c\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u8282\u7701\u91cf\u5b50\u6bd4\u7279\u3002\u7136\u800c\uff0c\u6700\u5927\u72ec\u7acb\u96c6\uff08MIS\uff09\u867d\u7136\u4e0d\u80fd\u76f4\u63a5\u4ece\u6d88\u9664\u677e\u5f1b\u4e2d\u53d7\u76ca\uff0c\u4f46\u901a\u8fc7\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u7684\u66f4\u65b0\u53ef\u4ee5\u63d0\u9ad8\u53ef\u884c\u6027\u548c\u76ee\u6807\u8d28\u91cf\u3002\u5728\u7ecf\u5178\u56f0\u96be\u7684\u5b9e\u4f8b\u4e0a\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u91cf\u5b50\u6bd4\u7279\u4f7f\u7528\u3001\u53ef\u884c\u6027\u548c\u6700\u4f18\u6027\u5dee\u8ddd\u65b9\u9762\u7684\u6743\u8861\u3002", "conclusion": "\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u4e3a\u91cf\u5b50\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7ea6\u675f\u5904\u7406\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8282\u7701\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u9645\u89c1\u89e3\u3002"}}
{"id": "2507.11847", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11847", "abs": "https://arxiv.org/abs/2507.11847", "authors": ["Yu-Jie Zhang", "Sheng-An Xu", "Peng Zhao", "Masashi Sugiyama"], "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "comment": null, "summary": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e7f\u4e49\u7ebf\u6027\u8001\u864e\u673a\uff08GLB\uff09\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u540e\u6094\u754c\u9650\u548c\u6052\u5b9a\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e7f\u4e49\u7ebf\u6027\u8001\u864e\u673a\uff08GLB\uff09\u95ee\u9898\u4e2d\u5b58\u5728\u7684\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u8be5\u7b97\u6cd5\u7684\u6838\u5fc3\u662f\u4e3a\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\uff08OMD\uff09\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e00\u4e2a\u7cbe\u786e\u7684\u7f6e\u4fe1\u96c6\uff0c\u8be5\u7f6e\u4fe1\u96c6\u662f\u901a\u8fc7\u5229\u7528\u5728\u7ebf\u9884\u6d4b\u4e2d\u7684\u6df7\u5408\u635f\u5931\u6982\u5ff5\u8fdb\u884c\u7684\u65b0\u9896\u5206\u6790\u5f97\u51fa\u7684\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u540e\u6094\u754c\u9650\uff0c\u5e76\u4e14\u6bcf\u8f6e\u5177\u6709\u6052\u5b9a\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u5176\u7edf\u8ba1\u6548\u7387\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u76f8\u5f53\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u540e\u6094\u754c\u9650\uff0c\u5e76\u4e14\u6bcf\u8f6e\u5177\u6709\u6052\u5b9a\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.12027", "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12027", "abs": "https://arxiv.org/abs/2507.12027", "authors": ["Beining Xu", "Siting Zhu", "Hesheng Wang"], "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "comment": "8 pages, 2 figures, IROS 2025", "summary": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.", "AI": {"tldr": "SGLoc \u662f\u4e00\u79cd\u65b0\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5b83\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u76f4\u63a5\u4ece 3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u56de\u5f52\u76f8\u673a\u4f4d\u59ff\uff0c\u65e0\u9700\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u3002\u5b83\u901a\u8fc7\u591a\u5c42\u6b21\u59ff\u6001\u56de\u5f52\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u5168\u5c40\u68c0\u7d22\u6765\u5b9e\u73b0\u7cbe\u786e\u7684\u5168\u5c40\u5b9a\u4f4d\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u76f4\u63a5\u4ece 3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u56de\u5f52\u76f8\u673a\u4f4d\u59ff\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u4ee5\u5728\u6ca1\u6709\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4f30\u8ba1 6DoF \u4f4d\u59ff\u3002", "method": "SGLoc \u901a\u8fc7\u5229\u7528\u8bed\u4e49\u4fe1\u606f\uff0c\u76f4\u63a5\u4ece 3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u56de\u5f52\u76f8\u673a\u4f4d\u59ff\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u591a\u5c42\u6b21\u59ff\u6001\u56de\u5f52\u7b56\u7565\uff0c\u4ece\u5168\u5c40 3DGS \u5730\u56fe\u4e2d\u9010\u6b65\u4f30\u8ba1\u548c\u4f18\u5316\u67e5\u8be2\u56fe\u50cf\u7684\u4f4d\u59ff\uff0c\u65e0\u9700\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u5168\u5c40\u68c0\u7d22\u7b97\u6cd5\uff0c\u4ee5\u5efa\u7acb 2D\uff08\u56fe\u50cf\uff09\u548c 3D\uff083DGS \u5730\u56fe\uff09\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u901a\u8fc7\u5339\u914d 2D \u67e5\u8be2\u56fe\u50cf\u7684\u8bed\u4e49\u63cf\u8ff0\u7b26\u548c 3DGS \u8bed\u4e49\u8868\u793a\uff0c\u5c06\u56fe\u50cf\u4e0e\u5168\u5c40 3DGS \u5730\u56fe\u7684\u5c40\u90e8\u533a\u57df\u5bf9\u9f50\uff0c\u4ece\u800c\u83b7\u5f97\u7c97\u7565\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002\u968f\u540e\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u67e5\u8be2\u56fe\u50cf\u4e0e\u4ece 3DGS \u6e32\u67d3\u7684\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u4f18\u5316\u7c97\u7565\u4f4d\u59ff\u3002", "result": "SGLoc \u8bc1\u660e\u4e86\u5176\u5728 12scenes \u548c 7scenes \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6ca1\u6709\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u5168\u5c40\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "SGLoc \u8bc1\u660e\u4e86\u5176\u5728 12scenes \u548c 7scenes \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6ca1\u6709\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u5168\u5c40\u5b9a\u4f4d\u80fd\u529b\u3002"}}
{"id": "2507.12064", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12064", "abs": "https://arxiv.org/abs/2507.12064", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "This paper presents a non-neural AI detection method using spaCy for feature extraction and light-gradient boosting for classification, trained on a large dataset of AI-generated texts.", "motivation": "To detect binary AI in text using a robust and explainable method.", "method": "A modular stylometric pipeline using public spaCy models for text preprocessing and feature extraction, with light-gradient boosting machines as the classifier.", "result": "The pipeline extracts thousands of features from linguistic annotations and is trained on a large corpus of over 500,000 machine-generated texts. Parameter options were explored to enhance classifier capacity.", "conclusion": "The approach follows a non-neural, computationally inexpensive, and explainable method, proving effective."}}
{"id": "2507.11990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11990", "abs": "https://arxiv.org/abs/2507.11990", "authors": ["Hyun-Jun Jin", "Young-Eun Kim", "Seong-Whan Lee"], "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "comment": null, "summary": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.", "AI": {"tldr": "\u63d0\u51faID-EA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4e2a\u6027\u5316\u4eba\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u5e76\u5b9e\u73b015\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Textual Inversion\u65b9\u6cd5\u5728\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u5728\u8eab\u4efd\u8868\u793a\u4e0a\u5b58\u5728\u8bed\u4e49\u4e0d\u5339\u914d\u3002", "method": "ID-EA\u6846\u67b6\u5305\u542bID-\u9a71\u52a8\u589e\u5f3a\u5668\uff08ID-Enhancer\uff09\u548cID\u6761\u4ef6\u9002\u914d\u5668\uff08ID-Adapter\uff09\u3002ID-Enhancer\u5c06\u8eab\u4efd\u5d4c\u5165\u4e0e\u6587\u672cID\u951a\u70b9\u7ed3\u5408\uff0c\u5229\u7528\u4ee3\u8868\u6027\u6587\u672c\u5d4c\u5165\u6765\u4f18\u5316\u4ece\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u83b7\u5f97\u7684\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\u3002ID-Adapter\u5229\u7528\u8eab\u4efd\u589e\u5f3a\u5d4c\u5165\u6765\u8c03\u6574\u9884\u8bad\u7ec3UNet\u6a21\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u5339\u914d\u6587\u672c\u6761\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u3002", "result": "ID-EA\u5728\u8eab\u4efd\u4fdd\u6301\u6307\u6807\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u751f\u6210\u4e2a\u6027\u5316\u4eba\u50cf\u7684\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u7ea615\u500d\u3002", "conclusion": "ID-EA\u901a\u8fc7\u5f15\u5bfc\u6587\u672c\u5d4c\u5165\u4e0e\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e2a\u6027\u5316\u4eba\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.11855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11855", "abs": "https://arxiv.org/abs/2507.11855", "authors": ["Davin Hill", "Brian L. Hill", "Aria Masoomi", "Vijay S. Nori", "Robert E. Tillman", "Jennifer Dy"], "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "comment": null, "summary": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "AI": {"tldr": "OrdShap \u662f\u4e00\u79cd\u65b0\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u533a\u5206\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7279\u5f81\u503c\u548c\u7279\u5f81\u4f4d\u7f6e\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u5bf9\u5176\u884c\u4e3a\u7684\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u5f52\u56e0\u6280\u672f\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4f46\u5b83\u4eec\u56fa\u6709\u5730\u5047\u8bbe\u7279\u5f81\u6392\u5e8f\u56fa\u5b9a\uff0c\u6df7\u6dc6\u4e86\uff081\uff09\u7279\u5f81\u503c\u548c\uff082\uff09\u5b83\u4eec\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 OrdShap\u3002", "method": "OrdShap \u901a\u8fc7\u7f6e\u6362\u7279\u5f81\u4f4d\u7f6e\u6765\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u7684\u53d8\u5316\uff0c\u4ece\u800c\u533a\u5206\u7279\u5f81\u503c\u548c\u7279\u5f81\u4f4d\u7f6e\u7684\u5f71\u54cd\u3002OrdShap \u4e0e Sanchez-Berganti\n\nos \u503c\u4e4b\u95f4\u5b58\u5728\u535a\u5f08\u8bba\u8054\u7cfb\uff0c\u4e3a\u4f4d\u7f6e\u654f\u611f\u5f52\u56e0\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u5408\u7406\u7684\u65b9\u6cd5\u3002", "result": "OrdShap \u5728\u5065\u5eb7\u3001\u81ea\u7136\u8bed\u8a00\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u7ecf\u9a8c\u7ed3\u679c\u7a81\u663e\u4e86 OrdShap \u5728\u6355\u6349\u7279\u5f81\u503c\u548c\u7279\u5f81\u4f4d\u7f6e\u5f52\u56e0\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002", "conclusion": "OrdShap \u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u5bf9\u7f6e\u6362\u7279\u5f81\u4f4d\u7f6e\u7684\u54cd\u5e94\u6765\u533a\u5206\u8fd9\u4e9b\u6548\u5e94\uff0c\u5728\u5065\u5eb7\u3001\u81ea\u7136\u8bed\u8a00\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u7ecf\u9a8c\u7ed3\u679c\u7a81\u663e\u4e86 OrdShap \u5728\u6355\u6349\u7279\u5f81\u503c\u548c\u7279\u5f81\u4f4d\u7f6e\u5f52\u56e0\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.12083", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12083", "abs": "https://arxiv.org/abs/2507.12083", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Xuesong Chen", "Xu Liu", "Shaojie Shen"], "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "comment": "Accepted by ICCV 2025", "summary": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5148\u63a8\u7406\uff0c\u540e\u9884\u6d4b\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u660e\u786e\u7eb3\u5165\u884c\u4e3a\u610f\u56fe\u4f5c\u4e3a\u7a7a\u95f4\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u7cbe\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\uff0c\u5bf9\u9053\u8def\u4ea4\u901a\u4ee3\u7406\u8fdb\u884c\u8fd0\u52a8\u9884\u6d4b\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u4e5f\u662f\u5173\u952e\u9700\u6c42\u3002\u4e0e\u5927\u591a\u6570\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u7684\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u7814\u7a76\u4ece\u89c4\u5212\u7684\u89d2\u5ea6\u91cd\u65b0\u601d\u8003\u4e86\u8fd9\u9879\u4efb\u52a1\uff0c\u63d0\u5021\u201c\u5148\u63a8\u7406\uff0c\u540e\u9884\u6d4b\u201d\u7684\u7b56\u7565\uff0c\u5c06\u884c\u4e3a\u610f\u56fe\u660e\u786e\u5730\u4f5c\u4e3a\u7a7a\u95f4\u6307\u5bfc\u7eb3\u5165\u8f68\u8ff9\u9884\u6d4b\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5c06\u4ea4\u901a\u4ee3\u7406\u548c\u573a\u666f\u5143\u7d20\u7f16\u7801\u4e3a\u7edf\u4e00\u7684\u5411\u91cf\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u4ee5\u67e5\u8be2\u4e3a\u4e2d\u5fc3\u7684\u8303\u4f8b\u805a\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u901a\u8fc7\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u63a8\u5bfc\u51fa\u5956\u52b1\u5206\u5e03\uff0c\u8be5\u5206\u5e03\u662f\u76ee\u6807\u4ee3\u7406\u5728\u7ed9\u5b9a\u573a\u666f\u4e0a\u4e0b\u6587\u4e2d\u7684\u884c\u4e3a\u7684\u7d27\u51d1\u800c\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u793a\u3002\u5728\u5956\u52b1\u542f\u53d1\u5f0f\u6307\u5bfc\u4e0b\uff0c\u8be5\u65b9\u6cd5\u6267\u884c\u7b56\u7565\u6eda\u52a8\u4ee5\u63a8\u7406\u591a\u4e2a\u53ef\u80fd\u7684\u610f\u56fe\uff0c\u4e3a\u540e\u7eed\u7684\u8f68\u8ff9\u751f\u6210\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5148\u9a8c\u3002\u6700\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u53cc\u5411\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u7c7b\u4f3cDETR\u7684\u5c42\u6b21\u5316\u89e3\u7801\u5668\uff0c\u4ee5\u751f\u6210\u51c6\u786e\u7684\u672a\u6765\u8f68\u8ff9\u53ca\u5176\u76f8\u5173\u7684\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u53d6\u5f97\u4e86\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728Argoverse\u548cnuScenes\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u5ea6\u7ade\u4e89\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12075", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12075", "abs": "https://arxiv.org/abs/2507.12075", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Llu\u00eds Huguet Cabot", "Roberto Navigli"], "title": "BOOKCOREF: Coreference Resolution at Book Scale", "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 BOOKCOREF\uff0c\u4e00\u4e2a\u5305\u542b\u5168\u7bc7\u53d9\u4e8b\u6587\u672c\uff08\u5e73\u5747\u957f\u5ea6\u8d85\u8fc7 200,000 \u8bcd\u5143\uff09\u7684\u5f00\u521b\u6027\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6307\u4ee3\u6d88\u89e3\u7cfb\u7edf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u4e66\u672c\u89c4\u6a21\u7684\u6587\u672c\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u548c\u673a\u4f1a\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\uff08\u5982 LitBank\uff09\u5728\u8bc4\u4f30\u957f\u6587\u672c\uff08\u5c24\u5176\u662f\u4e66\u672c\u89c4\u6a21\uff09\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6307\u4ee3\u6d88\u89e3\u7cfb\u7edf\u5728\u5904\u7406\u5305\u542b\u6570\u5341\u4e07\u4e2a\u8bcd\u5143\u7684\u8d85\u957f\u6587\u672c\u65f6\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u5728\u5168\u7bc7\u53d9\u4e8b\u6587\u672c\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6307\u4ee3\u6d88\u89e3\u6ce8\u91ca\uff0c\u5e76\u5229\u7528\u8be5\u6d41\u6c34\u7ebf\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u4e66\u672c\u89c4\u6a21\u7684\u6307\u4ee3\u6d88\u89e3\u57fa\u51c6 BOOKCOREF\uff0c\u5e73\u5747\u6587\u6863\u957f\u5ea6\u8d85\u8fc7 200,000 \u4e2a\u8bcd\u5143\u3002", "result": "BOOKCOREF \u662f\u7b2c\u4e00\u4e2a\u4e66\u672c\u89c4\u6a21\u7684\u6307\u4ee3\u6d88\u89e3\u57fa\u51c6\uff0c\u5e73\u5747\u6587\u6863\u957f\u5ea6\u8d85\u8fc7 200,000 \u4e2a\u8bcd\u5143\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 BOOKCOREF \u4e0a\u8bc4\u4f30\u65f6\uff0c\u5f53\u524d\u7684\u6307\u4ee3\u6d88\u89e3\u7cfb\u7edf\u53ef\u4ee5\u63d0\u9ad8\u9ad8\u8fbe +20 CoNLL-F1 \u5206\u6570\u3002\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u4e66\u672c\u89c4\u6a21\u7684\u6307\u4ee3\u6d88\u89e3\u6240\u5e26\u6765\u7684\u65b0\u6311\u6218\uff0c\u5e76\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e00\u89c4\u6a21\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "conclusion": "\u73b0\u6709\u9488\u5bf9\u957f\u6587\u6863\u7684\u6307\u4ee3\u6d88\u89e3\u7cfb\u7edf\u5728\u5168\u4e66\u89c4\u6a21\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u9700\u8981\u65b0\u7684\u6a21\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.11994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11994", "abs": "https://arxiv.org/abs/2507.11994", "authors": ["Jun Yin", "Fei Wu", "Yupeng Ren", "Jisheng Huang", "Qiankun Li", "Heng jin", "Jianhai Fu", "Chanjie Cui"], "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation", "comment": "IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.),\n  cuichj@mail2.sysu.edu.cn (C.C.)", "summary": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.", "AI": {"tldr": "\u63d0\u51faSAMST\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528SAM\u6a21\u578b\u5904\u7406\u9065\u611f\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u548cSAM\u4f2a\u6807\u7b7e\u4f18\u5316\u5668\u63d0\u5347\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528\u6d77\u91cf\u7684\u672a\u6807\u8bb0\u9065\u611f\u6570\u636e\uff0c\u5e76\u89e3\u51b3\u516c\u5171\u9065\u611f\u6570\u636e\u96c6\u5728\u6cdb\u5316\u6027\u65b9\u9762\u56e0\u5206\u8fa8\u7387\u53d8\u5316\u548c\u571f\u5730\u8986\u76d6\u7c7b\u522b\u5b9a\u4e49\u4e0d\u4e00\u81f4\u800c\u9762\u4e34\u7684\u9650\u5236\uff0c\u63d0\u51faSAMST\u65b9\u6cd5\u3002", "method": "SAMST\u662f\u4e00\u79cd\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u8fed\u4ee3\u5730\u4f18\u5316\u4f2a\u6807\u7b7e\uff1a1. \u4f7f\u7528\u6807\u7b7e\u548c\u4f2a\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u76d1\u7763\u6a21\u578b\u81ea\u8bad\u7ec3\uff1b2. \u57fa\u4e8eSAM\u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u5668\u3002\u4f2a\u6807\u7b7e\u4f18\u5316\u5668\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u7528\u4e8e\u9884\u5904\u7406\u7684\u9608\u503c\u8fc7\u6ee4\u6a21\u5757\u3001\u7528\u4e8e\u63d0\u53d6\u8fde\u63a5\u533a\u57df\u548c\u4e3aSAM\u751f\u6210\u63d0\u793a\u7684\u63d0\u793a\u751f\u6210\u6a21\u5757\uff0c\u4ee5\u53ca\u7528\u4e8e\u6700\u7ec8\u6807\u7b7e\u7f1d\u5408\u7684\u6807\u7b7e\u4f18\u5316\u6a21\u5757\u3002", "result": "SAMST\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002\u5728Potsdam\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86SAMST\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "SAMST\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408SAM\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u76d1\u7763\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u7684\u51c6\u786e\u6027\uff0c\u8fdb\u800c\u63d0\u5347\u4e86\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002\u5728Potsdam\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SAMST\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u9065\u611f\u8bed\u4e49\u5206\u5272\u4e2d\u6709\u9650\u6807\u7b7e\u6570\u636e\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12189", "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.12189", "abs": "https://arxiv.org/abs/2507.12189", "authors": ["Azhar Ikhtiarudin", "Aditi Das", "Param Thakkar", "Akash Kundu"], "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search", "comment": "Comprehensive RL agent benchmark for QAS. Contributions are welcomed\n  here: https://github.com/azhar-ikhtiarudin/bench-rlqas", "summary": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.", "AI": {"tldr": "BenchRL-QAS is a new framework to benchmark RL algorithms for quantum architecture search (QAS). It tested 9 RL agents on 4 quantum tasks and found that RL classifiers work well, but no single RL algorithm is best for all QAS tasks. Performance depends on the specific task, qubit count, and noise, so choosing the right RL algorithm is important.", "motivation": "To systematically evaluate reinforcement learning (RL) algorithms in quantum architecture search (QAS) across diverse tasks and system sizes, addressing the need for fair and comprehensive comparison.", "method": "Introduced BenchRL-QAS, a unified benchmarking framework for evaluating RL algorithms in QAS across diverse tasks and system sizes (2- to 8-qubit). Benchmarked nine RL agents (value-based and policy-gradient) on tasks like VQE, VQD, quantum classification, and state preparation, in both noiseless and noisy regimes. Proposed a weighted ranking metric balancing accuracy, circuit depth, gate count, and computational efficiency.", "result": "RL-based quantum classifiers outperformed baseline variational classifiers. Algorithmic performance in QAS is highly context-dependent, varying with task structure, qubit count, and noise, supporting the \"no free lunch\" principle in RL-based quantum circuit design.", "conclusion": "RL-based quantum classifier outperforms baseline variational classifiers. No single RL algorithm is universally optimal for QAS tasks; performance is context-dependent, highlighting the need for tailored algorithm selection and systematic benchmarking."}}
{"id": "2507.11865", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11865", "abs": "https://arxiv.org/abs/2507.11865", "authors": ["Hanwen Dai", "Chang Gao", "Fang He", "Congyuan Ji", "Yanni Yang"], "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "comment": null, "summary": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "AI": {"tldr": "\u4e3a\u4e86\u4f18\u5316\u7f51\u7ea6\u8f66\u5e73\u53f0\u4e0a\u7684\u6298\u6263\u5feb\u8f66\u670d\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3api-DDPG\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u7b56\u7565\u5b66\u4e60\u548c\u65f6\u7a7a\u6a21\u5f0f\u6355\u6349\uff0c\u80fd\u6709\u6548\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u670d\u52a1\u7684\u51b3\u7b56\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u65e9\u671f\u9636\u6bb5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u7f51\u7ea6\u8f66\u5e73\u53f0\u6574\u5408\u5e26\u6765\u7684\u5e02\u573a\u788e\u7247\u5316\u95ee\u9898\uff0c\u9700\u8981\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u6298\u6263\u5feb\u8f66\u670d\u52a1\u7684\u884c\u4e3a\u3002\u7136\u800c\uff0c\u65b0\u4e1a\u52a1\u6a21\u5f0f\u4e0b\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\uff0c\u9700\u8981\u5728\u7ebf\u5b66\u4e60\u3002\u4f46\u65e9\u671f\u63a2\u7d22\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u65e9\u671f\u6027\u80fd\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u52a8\u6001\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u6298\u6263\u5feb\u8f66\u670d\u52a1\u7684\u6bd4\u4f8b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3api-DDPG\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\uff1a1. \u7b56\u7565\u6539\u8fdb\u6a21\u5757\uff1a\u7528\u4e8e\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u7b56\u7565\u6027\u80fd\u30022. \u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08CLSTM\uff09\uff1a\u7528\u4e8e\u6355\u6349\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\u30023. \u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u673a\u5236\uff1a\u7528\u4e8e\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u8be5\u6846\u67b6\u5c06\u53f8\u673a\u63a5\u53d7\u6298\u6263\u5feb\u8f66\u670d\u52a1\u7684\u6bd4\u4f8b\u51b3\u7b56\u5236\u5b9a\u4e3a\u4e00\u4e2a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u7684\u6a21\u62df\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cpi-DDPG\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u65e9\u671f\u8bad\u7ec3\u7684\u635f\u5931\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3api-DDPG\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7b56\u7565\u6539\u8fdb\u6a21\u5757\u3001\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u673a\u5236\uff0c\u6765\u52a8\u6001\u7ba1\u7406\u7f51\u7ea6\u8f66\u5e73\u53f0\u53f8\u673a\u7684\u6298\u6263\u5feb\u8f66\u670d\u52a1\u63a5\u53d7\u884c\u4e3a\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u7a00\u758f\u3001\u9ad8\u968f\u673a\u6027\u548c\u4e0d\u900f\u660e\u7684\u5339\u914d\u673a\u5236\u7b49\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cpi-DDPG\u5728\u5b66\u4e60\u6548\u7387\u548c\u65e9\u671f\u8bad\u7ec3\u635f\u5931\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12414", "abs": "https://arxiv.org/abs/2507.12414", "authors": ["Santosh Vasa", "Aditi Ramadwar", "Jnana Rama Krishna Darabattula", "Md Zafar Anwar", "Stanislaw Antol", "Andrei Vatavu", "Thomas Monninger", "Sihao Ding"], "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "comment": null, "summary": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12079", "abs": "https://arxiv.org/abs/2507.12079", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEGA\u7684\u65b0\u578b\u6570\u5b66\u6559\u5b66\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u63d0\u5347\u5b66\u751f\u6570\u5b66\u5b66\u4e60\u4f53\u9a8c\u548c\u89e3\u91ca\u56f0\u96be\u6570\u5b66\u95ee\u9898\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u9010\u6b65\u6559\u5b66\u6cd5\u3002", "motivation": "\u90e8\u5206\u5b66\u751f\u5728\u6570\u5b66\u5b66\u4e60\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u4ed6\u4eec\u56de\u907f\u6570\u5b66\u76f8\u5173\u5b66\u79d1\uff0c\u5c3d\u7ba1\u6570\u5b66\u5728\u5305\u62ec\u4fe1\u53f7\u5904\u7406\u5728\u5185\u7684\u8bb8\u591a\u9886\u57df\u90fd\u81f3\u5173\u91cd\u8981\u3002\u5b66\u751f\u6570\u5b66\u56f0\u96be\u7684\u6839\u6e90\u5f80\u5f80\u5728\u4e8e\u6b21\u4f18\u7684\u6559\u5b66\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u82cf\u683c\u62c9\u5e95\u65b9\u6cd5\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u3001\u7b80\u5316\u6e38\u620f\u5316\u548c\u5f62\u6210\u6027\u53cd\u9988\u7684\u5e72\u9884\u7814\u7a76\uff0c\u79f0\u4e3aMEGA\uff08Mathematics Explanations through Games by AI LLMs\uff09\u3002\u7814\u7a76\u901a\u8fc7\u7ec4\u5185\u8bbe\u8ba1\uff0c\u5728GSM8K\u548cMATH\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4\u8f83\u4e86MEGA\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u9010\u6b65\uff08CoT\uff09\u65b9\u6cd5\u5728\u5927\u5b66\u751f\u7684\u6570\u5b66\u5b66\u4e60\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u5b66\u751f\u4eec\u666e\u904d\u8ba4\u4e3aMEGA\u65b9\u6cd5\u5728\u5b66\u4e60\u4f53\u9a8c\u4e0a\u4f18\u4e8eCoT\u65b9\u6cd5\u3002\u5728\u66f4\u56f0\u96be\u7684MATH\u6570\u636e\u96c6\u4e0a\uff0cMEGA\u65b9\u6cd5\u7684\u5b66\u751f\u8ba4\u540c\u5ea6\uff0847.5%\uff09\u8fdc\u9ad8\u4e8eCoT\u65b9\u6cd5\uff0826.67%\uff09\uff0c\u8868\u660eMEGA\u65b9\u6cd5\u5728\u89e3\u91ca\u56f0\u96be\u6570\u5b66\u95ee\u9898\u65b9\u9762\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "MEGA\u65b9\u6cd5\u5728\u89e3\u91ca\u56f0\u96be\u6570\u5b66\u95ee\u9898\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u9010\u6b65\uff08CoT\uff09\u65b9\u6cd5\uff0c\u5728\u66f4\u96be\u7684MATH\u6570\u636e\u96c6\u4e0a\uff0cMEGA\u65b9\u6cd5\u7684\u5b66\u751f\u8ba4\u540c\u5ea6\u4e3a47.5%\uff0c\u800cCoT\u65b9\u6cd5\u4e3a26.67%\u3002"}}
{"id": "2507.12001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12001", "abs": "https://arxiv.org/abs/2507.12001", "authors": ["Hao Li", "Ju Dai", "Feng Zhou", "Kaida Ning", "Lei Li", "Junjun Pan"], "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation", "comment": "ICCV 2025", "summary": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 AUBlendSet \u6570\u636e\u96c6\u548c AUBlendNet \u7f51\u7edc\uff0c\u89e3\u51b3\u4e86 3D \u9762\u90e8\u8868\u60c5\u64cd\u63a7\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u5316\u9762\u90e8\u8868\u60c5\u64cd\u63a7\u3002", "motivation": "\u73b0\u6709\u7684 3D \u9762\u90e8\u52a8\u753b\u6280\u672f\u5728\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u98ce\u683c\u5316 3D \u9762\u90e8\u8868\u60c5\u64cd\u63a7\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa AUBlendSet \u6570\u636e\u96c6\uff0c\u5305\u542b 32 \u4e2a\u9762\u90e8\u52a8\u4f5c\u5355\u5143 (AUs) \u7684\u878d\u5408\u5f62\u72b6\u6570\u636e\uff0c\u8986\u76d6 500 \u4e2a\u8eab\u4efd\u3002\u63d0\u51fa AUBlendNet \u7f51\u7edc\uff0c\u5b66\u4e60\u4e0d\u540c\u98ce\u683c\u7684 AU-\u878d\u5408\u5f62\u72b6\u57fa\u5411\u91cf\uff0c\u80fd\u591f\u4e3a\u7ed9\u5b9a\u7684\u8eab\u4efd\u7f51\u683c\u9884\u6d4b\u76f8\u5e94\u98ce\u683c\u7684 AU-\u878d\u5408\u5f62\u72b6\u57fa\u5411\u91cf\uff0c\u5b9e\u73b0\u98ce\u683c\u5316\u7684 3D \u60c5\u7eea\u9762\u90e8\u64cd\u63a7\u3002", "result": "AUBlendSet \u548c AUBlendNet \u5728\u98ce\u683c\u5316\u9762\u90e8\u8868\u60c5\u64cd\u63a7\u3001\u8bed\u97f3\u9a71\u52a8\u60c5\u7eea\u9762\u90e8\u52a8\u753b\u548c\u60c5\u7eea\u8bc6\u522b\u6570\u636e\u589e\u5f3a\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u5176\u5728 3D \u9762\u90e8\u52a8\u753b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u548c\u91cd\u8981\u6027\u3002", "conclusion": "AUBlendSet \u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u7cbe\u7ec6\u5316\u9762\u90e8\u8868\u60c5\u64cd\u63a7\u7684 3D \u9762\u90e8\u6570\u636e\u96c6\uff0cAUBlendNet \u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u901a\u8fc7\u9762\u90e8 AU \u5b9e\u73b0\u4efb\u610f\u8eab\u4efd\u7684\u8fde\u7eed 3D \u9762\u90e8\u8868\u60c5\u64cd\u63a7\u7684\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u9762\u90e8\u8868\u60c5\u64cd\u63a7\u3001\u8bed\u97f3\u9a71\u52a8\u60c5\u7eea\u9762\u90e8\u52a8\u753b\u548c\u60c5\u7eea\u8bc6\u522b\u6570\u636e\u589e\u5f3a\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12200", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12200", "abs": "https://arxiv.org/abs/2507.12200", "authors": ["Markus Teller", "Susana Plascencia", "Cristina Sastre Jachimska", "Samuele Grandi", "Hugues de Riedmatten"], "title": "A solid-state temporally multiplexed quantum memory array at the single-photon level", "comment": "18 pages, 6 figures, supplementary material (2 pages, 1 figure)", "summary": "The exploitation of multimodality in different degrees of freedom is one of\nthe most promising ways to increase the rate of heralded entanglement between\ndistant quantum nodes. In this paper, we realize a spatially-multiplexed\nsolid-state quantum memory array with ten individually controllable spin-wave\nmemory cells featuring on-demand read-out and temporal multiplexing. By\ncombining spatial and temporal multiplexing, we store weak coherent pulses at\nthe single-photon level in up to 250 spatio-temporal modes, with an average\nsignal-to-noise ratio of 10(2). We perform a thorough characterization of the\nwhole system, including its multiplexing and demultiplexing stage. We verify\nthat the memory array exhibits low cross-talk even at the single-photon level.\nThe measured performance indicates readiness for storing non-classical states\nand promises a speed-up in entanglement distribution rates.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u56fa\u6001\u91cf\u5b50\u5185\u5b58\u9635\u5217\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u590d\u7528\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5355\u5149\u5b50\u7ea7\u522b\u7684\u5f31\u76f8\u5e72\u8109\u51b2\u5b58\u50a8\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4f4e\u4e32\u6270\u548c\u6309\u9700\u8bfb\u51fa\u7684\u80fd\u529b\uff0c\u4e3a\u63d0\u9ad8\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u7ea0\u7f20\u5206\u53d1\u901f\u7387\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "motivation": "\u5229\u7528\u4e0d\u540c\u81ea\u7531\u5ea6\u4e0b\u7684\u591a\u6a21\u6001\u662f\u63d0\u9ad8\u8fdc\u7a0b\u91cf\u5b50\u8282\u70b9\u4e4b\u95f4\u5df2\u5ba3\u544a\u7684\u7ea0\u7f20\u901f\u7387\u7684\u6700\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u590d\u7528\uff0c\u5728\u591a\u8fbe 250 \u4e2a\u65f6\u7a7a\u6a21\u5f0f\u4e2d\u5b58\u50a8\u4e86\u5355\u5149\u5b50\u7ea7\u522b\u7684\u5f31\u76f8\u5e72\u8109\u51b2\uff0c\u5e73\u5747\u4fe1\u566a\u6bd4\u4e3a 10^2\uff0c\u5e76\u5bf9\u6574\u4e2a\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7279\u6027\u5206\u6790\uff0c\u5305\u62ec\u5176\u590d\u7528\u548c\u89e3\u590d\u7528\u9636\u6bb5\u3002", "result": "\u5b9e\u73b0\u4e86\u5177\u6709 10 \u4e2a\u5355\u72ec\u53ef\u63a7\u5236\u7684\u3001\u5177\u6709\u6309\u9700\u8bfb\u51fa\u548c\u65f6\u95f4\u590d\u7528\u7684\u81ea\u65cb\u6ce2\u5185\u5b58\u5355\u5143\u7684\u3001\u7a7a\u95f4\u590d\u7528\u7684\u56fa\u6001\u91cf\u5b50\u5185\u5b58\u9635\u5217\u3002", "conclusion": "\u8be5\u5185\u5b58\u9635\u5217\u5177\u6709\u4f4e\u4e32\u6270\uff0c\u5373\u4f7f\u5728\u5355\u5149\u5b50\u7ea7\u522b\u4e5f\u80fd\u4e3a\u5b58\u50a8\u975e\u7ecf\u5178\u72b6\u6001\u505a\u597d\u51c6\u5907\uff0c\u5e76\u6709\u671b\u63d0\u9ad8\u7ea0\u7f20\u5206\u53d1\u901f\u7387\u3002"}}
{"id": "2507.11901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11901", "abs": "https://arxiv.org/abs/2507.11901", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Imbalanced Regression Pipeline Recommendation", "comment": null, "summary": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Meta-IR \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u5143\u5b66\u4e60\u6765\u81ea\u52a8\u9009\u62e9\u6700\u4f73\u7684\u91cd\u91c7\u6837\u548c\u5b66\u4e60\u6a21\u578b\u7ec4\u5408\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c AutoML \u6846\u67b6\u3002", "motivation": "\u4e0d\u5e73\u8861\u95ee\u9898\u5e7f\u6cdb\u5b58\u5728\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5404\u79cd\u573a\u666f\u4e2d\uff0c\u5e76\u4e14\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7814\u7a76\u3002\u7136\u800c\uff0c\u7531\u4e8e\u67d0\u4e9b\u76ee\u6807\u503c\u7684\u7a00\u6709\u6027\uff0c\u5b83\u4eec\u4e5f\u7ed9\u56de\u5f52\u4efb\u52a1\u5e26\u6765\u4e86\u6311\u6218\u3002\u5e38\u89c1\u7684\u66ff\u4ee3\u65b9\u6cd5\u662f\u91c7\u7528\u9884\u5904\u7406\u4e2d\u7684\u5e73\u8861\u7b97\u6cd5\u6765\u89e3\u51b3\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u4f46\u7531\u4e8e\u91cd\u91c7\u6837\u65b9\u6cd5\u548c\u5b66\u4e60\u6a21\u578b\u7684\u5404\u79cd\u7ec4\u5408\uff0c\u786e\u5b9a\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u9700\u8981\u6d4b\u8bd5\u5927\u91cf\u7684\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u90fd\u4f1a\u5f71\u54cd\u6700\u4f73\u7b56\u7565\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u9009\u62e9\u6700\u4f73\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cMeta-learning for Imbalanced Regression (Meta-IR)\u201d\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u4e8e\u73b0\u6709\u7684\u6587\u732e\uff0c\u800c\u662f\u901a\u8fc7\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u6765\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5728\u96f6\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u63a8\u8350\u6700\u4f73\u7684\u91cd\u91c7\u6837\u7b56\u7565\u548c\u5b66\u4e60\u6a21\u578b\u7ec4\u5408\u3002\u5143\u5206\u7c7b\u5668\u5229\u7528\u4e00\u7ec4\u5143\u7279\u5f81\u6765\u5b66\u4e60\u5982\u4f55\u5c06\u8fd9\u4e9b\u5143\u7279\u5f81\u6620\u5c04\u5230\u6307\u793a\u6700\u4f73\u7ba1\u9053\u7684\u7c7b\u522b\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u201cIndependent\u201d \u65b9\u6cd5\u5206\u522b\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u6765\u6307\u793a\u6700\u4f73\u5b66\u4e60\u7b97\u6cd5\u548c\u91cd\u91c7\u6837\u7b56\u7565\uff1b\u201cChained\u201d \u65b9\u6cd5\u5219\u901a\u8fc7\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u6b65\u9aa4\u7684\u8fc7\u7a0b\uff0c\u5c06\u4e00\u4e2a\u5143\u5206\u7c7b\u5668\u7684\u8f93\u51fa\u4f5c\u4e3a\u53e6\u4e00\u4e2a\u7684\u8f93\u5165\uff0c\u4ee5\u6a21\u62df\u5185\u5728\u7684\u5173\u7cfb\u56e0\u7d20\u3002", "result": "\u4e0e AutoML \u6846\u67b6\u76f8\u6bd4\uff0c\u201cMeta-IR\u201d \u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u4e0e\u5305\u62ec\u516d\u79cd\u5b66\u4e60\u7b97\u6cd5\u548c\u516d\u79cd\u91cd\u91c7\u6837\u7b97\u6cd5\uff08\u52a0\u4e0a\u65e0\u91cd\u91c7\u6837\uff0c\u603b\u5171 42 \u79cd\u914d\u7f6e\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u201cMeta-IR\u201d \u8868\u73b0\u66f4\u4f18\u3002\u7279\u522b\u662f\uff0c\u201cChained\u201d \u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8868\u660e\u5b66\u4e60\u7b97\u6cd5\u548c\u91cd\u91c7\u6837\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u7684\u5185\u5728\u5173\u7cfb\u3002", "conclusion": "\"Meta-IR\" \u6846\u67b6\u5728\u5904\u7406\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u65f6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u6765\u63a8\u8350\u6700\u4f73\u7684\u91cd\u91c7\u6837\u7b56\u7565\u548c\u5b66\u4e60\u6a21\u578b\u7ec4\u5408\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8e AutoML \u6846\u67b6\u548c\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5176\u4e2d\uff0c\u201cChained\u201d \u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u5b66\u4e60\u7b97\u6cd5\u548c\u91cd\u91c7\u6837\u7b56\u7565\u4e4b\u95f4\u7684\u5185\u5728\u5173\u7cfb\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12126", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12126", "abs": "https://arxiv.org/abs/2507.12126", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u6587\u672c\u589e\u5f3a\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8NLP\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u6307\u51fa\u4e86GPT-3.5 Turbo\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6570\u636e\u589e\u5f3a\u6280\u672f\u5728\u8bed\u4e49\u4fdd\u6301\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u8fed\u4ee3\u751f\u6210\u573a\u666f\u4e0b\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u548c\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u201c\u53ef\u6269\u5c55\u6027\u5206\u6790\u201d\u548c\u201c\u5e26\u6458\u8981\u7cbe\u70bc\u7684\u8fed\u4ee3\u589e\u5f3a\uff08IASR\uff09\u201d\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u6587\u672c\u589e\u5f3a\u7684\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e3b\u9898\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528GPT\u589e\u5f3a\u7684\u5c11\u6837\u672c\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86400%\u7684\u4e3b\u9898\u7c92\u5ea6\u589e\u957f\uff0c\u5e76\u6d88\u9664\u4e86\u4e3b\u9898\u91cd\u53e0\u3002GPT-3.5 Turbo\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u751f\u6210\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6587\u672c\u589e\u5f3a\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u7684NLP\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.12006", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12006", "abs": "https://arxiv.org/abs/2507.12006", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "comment": "Accepted by ICCV 2025", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\n\\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.", "AI": {"tldr": "FDAM \u662f\u4e00\u79cd\u7535\u8def\u7406\u8bba\u542f\u53d1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u8f6c\u6ce8\u610f\u529b\u548c\u52a8\u6001\u7f29\u653e\u6765\u89e3\u51b3 ViT \u7684\u9891\u7387\u8870\u51cf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 Vision Transformer (ViT) \u6a21\u578b\u867d\u7136\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6ce8\u610f\u529b\u673a\u5236\u4f1a\u5bfc\u81f4\u6bcf\u5c42\u90fd\u8868\u73b0\u4e3a\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u5806\u53e0\u7684\u5c42\u7ed3\u6784\u4f1a\u9020\u6210\u9891\u7387\u8870\u51cf\uff0c\u4ece\u800c\u4e22\u5931\u5173\u952e\u7684\u7ec6\u8282\u548c\u7eb9\u7406\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9891\u57df\u52a8\u6001\u6ce8\u610f\u529b\u8c03\u5236\u201d\uff08FDAM\uff09\u7684\u65b0\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53d7\u7535\u8def\u7406\u8bba\u542f\u53d1\uff0c\u5305\u542b\u201c\u6ce8\u610f\u529b\u53cd\u8f6c\u201d\uff08AttInv\uff09\u548c\u201c\u9891\u57df\u52a8\u6001\u7f29\u653e\u201d\uff08FreqScale\uff09\u4e24\u79cd\u6280\u672f\u3002AttInv \u901a\u8fc7\u53cd\u8f6c\u6ce8\u610f\u529b\u77e9\u9635\u4e2d\u7684\u4f4e\u901a\u6ee4\u6ce2\u5668\u6765\u751f\u6210\u4e92\u8865\u7684\u9ad8\u901a\u6ee4\u6ce2\uff0c\u5e76\u52a8\u6001\u5730\u7ec4\u5408\u4e24\u8005\u3002FreqScale \u7528\u4e8e\u5bf9\u4e0d\u540c\u9891\u7387\u5206\u91cf\u8fdb\u884c\u52a0\u6743\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u76ee\u6807\u54cd\u5e94\u51fd\u6570\u7684\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002", "result": "FDAM \u80fd\u591f\u907f\u514d\u8868\u793a\u574d\u584c\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u76f8\u4f3c\u6027\u5206\u6790\u548c\u6709\u6548\u79e9\u8bc4\u4f30\u5f97\u5230\u8bc1\u5b9e\u3002\u8be5\u65b9\u6cd5\u5728 SegFormer\u3001DeiT \u548c MaskDINO \u7b49\u591a\u79cd\u6a21\u578b\u4e0a\u5747\u5b9e\u73b0\u4e86\u6027\u80fd\u4e00\u81f4\u6027\u63d0\u5347\uff0c\u5e76\u5728\u8bed\u4e49\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u5728\u9065\u611f\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cFDAM \u5728\u5355\u5c3a\u5ea6\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u5148\u8fdb\u6c34\u5e73\u7684\u6210\u679c\u3002", "conclusion": "FDAM \u901a\u8fc7\u5f15\u5165 AttInv \u548c FreqScale \u6280\u672f\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684 Vision Transformer \u6a21\u578b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86 ViT \u4e2d\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u7684\u9891\u7387\u8870\u51cf\u95ee\u9898\uff0c\u907f\u514d\u4e86\u8868\u793a\u574d\u584c\uff0c\u5e76\u5728\u8bed\u4e49\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u9065\u611f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u6c34\u5e73\u7684\u6210\u679c\u3002"}}
{"id": "2507.12223", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2507.12223", "abs": "https://arxiv.org/abs/2507.12223", "authors": ["Olimpia Lombardi", "Sebastian Fortin", "Juan Camilo Martinez Gonzalez", "Hernan Lucas Accorinti"], "title": "What are we talking about when we discuss the Born-Oppenheimer approximation?", "comment": "This is an answer to the article \"On the Quantum Theory of Molecules:\n  Rigour, Idealization, and Uncertainty\" arXiv:2411.01942", "summary": "Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a\ncomprehensive article examining the Born-Oppenheimer Approximation (BOA). Their\ncentral objective is to challenge our position on the matter-namely, that the\nBOA incorporates a classical assumption incompatible with the Heisenberg\nUncertainty Principle. In contrast, HLT contend that the BOA involves no such\nclassical assumption and, as a result, supports the view that chemistry can be\nreduced to physics. The purpose of this paper is to offer a critical analysis\nof the HLT article and to clarify why we consider their arguments unpersuasive.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u5224\u4e86HLT\u5173\u4e8e\u5206\u5b50\u54c8\u5bc6\u987f\u91cf\u7684\u89c2\u70b9\uff0c\u8ba4\u4e3a\u4ed6\u4eec\u7684\u8bba\u70b9\u662f\u4e0d\u53ef\u4fe1\u7684\u3002", "motivation": "\u53cd\u9a73HLT\u5173\u4e8e\u5206\u5b50\u54c8\u5bc6\u987f\u91cf\u7684\u89c2\u70b9\uff0c\u5373\u4ed6\u4eec\u8ba4\u4e3a\u5206\u5b50\u54c8\u5bc6\u987f\u91cf\u53ef\u4ee5\u5b8c\u5168\u5728\u91cf\u5b50\u529b\u5b66\u6846\u67b6\u5185\u8fdb\u884c\u63cf\u8ff0\u3002", "method": "\u5bf9HLT\u7684\u6587\u7ae0\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790", "result": "HLT\u7684\u6587\u7ae0\u548c\u4ed6\u4eec\u7684\u8bba\u70b9\u88ab\u8ba4\u4e3a\u662f\u4e0d\u53ef\u4fe1\u7684\u3002", "conclusion": "\u8be5\u8bba\u6587\u65e8\u5728\u6279\u5224\u6027\u5730\u5206\u6790HLT\u7684\u6587\u7ae0\uff0c\u5e76\u9610\u8ff0\u4e3a\u4ec0\u4e48\u8ba4\u4e3a\u4ed6\u4eec\u7684\u8bba\u70b9\u4e0d\u5177\u8bf4\u670d\u529b\u3002"}}
{"id": "2507.11902", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11902", "abs": "https://arxiv.org/abs/2507.11902", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "comment": null, "summary": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u867d\u7136\u5728\u5206\u7c7b\u9886\u57df\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u76ee\u6807\u503c\u4e3a\u8fde\u7eed\u7684\u56de\u5f52\u4efb\u52a1\u4e5f\u5b58\u5728\u540c\u6837\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u4e2a\u5173\u952e\u6807\u51c6\uff08\u56de\u5f52\u6a21\u578b\u3001\u5b66\u4e60\u8fc7\u7a0b\u548c\u8bc4\u4f30\u6307\u6807\uff09\u7684\u4e0d\u5e73\u8861\u56de\u5f52\u65b9\u6cd5\u5206\u7c7b\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u5305\u62ec\u5404\u79cd\u5e73\u8861\u548c\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u7684\u5404\u79cd\u5e73\u8861\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6307\u6807\u6355\u6349\u4e86\u7528\u6237\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4ee5\u8bc4\u4f30\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u56de\u5f52\u6570\u636e\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0d\u5e73\u8861\u56de\u5f52\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5bf9\u5404\u79cd\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u7684\u4f18\u52bf\uff0c\u5e76\u6307\u660e\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2507.12143", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12143", "abs": "https://arxiv.org/abs/2507.12143", "authors": ["Pavel \u0160indel\u00e1\u0159", "Ond\u0159ej Bojar"], "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "ELOQUENT\u7684Sensemaking\u4efb\u52a1\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u6587\u672c\u548c\u95ee\u7b54\u80fd\u529b\u30022025\u5e74\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u751f\u6210\u95ee\u9898\u3001\u4e25\u683c\u4f9d\u636e\u6587\u672c\u56de\u7b54\u4ee5\u53caLLM\u4f5c\u4e3a\u88c1\u5224\u8bc4\u5206\u65b9\u9762\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002", "motivation": "ELOQUENT\u65e8\u5728\u521b\u5efa\u6613\u4e8e\u6d4b\u8bd5\u7684\u9ad8\u5c42\u6807\u51c6\u6765\u8bc4\u4f30\u751f\u6210\u8bed\u8a00\u6a21\u578b\uff0cSensemaking\u662f\u5176\u4e2d\u7684\u4e00\u9879\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u201c\u7406\u89e3\u7ed9\u5b9a\u6587\u672c\u201d\u7684\u80fd\u529b\uff0c\u5e76\u6a21\u4eff\u8bfe\u5802\u8003\u8bd5\u7684\u4e09\u4e2a\u6b65\u9aa4\uff1a\u51fa\u9898\u3001\u7b54\u9898\u548c\u8bc4\u5206\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86ELOQUENT\u7684Sensemaking\u5171\u4eab\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u6587\u672c\u5e76\u636e\u6b64\u8fdb\u884c\u95ee\u7b54\u7684\u80fd\u529b\u3002\u4efb\u52a1\u5206\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\u7531\u6559\u5e08\u7cfb\u7edf\u751f\u6210\u95ee\u9898\uff0c\u7531\u5b66\u751f\u7cfb\u7edf\u56de\u7b54\u95ee\u9898\uff0c\u7531\u8bc4\u4f30\u7cfb\u7edf\u8bc4\u5206\uff0c\u6240\u6709\u73af\u8282\u5747\u4e25\u683c\u4f9d\u636e\u63d0\u4f9b\u7684\u8f93\u5165\u6750\u6599\u3002\u7814\u7a76\u62a5\u544a\u4e862025\u5e74\u7684Sensemaking\u4efb\u52a1\uff0c\u4f7f\u7528\u4e867\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u6d4b\u8bd5\u6750\u6599\uff08\u5305\u62ec\u4e8b\u5b9e\u6838\u67e5\u5206\u6790\u3001\u6559\u79d1\u4e66\u3001\u8bb2\u5ea7\u5f55\u97f3\u548c\u6559\u80b2\u89c6\u9891\uff09\uff0c\u6db5\u76d6\u82f1\u3001\u5fb7\u3001\u4e4c\u3001\u6377\u56db\u79cd\u8bed\u8a00\u3002\u5171\u67094\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u63d0\u4ea4\u4e862\u4e2a\u6559\u5e08\u7cfb\u7edf\u30012\u4e2a\u5b66\u751f\u7cfb\u7edf\u548c2\u4e2a\u8bc4\u4f30\u7cfb\u7edf\u3002\u7814\u7a76\u8fd8\u4e3a\u6559\u5e08\u548c\u5b66\u751f\u7cfb\u7edf\u589e\u52a0\u4e86\u57fa\u4e8e\u5546\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u540c\u65f6\u4e0e\u6781\u7b80\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u57282025\u5e74\u7684Sensemaking\u4efb\u52a1\u4e2d\uff0c\u7814\u7a76\u53d1\u73b0\u6559\u5e08\u7cfb\u7edf\u5728\u751f\u6210\u95ee\u9898\u65b9\u9762\uff0c\u8bc4\u4f30\u7b56\u7565\u4ecd\u9700\u6539\u8fdb\uff0c\u96be\u4ee5\u533a\u5206\u95ee\u9898\u96c6\u8d28\u91cf\u3002\u5b66\u751f\u7cfb\u7edf\u5728\u56de\u7b54\u95ee\u9898\u65b9\u9762\u6574\u4f53\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u4e25\u683c\u9650\u5236\u4e8e\u7ed9\u5b9a\u6587\u672c\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002\u8bc4\u4f30\u7cfb\u7edf\u5728LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u8303\u5f0f\u4e0b\uff0c\u4f1a\u51fa\u73b0\u5c06\u6df7\u4e71\u7684\u95ee\u7b54\u5bf9\u548c\u7b54\u975e\u6240\u95ee\u7684\u7b54\u6848\u8bef\u5224\u4e3a\u53ef\u63a5\u53d7\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5728ELOQUENT\u7684Sensemaking\u4efb\u52a1\u4e2d\uff0c\u6559\u5e08\u7cfb\u7edf\uff08\u751f\u6210\u95ee\u9898\uff09\u7684\u8bc4\u4f30\u7b56\u7565\u4ecd\u9700\u6539\u8fdb\uff0c\u56e0\u4e3a\u5f88\u96be\u533a\u5206\u4e0d\u540c\u95ee\u9898\u96c6\u7684\u8d28\u91cf\u3002\u5b66\u751f\u7cfb\u7edf\uff08\u56de\u7b54\u95ee\u9898\uff09\u5728LLM\u7684\u52a0\u6301\u4e0b\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u4e25\u683c\u4f9d\u636e\u7ed9\u5b9a\u6587\u672c\u56de\u7b54\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u8bc4\u4f30\u7cfb\u7edf\uff08\u8bc4\u5206\uff09\u5728LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u8303\u5f0f\u4e0b\u5b58\u5728\u7f3a\u9677\uff0c\u4f1a\u5c06\u6df7\u4e71\u7684\u95ee\u7b54\u5bf9\u548c\u7b54\u975e\u6240\u95ee\u7684\u7b54\u6848\u8bef\u5224\u4e3a\u53ef\u63a5\u53d7\u3002"}}
{"id": "2507.12008", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12008", "abs": "https://arxiv.org/abs/2507.12008", "authors": ["Jiawen Wang", "Yinda Chen", "Xiaoyu Liu", "Che Liu", "Dong Liu", "Jianqing Gao", "Zhiwei Xiong"], "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation", "comment": "Accepted by ICML 2025", "summary": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.", "AI": {"tldr": "MaskTwins \u6846\u67b6\u5c06\u63a9\u7801\u91cd\u6784\u89c6\u4e3a\u7a00\u758f\u4fe1\u53f7\u91cd\u6784\u95ee\u9898\uff0c\u7406\u8bba\u8bc1\u660e\u4e92\u8865\u63a9\u7801\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u7684\u80fd\u529b\u66f4\u5f3a\uff0c\u5e76\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u4e92\u8865\u63a9\u7801\u56fe\u50cf\u9884\u6d4b\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u57df\u6cdb\u5316\u548c\u7279\u5f81\u63d0\u53d6\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u5c06\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u4e0e\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u4e2d\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f8\u5173\u8054\uff0c\u4f46\u4ec5\u5c06\u63a9\u7801\u89c6\u4e3a\u8f93\u5165\u56fe\u50cf\u4e0a\u7684\u53d8\u5f62\uff0c\u5ffd\u89c6\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5bfc\u81f4\u5bf9\u63a9\u7801\u91cd\u5efa\u7684\u7406\u89e3\u80a4\u6d45\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6398\u5176\u5728\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u548c\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa MaskTwins \u6846\u67b6\uff0c\u5c06\u63a9\u7801\u91cd\u6784\u76f4\u63a5\u6574\u5408\u5230\u4e3b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u4e92\u8865\u63a9\u7801\u56fe\u50cf\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u6765\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "MaskTwins \u5728\u81ea\u7136\u548c\u751f\u7269\u56fe\u50cf\u5206\u5272\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u53d6\u4e86\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u65e0\u9700\u5355\u72ec\u9884\u8bad\u7ec3\u3002", "conclusion": "MaskTwins \u901a\u8fc7\u5f3a\u5236\u6267\u884c\u4e92\u8865\u63a9\u7801\u9884\u6d4b\u7684\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u8de8\u4e0d\u540c\u57df\u7684\u5185\u5728\u7ed3\u6784\u6a21\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u57df\u6cdb\u5316\u3002\u5b9e\u9a8c\u8bc1\u660e MaskTwins \u5728\u81ea\u7136\u548c\u751f\u7269\u56fe\u50cf\u5206\u5272\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u65e0\u9700\u5355\u72ec\u9884\u8bad\u7ec3\uff0c\u4e3a\u57df\u9002\u5e94\u6027\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\u3002"}}
{"id": "2507.12225", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12225", "abs": "https://arxiv.org/abs/2507.12225", "authors": ["Pavel Babaian", "Gennady Koval"], "title": "Relations between parameters of the Hamiltonian and Neel-type states in the anisotropic Heisenberg model", "comment": "6 pages", "summary": "This article investigates Neel-type states in the anisotropic Heisenberg\nmodel with an external field. For arbitrary spin $s$ and arbitrary dimension of\nthe space $d$ we find the expressions relating the angles which define the\ndirections of spin polarisation to the parameters of the Hamiltonian.", "AI": {"tldr": "The paper analyzes Neel-type states in the anisotropic Heisenberg model with an external field, providing formulas for spin directions based on model parameters for any spin and dimension.", "motivation": "Investigate Neel-type states in the anisotropic Heisenberg model with an external field.", "method": "Investigation of Neel-type states in the anisotropic Heisenberg model with an external field.", "result": "Expressions relating the angles which define the directions of spin polarisation to the parameters of the Hamiltonian.", "conclusion": "We find expressions relating the angles which define the directions of spin polarisation to the parameters of the Hamiltonian for arbitrary spin s and arbitrary dimension of the space d."}}
{"id": "2507.11926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11926", "abs": "https://arxiv.org/abs/2507.11926", "authors": ["Max Hopkins", "Sihan Liu", "Christopher Ye", "Yuichi Yoshida"], "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "comment": "67 pages", "summary": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u590d\u73b0\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u6837\u672c\u590d\u6742\u5ea6\u4ece O(S^7 A^7) \u964d\u4f4e\u5230 O(S^2A)\uff0c\u89e3\u51b3\u4e86\u53ef\u590d\u73b0\u63a2\u7d22\u6bd4\u6279\u91cf\u5b66\u4e60\u66f4\u6602\u8d35\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7ecf\u9a8c\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\u7684\u590d\u73b0\u6027\u5931\u8d25\u95ee\u9898\uff0c\u5bf9\u53ef\u590d\u73b0\u5b66\u4e60\u7b97\u6cd5\u7684\u7814\u7a76\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u63a7\u5236\u8bbe\u7f6e\u4e2d\uff0c\u5b58\u5728\u7740\u6570\u636e\u6548\u7387\u548c\u590d\u73b0\u6027\u7684\u5de8\u5927\u77e5\u8bc6\u7a7a\u767d\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u53ef\u590d\u73b0\u63a2\u7d22\u7684\u6210\u672c\u4ee5\u53ca\u6837\u672c\u6548\u7387\u53ef\u590d\u73b0 RL \u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u590d\u73b0\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5176\u6837\u672c\u590d\u6742\u5ea6\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u6700\u4f18\u4e0a\u754c O(S^7 A^7)\uff0c\u5e76\u8bc1\u660e\u4e86 O(S^2A) \u7684\u4e0b\u754c\uff0c\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u63a5\u8fd1\u6700\u4f18\u3002", "result": "\u672c\u6587\u7684\u4e3b\u8981\u6210\u679c\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u590d\u73b0\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5176\u6837\u672c\u590d\u6742\u5ea6\u4e3a O(S^2A)\uff08\u5728\u4f4e\u65f6\u9650\u8868\u683c\u578b MDPs \u4e0a\uff09\uff0c\u63a5\u8fd1\u4e86\u751f\u6210\u6a21\u578b\u8bbe\u7f6e\u4e0b\u7684 O(S^2A^2) \u548c\u975e\u751f\u6210\u6a21\u578b\u8bbe\u7f6e\u4e0b\u7684 O(S^7 A^7\uff09\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86 O(S^2A) \u7684\u4e0b\u754c\uff08\u5728\u751f\u6210\u6a21\u578b\u8bbe\u7f6e\u4e0b\uff0c\u5177\u6709\u5e76\u884c\u91c7\u6837\u5047\u8bbe\uff09\u548c O(S^2) \u7684\u65e0\u6761\u4ef6\u4e0b\u754c\uff08\u5728\u7247\u6bb5\u5f0f\u8bbe\u7f6e\u4e0b\uff09\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u72b6\u6001\u7a7a\u95f4 S \u65b9\u9762\u7684\u8fd1\u4e4e\u6700\u4f18\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u65f6\u9650\u8868\u683c\u578b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u53ef\u590d\u73b0\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5176\u6837\u672c\u590d\u6742\u5ea6\u4e3a O(S^2A)\uff0c\u8fd9\u663e\u8457\u7f29\u5c0f\u4e86\u751f\u6210\u6a21\u578b\u548c\u975e\u751f\u6210\u6a21\u578b\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63a5\u8fd1\u4e8e\u65e0\u751f\u6210\u6a21\u578b\u60c5\u51b5\u4e0b\u7684\u4e0b\u754c O(S^2)\u3002"}}
{"id": "2507.12208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12208", "abs": "https://arxiv.org/abs/2507.12208", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u884c\u4e3a\u7ffb\u8bd1\u98ce\u683c\u7a7a\u95f4\uff08BTSS\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5206\u5c42\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u6790\u773c\u52a8\u548c\u952e\u76d8\u8bb0\u5f55\u6570\u636e\u6765\u6a21\u62df\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u7684\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u884c\u4e3a\u3002", "motivation": "\u63d0\u51fa\u53ef\u89c2\u5bdf\u7684\u7ffb\u8bd1\u884c\u4e3a\uff08\u773c\u52a8\u548c\u6307\u5c16\u79fb\u52a8\uff09\u662f\u6267\u884c\u7ffb\u8bd1\u7684\u7269\u7406\u884c\u4e3a\u7684\u57fa\u7840\uff0c\u4f46\u5b83\u662f\u7531\u66f4\u9ad8\u5c42\u6b21\u7684\u8ba4\u77e5\u8fc7\u7a0b\u548c\u60c5\u611f\u7ffb\u8bd1\u72b6\u6001\u5f15\u8d77\u548c\u5851\u9020\u7684\u3002", "method": "\u901a\u8fc7\u5206\u6790\u952e\u76d8\u8bb0\u5f55\u548c\u773c\u52a8\u6570\u636e\u4f5c\u4e3a\u9690\u85cf\u5fc3\u7406\u5904\u7406\u7ed3\u6784\u7684\u6307\u6807\uff0c\u5c06\u884c\u4e3a\u6a21\u5f0f\u7ec4\u7ec7\u6210\u591a\u5c42\u5d4c\u5165\u5f0fBTSS\u3002", "result": "\u8be5BTSS\u4e3a\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u8be5\u4ee3\u7406\u53ef\u4ee5\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u60c5\u611f\u3001\u81ea\u52a8\u5316\u884c\u4e3a\u548c\u8ba4\u77e5\u7684 temporal dynamics\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u884c\u4e3a\u7ffb\u8bd1\u98ce\u683c\u7a7a\u95f4\uff08BTSS\uff09\uff0c\u8be5\u7a7a\u95f4\u662f\u4e00\u4e2a\u5206\u5c42\u7ed3\u6784\uff0c\u5305\u542b\u5404\u79cd\u5d4c\u5165\u5f0f\u5904\u7406\u5c42\uff0c\u7528\u4e8e\u63cf\u8ff0\u53ef\u80fd\u7684\u884c\u4e3a\u7ffb\u8bd1\u6a21\u5f0f\u3002"}}
{"id": "2507.12009", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12009", "abs": "https://arxiv.org/abs/2507.12009", "authors": ["Florian David", "Michael Chan", "Elenor Morgenroth", "Patrik Vuilleumier", "Dimitri Van De Ville"], "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "comment": "Accepted in International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC) 2025", "summary": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790fMRI\u6570\u636e\uff0c\u4ee5\u4e86\u89e3\u5927\u8111\u5982\u4f55\u5904\u7406\u7535\u5f71\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "motivation": "\u5229\u7528\u6765\u81ea\u8fde\u7eed\u7535\u5f71\u5e27\u7684\u65f6\u95f4\u76f8\u5173\u8f93\u5165\uff0c\u5e76\u5f25\u5408\u81ea\u7136\u7535\u5f71\u523a\u6fc0\u548cfMRI\u91c7\u96c6\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5229\u7528\u65f6\u95f4\u5377\u79ef\u5c42\u6765\u7f16\u7801\u548c\u89e3\u7801fMRI\u6570\u636e\uff0c\u4ee5\u54cd\u5e94\u81ea\u7136\u4e3b\u4e49\u523a\u6fc0\u3002", "result": "\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u89c6\u89c9\u76ae\u5c42\u5185\u5916\u7684\u795e\u7ecf\u5143\u6d3b\u52a8\uff0c\u5e76\u4ece\u795e\u7ecf\u6d3b\u52a8\u4e2d\u91cd\u5efa\u76f8\u5e94\u7684\u89c6\u89c9\u8f93\u5165\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u89c6\u89c9\u89e3\u7801\u8d21\u732e\u6700\u5927\u7684\u8111\u533a\u662f\u6795\u4e2d\u533a\u3001\u68ad\u72b6\u56de\u533a\u548c\u8ddd\u72b6\u6c9f\u533a\uff0c\u5b83\u4eec\u5206\u522b\u8d1f\u8d23\u5f62\u72b6\u611f\u77e5\u3001\u590d\u6742\u8bc6\u522b\uff08\u7279\u522b\u662f\u9762\u90e8\u8bc6\u522b\uff09\u4ee5\u53ca\u8fb9\u7f18\u548c\u5bf9\u6bd4\u5ea6\u7b49\u57fa\u672c\u89c6\u89c9\u7279\u5f81\u3002", "conclusion": "\u8be5\u6a21\u578b\u8868\u660e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\uff09\u7684\u884c\u4e3a\uff0c\u53ef\u4ee5\u63a2\u7a76\u6211\u4eec\u5bf9\u7535\u5f71\u4e2d\u89c6\u89c9\u5904\u7406\u7684\u7406\u89e3\u3002"}}
{"id": "2507.12250", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.12250", "abs": "https://arxiv.org/abs/2507.12250", "authors": ["Rub\u00e9n Gordillo", "Ricardo Puebla"], "title": "Comment on \"Properties and dynamics of generalized squeezed states\"", "comment": "4 pages, 2 figures, Comment on arXiv: 2411.17022", "summary": "A recent article [S. Ashhab and M. Ayyash, New J. Phys. 27, 054104 (2025)]\nhas reported unexpected oscillatory dynamics in generalized squeezed states of\norder higher than two as their squeezing parameter increases. This behaviour,\nobserved through numerical simulations using truncated bosonic annihilation and\ncreation operators, appeared in several properties of these states, including\ntheir average photon number. The authors argued that these oscillations reflect\na genuine physical effect. Here, however, we demonstrate that the observed\noscillatory behaviour is a consequence of numerical artefacts. A numerical\nanalysis reveals that the oscillations are highly sensitive to the truncation\nof the Fock basis, indicating a lack of convergence. This is further supported\nby a theoretical analysis of the Taylor series of the average photon number,\nsuggesting that these generalized squeezed states contain infinite energy after\na finite value of the squeezing parameter. Finally, we provide an analytical\nproof that the average photon number of any generalized squeezed state is a\nnon-decreasing function, thereby ruling out the possibility of intrinsic\noscillatory dynamics. We hope these results help clarify the origin of the\nreported oscillations and highlight the special care required when dealing with\nhigh-order squeezing states.", "AI": {"tldr": "\u4e00\u7bc7\u58f0\u79f0\u89c2\u5bdf\u5230\u5e7f\u4e49\u538b\u7f29\u6001\u632f\u8361\u52a8\u529b\u5b66\u7684\u6587\u7ae0\u7684\u4f5c\u8005\u9519\u8bef\u5730\u5c06\u6570\u503c\u5236\u54c1\u5f52\u56e0\u4e8e\u7269\u7406\u73b0\u8c61\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6570\u503c\u548c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u8fd9\u4e9b\u632f\u8361\u662f\u7531\u4e8e\u622a\u65ad\u5f15\u8d77\u7684\uff0c\u5e76\u8868\u660e\u5e73\u5747\u5149\u5b50\u6570\u662f\u975e\u9012\u51cf\u7684\uff0c\u4ece\u800c\u53cd\u9a73\u4e86\u8fd9\u4e9b\u73b0\u8c61\u3002", "motivation": "\u53cd\u9a73\u4e00\u7bc7\u58f0\u79f0\u5728\u5e7f\u4e49\u538b\u7f29\u6001\uff08\u4e8c\u9636\u4ee5\u4e0a\uff09\u7684\u538b\u7f29\u53c2\u6570\u589e\u52a0\u65f6\u51fa\u73b0\u610f\u5916\u632f\u8361\u52a8\u529b\u5b66\u7684\u6587\u7ae0\u3002", "method": "\u901a\u8fc7\u5bf9\u5e73\u5747\u5149\u5b50\u6570\u7684\u6cf0\u52d2\u7ea7\u6570\u8fdb\u884c\u6570\u503c\u5206\u6790\uff0c\u5e76\u7ed9\u51fa\u89e3\u6790\u8bc1\u660e\uff0c\u8bc1\u660e\u4e86\u6240\u89c2\u5bdf\u5230\u7684\u632f\u8361\u884c\u4e3a\u662f\u6570\u503c\u622a\u65ad\u5f15\u8d77\u7684\u6570\u503c\u5236\u54c1\uff0c\u800c\u975e\u771f\u5b9e\u7684\u7269\u7406\u6548\u5e94\u3002", "result": "\u6240\u89c2\u5bdf\u5230\u7684\u632f\u8361\u884c\u4e3a\u662f\u6570\u503c\u5236\u54c1\uff0c\u800c\u975e\u771f\u5b9e\u7684\u7269\u7406\u6548\u5e94\uff0c\u56e0\u4e3a\u632f\u8361\u5bf9\u798f\u514b\u57fa\u7684\u622a\u65ad\u975e\u5e38\u654f\u611f\uff0c\u5e76\u4e14\u5e7f\u4e49\u538b\u7f29\u6001\u5728\u6709\u9650\u538b\u7f29\u53c2\u6570\u540e\u5305\u542b\u65e0\u9650\u80fd\u91cf\u3002", "conclusion": "\u5e7f\u4e49\u538b\u7f29\u6001\u7684\u5e73\u5747\u5149\u5b50\u6570\u662f\u5173\u4e8e\u538b\u7f29\u53c2\u6570\u7684\u975e\u9012\u51cf\u51fd\u6570\uff0c\u6392\u9664\u4e86\u5176\u5185\u7980\u7684\u632f\u8361\u52a8\u529b\u5b66\u53ef\u80fd\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u6f84\u6e05\u6240\u62a5\u544a\u632f\u8361\u7684\u6765\u6e90\uff0c\u5e76\u5f3a\u8c03\u5904\u7406\u9ad8\u9636\u538b\u7f29\u6001\u65f6\u9700\u8981\u7279\u522b\u6ce8\u610f\u3002"}}
{"id": "2507.11928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11928", "abs": "https://arxiv.org/abs/2507.11928", "authors": ["Abhishek Sriram", "Neal Tuffy"], "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "comment": "This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)", "summary": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528MaxMin Latin Hypercube Sampling\u548cCatBoost\u68af\u5ea6\u63d0\u5347\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u52a0\u901f\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c04\u9891\u529f\u7387\u653e\u5927\u5668\u8bbe\u8ba1\uff0c\u5c06\u4eff\u771f\u9700\u6c42\u51cf\u5c1165%\uff0c\u540c\u65f6\u4fdd\u6301\u00b10.3\u81f3\u00b10.4 dBm\u7684\u7cbe\u5ea6\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u52a0\u901f\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c04\u9891\u529f\u7387\u653e\u5927\u5668\u8bbe\u8ba1\uff0c\u53ef\u5c06\u4eff\u771f\u9700\u6c42\u51cf\u5c1165%\uff0c\u540c\u65f6\u4fdd\u6301\u00b10.3\u81f3\u00b10.4 dBm\u7684\u7cbe\u5ea6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86MaxMin Latin Hypercube Sampling\u548cCatBoost\u68af\u5ea6\u63d0\u5347\uff0c\u4ee5\u667a\u80fd\u5730\u63a2\u7d22\u591a\u7ef4\u53c2\u6570\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u4e0d\u662f\u901a\u8fc7\u7a77\u4e3e\u6a21\u62df\u6240\u6709\u53c2\u6570\u7ec4\u5408\u6765\u8fbe\u5230\u76ee\u6807P2dB\u538b\u7f29\u89c4\u683c\uff0c\u800c\u662f\u6709\u9009\u62e9\u6027\u5730\u9009\u53d6\u7ea635%\u7684\u5173\u952e\u4eff\u771f\u70b9\u3002\u8be5\u6846\u67b6\u5904\u7406ADS\u7f51\u8868\uff0c\u5728\u7b80\u5316\u7684\u6570\u636e\u96c6\u4e0a\u6267\u884c\u8c10\u6ce2\u5e73\u8861\u4eff\u771f\uff0c\u5e76\u8bad\u7ec3CatBoost\u6a21\u578b\u6765\u9884\u6d4b\u6574\u4e2a\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684P2dB\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u572815\u4e2aPA\u5de5\u4f5c\u6a21\u5f0f\u4e0b\u7684\u9a8c\u8bc1\u4ea7\u751f\u4e860.901\u7684\u5e73\u5747R\u00b2\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5bf9\u53c2\u6570\u7ec4\u5408\u6309\u5176\u6ee1\u8db3\u76ee\u6807\u89c4\u683c\u7684\u53ef\u80fd\u6027\u8fdb\u884c\u6392\u540d\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u57fa\u4e8eGUI\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u4eff\u771f\u65f6\u95f4\u7f29\u77ed\u4e8658.24%\u81f377.78%\uff0c\u4ece\u800c\u5728\u4e0d\u5f71\u54cd\u751f\u4ea7\u5c04\u9891\u7535\u8def\u6240\u9700\u7684\u7cbe\u5ea6\u6807\u51c6\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8bbe\u8ba1\u8fed\u4ee3\u3002"}}
{"id": "2507.12217", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12217", "abs": "https://arxiv.org/abs/2507.12217", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "title": "Towards few-shot isolated word reading assessment", "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "An ASR-free, few-shot method using SSL for child speech reading assessment showed promise but highlighted limitations of SSL for child data in such systems.", "motivation": "To explore an ASR-free method for isolated word reading assessment in low-resource settings.", "method": "A few-shot, ASR-free method comparing child speech input to adult-provided reference templates using intermediate layers from large self-supervised learned (SSL) models. Design options such as discretising SSL features and barycentre averaging of templates were investigated.", "result": "Idealised experiments showed reasonable performance for adults but a substantial drop for child speech input, even with child templates.", "conclusion": "SSL representations have limitations for processing child data in few-shot classification systems, despite their success in low-resource speech tasks."}}
{"id": "2507.12012", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12012", "abs": "https://arxiv.org/abs/2507.12012", "authors": ["Matthias Perkonigg", "Nina Bastati", "Ahmed Ba-Ssalamah", "Peter Mesenbrink", "Alexander Goehler", "Miljen Martic", "Xiaofei Zhou", "Michael Trauner", "Georg Langs"], "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease", "comment": null, "summary": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u805a\u7c7b\u7f51\u7edc\uff0c\u4ece\u809d\u810f\u78c1\u5171\u632f\u56fe\u50cf\u4e2d\u8bc6\u522b\u51fa\u53ef\u91cf\u5316\u6cbb\u7597\u53cd\u5e94\u7684\u7ec4\u7ec7\u6a21\u5f0f\u8bcd\u6c47\uff0c\u5e76\u5728\u975e\u9152\u7cbe\u6027\u8102\u80aa\u6027\u809d\u708e\u60a3\u8005\u7684\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5316\u7684\u4e0e\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u53cd\u5e94\u76f8\u5173\u7684\u56fe\u50cf\u6a21\u5f0f\u5bf9\u4e8e\u6307\u5bfc\u4e2a\u4f53\u6cbb\u7597\u548c\u5f00\u53d1\u65b0\u7597\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6df1\u5ea6\u805a\u7c7b\u7f51\u7edc\u540c\u65f6\u7f16\u7801\u548c\u805a\u7c7b\u533b\u5b66\u56fe\u50cf\u6591\u5757\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u5efa\u7acb\u7ec4\u7ec7\u6a21\u5f0f\u8bcd\u6c47\u3002", "result": "\u8be5\u8bcd\u6c47\u80fd\u591f\u6355\u6349\u4e0e\u6cbb\u7597\u53cd\u5e94\u76f8\u5173\u7684\u5dee\u5f02\u6027\u7ec4\u7ec7\u53d8\u5316\u53ca\u5176\u5728\u809d\u810f\u4e2d\u7684\u4f4d\u7f6e\u3002\u5728\u975e\u9152\u7cbe\u6027\u8102\u80aa\u6027\u809d\u708e\u60a3\u8005\u7684\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u961f\u5217\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u4e0e\u6cbb\u7597\u76f8\u5173\u7684\u7279\u5b9a\u809d\u810f\u7ec4\u7ec7\u53d8\u5316\u901a\u8def\uff0c\u5e76\u4e14\u6bd4\u5df2\u6709\u7684\u975e\u5f71\u50cf\u5b66\u6d4b\u91cf\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u533a\u5206\u6cbb\u7597\u7ec4\u3002\u6b64\u5916\uff0c\u8be5\u8bcd\u6c47\u8fd8\u53ef\u4ee5\u6839\u636e\u975e\u4fb5\u5165\u6027\u5f71\u50cf\u6570\u636e\u9884\u6d4b\u6d3b\u68c0\u884d\u751f\u7684\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u72ec\u7acb\u7684\u9a8c\u8bc1\u961f\u5217\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.12253", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12253", "abs": "https://arxiv.org/abs/2507.12253", "authors": ["Archisman Ghosh", "Avimita Chatterjee", "Swaroop Ghosh"], "title": "Design Automation in Quantum Error Correction", "comment": "50 pages, 7 figures", "summary": "Quantum error correction (QEC) underpins practical fault-tolerant quantum\ncomputing (FTQC) by addressing the fragility of quantum states and mitigating\ndecoherence-induced errors. As quantum devices scale, integrating robust QEC\nprotocols is imperative to suppress logical error rates below threshold and\nensure reliable operation, though current frameworks suffer from substantial\nqubit overheads and hardware inefficiencies. Design automation in the QEC flow\nis thus critical, enabling automated synthesis, transpilation, layout, and\nverification of error-corrected circuits to reduce qubit footprints and push\nfault-tolerance margins. This chapter presents a comprehensive treatment of\ndesign automation in QEC, structured into four main sections. The first section\ndelves into the theoretical aspects of QEC, covering logical versus physical\nqubit representations, stabilizer code construction, and error syndrome\nextraction mechanisms. In the second section, we outline the QEC design flow,\ndetailing the areas highlighting the need for design automation. The third\nsection surveys recent advancements in design automation techniques, including\nalgorithmic $T$-gate optimization, modified surface code architecture to\nincorporate lesser qubit overhead, and machine-learning-based decoder\nautomation. The final section examines near-term FTQC architectures,\nintegrating automated QEC pipelines into scalable hardware platforms and\ndiscussing end-to-end verification methodologies. Each section is complemented\nby case studies of recent research works, illustrating practical\nimplementations and performance trade-offs. Collectively, this chapter aims to\nequip readers with a holistic understanding of design automation in QEC system\ndesign in the fault-tolerant landscape of quantum computing.", "AI": {"tldr": "\u672c\u7ae0\u5168\u9762\u4ecb\u7ecd\u4e86\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u4e2d\u7684\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u6bd4\u7279\u5f00\u9500\u548c\u786c\u4ef6\u6548\u7387\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u7406\u8bba\u3001\u6d41\u7a0b\u3001\u6280\u672f\u8fdb\u5c55\uff08\u5982T\u95e8\u4f18\u5316\u3001\u6539\u8fdb\u8868\u9762\u7801\u3001ML\u89e3\u7801\u5668\uff09\u548c\u8fd1\u5730\u5e73\u67b6\u6784\uff0c\u65e8\u5728\u4e3a\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u53ef\u6269\u5c55\u7684QEC\u7cfb\u7edf\u8bbe\u8ba1\u7406\u89e3\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8bbe\u5907\u7684\u89c4\u6a21\u5316\uff0c\u96c6\u6210\u5f3a\u5927\u7684QEC\u534f\u8bae\u4ee5\u5c06\u903b\u8f91\u9519\u8bef\u7387\u6291\u5236\u5728\u9608\u503c\u4ee5\u4e0b\u5e76\u786e\u4fdd\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684QEC\u6846\u67b6\u5b58\u5728\u663e\u8457\u7684\u91cf\u5b50\u6bd4\u7279\u5f00\u9500\u548c\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u56e0\u6b64\uff0cQEC\u6d41\u7a0b\u4e2d\u7684\u8bbe\u8ba1\u81ea\u52a8\u5316\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u7684\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7ae0\u5185\u5bb9\u7ed3\u6784\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\uff1a\u9996\u5148\uff0c\u9610\u8ff0QEC\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5305\u62ec\u903b\u8f91\u4e0e\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u8868\u793a\u3001\u7a33\u5b9a\u5668\u7801\u6784\u9020\u53ca\u9519\u8bef \u0627\u0644\u0635\u064a\u6536\u63d0\u53d6\uff1b\u5176\u6b21\uff0c\u6982\u8ff0QEC\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u5fc5\u8981\u6027\uff1b\u518d\u6b21\uff0c\u8c03\u7814QEC\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u7b97\u6cd5T\u95e8\u4f18\u5316\u3001\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u5f00\u9500\u7684\u6539\u8fdb\u578b\u8868\u9762\u7801\u67b6\u6784\u4ee5\u53ca\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u89e3\u7801\u5668\u81ea\u52a8\u5316\uff1b\u6700\u540e\uff0c\u63a2\u8ba8\u8fd1\u5730\u5e73\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\uff0c\u5c06\u81ea\u52a8\u5316QEC\u6d41\u7a0b\u6574\u5408\u5230\u53ef\u6269\u5c55\u786c\u4ef6\u5e73\u53f0\uff0c\u5e76\u8ba8\u8bba\u7aef\u5230\u7aef\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u672c\u7ae0\u5168\u9762\u4ecb\u7ecd\u4e86QEC\u7684\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u6db5\u76d6\u4e86\u7406\u8bba\u57fa\u7840\u3001\u8bbe\u8ba1\u6d41\u7a0b\u3001\u6700\u65b0\u6280\u672f\u8fdb\u5c55\uff08\u5982T\u95e8\u4f18\u5316\u3001\u6539\u8fdb\u578b\u8868\u9762\u7801\u3001\u673a\u5668\u5b66\u4e60\u89e3\u7801\u5668\uff09\u4ee5\u53ca\u8fd1\u5730\u5e73\u67b6\u6784\u7684\u6574\u5408\u4e0e\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u672c\u7ae0\u65e8\u5728\u5168\u9762\u4ecb\u7ecd\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u4e2d\u7684\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u5728\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u4e2dQEC\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6574\u4f53\u7406\u89e3\u3002"}}
{"id": "2507.11948", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11948", "abs": "https://arxiv.org/abs/2507.11948", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "comment": null, "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "AI": {"tldr": "\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c Kevin\u6a21\u578b\u5728AI\u6027\u80fd\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6b63\u786e\u7387\u548c\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "GPU\u6838\u51fd\u6570\u7f16\u5199\u5bf9\u4e8eAI\u7cfb\u7edf\u7684\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u8fc7\u7a0b\u5145\u6ee1\u6311\u6218\u4e14\u9700\u8981\u53cd\u590d\u8bd5\u9a8c\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u975e\u5e38\u9002\u5408\u6b64\u573a\u666f\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\uff08\u5982\u6b63\u786e\u6027\u548c\u52a0\u901f\uff09\uff0c\u5e76\u4e14\u8be5\u8fc7\u7a0b\u672c\u8eab\u5177\u6709\u8fed\u4ee3\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u8f68\u8ff9\u5b66\u4e60\u548c\u8de8\u8f6e\u6b21\u5956\u52b1\u5f52\u56e0\u7b49\u73b0\u5b9e\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u4f7f\u7528\u8be5\u65b9\u6cd5\u8fdb\u884cCUDA\u6838\u751f\u6210\u548c\u4f18\u5316\u7684\u6a21\u578bKevin\u3002", "result": "Kevin\u6a21\u578b\u5c06\u7eafCUDA\u4ee3\u7801\u7684\u6b63\u786e\u7387\u4ece56%\u63d0\u5347\u81f382%\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u4ece\u57fa\u7ebf\uff08PyTorch Eager\uff09\u76840.53\u500d\u63d0\u5347\u81f31.10\u500d\uff0c\u5e76\u4e14\u8d85\u8d8a\u4e86o4-mini\u6a21\u578b\uff080.78\u500d\uff09\u3002", "conclusion": "Kevin\u5728\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7684\u52a0\u6301\u4e0b\uff0c\u5728CUDA\u6838\u751f\u6210\u548c\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u65b9\u9762\uff0c\u901a\u8fc7\u589e\u52a0\u4e32\u884c\u4f18\u5316\u8f6e\u6570\u80fd\u6bd4\u5e76\u884c\u91c7\u6837\u5e26\u6765\u66f4\u9ad8\u7684\u6027\u80fd\u63d0\u5347\u7387\u3002"}}
{"id": "2507.12252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12252", "abs": "https://arxiv.org/abs/2507.12252", "authors": ["Shilin Zhou", "Zhenghua Li"], "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86token\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\uff0c\u4ee5\u63d0\u9ad8ASR\u5bf9\u5173\u952e\u8bcd\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5728\u4e2d\u6587\u548c\u82f1\u6587\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u7aef\u5230\u7aef\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u5728\u8f6c\u5f55\u4e00\u822c\u8bed\u97f3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u51c6\u786e\u8bc6\u522b\u4e13\u6709\u540d\u8bcd\u6216\u7528\u6237\u7279\u5b9a\u5b9e\u4f53\u7b49\u4e0a\u4e0b\u6587\u76f8\u5173\u5173\u952e\u8bcd\u65b9\u9762\u5374\u5e38\u5e38\u9047\u5230\u56f0\u96be\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u6587\u672c\u6a21\u5f0f\u4e2d\u5229\u7528\u5173\u952e\u8bcd\u8bcd\u5178\u6765\u63d0\u9ad8\u5173\u952e\u8bcd\u8bc6\u522b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u7c92\u5ea6\u4e0a\u64cd\u4f5c\u5e76\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684token\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\u7684\u4f18\u70b9\uff0c\u5e76\u901a\u8fc7\u665a\u671f\u878d\u5408\u7b56\u7565\u7ed3\u5408\u4e86ASR\u7684\u58f0\u5b66\u4fe1\u606f\u548cLLM\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5173\u952e\u8bcd\u76f8\u5173\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u975e\u5173\u952e\u8bcd\u6587\u672c\u7684\u9ad8\u51c6\u786e\u7387\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86token\u7ea7\u548c\u77ed\u8bed\u7ea7\u7ec4\u4ef6\u5728\u6b64\u8054\u5408\u591a\u7c92\u5ea6\u6846\u67b6\u4e2d\u5bf9\u6027\u80fd\u63d0\u5347\u505a\u51fa\u4e86\u663e\u8457\u8d21\u732e\uff0c\u5e76\u80fd\u76f8\u4e92\u8865\u5145\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e2d\u6587\u548c\u82f1\u6587\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u975e\u5173\u952e\u8bcd\u6587\u672c\u7684\u9ad8\u51c6\u786e\u7387\u3002"}}
{"id": "2507.12017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12017", "abs": "https://arxiv.org/abs/2507.12017", "authors": ["Xiwei Zhang", "Chunjin Yang", "Yiming Xiao", "Runtong Zhang", "Fanman Meng"], "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection", "comment": "8 main-pages, 3 reference-pages, 5 figures, 6 tables", "summary": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SS-DC\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026-\u8026\u5408\u7b56\u7565\u548cSAID\u6a21\u5757\uff0c\u6709\u6548\u5904\u7406RGB-IR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u591a\u5b50\u57df\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RGB-IR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5c06RGB\u57df\u89c6\u4e3a\u5355\u4e00\u57df\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u5b58\u5728\u7684\u767d\u5929\u3001\u9ed1\u591c\u3001\u96fe\u5929\u7b49\u591a\u4e2a\u5b50\u57df\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u8de8\u8d8a\u8fd9\u4e9b\u5b50\u57df\u89e3\u8026\u57df\u4e0d\u53d8\uff08DI\uff09\u548c\u57df\u7279\u5b9a\uff08DS\uff09\u7279\u5f81\uff0c\u5bf9RGB-IR\u57df\u9002\u5e94\u6027\u5177\u6709\u79ef\u6781\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SS-DC\u6846\u67b6\uff0c\u5305\u542b\u89e3\u8026\u548c\u8026\u5408\u4e24\u4e2a\u6838\u5fc3\u90e8\u5206\u3002\u89e3\u8026\u65b9\u9762\uff0c\u8bbe\u8ba1\u4e86\u5149\u8c31\u81ea\u9002\u5e94\u5e42\u7b49\u89e3\u8026\uff08SAID\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u6ee4\u6ce2\u94f6\u884c\u548c\u81ea\u84b8\u998f\u89e3\u8026\u635f\u5931\u6765\u5206\u79bb\u57df\u4e0d\u53d8\uff08DI\uff09\u548c\u57df\u7279\u5b9a\uff08DS\uff09\u7279\u5f81\u3002\u8026\u5408\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4-\u5149\u8c31\u8026\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u7a7a\u95f4\u548c\u5149\u8c31\u7684DI\u7279\u5f81\u91d1\u5b57\u5854\u8fdb\u884c\u8054\u5408\u8026\u5408\uff0c\u5e76\u5f15\u5165DS\u4ee5\u51cf\u5c11\u57df\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684SS-DC\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5305\u542bFLIR-ADAS\u6570\u636e\u96c6\u7684\u65b0\u5b9e\u9a8c\u534f\u8bae\u4e0b\uff0c\u5728\u591a\u4e2aRGB-IR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684UDAOD\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SS-DC\u6846\u67b6\u901a\u8fc7\u89e3\u8026-\u8026\u5408\u7b56\u7565\uff0c\u5728\u5904\u7406RGB-IR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2aRGB-IR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12256", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.12256", "abs": "https://arxiv.org/abs/2507.12256", "authors": ["Monica L\u0103c\u0103tu\u015f", "Matthias M\u00f6ller"], "title": "Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator", "comment": "31 pages, 14 figures", "summary": "Direct numerical simulation of turbulent flows at high Reynolds numbers\nremains a major challenge for traditional computational fluid dynamics (CFD)\ntools running on classical computer hardware. This has motivated growing\ninterest in quantum algorithms for CFD to enable flow simulations on quantum\ncomputers. The reason being that these computers are expected to deliver\npotential speed-ups for certain problems. One promising quantum CFD approach is\na fully quantum implementation of the lattice Boltzmann method called QLBM.\nAlthough efficient quantum routines are now available for the streaming step,\nimplementing the nonlinear, irreversible collision step with a low depth\ncircuit that avoids additional ancilla qubits, probabilistic post-selection and\nrepeated executions remains a significant challenge. In this study, we address\nthis challenge by introducing a framework for learning a surrogate quantum\ncircuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision\noperator for the D2Q9 lattice. The four qubit circuit is trained to respect the\nphysical properties of the BGK collision operator, including mass and momentum\nconservation, D8 equivariance and scale equivariance. When compiled to the gate\nset used by IBM Heron processor under the assumption of full qubit\nconnectivity, the 15 block SQC requires only 2,430 native gates and uses\nneither ancilla qubits nor post-selection or repeated executions. Moreover, its\ndepth is independent of the grid resolution, as collision is a local operation\nthat can exploit quantum parallelism to its full extent. We validate the SQC on\ntwo benchmark flows, the Taylor Green vortex decay and the lid driven cavity,\ndemonstrating that it accurately captures vortex dissipation and flow\nrecirculation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\uff08SQC\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u9ad8\u96f7\u8bfa\u6570\u6e4d\u6d41\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6a21\u62df\u7ed3\u679c\u4e2d\u8868\u73b0\u51fa\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u5de5\u5177\u96be\u4ee5\u5728\u9ad8\u96f7\u8bfa\u6570\u4e0b\u8fdb\u884c\u76f4\u63a5\u6570\u503c\u6a21\u62df\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5bf9\u91cf\u5b50\u7b97\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884cCFD\u6a21\u62df\u7684\u5174\u8da3\uff0c\u56e0\u4e3a\u91cf\u5b50\u8ba1\u7b97\u673a\u6709\u671b\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u63d0\u4f9b\u6f5c\u5728\u7684\u52a0\u901f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u66ff\u4ee3\u91cf\u5b50\u7535\u8def\uff08SQC\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u8fd1\u4f3cD2Q9\u683c\u7684BGK\u78b0\u649e\u7b97\u5b50\uff0c\u5e76\u5bf9\u56db\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7535\u8def\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u6ee1\u8db3\u8d28\u91cf\u5b88\u6052\u3001\u52a8\u91cf\u5b88\u6052\u3001D8\u7b49\u8ddd\u548c\u5c3a\u5ea6\u7b49\u8ddd\u7b49\u7269\u7406\u6027\u8d28\u3002", "result": "\u6240\u63d0\u51fa\u7684SQC\u6846\u67b6\u5728IBM Heron\u5904\u7406\u5668\u4e0a\u7f16\u8bd1\u65f6\uff0c\u53ea\u9700\u89812,430\u4e2a\u672c\u5730\u95e8\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u3001\u6982\u7387\u6027\u540e\u9009\u6216\u91cd\u590d\u6267\u884c\u3002\u6b64\u5916\uff0c\u5176\u6df1\u5ea6\u4e0e\u7f51\u683c\u5206\u8fa8\u7387\u65e0\u5173\u3002\u5728\u6cf0\u52d2-\u683c\u6797\u6da1\u65cb\u8870\u51cf\u548c\u76d6\u9a71\u52a8\u8154\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSQC\u80fd\u591f\u51c6\u786e\u6355\u6349\u6da1\u65cb\u8017\u6563\u548c\u6d41\u52a8\u518d\u5faa\u73af\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u66ff\u4ee3\u91cf\u5b50\u7535\u8def\uff08SQC\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u8fd1\u4f3cD2Q9\u683c\u7684Bhatnagar Gross Krook\uff08BGK\uff09\u78b0\u649e\u7b97\u5b50\uff0c\u4ee5\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u4e2d\u78b0\u649e\u6b65\u9aa4\u7684\u6311\u6218\u3002"}}
{"id": "2507.12260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 T-index\uff0c\u4e00\u79cd\u91cf\u5316\u7ffb\u8bd1\u75d5\u8ff9\u7684\u65b0\u6307\u6807\uff0c\u8be5\u6307\u6807\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u6bd4\u8ba1\u7b97\uff0c\u5df2\u88ab\u8bc1\u660e\u5728\u8de8\u9886\u57df\u8bc4\u4f30\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u7684\u8865\u5145\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u91cf\u5316\u7ffb\u8bd1\u75d5\u8ff9\uff08translationese\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u4f5c\u4e3a\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u8865\u5145\u6307\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7ffb\u8bd1\u75d5\u8ff9\u6307\u6570\u201d\uff08T-index\uff09\u7684\u91cf\u5316\u6307\u6807\uff0c\u8be5\u6307\u6807\u901a\u8fc7\u5bf9\u6bd4\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u7684\u4f3c\u7136\u6bd4\u8ba1\u7b97\u5f97\u51fa\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u7ffb\u8bd1\u6570\u636e\u96c6\u5bf9 T-index \u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "T-index \u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u6355\u6349\u7ffb\u8bd1\u4e2d\u7684\u7ffb\u8bd1\u75d5\u8ff9\u3002\u4f7f\u7528\u5c11\u91cf\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u5373\u53ef\u83b7\u5f97\u826f\u597d\u7684 T-index \u8bc4\u5206\u3002T-index \u7684\u76f8\u5bf9\u5dee\u5f02\u53ef\u4ee5\u9884\u6d4b\u4eba\u7c7b\u5bf9\u7ffb\u8bd1\u75d5\u8ff9\u7684\u6807\u6ce8\uff0c\u5176\u7edd\u5bf9\u503c\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5177\u6709\u826f\u597d\u7684\u76f8\u5173\u6027\uff08Pearson", "conclusion": "T-index \u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u91cf\u5316\u7684\u7ffb\u8bd1\u6307\u6807\uff0c\u53ef\u4ee5\u6709\u6548\u8861\u91cf\u7ffb\u8bd1\u4e2d\u201c\u7ffb\u8bd1\u75d5\u8ff9\u201d\u7684\u7a0b\u5ea6\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u5982 BLEU \u548c COMET\uff09\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u8865\u5145\u6307\u6807\u3002"}}
{"id": "2507.12022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12022", "abs": "https://arxiv.org/abs/2507.12022", "authors": ["Yuechen Xie", "Jie Song", "Yicheng Shan", "Xiaoyan Zhang", "Yuanyu Wan", "Shengxuming Zhang", "Jiarui Duan", "Mingli Song"], "title": "Dataset Ownership Verification for Pre-trained Masked Models", "comment": "Accepted by ICCV 2025", "summary": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOV4MM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u63a9\u7801\u6a21\u578b\u662f\u5426\u4f7f\u7528\u4e86\u7279\u5b9a\u7684\u672a\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u4ee5\u4fdd\u62a4\u6570\u636e\u96c6\u6240\u6709\u8005\u7684\u6743\u76ca\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u91cd\u5efa\u63a9\u7801\u4fe1\u606f\u7684\u96be\u6613\u7a0b\u5ea6\u6765\u5de5\u4f5c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\u6280\u672f\u4e0d\u9002\u7528\u4e8e\u65e5\u76ca\u589e\u957f\u7684\u63a9\u7801\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u6570\u636e\u96c6\u6240\u6709\u8005\u7684\u6743\u76ca\u3002", "method": "DOV4MM\u662f\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u91cd\u5efa\u63a9\u7801\u4fe1\u606f\u7684\u96be\u6613\u7a0b\u5ea6\u5dee\u5f02\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u53ef\u7591\u9ed1\u76d2\u6a21\u578b\u662f\u5426\u5728\u7279\u5b9a\u7684\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5728ImageNet-1K\u4e0a\u7684\u5341\u4e2a\u63a9\u7801\u56fe\u50cf\u6a21\u578b\u548c\u5728WikiText-103\u4e0a\u7684\u56db\u4e2a\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0cDOV4MM\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DOV4MM\u6210\u529f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u62d2\u7edd\u4e86\u96f6\u5047\u8bbe\uff0cp\u503c\u8fdc\u4f4e\u4e8e0.05\uff0c\u5e76\u4e14\u4f18\u4e8e\u4e4b\u524d\u6240\u6709\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.12302", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12302", "abs": "https://arxiv.org/abs/2507.12302", "authors": ["Julius A. Zeiss", "Gereon Ko\u00dfmann", "Omar Fawzi", "Mario Berta"], "title": "Approximating fixed size quantum correlations in polynomial time", "comment": "41 pages + 43 pages supplementary material", "summary": "We show that $\\varepsilon$-additive approximations of the optimal value of\nfixed-size two-player free games with fixed-dimensional entanglement assistance\ncan be computed in time $\\mathrm{poly}(1/\\varepsilon)$. This stands in contrast\nto previous analytic approaches, which focused on scaling with the number of\nquestions and answers, but yielded only strict $\\mathrm{exp}(1/\\varepsilon)$\nguarantees. Our main result is based on novel Bose-symmetric quantum de Finetti\ntheorems tailored for constrained quantum separability problems. These results\ngive rise to semidefinite programming (SDP) outer hierarchies for approximating\nthe entangled value of such games. By employing representation-theoretic\nsymmetry reduction techniques, we demonstrate that these SDPs can be formulated\nand solved with computational complexity $\\mathrm{poly}(1/\\varepsilon)$,\nthereby enabling efficient $\\varepsilon$-additive approximations. In addition,\nwe introduce a measurement-based rounding scheme that translates the resulting\nouter bounds into certifiably good inner sequences of entangled strategies.\nThese strategies can, for instance, serve as warm starts for see-saw\noptimization methods. We believe that our techniques are of independent\ninterest for broader classes of constrained separability problems in quantum\ninformation theory.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u7684\u91cf\u5b50de Finetti\u5b9a\u7406\u548cSDP\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e24\u4eba\u81ea\u7531\u535a\u5f08\u6700\u4f18\u503c\u7684\u591a\u9879\u5f0f\u7ea7\u03b5-\u52a0\u6027\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u5148\u524d\u5206\u6790\u65b9\u6cd5\u5728\u8ba1\u7b97\u56fa\u5b9a\u89c4\u6a21\u4e24\u4eba\u81ea\u7531\u535a\u5f08\uff08\u5177\u6709\u56fa\u5b9a\u7ef4\u5ea6\u7ea0\u7f20\u8f85\u52a9\uff09\u7684\u6700\u4f18\u503c\u65f6\uff0c\u867d\u7136\u5173\u6ce8\u4e8e\u4e0e\u95ee\u9898\u89c4\u6a21\uff08\u95ee\u7b54\u6570\u91cf\uff09\u7684\u4f9d\u8d56\u6027\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9bexp(1/\u03b5)\u7684\u4fdd\u8bc1\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u5c40\u9650\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u5173\u4e8e\u03b5\u7684\u591a\u9879\u5f0f\u7ea7\u3002", "method": "\u672c\u7814\u7a76\u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u5229\u7528\u65b0\u9896\u7684Bose\u5bf9\u79f0\u91cf\u5b50de Finetti\u5b9a\u7406\uff0c\u8be5\u5b9a\u7406\u9488\u5bf9\u7ea6\u675f\u91cf\u5b50\u53ef\u5206\u6027\u95ee\u9898\u8fdb\u884c\u4e86\u5b9a\u5236\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5b9a\u7406\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u7528\u4e8e\u8fd1\u4f3c\u6b64\u7c7b\u535a\u5f08\u7ea0\u7f20\u503c\u7684SDP\uff08\u534a\u5b9a\u89c4\u5212\uff09\u5916\u5c42\u5c42\u7ea7\u3002\u901a\u8fc7\u8fd0\u7528\u8868\u793a\u8bba\u4e2d\u7684\u5bf9\u79f0\u7ea6\u7b80\u6280\u672f\uff0c\u7814\u7a76\u6210\u529f\u5730\u5c06\u8fd9\u4e9bSDP\u7684\u8ba1\u7b97\u590d\u6742\u6027\u964d\u4f4e\u81f3poly(1/\u03b5)\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u03b5-\u52a0\u6027\u8fd1\u4f3c\u3002", "result": "\u672c\u7814\u7a76\u7684\u6838\u5fc3\u6210\u679c\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u4ee5poly(1/\u03b5)\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5b9e\u73b0\u5bf9\u56fa\u5b9a\u89c4\u6a21\u4e24\u4eba\u81ea\u7531\u535a\u5f08\uff08\u5177\u6709\u56fa\u5b9a\u7ef4\u5ea6\u7ea0\u7f20\u8f85\u52a9\uff09\u6700\u4f18\u503c\u8fdb\u884c\u03b5-\u52a0\u6027\u8fd1\u4f3c\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u6d4b\u91cf\u57fa\u7ea6\u6574\u65b9\u6848\uff0c\u80fd\u591f\u5c06SDP\u8ba1\u7b97\u5f97\u5230\u7684\u5916\u5c42\u754c\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u53ef\u8ba4\u8bc1\u7684\u3001\u6027\u80fd\u4f18\u8d8a\u7684\u7ea0\u7f20\u7b56\u7565\u5185\u5c42\u5e8f\u5217\uff0c\u8fd9\u4e9b\u7b56\u7565\u53ef\u4f5c\u4e3a\u67d0\u4e9b\u4f18\u5316\u65b9\u6cd5\u7684\u521d\u59cb\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eBose\u5bf9\u79f0\u91cf\u5b50de Finetti\u5b9a\u7406\u548c\u8868\u793a\u8bba\u5bf9\u79f0\u7ea6\u7b80\u6280\u672f\u7684SDP\u5916\u5c42\u5c42\u7ea7\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u56fa\u5b9a\u89c4\u6a21\u4e24\u4eba\u81ea\u7531\u535a\u5f08\u5728\u56fa\u5b9a\u7ef4\u5ea6\u7ea0\u7f20\u8f85\u52a9\u4e0b\u7684\u6700\u4f18\u503c\u8fdb\u884c\u03b5-\u52a0\u6027\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3apoly(1/\u03b5)\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u4ec5\u63d0\u4f9bexp(1/\u03b5)\u4fdd\u8bc1\u7684\u89e3\u6790\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u57fa\u7ea6\u6574\u65b9\u6848\uff0c\u53ef\u5c06SDP\u5916\u5c42\u754c\u8f6c\u5316\u4e3a\u53ef\u8ba4\u8bc1\u7684\u826f\u597d\u7ea0\u7f20\u7b56\u7565\u5185\u5c42\u5e8f\u5217\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u4e2d\u66f4\u5e7f\u6cdb\u7684\u7ea6\u675f\u53ef\u5206\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u601d\u8def\u3002"}}
{"id": "2507.11997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11997", "abs": "https://arxiv.org/abs/2507.11997", "authors": ["Tairan Huang", "Yili Wang"], "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "comment": null, "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "AI": {"tldr": "MLED\u6846\u67b6\u5229\u7528LLM\u589e\u5f3a\u56fe\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u7ea7\u589e\u5f3a\u5668\u878d\u5408\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u539f\u59cb\u6587\u672c\u4fe1\u606f\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u4e14\u5728\u591a\u6a21\u6001\u6570\u636e\u4e2d\u878d\u5408\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "MLED\u6846\u67b6\u5229\u7528LLM\u4ece\u6587\u672c\u4fe1\u606f\u4e2d\u63d0\u53d6\u5916\u90e8\u77e5\u8bc6\u6765\u589e\u5f3a\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u7c7b\u578b\u7ea7\u589e\u5f3a\u5668\u548c\u4e00\u4e2a\u5173\u7cfb\u7ea7\u589e\u5f3a\u5668\uff0c\u5206\u522b\u7528\u4e8e\u589e\u5f3a\u6b3a\u8bc8\u8005\u4e0e\u826f\u6027\u5b9e\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u6b3a\u8bc8\u8005\u5728\u4e0d\u540c\u5173\u7cfb\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMLED\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MLED\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12261", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12261", "abs": "https://arxiv.org/abs/2507.12261", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "Infherno \u662f\u4e00\u4e2a\u5229\u7528 LLM \u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u7597\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7531\u683c\u5f0f\u4e34\u5e8a\u7b14\u8bb0\u81ea\u52a8\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316 FHIR \u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u81ea\u7531\u683c\u5f0f\u4e34\u5e8a\u7b14\u8bb0\u5230\u7ed3\u6784\u5316 FHIR \u8d44\u6e90\u7ffb\u8bd1\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u6a21\u5757\u5316\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u6216\u7ecf\u8fc7\u6307\u4ee4\u8c03\u4f18\u548c\u7ea6\u675f\u89e3\u7801\u7684 LLM\uff0c\u901a\u5e38\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7531 LLM \u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u7597\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\u63d0\u4f9b\u652f\u6301\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u79f0\u4e3a Infherno\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u4ece\u81ea\u7531\u683c\u5f0f\u4e34\u5e8a\u7b14\u8bb0\u5230\u7ed3\u6784\u5316 FHIR \u8d44\u6e90\u7ffb\u8bd1\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "Infherno \u6846\u67b6\u5728\u9884\u6d4b FHIR \u8d44\u6e90\u65b9\u9762\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "Infherno \u6846\u67b6\u80fd\u591f\u751f\u6210\u7b26\u5408 FHIR \u6a21\u5f0f\u7684 FHIR \u8d44\u6e90\uff0c\u5e76\u4e14\u5728\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u6d4b FHIR \u8d44\u6e90\u65b9\u9762\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.12023", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12023", "abs": "https://arxiv.org/abs/2507.12023", "authors": ["Xu Fan", "Zhihao Wang", "Yuetan Lin", "Yan Zhang", "Yang Xiang", "Hao Li"], "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model", "comment": null, "summary": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVAR\u7684\u591a\u53d8\u91cf\u7a7a\u6c14\u6c61\u67d3\u7269\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u51cf\u5c11\u5bf9\u957f\u8f93\u5165\u7a97\u53e3\u7684\u4f9d\u8d56\u548c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u5b9e\u73b0\u4e86120\u5c0f\u65f6\u7684\u957f\u671f\u9884\u6d4b\u3002\u8be5\u6a21\u578b\u8fd8\u80fd\u8026\u5408\u6c14\u8c61\u9884\u62a5\uff0c\u5b66\u4e60\u6c61\u67d3\u7269\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u7a7a\u95f4\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b75\u4e2a\u4e2d\u56fd\u5317\u65b9\u57ce\u5e026\u79cd\u4e3b\u8981\u6c61\u67d3\u7269\u548c\u6c14\u8c61\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6c61\u67d3\u7269\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u6c61\u67d3\u7269\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u4e0d\u540c\u7684\u7a7a\u95f4\u54cd\u5e94\u3002\u4e3a\u4e86\u6ee1\u8db3\u9884\u6d4b\u591a\u79cd\u7a7a\u6c14\u6c61\u67d3\u7269\u8fd9\u4e00\u5b9e\u9645\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u7a7a\u6c14\u6c61\u67d3\u7269\u9884\u6d4b\u6a21\u578b\uff08MVAR\uff09\uff0c\u8be5\u6a21\u578b\u51cf\u5c11\u4e86\u5bf9\u957f\u65f6\u95f4\u7a97\u53e3\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u5229\u7528\u6548\u7387\u3002\u8be5\u6a21\u578b\u8fd8\u8bbe\u8ba1\u4e86\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86120\u5c0f\u65f6\u7684\u957f\u671f\u5e8f\u5217\u9884\u6d4b\u3002\u6b64\u5916\uff0cMVAR\u5f00\u53d1\u4e86\u6c14\u8c61\u8026\u5408\u7a7a\u95f4\u8f6c\u6362\u5668\u5757\uff0c\u80fd\u591f\u7075\u6d3b\u5730\u8026\u5408\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6c14\u8c61\u9884\u6d4b\uff0c\u540c\u65f6\u5b66\u4e60\u6c61\u67d3\u7269\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u4e0d\u540c\u7684\u7a7a\u95f4\u54cd\u5e94\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684MVAR\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u67b6\u6784\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12310", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12310", "abs": "https://arxiv.org/abs/2507.12310", "authors": ["Takla Nateeboon"], "title": "Uncertainty and entropies of classical channels", "comment": "Master's Thesis, University of Calgary, 150 pages, 14 figures. This\n  version may include minor edits and corrections. All comments and suggestions\n  are welcome", "summary": "In this thesis, I studied a mathematical development to define and quantify\nthe uncertainty inherent in classical channels. This thesis starts with the\nintroduction and background on how to formally think about uncertainty in the\ndomain of classical states. The concept of probability vector majorization and\nits variants, relative majorization and conditional majorization, are reviewed.\nThis thesis introduces three conceptually distinct approaches to formalize the\nnotion of uncertainty inherent in classical channels. These three approaches\ndefine the same preordering on the domain of classical channels, leading to\ncharacterizations from many perspectives. With the solid foundation of\nuncertainty comparison, classical channel entropy is then defined to be an\nadditive monotone with respect to the majorization relation. The well-known\nentropies in the domain of classical states are uniquely extended to the domain\nof channels via the optimal extensions, providing not only a solid foundation\nbut also the quantifiers of uncertainty inherent in classical channels.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u7ecf\u5178\u4fe1\u9053\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u71b5\u7684\u5b9a\u4e49\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5b9a\u4e49\u548c\u91cf\u5316\u7ecf\u5178\u4fe1\u9053\u4e2d\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u8bba\u6587\u56de\u987e\u4e86\u6982\u7387\u5411\u91cf\u7684 \u0ab6\u0acd\u0ab0\u0ac7\u0aa3\u0ac0\u5316\u53ca\u5176\u53d8\u4f53\uff0c\u5e76\u5f15\u5165\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u5f62\u5f0f\u5316\u7ecf\u5178\u4fe1\u9053\u4e2d\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u6982\u5ff5\u3002", "result": "\u8bba\u6587\u4e2d\u7684\u4e09\u79cd\u65b9\u6cd5\u5b9a\u4e49\u4e86\u76f8\u540c\u7684\u9884\u5e8f\u5173\u7cfb\uff0c\u5e76\u4e14\u6210\u529f\u5730\u5c06\u7ecf\u5178\u72b6\u6001\u71b5\u6269\u5c55\u5230\u4fe1\u9053\u9886\u57df\uff0c\u4e3a\u7ecf\u5178\u4fe1\u9053\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u6765\u5f62\u5f0f\u5316\u7ecf\u5178\u4fe1\u9053\u4e2d\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u6982\u5ff5\uff0c\u5e76\u5b9a\u4e49\u4e86\u7ecf\u5178\u4fe1\u9053\u71b5\uff0c\u5c06\u5176\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u3002"}}
{"id": "2507.12002", "categories": ["cs.LG", "I.2.0; J.4"], "pdf": "https://arxiv.org/pdf/2507.12002", "abs": "https://arxiv.org/abs/2507.12002", "authors": ["Alice Zhang", "Callihan Bertley", "Dawei Liang", "Edison Thomaz"], "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "comment": null, "summary": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "AI": {"tldr": "\u4e00\u9879\u7814\u7a76\u5229\u7528\u667a\u80fd\u624b\u8868\u6355\u83b7\u7684\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u58f0\u5b66\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u878d\u5408\u53e3\u5934\u548c\u975e\u53e3\u5934\u7ebf\u7d22\u6765\u68c0\u6d4b\u9762\u5bf9\u9762\u7684\u53e3\u5934\u4ea4\u8c08\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u548c\u534a\u81ea\u7136\u73af\u5883\u4e2d\u5206\u522b\u53d6\u5f97\u4e8682.0%\u548c77.2%\u7684\u5b8f\u89c2F1\u5206\u6570\u3002", "motivation": "\u4e3a\u4e86\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u58f0\u5b66\u73af\u5883\u4e2d\uff0c\u4ec5\u5229\u7528\u5546\u54c1\u667a\u80fd\u624b\u8868\u6355\u83b7\u7684\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u6765\u68c0\u6d4b\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u57fa\u7840\u65b9\u9762\u2014\u2014\u9762\u5bf9\u9762\u7684\u53e3\u5934\u4ea4\u8c08\u3002", "method": "\u5229\u7528\u5546\u54c1\u667a\u80fd\u624b\u8868\u6355\u83b7\u7684\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u5728\u58f0\u5b66\u6311\u6218\u573a\u666f\u4e2d\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u68c0\u6d4b\u9762\u5bf9\u9762\u7684\u53e3\u5934\u4ea4\u8c08\u3002\u5bf9\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u91c7\u7528\u4e863\u79cd\u4e0d\u540c\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u878d\u5408\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\u6765\u8003\u8651\u5bf9\u8bdd\u4e2d\u7684\u53e3\u5934\u548c\u975e\u53e3\u5934\u7ebf\u7d22\u7684\u4f18\u52bf\u3002", "result": "\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u54083\u79cd\u4e0d\u540c\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u878d\u5408\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\u80fd\u591f\u5229\u7528\u53e3\u5934\u7ebf\u7d22\u548c\u975e\u53e3\u5934\u59ff\u6001\u3002\u5728\u5b9e\u9a8c\u5ba4\u7814\u7a76\u548c\u534a\u81ea\u7136\u7814\u7a76\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u5bf9\u8bdd\u68c0\u6d4b\u4e0a\u7684\u5b8f\u89c2F1\u5206\u6570\u5206\u522b\u4e3a82.0\u00b13.0%\u548c77.2\u00b11.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u7684\u5bf9\u8bdd\u68c0\u6d4b\u4e0a\u8fbe\u5230\u4e8682.0\u00b13.0%\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u5728\u534a\u81ea\u7136\u73af\u5883\u4e2d\u7684\u5bf9\u8bdd\u68c0\u6d4b\u4e0a\u8fbe\u5230\u4e8677.2\u00b11.8%\u7684\u5b8f\u89c2F1\u5206\u6570\u3002"}}
{"id": "2507.12295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u5f15\u5165\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u5728\u591a\u79cd\u6587\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5d4c\u5165\u8d28\u91cf\u662f\u5173\u952e\uff0cLLM\u5d4c\u5165\u5e76\u672a\u4f7f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6d45\u5c42\u7b97\u6cd5\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u5229\u7528\u8de8\u6a21\u578b\u6027\u80fd\u77e9\u9635\u4f4e\u79e9\u7279\u6027\u8fdb\u884c\u5feb\u901f\u6a21\u578b\u8bc4\u4f30\u7684\u7b56\u7565\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u5de5\u5177\u5305\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u6587\u672c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u6807\u51c6\u5316\u548c\u5168\u9762\u7684\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u4e25\u683c\u7684\u6bd4\u8f83\u548c\u521b\u65b0\u65b9\u6cd5\u7684\u5f00\u53d1\u3002", "method": "\u8be5\u7814\u7a76\u8fdb\u884c\u4e86\u4e00\u9879\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5229\u7528\u4e86\u6765\u81ea\u5404\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u6587\u672c\u6570\u636e\u96c6\u3002\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u6027\uff0c\u7eb3\u5165\u4e86\u65e9\u671f\u8bed\u8a00\u6a21\u578b\uff08GloVe, BERT\uff09\u3001\u591a\u4e2aLLM\uff08LLaMa-2, Llama-3, Mistral, OpenAI\uff09\u3001\u591a\u9886\u57df\u6587\u672c\u6570\u636e\u96c6\uff08\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u79d1\u5b66\u51fa\u7248\u7269\uff09\u4ee5\u53ca\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff08AUROC, AUPRC\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5d4c\u5165\u8d28\u91cf\u5bf9\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u6027\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e14\u5728\u4f7f\u7528LLM\u5d4c\u5165\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5e76\u672a\u4f18\u4e8e\u4f20\u7edf\u7684\u6d45\u5c42\u7b97\u6cd5\u3002\u8de8\u6a21\u578b\u6027\u80fd\u77e9\u9635\u7684\u4f4e\u79e9\u7279\u6027\u4e3a\u5feb\u901f\u6a21\u578b\u8bc4\u4f30\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002\u901a\u8fc7\u5f00\u6e90\u57fa\u51c6\u5de5\u5177\u5305\uff0c\u4e3a\u672a\u6765\u5728\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u65b9\u9762\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5165\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5d4c\u5165\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5e76\u4e0d\u6bd4\u4f20\u7edf\u7684\u6d45\u5c42\u7b97\u6cd5\uff08\u5982KNN\u3001Isolation Forest\uff09\u8868\u73b0\u51fa\u6027\u80fd\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u8de8\u6a21\u578b\u6027\u80fd\u77e9\u9635\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u8fd9\u4e3a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5feb\u901f\u8bc4\u4f30\u548c\u9009\u62e9\u6a21\u578b\uff08\u6216\u5d4c\u5165\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u3002"}}
{"id": "2507.12026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12026", "abs": "https://arxiv.org/abs/2507.12026", "authors": ["Rongtao Xu", "Han Gao", "Mingming Yu", "Dong An", "Shunpeng Chen", "Changwei Wang", "Li Guo", "Xiaodan Liang", "Shibiao Xu"], "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering", "comment": "Accepted by IROS 2025", "summary": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.", "AI": {"tldr": "3D-MoRe \u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u5927\u89c4\u6a21 3D-\u8bed\u8a00\u6570\u636e\u96c6\u7684\u65b0\u8303\u4f8b\uff0c\u53ef\u63d0\u9ad8\u5ba4\u5185\u573a\u666f\u95ee\u7b54\u548c\u5bc6\u96c6\u5b57\u5e55\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5ba4\u5185\u573a\u666f\u4efb\u52a1\uff08\u5982\u95ee\u7b54\u548c\u5bc6\u96c6\u5b57\u5e55\uff09\u5bf9\u591a\u6837\u5316\u548c\u53ef\u6269\u5c55\u6570\u636e\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u751f\u6210 3D-\u8bed\u8a00\u6570\u636e\u96c6\u3002", "method": "3D-MoRe \u6846\u67b6\u6574\u5408\u4e86\u591a\u6a21\u6001\u5d4c\u5165\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\uff0c\u4ee5\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c 3D \u573a\u666f\u6570\u636e\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u5927\u89c4\u6a21 3D-\u8bed\u8a00\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528 ScanNet 3D \u573a\u666f\u6570\u636e\u96c6\u4ee5\u53ca\u6765\u81ea ScanQA \u548c ScanRefer \u7684\u6587\u672c\u6ce8\u91ca\uff0c3D-MoRe \u751f\u6210\u4e86 62,000 \u4e2a\u95ee\u7b54\u5bf9\u548c 73,000 \u4e2a\u8de8\u8d8a 1,513 \u4e2a\u573a\u666f\u7684\u5bf9\u8c61\u63cf\u8ff0\u3002\u5728 ScanQA \u4e0a\uff0cCIDEr \u5206\u6570\u63d0\u9ad8\u4e86 2.15%\uff1b\u5728 ScanRefer \u4e0a\uff0cCIDEr@0.5 \u63d0\u9ad8\u4e86 1.84%\u3002", "conclusion": "3D-MoRe \u5728 ScanQA \u548c ScanRefer \u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5206\u522b\u5c06 CIDEr \u5206\u6570\u63d0\u9ad8\u4e86 2.15% \u548c 1.84%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12011", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12011", "abs": "https://arxiv.org/abs/2507.12011", "authors": ["Yao Lu", "Hongyu Gao", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "comment": null, "summary": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "AI": {"tldr": "A new framework called DUSE expands data for automatic modulation recognition by using uncertainty scoring and active learning to select useful samples, addressing data scarcity issues and outperforming existing methods.", "motivation": "Deep neural networks for AMR require a large amount of labeled data, but target domain data is often scarce in practical scenarios, making manual annotation costly and data augmentation insufficient.", "method": "DUSE uses an uncertainty scoring function to filter out useful samples from relevant AMR datasets and employs an active learning strategy to continuously refine the scorer.", "result": "DUSE outperforms 8 coreset selection baselines and shows strong cross-architecture generalization.", "conclusion": "DUSE consistently outperforms 8 coreset selection baselines in both class-balance and class-imbalance settings, and exhibits strong cross-architecture generalization for unseen models."}}
{"id": "2507.12323", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12323", "abs": "https://arxiv.org/abs/2507.12323", "authors": ["Filip Mazurek", "Marissa D'Onofrio", "Andrew Van Horn", "Jiyong Yu", "Kavyashree Ranawat", "Jungsang Kim", "Kenneth R. Brown"], "title": "Tailored Quantum Device Calibration with Statistical Model Checking", "comment": null, "summary": "Quantum devices require precisely calibrated analog signals, a process that\nis complex and time-consuming. Many calibration strategies exist, and all\nrequire careful analysis and tuning to optimize system availability. To enable\nrigorous statistical evaluation of quantum calibration procedures, we leverage\nstatistical model checking (SMC), a technique used in fields that require\nstatistical guarantees. SMC allows for probabilistic evaluation of properties\nof interest, such as a certain parameter's time to failure. We extend the SMC\nfor Processor Analysis (SPA) framework, which uses SMC for evaluation of\nclassical systems, to create SPA for Quantum calibration (SPAQ) enabling\nsimplified tuning and analysis of quantum system calibration. We focus on a\ndirected acyclic graph-based calibration optimization scheme and demonstrate\nhow to craft properties of interest for its analysis. We show how to use SPAQ\nto find lower bounds of time to failure information, hidden node dependencies,\nand parameter threshold values and use that information to improve simulated\nquantum system availability through calibration scheme adjustments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faSPAQ\u6846\u67b6\uff0c\u5229\u7528\u7edf\u8ba1\u6a21\u578b\u68c0\u9a8c\u7b80\u5316\u91cf\u5b50\u7cfb\u7edf\u6821\u51c6\u7684\u8c03\u4f18\u548c\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u8bc1\u660e\u5176\u80fd\u63d0\u9ad8\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "motivation": "\u91cf\u5b50\u8bbe\u5907\u9700\u8981\u7cbe\u786e\u6821\u51c6\u7684\u6a21\u62df\u4fe1\u53f7\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\u3002\u4e3a\u4e86\u5bf9\u91cf\u5b50\u6821\u51c6\u7a0b\u5e8f\u8fdb\u884c\u4e25\u683c\u7684\u7edf\u8ba1\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u5229\u7528\u4e86\u7edf\u8ba1\u6a21\u578b\u68c0\u9a8c\uff08SMC\uff09\u6280\u672f\u3002", "method": "\u672c\u7814\u7a76\u5c06\u7edf\u8ba1\u6a21\u578b\u68c0\u9a8c\uff08SMC\uff09\u6269\u5c55\u5230SPA\uff08\u7528\u4e8e\u5904\u7406\u5668\u5206\u6790\uff09\u6846\u67b6\uff0c\u521b\u5efa\u4e86SPAQ\uff08\u7528\u4e8e\u91cf\u5b50\u6821\u51c6\uff09\u3002SPAQ\u5141\u8bb8\u5bf9\u91cf\u5b50\u7cfb\u7edf\u6821\u51c6\u7684\u5c5e\u6027\u8fdb\u884c\u6982\u7387\u8bc4\u4f30\uff0c\u5e76\u5e94\u7528\u4e8e\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\u7684\u6821\u51c6\u4f18\u5316\u65b9\u6848\uff0c\u4ee5\u67e5\u627e\u5931\u6548\u65f6\u95f4\u7684\u4e0b\u754c\u3001\u9690\u85cf\u8282\u70b9\u4f9d\u8d56\u5173\u7cfb\u548c\u53c2\u6570\u9608\u503c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cSPAQ\u80fd\u591f\u627e\u5230\u5931\u6548\u65f6\u95f4\u7684\u4e0b\u754c\u3001\u9690\u85cf\u8282\u70b9\u4f9d\u8d56\u5173\u7cfb\u548c\u53c2\u6570\u9608\u503c\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6821\u51c6\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u91cf\u5b50\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SPAQ\u6846\u67b6\uff0c\u5c06\u7edf\u8ba1\u6a21\u578b\u68c0\u9a8c\uff08SMC\uff09\u5e94\u7528\u4e8e\u91cf\u5b50\u7cfb\u7edf\u6821\u51c6\uff0c\u4ee5\u7b80\u5316\u8c03\u4f18\u548c\u5206\u6790\u8fc7\u7a0b\u3002\u901a\u8fc7SPAQ\uff0c\u53ef\u4ee5\u8bc4\u4f30\u6821\u51c6\u7a0b\u5e8f\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\u53c2\u6570\u7684\u5931\u6548\u65f6\u95f4\uff0c\u4ece\u800c\u627e\u5230\u5931\u6548\u65f6\u95f4\u7684\u4e0b\u754c\u3001\u9690\u85cf\u8282\u70b9\u4f9d\u8d56\u5173\u7cfb\u548c\u53c2\u6570\u9608\u503c\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6539\u8fdb\u6a21\u62df\u91cf\u5b50\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2507.12041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12041", "abs": "https://arxiv.org/abs/2507.12041", "authors": ["Anmol Kagrecha", "Henrik Marklund", "Potsawee Manakul", "Richard Zeckhauser", "Benjamin Van Roy"], "title": "Granular feedback merits sophisticated aggregation", "comment": "31 pages, 8 figures", "summary": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u6709\u9650\u7684\u4e2a\u4f53\u53cd\u9988\u6765\u9884\u6d4b\u4eba\u7fa4\u7684\u53cd\u9988\u5206\u5e03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u590d\u6742\u7684\u53cd\u9988\u7ec4\u5408\u65b9\u6cd5\u6bd4\u6b63\u5219\u5316\u5e73\u5747\u66f4\u80fd\u6709\u6548\u5229\u7528\u53cd\u9988\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u53cd\u9988\u7c92\u5ea6\u8f83\u9ad8\u65f6\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6240\u9700\u4e2a\u4f53\u6570\u91cf\u4ee5\u8fbe\u5230\u76f8\u540c\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5229\u7528\u6709\u9650\u4e2a\u4f53\u53cd\u9988\u9884\u6d4b\u4eba\u7fa4\u53cd\u9988\u5206\u5e03\u7684\u7814\u7a76\u4e2d\uff0c\u63a2\u7d22\u6bd4\u6b63\u5219\u5316\u5e73\u5747\u66f4\u4f18\u8d8a\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u53cd\u9988\u7c92\u5ea6\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e2a\u4f53\u53cd\u9988\u7684\u590d\u6742\u65b9\u6cd5\uff0c\u4ee5\u4f18\u4e8e\u6b63\u5219\u5316\u5e73\u5747\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u4eba\u7fa4\u7684\u53cd\u9988\u5206\u5e03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\uff0c\u968f\u7740\u53cd\u9988\u7c92\u5ea6\u7684\u589e\u52a0\uff0c\u590d\u6742\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u4f18\u4e8e\u6b63\u5219\u5316\u5e73\u5747\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u4e8c\u5143\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u590d\u6742\u65b9\u6cd5\u5728\u63d0\u9ad8\u6027\u80fd\u65b9\u9762\u6240\u9700\u4e2a\u4f53\u6570\u91cf\u7684\u51cf\u5c11\u6548\u679c\u4e0d\u660e\u663e\uff1b\u800c\u5728\u4e94\u70b9\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u590d\u6742\u65b9\u6cd5\u6240\u9700\u7684\u4e2a\u4f53\u6570\u91cf\u5927\u7ea6\u53ea\u6709\u6b63\u5219\u5316\u5e73\u5747\u7684\u4e00\u534a\u3002", "conclusion": "\u968f\u7740\u53cd\u9988\u7c92\u5ea6\u7684\u589e\u52a0\uff0c\u53ef\u4ee5\u901a\u8fc7\u6bd4\u6b63\u5219\u5316\u5e73\u5747\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u7ec4\u5408\u4e2a\u4f53\u53cd\u9988\uff0c\u4ece\u800c\u5728\u9884\u6d4b\u4eba\u7fa4\u53cd\u9988\u5206\u5e03\u65b9\u9762\u83b7\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.12356", "categories": ["cs.CL", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12029", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12029", "abs": "https://arxiv.org/abs/2507.12029", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "comment": null, "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u56fe\u65b0\u7c7b\u522b\u53d1\u73b0\uff08NCD\uff09\u6846\u67b6\uff08IICMVNCD\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u5355\u89c6\u56fe\u6570\u636e\u548c\u4f9d\u8d56\u4e0d\u7a33\u5b9a\u7684\u4f2a\u6807\u7b7e\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89c6\u56fe\u5185\uff08\u77e9\u9635\u5206\u89e3\uff09\u548c\u89c6\u56fe\u95f4\uff08\u89c6\u56fe\u5173\u7cfb\u5f15\u5bfc\uff09\u7684\u76f8\u5173\u6027\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709NCD\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u89c6\u56fe\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u591a\u89c6\u56fe\u6570\u636e\u7684\u666e\u904d\u6027\uff0c\u5e76\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u4f2a\u6807\u7b7e\u8fdb\u884c\u805a\u7c7b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u89c6\u56fe\u5185\u548c\u89c6\u56fe\u95f4\u76f8\u5173\u6027\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u65b0\u7c7b\u522b\u53d1\u73b0\u201d\uff08IICMVNCD\uff09\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u89c6\u56fe\u5185\u5c42\u9762\u5229\u7528\u77e9\u9635\u5206\u89e3\u6765\u5206\u89e3\u7279\u5f81\uff0c\u4ee5\u6355\u6349\u6570\u636e\u5206\u5e03\u7684\u4e00\u81f4\u6027\u5e76\u5efa\u7acb\u6837\u672c\u95f4\u7684\u6210\u5bf9\u5173\u7cfb\u3002\u5728\u89c6\u56fe\u95f4\u5c42\u9762\uff0c\u5229\u7528\u5df2\u77e5\u7c7b\u522b\u95f4\u7684\u89c6\u56fe\u5173\u7cfb\u6765\u6307\u5bfc\u65b0\u7c7b\u522b\u7684\u805a\u7c7b\uff0c\u5305\u62ec\u901a\u8fc7\u52a0\u6743\u878d\u5408\u56e0\u5b50\u77e9\u9635\u751f\u6210\u9884\u6d4b\u6807\u7b7e\uff0c\u5e76\u6839\u636e\u76d1\u7763\u635f\u5931\u52a8\u6001\u8c03\u6574\u5df2\u77e5\u7c7b\u522b\u7684\u89c6\u56fe\u6743\u91cd\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6743\u91cd\u8f6c\u79fb\u5230\u65b0\u7c7b\u522b\u7684\u5b66\u4e60\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\uff08IICMVNCD\uff09\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u65b0\u7c7b\u522b\u53d1\u73b0\uff08NCD\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2507.12326", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12326", "abs": "https://arxiv.org/abs/2507.12326", "authors": ["Gereon Ko\u00dfmann", "Julius A. Zeiss", "Omar Fawzi", "Mario Berta"], "title": "On approximate quantum error correction for symmetric noise", "comment": null, "summary": "We revisit the extendability-based semi-definite programming hierarchy\nintroduced by Berta et al. [Mathematical Programming, 1 - 49 (2021)], which\nprovides converging outer bounds on the optimal fidelity of approximate quantum\nerror correction (AQEC). As our first contribution, we introduce a\nmeasurement-based rounding scheme that extracts inner sequences of certifiably\ngood encoder-decoder pairs from this outer hierarchy. To address the\ncomputational complexity of evaluating fixed levels of the hierarchy, we\ninvestigate the use of symmetry-based dimension reduction. In particular, we\ncombine noise symmetries - such as those present in multiple copies of the\nqubit depolarizing channel - with the permutational symmetry arising from the\nextendability of the optimization variable. This framework is illustrated\nthrough basic, but already challenging numerical examples that showcase its\npractical effectiveness. Our results contribute to narrowing the gap between\ntheoretical developments in quantum information theory and their practical\napplications in the analysis of small-scale quantum error-correcting codes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7ea0\u9519\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u820d\u5165\u548c\u5bf9\u79f0\u6027\u964d\u7ef4\u6765\u63d0\u9ad8\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u53ef\u6269\u5c55\u6027\u7684\u534a\u5b9a\u89c4\u5212\u5c42\u7ea7\u5728\u8bc4\u4f30\u56fa\u5b9a\u5c42\u7ea7\u65f6\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u4e3a\u8fd1\u4f3c\u91cf\u5b50\u7ea0\u9519\u63d0\u4f9b\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u7f29\u5c0f\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u4ecb\u7ecd\u4e86Berta et al.\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u6269\u5c55\u6027\u7684\u534a\u5b9a\u89c4\u5212\u5c42\u7ea7\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u6d4b\u91cf\u57fa\u7840\u7684\u820d\u5165\u65b9\u6848\u6765\u63d0\u53d6\u5185\u90e8\u5e8f\u5217\u3002\u63a5\u7740\uff0c\u4e3a\u4e86\u5e94\u5bf9\u5c42\u7ea7\u8bc4\u4f30\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7814\u7a76\u8005\u4eec\u63a2\u7d22\u4e86\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u964d\u7ef4\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u566a\u58f0\u5bf9\u79f0\u6027\uff08\u5982\u91cf\u5b50\u6bd4\u7279\u8870\u51cf\u4fe1\u9053\u7684\u591a\u91cd\u526f\u672c\uff09\u548c\u4f18\u5316\u53d8\u91cf\u53ef\u6269\u5c55\u6027\u4ea7\u751f\u7684\u6392\u5217\u5bf9\u79f0\u6027\u3002", "result": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u6d4b\u91cf\u57fa\u7840\u7684\u820d\u5165\u65b9\u6848\uff0c\u7528\u4e8e\u4ece\u534a\u5b9a\u89c4\u5212\u5c42\u7ea7\u4e2d\u63d0\u53d6\u53ef\u8ba4\u8bc1\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5bf9\u3002\u901a\u8fc7\u7ed3\u5408\u566a\u58f0\u5bf9\u79f0\u6027\u548c\u6392\u5217\u5bf9\u79f0\u6027\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u964d\u7ef4\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u503c\u793a\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6d4b\u91cf\u7684\u820d\u5165\u65b9\u6848\uff0c\u4ece\u534a\u5b9a\u89c4\u5212\u5c42\u7ea7\u4e2d\u63d0\u53d6\u51fa\u53ef\u8ba4\u8bc1\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5bf9\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u964d\u7ef4\u6280\u672f\u6765\u5904\u7406\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u8fd1\u4f3c\u91cf\u5b50\u7ea0\u9519\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u7684\u7406\u8bba\u53d1\u5c55\u4e0e\u5176\u5728\u5c0f\u578b\u91cf\u5b50\u7ea0\u9519\u7801\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.12043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12043", "abs": "https://arxiv.org/abs/2507.12043", "authors": ["Wen Wen", "Tieliang Gong", "Yunjiao Zhang", "Zeyu Gao", "Weizhan Zhang", "Yong-Jin Liu"], "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "comment": null, "summary": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u57fa\u4e8e\u91cd\u653e\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u754c\u9650\u63ed\u793a\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u6709\u9650\u7684\u91cd\u653e\u4f18\u4e8e\u8be6\u5c3d\u91cd\u653e\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u6613\u4e8e\u8ba1\u7b97\u7684\u6cdb\u5316\u5dee\u8ddd\u754c\u9650\u3002", "motivation": "\u5c3d\u7ba1\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u4ece\u987a\u5e8f\u4efb\u52a1\u4e2d\u83b7\u53d6\u77e5\u8bc6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u7ecf\u9a8c\u6027\u80fd\uff0c\u4f46\u5bf9\u5176\u6cdb\u5316\u884c\u4e3a\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u4fe1\u606f\u8bba\u754c\u9650\uff0c\u5e76\u4f7f\u7528\u4f4e\u7ef4\u53d8\u91cf\u6765\u83b7\u5f97\u66f4\u7d27\u5bc6\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684\u6cdb\u5316\u5dee\u8ddd\u4e0a\u9650\u3002\u4ee5\u968f\u673a\u68af\u5ea6 Langevin \u52a8\u529b\u5b66\uff08SGLD\uff09\u4f5c\u4e3a\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u4e86\u4e3e\u4f8b\u8bf4\u660e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u5148\u524d\u4efb\u52a1\u7684\u6709\u9650\u793a\u4f8b\u4ee5\u53ca\u5f53\u524d\u4efb\u52a1\u6570\u636e\uff0c\u800c\u4e0d\u662f\u8be6\u5c3d\u7684\u91cd\u653e\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u9884\u6d4b\u7684\u754c\u9650\u901a\u8fc7\u4f7f\u7528\u4f4e\u7ef4\u53d8\u91cf\uff0c\u4e3a\u6cdb\u5316\u5dee\u8ddd\u63d0\u4f9b\u4e86\u66f4\u7d27\u5bc6\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684\u4e0a\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u91cd\u653e\u7684\u6301\u7eed\u5b66\u4e60\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e86\u4e00\u7cfb\u5217\u4fe1\u606f\u8bba\u754c\u9650\uff0c\u660e\u786e\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u5982\u4f55\u4e0e\u5f53\u524d\u4efb\u52a1\u4ea4\u4e92\u4ee5\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12370", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u591a\u4e2aLLM\uff08\u5305\u62ecLlama3-8B\u548cMistral-7B\uff09\u8fdb\u884c\u8fa9\u8bba\u6765\u63d0\u9ad8\u5904\u7406\u6b67\u4e49\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662fMistral-7B\uff0c\u6210\u529f\u7387\u8fbe\u523076.7%\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u8fa9\u8bba\u5728\u589e\u5f3aLLM\u6e05\u6670\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u7528\u6237\u8bf7\u6c42\u65f6\u9762\u4e34\u7684\u6b67\u4e49\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7531\u4e09\u79cdLLM\u67b6\u6784\uff08Llama3-8B\u3001Gemma2-9B\u548cMistral-7B\u53d8\u4f53\uff09\u548c\u4e00\u4e2a\u5305\u542b\u5404\u79cd\u6b67\u4e49\u7684\u6570\u636e\u96c6\u7ec4\u6210\u3002", "result": "\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86Llama3-8B\u548cMistral-7B\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5176\u4e2dMistral-7B\u4e3b\u5bfc\u7684\u8fa9\u8bba\u6210\u529f\u7387\u8fbe\u523076.7%\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u7684\u6b67\u4e49\u548c\u5b9e\u73b0\u6709\u6548\u7684\u5171\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u8fa9\u8bba\u6846\u67b6\u4f5c\u4e3a\u4e00\u79cd\u6709\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u81ea\u9002\u5e94\u7684\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u8fa9\u8bba\u53ef\u4ee5\u63d0\u9ad8\u4ea4\u4e92\u7cfb\u7edf\u7684\u6e05\u6670\u5ea6\u3002"}}
{"id": "2507.12049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12049", "abs": "https://arxiv.org/abs/2507.12049", "authors": ["Manuel Barusco", "Francesco Borsatti", "Arianna Stropeni", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "MoViAD: Modular Visual Anomaly Detection", "comment": null, "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.", "AI": {"tldr": "MoViAD\u662f\u4e00\u4e2a\u7528\u4e8eVAD\u7684\u5e93\uff0c\u652f\u6301\u5404\u79cd\u573a\u666f\u548c\u90e8\u7f72\uff0c\u6613\u4e8e\u4f7f\u7528\u548c\u6269\u5c55\u3002", "motivation": "\u52a0\u901fVAD\u9886\u57df\u7684\u7814\u7a76\u548c\u90e8\u7f72\uff0c\u89e3\u51b3VAD\u6570\u636e\u7a00\u7f3a\u548c\u65e0\u76d1\u7763\u8bad\u7ec3\u7684\u6311\u6218\u3002", "method": "MoViAD\u662f\u4e00\u4e2a\u5305\u542b\u6700\u5148\u8fdbVAD\u6a21\u578b\u3001\u8bad\u7ec3\u5668\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\u7684\u5e93\uff0c\u652f\u6301\u6301\u7eed\u5b66\u4e60\u3001\u534a\u76d1\u7763\u3001\u5c11\u6837\u672c\u3001\u566a\u58f0\u7b49\u591a\u79cd\u573a\u666f\uff0c\u5e76\u9488\u5bf9\u8fb9\u7f18\u548c\u7269\u8054\u7f51\u90e8\u7f72\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u63d0\u4f9b\u91cf\u5316\u548c\u538b\u7f29\u5de5\u5177\u3002", "result": "MoViAD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u4f7f\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u80fd\u591f\u8f7b\u677e\u90e8\u7f72\u5b9a\u5236\u6a21\u578b\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u5b9e\u9a8c\u65b0\u65b9\u6cd5\u3002", "conclusion": "MoViAD\u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u6a21\u5757\u5316\u7684\u5e93\uff0c\u65e8\u5728\u4e3aVAD\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u5feb\u901f\u7b80\u4fbf\u7684\u8bbf\u95ee\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u548c\u90e8\u7f72\u6311\u6218\u3002"}}
{"id": "2507.12332", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12332", "abs": "https://arxiv.org/abs/2507.12332", "authors": ["Moorad Alexanian"], "title": "Quantum Phase Transition in the Dicke Model", "comment": null, "summary": "We consider a previously modified Jaynes-Cummings model with single-photon\ncavity radiation field and atomic system exchanging a squeezed photon and\ndeduce a normal/superradiance quantum phase transition in the Dicke model of N\natoms of arbitrary spin with independent co- and counter-rotating coupling\nterms.", "AI": {"tldr": "\u7814\u7a76\u4e86Dicke\u6a21\u578b\u4e2d\u7684\u91cf\u5b50\u76f8\u53d8\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u91cf\u5b50\u5149\u5b66\u548c\u91cf\u5b50\u76f8\u53d8\uff0c\u7279\u522b\u662f\u7814\u7a76Dicke\u6a21\u578b\u4e2d\u539f\u5b50\u4e0e\u5149\u573a\u76f8\u4e92\u4f5c\u7528\u65f6\u53d1\u751f\u7684\u76f8\u53d8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff0c\u5728\u8003\u8651\u4e86\u81ea\u65cb\u548c\u76f8\u4e92\u4f5c\u7528\u9879\u4e4b\u540e\uff0c\u6211\u4eec\u80fd\u591f\u4ece\u6570\u5b66\u4e0a\u63a8\u5bfc\u51fa\u6001\u7684\u6f14\u5316\u3002", "result": "\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u6b63\u5e38/\u8d85\u8f90\u5c04\u91cf\u5b50\u76f8\u53d8\u7684\u7ed3\u679c\uff0c\u8be5\u7ed3\u679c\u57fa\u4e8e\u5bf9Dicke\u6a21\u578b\u7684\u5206\u6790\uff0c\u8be5\u6a21\u578b\u5305\u542bN\u4e2a\u4efb\u610f\u81ea\u65cb\u7684\u539f\u5b50\u4ee5\u53ca\u72ec\u7acb\u7684\u540c\u65cb\u8f6c\u548c\u53cd\u65cb\u8f6c\u8026\u5408\u9879\u3002", "conclusion": "\u672a\u63d0\u4f9b"}}
{"id": "2507.12053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12053", "abs": "https://arxiv.org/abs/2507.12053", "authors": ["Seanglidet Yean", "Jiazu Zhou", "Bu-Sung Lee", "Markus Schl\u00e4pfer"], "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "comment": "International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)", "summary": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ecGAN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u8003\u8651\u52a8\u6001\u57ce\u5e02\u56e0\u7d20\uff08\u5982\u571f\u5730\u5229\u7528\uff09\u7684\u51fa\u884c\u6d41\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u52a0\u5761\u7684\u624b\u673a\u6570\u636e\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u5c45\u6c11\u7684\u51fa\u884c\u6a21\u5f0f\u968f\u571f\u5730\u5229\u7528\u548c\u4eba\u53e3\u7684\u53d8\u5316\u800c\u6f14\u53d8\uff0c\u8fd9\u4f7f\u5f97\u57ce\u5e02\u89c4\u5212\u8005\u5fc5\u987b\u6a21\u62df\u548c\u5206\u6790\u4eba\u7c7b\u51fa\u884c\u6a21\u5f0f\uff0c\u4ee5\u4f18\u5316\u4ea4\u901a\u548c\u5b9e\u73b0\u53ef\u6301\u7eed\u57ce\u5e02\u53d1\u5c55\u3002\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u4e8e\u5386\u53f2\u8f68\u8ff9\uff0c\u5e76\u5ffd\u7565\u4e86\u4eba\u53e3\u5bc6\u5ea6\u548c\u571f\u5730\u5229\u7528\u7b49\u4e0d\u65ad\u53d8\u5316\u7684\u56e0\u7d20\uff1b\u800c\u673a\u68b0\u65b9\u6cd5\u5219\u5047\u8bbe\u573a\u666f\u9759\u6001\u4e0d\u53d8\uff0c\u9650\u5236\u4e86\u5176\u5728\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\u8fdb\u884c\u6821\u51c6\u7684\u672a\u6765\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5386\u53f2\u8f68\u8ff9\u3001\u4eba\u53e3\u5bc6\u5ea6\u3001\u571f\u5730\u5229\u7528\u3001\u52a8\u6001\u533a\u57df\u5927\u5c0f\u548c\u571f\u5730\u5229\u7528\u7c7b\u578b\u7b49\u81ea\u9002\u5e94\u56e0\u7d20\uff0c\u4ee5\u751f\u6210\u51fa\u53d1\u5730-\u76ee\u7684\u5730\u8fc1\u79fb\u6d41\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u65b0\u52a0\u5761\u7684\u624b\u673a\u6570\u636e\uff0c\u5e76\u4e0e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5feb\u901f\u751f\u6210\u5177\u6709\u53ef\u8c03\u7a7a\u95f4\u7c92\u5ea6\u7684\u8fc1\u79fb\u6d41\uff0c\u4e14\u65e0\u9700\u590d\u6742\u7684\u6821\u51c6\u6570\u636e\u6216\u884c\u4e3a\u5efa\u6a21\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u6a21\u62df\u7684\u57ce\u5e02\u60c5\u666f\u751f\u6210\u51fa\u53d1\u5730-\u76ee\u7684\u5730\u8fc1\u79fb\u6d41\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u52a8\u6001\u533a\u57df\u5927\u5c0f\u548c\u571f\u5730\u5229\u7528\u7c7b\u578b\u7b49\u81ea\u9002\u5e94\u56e0\u7d20\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u5c06\u5386\u53f2\u6570\u636e\u4e0e\u8fd9\u4e9b\u81ea\u9002\u5e94\u53c2\u6570\u76f8\u7ed3\u5408\u3002"}}
{"id": "2507.12372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12372", "abs": "https://arxiv.org/abs/2507.12372", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "\u5177\u5907\u7f51\u7edc\u6d4f\u89c8\u80fd\u529b\u7684LLM\u53ef\u4ee5\u4ece\u793e\u4ea4\u5a92\u4f53\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u5c5e\u6027\uff0c\u4f46\u5b58\u5728\u504f\u89c1\u98ce\u9669\uff0c\u5e94\u9650\u5236\u5176\u5728\u516c\u5f00\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "motivation": "\u8bc4\u4f30\u5177\u5907\u7f51\u7edc\u6d4f\u89c8\u80fd\u529b\u7684LLM\u5728\u76f4\u63a5\u68c0\u7d22\u548c\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4ee5\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u5c5e\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\uff0848\u4e2aX\uff08Twitter\uff09\u8d26\u6237\uff09\u548c\u8c03\u67e5\u6570\u636e\u96c6\uff081,384\u540d\u56fd\u9645\u53c2\u4e0e\u8005\uff09\u6765\u8bc4\u4f30LLM\u901a\u8fc7\u7528\u6237\u540d\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u5c5e\u6027\u7684\u80fd\u529b\u3002\u5206\u6790\u4e86LLM\u5982\u4f55\u89e3\u6790\u548c\u89e3\u91ca\u793e\u4ea4\u5a92\u4f53\u8d44\u6599\uff0c\u4ee5\u4e86\u89e3\u6f5c\u5728\u7684\u504f\u89c1\u3002", "result": "LLM\u80fd\u591f\u8bbf\u95ee\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5e76\u4ee5\u5408\u7406\u7684\u51c6\u786e\u6027\u9884\u6d4b\u7528\u6237\u4eba\u53e3\u5c5e\u6027\u3002\u5206\u6790\u663e\u793a\uff0cLLM\u5bf9\u793e\u4ea4\u5a92\u4f53\u8d44\u6599\u7684\u89e3\u6790\u53ef\u80fd\u5bfc\u81f4\u5bf9\u6d3b\u52a8\u91cf\u5c11\u7684\u8d26\u6237\u4ea7\u751f\u6027\u522b\u548c\u653f\u6cbb\u504f\u89c1\u3002", "conclusion": "LLM\u5728\u8bbf\u95ee\u548c\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4ee5\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u5c5e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6027\u522b\u548c\u653f\u6cbb\u504f\u89c1\u4ee5\u53ca\u88ab\u6ee5\u7528\u7684\u98ce\u9669\u3002\u5efa\u8bae\u9650\u5236\u5176\u5728\u516c\u5f00\u5e94\u7528\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u9a8c\u8bc1\u8fc7\u7684\u7814\u7a76\u76ee\u7684\u4fdd\u7559\u53d7\u63a7\u8bbf\u95ee\u3002"}}
{"id": "2507.12060", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.12060", "abs": "https://arxiv.org/abs/2507.12060", "authors": ["Kun-Hsiang Lin", "Yu-Wen Tseng", "Kang-Yang Huang", "Jhih-Ciang Wu", "Wen-Huang Cheng"], "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "comment": "Accepted by MM'25", "summary": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "AI": {"tldr": "InstructFLIP\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5143\u57df\u7b56\u7565\uff0c\u901a\u8fc7\u6587\u672c\u6307\u5bfc\u589e\u5f3a\u4eba\u8138\u53cd\u6b3a\u9a97\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u5347\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8bad\u7ec3\u5197\u4f59\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u53cd\u6b3a\u9a97\uff08FAS\uff09\u6280\u672f\u5728\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u5bf9\u653b\u51fb\u7c7b\u578b\u7684\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u4ee5\u53ca\u8de8\u57df\u8bad\u7ec3\u5197\u4f59\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165VLM\u548c\u5143\u57df\u7b56\u7565\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "InstructFLIP\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u5b83\u96c6\u6210VLM\u6765\u589e\u5f3a\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u5143\u57df\u7b56\u7565\u5b66\u4e60\u4e00\u4e2a\u80fd\u8de8\u591a\u4e2a\u57df\u6cdb\u5316\u7684\u7edf\u4e00\u6a21\u578b\u3002\u8be5\u6846\u67b6\u5c06\u6307\u4ee4\u663e\u5f0f\u89e3\u8026\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u7ec4\u4ef6\uff0c\u5206\u522b\u5173\u6ce8\u6b3a\u9a97\u7684\u6838\u5fc3\u8bed\u4e49\u4ee5\u53ca\u73af\u5883\u548c\u6444\u50cf\u5934\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cInstructFLIP\u5728\u4eba\u8138\u53cd\u6b3a\u9a97\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u51c6\u786e\u6027\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\uff08SOTA\uff09\u6a21\u578b\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8de8\u4e0d\u540cFAS\u57df\u7684\u8bad\u7ec3\u5197\u4f59\u3002", "conclusion": "InstructFLIP\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5143\u57df\u7b56\u7565\uff0c\u5728\u4eba\u8138\u53cd\u6b3a\u9a97\uff08FAS\uff09\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u6cdb\u5316\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8bad\u7ec3\u5197\u4f59\u6311\u6218\u3002"}}
{"id": "2507.12346", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12346", "abs": "https://arxiv.org/abs/2507.12346", "authors": ["Ana Bl\u00e1zquez-Co\u00eddo", "Fadri Gr\u00fcnenfelder", "Anthony Martin", "Raphael Houlmann", "Hugo Zbinden", "Davide Rusca"], "title": "Modulator-free, self-testing quantum random number generator", "comment": "6 pages, 7 figures", "summary": "Quantum random number generators (QRNGs) use the inherent unpredictability of\nquantum mechanics to generate true randomness, as opposed to classical random\nnumber generators. However, ensuring the authenticity of this randomness still\nrequires robust verification. Self-testing QRNGs address this need by enabling\nthe validation of the randomness produced based on the observed data from the\nexperiment while requiring few assumptions. In this work, we present a\npractical, self-testing QRNG designed to operate with an untrusted measurement\ndevice and a partially characterized source, allowing the user to check the\nadequate functioning of the setup in real time. Our experiment yields a rate of\ncertified random bits of 450kbps", "AI": {"tldr": "\u4e00\u4e2a\u80fd\u5728\u4e0d\u53ef\u4fe1\u8bbe\u5907\u4e0a\u5b9e\u65f6\u9a8c\u8bc1\u968f\u673a\u6570\u4ea7\u751f\u7684\u91cf\u5b50\u968f\u673a\u6570\u751f\u6210\u5668\uff0c\u901f\u7387\u4e3a450kbps\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u91cf\u5b50\u968f\u673a\u6570\u751f\u6210\u5668\uff08QRNG\uff09\u4ea7\u751f\u7684\u968f\u673a\u6570\u7684\u771f\u5b9e\u6027\uff0c\u9700\u8981\u8fdb\u884c\u9c81\u68d2\u7684\u9a8c\u8bc1\u3002\u81ea\u6d4b\u8bd5QRNG\u901a\u8fc7\u5141\u8bb8\u57fa\u4e8e\u5b9e\u9a8c\u89c2\u6d4b\u6570\u636e\u6765\u9a8c\u8bc1\u751f\u6210\u7684\u968f\u673a\u6570\uff0c\u540c\u65f6\u4ec5\u9700\u5f88\u5c11\u7684\u5047\u8bbe\u6765\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u81ea\u6d4b\u8bd5\u7684QRNG\uff0c\u8be5\u8bbe\u5907\u80fd\u591f\u4f7f\u7528\u4e0d\u53ef\u4fe1\u7684\u6d4b\u91cf\u8bbe\u5907\u548c\u90e8\u5206\u8868\u5f81\u7684\u4fe1\u6e90\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5b9e\u65f6\u9a8c\u8bc1\u5176\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u6bcf\u79d2450kb\u7684\u8ba4\u8bc1\u968f\u673a\u6bd4\u7279\u751f\u6210\u901f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u53ef\u81ea\u6d4b\u8bd5\u7684\u91cf\u5b50\u968f\u673a\u6570\u751f\u6210\u5668\uff08QRNG\uff09\uff0c\u53ef\u5728\u4e0d\u53ef\u4fe1\u7684\u6d4b\u91cf\u8bbe\u5907\u548c\u90e8\u5206\u8868\u5f81\u7684\u4fe1\u6e90\u4e0b\u8fd0\u884c\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5b9e\u65f6\u68c0\u67e5\u8bbe\u5907\u7684\u8fd0\u884c\u60c5\u51b5\u3002"}}
{"id": "2507.12070", "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.12070", "abs": "https://arxiv.org/abs/2507.12070", "authors": ["George Bird"], "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "comment": "36 pages, 31 figures", "summary": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u5f81\u5bf9\u9f50\u65b9\u6cd5\uff0c\u53d1\u73b0\u4ee3\u6570\u5bf9\u79f0\u6027\u662f\u4efb\u52a1\u65e0\u5173\u7ed3\u6784\u7684\u5173\u952e\u3002\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u4f1a\u5f71\u54cd\u8868\u5f81\u662f\u79bb\u6563\u5316\u8fd8\u662f\u8fde\u7eed\u5316\uff0c\u79bb\u6563\u5bf9\u79f0\u6027\u5bfc\u81f4\u79bb\u6563\u5316\uff0c\u8fde\u7eed\u5bf9\u79f0\u6027\u5219\u4fdd\u6301\u8fde\u7eed\u3002\u8fd9\u79cd\u91cf\u5316\u6548\u5e94\u53ef\u80fd\u662f\u4e0d\u5229\u4e8e\u91cd\u5efa\u7684\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u4ee3\u6570\u5bf9\u79f0\u6027\u5982\u4f55\u9884\u6d4b\u4efb\u52a1\u65e0\u5173\u7ed3\u6784\u5728\u8868\u5f81\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5728\u81ea\u7f16\u7801\u5668\u6a21\u578b\u4e2d\u79bb\u6563\u8868\u5f81\u662f\u5982\u4f55\u5f62\u6210\u548c\u6392\u5217\u7684\uff0c\u7279\u522b\u662f\u6fc0\u6d3b\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u5f81\u5bf9\u9f50\u786e\u5b9a\u65b9\u6cd5\uff0c\u5e76\u501f\u9274\u4e86\u73b0\u6709\u7684Spotlight Resonance\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9\u81ea\u7f16\u7801\u5668\u6a21\u578b\u8fdb\u884c\u6d88\u878d\u7814\u7a76\uff08\u4ec5\u6539\u53d8\u6fc0\u6d3b\u51fd\u6570\uff09\uff0c\u6765\u7814\u7a76\u79bb\u6563\u8868\u5f81\u7684\u5f62\u6210\u548c\u6392\u5217\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u6fc0\u6d3b\u51fd\u6570\u901a\u8fc7\u79bb\u6563\u7684\u4ee3\u6570\u7f6e\u6362\u7b49\u53d8\u5bf9\u79f0\u6027\u5b9a\u4e49\u65f6\uff0c\u8868\u5f81\u503e\u5411\u4e8e\u79bb\u6563\u5316\uff1b\u800c\u5728\u8fde\u7eed\u7684\u4ee3\u6570\u6b63\u4ea4\u7b49\u53d8\u5b9a\u4e49\u4e0b\uff0c\u8868\u5f81\u4fdd\u6301\u8fde\u7eed\u3002\u8fd9\u8868\u660e\u529f\u80fd\u5f62\u5f0f\u7684\u9009\u62e9\u4f1a\u5e26\u6765\u975e\u9884\u671f\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u5bfc\u81f4\u8868\u5f81\u7ed3\u6784\u7684\u4eba\u5de5\u79bb\u6563\u5316\uff08\u91cf\u5316\u6548\u5e94\uff09\u3002\u91cf\u5316\u6548\u5e94\u4e0e\u91cd\u5efa\u8bef\u5dee\u7684\u589e\u52a0\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u786e\u5b9a\u8868\u5f81\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4ee3\u6570\u5bf9\u79f0\u6027\u662f\u4efb\u52a1\u65e0\u5173\u7ed3\u6784\u5728\u8868\u5f81\u4e2d\u7684\u6709\u529b\u9884\u6d4b\u56e0\u5b50\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u81ea\u7f16\u7801\u5668\u6a21\u578b\u4e2d\uff0c\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u4f1a\u5f71\u54cd\u8868\u5f81\u7684\u79bb\u6563\u5316\u6216\u8fde\u7eed\u6027\uff0c\u79bb\u6563\u7684\u4ee3\u6570\u7f6e\u6362\u7b49\u53d8\u5bf9\u79f0\u6027\u503e\u5411\u4e8e\u4f7f\u8868\u5f81\u79bb\u6563\u5316\uff0c\u800c\u8fde\u7eed\u7684\u4ee3\u6570\u6b63\u4ea4\u7b49\u53d8\u5b9a\u4e49\u5219\u4fdd\u6301\u8868\u5f81\u8fde\u7eed\u3002\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u4e86\u529f\u80fd\u5f62\u5f0f\u7684\u9009\u62e9\u53ef\u80fd\u5e26\u6709\u975e\u9884\u671f\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u4ece\u800c\u5728\u8868\u5f81\u4e2d\u4ea7\u751f\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u4eba\u5de5\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5f53\u524d\u5f62\u5f0f\u4f1a\u4f7f\u539f\u672c\u8fde\u7eed\u7684\u7ed3\u6784\u79bb\u6563\u5316\uff08\u91cf\u5316\u6548\u5e94\uff09\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u652f\u6301\u4e86\u4e00\u4e2a\u5173\u4e8e\u79bb\u6563\u8868\u5f81\u5982\u4f55\u5f62\u6210\u7684\u901a\u7528\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u53ef\u80fd\u4e3a\u4e0b\u6e38\u7684\u53ef\u89e3\u91ca\u6027\u73b0\u8c61\uff08\u5982\u7956\u6bcd\u795e\u7ecf\u5143\u3001\u79bb\u6563\u7f16\u7801\u65b9\u6848\u3001\u901a\u7528\u7ebf\u6027\u7279\u5f81\u548c\u53e0\u52a0\uff09\u63d0\u4f9b\u5148\u51b3\u6761\u4ef6\u3002\u6700\u540e\uff0c\u521d\u6b65\u7ed3\u679c\u8868\u660e\u8868\u5f81\u7684\u91cf\u5316\u4e0e\u91cd\u5efa\u8bef\u5dee\u7684\u589e\u52a0\u76f8\u5173\uff0c\u8fd9\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u5d29\u6e83\u53ef\u80fd\u662f\u6709\u5bb3\u7684\u3002"}}
{"id": "2507.12379", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12379", "abs": "https://arxiv.org/abs/2507.12379", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "title": "Probing for Arithmetic Errors in Language Models", "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "\u5185\u90e8\u6fc0\u6d3b\u53ef\u7528\u4e8e\u68c0\u6d4b\u548c\u7ea0\u6b63\u8bed\u8a00\u6a21\u578b\u7684\u7b97\u672f\u9519\u8bef\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u662f\u5426\u80fd\u7528\u4e8e\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u6765\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u5e76\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7\u7684\u9519\u8bef\u68c0\u6d4b\u5668\u6765\u9884\u6d4b\u6a21\u578b\u6b63\u786e\u6027\uff0c\u5c06\u5206\u6790\u6269\u5c55\u5230\u94fe\u5f0f\u601d\u8003\u8f68\u8ff9\uff0c\u5e76\u6f14\u793a\u4e86\u63a2\u9488\u5982\u4f55\u6307\u5bfc\u9009\u62e9\u6027\u91cd\u63d0\u793a\u3002", "result": "\u63a2\u9488\u53ef\u4ee5\u51c6\u786e\u5730\u4ece\u9690\u85cf\u72b6\u6001\u89e3\u7801\u6a21\u578b\u9884\u6d4b\u7684\u8f93\u51fa\u548c\u6b63\u786e\u7b54\u6848\uff0c\u9519\u8bef\u68c0\u6d4b\u5668\u53ef\u4ee5\u4ee5\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\u9884\u6d4b\u6a21\u578b\u6b63\u786e\u6027\uff0c\u63a2\u9488\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u66f4\u590d\u6742\u7684\u94fe\u5f0f\u601d\u8003\u8bbe\u7f6e\uff0c\u5e76\u80fd\u6307\u5bfc\u9009\u62e9\u6027\u91cd\u63d0\u793a\u4ee5\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "\u5185\u90e8\u6fc0\u6d3b\u53ef\u4ee5\u7528\u6765\u9884\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684\u63a2\u9488\u6765\u6307\u5bfc\u6a21\u578b\u7684\u9009\u62e9\u6027\u91cd\u63d0\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002"}}
{"id": "2507.12062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12062", "abs": "https://arxiv.org/abs/2507.12062", "authors": ["Hongxu Ma", "Guanshuo Wang", "Fufu Yu", "Qiong Jia", "Shouhong Ding"], "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "comment": "Accepted by ACM MM'25", "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.", "AI": {"tldr": "\u63d0\u51faMS-DETR\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u53bb\u566a\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u77e9\u68c0\u7d22\u548c\u9ad8\u4eae\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "DETR\u7c7b\u8054\u5408\u6846\u67b6\u5728\u89c6\u9891\u77e9\u68c0\u7d22\uff08MR\uff09\u548c\u9ad8\u4eae\u68c0\u6d4b\uff08HD\uff09\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u89c6\u9891\u5185\u5bb9\u4e2d\u65f6\u95f4\u8fd0\u52a8\u548c\u7a7a\u95f4\u8bed\u4e49\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u4ecd\u6709\u5f85\u6316\u6398\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMS-DETR\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u6765\u6355\u83b7\u4e30\u5bcc\u7684\u8fd0\u52a8-\u8bed\u4e49\u7279\u5f81\uff0c\u4ee5\u5b8c\u6210MR/HD\u4efb\u52a1\u3002\u7f16\u7801\u5668\u9996\u5148\u660e\u786e\u5730\u5728\u8fd0\u52a8\u548c\u8bed\u4e49\u7ef4\u5ea6\u5185\u5bf9\u6a21\u578b\u8fdb\u884c\u89e3\u8026\u7684\u6a21\u5f0f\u5185\u76f8\u5173\u6027\uff0c\u5e76\u4ee5\u7ed9\u5b9a\u7684\u6587\u672c\u67e5\u8be2\u4e3a\u6307\u5bfc\u3002\u968f\u540e\uff0c\u89e3\u7801\u5668\u5229\u7528\u8de8\u65f6\u95f4\u8fd0\u52a8\u548c\u7a7a\u95f4\u8bed\u4e49\u7ef4\u5ea6\u7684\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u4ee5\u5b9e\u73b0MR\u7684\u7cbe\u786e\u67e5\u8be2\u5f15\u5bfc\u5b9a\u4f4d\u548cHD\u7684\u7cbe\u7ec6\u9ad8\u4eae\u8fb9\u754c\u5212\u5206\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u89e3\u51b3MR/HD\u6570\u636e\u96c6\u4e2d\u56fa\u6709\u7684\u7a00\u758f\u56f0\u5883\uff0c\u901a\u8fc7\u751f\u6210\u7b56\u7565\u4e30\u5bcc\u4e86\u4e24\u4e2a\u7ef4\u5ea6\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u63d0\u51fa\u5bf9\u6bd4\u53bb\u566a\u5b66\u4e60\uff0c\u4ee5\u786e\u4fdd\u4e0a\u8ff0\u7ec4\u4ef6\u80fd\u591f\u7a33\u5065\u6709\u6548\u5730\u5b66\u4e60\u3002", "result": "MS-DETR\u5728\u56db\u4e2aMR/HD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "MS-DETR\u5728\u56db\u4e2aMR/HD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2507.12389", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12389", "abs": "https://arxiv.org/abs/2507.12389", "authors": ["A. A. Melkozerov", "M. Yu. Saygin", "S. S. Straupe"], "title": "Entanglement-efficiency trade-offs in the fusion-based generation of photonic GHZ-like states", "comment": "11 pages, 10 figures", "summary": "Probabilistic entangling measurements are key operations in linear-optical\nquantum technologies, enabling the generation and manipulation of\nhigh-dimensional quantum states. While prior research has focused predominantly\non specific entangled states, notably graph states and\nGreenberger-Horne-Zeilinger (GHZ) states, broader classes of states with\nvariable entanglement remain underexplored. In this work, we present a\nlinear-optical approach for generating and fusing GHZ-like states, which\ngeneralize standard GHZ states to include variable entanglement degrees. We\nintroduce two schemes based on modified fusion gates that allow flexible\ncontrol over generation efficiency and the entanglement of the output states.\nThese results offer a promising pathway toward resource-efficient\nentangled-state generation for scalable quantum computing and communication.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u5149\u5b66\u65b9\u6cd5\u6765\u751f\u6210\u5177\u6709\u53ef\u53d8\u7ea0\u7f20\u5ea6\u7684\u7c7b GHZ \u72b6\u6001\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7279\u5b9a\u7684\u7ea0\u7f20\u6001\uff0c\u5982\u56fe\u6001\u548c GHZ \u6001\uff0c\u800c\u5177\u6709\u53ef\u53d8\u7ea0\u7f20\u5ea6\u7684\u66f4\u5e7f\u6cdb\u72b6\u6001\u7c7b\u522b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6539\u8fdb\u7684\u878d\u5408\u95e8\u7684\u7ebf\u6027\u5149\u5b66\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u548c\u878d\u5408\u7c7b GHZ \u72b6\u6001\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u6548\u7387\u548c\u8f93\u51fa\u72b6\u6001\u7ea0\u7f20\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7684\u878d\u5408\u95e8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u751f\u6210\u548c\u878d\u5408\u7c7b GHZ \u72b6\u6001\u7684\u7ebf\u6027\u5149\u5b66\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u72b6\u6001\u5177\u6709\u53ef\u53d8\u7684\u7ea0\u7f20\u5ea6\uff0c\u53ef\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8ba1\u7b97\u548c\u901a\u4fe1\u3002"}}
{"id": "2507.12425", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.12425", "abs": "https://arxiv.org/abs/2507.12425", "authors": ["Chandana Cheerla"], "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "\u4e00\u4e2a\u6539\u8fdb\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u68c0\u7d22\u3001\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u8bed\u4e49\u5206\u5757\u548c\u4fdd\u7559\u8868\u683c\u7ed3\u6784\u6765\u589e\u5f3a\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u4f01\u4e1a\u6570\u636e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u53d7\u5230\u9759\u6001\u9884\u8bad\u7ec3\u3001\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u4ee5\u53ca\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f\u7684\u6311\u6218\u7684\u9650\u5236\u3002\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u867d\u7136\u89e3\u51b3\u4e86\u4e00\u4e9b\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u5728\u5904\u7406\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684RAG\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f7f\u7528\u5bc6\u96c6\u5d4c\u5165\uff08all-mpnet-base-v2\uff09\u548cBM25\u7684\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u5e76\u901a\u8fc7SpaCy NER\u7684\u5143\u6570\u636e\u611f\u77e5\u8fc7\u6ee4\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u8fdb\u884c\u4e86\u589e\u5f3a\u3002\u8be5\u6846\u67b6\u5e94\u7528\u8bed\u4e49\u5206\u5757\u4ee5\u4fdd\u6301\u6587\u672c\u8fde\u8d2f\u6027\uff0c\u5e76\u4fdd\u7559\u8868\u683c\u6570\u636e\u7ed3\u6784\u4ee5\u4fdd\u6301\u884c-\u5217\u5b8c\u6574\u6027\u3002\u91cf\u5316\u7d22\u5f15\u4f18\u5316\u4e86\u68c0\u7d22\u6548\u7387\uff0c\u800c\u4eba\u5728\u56de\u8def\u53cd\u9988\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4f01\u4e1a\u6570\u636e\u96c6\u4e0a\u7684Precision@5\u63d0\u9ad8\u4e8615%\uff0890\u5bf975\uff09\uff0cRecall@5\u63d0\u9ad8\u4e8613%\uff0887\u5bf974\uff09\uff0c\u5e73\u5747\u5012\u6570\u6392\u540d\u63d0\u9ad8\u4e8616%\uff080.85\u5bf90.69\uff09\u3002\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u57285\u70b9\u674e\u514b\u7279\u91cf\u8868\u4e0a\uff0c\u5fe0\u5b9e\u5ea6\uff084.6\u5bf93.0\uff09\u3001\u5b8c\u6574\u6027\uff084.2\u5bf92.5\uff09\u548c\u76f8\u5173\u6027\uff084.5\u5bf93.2\uff09\u5f97\u5206\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4f01\u4e1a\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u5168\u9762\u4e14\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u54cd\u5e94\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u5305\u62ec\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u548c\u6574\u5408\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u3002"}}
{"id": "2507.12394", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12394", "abs": "https://arxiv.org/abs/2507.12394", "authors": ["Erik Altelarrea-Ferr\u00e9", "J\u00falia Barber\u00e0-Rodr\u00edguez", "David Jansen", "Antonio Ac\u00edn"], "title": "Beyond Ground States: Physics-Inspired Optimization of Excited States of Classical Hamiltonians", "comment": "12 pages, 6 figures", "summary": "We introduce excited local quantum annealing (ExcLQA), a classical,\nphysics-inspired algorithm that extends local quantum annealing (LQA) to\nidentify excited states of classical Ising Hamiltonians. LQA simulates quantum\nannealing while constraining the quantum state to remain in a product state and\nuses a gradient-based approach to find approximate solutions to large-scale\nquadratic unconstrained binary optimization problems. ExcLQA extends this\nframework by adding a penalty term in the cost function to target excited\nstates, with a single hyperparameter that can be tuned via binary search to set\nthe desired penalization level. We benchmark ExcLQA on the shortest vector\nproblem (SVP), a fundamental lattice problem underlying the security of many\npostquantum cryptographic schemes. Solving an SVP instance can be mapped to\nidentifying the first excited state of a Hamiltonian, with approximate\nsolutions located among nearby excited states. Our results show that ExcLQA\nmanages to solve SVP instances up to rank 46, and outperforms the\nMetropolis-Hastings algorithm in solved ratio, number of shots, and\napproximation factor in the tested instances.", "AI": {"tldr": "ExcLQA \u662f\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u7b97\u6cd5\uff0c\u53ef\u4ee5\u8bc6\u522b\u7ecf\u5178\u4f0a\u8f9b\u54c8\u5bc6\u987f\u91cf\u7684\u6fc0\u53d1\u6001\uff0c\u5e76\u80fd\u6709\u6548\u89e3\u51b3\u6700\u77ed\u5411\u91cf\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e Metropolis-Hastings \u7b97\u6cd5\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6269\u5c55\u5c40\u90e8\u91cf\u5b50\u9000\u706b\uff08LQA\uff09\u4ee5\u8bc6\u522b\u7ecf\u5178\u4f0a\u8f9b\u54c8\u5bc6\u987f\u91cf\u7684\u6fc0\u53d1\u6001\uff0c\u5e76\u5229\u7528\u6b64\u80fd\u529b\u89e3\u51b3\u6f5c\u5728\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u65b9\u6848\u7684\u5b89\u5168\u6027\u7684\u6700\u77ed\u5411\u91cf\u95ee\u9898\uff08SVP\uff09\u3002", "method": "ExcLQA \u7b97\u6cd5\u901a\u8fc7\u5728\u6210\u672c\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u6269\u5c55 LQA\uff0c\u4ee5\u76ee\u6807\u6fc0\u53d1\u6001\uff0c\u5e76\u4f7f\u7528\u5355\u4e2a\u8d85\u53c2\u6570\u901a\u8fc7\u4e8c\u5206\u67e5\u627e\u6765\u8bbe\u7f6e\u6240\u9700\u7684\u60e9\u7f5a\u7ea7\u522b\u3002", "result": "ExcLQA \u80fd\u591f\u89e3\u51b3\u9ad8\u8fbe 46 \u9636\u7684 SVP \u5b9e\u4f8b\uff0c\u5e76\u4e14\u5728\u89e3\u51b3\u6bd4\u4f8b\u3001\u8ba1\u7b97\u6b21\u6570\u548c\u8fd1\u4f3c\u56e0\u5b50\u65b9\u9762\u4f18\u4e8e Metropolis-Hastings \u7b97\u6cd5\u3002", "conclusion": "ExcLQA \u7b97\u6cd5\u5728\u89e3\u51b3\u6700\u77ed\u5411\u91cf\u95ee\u9898\uff08SVP\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u8fbe 46 \u9636\u7684\u5b9e\u4f8b\uff0c\u5e76\u5728\u5df2\u6d4b\u8bd5\u7684\u5b9e\u4f8b\u4e2d\u5728\u89e3\u51b3\u6bd4\u4f8b\u3001\u8ba1\u7b97\u6b21\u6570\u548c\u8fd1\u4f3c\u56e0\u5b50\u65b9\u9762\u4f18\u4e8e Metropolis-Hastings \u7b97\u6cd5\u3002"}}
{"id": "2507.12127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12127", "abs": "https://arxiv.org/abs/2507.12127", "authors": ["Ngoc Duy Pham", "Thusitha Dayaratne", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "comment": null, "summary": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u7ebf\u901a\u4fe1\u9891\u8c31\u611f\u77e5\u4e2d\u5927\u6709\u53ef\u4e3a\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u7528\u201c\u75ab\u82d7\u201d\u62b5\u5fa1\u4e86\u6076\u610f\u653b\u51fb\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b35G\u548c6G\u53d1\u5c55\u5e26\u6765\u7684\u9891\u8c31\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u53ca\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u5728\u52a8\u6001\u9891\u8c31\u5206\u914d\uff08DSA\uff09\u4e2d\u7684\u9690\u79c1\u3001\u5e26\u5bbd\u548c\u76d1\u7ba1\u9650\u5236\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u9891\u8c31\u611f\u77e5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u80fd\u91cf\u68c0\u6d4b\u7684\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u75ab\u82d7\u63a5\u79cd\u601d\u60f3\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\uff0cFLSS\u5728\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u62b5\u5fa1\u5927\u91cf\u6076\u610f\u53c2\u4e0e\u8005\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u9891\u8c31\u611f\u77e5\uff08FLSS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u4e86\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u75ab\u82d7\u63a5\u79cd\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u548c\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.12428", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790CoT\u7684\u6fc0\u6d3b\uff0c\u53ef\u4ee5\u63d0\u524d\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u7ec8\u54cd\u5e94\u662f\u5426\u5b89\u5168\u3002", "motivation": "\u5f00\u653e\u6743\u91cd\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u4f1a\u751f\u6210\u957fCoT\uff0c\u8fd9\u4f1a\u5e26\u6765\u989d\u5916\u7684\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u4e3a\u6709\u5bb3\u5185\u5bb9\u7ecf\u5e38\u51fa\u73b0\u5728CoT\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528CoT\u6765\u9884\u6d4b\u6700\u7ec8\u54cd\u5e94\u7684\u9519\u8bef\u3002", "method": "\u7814\u7a76\u4e86\u4eba\u7c7b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5206\u7c7b\u5668\u7b49\u591a\u79cd\u76d1\u63a7\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4e86CoT\u6587\u672c\u548c\u6fc0\u6d3b\u3002", "result": "\u4e0e\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7b80\u5355\u7ebf\u6027\u63a2\u9488\u5728\u9884\u6d4b\u6700\u7ec8\u54cd\u5e94\u662f\u5426\u5b89\u5168\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u8be5\u63a2\u9488\u5728\u63a8\u7406\u65e9\u671f\u5c31\u80fd\u505a\u51fa\u51c6\u786e\u9884\u6d4b\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u3001\u7cfb\u5217\u548c\u5b89\u5168\u57fa\u51c6\u4e0a\u90fd\u5177\u6709\u666e\u904d\u6027\u3002", "conclusion": "CoT\u7684\u6fc0\u6d3b\u53ef\u4ee5\u7528\u6765\u9884\u6d4b\u6700\u7ec8\u54cd\u5e94\u7684\u5b89\u5168\u6027\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u5b8c\u6210\u4e4b\u524d\u5c31\u80fd\u505a\u51fa\u51c6\u786e\u7684\u9884\u6d4b\u3002"}}
{"id": "2507.12087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12087", "abs": "https://arxiv.org/abs/2507.12087", "authors": ["Xiang Yu", "Xinyao Liu", "Guang Liang"], "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association", "comment": null, "summary": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u89c6\u89d2\u4e0b\u5c0f\u578b\u3001\u654f\u6377\u591a\u76ee\u6807\u8ddf\u8e2a\uff08SMOT\uff09\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u76ee\u6807\u5916\u89c2\u7279\u5f81\u7a00\u758f\u3001\u8fd0\u52a8\u7ea0\u7f20\u548c\u9891\u7e41\u906e\u6321\u7b49\u96be\u9898\u3002\u901a\u8fc7\u540d\u4e3aSliceTrain\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u4e0d\u4f9d\u8d56\u5916\u89c2\u4fe1\u606f\u7684OC-SORT\u6539\u8fdb\u8ddf\u8e2a\u5668\uff08\u5305\u542bEMA\u548c\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5ea6\u91cf\uff09\uff0c\u5728SMOT4SB\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u4ece\u65e0\u4eba\u673a\uff08UAV\uff09\u89c6\u89d2\u8ddf\u8e2a\u5c0f\u578b\u3001\u654f\u6377\u7684\u591a\u76ee\u6807\uff08SMOT\uff09\uff0c\u4f8b\u5982\u9e1f\u7c7b\uff0c\u662f\u4e00\u4e2a\u6781\u5177\u6311\u6218\u6027\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u5176\u56f0\u96be\u4e3b\u8981\u6e90\u4e8e\u4e09\u4e2a\u65b9\u9762\uff1a\u76ee\u6807\u5916\u89c2\u7279\u5f81\u7684\u6781\u7aef\u7a00\u758f\u6027\u3001\u7531\u76f8\u673a\u548c\u76ee\u6807\u81ea\u8eab\u52a8\u529b\u5b66\u5f15\u8d77\u7684\u590d\u6742\u8fd0\u52a8\u7ea0\u7f20\uff0c\u4ee5\u53ca\u5bc6\u96c6\u7fa4\u4f53\u884c\u4e3a\u5bfc\u81f4\u7684\u9891\u7e41\u906e\u6321\u548c\u8eab\u4efd\u6a21\u7cca\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u8ddf\u8e2a\u68c0\u6d4b\u8303\u5f0f\uff0c\u5e76\u5728\u68c0\u6d4b\u548c\u5173\u8054\u5c42\u9762\u8fdb\u884c\u4e86\u521b\u65b0\u3002\u5728\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSliceTrain\u7684\u7cfb\u7edf\u6027\u8bad\u7ec3\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u5168\u8986\u76d6\u5207\u7247\u548c\u5207\u7247\u7ea7\u968f\u673a\u589e\u5f3a\u6765\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8bad\u7ec3\u4e2d\u5c0f\u578b\u7269\u4f53\u5b66\u4e60\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5728\u8ddf\u8e2a\u65b9\u9762\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b8c\u5168\u72ec\u7acb\u4e8e\u5916\u89c2\u4fe1\u606f\u7684\u9c81\u68d2\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u96c6\u6210\u8fd0\u52a8\u65b9\u5411\u7ef4\u6301\uff08EMA\uff09\u673a\u5236\u548c\u7ed3\u5408\u8fb9\u754c\u6846\u6269\u5c55\u4e0e\u8ddd\u79bb\u60e9\u7f5a\u7684\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u5230OC-SORT\u6846\u67b6\u4e2d\uff0c\u80fd\u591f\u7a33\u5b9a\u5904\u7406\u4e0d\u89c4\u5219\u8fd0\u52a8\u5e76\u4fdd\u6301\u76ee\u6807\u8eab\u4efd\u3002", "result": "\u5728SMOT4SB\u516c\u5171\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8655.205\u7684SO-HOTA\u5206\u6570\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728SMOT4SB\u516c\u5171\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8655.205\u7684SO-HOTA\u5206\u6570\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u4e16\u754cSMOT\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u548c\u5148\u8fdb\u6027\u3002"}}
{"id": "2507.12408", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12408", "abs": "https://arxiv.org/abs/2507.12408", "authors": ["Matilde Baroni", "Dominik Leichtle", "Sini\u0161a Jankovi\u0107", "Ivan \u0160upi\u0107"], "title": "Bounding the asymptotic quantum value of all multipartite compiled non-local games", "comment": "74 pages, 14 figures", "summary": "Non-local games are a powerful tool to distinguish between correlations\npossible in classical and quantum worlds. Kalai et al. (STOC'23) proposed a\ncompiler that converts multipartite non-local games into interactive protocols\nwith a single prover, relying on cryptographic tools to remove the assumption\nof physical separation of the players. While quantum completeness and classical\nsoundness of the construction have been established for all multipartite games,\nquantum soundness is known only in the special case of bipartite games.\n  In this paper, we prove that the Kalai et al.'s compiler indeed achieves\nquantum soundness for all multipartite compiled non-local games, by showing\nthat any correlations that can be generated in the asymptotic case correspond\nto quantum commuting strategies.\n  Our proof uses techniques from the theory of operator algebras, and relies on\na characterisation of sequential operationally no-signalling strategies as\nquantum commuting operator strategies in the multipartite case, thereby\ngeneralising several previous results. On the way, we construct universal\nC*-algebras of sequential PVMs and prove a new chain rule for Radon-Nikodym\nderivatives of completely positive maps on C*-algebras which may be of\nindependent interest.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86 Kalai \u7b49\u4eba\u7684\u7f16\u8bd1\u5668\uff0c\u4ee5\u5b9e\u73b0\u6240\u6709\u591a\u65b9\u975e\u5c40\u90e8\u535a\u5f08\u7684\u91cf\u5b50\u5065\u5168\u6027\uff0c\u4f7f\u7528\u4e86\u7b97\u5b50\u4ee3\u6570\u65b9\u6cd5\u3002", "motivation": "\u533a\u5206\u7ecf\u5178\u4e16\u754c\u548c\u91cf\u5b50\u4e16\u754c\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u76f8\u5173\u6027\uff0c\u5e76\u89e3\u51b3\u4e86 Kalai \u7b49\u4eba\u63d0\u51fa\u7684\u7f16\u8bd1\u5668\u5728\u591a\u65b9\u535a\u5f08\u7684\u91cf\u5b50\u5065\u5168\u6027\u65b9\u9762\u7684\u5df2\u77e5\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u7b97\u5b50\u4ee3\u6570\u7406\u8bba\u6280\u672f\uff0c\u5c06\u987a\u5e8f\u64cd\u4f5c\u65e0\u4fe1\u53f7\u7b56\u7565\u523b\u753b\u4e3a\u591a\u65b9\u60c5\u51b5\u4e0b\u7684\u91cf\u5b50\u4ea4\u6362\u7b97\u5b50\u7b56\u7565\uff0c\u4ece\u800c\u63a8\u5e7f\u4e86\u5148\u524d\u7684\u4e00\u4e9b\u7ed3\u679c\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fd8\u6784\u5efa\u4e86\u987a\u5e8f PVM \u7684\u901a\u7528 C*-\u4ee3\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u9002\u7528\u4e8e C*-\u4ee3\u6570\u4e0a\u7684\u5b8c\u5168\u6b63\u6620\u5c04\u7684 Radon-Nikodym \u5bfc\u6570\u7684\u94fe\u5f0f\u6cd5\u5219\u3002", "result": "\u8bc1\u660e\u4e86 Kalai \u7b49\u4eba\u7684\u7f16\u8bd1\u5668\u5b9e\u73b0\u4e86\u6240\u6709\u591a\u65b9\u7f16\u8bd1\u7684\u975e\u5c40\u90e8\u535a\u5f08\u7684\u91cf\u5b50\u5065\u5168\u6027\u3002", "conclusion": "\u672c\u8bba\u6587\u8bc1\u660e\u4e86 Kalai \u7b49\u4eba\u7684\u7f16\u8bd1\u5668\u786e\u5b9e\u4e3a\u6240\u6709\u591a\u65b9\u7f16\u8bd1\u7684\u975e\u5c40\u90e8\u535a\u5f08\u5b9e\u73b0\u4e86\u91cf\u5b50\u5065\u5168\u6027\uff0c\u65b9\u6cd5\u662f\u5c06\u6e10\u8fdb\u60c5\u51b5\u4e0b\u7684\u4efb\u4f55\u53ef\u751f\u6210\u7684\u76f8\u5173\u6027\u5f52\u7ed3\u4e3a\u91cf\u5b50\u4ea4\u6362\u7b56\u7565\u3002"}}
{"id": "2507.12451", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12451", "abs": "https://arxiv.org/abs/2507.12451", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "S2WTM\u662f\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\uff0c\u901a\u8fc7\u4f7f\u7528\u7403\u9762\u5207\u7247\u74e6\u745f\u65af\u5766\u8ddd\u79bb\u6765\u89e3\u51b3\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff0c\u5e76\u5728\u4e3b\u9898\u8fde\u8d2f\u6027\u3001\u591a\u6837\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u6a21\u8d85\u7403\u7ed3\u6784\uff0c\u540c\u65f6\u7f13\u89e3VAE-NTMs\u5e38\u51fa\u73b0\u7684\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff08\u8fd9\u4f1a\u5bfc\u81f4KL\u6563\u5ea6\u9879\u5728\u76ee\u6807\u51fd\u6570\u4e2d\u6025\u5267\u51cf\u5c0f\uff0c\u4ea7\u751f\u65e0\u6548\u7684\u6f5c\u5728\u8868\u793a\uff09\u3002", "method": "S2WTM\u91c7\u7528\u652f\u6301\u5355\u4f4d\u8d85\u7403\u9762\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u7403\u9762\u5207\u7247\u74e6\u745f\u65af\u5766\u8ddd\u79bb\u4f7f\u805a\u5408\u540e\u9a8c\u5206\u5e03\u4e0e\u5148\u9a8c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS2WTM\u5728\u4e3b\u9898\u6a21\u578b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u80fd\u751f\u6210\u66f4\u8fde\u8d2f\u3001\u66f4\u591a\u6837\u7684\u4e3b\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "S2WTM\u5728\u4e3b\u9898\u6a21\u578b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u80fd\u751f\u6210\u66f4\u8fde\u8d2f\u3001\u66f4\u591a\u6837\u7684\u4e3b\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12092", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12092", "abs": "https://arxiv.org/abs/2507.12092", "authors": ["Nataliia Molchanova", "Alessandro Cagol", "Mario Ocampo-Pineda", "Po-Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin Beck", "Charidimos Tsagkas", "Daniel Reich", "Colin Vanden Bulcke", "Anna Stolting", "Serena Borrelli", "Pietro Maggi", "Adrien Depeursinge", "Cristina Granziera", "Henning Mueller", "Pedro M. Gordaliza", "Meritxell Bach Cuadra"], "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis", "comment": null, "summary": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684 MRI \u76ae\u5c42\u75c5\u53d8\u68c0\u6d4b\u548c\u5206\u5272\u7cfb\u7edf\uff0c\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u548c\u5efa\u8bae\u3002", "motivation": "\u7531\u4e8e CL \u5728 MRI \u4e0a\u7684\u6210\u50cf\u7279\u70b9\u3001\u4e13\u5bb6\u6807\u6ce8\u7684\u6311\u6218\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5176\u5728\u4e34\u5e8a\u4e0a\u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u6539\u8fdb CL \u7684\u68c0\u6d4b\u548c\u5206\u5272\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528 nnU-Net \u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4e86\u9488\u5bf9\u6027\u7684\u6539\u8fdb\uff0c\u5bf9\u6765\u81ea\u56db\u4e2a\u673a\u6784\u7684 656 \u4e2a MRI \u626b\u63cf\u8fdb\u884c\u4e86 CL \u68c0\u6d4b\u548c\u5206\u5272\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5206\u5e03\u5916\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u540c\u57df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa 0.64 \u7684 F1 \u5206\u6570\uff0c\u5728\u5206\u5e03\u5916\u6d4b\u8bd5\u4e2d\u4e3a 0.5\uff0c\u8868\u660e\u4e86\u5176\u5f3a\u5927\u7684\u75c5\u53d8\u68c0\u6d4b\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u6a21\u578b\u5185\u90e8\u7279\u5f81\u548c\u9519\u8bef\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3 AI \u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u53d8\u5f02\u6027\u3001\u75c5\u53d8\u6a21\u7cca\u6027\u548c\u65b9\u6848\u5dee\u5f02\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u5272\u591a\u53d1\u6027\u786c\u5316\u75c7\uff08MS\uff09\u76ae\u5c42\u75c5\u53d8\uff08CL\uff09\u7684\u7efc\u5408\u6027\u591a\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u4e3a\u514b\u670d\u4e34\u5e8a\u5e94\u7528\u969c\u788d\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2507.12429", "categories": ["quant-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2507.12429", "abs": "https://arxiv.org/abs/2507.12429", "authors": ["Wes Johnson", "Bryce Bullock", "Athreya Shankar", "John Zaris", "John J. Bollinger", "Scott E. Parker"], "title": "Adiabatic Cooling of Planar Motion in a Penning Trap Ion Crystal to Sub-Millikelvin Temperatures", "comment": "11 pages, 6 figures. Submitted to Physical Review A", "summary": "Two-dimensional planar ion crystals in a Penning trap are a platform for\nquantum information science experiments. However, the low-frequency planar\nmodes of these crystals are not efficiently cooled by laser cooling, which can\nlimit the utility of the drumhead modes for quantum information processing.\nRecently, it has been shown that nonlinear mode coupling can enhance the\ncooling of the low-frequency planar modes. Here, we demonstrate in numerical\nsimulations that this coupling can be dynamically tuned by adiabatically\nchanging the rotation frequency of the ion crystal during experiments.\nFurthermore, we show that this technique can, in addition, produce lower\ntemperatures for the low-frequency planar modes via an adiabatic cooling\nprocess. This result allows cooling of the planar modes to sub-millikelvin\ntemperatures, resulting in improved spectral resolution of the drumhead modes\nat experimentally relevant rotation frequencies, which is crucial for quantum\ninformation processing applications.", "AI": {"tldr": "\u901a\u8fc7\u6539\u53d8\u79bb\u5b50\u6676\u4f53\u65cb\u8f6c\u9891\u7387\u6765\u51b7\u5374\u4f4e\u9891\u6a21\u5f0f\u4ee5\u63d0\u9ad8\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u6548\u7387\u3002", "motivation": "\u4e8c\u7ef4\u5e73\u9762\u79bb\u5b50\u6676\u4f53\u5728 Penning \u9677\u9631\u4e2d\u662f\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u5b9e\u9a8c\u7684\u5e73\u53f0\uff0c\u4f46\u4f4e\u9891\u5e73\u9762\u6a21\u5f0f\u7684\u51b7\u5374\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u6f14\u793a\u4e86\u7edd\u70ed\u5730\u6539\u53d8\u79bb\u5b50\u6676\u4f53\u7684\u65cb\u8f6c\u9891\u7387\u6765\u52a8\u6001\u8c03\u6574\u975e\u7ebf\u6027\u6a21\u5f0f\u8026\u5408\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6280\u672f\u53ef\u4ee5\u4ea7\u751f\u66f4\u4f4e\u7684\u4e9a\u6beb\u5f00\u5c14\u6587\u6e29\u5ea6\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u6280\u672f\u53ef\u4ee5\u5c06\u4f4e\u9891\u5e73\u9762\u6a21\u5f0f\u51b7\u5374\u81f3\u4e9a\u6beb\u5f00\u5c14\u6587\u6e29\u5ea6\uff0c\u63d0\u9ad8\u4e86\u9f13\u819c\u6a21\u5f0f\u5728\u5b9e\u9a8c\u76f8\u5173\u65cb\u8f6c\u9891\u7387\u4e0b\u7684\u5149\u8c31\u5206\u8fa8\u7387\u3002", "conclusion": "\u901a\u8fc7\u7edd\u70ed\u5730\u6539\u53d8\u79bb\u5b50\u6676\u4f53\u7684\u65cb\u8f6c\u9891\u7387\uff0c\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u975e\u7ebf\u6027\u6a21\u5f0f\u8026\u5408\uff0c\u4ece\u800c\u589e\u5f3a\u4f4e\u9891\u5e73\u9762\u6a21\u5f0f\u7684\u51b7\u5374\u6548\u679c\uff0c\u5e76\u4ea7\u751f\u66f4\u4f4e\u7684\u4e9a\u6beb\u5f00\u5c14\u6587\u6e29\u5ea6\uff0c\u63d0\u9ad8\u4e86\u9f13\u819c\u6a21\u5f0f\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5e94\u7528\u4e2d\u7684\u5149\u8c31\u5206\u8fa8\u7387\u3002"}}
{"id": "2507.12466", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "BETR\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u8bc4\u4f30\u57fa\u51c6\u5bf9\u9f50\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\u3002", "motivation": "\u63a2\u7d22\u5f53\u6570\u636e\u9009\u62e9\u7684\u4f18\u5316\u663e\u5f0f\u5316\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBenchmark-Targeted Ranking (BETR)\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u4e0e\u57fa\u51c6\u8bad\u7ec3\u6837\u672c\u7684\u76f8\u4f3c\u6027\u6765\u9009\u62e9\u9884\u8bad\u7ec3\u6587\u6863\u3002BETR\u5c06\u57fa\u51c6\u6837\u672c\u548c\u9884\u8bad\u7ec3\u6587\u6863\u7684\u6837\u672c\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e0e\u57fa\u51c6\u7684\u76f8\u4f3c\u6027\u5bf9\u8be5\u6837\u672c\u8fdb\u884c\u8bc4\u5206\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u6765\u9884\u6d4b\u6574\u4e2a\u8bed\u6599\u5e93\u7684\u8fd9\u4e9b\u5206\u6570\u3002", "result": "BETR\u5728\u6240\u6709\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u4e862.1\u500d\u7684\u8ba1\u7b97\u4e58\u6570\uff08\u76f8\u5bf9\u4e8eDCLM-Baseline\uff0c\u76f8\u5bf9\u4e8e\u672a\u8fc7\u6ee4\u6570\u636e\u4e3a4.7\u500d\uff09\uff0c\u5e76\u572810\u4e2a\u4efb\u52a1\u4e2d\u76849\u4e2a\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002BETR\u7684\u6cdb\u5316\u80fd\u529b\u4e5f\u5f88\u597d\uff0c\u5f53\u9488\u5bf9\u4e0e\u8bc4\u4f30\u5957\u4ef6\u4e0d\u91cd\u53e0\u7684\u591a\u6837\u5316\u57fa\u51c6\u65f6\uff0c\u5b83\u4ecd\u7136\u80fd\u5339\u914d\u6216\u4f18\u4e8e\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u66f4\u5927\u7684\u6a21\u578b\u9700\u8981\u4e0d\u592a\u6fc0\u8fdb\u7684\u8fc7\u6ee4\u3002", "conclusion": "\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u4efb\u52a1\u5339\u914d\u53ef\u4ee5\u7cbe\u786e\u5730\u5851\u9020\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u4e14\u6700\u4f18\u7684\u9009\u62e9\u7b56\u7565\u5fc5\u987b\u9002\u5e94\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2507.12095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12095", "abs": "https://arxiv.org/abs/2507.12095", "authors": ["Davide Di Nucci", "Matteo Tomei", "Guido Borghi", "Luca Ciuffreda", "Roberto Vezzani", "Rita Cucchiara"], "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "comment": null, "summary": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7a00\u758f\u89c6\u56fe\u8fdb\u884c\u8f66\u8f86\u4e09\u7ef4\u91cd\u5efa\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u8f93\u5165\u89c6\u56fe\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u8f66\u8f86\u68c0\u67e5\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u57ce\u5e02\u89c4\u5212\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u9009\u62e9\u6027\u5149\u5ea6\u635f\u5931\u548c\u4f7f\u7528DUSt3R\u67b6\u6784\u66ff\u4ee3\u6807\u51c6\u7684Structure-from-Motion\u6d41\u6c34\u7ebf\u6765\u589e\u5f3a\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u56fe\u548c\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u67b6\u6784\u6765\u5408\u6210\u65b0\u89c6\u56fe\u548c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8f66\u8f86\u4e09\u7ef4\u91cd\u5efa\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12438", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12438", "abs": "https://arxiv.org/abs/2507.12438", "authors": ["Davide Castaldo", "Stefano Corni"], "title": "Heisenberg limited multiple eigenvalue estimation via off-the-grid compressed sensing", "comment": null, "summary": "Quantum phase estimation is the flagship algorithm for quantum simulation on\nfault-tolerant quantum computers. We demonstrate that an \\emph{off-grid}\ncompressed sensing protocol, combined with a state-of-the-art signal\nclassification method, enables the simultaneous estimation of multiple\neigenvalues of a unitary matrix using the Hadamard test while sampling only a\nfew percent of the full autocorrelation function. Our numerical evidence\nindicates that the proposed algorithm achieves the Heisenberg limit in both\nstrongly and weakly correlated regimes and requires very short evolution times\nto obtain an $\\epsilon$-accurate estimate of multiple eigenvalues at once.\n  Additionally -- and of independent interest -- we develop a modified off-grid\nprotocol that leverages prior knowledge of the underlying signal for faster and\nmore accurate recovery. Finally, we argue that this algorithm may offer a\npotential quantum advantage by analyzing its resilience with respect to the\nquality of the initial input state.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u4e86\u975e\u7f51\u683c\u538b\u7f29\u611f\u77e5\u548c\u4fe1\u53f7\u5206\u7c7b\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u540c\u65f6\u4f30\u8ba1\u9149\u77e9\u9635\u7684\u591a\u4e2a\u7279\u5f81\u503c\uff0c\u5e76\u53ef\u80fd\u63d0\u4f9b\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u91cf\u5b50\u76f8\u4f4d\u4f30\u7b97\u662f\u6709\u5dee\u9519\u5bb9\u5fcd\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u65d7\u8230\u7b97\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u975e\u7f51\u683c\u538b\u7f29\u611f\u77e5\u534f\u8bae\u548c\u5148\u8fdb\u7684\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u54c8\u8fbe\u739b\u6d4b\u8bd5\u540c\u65f6\u4f30\u8ba1\u9149\u77e9\u9635\u7684\u591a\u4e2a\u7279\u5f81\u503c\uff0c\u5e76\u4e14\u4ec5\u5bf9\u81ea\u76f8\u5173\u51fd\u6570\u8fdb\u884c\u5c11\u91cf\u91c7\u6837\u3002", "result": "\u5b9e\u73b0\u4e86\u540c\u65f6\u4f30\u8ba1\u591a\u4e2a\u7279\u5f81\u503c\uff0c\u5e76\u5728\u5f3a\u76f8\u5173\u548c\u5f31\u76f8\u5173\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u6d77\u68ee\u5821\u6781\u9650\uff0c\u540c\u65f6\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u66f4\u5feb\u66f4\u7cbe\u786e\u6062\u590d\u7684\u6539\u8fdb\u578b\u975e\u7f51\u683c\u534f\u8bae\u3002 \u8bba\u8bc1\u4e86\u8be5\u7b97\u6cd5\u53ef\u80fd\u901a\u8fc7\u5206\u6790\u5176\u5bf9\u521d\u59cb\u8f93\u5165\u72b6\u6001\u8d28\u91cf\u7684\u9c81\u68d2\u6027\u6765\u63d0\u4f9b\u6f5c\u5728\u7684\u91cf\u5b50\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5f3a\u76f8\u5173\u548c\u5f31\u76f8\u5173\u60c5\u51b5\u4e0b\u5747\u80fd\u8fbe\u5230\u6d77\u68ee\u5821\u6781\u9650\uff0c\u5e76\u4e14\u53ea\u9700\u5f88\u77ed\u7684\u6f14\u5316\u65f6\u95f4\u5373\u53ef\u83b7\u5f97\u591a\u4e2a\u7279\u5f81\u503c\u7684\u03b5\u7cbe\u5ea6\u4f30\u8ba1\u3002\u8be5\u7b97\u6cd5\u8fd8\u901a\u8fc7\u5206\u6790\u5176\u5bf9\u521d\u59cb\u8f93\u5165\u72b6\u6001\u8d28\u91cf\u7684\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u7684\u91cf\u5b50\u4f18\u52bf\u3002"}}
{"id": "2507.12144", "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "pdf": "https://arxiv.org/pdf/2507.12144", "abs": "https://arxiv.org/abs/2507.12144", "authors": ["Boris Bonev", "Thorsten Kurth", "Ankur Mahesh", "Mauro Bisson", "Jean Kossaifi", "Karthik Kashinath", "Anima Anandkumar", "William D. Collins", "Michael S. Pritchard", "Alexander Keller"], "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "comment": null, "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "AI": {"tldr": "FourCastNet 3 \u4f7f\u7528\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u5728\u5929\u6c14\u9884\u62a5\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u5176\u901f\u5ea6\u6bd4\u73b0\u6709\u6a21\u578b\u5feb 8-60 \u500d\uff0c\u51c6\u786e\u6027\u76f8\u5f53\uff0c\u5e76\u80fd\u8fdb\u884c\u957f\u8fbe 60 \u5929\u7684\u9884\u62a5\u3002", "motivation": "FourCastNet 3 \u65e8\u5728\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u5168\u7403\u5929\u6c14\u9884\u62a5\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "method": "FourCastNet 3 \u91c7\u7528\u53ef\u6269\u5c55\u7684\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7403\u5f62\u51e0\u4f55\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u6982\u7387\u96c6\u6210\u9884\u62a5\u3002\u5b83\u4f7f\u7528\u7ed3\u5408\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c\u7684\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u5728\u5927\u89c4\u6a21 GPU \u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "FourCastNet 3 \u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u96c6\u6210\u6a21\u578b\uff0c\u5ab2\u7f8e\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4e14\u901f\u5ea6\u5feb 8 \u5230 60 \u500d\u3002\u5b83\u8fd8\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6982\u7387\u6821\u51c6\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\uff0c\u5373\u4f7f\u5728\u957f\u8fbe 60 \u5929\u7684\u9884\u62a5\u671f\u5185\u4e5f\u80fd\u4fdd\u6301\u3002", "conclusion": "FourCastNet 3 \u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3001\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u6709\u671b\u6539\u8fdb\u6c14\u8c61\u9884\u62a5\u548c\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2507.12103", "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.12103", "abs": "https://arxiv.org/abs/2507.12103", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc73D\u6a21\u62df\u548c\u540d\u4e3aDeepShade\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b66\u4e60\u5e76\u751f\u6210\u57ce\u5e02\u906e\u9633\u4fe1\u606f\uff0c\u4ee5\u6539\u5584\u8def\u7ebf\u89c4\u5212\u548c\u57ce\u5e02\u89c4\u5212\uff0c\u5e94\u5bf9\u70ed\u6d6a\u5a01\u80c1\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5168\u7403\u53d8\u6696\u80cc\u666f\u4e0b\u70ed\u6d6a\u5bf9\u516c\u4f17\u5065\u5eb7\u6784\u6210\u7684\u5a01\u80c1\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u7684\u8def\u7ebf\u89c4\u5212\u7cfb\u7edf\uff08\u5982\u5728\u7ebf\u5730\u56fe\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u906e\u9633\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u751f\u6210\u51c6\u786e\u906e\u9633\u4fe1\u606f\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepShade\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86RGB\u56fe\u50cf\u548cCanny\u8fb9\u7f18\u5c42\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6765\u6355\u6349\u906e\u9633\u7684\u52a8\u6001\u53d8\u5316\u3002\u540c\u65f6\uff0c\u5229\u7528Blender\u8fdb\u884c3D\u6a21\u62df\u751f\u6210\u4e86\u5305\u542b\u906e\u9633\u4fe1\u606f\u7684\u4e30\u5bcc\u6570\u636e\u96c6\uff0c\u5e76\u89e3\u51b3\u4e86\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u63d0\u53d6\u906e\u9633\u4fe1\u606f\u7684\u6311\u6218\u3002", "result": "\u901a\u8fc7\u5728\u4e9a\u5229\u6851\u90a3\u5dde\u5766\u4f69\u5e02\u7684\u5b9e\u9645\u8def\u7ebf\u89c4\u5212\u4e2d\u5e94\u7528\u906e\u9633\u9884\u6d4b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8ba4\u4e3a\u8be5\u7814\u7a76\u6210\u679c\u5bf9\u6781\u7aef\u9ad8\u6e29\u5929\u6c14\u4e0b\u7684\u57ce\u5e02\u89c4\u5212\u548c\u73af\u5883\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u54083D\u6a21\u62df\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5728\u57ce\u5e02\u89c4\u5212\u548c\u8def\u7ebf\u89c4\u5212\u4e2d\u5229\u7528\u906e\u9633\u4fe1\u606f\u7684\u6280\u672f\u96be\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12145", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12145", "abs": "https://arxiv.org/abs/2507.12145", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "title": "PRISM: Distributed Inference for Foundation Models at Edge", "comment": null, "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "AI": {"tldr": "PRISM \u662f\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5206\u5e03\u5f0f Transformer \u63a8\u7406\u7684\u901a\u4fe1\u9ad8\u6548\u4e14\u8ba1\u7b97\u611f\u77e5\u578b\u7b56\u7565\uff0c\u5b83\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u6765\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u8fb9\u7f18\u90e8\u7f72\u65b9\u9762\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u7b56\u7565\u3002", "method": "PRISM \u662f\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u4e14\u8ba1\u7b97\u611f\u77e5\u578b\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f Transformer \u63a8\u7406\u3002\u5b83\u5229\u7528 Segment Means \u8868\u793a\u6765\u8fd1\u4f3c\u4e2d\u95f4\u8f93\u51fa\u7279\u5f81\uff0c\u4ee5\u51cf\u5c11\u8bbe\u5907\u95f4\u901a\u4fe1\uff0c\u5e76\u91cd\u6784\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u611f\u77e5\u5206\u533a\u56e0\u679c\u63a9\u7801\u65b9\u6848\u3002", "result": "PRISM \u5728 ViT\u3001BERT \u548c GPT-2 \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u901a\u4fe1\u5f00\u9500\uff08BERT \u5728 CR=128 \u65f6\u51cf\u5c11\u9ad8\u8fbe 99.2%\uff09\u548c\u6bcf\u8bbe\u5907\u8ba1\u7b97\u91cf\uff08BERT \u5728\u76f8\u540c\u8bbe\u7f6e\u4e0b\u51cf\u5c11 51.24%\uff09\u5747\u6709\u5927\u5e45\u51cf\u5c11\uff0c\u540c\u65f6\u51c6\u786e\u7387\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\u3002", "conclusion": "PRISM \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2507.12105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12105", "abs": "https://arxiv.org/abs/2507.12105", "authors": ["Yiquan Gao", "Duohui Xu"], "title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "comment": "This paper was published in Proceedings of SPIE Volume 13442 and is\n  reprinted with permission. The official version is available at\n  https://doi.org/10.1117/12.3052988. One personal copy is allowed.\n  Reproduction, distribution, or commercial use is prohibited", "summary": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "AI": {"tldr": "Med-OoD\u6846\u67b6\u901a\u8fc7\u5f15\u5165OoD\u6570\u636e\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u6807\u6ce8\uff0c\u5e76\u5728Lizard\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u4ec5\u4f7f\u7528OoD\u6570\u636e\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u5206\u5272\u7f51\u7edc\u5728\u5904\u7406\u6709\u9650\u548c\u4e0d\u5b8c\u7f8e\u7684\u6570\u636e\u96c6\u65f6\uff0c\u5bb9\u6613\u5728\u524d\u666f\u548c\u80cc\u666f\u5bf9\u8c61\u4e4b\u95f4\u4ea7\u751f\u610f\u5916\u7684\u8bef\u5206\u7c7b\u3002\u53d7 OoD \u6570\u636e\u5728\u5176\u4ed6\u89c6\u89c9\u4efb\u52a1\u4e2d\u5f3a\u5927\u80fd\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Med-OoD \u7684\u6570\u636e\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165 OoD\uff08\u5206\u5e03\u5916\uff09\u6570\u636e\u76d1\u7763\u6765\u89e3\u51b3\u751f\u7269\u533b\u5b66\u5206\u5272\u4e2d\u7684\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u6570\u636e\u6e90\u3001\u7279\u5f81\u6b63\u5219\u5316\u76ee\u6807\u6216\u989d\u5916\u6ce8\u91ca\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5206\u5272\u7f51\u7edc\u800c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002", "result": "Med-OoD \u6846\u67b6\u80fd\u6709\u6548\u9632\u6b62\u5404\u79cd\u5206\u5272\u7f51\u7edc\u5728\u533b\u5b66\u56fe\u50cf\u4e0a\u51fa\u73b0\u50cf\u7d20\u8bef\u5206\u7c7b\uff0c\u5e76\u5728 Lizard \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u4ec5\u4f7f\u7528\u4e0d\u542b\u524d\u666f\u7c7b\u522b\u6807\u7b7e\u7684 OoD \u6570\u636e\u8bad\u7ec3\u51fa\u7684\u533b\u5b66\u5206\u5272\u7f51\u7edc\uff0c\u6d4b\u8bd5\u7ed3\u679c\u8fbe\u5230\u4e86 76.1% \u7684 mIoU\u3002", "conclusion": "Med-OoD \u6846\u67b6\u901a\u8fc7\u5f15\u5165OoD\u6570\u636e\u76d1\u7763\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u5206\u5272\u7f51\u7edc\u5728\u6709\u9650\u548c\u4e0d\u5b8c\u7f8e\u6570\u636e\u96c6\u4e0a\u9047\u5230\u7684\u524d\u666f/\u80cc\u666f\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u5728 Lizard \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u4e0d\u542b\u524d\u666f\u7c7b\u522b\u6807\u7b7e\u7684OoD\u6570\u636e\u8bad\u7ec3\u533b\u5b66\u5206\u5272\u7f51\u7edc\u7684\u65b0\u8303\u5f0f\uff0c\u5e76\u53d6\u5f97\u4e86\u4ee4\u4eba\u60ca\u8bb6\u7684 76.1% mIoU \u7684\u6d4b\u8bd5\u7ed3\u679c\uff0c\u6709\u671b\u5f15\u53d1\u5bf9OoD\u6570\u636e\u4f5c\u7528\u7684\u91cd\u65b0\u601d\u8003\u3002"}}
{"id": "2507.12165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12165", "abs": "https://arxiv.org/abs/2507.12165", "authors": ["Fouad Oubari", "Mohamed El-Baha", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Multi-Component VAE with Gaussian Markov Random Field", "comment": null, "summary": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "AI": {"tldr": "GMRF MCVAE \u901a\u8fc7\u5c06\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u5d4c\u5165\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u6765\u89e3\u51b3\u591a\u7ec4\u4ef6\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u7ec4\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08MCVAE\uff09\u901a\u5e38\u4f9d\u8d56\u7b80\u5316\u7684\u805a\u5408\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u751f\u6210\u7ec4\u4ef6\u4e4b\u95f4\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6846\u67b6\u2014\u2014\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u591a\u7ec4\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08GMRF MCVAE\uff09\uff0c\u8be5\u6846\u67b6\u5c06\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u5d4c\u5165\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u4e2d\uff0c\u4ee5\u663e\u5f0f\u5730\u5bf9\u8de8\u7ec4\u4ef6\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002", "result": "GMRF MCVAE \u5728\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u7ec4\u4ef6\u5173\u7cfb\u7684\u5408\u6210 Copula \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 PolyMNIST \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b results\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684 BIKED \u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "GMRF MCVAE \u6846\u67b6\u5728 BIKED \u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5728 Copula \u548c PolyMNIST \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8868\u660e\u5176\u5728\u9700\u8981\u7a33\u5065\u4e14\u903c\u771f\u7684\u591a\u7ec4\u4ef6\u76f8\u5e72\u6027\u5efa\u6a21\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7279\u522b\u9002\u7528\u3002"}}
{"id": "2507.12107", "categories": ["cs.CV", "cs.AI", "cs.CR", "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.12107", "abs": "https://arxiv.org/abs/2507.12107", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Minsu Kim", "Jae Hong Seo"], "title": "Non-Adaptive Adversarial Face Generation", "comment": null, "summary": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7279\u5f81\u7a7a\u95f4\u7684\u7ed3\u6784\u7279\u6027\uff0c\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\u548c\u6a21\u578b\u53ef\u8f6c\u79fb\u6027\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u548c\u53ef\u63a7\u5c5e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff08FRS\uff09\u9762\u4e34\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5e26\u6765\u7684\u5b89\u5168\u548c\u9690\u79c1\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5f53\u8fd9\u4e9b\u7cfb\u7edf\u7528\u4e8e\u8eab\u4efd\u9a8c\u8bc1\u65f6\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u7279\u6027\uff0c\u7279\u522b\u662f\u540c\u4e00\u5c5e\u6027\u4e2a\u4f53\u7684\u5b50\u7403\u4f53\uff0c\u751f\u6210\u5bf9\u6297\u6027\u4eba\u8138\u3002\u4e0e\u57fa\u4e8e\u8fed\u4ee3\u4f18\u5316\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u53ea\u9700\u8981\u4e00\u6b21\u975e\u81ea\u9002\u5e94\u67e5\u8be2\uff08100\u5f20\u4eba\u8138\u56fe\u50cf\uff09\uff0c\u65e0\u9700\u4f9d\u8d56\u53ef\u8f6c\u79fb\u6027\u6216\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u9700\u4e00\u6b21\u975e\u81ea\u9002\u5e94\u67e5\u8be2\uff08100\u5f20\u4eba\u8138\u56fe\u50cf\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u9488\u5bf9AWS\u7684CompareFaces API\u5b9e\u73b0\u4e86\u8d85\u8fc793%\u7684\u9ad8\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u751f\u6210\u5177\u6709\u653b\u51fb\u8005\u9009\u62e9\u7684\u9ad8\u5c42\u5c5e\u6027\u7684\u5bf9\u6297\u6027\u4eba\u8138\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff08FRS\uff09\u7279\u5f81\u7a7a\u95f4\u4e2d\u5177\u6709\u76f8\u540c\u5c5e\u6027\uff08\u4f8b\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\u7684\u4e2a\u4f53\u5f62\u6210\u7684\u5b50\u7403\u4f53\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\uff08\u8d85\u8fc793%\uff09\u7684\u5355\u6b21\u975e\u81ea\u9002\u5e94\u67e5\u8be2\uff0c\u5e76\u80fd\u751f\u6210\u5177\u6709\u53ef\u63a7\u9ad8\u5c42\u5c5e\u6027\u7684\u5bf9\u6297\u6027\u4eba\u8138\u3002"}}
{"id": "2507.12341", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "LEOPARD\u662f\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u8868\u793a\u4e2d\u53bb\u9664\u654f\u611f\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u795e\u7ecf\u6a21\u578b\u4e0d\u4f1a\u4ece\u6587\u672c\u8868\u793a\u4e2d\u63a8\u65ad\u51fa\u654f\u611f\u4fe1\u606f\uff0c\u4ee5\u89e3\u51b3\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u77e9\u9635\uff0c\u4f7f\u654f\u611f\u6982\u5ff5\u7684\u7c7b\u6761\u4ef6\u7279\u5f81\u5206\u5e03\u5728\u6295\u5f71\u540e\u65e0\u6cd5\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6295\u5f71\u77e9\u9635\u7684\u79e9\u6765\u63a7\u5236\u4fe1\u606f\u53bb\u9664\u7a0b\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u7ed3\u6784\u3002", "result": "LEOPARD\u5728\u7ecf\u5178\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u975e\u7ebf\u6027\u5c5e\u6027\u64e6\u9664\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u8f7b\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u504f\u5dee\u3002", "conclusion": "LEOPARD\u65b9\u6cd5\u5728\u53bb\u9664\u6587\u672c\u8868\u793a\u4e2d\u7684\u654f\u611f\u4fe1\u606f\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u6d88\u9664\u4e86\u504f\u5dee\uff0c\u4fc3\u8fdb\u4e86\u516c\u5e73\u6027\u3002"}}
{"id": "2507.12114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12114", "abs": "https://arxiv.org/abs/2507.12114", "authors": ["Yuzhou Ji", "Ke Ma", "Hong Cai", "Anchun Zhang", "Lizhuang Ma", "Xin Tan"], "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "comment": null, "summary": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.", "AI": {"tldr": "LidarPainter is a fast and efficient diffusion model for dynamic driving scene reconstruction that improves view consistency and quality, and allows for stylized generation.", "motivation": "Dynamic driving scene reconstruction is of great importance in fields like digital twin system and autonomous driving simulation. However, existing methods suffer from unacceptable degradation on novel trajectories due to inconsistency, deformation, and time consumption.", "method": "LidarPainter, a one-step diffusion model", "result": "Extensive experiments show that LidarPainter outperforms state-of-the-art methods in speed (7x faster than StreetCrafter), quality and resource efficiency (using one fifth of GPU memory). It also supports stylized generation using text prompts.", "conclusion": "LidarPainter is a one-step diffusion model that recovers consistent driving views from sparse LiDAR condition and artifact-corrupted renderings in real-time, enabling high-fidelity lane shifts in driving scene reconstruction. It outperforms state-of-the-art methods in speed, quality and resource efficiency, and supports stylized generation using text prompts."}}
{"id": "2507.12192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12192", "abs": "https://arxiv.org/abs/2507.12192", "authors": ["Victor F. Lopes de Souza", "Karima Bakhti", "Sofiane Ramdani", "Denis Mottet", "Abdelhak Imoussaten"], "title": "Explainable Evidential Clustering", "comment": null, "summary": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "AI": {"tldr": "\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u8bc1\u636e\u805a\u7c7b\u7ed3\u679c\u7684\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86IEMM\u7b97\u6cd5\uff0c\u80fd\u591f\u4e3a\u4e0d\u5b8c\u7f8e\u6570\u636e\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u5e76\u83b7\u5f97\u4e86\u9ad8\u5ea6\u6ee1\u610f\u7684\u89e3\u91ca\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7cbe\u786e\u6027\u7684\u4e0d\u5b8c\u7f8e\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u8db3\u3002\u8bc1\u636e\u805a\u7c7b\uff08\u57fa\u4e8eDempster-Shafer\u7406\u8bba\uff09\u867d\u7136\u80fd\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5bf9\u5176\u7ed3\u679c\u7684\u89e3\u91ca\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u8fd9\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u4fdd\u5065\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u8868\u793a\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u51b3\u7b56\u6811\u4f5c\u4e3a\u6eaf\u56e0\u89e3\u91ca\u5668\u7684\u5145\u8981\u6761\u4ef6\uff0c\u7136\u540e\u5c06\u6b64\u6982\u5ff5\u63a8\u5e7f\u4ee5\u9002\u5e94\u90e8\u5206\u6807\u8bb0\uff0c\u901a\u8fc7\u6548\u7528\u51fd\u6570\u6765\u8868\u793a\u201c\u53ef\u5bb9\u5fcd\u201d\u7684\u9519\u8bef\uff0c\u5e76\u5c06\u8bc1\u636e\u9519\u8bef\u5b9a\u4e49\u4e3a\u89e3\u91ca\u6210\u672c\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u9002\u7528\u4e8e\u8bc1\u636e\u5206\u7c7b\u5668\u7684\u89e3\u91ca\u5668\uff0c\u5e76\u63d0\u51fa\u4e86IEMM\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8868\u793a\u6027\u662f\u51b3\u7b56\u6811\u4f5c\u4e3a\u6eaf\u56e0\u89e3\u91ca\u5668\u7684\u5145\u8981\u6761\u4ef6\u3002\u901a\u8fc7\u6548\u7528\u51fd\u6570\u53ef\u4ee5\u5904\u7406\u90e8\u5206\u6807\u8bb0\u548c\u201c\u53ef\u5bb9\u5fcd\u201d\u9519\u8bef\u3002IEMM\u7b97\u6cd5\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u8c28\u614e\u7684\u8bc1\u636e\u805a\u7c7b\u89e3\u91ca\uff0c\u5e76\u4e14\u5728\u8003\u8651\u51b3\u7b56\u8005\u504f\u597d\u65f6\uff0c\u89e3\u91ca\u4ee4\u4eba\u6ee1\u610f\u7684\u6bd4\u4f8b\u9ad8\u8fbe93%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8fed\u4ee3\u8bc1\u636e\u9519\u8bef\u6700\u5c0f\u5316\uff08IEMM\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e3a\u8bc1\u636e\u805a\u7c7b\u51fd\u6570\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u8c28\u614e\u7684\u51b3\u7b56\u6811\u89e3\u91ca\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u8003\u8651\u51b3\u7b56\u8005\u504f\u597d\u65f6\uff0c\u89e3\u91ca\u4ee4\u4eba\u6ee1\u610f\u7684\u6bd4\u4f8b\u9ad8\u8fbe93%\u3002"}}
{"id": "2507.12123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12123", "abs": "https://arxiv.org/abs/2507.12123", "authors": ["Sergey Linok", "Gleb Naumov"], "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph", "comment": "13 pages, 5 figures, 2 tables", "summary": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12125", "abs": "https://arxiv.org/abs/2507.12125", "authors": ["Yi-Kuan Hsieh", "Jun-Wei Hsieh", "Xin Li", "Yu-Ming Chang", "Yu-Chee Tseng"], "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers", "comment": null, "summary": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.", "AI": {"tldr": "\u63d0\u51fa BSPF-ViT \u6765\u89e3\u51b3 ViT \u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u901a\u8fc7\u5757\u72b6\u5bf9\u79f0\u4fee\u526a\u548c\u878d\u5408\u8054\u5408\u4f18\u5316 Q/K \u6807\u8bb0\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "ViT \u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u7684\u901a\u8fc7\u4fee\u526a\u6807\u8bb0\u6765\u964d\u4f4e ViT \u590d\u6742\u6027\u7684\u65b9\u6cd5\uff0c\u5e38\u5e38\u56e0\u4e3a\u72ec\u7acb\u4fee\u526a Q/K \u6807\u8bb0\u800c\u727a\u7272\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u6807\u8bb0\u95f4\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5757\u72b6\u5bf9\u79f0\u4fee\u526a\u548c\u878d\u5408\uff08BSPF-ViT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316 Q/K \u6807\u8bb0\u6765\u964d\u4f4e ViT \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u6807\u8bb0\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u76f8\u4f3c\u6027\u878d\u5408\u538b\u7f29\u4fdd\u7559\u7684\u6807\u8bb0\u3002\u5b83\u5229\u7528 Q/K \u6807\u8bb0\u7684\u5171\u4eab\u6743\u91cd\u6765\u52a0\u901f\u4fee\u526a\u8fc7\u7a0b\uff0c\u4ec5\u4fee\u526a\u4e0a\u4e09\u89d2\u90e8\u5206\u3002", "result": "BSPF-ViT \u5728\u6240\u6709\u4fee\u526a\u6c34\u5e73\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 ViT \u65b9\u6cd5\uff0c\u5728 DeiT-T \u4e0a\u5c06 ImageNet \u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 1.3%\uff0c\u5728 DeiT-S \u4e0a\u63d0\u9ad8\u4e86 2.0%\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e\u4e86 50%\u3002\u5b83\u5728\u5404\u79cd ViT \u4e0a\u5b9e\u73b0\u4e86 40% \u7684\u52a0\u901f\u548c\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002", "conclusion": "BSPF-ViT \u901a\u8fc7\u8054\u5408\u4f18\u5316 Q/K \u6807\u8bb0\u7684\u4fee\u526a\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 ViT \u65b9\u6cd5\uff0c\u5728 ImageNet \u5206\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u7684\u63d0\u9ad8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u964d\u4f4e\u3002"}}
{"id": "2507.12218", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.12218", "abs": "https://arxiv.org/abs/2507.12218", "authors": ["Tomohisa Okazaki"], "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "comment": null, "summary": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u7ebf\u6027\u6a21\u578b\uff08PILM\uff09\uff0c\u5b83\u4f7f\u7528\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8868\u793a PDE \u89e3\uff0c\u4ece\u800c\u80fd\u591f\u83b7\u5f97\u6700\u4f18\u89e3\u7684\u89e3\u6790\u8868\u793a\u3002\u8be5\u6a21\u578b\u9002\u7528\u4e8e\u7ebf\u6027\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5730\u58f3\u5e94\u53d8\u7387\u7684\u4f30\u7b97\uff0c\u5176\u4e2d\u6bd4\u8f83\u4e86\u7269\u7406\u6b63\u5219\u5316\u548c\u6570\u5b66\u6b63\u5219\u5316\uff0c\u53d1\u73b0\u540e\u8005\u5728\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8bb8\u591a\u7269\u7406\u7cfb\u7edf\u7531\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u63cf\u8ff0\uff0c\u6c42\u89e3\u8fd9\u4e9b\u65b9\u7a0b\u4ee5\u53ca\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u5176\u7cfb\u6570\u6216\u8fb9\u754c\u6761\u4ef6\uff08BC\uff09\u5728\u7406\u89e3\u76f8\u5173\u73b0\u8c61\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u6700\u8fd1\uff0c\u4e00\u79cd\u79f0\u4e3a\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6765\u81eaPDE\u3001BC\u548c\u6570\u636e\u7684\u6b8b\u5dee\u603b\u548c\u6765\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3PDE\uff0c\u5728\u79d1\u5b66\u754c\u83b7\u5f97\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u7ebf\u6027\u6a21\u578b\uff08PILM\uff09\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8868\u793a\u89e3\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u6700\u4f18\u89e3\u8fdb\u884c\u89e3\u6790\u8868\u793a\u3002PILM\u88ab\u5236\u5b9a\u5e76\u9a8c\u8bc1\u4e86\u7528\u4e8e\u8bf4\u660e\u6027\u7684\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\uff0c\u5305\u62ec\u5177\u6709\u4e0d\u786e\u5b9a\u8fb9\u754c\u6761\u4ef6\u7684\u6848\u4f8b\u3002\u6b64\u5916\uff0cPILM\u8fd8\u5e94\u7528\u4e8e\u4f7f\u7528\u5927\u5730\u6d4b\u91cf\u6570\u636e\u4f30\u7b97\u5730\u58f3\u5e94\u53d8\u7387\u3002\u7279\u522b\u5730\uff0c\u5c06\u5f3a\u5236\u5f39\u6027\u5e73\u8861\u4e8e\u901f\u5ea6\u573a\u7684\u7269\u7406\u6b63\u5219\u5316\u4e0e\u65bd\u52a0\u5e73\u6ed1\u7ea6\u675f\u7684\u6570\u5b66\u6b63\u5219\u5316\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\uff0c\u6570\u5b66\u6b63\u5219\u5316\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "PILM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u6790\u53ef\u89e3\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\u3001\u4e0d\u5b9a\u5b9a\u7cfb\u7edf\u548c\u7269\u7406\u6b63\u5219\u5316\u3002"}}
{"id": "2507.12135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12135", "abs": "https://arxiv.org/abs/2507.12135", "authors": ["Junyu Lou", "Xiaorui Zhao", "Kexuan Shi", "Shuhang Gu"], "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "comment": "Accepted to ICCV 2025", "summary": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684BPAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53cc\u8fb9\u7f51\u683c\u548c\u50cf\u7d20\u81ea\u9002\u5e94\u591a\u5c42\u611f\u77e5\u673a\uff0c\u89e3\u51b3\u4e86\u989c\u8272\u5173\u7cfb\u5efa\u6a21\u548c\u5c40\u90e8\u53d8\u5316\u5904\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53cc\u8fb9\u7f51\u683c\u5904\u7406\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7ebf\u6027\u53d8\u6362\uff0c\u96be\u4ee5\u5bf9\u590d\u6742\u7684\u989c\u8272\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff1b\u800c\u4f20\u7edf\u7684\u591a\u5c42\u611f\u77e5\u673a\u65b9\u6cd5\u7531\u4e8e\u4f7f\u7528\u5168\u5c40\u5171\u4eab\u53c2\u6570\uff0c\u96be\u4ee5\u5904\u7406\u5c40\u90e8\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8fb9\u7f51\u683c\u50cf\u7d20\u81ea\u9002\u5e94\u591a\u5c42\u611f\u77e5\u673a\uff08BPAM\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u751f\u6210\u5305\u542b\u591a\u5c42\u611f\u77e5\u673a\u53c2\u6570\u7684\u53cc\u8fb9\u7f51\u683c\uff0c\u5141\u8bb8\u6bcf\u4e2a\u50cf\u7d20\u6839\u636e\u5176\u7a7a\u95f4\u5750\u6807\u548c\u5f3a\u5ea6\u503c\u52a8\u6001\u68c0\u7d22\u72ec\u7279\u7684\u53d8\u6362\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u683c\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u591a\u5c42\u611f\u77e5\u673a\u53c2\u6570\u5206\u7c7b\u5b58\u50a8\u5728\u5355\u72ec\u7684\u5b50\u7f51\u683c\u4e2d\uff0c\u5e76\u4f7f\u7528\u591a\u901a\u9053\u5f15\u5bfc\u56fe\u4ece\u76f8\u5e94\u7684\u5b50\u7f51\u683c\u4e2d\u63d0\u53d6\u7279\u5b9a\u7c7b\u522b\u7684\u53c2\u6570\u3002", "result": "BPAM\u6846\u67b6\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "BPAM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53cc\u8fb9\u7f51\u683c\u7684\u7a7a\u95f4\u5efa\u6a21\u80fd\u529b\u548c\u591a\u5c42\u611f\u77e5\u673a\u7684\u975e\u7ebf\u6027\u6620\u5c04\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u989c\u8272\u5173\u7cfb\u7684\u5efa\u6a21\u548c\u5c40\u90e8\u53d8\u5316\u7684\u9002\u5e94\uff0c\u5e76\u5728\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12224", "abs": "https://arxiv.org/abs/2507.12224", "authors": ["Razvan Pascanu", "Clare Lyle", "Ionut-Vlad Modoranu", "Naima Elosegui Borras", "Dan Alistarh", "Petar Velickovic", "Sarath Chandar", "Soham De", "James Martens"], "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "comment": null, "summary": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "AI": {"tldr": "\u4f18\u5316\u5668\u4e0d\u4ec5\u5173\u4e4e\u901f\u5ea6\uff0c\u8fd8\u5f71\u54cd\u6a21\u578b\u5b66\u5230\u4ec0\u4e48\u7684\u672c\u8d28\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6539\u53d8\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\u5bf9\u4f18\u5316\u5668\u7814\u7a76\u7684\u5173\u6ce8\u70b9\uff0c\u4ece\u4ec5\u4ec5\u5173\u6ce8\u8bad\u7ec3\u6548\u7387\uff08\u5982\u8fed\u4ee3\u6b21\u6570\u3001FLOPs\u6216\u65f6\u95f4\uff09\u8f6c\u5411\u7406\u89e3\u548c\u8bbe\u8ba1\u80fd\u591f\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u6027\u8d28\u7684\u4f18\u5316\u5668\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8bba\u8bc1\u4f18\u5316\u5668\u4e0d\u4ec5\u5f71\u54cd\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u5f71\u54cd\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u7684\u6027\u8d28\u6765\u63d0\u51fa\u65b0\u89c2\u70b9\u3002\u4f5c\u8005\u8ba4\u4e3a\u4f18\u5316\u5668\u53ef\u4ee5\u88ab\u7528\u6765\u7f16\u7801\u5f52\u7eb3\u504f\u501a\u548c\u6539\u53d8\u6a21\u578b\u6709\u6548\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5e94\u4f5c\u4e3a\u4e00\u79cd\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7279\u5b9a\u5c5e\u6027\u7684\u624b\u6bb5\u3002", "result": "\u672c\u6587\u7684\u8bba\u70b9\u63ed\u793a\u4e86\u4f18\u5316\u5668\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff1a\u4e0d\u4ec5\u5f71\u54cd\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u5851\u9020\u4e86\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u7684\u5185\u5728\u5c5e\u6027\uff08\u5982\u5f52\u7eb3\u504f\u501a\u548c\u8868\u8fbe\u80fd\u529b\uff09\u3002\u4f5c\u8005\u5e0c\u671b\u6b64\u8bba\u70b9\u80fd\u6fc0\u53d1\u7814\u7a76\uff0c\u4fc3\u8fdb\u5bf9\u4f18\u5316\u5668\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u5347\u4f18\u5316\u5668\u8bbe\u8ba1\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5730\u4f4d\u3002", "conclusion": "\u793e\u533a\u5e94\u8be5\u901a\u8fc7\u7406\u89e3\u73b0\u6709\u65b9\u6cd5\u7684\u504f\u5dee\u5e76\u7740\u91cd\u4e8e\u521b\u5efa\u80fd\u591f\u5f15\u5165\u7279\u5b9a\u89e3\u5c5e\u6027\u7684\u65b0\u4f18\u5316\u5668\uff0c\u6765\u8d85\u8d8a\u4ec5\u4ec5\u57fa\u4e8e\u6536\u655b\u901f\u7387\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002\u4f18\u5316\u5668\u7684\u8bbe\u8ba1\u5e94\u88ab\u89c6\u4e3a\u5f71\u54cd\u6a21\u578b\u7ed3\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e0e\u67b6\u6784\u548c\u6570\u636e\u540c\u7b49\u91cd\u8981\u3002"}}
{"id": "2507.12137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12137", "abs": "https://arxiv.org/abs/2507.12137", "authors": ["Jiawei Xu", "Kai Deng", "Zexin Fan", "Shenlong Wang", "Jin Xie", "Jian Yang"], "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "comment": "Accepted by ICCV 2025", "summary": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.", "AI": {"tldr": "AD-GS\u662f\u4e00\u4e2a\u65e0\u9700\u6ce8\u91ca\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8fd0\u52a8\u5efa\u6a21\u548c\u573a\u666f\u5206\u89e3\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9a7e\u9a76\u573a\u666f\u6e32\u67d3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u5efa\u6a21\u548c\u6e32\u67d3\uff0c\u540c\u65f6\u514b\u670d\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u624b\u52a8\u6ce8\u91ca\u6216\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u8fd0\u52a8\u5efa\u6a21\u548c\u573a\u666f\u5206\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86AD-GS\u3002", "method": "AD-GS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u8fd0\u52a8\u6a21\u578b\uff08\u7ed3\u5408\u4e86\u5c40\u90e8\u611f\u77e5\u7684B\u6837\u6761\u66f2\u7ebf\u548c\u5168\u5c40\u611f\u77e5\u7684\u4e09\u89d2\u51fd\u6570\uff09\u6765\u7cbe\u786e\u5efa\u6a21\u52a8\u6001\u5bf9\u8c61\u3002\u5b83\u901a\u8fc7\u7b80\u5316\u7684\u4f2a2D\u5206\u5272\u81ea\u52a8\u5206\u5272\u573a\u666f\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u9ad8\u65af\u548c\u53cc\u5411\u65f6\u95f4\u53ef\u89c1\u6027\u63a9\u7801\u6765\u8868\u793a\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u5305\u62ec\u53ef\u89c1\u6027\u63a8\u7406\u548c\u7269\u7406\u521a\u6027\u6b63\u5219\u5316\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "AD-GS\u5728\u65e0\u9700\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65e0\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u4f9d\u8d56\u6ce8\u91ca\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "AD-GS\u901a\u8fc7\u5176\u521b\u65b0\u7684\u53ef\u5b66\u4e60\u8fd0\u52a8\u6a21\u578b\u3001\u7b80\u5316\u7684\u4f2a2D\u5206\u5272\u4ee5\u53ca\u53ef\u89c1\u6027\u63a8\u7406\u548c\u7269\u7406\u521a\u6027\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u81ea\u7531\u89c6\u89d2\u6e32\u67d3\uff0c\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65e0\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5e76\u53ef\u4e0e\u6709\u6ce8\u91ca\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.12257", "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2507.12257", "abs": "https://arxiv.org/abs/2507.12257", "authors": ["Matteo Tusoni", "Giuseppe Masi", "Andrea Coletta", "Aldo Glielmo", "Viviana Arrigoni", "Novella Bartolini"], "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "comment": null, "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "AI": {"tldr": "\u8bb8\u591a\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u4f46\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u7684\u5e42\u5f8b\u8c31\u7279\u5f81\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u63a2\u7d22\u968f\u673a\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u662f\u4e00\u9879\u8270\u5de8\u4f46\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u5728\u91d1\u878d\u3001\u7ecf\u6d4e\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u6c14\u5019\u79d1\u5b66\u7b49\u5e7f\u6cdb\u7684\u5e94\u7528\u9886\u57df\u4e2d\u90fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bb8\u591a\u56e0\u679c\u53d1\u73b0\uff08CD\uff09\u7b97\u6cd5\u901a\u5e38\u5bf9\u566a\u58f0\u9ad8\u5ea6\u654f\u611f\uff0c\u5728\u5e94\u7528\u4e8e\u5b9e\u9645\u6570\u636e\u65f6\u4f1a\u5bfc\u81f4\u8bef\u5bfc\u6027\u7684\u56e0\u679c\u63a8\u65ad\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u53d6\u529f\u7387\u8c31\u7279\u5f81\u7684\u7a33\u5065\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u8be5\u7279\u5f81\u80fd\u591f\u653e\u5927\u771f\u5b9e\u7684\u56e0\u679c\u4fe1\u53f7\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56e0\u679c\u53d1\u73b0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u57fa\u51c6\u548c\u5177\u6709\u5df2\u77e5\u56e0\u679c\u7ed3\u6784 \u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u76f8\u5173\u6027\u3002"}}
{"id": "2507.12138", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12138", "abs": "https://arxiv.org/abs/2507.12138", "authors": ["Michal Heker", "Sefy Kararlitsky", "David Tolpin"], "title": "Neural Human Pose Prior", "comment": "Work in progress", "summary": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u548cRealNVP\u7684\u4eba\u7c7b\u59ff\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e866D\u65cb\u8f6c\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u5efa\u6a21\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4ecb\u7ecd\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u3001\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u5bf9\u4eba\u7c7b\u59ff\u6001\u7684\u795e\u7ecf\u5148\u9a8c\u8fdb\u884c\u5efa\u6a21\u3002", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\uff08normalizing flows\uff09\u5bf9\u4eba\u7c7b\u59ff\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5229\u7528RealNVP\u5b66\u4e60\u59ff\u6001\u7684\u7075\u6d3b\u5bc6\u5ea6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9006\u8f6cGram-Schmidt\u8fc7\u7a0b\u6765\u89e3\u51b3\u5728\u6709\u65486D\u65cb\u8f6c\u6d41\u5f62\u4e0a\u8fdb\u884c\u5206\u5e03\u5efa\u6a21\u7684\u6311\u6218\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8bc1\u660e\u4e86\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u5176\u5f71\u54cd\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u5c06\u59ff\u6001\u5148\u9a8c\u96c6\u6210\u5230\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u548c\u91cd\u5efa\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6982\u7387\u57fa\u7840\u3002"}}
{"id": "2507.12262", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12262", "abs": "https://arxiv.org/abs/2507.12262", "authors": ["Zachary James", "Joseph Guinness"], "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "comment": null, "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u975e\u5e73\u7a33\u6838\u53c2\u6570\u7684\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u5e76\u5728\u5404\u79cd\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5e73\u7a33\u6838\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u9ad8\u9ad8\u65af\u8fc7\u7a0b\u5bf9\u975e\u5e73\u7a33\u6570\u636e\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5c06\u53c2\u6570\u89c6\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u5e76\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u8054\u5408\u8bad\u7ec3\uff0c\u4ee5\u4f7f\u7528\u94fe\u5f0f\u6cd5\u5219\u8ba1\u7b97\u5bfc\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5bf9\u6570\u5f97\u5206\u65b9\u9762\u4f18\u4e8e\u5e73\u7a33\u6a21\u578b\u548c\u5206\u5c42\u6a21\u578b\uff0c\u5e76\u4e14\u80fd\u591f\u6062\u590d\u975e\u5e73\u7a33\u53c2\u6570\u3002", "conclusion": "\u4e0e\u5e73\u7a33\u6a21\u578b\u548c\u5206\u5c42\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5bf9\u6570\u5f97\u5206\u65b9\u9762\u5747\u4f18\u4e8e\u5b83\u4eec\uff0c\u5e76\u4e14\u4e0e\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u8fd1\u4f3c\u65b9\u6cd5\u517c\u5bb9\u3002\u8be5\u65b9\u6cd5\u8fd8\u5c55\u793a\u4e86\u5176\u6062\u590d\u7a7a\u95f4\u6570\u636e\u96c6\u7684\u975e\u5e73\u7a33\u53c2\u6570\u7684\u80fd\u529b\u3002"}}
{"id": "2507.12157", "categories": ["cs.CV", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2507.12157", "abs": "https://arxiv.org/abs/2507.12157", "authors": ["Edwin Arkel Rios", "Fernando Mikael", "Oswin Gosal", "Femiloye Oyerinde", "Hao-Chun Liang", "Bo-Cheng Lai", "Min-Chun Hu"], "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation", "comment": "Main: 10 pages, 2 figures, 4 tables", "summary": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86TGDA\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u9ad8\u6027\u80fd\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8bc6\u522b\uff08FGIR\uff09\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\uff0c\u5e76\u652f\u6301L-RNets\u548cViTFS\u7b49\u9ad8\u6548\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTGDA\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u65b9\u9762\u53ef\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u91cf\u548c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u3002", "motivation": "\u4f20\u7edf\u7684FGIR\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5728\u5927\u578b\u6570\u636e\u96c6\uff08\u5982ImageNet\uff09\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u5e76\u963b\u788d\u4e86\u9488\u5bf9FGIR\u72ec\u7279\u6311\u6218\u91cf\u8eab\u5b9a\u5236\u7684\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u7684\u53d1\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u6311\u6218\u8fd9\u79cd\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u5e76\u8bc1\u660e\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u9ad8\u6027\u80fd\u7684FGIR\u7cfb\u7edf\u3002", "method": "TGDA\u6846\u67b6\u96c6\u6210\u6570\u636e\u611f\u77e5\u589e\u5f3a\u548c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u611f\u77e5\u6559\u5e08\u6a21\u578b\u7684\u5f31\u76d1\u7763\uff08\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\uff09\u3002\u8be5\u6846\u67b6\u652f\u6301\u7279\u5b9a\u4efb\u52a1\u548c\u786c\u4ef6\u611f\u77e5\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5982\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387FGIR\u7684LRNets\u548c\u7528\u4e8e\u9ad8\u6548\u63a8\u7406\u7684ViTFS\u7cfb\u5217Vision Transformer\u3002", "result": "\u5728\u4e09\u4e2aFGIR\u57fa\u51c6\u7684\u5404\u79cd\u4f4e\u5206\u8fa8\u7387\u548c\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTGDA\u6846\u67b6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u7279\u522b\u662f\u5728\u4f4e\u5206\u8fa8\u7387\u8bbe\u7f6e\u4e0b\uff0c\u4f7f\u7528TGDA\u8bad\u7ec3\u7684LRNets\u7684\u51c6\u786e\u7387\u6bd4\u5148\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8623%\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u51cf\u5c11\u4e86\u591a\u8fbe20.6\u500d\uff0cFLOPs\u66f4\u4f4e\uff0c\u5e76\u4e14\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u4e5f\u663e\u8457\u51cf\u5c11\u3002\u540c\u6837\uff0cViTFS-T\u5728\u4f7f\u7528\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u51cf\u5c1115.3\u500d\u4e14\u6240\u9700\u6570\u636e\u91cf\u51cf\u5c11\u591a\u4e2a\u6570\u91cf\u7ea7\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5339\u914d\u5728ImageNet-21k\u4e0a\u9884\u8bad\u7ec3\u7684ViT B-16\u7684\u6027\u80fd\u3002", "conclusion": "TGDA\u6846\u67b6\u6709\u6f5c\u529b\u6210\u4e3a\u9884\u8bad\u7ec3\u7684\u9002\u5e94\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.12297", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "AI": {"tldr": "RegCL\u901a\u8fc7\u5408\u5e76LoRA\u6a21\u5757\u53c2\u6570\u6765\u89e3\u51b3SAM\u5728\u591a\u9886\u57df\u9002\u5e94\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u4e14\u65e0\u9700\u91cd\u653e\u7684\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982SAM\uff09\u7684\u9002\u914d\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002\u5f53\u8fd9\u4e9b\u65b9\u6cd5\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u65f6\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u9ad8\u6548\u6574\u5408\u591a\u9886\u57df\u77e5\u8bc6\u7684\u6846\u67b6\u3002", "method": "RegCL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u975e\u91cd\u653e\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u9886\u57df\u77e5\u8bc6\u6574\u5408\u3002\u8be5\u6846\u67b6\u5c06\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u878d\u5165\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u5408\u5e76\u4e86\u5728\u4e0d\u540c\u9886\u57df\u8bad\u7ec3\u7684SAM\u9002\u914d\u6a21\u5757\uff08\u5982LoRA\u6a21\u5757\uff09\u7684\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u6743\u91cd\u4f18\u5316\u6765\u6307\u5bfc\u5408\u5e76\u8fc7\u7a0b\uff0c\u4ee5\u6700\u5c0f\u5316\u5408\u5e76\u6a21\u578b\u4e0e\u5404\u9886\u57df\u7279\u5b9a\u6a21\u578b\u4e4b\u95f4\u7684\u9884\u6d4b\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRegCL\u5728\u591a\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u6709\u6548\u5de9\u56fa\u4e86\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53c2\u6570\u6548\u7387\uff08\u6a21\u578b\u5927\u5c0f\u4e0d\u968f\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u800c\u6539\u53d8\uff09\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5386\u53f2\u6570\u636e\u5b58\u50a8\u3002", "conclusion": "RegCL\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6709\u6548\u6574\u5408\u4e86\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53c2\u6570\u6548\u7387\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u52a8\u6001\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12177", "abs": "https://arxiv.org/abs/2507.12177", "authors": ["Zahid Ullah", "Dragan Pamucar", "Jihie Kim"], "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u548c\u673a\u5668\u5b66\u4e60\u5206\u7c7b\uff0c\u5e76\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u53cc\u96c6\u6210\u6846\u67b6\u63d0\u9ad8\u4e86\u8111\u80bf\u7624MRI\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u5728\u89e3\u8bfbMRI\u56fe\u50cf\u65f6\u53ef\u80fd\u5b58\u5728\u75b2\u52b3\u3001\u7ecf\u9a8c\u4e0d\u8db3\u6216\u56fe\u50cf\u7ec6\u8282\u9650\u5236\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8111\u80bf\u7624\u8bca\u65ad\u7684\u51c6\u786e\u6027\u53d7\u5230\u5f71\u54cd\uff08\u4f8b\u5982\u6f0f\u8bca\u5c0f\u80bf\u7624\u6216\u8bef\u8bc6\u522b\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u7684\u96c6\u6210\u4ee5\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u53ca\u7ecf\u8fc7\u5fae\u8c03\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\u7684\u96c6\u6210\u4ee5\u8fdb\u884c\u5206\u7c7b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u9884\u5904\u7406\u548c\u589e\u5f3aMRI\u56fe\u50cf\uff1b2. \u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9Transformer\u7f51\u7edc\u63d0\u53d6\u56fe\u50cf\u7684\u6df1\u5ea6\u7279\u5f81\uff08\u8fc1\u79fb\u5b66\u4e60\uff09\uff1b3. \u5bf9\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u8fdb\u884c\u8d85\u53c2\u6570\u5fae\u8c03\uff1b4. \u96c6\u6210\u6df1\u5ea6\u7279\u5f81\u548c\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002\u5b9e\u9a8c\u5728\u4e09\u4e2a\u516c\u5f00\u7684Kaggle\u8111\u90e8MRI\u80bf\u7624\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u53cc\u96c6\u6210\u6846\u67b6\u5728\u8111\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6210\u679c\uff0c\u5176\u4e2d\u8d85\u53c2\u6570\u5fae\u8c03\u76f8\u6bd4\u5355\u7eaf\u7684\u96c6\u6210\u65b9\u6cd5\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u53cc\u96c6\u6210\u6846\u67b6\u901a\u8fc7\u878d\u5408\u6df1\u5ea6\u7279\u5f81\u548c\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u5fae\u8c03\uff0c\u5728\u8111\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6210\u679c\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u6210\u90e8\u5206\u5bf9\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002"}}
{"id": "2507.12305", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12305", "abs": "https://arxiv.org/abs/2507.12305", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "comment": "ICCV 2025", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53c2\u6570\u91cf\u5c11\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u7ebf\u6301\u7eed\u5b66\u4e60 (OCL) \u4e2d\u7684\u6570\u636e\u9690\u79c1\u7ea6\u675f\u4f7f\u5f97\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u590d\u6742\u5316\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u5185\u5b58\u4fdd\u5b58\u793a\u4f8b\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u4f46\u540e\u8005\u5b58\u5728\u53c2\u6570\u6570\u91cf\u4e0d\u65ad\u589e\u957f\u7684\u95ee\u9898\uff0c\u800c\u524d\u8005\u53ef\u80fd\u56e0\u6570\u636e\u5f00\u653e\u7b56\u7565\u800c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\u7684\u65b0\u9896\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff1a(1) \u4f5c\u4e3a\u901a\u7528\u77e5\u8bc6\u7684\u5355\u4e00\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\uff0c(2) \u4f5c\u4e3a\u7279\u5b9a\u77e5\u8bc6\u7684\u53ef\u8bad\u7ec3\u7f29\u653e\u5668\u548c\u79fb\u4f4d\u5668\uff0c(3) \u9884\u8bad\u7ec3\u6a21\u578b (PTM) \u6cdb\u5316\u4fdd\u6301\uff0c\u4ee5\u53ca (4) \u786c\u8f6f\u66f4\u65b0\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 CIFAR100\u3001ImageNet-R\u3001ImageNet-A \u548c CUB \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u53c2\u6570\u6570\u91cf\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u90fd\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\uff0c\u5e76\u5728 CIFAR100\u3001ImageNet-R\u3001ImageNet-A \u548c CUB \u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.12188", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12188", "abs": "https://arxiv.org/abs/2507.12188", "authors": ["Shuangli Du", "Siming Yan", "Zhenghao Shi", "Zhenzhen You", "Lu Sun"], "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "comment": null, "summary": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u89e3\u8026\u7279\u5f81\uff0c\u5206\u522b\u5904\u7406\u5149\u7167\u548c\u7eb9\u7406\uff0c\u5e76\u5229\u7528\u8de8\u89c6\u56fe\u4fe1\u606f\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u4f4e\u5149\u56fe\u50cf\u5b58\u5728\u590d\u6742\u7684\u964d\u8d28\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u964d\u8d28\u56e0\u7d20\u7f16\u7801\u5230\u5355\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u5bfc\u81f4\u7279\u5f81\u7ea0\u7f20\u548c\u9ed1\u76d2\u7279\u6027\uff0c\u5bb9\u6613\u5f15\u53d1\u6a21\u578b\u6377\u5f84\u5b66\u4e60\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u7279\u5f81\u3001\u6709\u6548\u5904\u7406\u5149\u7167\u548c\u7eb9\u7406\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u5728\u4e8e\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u91cf\uff0c\u4f4e\u9891\u5206\u91cf\u7528\u4e8e\u5149\u7167\u8c03\u6574\uff0c\u9ad8\u9891\u5206\u91cf\u7528\u4e8e\u7eb9\u7406\u6062\u590d\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u4e86\u9ad8\u9891\u5f15\u5bfc\u8de8\u89c6\u56fe\u4ea4\u4e92\u6a21\u5757\uff08HF-CIM\uff09\u6765\u4fc3\u8fdb\u53cc\u76ee\u89c6\u56fe\u95f4\u7684\u9ad8\u9891\u4fe1\u606f\u4ea4\u4e92\uff0c\u4ee5\u53ca\u7ec6\u8282\u548c\u7eb9\u7406\u589e\u5f3a\u6a21\u5757\uff08DTEM\uff09\u6765\u589e\u5f3a\u7eb9\u7406\u7ec6\u8282\u3002\u6a21\u578b\u5728\u5305\u542b\u5747\u5300\u548c\u975e\u5747\u5300\u5149\u7167\u7684\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5149\u7167\u8c03\u6574\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u80fd\u6709\u6548\u6062\u590d\u9ad8\u9891\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u548c\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u7684\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u5904\u7406\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5149\u7167\u8c03\u6574\u548c\u7eb9\u7406\u6062\u590d\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u9891\u5f15\u5bfc\u8de8\u89c6\u56fe\u4ea4\u4e92\u6a21\u5757\uff08HF-CIM\uff09\u6709\u6548\u5229\u7528\u53e6\u4e00\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ec6\u8282\u548c\u7eb9\u7406\u589e\u5f3a\u6a21\u5757\uff08DTEM\uff09\u6765\u63d0\u5347\u9ad8\u9891\u4fe1\u606f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5149\u7167\u8c03\u6574\u548c\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.12314", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12314", "abs": "https://arxiv.org/abs/2507.12314", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Zhenfang Liu", "Qing Sheng", "Jie Xiao", "Jungang Lou"], "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "comment": null, "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Thought Purity (TP) \u9632\u5fa1\u8303\u5f0f\uff0c\u4ee5\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662fChain-of-Thought\uff08CoT\uff09\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u540e\u95e8\u63d0\u793a\u653b\u51fb\uff08CoTA\uff09\u3002TP\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5904\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u89c4\u5219\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u76d1\u63a7\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u6076\u610f\u5185\u5bb9\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u4efb\u52a1\u6027\u80fd\uff0c\u65e8\u5728\u63d0\u5347AI\u7684\u5b89\u5168\u4e0e\u529f\u80fd\u5e73\u8861\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LRM\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5b89\u5168\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728Chain-of-Thought\uff08CoT\uff09\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u540e\u95e8\u63d0\u793a\u653b\u51fb\uff08\u5982CoTA\uff09\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u7834\u574f\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\uff0c\u964d\u4f4e\u5176\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8fd9\u79cd\u5b89\u5168-\u6027\u80fd\u590d\u5408\u6f0f\u6d1e\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Thought Purity (TP) \u9632\u5fa1\u8303\u5f0f\u5305\u542b\u4e09\u4e2a\u534f\u540c\u90e8\u5206\uff1a1. \u4e00\u4e2a\u5b89\u5168\u4f18\u5316\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff1b2. \u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u89c4\u5219\u7ea6\u675f\uff1b3. \u81ea\u9002\u5e94\u76d1\u63a7\u6307\u6807\u3002\u8be5\u8303\u5f0f\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8\u6a21\u578b\u62b5\u6297\u6076\u610f\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u64cd\u4f5c\u6548\u7387\uff0c\u4ee5\u5e94\u5bf9CoTA\u653b\u51fb\u3002", "result": "Thought Purity (TP) \u9632\u5fa1\u8303\u5f0f\u901a\u8fc7\u5176\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff0c\u6210\u529f\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684CoTA\u6f0f\u6d1e\u7684\u7efc\u5408\u9632\u5fa1\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AI\u67b6\u6784\u5728\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Thought Purity (TP) \u9632\u5fa1\u8303\u5f0f\uff0c\u901a\u8fc7\u4e09\u9879\u534f\u540c\u7ec4\u4ef6\uff08\u5b89\u5168\u4f18\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u89c4\u5219\u7ea6\u675f\u3001\u81ea\u9002\u5e94\u76d1\u63a7\u6307\u6807\uff09\u6765\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u9488\u5bf9Chain-of-Thought (CoT)\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u540e\u95e8\u63d0\u793a\u653b\u51fb\uff08Chain-of-Thought Attack, CoTA\uff09\u3002TP\u65e8\u5728\u589e\u5f3a\u6a21\u578b\u5bf9\u6076\u610f\u5185\u5bb9\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u67b6\u6784\u7684\u5b89\u5168\u4e0e\u529f\u80fd\u5e73\u8861\u6811\u7acb\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002"}}
{"id": "2507.12195", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12195", "abs": "https://arxiv.org/abs/2507.12195", "authors": ["Arkaprabha Basu"], "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "comment": null, "summary": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5206\u5f62\u5377\u79ef\u3001\u81ea\u654f\u611f\u74f7\u7816\u586b\u5145\u548c\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff0c\u7ed3\u5408\u9a6c\u8d5b\u514b\u5207\u7247\u6570\u636e\u589e\u5f3a\uff0c\u7528\u4e8e\u4fee\u590d\u5370\u5ea6\u6587\u5316\u9057\u4ea7\uff0c\u5c24\u5176\u5173\u6ce8Bankura\u7684\u8d64\u9676\u5e99\uff0c\u65e8\u5728\u5e73\u8861\u4f20\u7edf\u4e0e\u521b\u65b0\uff0c\u63d0\u9ad8\u4fdd\u62a4\u6548\u7387\u548c\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u73b0\u4ee3\u6570\u5b57\u5316\u4fdd\u5b58\u548c\u4fee\u590d\u6587\u5316\u9057\u4ea7\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5370\u5ea6 monumento \u7684\u72ec\u7279\u4e4b\u5904\uff0c\u5982\u5176\u5efa\u7b51\u6280\u827a\u548c\u7f8e\u5b66\u5438\u5f15\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u79cd\u6280\u672f\uff1a1. \u5206\u5f62\u5377\u79ef\uff08Fractal Convolution\uff09\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5904\u7406\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u63ed\u793a\u5efa\u7b51\u7684\u7ec6\u5fae\u6a21\u5f0f\u3002 2. \u81ea\u654f\u611f\u74f7\u7816\u586b\u5145\uff08Self-Sensitive Tile Filling, SSTF\uff09\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u2014\u2014\u9a6c\u8d5b\u514b\u5207\u7247\uff08MosaicSlice\uff09\uff0c\u4e13\u95e8\u7528\u4e8e\u4fee\u590dBankura\u7684\u8d64\u9676\u5e99\u3002 3. \u8d85\u5206\u8fa8\u7387\uff08Super Resolution\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u4e0d\u635f\u5931\u8fc7\u591a\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u65e0\u7f1d\u7684\u533a\u57df\u586b\u5145\u548c\u9ad8\u7ec6\u8282\u7684\u74f7\u7816\uff0c\u540c\u65f6\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5728\u53ef\u63a7\u6210\u672c\u5185\u4fdd\u6301\u771f\u5b9e\u6027\u5e76\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u4e86\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7684\u6548\u7387\u548c\u7f8e\u5b66\u8d28\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4e09\u79cd\u65b9\u6cd5\uff08\u5206\u5f62\u5377\u79ef\u3001\u81ea\u654f\u611f\u74f7\u7816\u586b\u5145\u548c\u9a6c\u8d5b\u514b\u5207\u7247\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u53ca\u8d85\u5206\u8fa8\u7387\uff09\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u4f20\u7edf\u4e0e\u521b\u65b0\u7684\u5e73\u8861\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7f8e\u89c2\u6027\u3002"}}
{"id": "2507.12201", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.12201", "abs": "https://arxiv.org/abs/2507.12201", "authors": ["Yiqi Tian", "Pengfei Jin", "Mingze Yuan", "Na Li", "Bo Zeng", "Quanzheng Li"], "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models", "comment": null, "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.", "AI": {"tldr": "RODS \u662f\u4e00\u79cd\u65b0\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u89c6\u89d2\uff0c\u5229\u7528\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ebf\u7d22\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u5e7b\u89c9\uff0c\u4ece\u800c\u63d0\u9ad8\u91c7\u6837\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5176\u91c7\u6837\u8fc7\u7a0b\u4ecd\u7136\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u8fd9\u901a\u5e38\u6e90\u4e8e\u8bc4\u5206\u8fd1\u4f3c\u4e2d\u7684\u4e0d\u51c6\u786e\u6027\u3002", "method": "RODS (Robust Optimization-inspired Diffusion Sampler) \u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u89c6\u89d2\u91cd\u65b0\u89e3\u91ca\u6269\u6563\u91c7\u6837\uff0c\u5229\u7528\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ebf\u7d22\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u9ad8\u98ce\u9669\u7684\u91c7\u6837\u6b65\u9aa4\u3002", "result": "RODS \u5f3a\u5236\u6267\u884c\u66f4\u5e73\u6ed1\u7684\u91c7\u6837\u8f68\u8ff9\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6270\u52a8\uff0c\u68c0\u6d4b\u5230\u8d85\u8fc7 70% \u7684\u5e7b\u89c9\u6837\u672c\u5e76\u7ea0\u6b63\u4e86\u8d85\u8fc7 25% \u7684\u6837\u672c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5f15\u5165\u65b0\u7684\u4f2a\u5f71\u3002", "conclusion": "RODS \u901a\u8fc7\u5229\u7528\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u6700\u5c0f\u7684\u63a8\u7406\u6210\u672c\uff0c\u5728 AFHQv2\u3001FFHQ \u548c 11k-hands \u7684\u91c7\u6837\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5f15\u5165\u65b0\u7684\u4f2a\u5f71\u3002"}}
{"id": "2507.12380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12380", "abs": "https://arxiv.org/abs/2507.12380", "authors": ["Maximilian Krahn", "Vikas Garg"], "title": "Heat Kernel Goes Topological", "comment": null, "summary": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u590d\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u9ad8\u6548\u8ba1\u7b97\u70ed\u6838\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u62d3\u6251\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u5b9e\u8df5\u4e2d\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u5206\u5b50\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u62d3\u6251\u795e\u7ecf\u7f51\u7edc\u6d89\u53ca\u8f83\u9ad8\u9636\u6d88\u606f\u4f20\u9012\u5e26\u6765\u7684\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u7ec4\u5408\u590d\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u70ed\u6838\u4f5c\u4e3a\u8282\u70b9\u63cf\u8ff0\u7b26\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u751f\u6210\u6392\u5217\u7b49\u53d8\u8868\u793a\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u4ee3\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u7406\u8bba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u88ab\u8bc1\u660e\u5177\u6709\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u53ef\u4ee5\u533a\u5206\u4efb\u610f\u975e\u540c\u6784\u7684\u7ec4\u5408\u590d\u5f62\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u62d3\u6251\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6807\u51c6\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u63cf\u8ff0\u7b26\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u533a\u5206\u590d\u6742\u7684\u62d3\u6251\u7ed3\u6784\u548c\u907f\u514d\u62d3\u6251\u57fa\u51c6\u6d4b\u8bd5\u7684\u76f2\u70b9\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u7ec4\u5408\u590d\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u70ed\u6838\u4f5c\u4e3a\u8282\u70b9\u63cf\u8ff0\u7b26\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u751f\u6210\u6392\u5217\u7b49\u53d8\u8868\u793a\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u4e2d\u3002\u5728\u7406\u8bba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u533a\u5206\u4efb\u610f\u975e\u540c\u6784\u7684\u7ec4\u5408\u590d\u5f62\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u62d3\u6251\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u63cf\u8ff0\u7b26\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u533a\u5206\u590d\u6742\u7684\u62d3\u6251\u7ed3\u6784\u548c\u907f\u514d\u62d3\u6251\u57fa\u51c6\u6d4b\u8bd5\u7684\u76f2\u70b9\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u5177\u6709\u8868\u8fbe\u80fd\u529b\u4e14\u53ef\u6269\u5c55\u7684\u8868\u793a\uff0c\u63a8\u52a8\u4e86\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u4e3a\u5206\u5b50\u5206\u7c7b\u548c\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u5f00\u8f9f\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.12232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12232", "abs": "https://arxiv.org/abs/2507.12232", "authors": ["Tao Chen", "Jingyi Zhang", "Decheng Liu", "Chunlei Peng"], "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "comment": null, "summary": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12383", "abs": "https://arxiv.org/abs/2507.12383", "authors": ["Mohit Prashant", "Arvind Easwaran"], "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "comment": "Preprint", "summary": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u903c\u8fd1\u539f\u59cbMDP\u6765\u964d\u4f4eRL\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u5c06\u6837\u672c\u590d\u6742\u5ea6\u63d0\u9ad8\u5230O(SA log A)\u65f6\u95f4\u6b65\u957f\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u6709\u5f85\u63d0\u9ad8\uff0c\u5e76\u4e14\u7814\u7a76\u4e86\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u79fb\u5173\u7cfb\u5bf9\u6837\u672c\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u7684\u5b50\u96c6\u6765\u903c\u8fd1\u539f\u59cbMDP\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u4e00\u4e2aPAC-MDP\u7b97\u6cd5\uff0c\u5c06\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u5230O(SA log A)\u65f6\u95f4\u6b65\u957f\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u6bd4\u73b0\u6709\u6587\u732e\u66f4\u4f18\u7684\u3001\u5173\u4e8e\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u8bbe\u7f6e\u4e0b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6e10\u8fd1\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u65e0\u6a21\u578b\u7684\u8bbe\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.12236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12236", "abs": "https://arxiv.org/abs/2507.12236", "authors": ["Felix N\u00fctzel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models", "comment": "20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)", "summary": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u548cBBM\u6280\u672f\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u7684\u77ed\u8bed\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u7684\u77ed\u8bed\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u671f\u83b7\u5f97\u6bd4\u73b0\u6709\u5224\u522b\u5f0f\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff09\u8fdb\u884c\u96f6\u6837\u672c\u77ed\u8bed\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aBimodal Bias Merging\uff08BBM\uff09\u7684\u65b0\u578b\u540e\u5904\u7406\u6280\u672f\uff0c\u4ee5\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u504f\u5dee\u6765\u8bc6\u522b\u9ad8\u786e\u5b9a\u6027\u533a\u57df\u3002", "result": "\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5224\u522b\u5f0f\u81ea\u76d1\u7763\u5bf9\u6bd4\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u77ed\u8bed\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0cmIoU\u5206\u6570\u7ffb\u500d\u3002\u901a\u8fc7BBM\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u7684\u77ed\u8bed\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8303\u5f0f\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u66f4\u9c81\u68d2\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.12245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12245", "abs": "https://arxiv.org/abs/2507.12245", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Calisthenics Skills Temporal Video Segmentation", "comment": "9 pages, 6 figures, In Proceedings of the 19th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications - Volume 2", "summary": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u808c\u529b\u8bad\u7ec3\u9759\u6001\u6280\u80fd\u89c6\u9891\u6570\u636e\u96c6\u548c\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u6280\u80fd\u65f6\u95f4\u5206\u5272\u95ee\u9898\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u808c\u529b\u8bad\u7ec3\u9886\u57df\u7684\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u808c\u529b\u8bad\u7ec3\u9886\u57df\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5305\u62ec\u534f\u52a9\u8fd0\u52a8\u5458\u8bad\u7ec3\u548c\u88c1\u5224\u5728\u6bd4\u8d5b\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u808c\u529b\u8bad\u7ec3\u6280\u80fd\u7684\u65f6\u95f4\u89c6\u9891\u5206\u5272\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u808c\u529b\u8bad\u7ec3\u9759\u6001\u6280\u80fd\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u89c6\u9891\u8fdb\u884c\u65f6\u95f4\u5206\u5272\u6807\u6ce8\uff0c\u4ee5\u786e\u5b9a\u6bcf\u4e2a\u6280\u80fd\u7684\u8303\u56f4\u3002\u7136\u540e\uff0c\u62a5\u544a\u4e86\u4e00\u4e2a\u7528\u4e8e\u5728\u6240\u63d0\u51fa\u6570\u636e\u96c6\u4e0a\u89e3\u51b3\u6280\u80fd\u65f6\u95f4\u5206\u5272\u95ee\u9898\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u57fa\u7ebf\u65b9\u6cd5\u5728\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u7a81\u51fa\u4e86\u6240\u63d0\u51fa\u95ee\u9898\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u808c\u529b\u8bad\u7ec3\u9759\u6001\u6280\u80fd\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u62a5\u544a\u4e86\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u6280\u80fd\u65f6\u95f4\u5206\u5272\u95ee\u9898\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\u8be5\u95ee\u9898\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2507.12399", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12399", "abs": "https://arxiv.org/abs/2507.12399", "authors": ["Florian E. Dorner", "Yatong Chen", "Andr\u00e9 F. Cruz", "Fanny Yang"], "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "comment": "35 pages, 9 Figures", "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "AI": {"tldr": "Test-time scaling methods leverage extra compute for better language models, but verifier imperfections were poorly understood theoretically. We proved scaling depends on ROC curve geometry: rejection sampling on local, Best-of-N on global. Rejection sampling can't be predicted from low compute if ROC is unknown. Both methods hit same accuracy with infinite compute, based on ROC slope near origin. Experiments confirm this.", "motivation": "While test-time scaling techniques like Best-of-N and rejection sampling empirically improve language model performance by using additional compute during inference, there's a lack of theoretical understanding regarding the impact of verifier imperfection on their performance. This work aims to fill this gap by providing a theoretical framework to analyze these methods.", "method": "We theoretically analyze the instance-level accuracy of test-time scaling methods, specifically Best-of-N and rejection sampling, by characterizing their performance based on the geometry of the verifier's Receiver Operating Characteristic (ROC) curve. We mathematically derive the relationships between ROC curve properties (local vs. global geometry, slope near the origin) and the scaling behavior of these methods. We then experimentally validate our theoretical results using different versions of Llama and Qwen on the GSM8K dataset.", "result": "We proved that rejection sampling's scaling is determined by the local geometry of the ROC curve, whereas Best-of-N's scaling depends on global properties. Consequently, performance extrapolation for rejection sampling from low-compute regimes is not feasible without knowing the ROC curve. We also found that while rejection sampling outperforms Best-of-N for fixed compute, both methods achieve the same accuracy in the infinite-compute limit, which is dictated by the ROC curve's slope near the origin. Our experiments confirmed these theoretical predictions.", "conclusion": "We theoretically prove how instance-level accuracy of test-time scaling methods (Best-of-N and rejection sampling) is characterized by the geometry of the verifier's ROC curve. Rejection sampling's scaling depends on local ROC geometry, while Best-of-N depends on global properties. Extrapolating rejection sampling performance from low-compute regimes is impossible when the ROC curve is unknown. Both methods converge to the same accuracy in the infinite-compute limit, determined by the ROC curve's slope near the origin. Experiments on GSM8K confirm these findings."}}
{"id": "2507.12248", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12248", "abs": "https://arxiv.org/abs/2507.12248", "authors": ["Anida Nezovi\u0107", "Jalal Romano", "Nada Mari\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST", "comment": null, "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9 Keras\u3001PyTorch \u548c JAX \u4e2d\u7684 CNN \u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u91cd\u70b9\u5173\u6ce8\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\u7684\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u548c\u6bd4\u8f83 Keras\u3001PyTorch \u548c JAX \u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u57fa\u4e8e CNN \u7684\u5b9e\u73b0\uff0c\u56e0\u4e3a\u4e0d\u540c\u6846\u67b6\u7684\u76f8\u5bf9\u6027\u80fd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528 PathMNIST \u6570\u636e\u96c6\uff0c\u8bc4\u4f30 Keras\u3001PyTorch \u548c JAX \u4e2d CNN \u5b9e\u73b0\u7684\u8bad\u7ec3\u6548\u7387\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u5168\u9762\u5206\u6790\u4e86 Keras\u3001PyTorch \u548c JAX \u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u5b9e\u73b0\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u8bad\u7ec3\u6548\u7387\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u4ee5 PathMNIST \u6570\u636e\u96c6\u4e3a\u57fa\u51c6\u3002"}}
{"id": "2507.12412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12412", "abs": "https://arxiv.org/abs/2507.12412", "authors": ["Dzung Dinh", "Boqi Chen", "Marc Niethammer", "Junier Oliva"], "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "comment": null, "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "AI": {"tldr": "NOCTA is a new method for acquiring informative features sequentially, considering cost and temporal dynamics, outperforming existing methods in medical datasets.", "motivation": "Resource constraints in critical applications (e.g., healthcare) limit information gathering for predictions. Temporal prediction tasks add complexity in deciding when/what information is important, considering feature costs (time, money, risk) and evolving data.", "method": "NOCTA is a Non-Greedy Objective Cost-Tradeoff Acquisition method that sequentially acquires the most informative features at inference time. It utilizes a non-parametric method based on nearest neighbors (NOCTA-NP) and a parametric method that predicts the utility of potential acquisitions (NOCTA-P).", "result": "Both NOCTA variants, NOCTA-NP and NOCTA-P, demonstrate superior performance compared to existing methods.", "conclusion": "NOCTA-NP and NOCTA-P outperform existing baselines on both synthetic and real-world medical datasets."}}
{"id": "2507.12269", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12269", "abs": "https://arxiv.org/abs/2507.12269", "authors": ["Sybelle Goedicke-Fritz", "Michelle Bous", "Annika Engel", "Matthias Flotho", "Pascal Hirsch", "Hannah Wittig", "Dino Milanovic", "Dominik Mohr", "Mathias Kaspar", "Sogand Nemat", "Dorothea Kerner", "Arno B\u00fccker", "Andreas Keller", "Sascha Meyer", "Michael Zemlin", "Philipp Flotho"], "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship", "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\u65b0\u751f\u513f\u7b2c\u4e00\u5929\u7684\u80f8\u90e8X\u5149\u7247\uff0c\u4ee5\u9884\u6d4b\u652f\u6c14\u7ba1\u80ba\u53d1\u80b2\u4e0d\u826f\uff08BPD\uff09\u7684\u98ce\u9669\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u4e3a\u4e86\u907f\u514d\u5bf9\u4f4e\u98ce\u9669\u5a74\u513f\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u6bd2\u526f\u4f5c\u7528\uff0c\u53ca\u65e9\u9884\u6d4bBPD\u53ca\u5176\u7ed3\u5c40\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u9884\u9632\u6027\u5e72\u9884\u63aa\u65bd\u5b58\u5728\u4e25\u91cd\u7684\u98ce\u9669\u3002\u5229\u7528\u65b0\u751f\u513f\u5165\u9662\u521d\u671f\uff0824\u5c0f\u65f6\u5185\uff09\u7684X\u5149\u7247\u4f5c\u4e3a\u65e0\u521b\u9884\u6d4b\u5de5\u5177\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528163\u540d\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\uff08$\\\\\\leq$32\u5468\u80ce\u9f84\uff0c401-999\u514b\uff09\u51fa\u751f24\u5c0f\u65f6\u5185\u7684\u80f8\u90e8X\u5149\u7247\uff0c\u5bf9ResNet-50\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u3002\u7814\u7a76\u4e2d\u91c7\u7528\u4e86\u6e10\u8fdb\u5f0f\u5c42\u51bb\u7ed3\u3001\u5224\u522b\u6027\u5b66\u4e60\u7387\u3001CutMix\u6570\u636e\u589e\u5f3a\u548c\u7ebf\u6027\u63a2\u6d4b\u7b49\u6280\u672f\u6765\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u9884\u6d4b\u4e2d/\u91cd\u5ea6BPD\u7ed3\u5c40\u65b9\u9762\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u8fbe\u5230\u4e860.78 $\\\\%\\'+\\'+/- 0.10\u7684AUROC\uff0c0.69 $\\\\%\\'+\\'+/- 0.10\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c0.67 $\\\\%\\'+\\'+/- 0.11\u7684F1\u5206\u6570\u3002\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u663e\u8457\u4f18\u4e8eImageNet\u521d\u59cb\u5316\uff08p = 0.031\uff09\uff0c\u8868\u660e\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u5bf9\u4e8eBPD\u7ed3\u5c40\u9884\u6d4b\u7684\u91cd\u8981\u6027\u3002\u800c\u5e38\u89c4\u7684IRDS\u5206\u7ea7\u9884\u6d4b\u4ef7\u503c\u6709\u9650\uff08AUROC 0.57 $\\\\%\\'+\\'+/- 0.11\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5bf9\u65b0\u751f\u513f\u7b2c\u4e00\u5929\u7684\u80f8\u90e8X\u5149\u7247\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u652f\u6c14\u7ba1\u80ba\u53d1\u80b2\u4e0d\u826f\uff08BPD\uff09\u7684\u7ed3\u5c40\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6e10\u8fdb\u5f0f\u51bb\u7ed3\u548c\u7ebf\u6027\u63a2\u6d4b\uff0c\u5728\u8ba1\u7b97\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fdb\u884c\u73b0\u573a\u5b9e\u65bd\u548c\u672a\u6765\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u3002"}}
{"id": "2507.12419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12419", "abs": "https://arxiv.org/abs/2507.12419", "authors": ["Andrea Perin", "Giacomo Lagomarsini", "Claudio Gallicchio", "Giuseppe Nuti"], "title": "Mixture of Raytraced Experts", "comment": "Preliminary version (pre-submission)", "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5149\u7ebf\u8ffd\u8e2a\u4e13\u5bb6\u6df7\u5408\u4f53\u201d\u7684\u65b0\u578b MoE \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u53ef\u4ee5\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u53d8\u8ba1\u7b97\u91cf\u548c\u53ef\u53d8\u6df1\u5ea6\uff0c\u5e76\u80fd\u5728\u8bad\u7ec3\u4e2d\u51cf\u5c11 epoch \u6570\u91cf\u548c\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709 MoE \u67b6\u6784\u901a\u5e38\u9700\u8981\u4e3a\u7ed9\u5b9a\u6837\u672c\u6267\u884c\u56fa\u5b9a\u91cf\u7684\u8ba1\u7b97\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u63a2\u7d22\u80fd\u591f\u968f\u7740\u8ba1\u7b97\u901a\u8fc7\u4e13\u5bb6\u5e8f\u5217\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5806\u53e0\u5f0f MoE \u67b6\u6784\uff0c\u79f0\u4e3a\u201c\u5149\u7ebf\u8ffd\u8e2a\u4e13\u5bb6\u6df7\u5408\u4f53\u201d\uff08Mixture of Raytraced Experts\uff09\uff0c\u80fd\u591f\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\uff0c\u4ece\u800c\u4ea7\u751f\u53ef\u53d8\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u8ba1\u7b97\u56fe\u3002\u8bad\u7ec3\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8fed\u4ee3\u5730\u4ece\u5019\u9009\u4e13\u5bb6\u96c6\u4e2d\u91c7\u6837\u6765\u5c55\u5f00\u5e8f\u5217\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u8bad\u7ec3\u7684 epoch \u6570\u91cf\u4e0a\u51cf\u5c11\u4e86 10% \u5230 40%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a MoE \u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u6f5c\u529b\u8bbe\u8ba1\u51fa\u66f4\u5feb\u3001\u66f4\u5177\u8868\u73b0\u529b\u7684\u6a21\u578b\u3002"}}
{"id": "2507.12283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12283", "abs": "https://arxiv.org/abs/2507.12283", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wang", "Ze Niu", "Dacheng Yu", "Emily Davis", "Bo Zhang"], "title": "FADE: Adversarial Concept Erasure in Flow Models", "comment": "Camera Ready", "summary": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.", "AI": {"tldr": "FADE \u662f\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u79fb\u9664\u6a21\u578b\u4e2d\u7684\u654f\u611f\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6269\u6563\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u8bb0\u5fc6\u654f\u611f\u6982\u5ff5\u6216\u56fa\u5316\u504f\u89c1\u800c\u5e26\u6765\u9690\u79c1\u548c\u516c\u5e73\u6027\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u79fb\u9664\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u6982\u5ff5\uff0c\u4f8b\u5982\u79c1\u4eba\u4e2a\u4f53\u6216\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\u3002", "method": "FADE\uff08Fair Adversarial Diffusion Erasure\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8f68\u8ff9\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u548c\u5bf9\u6297\u6027\u76ee\u6807\uff0c\u65e8\u5728\u4ece\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u4e2d\u79fb\u9664\u6307\u5b9a\u7684\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\uff0c\u80fd\u591f\u6700\u5c0f\u5316\u64e6\u9664\u6982\u5ff5\u4e0e\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u4ece\u800c\u786e\u4fdd\u9690\u79c1\u548c\u516c\u5e73\u6027\u3002", "result": "FADE \u5728 Stable Diffusion \u548c FLUX \u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6982\u5ff5\u79fb\u9664\u6027\u80fd\uff0c\u5728\u79fb\u9664\u6548\u679c\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e ESD\u3001UCE\u3001MACE \u548c ANT \u7b49\u8fd1\u671f\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFADE \u5728\u6982\u5ff5\u79fb\u9664\u548c\u4fdd\u771f\u5ea6\u7684\u8c03\u548c\u5e73\u5747\u503c\u65b9\u9762\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86 5-10%\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u5bf9\u6297\u6027\u548c\u8f68\u8ff9\u4fdd\u6301\u76ee\u6807\u5bf9\u63d0\u5347\u6027\u80fd\u7684\u8d21\u732e\u3002", "conclusion": "FADE \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u548c\u5bf9\u6297\u6027\u76ee\u6807\uff0c\u53ef\u4ee5\u53ef\u9760\u5730\u79fb\u9664\u6307\u5b9a\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u79c1\u4eba\u4e2a\u4eba\u6216\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u6a21\u578b\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u5728 Stable Diffusion \u548c FLUX \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002FADE \u901a\u8fc7\u975e\u5bf9\u6297\u6027\u548c\u8f68\u8ff9\u4fdd\u6301\u76ee\u6807\uff0c\u5728\u4e0d\u8fdb\u884c\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6307\u5b9a\u6982\u5ff5\u7684\u9057\u5fd8\uff0c\u4e3a\u5b89\u5168\u548c\u516c\u5e73\u7684\u751f\u6210\u6a21\u578b\u6811\u7acb\u4e86\u65b0\u7684\u6807\u51c6\u3002"}}
{"id": "2507.12435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12435", "abs": "https://arxiv.org/abs/2507.12435", "authors": ["Yi Li", "David Mccoy", "Nolan Gunter", "Kaitlyn Lee", "Alejandro Schuler", "Mark van der Laan"], "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "comment": null, "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "AI": {"tldr": "TDA\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5c06TMLE\u76f4\u63a5\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u65e0\u9700\u5bf9\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u9650\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u56e0\u679c\u53c2\u6570\uff08\u5982\u5904\u7406\u6548\u5e94\u6216\u6574\u4e2a\u751f\u5b58\u66f2\u7ebf\uff09\u7684\u65e0\u504f\u4f30\u8ba1\u548c\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56e0\u679c\u53c2\u6570\u63a8\u65ad\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u548c\u76ee\u6807\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u8981\u4e48\u4f9d\u8d56\u4e8e\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u51b3\u6709\u6548\u5f71\u54cd\u51fd\u6570\u65b9\u7a0b\u7684\u76ee\u6807\u635f\u5931\uff0c\u8981\u4e48\u5728\u591a\u53c2\u6570\u8bbe\u7f6e\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "TDA\u6846\u67b6\u5c06TMLE\u5d4c\u5165\u5230\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u51bb\u7ed3\u9664\u4e00\u5c0f\u90e8\u5206\u201c\u76ee\u6807\u201d\u5b50\u96c6\u5916\u7684\u6240\u6709\u53c2\u6570\uff0c\u5e76\u6cbf\u7740\u4ece\u5f71\u54cd\u51fd\u6570\u5230\u635f\u5931\u5173\u4e8e\u6743\u91cd\u7684\u68af\u5ea6\u6295\u5f71\u5bfc\u51fa\u7684\u76ee\u6807\u68af\u5ea6\u8fdb\u884c\u8fed\u4ee3\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u65e0\u504f\u4f30\u8ba1\u548c\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "TDA\u5728IHDP\u6570\u636e\u96c6\uff08\u5e73\u5747\u5904\u7406\u6548\u5e94\uff09\u548c\u5177\u6709\u4fe1\u606f\u6027\u5ba1\u67e5\u7684\u6a21\u62df\u751f\u5b58\u6570\u636e\u4e0a\uff0c\u76f8\u6bd4\u4e8e\u6807\u51c6\u7684\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\u548c\u5148\u524d\u7684\u4e8b\u540e\u65b9\u6cd5\uff0c\u80fd\u591f\u51cf\u5c11\u504f\u5dee\u5e76\u63d0\u9ad8\u8986\u76d6\u7387\u3002", "conclusion": "TDA\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u5728\u73b0\u4ee3\u6df1\u5ea6\u67b6\u6784\u4e2d\u5bf9\u590d\u6742\u7684\u591a\u53c2\u6570\u76ee\u6807\u8fdb\u884c\u4e25\u683c\u7684\u56e0\u679c\u63a8\u65ad\u3002"}}
{"id": "2507.12292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12292", "abs": "https://arxiv.org/abs/2507.12292", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation", "comment": "13 pages, 4 figures, In International Conference on Image Analysis\n  and Processing", "summary": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.", "AI": {"tldr": "\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u4f30\u8ba1\u548c\u8fd0\u52a8\u5458\u56fe\u50cf\u5757\u68c0\u7d22\u7684\u5065\u7f8e\u64cd\u6280\u80fd\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u6bd4\u4f20\u7edf\u59ff\u52bf\u4f30\u8ba1\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u3002", "motivation": "\u4f20\u7edf\u7684\u5065\u7f8e\u64cd\u6280\u80fd\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u59ff\u52bf\u4f30\u8ba1\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u63a8\u7406\u65f6\u95f4\u957f\u3001\u8bbe\u7f6e\u590d\u6742\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u6216\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u5feb\u901f\u7684\u5065\u7f8e\u64cd\u6280\u80fd\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u7684\u5065\u7f8e\u64cd\u6280\u80fd\u8bc6\u522b\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u4f30\u8ba1\u548c\u8fd0\u52a8\u5458\u56fe\u50cf\u5757\u68c0\u7d22\u6765\u66ff\u4ee3\u8ba1\u7b97\u91cf\u5927\u7684\u59ff\u52bf\u4f30\u8ba1\u6a21\u5757\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4f7f\u7528\u4e86 Depth Anything V2 \u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u4f7f\u7528 YOLOv10 \u8fdb\u884c\u8fd0\u52a8\u5458\u5b9a\u4f4d\uff0c\u4ece\u800c\u5c06\u8fd0\u52a8\u5458\u4ece\u80cc\u666f\u4e2d\u5206\u5272\u51fa\u6765\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4f20\u7edf\u7684\u59ff\u52bf\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u4f7f\u7528 RGB \u56fe\u50cf\u5757\u65f6\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u9aa8\u9abc\u7684\u65b9\u6cd5\u5feb 38.3 \u500d\uff1b\u4f7f\u7528\u6df1\u5ea6\u56fe\u50cf\u5757\u65f6\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4ece 0.815 \u63d0\u9ad8\u5230 0.837\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u907f\u514d\u4e86\u8ba1\u7b97\u91cf\u5927\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u540e\uff0c\u5728\u533a\u5206\u4e0d\u540c\u5065\u7f8e\u64cd\u6280\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\uff0c\u4e0e\u57fa\u4e8e\u9aa8\u9abc\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528 RGB \u56fe\u50cf\u5757\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86 38.3 \u500d\uff0c\u4f7f\u7528\u6df1\u5ea6\u56fe\u50cf\u5757\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e5f\u4ece 0.815 \u63d0\u9ad8\u5230 0.837\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u6613\u4e8e\u672a\u6765\u6539\u8fdb\u548c\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.12318", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12318", "abs": "https://arxiv.org/abs/2507.12318", "authors": ["Samuel Lavoie", "Michael Noukhovitch", "Aaron Courville"], "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "comment": "In submission, 22 pages, 7 tables, 12 figures", "summary": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "AI": {"tldr": "Discrete Latent Codes (DLCs) improve diffusion models for image generation and enable text-to-image synthesis by offering better representation properties like compositionality.", "motivation": "To investigate image representations for diffusion models, focusing on improving sample fidelity, ease of generation, and compositionality for out-of-training sample generation.", "method": "Introduced Discrete Latent Code (DLC), a sequence of discrete tokens derived from Simplicial Embeddings trained with self-supervised learning. Diffusion models were trained using DLCs, and their performance was evaluated on unconditional image generation (ImageNet) and text-to-image generation tasks.", "result": "Diffusion models trained with DLCs achieved improved generation fidelity, setting a new state-of-the-art for unconditional image generation on ImageNet. DLC compositionality enabled the generation of novel out-of-distribution samples with combined semantics. Text-to-image generation was successfully demonstrated by fine-tuning a text diffusion model to generate DLCs.", "conclusion": "DLCs, a novel discrete latent code representation derived from Simplicial Embeddings, improve diffusion model performance by enhancing sample fidelity, ease of generation, and compositionality. This allows for out-of-distribution sample generation and enables text-to-image synthesis by leveraging pretrained language models."}}
{"id": "2507.12453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12453", "abs": "https://arxiv.org/abs/2507.12453", "authors": ["Qian Xie", "Linda Cai", "Alexander Terenin", "Peter I. Frazier", "Ziv Scully"], "title": "Cost-aware Stopping for Bayesian Optimization", "comment": null, "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6210\u672c\u611f\u77e5\u505c\u6b62\u89c4\u5219\uff0c\u4e0e PBGI \u7ed3\u5408\u4f7f\u7528\u53ef\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u3001\u79d1\u5b66\u53d1\u73b0\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7b49\u5e94\u7528\u7684\u6602\u8d35\u7684\u9ed1\u76d2\u51fd\u6570\u8bc4\u4f30\u4e2d\uff0c\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u8bc4\u4f30\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5b9e\u9645\u8003\u8651\u56e0\u7d20\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6210\u672c\u611f\u77e5\u578b\u505c\u6b62\u89c4\u5219\u7f3a\u4e4f\u4fdd\u8bc1\u5b83\u4eec\u5728\u4ea7\u751f\u8fc7\u591a\u7684\u51fd\u6570\u8bc4\u4f30\u6210\u672c\u4e4b\u524d\u505c\u6b62\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u611f\u77e5\u578b\u505c\u6b62\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u80fd\u591f\u9002\u5e94\u53d8\u5316\u7684\u8bc4\u4f30\u6210\u672c\u4e14\u65e0\u9700\u8fdb\u884c\u542f\u53d1\u5f0f\u8c03\u6574\u3002\u8be5\u89c4\u5219\u5728\u7406\u8bba\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u6210\u672c\u611f\u77e5\u578b\u91c7\u96c6\u51fd\u6570\uff08\u5373 Pandora's Box Gittins Index (PBGI) \u548c\u6bcf\u6b21\u65e5\u5fd7\u671f\u671b\u6539\u8fdb\u6210\u672c\uff09\u76f8\u5173\u8054\u3002\u8bc1\u660e\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86\u4e0e\u8fd9\u4e24\u79cd\u91c7\u96c6\u51fd\u6570\u914d\u5bf9\u7684\u505c\u6b62\u89c4\u5219\u6240\u4ea7\u751f\u7684\u9884\u671f\u7d2f\u79ef\u8bc4\u4f30\u6210\u672c\u3002", "result": "\u5728\u5408\u6210\u548c\u7ecf\u9a8c\u4efb\u52a1\uff08\u5305\u62ec\u8d85\u53c2\u6570\u4f18\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5927\u5c0f\u641c\u7d22\uff09\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6210\u672c\u611f\u77e5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6210\u672c\u611f\u77e5\u578b\u505c\u6b62\u89c4\u5219\u5728\u4e0e PBGI \u91c7\u96c6\u51fd\u6570\u914d\u5bf9\u65f6\uff0c\u5728\u6210\u672c\u8c03\u6574\u7684\u7b80\u5355\u9057\u61be\u65b9\u9762\uff0c\u6301\u7eed\u5339\u914d\u6216\u4f18\u4e8e\u5176\u4ed6\u91c7\u96c6\u51fd\u6570-\u505c\u6b62\u89c4\u5219\u5bf9\uff0c\u8be5\u5ea6\u91cf\u6355\u6349\u4e86\u89e3\u89c9\u8d28\u91cf\u548c\u7d2f\u79ef\u8bc4\u4f30\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2507.12336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12336", "abs": "https://arxiv.org/abs/2507.12336", "authors": ["Subin Jeon", "In Cho", "Junyoung Hong", "Seon Joo Kim"], "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "comment": null, "summary": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.", "AI": {"tldr": "KeyDiff3D\uff1a\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u548c3D\u5bf9\u8c61\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u624b\u52a8\u6ce8\u91ca\u6216\u6807\u5b9a\u597d\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u800cKeyDiff3D\u65e8\u5728\u4ec5\u4f7f\u7528\u5355\u89c6\u89d2\u56fe\u50cf\u96c6\u5408\u5b9e\u73b0\u6b64\u76ee\u6807\u3002", "method": "KeyDiff3D\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u63d0\u53d6\u5176\u4e2d\u95f4\u8868\u793a\u4ee5\u6784\u5efa3D\u7279\u5f81\u4f53\uff0c\u4ece\u800c\u5b9e\u73b0\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKeyDiff3D\u5728\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca3D\u5bf9\u8c61\u64cd\u4f5c\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u5404\u79cd\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5728\u91ce\u5916\u548c\u975e\u9886\u57df\u6570\u636e\u96c6\u3002", "conclusion": "KeyDiff3D\u662f\u4e00\u4e2a\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\u6216\u591a\u89c6\u89d2\u6807\u5b9a\u5373\u53ef\u8fdb\u884c\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u7684\u6846\u67b6\u3002\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u63d0\u53d62D\u591a\u89c6\u89d2\u7279\u5f81\u6784\u5efa3D\u7279\u5f81\u4f53\uff0c\u5c06\u6269\u6563\u6a21\u578b\u9690\u5f0f\u5b66\u4e60\u76843D\u5148\u9a8c\u8f6c\u5316\u4e3a\u663e\u5f0f3D\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u5bf9\u6269\u6563\u6a21\u578b\u751f\u6210\u76843D\u5bf9\u8c61\u8fdb\u884c\u64cd\u4f5c\u3002\u5728Human3.6M\u3001Stanford Dogs\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12344", "abs": "https://arxiv.org/abs/2507.12344", "authors": ["Ahmet O\u011fuz Salt\u0131k", "Max Voigt", "Sourav Modak", "Mike Beckworth", "Anthony Stein"], "title": "Improving Lightweight Weed Detection via Knowledge Distillation", "comment": null, "summary": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u901a\u9053\u77e5\u8bc6\u84b8\u998f\uff08CWD\uff09\u548c\u63a9\u7801\u751f\u6210\u84b8\u998f\uff08MGD\uff09\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u7cbe\u51c6\u519c\u4e1a\u548c\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u7684\u6742\u8349\u68c0\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u4e0a\u90e8\u7f72\u7cbe\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u533a\u5206\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u5e38\u89c1\u7684\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u6742\u8349\u7269\u79cd\u65f6\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u901a\u9053\u77e5\u8bc6\u84b8\u998f\uff08CWD\uff09\u548c\u63a9\u7801\u751f\u6210\u84b8\u998f\uff08MGD\uff09\uff0c\u4ee5\u63d0\u9ad8\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u5b9e\u65f6\u667a\u80fd\u55b7\u6d12\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u3002\u4f7f\u7528YOLO11x\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0cYOLO11n\u4f5c\u4e3a\u53c2\u8003\u548c\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCWD\u548cMGD\u6709\u6548\u5730\u5c06\u77e5\u8bc6\u4ece\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u5b66\u751f\u6a21\u578b\uff0c\u5728\u6240\u6709\u7c7b\u522b\u7684AP50\u4e0a\u90fd\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u84b8\u998f\u540e\u7684CWD\u5b66\u751f\u6a21\u578b\u5728mAP50\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e862.5%\uff0cMGD\u63d0\u9ad8\u4e861.9%\uff0c\u800c\u6ca1\u6709\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u9a8c\u8bc1\u4e86CWD\u548cMGD\u5728Jetson Orin Nano\u548cRaspberry Pi 5\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "CWD\u548cMGD\u662f\u63d0\u9ad8\u519c\u4e1a\u548c\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6742\u8349\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u6709\u6548\u3001\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.12359", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12359", "abs": "https://arxiv.org/abs/2507.12359", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "title": "Cluster Contrast for Unsupervised Visual Representation Learning", "comment": "ICIP 2025", "summary": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.", "AI": {"tldr": "CueCo\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u3002", "motivation": "CueCo\u65e8\u5728\u6709\u6548\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u540c\u65f6\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5206\u6563\u548c\u5bf9\u9f50\u7279\u5f81\u8868\u793a\u3002", "method": "CueCo\u91c7\u7528\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff08\u67e5\u8be2\u548c\u952e\uff09\uff0c\u5176\u4e2d\u952e\u7f51\u7edc\u901a\u8fc7\u67e5\u8be2\u8f93\u7684\u6162\u79fb\u5e73\u5747\u503c\u8fdb\u884c\u66f4\u65b0\u3002\u5b83\u5229\u7528\u5bf9\u6bd4\u635f\u5931\u6765\u5206\u79bb\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u6027\uff0c\u5e76\u5229\u7528\u805a\u7c7b\u76ee\u6807\u5c06\u540c\u4e00\u7c07\u7684\u7279\u5f81\u62c9\u5230\u4e00\u8d77\uff0c\u4fc3\u8fdb\u7c7b\u5185\u7d27\u5bc6\u5ea6\u3002", "result": "CueCo\u5728CIFAR-10\u4e0a\u5b9e\u73b0\u4e8691.40%\u7684top-1\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5728CIFAR-100\u4e0a\u5b9e\u73b0\u4e8668.56%\uff0c\u5728ImageNet-100\u4e0a\u4f7f\u7528ResNet-18\u9aa8\u5e72\u7f51\u7edc\u548c\u7ebf\u6027\u8bc4\u4f30\u5b9e\u73b0\u4e8678.65%\u3002", "conclusion": "CueCo\u901a\u8fc7\u6574\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\uff0c\u4e3a\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u8bbe\u5b9a\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.12382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12382", "abs": "https://arxiv.org/abs/2507.12382", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "comment": "10 pages; 2 figures; Have been accepted by MICCAI 2025", "summary": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "AI": {"tldr": "\u63d0\u51faText-SemiSeg\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u4fe1\u606f\u63d0\u5347\u534a\u76d1\u7763\u4e09\u7ef4\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u6587\u672c\u4fe1\u606f\u63d0\u4f9b\u989d\u5916\u4e0a\u4e0b\u6587\u4ee5\u589e\u5f3a\u89c6\u89c9\u8bed\u4e49\u7406\u89e3\uff0c\u5bf9\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u63a2\u7d22\u5229\u7528\u6587\u672c\u6570\u636e\u589e\u5f3a\u4e09\u7ef4\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u8bed\u4e49\u5d4c\u5165\u7684\u7814\u7a76\u4ecd\u7136\u5f88\u5c11\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u9a71\u52a8\u7684\u591a\u5e73\u9762\u89c6\u89c9\u4ea4\u4e92\u6846\u67b6Text-SemiSeg\uff0c\u5305\u542b\u6587\u672c\u589e\u5f3a\u591a\u5e73\u9762\u8868\u793a\uff08TMR\uff09\u3001\u7c7b\u522b\u611f\u77e5\u8bed\u4e49\u5bf9\u9f50\uff08CSA\uff09\u548c\u52a8\u6001\u8ba4\u77e5\u589e\u5f3a\uff08DCA\uff09\u4e09\u4e2a\u6a21\u5757\u3002TMR\u901a\u8fc7\u5e73\u9762\u6620\u5c04\u4fc3\u8fdb\u6587\u672c-\u89c6\u89c9\u4ea4\u4e92\uff0c\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\u7684\u7c7b\u522b\u611f\u77e5\u80fd\u529b\u3002CSA\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u53d8\u91cf\u5bf9\u6587\u672c\u7279\u5f81\u548c\u89c6\u89c9\u7279\u5f81\u4e2d\u95f4\u5c42\u8fdb\u884c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3002DCA\u901a\u8fc7\u6570\u636e\u4ea4\u4e92\u51cf\u5c0f\u4e86\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5229\u7528\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4e14\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Text-SemiSeg\u6846\u67b6\u901a\u8fc7\u878d\u5408\u6587\u672c\u4fe1\u606f\u6709\u6548\u589e\u5f3a\u4e86\u4e09\u7ef4\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.12396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12396", "abs": "https://arxiv.org/abs/2507.12396", "authors": ["Hayat Ullah", "Abbas Khan", "Arslan Munir", "Hari Kalva"], "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments", "comment": "14 pages", "summary": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12416", "abs": "https://arxiv.org/abs/2507.12416", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "comment": "Accepted to ICML 2025", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "AI": {"tldr": "QuRe \u662f\u4e00\u79cd\u65b0\u7684\u590d\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u9519\u8bef\u8d1f\u4f8b\u548c\u5f15\u5165\u786c\u8d1f\u4f8b\u91c7\u6837\u7b56\u7565\u6765\u63d0\u9ad8\u68c0\u7d22\u76f8\u5173\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u590d\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u65b9\u6cd5\u4ec5\u5173\u6ce8\u68c0\u7d22\u76ee\u6807\u56fe\u50cf\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u56fe\u50cf\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u68c0\u7d22\u4e0d\u76f8\u5173\u7684\u56fe\u50cf\u5e76\u964d\u4f4e\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a QuRe \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u5956\u52b1\u6a21\u578b\u76ee\u6807\u6765\u51cf\u5c11\u9519\u8bef\u8d1f\u4f8b\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u786c\u8d1f\u4f8b\u91c7\u6837\u7b56\u7565\u6765\u6709\u6548\u8fc7\u6ee4\u9519\u8bef\u8d1f\u4f8b\u3002", "result": "QuRe \u5728 FashionIQ \u548c CIRR \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728 HP-FashionIQ \u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u6700\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "QuRe \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728 HP-FashionIQ \u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u504f\u597d\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.12420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12420", "abs": "https://arxiv.org/abs/2507.12420", "authors": ["Haoyuan Liu", "Hiroshi Watanabe"], "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization", "comment": null, "summary": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.", "AI": {"tldr": "\u63d0\u51faInterpIoU\u548cDynamic InterpIoU\u635f\u5931\u51fd\u6570\uff0c\u7528\u63d2\u503c\u6846\u7684IoU\u66ff\u4ee3\u624b\u5de5\u51e0\u4f55\u60e9\u7f5a\uff0c\u89e3\u51b3\u4e86\u8fb9\u754c\u6846\u56de\u5f52\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u548c\u8fb9\u754c\u6846\u653e\u5927\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eIoU\u7684\u635f\u5931\u51fd\u6570\u901a\u5e38\u5305\u542b\u624b\u5de5\u8bbe\u8ba1\u7684\u51e0\u4f55\u60e9\u7f5a\u9879\uff0c\u4ee5\u89e3\u51b3IoU\u5728\u975e\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u4e0d\u53ef\u5fae\u6027\u95ee\u9898\u5e76\u63d0\u5347\u8fb9\u754c\u6846\u56de\u5f52\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u60e9\u7f5a\u9879\u5bf9\u6846\u7684\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u5206\u5e03\u654f\u611f\uff0c\u5e38\u5e38\u5bfc\u81f4\u4f18\u5316\u4e0d\u7406\u60f3\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u5e76\u4e14\u53ef\u80fd\u56e0\u4e0eIoU\u76ee\u6807\u4e0d\u5339\u914d\u800c\u4ea7\u751f\u8fb9\u754c\u6846\u653e\u5927\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInterpIoU\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7528\u63d2\u503c\u6846\u4e0e\u76ee\u6807\u6846\u4e4b\u95f4IoU\u7684\u9879\u6765\u66ff\u4ee3\u624b\u5de5\u8bbe\u8ba1\u7684\u51e0\u4f55\u60e9\u7f5a\u9879\u3002\u901a\u8fc7\u4f7f\u7528\u63d2\u503c\u6846\u6765\u8fde\u63a5\u9884\u6d4b\u6846\u548c\u771f\u5b9e\u6846\u4e4b\u95f4\u7684\u5dee\u5f02\uff0cInterpIoU\u5728\u975e\u91cd\u53e0\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u68af\u5ea6\uff0c\u5e76\u80fd\u56fa\u6709\u5730\u907f\u514d\u56e0\u4e0d\u5339\u914d\u7684\u60e9\u7f5a\u9879\u5bfc\u81f4\u7684\u8fb9\u754c\u6846\u653e\u5927\u95ee\u9898\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86Dynamic InterpIoU\uff0c\u8be5\u65b9\u6cd5\u6839\u636eIoU\u503c\u52a8\u6001\u8c03\u6574\u63d2\u503c\u7cfb\u6570\uff0c\u589e\u5f3a\u4e86\u5bf9\u4e0d\u540c\u76ee\u6807\u5206\u5e03\u573a\u666f\u7684\u9002\u5e94\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cIoU\u672c\u8eab\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u7406\u60f3\u7684\u56de\u5f52\u76ee\u6807\uff0c\u800c\u73b0\u6709\u7684\u51e0\u4f55\u60e9\u7f5a\u9879\u662f\u4e0d\u5fc5\u8981\u7684\u4e14\u6b21\u4f18\u7684\u3002InterpIoU\u901a\u8fc7\u63d2\u503c\u6846\u6709\u6548\u89e3\u51b3\u4e86\u975e\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u5e76\u907f\u514d\u4e86\u8fb9\u754c\u6846\u653e\u5927\u3002Dynamic InterpIoU\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63d2\u503c\u7cfb\u6570\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "InterpIoU\u53ca\u5176\u52a8\u6001\u7248\u672cDynamic InterpIoU\u5728COCO\u3001VisDrone\u548cPASCAL VOC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eIoU\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u68c0\u6d4b\u6846\u67b6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eIoU\u7684\u635f\u5931\u51fd\u6570\uff0c\u5c24\u5176\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.12426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12426", "abs": "https://arxiv.org/abs/2507.12426", "authors": ["Hayat Ullah", "Muhammad Ali Shafique", "Abbas Khan", "Arslan Munir"], "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition", "comment": "17 pages", "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.", "AI": {"tldr": "DVFL-Net\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u65f6\u7a7a\u7279\u5f81\u8c03\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5728\u89c6\u9891\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u8ba1\u7b97\u4e0a\u4ecd\u7136\u975e\u5e38\u6602\u8d35\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6570\u636e\u65f6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6765\u5b9e\u73b0\u8bbe\u5907\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u9891\u7126\u70b9\u8c03\u5236\u7f51\u7edc\uff08DVFL-Net\uff09\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u65f6\u7a7a\u7279\u5f81\u8c03\u5236\uff0c\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8bbe\u5907\u90e8\u7f72\u3002\u91c7\u7528\u524d\u5411 Kullback-Leibler (KL) \u6563\u5ea6\u4e0e\u65f6\u7a7a\u7126\u70b9\u8c03\u5236\u6765\u6709\u6548\u4f20\u9012\u4e0a\u4e0b\u6587\u3002", "result": "DVFL-Net\u5728UCF50\u3001UCF101\u3001HMDB51\u3001SSV2\u548cKinetics-400\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\uff08HAR\uff09\u65b9\u9762\u4e0e\u6700\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "DVFL-Net\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3001\u51cf\u5c11\u7684GFLOPs\u548c\u5f3a\u5927\u7684\u51c6\u786e\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u5b9e\u65f6\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\uff08HAR\uff09\u5e94\u7528\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12441", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12441", "abs": "https://arxiv.org/abs/2507.12441", "authors": ["Yen-Linh Vu", "Dinh-Thang Duong", "Truong-Binh Duong", "Anh-Khoi Nguyen", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Jianhua Xing", "Xingjian Li", "Tianyang Wang", "Ulas Bagci", "Min Xu"], "title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "comment": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "summary": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "AI": {"tldr": "DAM-QA \u662f\u4e00\u4e2a\u5229\u7528 DAM \u7684\u533a\u57df\u611f\u77e5\u80fd\u529b\u6765\u89e3\u51b3\u6587\u672c\u4e30\u5bcc VQA \u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408\u591a\u533a\u57df\u7b54\u6848\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5728 DocVQA \u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u5728\u5305\u542b\u5bc6\u96c6\u6587\u672c\u7684\u56fe\u50cf\u4e2d\uff0c\u7ec6\u7c92\u5ea6\u5730\u63d0\u53d6\u6587\u672c\u4fe1\u606f\u5bf9\u4e8e VQA \u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u6211\u4eec\u5047\u8bbe DAM \u7684\u533a\u57df\u7ea7\u63cf\u8ff0\u80fd\u529b\u5bf9 VQA \u4efb\u52a1\u6709\u76ca\uff0c\u7279\u522b\u662f\u5728\u8fd9\u4e9b\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u3002", "method": "DAM-QA \u6846\u67b6\u5229\u7528 DAM \u7684\u533a\u57df\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u805a\u5408\u6765\u81ea\u56fe\u50cf\u4e0d\u540c\u533a\u57df\u7684\u7b54\u6848\u6765\u89e3\u51b3\u6587\u672c\u4e30\u5bcc\u7684 VQA \u95ee\u9898\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u673a\u5236\u6765\u6574\u5408\u591a\u533a\u57df\u89c6\u56fe\u7684\u7b54\u6848\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8bc6\u522b\u4e0e\u6587\u672c\u76f8\u5173\u7684\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDAM-QA \u5728\u516d\u4e2a VQA \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf DAM \u6a21\u578b\uff0c\u5728 DocVQA \u4e0a\u53d6\u5f97\u4e86 7 \u4e2a\u767e\u5206\u70b9\u7684\u63d0\u5347\u3002\u6b64\u5916\uff0cDAM-QA \u5728\u53c2\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u533a\u57df\u611f\u77e5\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6027\u80fd\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5f3a\u5927\u7684\u901a\u7528 VLM \u7684\u5dee\u8ddd\u3002", "conclusion": "DAM-QA \u6846\u67b6\u5728\u6587\u672c\u4e30\u5bcc\u7684 VQA \u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4e0e\u9ad8\u6548\u7684\u96c6\u6210\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684 VQA \u4efb\u52a1\u3002"}}
{"id": "2507.12449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12449", "abs": "https://arxiv.org/abs/2507.12449", "authors": ["Van-Hoang-Anh Phan", "Chi-Tam Nguyen", "Doan-Trung Au", "Thanh-Danh Phan", "Minh-Thien Duong", "My-Ha Le"], "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios", "comment": "7 pages, 6 figures, 4 tables, HSI 2025", "summary": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8", "AI": {"tldr": "\u4e00\u4e2a\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u5c31\u80fd\u6709\u6548\u907f\u969c\u7684\u7cfb\u7edf\uff0c\u57fa\u4e8eYOLOv11\u548cDepth Anything V2\u7684\u7cfb\u7edf\uff0c\u5728\u5927\u5b66\u6821\u56ed\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4e3a\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\uff0c\u907f\u969c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4ec5\u6444\u50cf\u5934\u611f\u77e5\u6a21\u5757\u548c\u57fa\u4e8eFrenet-Pure Pursuit\u7684\u89c4\u5212\u7b56\u7565\u7684\u9ad8\u6548\u907f\u969c\u6d41\u7a0b\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u5229\u7528YOLOv11\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u5229\u7528Depth Anything V2\u7b49\u5148\u8fdb\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u7b97\u6a21\u578b\u6765\u4f30\u7b97\u7269\u4f53\u8ddd\u79bb\u3002", "result": "\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u6df1\u5165\u4e86\u89e3\u5b83\u4eec\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5927\u5b66\u6821\u56ed\u7684\u5404\u79cd\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u5404\u79cd\u969c\u788d\u7269\u548c\u589e\u5f3a\u81ea\u4e3b\u5bfc\u822a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12455", "abs": "https://arxiv.org/abs/2507.12455", "authors": ["Shangpin Peng", "Senqiao Yang", "Li Jiang", "Zhuotao Tian"], "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "comment": null, "summary": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "AI": {"tldr": "SENTINEL\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u53e5\u5b50\u751f\u6210\u65e9\u671f\u8fdb\u884c\u5e72\u9884\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\uff0c\u901a\u8fc7\u751f\u6210\u548c\u9a8c\u8bc1\u504f\u597d\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u504f\u597d\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8de8\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u4e0e\u89c6\u89c9\u8f93\u5165\u76f8\u77db\u76fe\u7684\u865a\u6784\u5185\u5bb9\uff08\u5e7b\u89c9\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u73b0\u6709\u7684\u7f13\u89e3\u5e7b\u89c9\u7684\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8981\u4e48\u5728\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u5f15\u5165\u5206\u5e03\u4e0d\u5339\u914d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3MLLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "SENTINEL\u6846\u67b6\uff0c\u5305\u62ec\u5f15\u5bfc\u9ad8\u8d28\u91cf\u7684\u9886\u57df\u5185\u504f\u597d\u5bf9\uff08\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u6a21\u578b\u8f93\u51fa\u6765\u9a8c\u8bc1\u5bf9\u8c61\u5b58\u5728\uff0c\u5e76\u4ea4\u53c9\u68c0\u67e5\u4e24\u4e2a\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff0c\u5c06\u53e5\u5b50\u5206\u7c7b\u4e3a\u5e7b\u89c9/\u975e\u5e7b\u89c9\uff09\uff0c\u7136\u540e\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u504f\u597d\u6570\u636e\uff08\u4f7f\u7528\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u6b63\u6837\u672c\u548c\u5e7b\u89c9\u8d1f\u6837\u672c\uff09\uff0c\u6700\u540e\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u504f\u597d\u635f\u5931\uff08C-DPO\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u635f\u5931\u5728\u5e7b\u89c9\u6700\u521d\u51fa\u73b0\u7684\u53e5\u5b50\u7ea7\u522b\u4e0a\u5f3a\u8c03\u5224\u522b\u6027\u5b66\u4e60\u3002", "result": "SENTINEL\u53ef\u4ee5\u5c06\u5e7b\u89c9\u51cf\u5c1190%\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5728\u5e7b\u89c9\u57fa\u51c6\u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u4e0a\u90fd\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SENTINEL\u901a\u8fc7\u53e5\u5b50\u7ea7\u522b\u7684\u65e9\u671f\u5e72\u9884\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5e7b\u89c9\u51cf\u5c1190%\u4ee5\u4e0a\uff0c\u5e76\u5728\u5e7b\u89c9\u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12461", "abs": "https://arxiv.org/abs/2507.12461", "authors": ["Trong-Thang Pham", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis", "comment": "ACM MM 2025", "summary": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.", "AI": {"tldr": "\u63d0\u51fa RadGazeIntent\uff0c\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u91ca\u653e\u5c04\u79d1\u533b\u751f\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u6ce8\u89c6\u610f\u56fe\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cRadGazeIntent \u5728\u610f\u56fe\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u672a\u80fd\u6355\u6349\u6bcf\u6b21\u6ce8\u89c6\u80cc\u540e\u7684\u6f5c\u5728\u610f\u56fe\uff0c\u800c\u8bad\u7ec3\u6709\u7d20\u7684\u653e\u5c04\u79d1\u533b\u751f\u5728\u641c\u7d22\u65f6\u4f1a\u9075\u5faa\u4e00\u4e2a\u5fc3\u7406\u6e05\u5355\u6765\u5b9a\u4f4d\u75c5\u53d8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RadGazeIntent \u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e Transformer \u7684\u67b6\u6784\uff0c\u5904\u7406\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u7684\u65f6\u57df\u548c\u7a7a\u57df\uff0c\u5c06\u7ec6\u7c92\u5ea6\u7684\u6ce8\u89c6\u7279\u5f81\u8f6c\u5316\u4e3a\u7c97\u7c92\u5ea6\u7684\u3001\u6709\u610f\u4e49\u7684\u8bca\u65ad\u610f\u56fe\u8868\u793a\uff0c\u4ee5\u89e3\u91ca\u653e\u5c04\u79d1\u533b\u751f\u7684\u76ee\u6807\u3002", "result": "RadGazeIntent \u80fd\u591f\u9884\u6d4b\u653e\u5c04\u79d1\u533b\u751f\u5728\u7279\u5b9a\u65f6\u523b\u6b63\u5728\u68c0\u67e5\u7684\u53d1\u73b0\uff0c\u5e76\u4e14\u5728\u6240\u6709\u610f\u56fe\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RadGazeIntent \u80fd\u591f\u9884\u6d4b\u653e\u5c04\u79d1\u533b\u751f\u5728\u7279\u5b9a\u65f6\u523b\u6b63\u5728\u68c0\u67e5\u7684\u53d1\u73b0\uff0c\u5e76\u4e14\u5728\u6240\u6709\u610f\u56fe\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2507.12462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12462", "abs": "https://arxiv.org/abs/2507.12462", "authors": ["Yuxi Xiao", "Jianyuan Wang", "Nan Xue", "Nikita Karaev", "Yuri Makarov", "Bingyi Kang", "Xing Zhu", "Hujun Bao", "Yujun Shen", "Xiaowei Zhou"], "title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "comment": "International Conference on Computer Vision, ICCV 2025. Huggingface\n  Demo: https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2, Code:\n  https://github.com/henry123-boy/SpaTrackerV2", "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.", "AI": {"tldr": "SpatialTrackerV2 is a fast and accurate monocular 3D point tracking method that unifies geometry and motion estimation in an end-to-end architecture.", "motivation": "To develop a high-performing and feedforward 3D point tracker that unifies intrinsic connections between point tracking, monocular depth, and camera pose estimation, moving beyond modular pipelines.", "method": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that unifies point tracking, monocular depth, and camera pose estimation into a single, fully differentiable, end-to-end architecture. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, enabling scalable training across diverse datasets.", "result": "The method achieves a 30% improvement over existing 3D tracking methods and matches the accuracy of leading dynamic 3D reconstruction approaches while operating 50 times faster.", "conclusion": "SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50x faster."}}
{"id": "2507.12463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12463", "abs": "https://arxiv.org/abs/2507.12463", "authors": ["Renjie Li", "Ruijie Ye", "Mingyang Wu", "Hao Frank Yang", "Zhiwen Fan", "Hezhen Hu", "Zhengzhong Tu"], "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding", "comment": null, "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.", "AI": {"tldr": "MMHU\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u6ce8\u91ca\u548c\u591a\u6837\u7684\u8bc4\u4f30\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u5b89\u5168\u9a7e\u9a76\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u7406\u89e3\u4eba\u7c7b\u5728\u4ea4\u901a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u3002", "method": "MMHU\u5305\u542b57k\u4e2a\u4eba\u7c7b\u8fd0\u52a8\u7247\u6bb5\u548c1.73M\u5e27\uff0c\u6570\u636e\u6765\u6e90\u591a\u6837\uff0c\u5305\u62ecWaymo\u3001YouTube\u548c\u81ea\u884c\u6536\u96c6\u7684\u6570\u636e\u3002\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u8be6\u7ec6\u7684\u884c\u4e3a\u63cf\u8ff0\u3002", "result": "MMHU\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u80fd\u591f\u5bf9\u8fd0\u52a8\u9884\u6d4b\u3001\u8fd0\u52a8\u751f\u6210\u548c\u4eba\u7c7b\u884c\u4e3a\u95ee\u7b54\u7b49\u591a\u4e2a\u4efb\u52a1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "MMHU\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u3002\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4eba\u7c7b\u8fd0\u52a8\u3001\u8f68\u8ff9\u3001\u610f\u56fe\u548c\u9a7e\u9a76\u5b89\u5168\u76f8\u5173\u7684\u5173\u952e\u884c\u4e3a\u6807\u7b7e\u3002"}}
{"id": "2507.12464", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.12464", "abs": "https://arxiv.org/abs/2507.12464", "authors": ["Muhammed Furkan Dasdelen", "Hyesu Lim", "Michele Buck", "Katharina S. G\u00f6tze", "Carsten Marr", "Steffen Schneider"], "title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "comment": "11 pages, 5 figures", "summary": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.", "AI": {"tldr": "CytoSAE\u662f\u4e00\u79cd\u7528\u4e8e\u8840\u6db2\u5b66\u56fe\u50cf\u5206\u6790\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5b83\u80fd\u8bc6\u522b\u548c\u751f\u6210\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u7ec6\u80de\u6982\u5ff5\uff0c\u63d0\u4f9b\u4e9a\u7ec6\u80de\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728AML\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533b\u5b66\u6210\u50cf\u9886\u57df\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65e5\u76ca\u589e\u591a\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u8840\u6db2\u5b66\u9886\u57df\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCytoSAE\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u4f7f\u7528\u8d85\u8fc740,000\u5f20\u5916\u5468\u8840\u5355\u7ec6\u80de\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u5176\u5728\u9aa8\u9ad3\u7ec6\u80de\u5b66\u7b49\u57df\u5916\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc6\u522b\u6982\u5ff5\u7684\u80fd\u529b\u3002", "result": "CytoSAE\u80fd\u591f\u8bc6\u522b\u5f62\u6001\u5b66\u76f8\u5173\u7684\u7ec6\u80de\u6982\u5ff5\uff0c\u5e76\u80fd\u751f\u6210\u60a3\u8005\u548c\u75be\u75c5\u7279\u5f02\u6027\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc6\u522b\u75c5\u7076\u7ec6\u80de\u548c\u7ec6\u80de\u5f02\u5e38\u3002\u5728AML\u5b50\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cCytoSAE\u6982\u5ff5\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e9a\u7ec6\u80de\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CytoSAE\u5728\u7ec6\u80de\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5c55\u73b0\u4e86\u5176\u9002\u7528\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u3001\u751f\u6210\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u7ec6\u80de\u6982\u5ff5\uff0c\u5e76\u63d0\u4f9b\u4e9a\u7ec6\u80de\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728AML\u5b50\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12465", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12465", "abs": "https://arxiv.org/abs/2507.12465", "authors": ["Ziang Cao", "Zhaoxi Chen", "Linag Pan", "Ziwei Liu"], "title": "PhysX: Physical-Grounded 3D Asset Generation", "comment": "Project page: https://physx-3d.github.io/", "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "AI": {"tldr": "\u63d0\u51faPhysX\u6846\u67b6\uff0c\u5305\u542bPhysXNet\u6570\u636e\u96c6\u548cPhysXGen\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b\u7269\u7406\u5c5e\u6027\u76843D\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u751f\u6210\u6a21\u578b\u5ffd\u89c6\u7269\u7406\u5c5e\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76843D\u751f\u6210\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u548c\u7eb9\u7406\uff0c\u5ffd\u89c6\u4e86\u7269\u7406\u5c5e\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u76843D\u8d44\u6e90\u5728\u6a21\u62df\u548c\u5177\u8eab\u667a\u80fd\u7b49\u7269\u7406\u9886\u57df\u5e94\u7528\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhysX\u7684\u7aef\u5230\u7aef\u7269\u7406\u5c5e\u60273D\u8d44\u6e90\u751f\u6210\u8303\u5f0f\uff0c\u5305\u62ec\uff1a1\uff09PhysXNet\uff0c\u4e00\u4e2a\u5305\u542b\u7edd\u5bf9\u5c3a\u5ea6\u3001\u6750\u8d28\u3001\u53ef\u53ca\u6027\u3001\u8fd0\u52a8\u5b66\u548c\u529f\u80fd\u63cf\u8ff0\u4e94\u4e2a\u57fa\u7840\u7ef4\u5ea6\u7684\u7269\u7406\u5c5e\u60273D\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u3001\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u8f85\u52a9\u6807\u6ce8\u6d41\u7a0b\uff1b2\uff09PhysXGen\uff0c\u4e00\u4e2a\u524d\u9988\u7684\u3001\u7528\u4e8e\u7269\u7406\u5c5e\u6027\u56fe\u50cf\u52303D\u8d44\u6e90\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u660e\u786e\u5efa\u6a213D\u7ed3\u6784\u548c\u7269\u7406\u5c5e\u6027\u4e4b\u95f4\u7684\u6f5c\u5728\u76f8\u5173\u6027\uff0c\u5c06\u7269\u7406\u77e5\u8bc6\u6ce8\u5165\u9884\u8bad\u7ec3\u76843D\u7ed3\u6784\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PhysXGen\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PhysXGen\u5728\u4fdd\u7559\u539f\u751f\u51e0\u4f55\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u53ef\u4fe1\u7269\u7406\u9884\u6d4b\u76843D\u8d44\u6e90\u3002"}}
