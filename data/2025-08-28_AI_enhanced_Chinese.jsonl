{"id": "2508.19739", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19739", "abs": "https://arxiv.org/abs/2508.19739", "authors": ["Sebastian Lotter", "Marco Seiter", "Maryam Pirmoradi", "Lukas Brand", "Dagmar Fischer", "Robert Schober"], "title": "MC for Gastroretentive Drug Delivery", "comment": "4 pages, 2 figures, This paper has been submitted to IEEE\n  Transactions on Molecular, Biological, and Multi-Scale Communications as\n  Transactions Letter", "summary": "Recently, bacterial nanocellulose (BNC), a biological material produced by\nnon-pathogenic bacteria that possesses excellent material properties for\nvarious medical applications, has received increased interest as a carrier\nsystem for drug delivery. However, the vast majority of existing studies on\ndrug release from BNC are feasibility studies with modeling and design aspects\nremaining largely unexplored. To narrow this research gap, this paper proposes\na novel model for the drug release from BNC. Specifically, the drug delivery\nsystem considered in this paper consists of a BNC fleece coated with a polymer.\nThe polymer coating is used as an additional diffusion barrier, enabling the\ncontrolled release of an active pharmaceutical ingredient. The proposed\nphysics-based model reflects the geometry of the BNC and incorporates the\nimpact of the polymer coating on the drug release. Hence, it can be useful for\ndesigning BNC-based drug delivery systems in the future. The accuracy of the\nmodel is validated with experimental data obtained in wet lab experiments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ec6\u83cc\u7eb3\u7c73\u7ea4\u7ef4\u7d20\uff08BNC\uff09\u836f\u7269\u9012\u9001\u7cfb\u7edf\u7684\u7269\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86\u805a\u5408\u7269\u6d82\u5c42\u7684\u6269\u6563\u963b\u788d\u4f5c\u7528\uff0c\u4ee5\u5b9e\u73b0\u836f\u7269\u7684\u63a7\u91ca\u3002\u6a21\u578b\u5df2\u901a\u8fc7\u6e7f\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u53ef\u7528\u4e8e\u672a\u6765BNC\u836f\u7269\u9012\u9001\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u5173\u4e8eBNC\u836f\u7269\u91ca\u653e\u7684\u7814\u7a76\u591a\u4e3a\u53ef\u884c\u6027\u7814\u7a76\uff0c\u5728\u5efa\u6a21\u548c\u8bbe\u8ba1\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b66\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u53cd\u6620BNC\u7684\u51e0\u4f55\u7ed3\u6784\u5e76\u8003\u8651\u805a\u5408\u7269\u6d82\u5c42\u5bf9\u836f\u7269\u91ca\u653e\u7684\u5f71\u54cd\uff0c\u805a\u5408\u7269\u6d82\u5c42\u4f5c\u4e3a\u989d\u5916\u7684\u6269\u6563\u5c4f\u969c\uff0c\u7528\u4e8e\u63a7\u5236\u6d3b\u6027\u836f\u7269\u6210\u5206\u7684\u91ca\u653e\u3002", "result": "\u901a\u8fc7\u6e7f\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u57fa\u4e8eBNC\u7684\u836f\u7269\u9012\u9001\u7cfb\u7edf\u3002"}}
{"id": "2508.19905", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19905", "abs": "https://arxiv.org/abs/2508.19905", "authors": ["Imad Ali Shah", "Jiarong Li", "Roshan George", "Tim Brophy", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities", "comment": "Submitted and under review at IEEE OJVT, August 2025", "summary": "Hyperspectral imaging (HSI) offers a transformative sensing modality for\nAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)\napplications, enabling material-level scene understanding through fine spectral\nresolution beyond the capabilities of traditional RGB imaging. This paper\npresents the first comprehensive review of HSI for automotive applications,\nexamining the strengths, limitations, and suitability of current HSI\ntechnologies in the context of ADAS/AD. In addition to this qualitative review,\nwe analyze 216 commercially available HSI and multispectral imaging cameras,\nbenchmarking them against key automotive criteria: frame rate, spatial\nresolution, spectral dimensionality, and compliance with AEC-Q100 temperature\nstandards. Our analysis reveals a significant gap between HSI's demonstrated\nresearch potential and its commercial readiness. Only four cameras meet the\ndefined performance thresholds, and none comply with AEC-Q100 requirements. In\naddition, the paper reviews recent HSI datasets and applications, including\nsemantic segmentation for road surface classification, pedestrian separability,\nand adverse weather perception. Our review shows that current HSI datasets are\nlimited in terms of scale, spectral consistency, the number of spectral\nchannels, and environmental diversity, posing challenges for the development of\nperception algorithms and the adequate validation of HSI's true potential in\nADAS/AD applications. This review paper establishes the current state of HSI in\nautomotive contexts as of 2025 and outlines key research directions toward\npractical integration of spectral imaging in ADAS and autonomous systems.", "AI": {"tldr": "\u9ad8\u5149\u8c31\u6210\u50cf(HSI)\u5728\u6c7d\u8f66\u5e94\u7528\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u76ee\u524d\u5546\u4e1a\u5316\u51c6\u5907\u4e0d\u8db3\uff0c\u73b0\u6709\u6280\u672f\u548c\u6570\u636e\u96c6\u5728\u6c7d\u8f66\u7279\u5b9a\u8981\u6c42\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u9ad8\u5149\u8c31\u6210\u50cf(HSI)\u6280\u672f\u5728\u6c7d\u8f66\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf(ADAS)\u548c\u81ea\u52a8\u9a7e\u9a76(AD)\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u53ca\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u5176\u5b9e\u9645\u96c6\u6210\u63d0\u4f9b\u7814\u7a76\u65b9\u5411\u3002", "method": "\u5bf9216\u79cd\u5546\u7528\u9ad8\u5149\u8c31\u548c\u591a\u5149\u8c31\u6210\u50cf\u76f8\u673a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u5e27\u7387\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u5149\u8c31\u7ef4\u5ea6\u548cAEC-Q100\u6e29\u5ea6\u6807\u51c6\u7b26\u5408\u6027\u3002\u540c\u65f6\uff0c\u5bf9\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u9053\u8def\u8868\u9762\u5206\u7c7b\u3001\u884c\u4eba\u53ef\u5206\u6027\u548c\u6076\u52a3\u5929\u6c14\u611f\u77e5\u7b49\u65b9\u9762\u7684\u6700\u65b0\u6570\u636e\u96c6\u548c\u5e94\u7528\u8fdb\u884c\u56de\u987e\u3002", "result": "\u76ee\u524d\u4ec5\u67094\u6b3e\u76f8\u673a\u8fbe\u5230\u6027\u80fd\u9608\u503c\uff0c\u4e14\u65e0\u4e00\u6b3e\u7b26\u5408AEC-Q100\u8981\u6c42\u3002\u73b0\u6709\u9ad8\u5149\u8c31\u6210\u50cf\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u5149\u8c31\u4e00\u81f4\u6027\u3001\u901a\u9053\u6570\u91cf\u548c\u73af\u5883\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7ed9\u611f\u77e5\u7b97\u6cd5\u5f00\u53d1\u548c\u9ad8\u5149\u8c31\u6210\u50cf\u5728ADAS/AD\u5e94\u7528\u4e2d\u7684\u771f\u6b63\u6f5c\u529b\u9a8c\u8bc1\u5e26\u6765\u4e86\u6311\u6218\u3002", "conclusion": "\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u6c7d\u8f66\u9886\u57df\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u521d\u6b65\u9636\u6bb5\uff0c\u5546\u4e1a\u5316\u51c6\u5907\u4e0d\u8db3\uff0c\u6570\u636e\u96c6\u548c\u6280\u672f\u5747\u9700\u6539\u8fdb\uff0c\u624d\u80fd\u6ee1\u8db3ADAS/AD\u7684\u4e25\u82db\u8981\u6c42\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u91cd\u4e8e\u514b\u670d\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u96c6\u6210\u3002"}}
{"id": "2508.20016", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20016", "abs": "https://arxiv.org/abs/2508.20016", "authors": ["Matthias Maiterth", "Wesley H. Brewer", "Jaya S. Kuruvella", "Arunavo Dey", "Tanzima Z. Islam", "Kevin Menear", "Dmitry Duplyakin", "Rashadul Kabir", "Tapasya Patki", "Terry Jones", "Feiyi Wang"], "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling", "comment": null, "summary": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8c03\u5ea6\u4e0e\u6570\u5b57\u5b6a\u751f\u76f8\u7ed3\u5408\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u90e8\u7f72\u524d\u6216\u8fdb\u884c\u96be\u4ee5\u5b9e\u73b0\u7684\u66f4\u6539\u4e4b\u524d\uff0c\u5bf9\u8c03\u5ea6\u53c2\u6570\u548c\u51b3\u7b56\u5bf9\u7269\u7406\u8d44\u4ea7\u7684\u5f71\u54cd\u8fdb\u884c\u5efa\u6a21\u548c\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30HPC\u8c03\u5ea6\u5668\u901a\u5e38\u53d7\u9650\u4e8e\u90e8\u7f72\u540e\u5206\u6790\u6216\u4e0d\u5305\u542b\u57fa\u7840\u8bbe\u65bd\u7684\u6a21\u62df\u5668\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u8c03\u5ea6\u80fd\u529b\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u5e76\u6574\u5408\u4e86\u516c\u5171\u6570\u636e\u96c6\u548c\u5916\u90e8\u8c03\u5ea6\u6a21\u62df\u5668\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u7528\u4e8e\u8bc4\u4f30\u6fc0\u52b1\u7ed3\u6784\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8c03\u5ea6\u7684\u6269\u5c55\u3002", "result": "\u6210\u529f\u5730\u96c6\u6210\u4e86\u8c03\u5ea6\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u4e3aHPC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u201c\u5047\u8bbe\u201d\u5206\u6790\uff0c\u8bc4\u4f30\u53ef\u6301\u7eed\u6027\u548c\u5bf9\u6a21\u62df\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aHPC\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u8bc4\u4f30\u548c\u539f\u578b\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u4e86\u90e8\u7f72\u524d\u7684\u201c\u5047\u8bbe\u201d\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.19596", "categories": ["quant-ph", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.19596", "abs": "https://arxiv.org/abs/2508.19596", "authors": ["Xi Huang", "Dong An"], "title": "Fourier transform-based linear combination of Hamiltonian simulation", "comment": "18 pages, 3 figures", "summary": "Linear combination of Hamiltonian simulation (LCHS) connects the general\nlinear non-unitary dynamics with unitary operators and serves as the\nmathematical backbone of designing near-optimal quantum linear differential\nequation algorithms. However, the existing LCHS formalism needs to find a\nkernel function subject to complicated technical conditions on a half complex\nplane. In this work, we establish an alternative formalism of LCHS based on the\nFourier transform. Our new formalism completely removes the technical\nrequirements beyond the real axis, providing a simple and flexible way of\nconstructing LCHS kernel functions. Specifically, we construct a different\nfamily of the LCHS kernel function, providing a $1.81$ times reduction in the\nquantum differential equation algorithms based on LCHS, and an $8.27$ times\nreduction in its quantum circuit depth at a truncation error of $\\epsilon \\le\n10^{-8}$. Additionally, we extend the scope of the LCHS formula to the scenario\nof simulating linear unstable dynamics for a short or intermediate time period.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u7ebf\u6027\u54c8\u5bc6\u987f\u91cf\u6a21\u62df\uff08LCHS\uff09\u65b0\u5f62\u5f0f\uff0c\u7b80\u5316\u4e86\u73b0\u6709LCHS\u5f62\u5f0f\u7684\u590d\u6742\u6280\u672f\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7075\u6d3b\u7684LCHS\u6838\u51fd\u6570\u6784\u9020\u65b9\u6cd5\u3002\u65b0\u7684\u6838\u51fd\u6570\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5b50\u5fae\u5206\u65b9\u7a0b\u7b97\u6cd5\u7684\u8d44\u6e90\u6d88\u8017\uff08\u95e8\u6570\u91cf\u548c\u91cf\u5b50\u6bd4\u7279\u6df1\u5ea6\uff09\uff0c\u5e76\u9996\u6b21\u5c06LCHS\u63a8\u5e7f\u5230\u6a21\u62df\u7ebf\u6027\u4e0d\u786e\u5b9a\u52a8\u529b\u5b66\u3002", "motivation": "\u73b0\u6709LCHS\u5f62\u5f0f\u9700\u8981\u5bfb\u627e\u6ee1\u8db3\u590d\u6742\u6280\u672f\u6761\u4ef6\u7684\u6838\u51fd\u6570\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u7075\u6d3b\u7684LCHS\u5f62\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684LCHS\u65b0\u5f62\u5f0f\uff0c\u5e76\u6784\u9020\u4e86\u4e00\u65cf\u65b0\u7684LCHS\u6838\u51fd\u6570\u3002", "result": "\u65b0\u7684\u6838\u51fd\u6570\u5728\u91cf\u5b50\u5fae\u5206\u65b9\u7a0b\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e861.81\u500d\u7684\u95e8\u6570\u91cf\u51cf\u5c11\u548c8.27\u500d\u7684\u91cf\u5b50\u6bd4\u7279\u6df1\u5ea6\u51cf\u5c11\uff08\u8bef\u5dee$\rm{\\epsilon}\\le 10^{-8}$\uff09\u3002\u6b64\u5916\uff0c\u5c06LCHS\u7684\u9002\u7528\u8303\u56f4\u6269\u5c55\u5230\u6a21\u62df\u7ebf\u6027\u4e0d\u786e\u5b9a\u52a8\u529b\u5b66\u3002", "conclusion": "\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684LCHS\u65b0\u5f62\u5f0f\u7b80\u5316\u4e86\u6838\u51fd\u6570\u7684\u6784\u9020\uff0c\u964d\u4f4e\u4e86\u91cf\u5b50\u7b97\u6cd5\u7684\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u6269\u5c55\u4e86LCHS\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2508.19672", "categories": ["cs.LG", "cs.IT", "cs.NA", "math.IT", "math.NA", "33F05, 41A20, 41A25, 26C15"], "pdf": "https://arxiv.org/pdf/2508.19672", "abs": "https://arxiv.org/abs/2508.19672", "authors": ["Erion Morina", "Martin Holler"], "title": "$\\mathcal{C}^1$-approximation with rational functions and rational neural networks", "comment": null, "summary": "We show that suitably regular functions can be approximated in the\n$\\mathcal{C}^1$-norm both with rational functions and rational neural networks,\nincluding approximation rates with respect to width and depth of the network,\nand degree of the rational functions. As consequence of our results, we further\nobtain $\\mathcal{C}^1$-approximation results for rational neural networks with\nthe $\\text{EQL}^\\div$ and ParFam architecture, both of which are important in\nparticular in the context of symbolic regression for physical law learning.", "AI": {"tldr": "\u53ef\u4ee5\u4f7f\u7528\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u5728C^1\u8303\u6570\u4e0b\u903c\u8fd1\u6b63\u5219\u51fd\u6570\uff0c\u5e76\u7ed9\u51fa\u4e86\u903c\u8fd1\u7387\u3002", "motivation": "\u63a2\u8ba8\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u5728C^1\u8303\u6570\u4e0b\u903c\u8fd1\u6b63\u5219\u51fd\u6570\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5206\u6790\u903c\u8fd1\u7387\u3002", "method": "\u4f7f\u7528\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u903c\u8fd1\u3002", "result": "\u8bc1\u660e\u4e86\u5728C^1\u8303\u6570\u4e0b\uff0c\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u903c\u8fd1\u6b63\u5219\u51fd\u6570\uff0c\u5e76\u5f97\u5230\u4e86\u903c\u8fd1\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6709\u7406\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecEQL^div\u548cParFam\u67b6\u6784\uff09\u5728\u7269\u7406\u5b9a\u5f8b\u5b66\u4e60\u7b49\u7b26\u53f7\u56de\u5f52\u9886\u57df\u63d0\u4f9b\u4e86C^1\u903c\u8fd1\u7ed3\u679c\u3002"}}
{"id": "2508.19373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19373", "abs": "https://arxiv.org/abs/2508.19373", "authors": ["Haoran Lin", "Xianzhi Yu", "Kang Zhao", "Han Bao", "Zongyuan Zhan", "Ting Hu", "Wulong Liu", "Zekun Yin", "Xin Li", "Weiguo Liu"], "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "comment": null, "summary": "Current inference systems for Mixture-of-Experts (MoE) models primarily\nemploy static parallelization strategies. However, these static approaches\ncannot consistently achieve optimal performance across different inference\nscenarios, as they lack the flexibility to adapt to varying computational\nrequirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a\nnovel method that dynamically selects hybrid parallel strategies to enhance MoE\ninference efficiency. The fundamental innovation of HAP lies in hierarchically\ndecomposing MoE architectures into two distinct computational modules: the\nAttention module and the Expert module, each augmented with a specialized\ninference latency simulation model. This decomposition promotes the\nconstruction of a comprehensive search space for seeking model parallel\nstrategies. By leveraging Integer Linear Programming (ILP), HAP could solve the\noptimal hybrid parallel configurations to maximize inference efficiency under\nvarying computational constraints. Our experiments demonstrate that HAP\nconsistently determines parallel configurations that achieve comparable or\nsuperior performance to the TP strategy prevalent in mainstream inference\nsystems. Compared to the TP-based inference, HAP-based inference achieves\nspeedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,\nrespectively. Furthermore, HAP showcases remarkable generalization capability,\nmaintaining performance effectiveness across diverse MoE model configurations,\nincluding Mixtral and Qwen series models.", "AI": {"tldr": "HAP\u662f\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u81ea\u9002\u5e94\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5e76\u884c\u7b56\u7565\u6765\u63d0\u9ad8MoE\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u5e76\u80fd\u5728\u4e0d\u540cGPU\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684MoE\u63a8\u7406\u7cfb\u7edf\u4e3b\u8981\u4f7f\u7528\u9759\u6001\u5e76\u884c\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7684\u63a8\u7406\u573a\u666f\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "HAP\u5c06MoE\u6a21\u578b\u5206\u89e3\u4e3aAttention\u6a21\u5757\u548cExpert\u6a21\u5757\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6a21\u5757\u914d\u5907\u4e13\u95e8\u7684\u63a8\u7406\u5ef6\u8fdf\u6a21\u62df\u6a21\u578b\u3002\u5229\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u6765\u5bfb\u627e\u6700\u4f18\u7684\u6df7\u5408\u5e76\u884c\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728A100\u3001A6000\u548cV100 GPU\u5e73\u53f0\u4e0a\uff0cHAP\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684TP\u7b56\u7565\u5206\u522b\u5b9e\u73b0\u4e861.68\u500d\u30011.77\u500d\u548c1.57\u500d\u7684\u52a0\u901f\u3002HAP\u5728Mixtral\u548cQwen\u7cfb\u5217\u7b49\u4e0d\u540cMoE\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "HAP\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6df7\u5408\u81ea\u9002\u5e94\u5e76\u884c\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347MoE\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u514b\u670d\u4e86\u9759\u6001\u5e76\u884c\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19737", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19737", "abs": "https://arxiv.org/abs/2508.19737", "authors": ["Meng Qin", "Weihua Li", "Jinqiang Cui", "Sen Pei"], "title": "InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections", "comment": null, "summary": "Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides nodes of a graph into densely-connected blocks. From a perspective\nof graph signal processing, we find that graph Laplacian with a negative\ncorrection can derive graph frequencies beyond the conventional range $[0, 2]$.\nTo explore whether the low-frequency information beyond this range can encode\nmore informative properties about community structures, we propose InfraredGP.\nIt (\\romannumeral1) adopts a spectral GNN as its backbone combined with\nlow-pass filters and a negative correction mechanism, (\\romannumeral2) only\nfeeds random inputs to this backbone, (\\romannumeral3) derives graph embeddings\nvia one feed-forward propagation (FFP) without any training, and\n(\\romannumeral4) obtains feasible GP results by feeding the derived embeddings\nto BIRCH. Surprisingly, our experiments demonstrate that based solely on the\nnegative correction mechanism that amplifies low-frequency information beyond\n$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard\nclustering modules (e.g., BIRCH) and obtain high-quality results for GP without\nany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate\nInfraredGP for both static and streaming GP, where InfraredGP can achieve much\nbetter efficiency (e.g., 16x-23x faster) and competitive quality over various\nbaselines. We have made our code public at\nhttps://github.com/KuroginQin/InfraredGP", "AI": {"tldr": "InfraredGP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u5212\u5206\uff08GP\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u62c9\u666e\u62c9\u65af\u8d1f\u6821\u6b63\u6765\u63a2\u7d22\u8d85\u51fa\u5e38\u89c4\u8303\u56f4[0, 2]\u7684\u56fe\u9891\u7387\uff0c\u4ee5\u6355\u6349\u793e\u533a\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u63a2\u7d22\u8d85\u51fa\u5e38\u89c4\u9891\u7387\u8303\u56f4[0, 2]\u7684\u4f4e\u9891\u4fe1\u606f\u662f\u5426\u80fd\u7f16\u7801\u793e\u533a\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u5212\u5206\u65b9\u6cd5\u3002", "method": "InfraredGP\u91c7\u7528\u57fa\u4e8e\u8c31\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u4f4e\u901a\u6ee4\u6ce2\u5668\u548c\u8d1f\u6821\u6b63\u673a\u5236\uff0c\u4ec5\u8f93\u5165\u968f\u673a\u4fe1\u53f7\uff0c\u901a\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u64ad\uff08FFP\uff09\u83b7\u5f97\u56fe\u5d4c\u5165\uff0c\u6700\u540e\u5c06\u5d4c\u5165\u8f93\u5165BIRCH\u7b97\u6cd5\u5f97\u5230GP\u7ed3\u679c\u3002", "result": "InfraredGP\u4ec5\u901a\u8fc7\u8d1f\u6821\u6b63\u673a\u5236\u5c31\u80fd\u83b7\u5f97\u53ef\u533a\u5206\u7684\u5d4c\u5165\uff0c\u5e76\u4e3aGP\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u3002\u5728\u9759\u6001\u548c\u6d41\u5f0fGP\u4efb\u52a1\u4e2d\uff0cInfraredGP\u7684\u6548\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad816-23\u500d\uff0c\u540c\u65f6\u8d28\u91cf\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "InfraredGP\u901a\u8fc7\u5229\u7528\u56fe\u62c9\u666e\u62c9\u65af\u8d1f\u6821\u6b63\u6765\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u56fe\u9891\u7387\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u5730\u5b8c\u6210\u56fe\u5212\u5206\u4efb\u52a1\uff0c\u5e76\u4e14\u65e0\u9700\u8bad\u7ec3\uff0c\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "LLMs\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u5177\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faMultiPL-MoE\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408token\u548csegment\u5c42\u9762\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\uff08MultiPL\uff09\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u4fdd\u6301\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMultiPL-MoE\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86token\u548csegment\u4e24\u4e2a\u5c42\u9762\u7684MoE\u3002token-level MoE\u91c7\u7528\u5171\u4eab\u4e13\u5bb6\u548c\u95e8\u6743\u91cd\u5f52\u4e00\u5316\u65b9\u6cd5\uff1bsegment-level MoE\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5212\u5206\u8f93\u5165\u5e8f\u5217\uff0c\u5e76\u91c7\u7528\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u7b56\u7565\uff0c\u5141\u8bb8\u4e13\u5bb6\u9009\u62e9top-k\u4e2a\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86MultiPL-MoE\u7684\u6709\u6548\u6027\u3002", "conclusion": "MultiPL-MoE\u6a21\u578b\u901a\u8fc7\u5728token\u548csegment\u5c42\u9762\u4f18\u5316\u4e13\u5bb6\u9009\u62e9\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19504", "categories": ["cs.MA", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19504", "abs": "https://arxiv.org/abs/2508.19504", "authors": ["Kevin Song", "Anand Jayarajan", "Yaoyao Ding", "Qidong Su", "Zhanda Zhu", "Sihang Liu", "Gennady Pekhimenko"], "title": "Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) agents augmented with domain tools promise to\nautonomously execute complex tasks requiring human-level intelligence, such as\ncustomer service and digital assistance. However, their practical deployment is\noften limited by their low success rates under complex real-world environments.\nTo tackle this, prior research has primarily focused on improving the agents\nthemselves, such as developing strong agentic LLMs, while overlooking the role\nof the system environment in which the agent operates.\n  In this paper, we study a complementary direction: improving agent success\nrates by optimizing the system environment in which the agent operates. We\ncollect 142 agent traces (3,656 turns of agent-environment interactions) across\n5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we\npropose a taxonomy for agent-environment interaction failures that includes 6\nfailure modes. Guided by these findings, we design Aegis, a set of targeted\nenvironment optimizations: 1) environment observability enhancement, 2) common\ncomputation offloading, and 3) speculative agentic actions. These techniques\nimprove agent success rates on average by 6.7-12.5%, without any modifications\nto the agent and underlying LLM.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u7387\u4f4e\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u8fd0\u884c\u7684\u73af\u5883\u6765\u63d0\u9ad8\u5176\u6210\u529f\u7387\uff0c\u8bbe\u8ba1\u4e86Aegis\u7cfb\u7edf\uff0c\u5305\u542b\u73af\u5883\u53ef\u89c2\u5bdf\u6027\u589e\u5f3a\u3001\u901a\u7528\u8ba1\u7b97\u5378\u8f7d\u548c\u63a8\u6d4b\u6027\u4ee3\u7406\u52a8\u4f5c\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7406\u548c\u5e95\u5c42LLM\u5373\u53ef\u63d0\u9ad8\u4ee3\u7406\u6210\u529f\u73876.7-12.5%\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u6210\u529f\u7387\u4f4e\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6539\u8fdb\u4ee3\u7406\u672c\u8eab\uff0c\u800c\u5ffd\u89c6\u4e86\u4ee3\u7406\u6240\u5904\u7684\u7cfb\u7edf\u73af\u5883\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u4f18\u5316\u73af\u5883\u6765\u63d0\u9ad8\u4ee3\u7406\u6210\u529f\u7387\u3002", "method": "\u6536\u96c6\u4e86142\u4e2a\u4ee3\u7406\u8f68\u8ff9\uff083,656\u4e2a\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u56de\u5408\uff09\uff0c\u5206\u6790\u4e865\u4e2a\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4ee3\u7406\u5931\u8d25\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b6\u79cd\u5931\u8d25\u6a21\u5f0f\u7684\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u5931\u8d25\u5206\u7c7b\u6cd5\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86Aegis\u7cfb\u7edf\uff0c\u5305\u542b\u73af\u5883\u53ef\u89c2\u5bdf\u6027\u589e\u5f3a\u3001\u901a\u7528\u8ba1\u7b97\u5378\u8f7d\u548c\u63a8\u6d4b\u6027\u4ee3\u7406\u52a8\u4f5c\u3002", "result": "Aegis\u7cfb\u7edf\u5c06\u4ee3\u7406\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad8\u4e866.7-12.5%\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u4ee3\u7406\u548c\u5e95\u5c42LLM\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316LLM\u4ee3\u7406\u6240\u5904\u7684\u7cfb\u7edf\u73af\u5883\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\uff0cAegis\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u5957\u6709\u6548\u7684\u73af\u5883\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2508.19403", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19403", "abs": "https://arxiv.org/abs/2508.19403", "authors": ["Moritz C. Schmidt", "Agustin O. Alvarez", "Riccardo Pallotta", "Biruk A. Seid", "Jeroen J. de Boer", "Jarla Thiesbrummel", "Felix Lang", "Giulia Grancini", "Bruno Ehrler"], "title": "Quantification of mobile ions in perovskite solar cells with thermally activated ion current measurements", "comment": null, "summary": "Mobile ions play a key role in the degradation of perovskite solar cells,\nmaking their quantification essential for enhancing device stability. Various\nelectrical measurements have been applied to characterize mobile ions. However,\ndiscerning between different ionic migration processes can be difficult.\nFurthermore, multiple measurements at different temperatures are usually\nrequired to probe different ions and their activation energies. Here, we\ndemonstrate a new characterization technique based on measuring the thermally\nactivated ion current (TAIC) of perovskite solar cells. The method reveals\ndensity, diffusion coefficient, and activation energy of mobile ions within a\nsingle temperature sweep and offers an intuitive way to distinguish mobile ion\nspecies. We apply the TAIC technique to quantify mobile ions of MAPbI3 and\ntriple-cation perovskite solar cells. We find a higher activation energy and a\nlower diffusion coefficient in the triple-cation devices. TAIC measurements are\na simple yet powerful tool to better understand ion migration in perovskite\nsolar cells.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u70ed\u6fc0\u6d3b\u79bb\u5b50\u7535\u6d41\uff08TAIC\uff09\u7684\u65b0\u578b\u8868\u5f81\u6280\u672f\uff0c\u7528\u4e8e\u91cf\u5316\u548c\u533a\u5206\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u4e2d\u7684\u79fb\u52a8\u79bb\u5b50\u3002\u8be5\u6280\u672f\u4ec5\u9700\u4e00\u6b21\u6e29\u5ea6\u626b\u63cf\u5373\u53ef\u786e\u5b9a\u79bb\u5b50\u7684\u5bc6\u5ea6\u3001\u6269\u6563\u7cfb\u6570\u548c\u6fc0\u6d3b\u80fd\uff0c\u5e76\u80fd\u76f4\u89c2\u5730\u533a\u5206\u4e0d\u540c\u7684\u79bb\u5b50\u79cd\u7c7b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0eMAPbI3\u76f8\u6bd4\uff0c\u4e09\u9633\u79bb\u5b50\u9499\u949b\u77ff\u5668\u4ef6\u4e2d\u7684\u79fb\u52a8\u79bb\u5b50\u7684\u6fc0\u6d3b\u80fd\u66f4\u9ad8\uff0c\u6269\u6563\u7cfb\u6570\u66f4\u4f4e\u3002TAIC\u6280\u672f\u4e3a\u7406\u89e3\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u4e2d\u7684\u79bb\u5b50\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5668\u4ef6\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u7684\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u5bf9\u5f71\u54cd\u5176\u964d\u89e3\u7684\u79fb\u52a8\u79bb\u5b50\u8fdb\u884c\u91cf\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7535\u5b66\u6d4b\u91cf\u65b9\u6cd5\u5728\u533a\u5206\u4e0d\u540c\u7684\u79bb\u5b50\u8fc1\u79fb\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u591a\u6b21\u5728\u4e0d\u540c\u6e29\u5ea6\u4e0b\u8fdb\u884c\u6d4b\u91cf\u624d\u80fd\u8868\u5f81\u4e0d\u540c\u7684\u79bb\u5b50\u53ca\u5176\u6fc0\u6d3b\u80fd\u3002", "method": "\u63d0\u51fa\u5e76\u5e94\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u91cf\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u70ed\u6fc0\u6d3b\u79bb\u5b50\u7535\u6d41\uff08TAIC\uff09\u7684\u65b0\u578b\u8868\u5f81\u6280\u672f\u3002\u8be5\u6280\u672f\u901a\u8fc7\u4e00\u6b21\u6e29\u5ea6\u626b\u63cf\u6765\u6d4b\u91cf\u79bb\u5b50\u7684\u70ed\u6fc0\u6d3b\u7535\u6d41\uff0c\u4ece\u800c\u83b7\u5f97\u79bb\u5b50\u7684\u5bc6\u5ea6\u3001\u6269\u6563\u7cfb\u6570\u548c\u6fc0\u6d3b\u80fd\uff0c\u5e76\u80fd\u76f4\u89c2\u5730\u533a\u5206\u4e0d\u540c\u7684\u79fb\u52a8\u79bb\u5b50\u79cd\u7c7b\u3002", "result": "\u7814\u7a76\u5c06TAIC\u6280\u672f\u5e94\u7528\u4e8eMAPbI3\u548c\u4e09\u9633\u79bb\u5b50\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\uff0c\u53d1\u73b0\u4e09\u9633\u79bb\u5b50\u5668\u4ef6\u4e2d\u7684\u79fb\u52a8\u79bb\u5b50\u5177\u6709\u66f4\u9ad8\u7684\u6fc0\u6d3b\u80fd\u548c\u66f4\u4f4e\u7684\u6269\u6563\u7cfb\u6570\u3002", "conclusion": "TAIC\u6d4b\u91cf\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u4e2d\u7684\u79bb\u5b50\u8fc1\u79fb\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5668\u4ef6\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.19299", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.19299", "abs": "https://arxiv.org/abs/2508.19299", "authors": ["Rishov Sarkar", "Cong Hao"], "title": "OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs", "comment": "13 pages, 8 figures. Accepted at MICRO 2025", "summary": "High-Level Synthesis (HLS) is increasingly popular for hardware design using\nC/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware\nbehavior in a sequential language like C/C++, HLS tools introduce constructs\nsuch as infinite loops and dataflow modules connected by FIFOs. However,\nefficiently and accurately simulating these constructs at C level remains\nchallenging. First, without hardware timing information, functional\nverification typically requires slow RTL synthesis and simulation, as the\ncurrent approaches in commercial HLS tools. Second, cycle-accurate performance\nmetrics, such as end-to-end latency, also rely on RTL simulation. No existing\nHLS tool fully overcomes the first limitation. For the second, prior work such\nas LightningSim partially improves simulation speed but lacks support for\nadvanced dataflow features like cyclic dependencies and non-blocking FIFO\naccesses.\n  To overcome both limitations, we propose OmniSim, a framework that\nsignificantly extends the simulation capabilities of both academic and\ncommercial HLS tools. First, OmniSim enables fast and accurate simulation of\ncomplex dataflow designs, especially those explicitly declared unsupported by\ncommercial tools. It does so through sophisticated software multi-threading,\nwhere threads are orchestrated by querying and updating a set of FIFO tables\nthat explicitly record exact hardware timing of each FIFO access. Second,\nOmniSim achieves near-C simulation speed with near-RTL accuracy for both\nfunctionality and performance, via flexibly coupled and overlapped\nfunctionality and performance simulations.\n  We demonstrate that OmniSim successfully simulates eleven designs previously\nunsupported by any HLS tool, achieving up to 35.9x speedup over traditional\nC/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less\ncapable simulator, LightningSim, on its own benchmark suite.", "AI": {"tldr": "OmniSim\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u53ef\u4ee5\u52a0\u901f\u548c\u7cbe\u786e\u5730\u6a21\u62dfC/C++ HLS\u8bbe\u8ba1\uff0c\u652f\u6301\u4ee5\u524d\u4e0d\u652f\u6301\u7684\u590d\u6742\u6570\u636e\u6d41\u7279\u6027\uff0c\u5e76\u63d0\u4f9b\u8fd1\u4e4eC\u7684\u901f\u5ea6\u548c\u8fd1\u4e4eRTL\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684HLS\u5de5\u5177\u96be\u4ee5\u5728C\u7ea7\u522b\u4e0a\u9ad8\u6548\u51c6\u786e\u5730\u6a21\u62df\u5e76\u53d1\u786c\u4ef6\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5faa\u73af\u4f9d\u8d56\u548c\u975e\u963b\u585eFIFO\u8bbf\u95ee\u7b49\u9ad8\u7ea7\u6570\u636e\u6d41\u7279\u6027\uff0c\u5bfc\u81f4\u529f\u80fd\u9a8c\u8bc1\u4f9d\u8d56\u4e8e\u7f13\u6162\u7684RTL\u4eff\u771f\uff0c\u5e76\u4e14\u65e0\u6cd5\u83b7\u5f97\u5468\u671f\u7cbe\u786e\u7684\u6027\u80fd\u6307\u6807\u3002", "method": "OmniSim\u901a\u8fc7\u8f6f\u4ef6\u591a\u7ebf\u7a0b\u548cFIFO\u8868\u6765\u5b9e\u73b0\u5bf9\u590d\u6742\u6570\u636e\u6d41\u8bbe\u8ba1\u7684\u5feb\u901f\u51c6\u786e\u6a21\u62df\uff0c\u8fd9\u4e9b\u8868\u8bb0\u5f55\u4e86\u6bcf\u6b21FIFO\u8bbf\u95ee\u7684\u786e\u5207\u786c\u4ef6\u65f6\u5e8f\u3002\u5b83\u8fd8\u901a\u8fc7\u7075\u6d3b\u8026\u5408\u548c\u91cd\u53e0\u7684\u529f\u80fd\u4e0e\u6027\u80fd\u4eff\u771f\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1C\u7684\u4eff\u771f\u901f\u5ea6\u548c\u63a5\u8fd1RTL\u7684\u51c6\u786e\u6027\u3002", "result": "OmniSim\u6210\u529f\u6a21\u62df\u4e8611\u4e2a\u4ee5\u524d\u4e0d\u53d7\u4efb\u4f55HLS\u5de5\u5177\u652f\u6301\u7684\u8bbe\u8ba1\uff0c\u4e0e\u4f20\u7edf\u7684C/RTL\u8054\u5408\u4eff\u771f\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e8635.9\u500d\uff0c\u4e0e\u6700\u5148\u8fdb\u4f46\u529f\u80fd\u8f83\u5c11\u7684\u6a21\u62df\u5668LightningSim\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e866.61\u500d\u3002", "conclusion": "OmniSim\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8HLS\u8bbe\u8ba1\u7684\u6a21\u62df\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u590d\u6742\u6570\u636e\u6d41\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.19473", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19473", "abs": "https://arxiv.org/abs/2508.19473", "authors": ["Stephen Arndt", "Benjamin Moseley", "Kirk Pruhs", "Michael Zlatin"], "title": "Efficiently Coloring the Intersection of a General Matroid and Partition Matroids", "comment": null, "summary": "This paper shows a polynomial-time algorithm, that given a general matroid\n$M_1 = (X, \\mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \\ldots, M_k$,\nproduces a coloring of the intersection $M = \\cap_{i=1}^k M_i$ using at most\n$1+\\sum_{i=1}^k \\left(\\chi(M_i) -1\\right)$ colors. This is the first\npolynomial-time $O(1)$-approximation algorithm for matroid intersection\ncoloring where one of the matroids may be a general matroid. Leveraging the\nfact that all of the standard combinatorial matroids reduce to partition\nmatroids at a loss of a factor of two in the chromatic number, this algorithm\nalso yields a polynomial-time $O(1)$-approximation algorithm for matroid\nintersection coloring in the case where each of the matroids $ M_2, \\ldots,\nM_k$ are one of the standard combinatorial types.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u53ef\u4e3a\u4e00\u822c\u62df\u9635 $M_1 = (X, \\mathcal{I}_1)$ \u548c $k-1$ \u4e2a\u5212\u5206\u62df\u9635 $M_2, \\ldots, M_k$ \u7684\u4ea4\u96c6 $M = \bigcap_{i=1}^k M_i$ \u4ea7\u751f\u4e00\u79cd\u67d3\u8272\uff0c\u4f7f\u7528\u7684\u989c\u8272\u6570\u4e0d\u8d85\u8fc7 $1 + \\sum_{i=1}^k (\\chi(M_i) - 1)$\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u62df\u9635\u4ea4\u96c6\u7740\u8272\u95ee\u9898\u7684\u591a\u9879\u5f0f\u65f6\u95f4 $O(1)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u4e2d\u4e00\u4e2a\u62df\u9635\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u822c\u62df\u9635\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u6240\u6709\u6807\u51c6\u7684\u7ec4\u5408\u62df\u9635\u90fd\u53ef\u4ee5\u5f52\u7ea6\u5230\u5212\u5206\u62df\u9635\uff08\u6b64\u65f6\u67d3\u8272\u6570\u4f1a\u635f\u5931\u4e00\u500d\uff09\u8fd9\u4e00\u4e8b\u5b9e\uff0c\u4e5f\u4e3a\u6bcf\u4e2a\u62df\u9635 $M_2, \\ldots, M_k$ \u5747\u4e3a\u6807\u51c6\u7ec4\u5408\u7c7b\u578b\u7684\u62df\u9635\u4ea4\u96c6\u7740\u8272\u95ee\u9898\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u65f6\u95f4 $O(1)$-\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4e00\u822c\u62df\u9635\u4e0e\u5212\u5206\u62df\u9635\u4ea4\u96c6\u7684\u7740\u8272\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4 $O(1)$-\u8fd1\u4f3c\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u5229\u7528\u62df\u9635\u5f52\u7ea6\u5230\u5212\u5206\u62df\u9635\u7684\u6027\u8d28\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u7528\u4e8e\u4e00\u822c\u62df\u9635\u4e0e $k-1$ \u4e2a\u5212\u5206\u62df\u9635\u4ea4\u96c6\u7740\u8272\u7684\u591a\u9879\u5f0f\u65f6\u95f4 $O(1)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4f7f\u7528\u7684\u989c\u8272\u6570\u4e0d\u8d85\u8fc7 $1 + \\sum_{i=1}^k (\\chi(M_i) - 1)$\u3002\u540c\u65f6\uff0c\u8be5\u7b97\u6cd5\u4e5f\u9002\u7528\u4e8e $k-1$ \u4e2a\u62df\u9635\u5747\u4e3a\u6807\u51c6\u7ec4\u5408\u7c7b\u578b\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u62df\u9635\u4ea4\u96c6\u7740\u8272\u95ee\u9898\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e00\u822c\u62df\u9635\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.19390", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19390", "abs": "https://arxiv.org/abs/2508.19390", "authors": ["Jana Weber", "Marcel Weber", "Juan Miguel Lopez Alcaraz"], "title": "Depression diagnosis from patient interviews using multimodal machine learning", "comment": "15 pages, 4 figures, source code under\n  https://github.com/UOLMDA2025/Depression", "summary": "Background: Depression is a major public health concern, affecting an\nestimated five percent of the global population. Early and accurate diagnosis\nis essential to initiate effective treatment, yet recognition remains\nchallenging in many clinical contexts. Speech, language, and behavioral cues\ncollected during patient interviews may provide objective markers that support\nclinical assessment.\n  Methods: We developed a diagnostic approach that integrates features derived\nfrom patient interviews, including speech patterns, linguistic characteristics,\nand structured clinical information. Separate models were trained for each\nmodality and subsequently combined through multimodal fusion to reflect the\ncomplexity of real-world psychiatric assessment. Model validity was assessed\nwith established performance metrics, and further evaluated using calibration\nand decision-analytic approaches to estimate potential clinical utility.\n  Results: The multimodal model achieved superior diagnostic accuracy compared\nto single-modality models, with an AUROC of 0.88 and an F1-score of 0.75.\nImportantly, the fused model demonstrated good calibration and offered higher\nnet clinical benefit compared to baseline strategies, highlighting its\npotential to assist clinicians in identifying patients with depression more\nreliably.\n  Conclusion: Multimodal analysis of patient interviews using machine learning\nmay serve as a valuable adjunct to psychiatric evaluation. By combining speech,\nlanguage, and clinical features, this approach provides a robust framework that\ncould enhance early detection of depressive disorders and support\nevidence-based decision-making in mental healthcare.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u4e34\u5e8a\u4fe1\u606f\u7684\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u8f85\u52a9\u8bca\u65ad\u6291\u90c1\u75c7\uff0c\u5e76\u5728\u6291\u90c1\u75c7\u7b5b\u67e5\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6821\u51c6\u548c\u66f4\u9ad8\u7684\u4e34\u5e8a\u51c0\u6548\u76ca\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u4e00\u79cd\u666e\u904d\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bc6\u522b\u4ecd\u5177\u6311\u6218\u6027\u3002\u60a3\u8005\u8bbf\u8c08\u4e2d\u7684\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u884c\u4e3a\u7ebf\u7d22\u53ef\u4ee5\u4f5c\u4e3a\u5ba2\u89c2\u6807\u8bb0\u6765\u652f\u6301\u4e34\u5e8a\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6574\u5408\u6765\u81ea\u60a3\u8005\u8bbf\u8c08\u7684\u7279\u5f81\uff08\u5305\u62ec\u8bed\u97f3\u6a21\u5f0f\u3001\u8bed\u8a00\u7279\u5f81\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u4fe1\u606f\uff09\u7684\u8bca\u65ad\u65b9\u6cd5\u3002\u4e3a\u6bcf\u79cd\u6a21\u5f0f\u5206\u522b\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u8fdb\u884c\u7ec4\u5408\uff0c\u4ee5\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7cbe\u795e\u79d1\u8bc4\u4f30\u7684\u590d\u6742\u6027\u3002\u4f7f\u7528\u6027\u80fd\u6307\u6807\u3001\u6821\u51c6\u548c\u51b3\u7b56\u5206\u6790\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\u6709\u6548\u6027\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u53d6\u5f97\u4e86\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u6a21\u578b\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0cAUROC\u4e3a0.88\uff0cF1\u5206\u6570\u4e3a0.75\u3002\u6b64\u5916\uff0c\u878d\u5408\u6a21\u578b\u663e\u793a\u51fa\u826f\u597d\u7684\u6821\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bd4\u57fa\u7ebf\u7b56\u7565\u66f4\u9ad8\u7684\u4e34\u5e8a\u51c0\u6548\u76ca\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u6291\u90c1\u75c7\u60a3\u8005\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5bf9\u60a3\u8005\u8bbf\u8c08\u8fdb\u884c\u591a\u6a21\u6001\u5206\u6790\u53ef\u4ee5\u4f5c\u4e3a\u7cbe\u795e\u79d1\u8bc4\u4f30\u7684\u5b9d\u8d35\u8f85\u52a9\u624b\u6bb5\u3002\u901a\u8fc7\u7ed3\u5408\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u4e34\u5e8a\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u52a0\u5f3a\u5bf9\u6291\u90c1\u75c7\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u5e76\u652f\u6301\u5fc3\u7406\u5065\u5eb7\u51b3\u7b56\u7684\u5faa\u8bc1\u51b3\u7b56\u3002"}}
{"id": "2508.19255", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19255", "abs": "https://arxiv.org/abs/2508.19255", "authors": ["Hong-Yu Zou", "Bing-Bing Wang", "Yong Ge", "Ke-Qi Zhao", "Yu-Qi Chen", "Hong-Xiang Sun", "Shou-Qi Yuan", "Haoran Xue", "Baile Zhang"], "title": "Non-Hermitian edge burst of sound", "comment": null, "summary": "Non-Hermitian band topology can give rise to phenomena with no counterparts\nin Hermitian systems. A well-known example is the non-Hermitian skin effect\n(NHSE), where Bloch eigenstates localize at a boundary, induced by a nontrivial\nspectrum winding number. In contrast, recent studies on lossy non-Hermitian\nlattices have uncovered an unexpected boundary-localized loss probability-a\nphenomenon that requires not only non-Hermitian band topology but also the\nclosure of the imaginary (dissipative) gap. Here, we demonstrate the\nnon-Hermitian edge burst in a classical-wave metamaterial: a lossy\nnonreciprocal acoustic crystal. We show that, when the imaginary gap remains\nclosed, edge bursts can occur at the right boundary, left boundary, or both\nboundaries simultaneously, all under the same non-Hermitian band topology; the\nlatter scenario is known as a bipolar edge burst. The occurrence of each\nscenario depends on the number and location of the imaginary gap closure points\nin the eigenenergy spectra. These findings generalize the concept of edge burst\nfrom quantum to classical wave systems, establish it as an intrinsic material\nproperty, and enrich the physics of the complex interplay between non-Hermitian\nband topology and other physical properties in non-Hermitian systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u5728\u635f\u8017\u975e\u5384\u7c73\u58f0\u5b50\u6676\u4f53\u4e2d\u51fa\u73b0\u7684\u975e\u5384\u7c73\u8fb9\u754c\u7206\u53d1\u73b0\u8c61\uff0c\u8be5\u73b0\u8c61\u662f\u7ecf\u5178\u6ce2\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u53d1\u73b0\uff0c\u5e76\u4e14\u4e0e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u8fb9\u754c\u7206\u53d1\u7c7b\u4f3c\uff0c\u662f\u6750\u6599\u7684\u5185\u5728\u5c5e\u6027\u3002", "motivation": "\u7814\u7a76\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u65b0\u73b0\u8c61\uff0c\u7279\u522b\u662f\u4e0e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u975e\u5384\u7c73\u76ae\u80a4\u6548\u5e94\uff08NHSE\uff09\u4e0d\u540c\u7684\u73b0\u8c61\uff0c\u5e76\u5c06\u5176\u4ece\u91cf\u5b50\u9886\u57df\u63a8\u5e7f\u5230\u7ecf\u5178\u6ce2\u7cfb\u7edf\u3002", "method": "\u5728\u635f\u8017\u975e\u5384\u7c73\u58f0\u5b50\u6676\u4f53\uff08\u4e00\u79cd\u7ecf\u5178\u7684\u635f\u8017\u975e\u5384\u7c73\u8d85\u6750\u6599\uff09\u4e2d\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u7814\u7a76\u975e\u5384\u7c73\u8fb9\u754c\u7206\u53d1\u73b0\u8c61\u3002\u91cd\u70b9\u5173\u6ce8\u865a\u90e8\uff08\u8017\u6563\uff09\u80fd\u9699\u7684\u95ed\u5408\u60c5\u51b5\uff0c\u4ee5\u53ca\u5b83\u5982\u4f55\u5f71\u54cd\u8fb9\u754c\u4e0a\u80fd\u91cf\u5c40\u57df\u5316\u548c\u8017\u6563\u3002", "result": "\u8bc1\u660e\u4e86\u975e\u5384\u7c73\u8fb9\u754c\u7206\u53d1\u73b0\u8c61\u53ef\u4ee5\u5728\u635f\u8017\u975e\u5384\u7c73\u58f0\u5b50\u6676\u4f53\u4e2d\u5b9e\u73b0\uff0c\u5e76\u4e14\u6839\u636e\u865a\u90e8\u80fd\u9699\u95ed\u5408\u70b9\u7684\u6570\u91cf\u548c\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53d1\u751f\u5728\u53f3\u8fb9\u754c\u3001\u5de6\u8fb9\u754c\u6216\u540c\u65f6\u53d1\u751f\u5728\u4e24\u4e2a\u8fb9\u754c\u7684\u8fb9\u754c\u7206\u53d1\uff0c\u540e\u8005\u88ab\u79f0\u4e3a\u53cc\u6781\u8fb9\u754c\u7206\u53d1\u3002", "conclusion": "\u975e\u5384\u7c73\u8fb9\u754c\u7206\u53d1\u662f\u7ecf\u5178\u6ce2\u7cfb\u7edf\u4e2d\u7684\u4e00\u79cd\u73b0\u8c61\uff0c\u7c7b\u4f3c\u4e8e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u7c7b\u4f3c\u6548\u5e94\u3002\u5b83\u7684\u51fa\u73b0\u4e0e\u865a\u90e8\u80fd\u9699\u7684\u95ed\u5408\u6709\u5173\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6750\u6599\u7684\u5185\u5728\u5c5e\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u4e30\u5bcc\u4e86\u975e\u5384\u7c73\u7cfb\u7edf\u5728\u975e\u5384\u7c73\u5e26\u62d3\u6251\u4e0e\u5176\u4ed6\u7269\u7406\u6027\u8d28\u4e4b\u95f4\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u7684\u7269\u7406\u5b66\u7406\u89e3\u3002"}}
{"id": "2508.19276", "categories": ["quant-ph", "cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19276", "abs": "https://arxiv.org/abs/2508.19276", "authors": ["Marcos Guillermo Lammers", "Federico Hern\u00e1n Holik", "Alejandro Fern\u00e1ndez"], "title": "Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework", "comment": null, "summary": "Quantum computers represent a radical technological advancement in the way\ninformation is processed by using the principles of quantum mechanics to solve\nvery complex problems that exceed the capabilities of classical systems.\nHowever, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),\nthe available hardware presents several limitations, such as a limited number\nof qubits, high error rates, and reduced coherence times. Efficient management\nof quantum resources, both physical (qubits, error rates, connectivity) and\nlogical (quantum gates, algorithms, error correction), becomes particularly\nrelevant in the design and deployment of quantum algorithms. In this work, we\nanalyze the role of resources in the various uses of NISQ devices today,\nidentifying their relevance and implications for software engineering focused\non the use of quantum computers. We propose a vision for runtime-aware quantum\nsoftware development, identifying key challenges to its realization, such as\nlimited introspection capabilities and temporal constraints in current\nplatforms. As a proof of concept, we introduce Qonscious, a prototype framework\nthat enables conditional execution of quantum programs based on dynamic\nresource evaluation. With this contribution, we aim to strengthen the field of\nQuantum Resource Estimation (QRE) and move towards the development of scalable,\nreliable, and resource-aware quantum software.", "AI": {"tldr": "NISQ\u8bbe\u5907\u8d44\u6e90\u7ba1\u7406\u5bf9\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u8fd0\u884c\u65f6\u611f\u77e5\u65b9\u6cd5\u5e76\u7528Qonscious\u539f\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u5206\u6790NISQ\u8bbe\u5907\u786c\u4ef6\u548c\u903b\u8f91\u8d44\u6e90\u7684\u9650\u5236\uff0c\u4ee5\u53ca\u5b83\u4eec\u5bf9\u91cf\u5b50\u7b97\u6cd5\u8bbe\u8ba1\u548c\u90e8\u7f72\u7684\u91cd\u8981\u6027\u3002", "method": "\u5206\u6790NISQ\u8bbe\u5907\u8d44\u6e90\u7684\u89d2\u8272\u548c\u5f71\u54cd\uff0c\u63d0\u51fa\u8fd0\u884c\u65f6\u611f\u77e5\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1Qonscious\u539f\u578b\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u8fd0\u884c\u65f6\u611f\u77e5\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u613f\u666f\uff0c\u8bc6\u522b\u5173\u952e\u6311\u6218\uff08\u5982\u5185\u7701\u80fd\u529b\u6709\u9650\u3001\u65f6\u95f4\u7ea6\u675f\uff09\uff0c\u5e76\u7528Qonscious\u539f\u578b\u8bc1\u660e\u4e86\u57fa\u4e8e\u52a8\u6001\u8d44\u6e90\u8bc4\u4f30\u7684\u6761\u4ef6\u6267\u884c\u3002", "conclusion": "Qonscious\u539f\u578b\u65e8\u5728\u52a0\u5f3a\u91cf\u5b50\u8d44\u6e90\u4f30\u8ba1\uff08QRE\uff09\u9886\u57df\uff0c\u5e76\u63a8\u52a8\u53ef\u6269\u5c55\u3001\u53ef\u9760\u3001\u8d44\u6e90\u611f\u77e5\u7684\u91cf\u5b50\u8f6f\u4ef6\u53d1\u5c55\u3002"}}
{"id": "2508.19371", "categories": ["cs.GT", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19371", "abs": "https://arxiv.org/abs/2508.19371", "authors": ["Semih Kara", "Tamer Ba\u015far"], "title": "Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)", "comment": null, "summary": "Fictitious play (FP) is a well-studied algorithm that enables agents to learn\nNash equilibrium in games with certain reward structures. However, when agents\nhave no prior knowledge of the reward functions, FP faces a major challenge:\nthe joint action space grows exponentially with the number of agents, which\nslows down reward exploration. Anonymous games offer a structure that mitigates\nthis issue. In these games, the rewards depend only on the actions taken; not\non who is taking which action. Under such a structure, we introduce aggregate\nfictitious play (agg-FP), a variant of FP where each agent tracks the frequency\nof the number of other agents playing each action, rather than these agents'\nindividual actions. We show that in anonymous polymatrix games, agg-FP\nconverges to a Nash equilibrium under the same conditions as classical FP. In\nessence, by aggregating the agents' actions, we reduce the action space without\nlosing the convergence guarantees. Using simulations, we provide empirical\nevidence on how this reduction accelerates convergence.", "AI": {"tldr": "\u5728\u533fonym\u6e38\u620f\u4e2d\uff0c\u805a\u5408\u6027\u535a\u5f08\uff08agg-FP\uff09\u901a\u8fc7\u805a\u5408\u5176\u4ed6\u73a9\u5bb6\u7684\u884c\u4e3a\u6765\u51cf\u5c11\u72b6\u6001\u7a7a\u95f4\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u5411\u7eb3\u4ec0\u5747\u8861\u7684\u6536\u655b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u73a9\u5bb6\u4e0d\u4e86\u89e3\u5956\u52b1\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u535a\u5f08\u4e2d\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u968f\u73a9\u5bb6\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\u5bfc\u81f4\u5956\u52b1\u63a2\u7d22\u7f13\u6162\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u805a\u5408\u6027\u535a\u5f08\uff08agg-FP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u535a\u5f08\u65b9\u6cd5\uff0c\u5176\u4e2d\u6bcf\u4e2a\u73a9\u5bb6\u8ddf\u8e2a\u5176\u4ed6\u73a9\u5bb6\u6267\u884c\u6bcf\u4e2a\u52a8\u4f5c\u7684\u9891\u7387\uff0c\u800c\u4e0d\u662f\u8ddf\u8e2a\u73a9\u5bb6\u7684\u4e2a\u4f53\u52a8\u4f5c\u3002", "result": "\u805a\u5408\u6027\u535a\u5f08\uff08agg-FP\uff09\u5728\u533fonym\u77e9\u9635\u535a\u5f08\u4e2d\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u6761\u4ef6\u4e0e\u7ecf\u5178\u535a\u5f08\u76f8\u540c\uff0c\u5e76\u4e14\u901a\u8fc7\u805a\u5408\u52a8\u4f5c\u7a7a\u95f4\u800c\u65e0\u9700\u635f\u5931\u6536\u655b\u4fdd\u8bc1\u3002\u4eff\u771f\u7ed3\u679c\u4e5f\u8868\u660e\u805a\u5408\u6027\u535a\u5f08\uff08agg-FP\uff09\u53ef\u4ee5\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u805a\u5408\u6027\u535a\u5f08\uff08agg-FP\uff09\u901a\u8fc7\u805a\u5408\u73a9\u5bb6\u7684\u52a8\u4f5c\uff0c\u5728\u4e0d\u635f\u5931\u6536\u655b\u4fdd\u8bc1\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u4f20\u7edf\u535a\u5f08\u5728\u63a2\u7d22\u5956\u52b1\u65f6\u9047\u5230\u7684\u6311\u6218\u3002"}}
{"id": "2508.19254", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u751f\u6210\u5f0f\u7ed8\u56fe\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5c06\u5f62\u5f0f\u610f\u56fe\uff08\u8349\u56fe\u7684\u7ed3\u6784\u3001\u6784\u56fe\u548c\u98ce\u683c\u5c5e\u6027\uff09\u548c\u60c5\u5883\u610f\u56fe\uff08\u4ece\u5176\u89c6\u89c9\u5185\u5bb9\u4e2d\u63a8\u65ad\u51fa\u7684\u8bed\u4e49\u548c\u4e3b\u9898\u542b\u4e49\uff09\u6574\u5408\u5230\u7edf\u4e00\u7684\u53d8\u6362\u8fc7\u7a0b\u4e2d\u3002\u4e0e\u4e3b\u8981\u6355\u83b7\u9ad8\u7ea7\u60c5\u5883\u63cf\u8ff0\u7684\u4f20\u7edf\u6587\u672c\u63d0\u793a\u5f0f\u751f\u6210\u7cfb\u7edf\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u540c\u65f6\u5206\u6790\u4e86\u5730\u9762\u76f4\u89c2\u51e0\u4f55\u7279\u5f81\uff08\u5982\u7ebf\u6761\u8f68\u8ff9\u3001\u6bd4\u4f8b\u548c\u7a7a\u95f4\u6392\u5217\uff09\u548c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u9ad8\u7ea7\u8bed\u4e49\u7ebf\u7d22\u3002\u8fd9\u4e24\u79cd\u610f\u56fe\u4fe1\u53f7\u5728\u591a\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\u4e2d\u88ab\u8054\u5408\u8c03\u8282\uff0c\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u4fdd\u62a4\u8f6e\u5ed3\u7684\u7ed3\u6784\u63a7\u5236\u4e0e\u611f\u77e5\u98ce\u683c\u548c\u5185\u5bb9\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u63d0\u793a\u5f0f\u751f\u6210\u7cfb\u7edf\u4e3b\u8981\u4fa7\u91cd\u4e8e\u9ad8\u5c42\u7ea7\u63cf\u8ff0\uff0c\u800c\u5ffd\u7565\u4e86\u8349\u56fe\u7684\u5e95\u5c42\u51e0\u4f55\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u540c\u65f6\u7406\u89e3\u5e76\u878d\u5408\u5f62\u5f0f\u610f\u56fe\uff08\u7ed3\u6784\u3001\u6784\u56fe\u3001\u98ce\u683c\uff09\u548c\u60c5\u5883\u610f\u56fe\uff08\u8bed\u4e49\u3001\u4e3b\u9898\uff09\u7684\u751f\u6210\u5f0f\u7ed8\u56fe\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u76f4\u89c2\u3001\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u521b\u4f5c\u3002", "method": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u5206\u6790\u8349\u56fe\u7684\u51e0\u4f55\u7279\u5f81\uff08\u5982\u7ebf\u6761\u8f68\u8ff9\u3001\u6bd4\u4f8b\u3001\u7a7a\u95f4\u6392\u5217\uff09\u548c\u8bed\u4e49\u7ebf\u7d22\uff08\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\uff09\uff0c\u5e76\u5c06\u8fd9\u4e24\u79cd\u610f\u56fe\u4fe1\u53f7\u8054\u5408\u5e94\u7528\u4e8e\u4e00\u4e2a\u591a\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\u3002\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u4fdd\u6301\u8f6e\u5ed3\u7684\u7ed3\u6784\u63a7\u5236\u548c\u611f\u77e5\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u56fe\u50cf\u5408\u6210\u3002\u7cfb\u7edf\u91c7\u7528\u89e6\u6478\u5c4f\u754c\u9762\u548c\u5206\u5e03\u5f0f\u63a8\u7406\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e24\u9636\u6bb5\u53d8\u6362\uff0c\u5e76\u652f\u6301\u591a\u7528\u6237\u534f\u540c\u521b\u4f5c\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u4e24\u9636\u6bb5\u7684\u53d8\u6362\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5f62\u5f0f\u548c\u60c5\u5883\u610f\u56fe\uff0c\u652f\u6301\u591a\u7528\u6237\u5728\u5171\u4eab\u753b\u5e03\u4e0a\u8fdb\u884c\u534f\u540c\u521b\u4f5c\uff0c\u4f7f\u5f97\u4e0d\u540c\u827a\u672f\u80cc\u666f\u7684\u7528\u6237\u90fd\u80fd\u53c2\u4e0e\u5230\u540c\u6b65\u7684\u3001\u5171\u540c\u521b\u4f5c\u7684\u89c6\u89c9\u6d3b\u52a8\u4e2d\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5b9e\u73b0\u4e86\u4eba\u4e0e\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u5171\u521b\u548c\u76f8\u4e92\u4fc3\u8fdb\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4e3a\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8fdb\u884c\u540c\u6b65\u89c6\u89c9\u521b\u4f5c\u7684\u5e73\u53f0\uff0c\u8fd9\u91cd\u65b0\u5b9a\u4e49\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u65b9\u5f0f\u3002"}}
{"id": "2508.19345", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19345", "abs": "https://arxiv.org/abs/2508.19345", "authors": ["Mihitha Maithripala", "Zongli Lin"], "title": "Privacy-Preserving Distributed Control for a Networked Battery Energy Storage System", "comment": null, "summary": "The increasing deployment of distributed Battery Energy Storage Systems\n(BESSs) in modern power grids necessitates effective coordination strategies to\nensure state-of-charge (SoC) balancing and accurate power delivery. While\ndistributed control frameworks offer scalability and resilience, they also\nraise significant privacy concerns due to the need for inter-agent information\nexchange. This paper presents a novel privacy-preserving distributed control\nalgorithm for SoC balancing in a networked BESS. The proposed framework\nincludes distributed power allocation law that is designed based on two\nprivacy-preserving distributed estimators, one for the average unit state and\nthe other for the average desired power. The average unit state estimator is\ndesigned via the state decomposition method without disclosing sensitive\ninternal states. The proposed power allocation law based on these estimators\nensures asymptotic SoC balancing and global power delivery while safeguarding\nagent privacy from external eavesdroppers. The effectiveness and\nprivacy-preserving properties of the proposed control strategy are demonstrated\nthrough simulation results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u4e2d\u5b9e\u73b0\u8377\u7535\u72b6\u6001\uff08SoC\uff09\u5e73\u8861\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u5728\u7535\u7f51\u4e2d\u90e8\u7f72\u7684\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u534f\u8c03\u7b56\u7565\u6765\u5e73\u8861SoC\u548c\u8fdb\u884c\u7cbe\u786e\u7684\u529f\u7387\u8f93\u9001\u3002\u7136\u800c\uff0c\u5206\u5e03\u5f0f\u63a7\u5236\u6846\u67b6\u5728\u63d0\u4f9b\u53ef\u6269\u5c55\u6027\u548c\u5f39\u6027\u7684\u540c\u65f6\uff0c\u4e5f\u56e0\u9700\u8981\u4ee3\u7406\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\u800c\u5f15\u53d1\u4e86\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u63a7\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u7f51\u7edc\u5316BESS\u4e2d\u7684SoC\u5e73\u8861\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u4e24\u4e2a\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u4f30\u8ba1\u5668\u7684\u5206\u5e03\u5f0f\u529f\u7387\u5206\u914d\u5f8b\uff1a\u4e00\u4e2a\u7528\u4e8e\u5e73\u5747\u5355\u5143\u72b6\u6001\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u5e73\u5747\u671f\u671b\u529f\u7387\u3002\u5e73\u5747\u5355\u5143\u72b6\u6001\u4f30\u8ba1\u5668\u901a\u8fc7\u72b6\u6001\u5206\u89e3\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u65e0\u9700\u62ab\u9732\u654f\u611f\u7684\u5185\u90e8\u72b6\u6001\u3002", "result": "\u6240\u63d0\u51fa\u7684\u529f\u7387\u5206\u914d\u5f8b\u786e\u4fdd\u4e86\u6e10\u8fd1\u7684SoC\u5e73\u8861\u548c\u5168\u5c40\u529f\u7387\u8f93\u9001\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u4ee3\u7406\u9690\u79c1\u514d\u53d7\u5916\u90e8\u7a83\u542c\u8005\u4fb5\u5bb3\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u3002"}}
{"id": "2508.19548", "categories": ["cs.NE", "cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19548", "abs": "https://arxiv.org/abs/2508.19548", "authors": ["Madhuvanthi Srivatsav R", "Chiranjib Bhattacharyya", "Shantanu Chakrabartty", "Chetan Singh Thakur"], "title": "When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI", "comment": null, "summary": "Routing, switching, and the interconnect fabric are essential for large-scale\nneuromorphic computing. While this fabric only plays a supporting role in the\nprocess of computing, for large AI workloads it ultimately determines energy\nconsumption and speed. In this paper, we address this bottleneck by asking: (a)\nWhat computing paradigms are inherent in existing routing, switching, and\ninterconnect systems, and how can they be used to implement a\nprocessing-in-Interconnect (\\pi^2) computing paradigm? and (b) leveraging\ncurrent and future interconnect trends, how will a \\pi^2 system's performance\nscale compared to other neuromorphic architectures? For (a), we show that\noperations required for typical AI workloads can be mapped onto delays,\ncausality, time-outs, packet drop, and broadcast operations -- primitives\nalready implemented in packet-switching and packet-routing hardware. We show\nthat existing buffering and traffic-shaping embedded algorithms can be\nleveraged to implement neuron models and synaptic operations. Additionally, a\nknowledge-distillation framework can train and cross-map well-established\nneural network topologies onto $\\pi^2$ without degrading generalization\nperformance. For (b), analytical modeling shows that, unlike other neuromorphic\nplatforms, the energy scaling of $\\pi^2$ improves with interconnect bandwidth\nand energy efficiency. We predict that by leveraging trends in interconnect\ntechnology, a \\pi^2 architecture can be more easily scaled to execute\nbrain-scale AI inference workloads with power consumption levels in the range\nof hundreds of watts.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5904\u7406\u5185\u4e92\u8054\u201d\uff08\u03c0\u00b2\uff09\u7684\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u8303\u5f0f\uff0c\u5b83\u5229\u7528\u73b0\u6709\u7684\u8def\u7531\u3001\u4ea4\u6362\u548c\u4e92\u8054\u786c\u4ef6\u6765\u5b9e\u73b0\u8ba1\u7b97\uff0c\u4ece\u800c\u63d0\u9ad8\u80fd\u6548\u548c\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684\u4e92\u8054\u7ed3\u6784\uff08\u8def\u7531\u3001\u4ea4\u6362\u548c\u4e92\u8054\uff09\u662f\u80fd\u8017\u548c\u901f\u5ea6\u7684\u5173\u952e\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u5927\u578bAI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u73b0\u6709\u8def\u7531\u3001\u4ea4\u6362\u548c\u4e92\u8054\u7cfb\u7edf\u4e2d\u56fa\u6709\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u8303\u5f0f\uff08\u5982\u5ef6\u8fdf\u3001\u56e0\u679c\u5173\u7cfb\u3001\u8d85\u65f6\u3001\u4e22\u5305\u548c\u5e7f\u64ad\u64cd\u4f5c\uff09\u6765\u5b9e\u73b0\u5904\u7406\u5185\u4e92\u8054\uff08\u03c0\u00b2\uff09\u8ba1\u7b97\u8303\u5f0f\u3002\u63a5\u7740\uff0c\u7814\u7a76\u4eba\u5458\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u73b0\u6709\u7684\u7f13\u51b2\u548c\u6d41\u91cf\u6574\u5f62\u7b97\u6cd5\u6765\u5b9e\u73b0\u795e\u7ecf\u5143\u6a21\u578b\u548c\u7a81\u89e6\u64cd\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u6765\u8bad\u7ec3\u548c\u6620\u5c04\u795e\u7ecf\u7f51\u7edc\u62d3\u6251\u5230\u03c0\u00b2\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5206\u6790\u5efa\u6a21\uff0c\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u03c0\u00b2\u4e0e\u5176\u4ed6\u795e\u7ecf\u5f62\u6001\u67b6\u6784\u5728\u6027\u80fd\u548c\u80fd\u8017\u4e0a\u7684\u6269\u5c55\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u64cd\u4f5c\u53ef\u4ee5\u6620\u5c04\u5230\u73b0\u6709\u7684\u786c\u4ef6\u539f\u8bed\uff08\u5ef6\u8fdf\u3001\u56e0\u679c\u5173\u7cfb\u3001\u8d85\u65f6\u3001\u4e22\u5305\u3001\u5e7f\u64ad\uff09\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u5d4c\u5165\u5f0f\u7b97\u6cd5\u53ef\u7528\u4e8e\u5b9e\u73b0\u795e\u7ecf\u5143\u548c\u7a81\u89e6\u64cd\u4f5c\u3002\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u6cdb\u5316\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u62d3\u6251\u6620\u5c04\u5230\u03c0\u00b2\u3002\u5206\u6790\u5efa\u6a21\u663e\u793a\uff0c\u03c0\u00b2\u7684\u80fd\u8017\u6269\u5c55\u6027\u968f\u4e92\u8054\u5e26\u5bbd\u548c\u80fd\u6548\u7684\u589e\u52a0\u800c\u63d0\u9ad8\uff0c\u5e76\u9884\u6d4b\u03c0\u00b2\u67b6\u6784\u53ef\u4ee5\u6269\u5c55\u5230\u652f\u6301\u8111\u89c4\u6a21\u7684AI\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u529f\u8017\u5728\u51e0\u767e\u74e6\u8303\u56f4\u5185\u3002", "conclusion": "\u5904\u7406\u5185\u4e92\u8054\uff08\u03c0\u00b2\uff09\u8303\u5f0f\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7684\u7f51\u7edc\u786c\u4ef6\u548c\u6280\u672f\u8d8b\u52bf\uff0c\u4e3a\u89e3\u51b3\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684\u80fd\u8017\u548c\u901f\u5ea6\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u3001\u4f4e\u529f\u8017\u7684\u8111\u89c4\u6a21AI\u63a8\u7406\u3002"}}
{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "Sycophancy in LLMs is modeled as a composition of psychometric traits, rather than an isolated failure. CAA is used to map activations to these traits, allowing for interpretable interventions to mitigate sycophantic behavior.", "motivation": "The paper aims to model sycophancy in LLMs not as an isolated failure, but as a result of combined psychometric traits, similar to factor decomposition in psychometrics.", "method": "The study uses Contrastive Activation Addition (CAA) to map activation directions to psychometric factors (e.g., emotionality, openness, agreeableness). It then examines how combinations of these factors can lead to sycophancy and explores vector-based interventions like addition, subtraction, and projection for mitigation.", "result": "The proposed approach allows for interpretable and compositional vector-based interventions to mitigate safety-critical behaviors like sycophancy in LLMs by understanding it as a combination of psychometric traits.", "conclusion": "Sycophancy in LLMs can be understood and potentially mitigated by modeling it as a composition of underlying psychometric traits using methods like CAA, enabling targeted interventions."}}
{"id": "2508.19518", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19518", "abs": "https://arxiv.org/abs/2508.19518", "authors": ["Hail Song", "Seokhwan Yang", "Woontack Woo"], "title": "Fast Texture Transfer for XR Avatars via Barycentric UV Conversion", "comment": null, "summary": "We present a fast and efficient method for transferring facial textures onto\nSMPL-X-based full-body avatars. Unlike conventional affine-transform methods\nthat are slow and prone to visual artifacts, our method utilizes a barycentric\nUV conversion technique. Our approach precomputes the entire UV mapping into a\nsingle transformation matrix, enabling texture transfer in a single operation.\nThis results in a speedup of over 7000x compared to the baseline, while also\nsignificantly improving the final texture quality by eliminating boundary\nartifacts. Through quantitative and qualitative evaluations, we demonstrate\nthat our method offers a practical solution for personalization in immersive XR\napplications. The code is available online.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u9762\u90e8\u7eb9\u7406\u8f6c\u79fb\u5230\u57fa\u4e8eSMPL-X\u7684\u5168\u8eab\u5316\u8eab\u4e0a\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97UV\u6620\u5c04\u5230\u5355\u4e2a\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc77000\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c\u66f4\u4f18\u7684\u7eb9\u7406\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u4eff\u5c04\u53d8\u6362\u7684\u65b9\u6cd5\u5728\u8f6c\u79fb\u9762\u90e8\u7eb9\u7406\u5230SMPL-X\u5168\u8eab\u5316\u8eab\u4e0a\u65f6\u901f\u5ea6\u6162\u4e14\u5bb9\u6613\u51fa\u73b0\u89c6\u89c9\u7455\u75b5\uff0c\u9700\u8981\u66f4\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u91cd\u5fc3UV\u8f6c\u6362\u6280\u672f\u7684\u7eb9\u7406\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5c06\u6574\u4e2aUV\u6620\u5c04\u9884\u8ba1\u7b97\u4e3a\u5355\u4e2a\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u4e00\u6b21\u6027\u7eb9\u7406\u8f6c\u79fb\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u8d85\u8fc77000\u500d\uff0c\u540c\u65f6\u901a\u8fc7\u6d88\u9664\u8fb9\u754c\u4f2a\u5f71\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u7ec8\u7eb9\u7406\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c89\u6d78\u5f0fXR\u5e94\u7528\u4e2d\u7684\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.19367", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19367", "abs": "https://arxiv.org/abs/2508.19367", "authors": ["Alex Cuellar", "Ho Chit Siu", "Julie A Shah"], "title": "Inference of Human-derived Specifications of Object Placement via Demonstration", "comment": null, "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications.", "AI": {"tldr": "\u673a\u5668\u4eba\u6293\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u7c7b\u53ef\u63a5\u53d7\u7684\u7269\u4f53\u914d\u7f6e\u7684\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4f4d\u7f6e\u589e\u5f3a\u7684RCC\uff08PARCC\uff09\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6f14\u793a\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u7684\u7269\u4f53\u6392\u5217\u89c4\u5219\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u6293\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\u65b9\u6cd5\uff0c\u5728\u7406\u89e3\u4eba\u7c7b\u7269\u4f53\u914d\u7f6e\u7684\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u6355\u6349\u8fd9\u4e9b\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u8fde\u63a5\u6f14\u7b97\uff08RCC\uff09\u7684\u4f4d\u7f6e\u589e\u5f3aRCC\uff08PARCC\uff09\u5f62\u5f0f\u5316\u903b\u8f91\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u8fc7\u6f14\u793a\u5b66\u4e60PARCC\u89c4\u8303\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u6355\u6349\u4eba\u7c7b\u610f\u56fe\u7684\u89c4\u8303\uff0c\u5e76\u4e14\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u4eba\u7c7b\u63d0\u4f9b\u7684\u89c4\u8303\u3002", "conclusion": "PARCC\u6846\u67b6\u548c\u57fa\u4e8e\u6f14\u793a\u7684\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u4eba\u7c7b\u7684\u7269\u4f53\u6392\u5217\u89c4\u5219\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4f18\u4e8e\u76f4\u63a5\u63d0\u4f9b\u89c4\u8303\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.19551", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.19551", "abs": "https://arxiv.org/abs/2508.19551", "authors": ["Hong-Xue Jiang", "Jia-wan Li", "Shi-Bo Zhao", "Jie Wang", "Yusheng Hou"], "title": "Giant Anomalous Hall Conductivity and Gilbert Damping in Room-temperature Ferromagnetic Half-Heusler Alloys PtMnBi", "comment": "16 pages, 5 figures, accepted by Frontiers of Physics", "summary": "Half-Heusler alloys have emerged as promising candidates for novel spintronic\napplications due to their exceptional properties including the high Curie\ntemperature (TC) above room temperature and large anomalous Hall conductivity\n(AHC). In this work, we systematically study the magnetic and electronic\nproperties of PtMnBi in {\\alpha}-, \\{beta}-, and {\\gamma}-phase using\nfirst-principles calculations and Monte Carlo simulations. The three phases are\nfound to be ferromagnetic metals. In particular, the {\\alpha}-phase PtMnBi\nshows a high TC up to 802 K and a relatively large Gilbert damping of 0.085.\nAdditionally, the {\\gamma}-phase PtMnBi possesses a non-negligible AHC,\nreaching 203 {\\Omega}-1cm-1 at the Fermi level. To evaluate its potential in\nnanoscale devices, we further investigate the {\\alpha}-phase PtMnBi thin films.\nThe Gilbert dampings of {\\alpha}-phase PtMnBi thin films varies with film\nthickness and we attribute this variation to the distinct band structures at\nthe high-symmetry point {\\Gamma}, which arise from differences in film\nthickness. Moreover, the 1-layer (1L) {\\alpha}-phase thin film retains robust\nferromagnetism (TC = 688 K) and shows enhanced Gilbert damping (0.14) and AHC\n(1116 {\\Omega}-1cm-1) compared to the bulk. Intriguingly, under a 2% in-plane\nbiaxial compressive strain, the Gilbert damping of 1L {\\alpha}-phase PtMnBi\nthin film increases to 0.17 and the AHC reaches 2386 {\\Omega}-1cm-1. The\ncoexistence of giant Gilbert damping and large AHC makes {\\alpha}-phase PtMnBi\na compelling platform for practical spintronic applications, and highlights the\npotential of half-Heusler alloys in spintronic device design.", "AI": {"tldr": "PtMnBi\u5408\u91d1\uff08\u7279\u522b\u662f\u03b1\u76f8\uff09\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u9886\u57df\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5177\u6709\u9ad8\u5c45\u91cc\u6e29\u5ea6\u3001\u5927Gilbert\u963b\u5c3c\u548c\u5f02\u5e38\u970d\u5c14\u7535\u5bfc\u7387\uff0c\u7279\u522b\u662f\u5728\u8584\u819c\u548c\u5e94\u53d8\u6761\u4ef6\u4e0b\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u9ad8\u5c45\u91cc\u6e29\u5ea6\u548c\u9ad8\u5f02\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u7684\u6750\u6599\uff0c\u4ee5\u5e94\u7528\u4e8e\u65b0\u578b\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86PtMnBi\u5408\u91d1\uff08\u03b1\u3001\u03b2\u3001\u03b3\u76f8\uff09\u7684\u78c1\u6027\u548c\u7535\u5b50\u6027\u8d28\uff0c\u5305\u62ec\u8584\u819c\u548c\u5e94\u53d8\u6548\u5e94\u3002", "result": "\u03b1\u76f8PtMnBi\u5177\u6709\u9ad8\u8fbe802 K\u7684\u5c45\u91cc\u6e29\u5ea6\u548c0.085\u7684Gilbert\u963b\u5c3c\u3002\u03b3\u76f8PtMnBi\u5728\u8d39\u7c73\u80fd\u7ea7\u5904\u5177\u6709203 \u03a9\u207b\u00b9cm\u207b\u00b9\u7684\u5f02\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u3002\u03b1\u76f8PtMnBi\u8584\u819c\u7684Gilbert\u963b\u5c3c\u968f\u539a\u5ea6\u53d8\u5316\uff0c1L \u03b1\u76f8PtMnBi\u8584\u819c\u8868\u73b0\u51fa\u589e\u5f3a\u7684Gilbert\u963b\u5c3c\uff080.14\uff09\u548c\u5f02\u5e38\u970d\u5c14\u7535\u5bfc\u7387\uff081116 \u03a9\u207b\u00b9cm\u207b\u00b9\uff09\u30022%\u7684\u5e73\u9762\u53cc\u8f74\u538b\u7f29\u5e94\u53d8\u8fdb\u4e00\u6b65\u5c061L \u03b1\u76f8PtMnBi\u8584\u819c\u7684Gilbert\u963b\u5c3c\u63d0\u9ad8\u52300.17\uff0c\u5f02\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u63d0\u9ad8\u52302386 \u03a9\u207b\u00b9cm\u207b\u00b9\u3002", "conclusion": "\u03b1\u76f8PtMnBi\u56e0\u5176\u4f18\u5f02\u7684\u78c1\u6027\u548c\u7535\u5b66\u6027\u8d28\uff0c\u7279\u522b\u662f\u5728\u8584\u819c\u548c\u5e94\u53d8\u6761\u4ef6\u4e0b\uff0c\u662f\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u5e94\u7528\u7684\u7406\u60f3\u6750\u6599\u3002"}}
{"id": "2508.19888", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.19888", "abs": "https://arxiv.org/abs/2508.19888", "authors": ["Matthew Hague", "Artur Je\u017c", "Anthony W. Lin", "Oliver Markgraf", "Philipp R\u00fcmmer"], "title": "The Power of Regular Constraint Propagation (Technical Report)", "comment": null, "summary": "The past decade has witnessed substantial developments in string solving.\nMotivated by the complexity of string solving strategies adopted in existing\nstring solvers, we investigate a simple and generic method for solving string\nconstraints: regular constraint propagation. The method repeatedly computes\npre- or post-images of regular languages under the string functions present in\na string formula, inferring more and more knowledge about the possible values\nof string variables, until either a conflict is found or satisfiability of the\nstring formula can be concluded. Such a propagation strategy is applicable to\nstring constraints with multiple operations like concatenation, replace, and\nalmost all flavors of string transductions. We demonstrate the generality and\neffectiveness of this method theoretically and experimentally. On the\ntheoretical side, we show that RCP is sound and complete for a large fragment\nof string constraints, subsuming both straight-line and chain-free constraints,\ntwo of the most expressive decidable fragments for which some modern string\nsolvers provide formal completeness guarantees. On the practical side, we\nimplement regular constraint propagation within the open-source string solver\nOSTRICH.\n  Our experimental evaluation shows that this addition significantly improves\nOSTRICH's performance and makes it competitive with existing solvers. In fact,\nit substantially outperforms other solvers on random PCP and bioinformatics\nbenchmarks. The results also suggest that incorporating regular constraint\npropagation alongside other techniques could lead to substantial performance\ngains for existing solvers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6b63\u5219\u8868\u8fbe\u5f0f\u7ea6\u675f\u4f20\u64ad\u201d\uff08RCP\uff09\u7684\u7b80\u5355\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5b57\u7b26\u4e32\u7ea6\u675f\u95ee\u9898\u3002RCP\u901a\u8fc7\u5bf9\u5b57\u7b26\u4e32\u51fd\u6570\u5e94\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u9884/\u540e\u50cf\u6765\u63a8\u65ad\u5b57\u7b26\u4e32\u53d8\u91cf\u7684\u53ef\u80fd\u503c\uff0c\u76f4\u81f3\u627e\u5230\u51b2\u7a81\u6216\u786e\u5b9a\u53ef\u6ee1\u8db3\u6027\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u5b57\u7b26\u4e32\u64cd\u4f5c\uff0c\u5e76\u88ab\u8bc1\u660e\u5bf9\u4e8e\u5927\u91cf\u5b57\u7b26\u4e32\u7ea6\u675f\u662f\u5065\u5168\u548c\u5b8c\u6574\u7684\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u4e00\u4e9b\u5904\u7406\u65b9\u6cd5\u3002\u5728\u5b9e\u8df5\u4e2d\uff0cRCP\u5df2\u96c6\u6210\u5230OSTRICH\u5b57\u7b26\u4e32\u6c42\u89e3\u5668\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5176\u4ed6\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u5b57\u7b26\u4e32\u6c42\u89e3\u7b56\u7565\u7684\u590d\u6742\u6027\u4fc3\u4f7f\u6211\u4eec\u7814\u7a76\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5b57\u7b26\u4e32\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u6b63\u5219\u8868\u8fbe\u5f0f\u7ea6\u675f\u4f20\u64ad\uff08RCP\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u8ba1\u7b97\u6b63\u5219\u8868\u8fbe\u5f0f\u5728\u5b57\u7b26\u4e32\u51fd\u6570\u4e0b\u7684\u9884/\u540e\u50cf\u6765\u63a8\u65ad\u5b57\u7b26\u4e32\u53d8\u91cf\u7684\u503c\uff0c\u76f4\u81f3\u8fbe\u5230\u7a33\u5b9a\u72b6\u6001\uff08\u51b2\u7a81\u6216\u53ef\u6ee1\u8db3\u6027\uff09\u3002", "result": "RCP\u88ab\u8bc1\u660e\u662f\u5065\u5168\u548c\u5b8c\u6574\u7684\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u5728OSTRICH\u6c42\u89e3\u5668\u4e2d\u7684\u5b9e\u73b0\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u968f\u673aPCP\u548c\u751f\u7269\u4fe1\u606f\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6c42\u89e3\u5668\u3002", "conclusion": "\u6b63\u5219\u8868\u8fbe\u5f0f\u7ea6\u675f\u4f20\u64ad\uff08RCP\uff09\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u5b57\u7b26\u4e32\u7ea6\u675f\u6c42\u89e3\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u73b0\u6709\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u5176\u4ed6\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.19249", "categories": ["cs.LG", "math.DS", "stat.ME", "stat.ML", "37M99"], "pdf": "https://arxiv.org/pdf/2508.19249", "abs": "https://arxiv.org/abs/2508.19249", "authors": ["Jonas S\u00f8eborg Nielsen", "Marcus Galea Jacobsen", "Albert Brincker Olson", "Mads Peter S\u00f8rensen", "Allan Peter Engsig-Karup"], "title": "Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models", "comment": "For public PIR Julia package, see\n  https://github.com/MarcusGalea/PhysicsInformedRegression.jl", "summary": "We present a new efficient hybrid parameter estimation method based on the\nidea, that if nonlinear dynamic models are stated in terms of a system of\nequations that is linear in terms of the parameters, then regularized ordinary\nleast squares can be used to estimate these parameters from time series data.\nWe introduce the term \"Physics-Informed Regression\" (PIR) to describe the\nproposed data-driven hybrid technique as a way to bridge theory and data by use\nof ordinary least squares to efficiently perform parameter estimation of the\nmodel coefficients of different parameter-linear models; providing examples of\nmodels based on nonlinear ordinary equations (ODE) and partial differential\nequations (PDE). The focus is on parameter estimation on a selection of ODE and\nPDE models, each illustrating performance in different model characteristics.\nFor two relevant epidemic models of different complexity and number of\nparameters, PIR is tested and compared against the related technique,\nphysics-informed neural networks (PINN), both on synthetic data generated from\nknown target parameters and on real public Danish time series data collected\nduring the COVID-19 pandemic in Denmark. Both methods were able to estimate the\ntarget parameters, while PIR showed to perform noticeably better, especially on\na compartment model with higher complexity. Given the difference in\ncomputational speed, it is concluded that the PIR method is superior to PINN\nfor the models considered. It is also demonstrated how PIR can be applied to\nestimate the time-varying parameters of a compartment model that is fitted\nusing real Danish data from the COVID-19 pandemic obtained during a period from\n2020 to 2021. The study shows how data-driven and physics-informed techniques\nmay support reliable and fast -- possibly real-time -- parameter estimation in\nparameter-linear nonlinear dynamic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u5316\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u6df7\u5408\u53c2\u6570\u4f30\u8ba1\u7b97\u6cd5PIR\uff0c\u7528\u4e8e\u4f30\u8ba1\u53c2\u6570\u7ebf\u6027\u5316\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u7684\u7cfb\u6570\uff0c\u5e76\u901a\u8fc7\u4e0ePINN\u5728\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u4e0a\u7684\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86PIR\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728COVID-19\u75ab\u60c5\u6570\u636e\u4e0a\u7684\u5e94\u7528\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u53c2\u6570\u4f30\u8ba1\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\u53c2\u6570\u4f30\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5229\u7528\u53c2\u6570\u7ebf\u6027\u5316\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u7269\u7406\u4fe1\u606f\u56de\u5f52\u201d\uff08PIR\uff09\u7684\u6570\u636e\u9a71\u52a8\u6df7\u5408\u6280\u672f\uff0c\u8be5\u6280\u672f\u5229\u7528\u6b63\u5219\u5316\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6765\u4f30\u8ba1\u53c2\u6570\u7ebf\u6027\u5316\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u7cfb\u6570\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6a21\u578b\u3002", "result": "PIR\u65b9\u6cd5\u5728\u4f30\u8ba1\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u53c2\u6570\u65b9\u9762\u8868\u73b0\u4f18\u4e8ePINN\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u66f4\u9ad8\u590d\u6742\u6027\u7684\u6a21\u578b\u4e2d\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u901f\u5ea6\u4e0a\u66f4\u5177\u4f18\u52bf\u3002PIR\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u4f30\u8ba1COVID-19\u75ab\u60c5\u4e39\u9ea6\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u65f6\u53d8\u53c2\u6570\u3002", "conclusion": "PIR\u65b9\u6cd5\u662f\u5904\u7406\u53c2\u6570\u7ebf\u6027\u5316\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6709\u6548\u4e14\u5feb\u901f\u7684\u53c2\u6570\u4f30\u8ba1\u5de5\u5177\uff0c\u4f18\u4e8ePINN\uff0c\u5e76\u6709\u671b\u652f\u6301\u5b9e\u65f6\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2508.19452", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19452", "abs": "https://arxiv.org/abs/2508.19452", "authors": ["Andrea Esposito", "Francesco P. Rossi", "Marco Bernardo", "Francesco Fabris", "Hubert Garavel"], "title": "Formal Modeling and Verification of the Algorand Consensus Protocol in CADP", "comment": null, "summary": "Algorand is a scalable and secure permissionless blockchain that achieves\nproof-of-stake consensus via cryptographic self-sortition and binary Byzantine\nagreement. In this paper, we present a process algebraic model of the Algorand\nconsensus protocol with the aim of enabling rigorous formal verification. Our\nmodel captures the behavior of participants with respect to the structured\nalternation of consensus steps toward a committee-based agreement by means of a\nprobabilistic process calculus. We validate the correctness of the protocol in\nthe absence of adversaries and then extend our model to capture the influence\nof coordinated malicious nodes that can force the commit of an empty block\ninstead of the proposed one. The adversarial scenario is analyzed by using an\nequivalence-checking-based noninterference framework that we have implemented\nin the CADP verification toolkit. In addition to highlighting both the\nrobustness and the limitations of the Algorand protocol under adversarial\nassumptions, this work illustrates the added value of using formal methods for\nthe analysis of blockchain consensus algorithms.", "AI": {"tldr": "Algorand\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u65e0\u8bb8\u53ef\u533a\u5757\u94fe\uff0c\u901a\u8fc7\u52a0\u5bc6\u81ea\u6211\u7b5b\u9009\u548c\u4e8c\u5143\u62dc\u5360\u5ead\u5171\u8bc6\u8fbe\u6210\u6743\u76ca\u8bc1\u660e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAlgorand\u5171\u8bc6\u534f\u8bae\u7684\u8fdb\u7a0b\u4ee3\u6570\u6a21\u578b\uff0c\u65e8\u5728\u5b9e\u73b0\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u6982\u7387\u8fdb\u7a0b\u6f14\u7b97\u6355\u6349\u53c2\u4e0e\u8005\u5173\u4e8e\u7ed3\u6784\u5316\u5171\u8bc6\u6b65\u9aa4\u7684\u4ea4\u66ff\u884c\u4e3a\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u59d4\u5458\u4f1a\u7684\u5171\u8bc6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5bf9Algorand\u5171\u8bc6\u534f\u8bae\u8fdb\u884c\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u63ed\u793a\u5176\u5728\u9762\u5bf9\u6076\u610f\u8282\u70b9\u65f6\u7684\u9c81\u68d2\u6027\u548c\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u5206\u6790\u533a\u5757\u94fe\u5171\u8bc6\u7b97\u6cd5\u4e2d\u7684\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u6982\u7387\u8fdb\u7a0b\u6f14\u7b97\u6784\u5efaAlgorand\u5171\u8bc6\u534f\u8bae\u7684\u6a21\u578b\uff0c\u6a21\u62df\u53c2\u4e0e\u8005\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u7b49\u4ef7\u68c0\u67e5\u7684\u975e\u5e72\u6270\u6846\u67b6\u5206\u6790\u6076\u610f\u8282\u70b9\u7684\u5f71\u54cd\u3002", "result": "\u9a8c\u8bc1\u4e86\u5728\u6ca1\u6709\u6076\u610f\u8282\u70b9\u7684\u60c5\u51b5\u4e0b\u534f\u8bae\u7684\u6b63\u786e\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u5728\u5b58\u5728\u534f\u8c03\u6076\u610f\u8282\u70b9\u65f6\u53ef\u80fd\u5bfc\u81f4\u63d0\u4ea4\u7a7a\u5757\u7684\u573a\u666f\u3002", "conclusion": "Algorand\u534f\u8bae\u5728\u6ca1\u6709\u6076\u610f\u8282\u70b9\u65f6\u662f\u6b63\u786e\u7684\uff0c\u4f46\u5728\u5b58\u5728\u6076\u610f\u8282\u70b9\u65f6\u5b58\u5728\u88ab\u5f3a\u5236\u63d0\u4ea4\u7a7a\u5757\u7684\u98ce\u9669\u3002\u5f62\u5f0f\u5316\u65b9\u6cd5\u5bf9\u4e8e\u5206\u6790\u533a\u5757\u94fe\u5171\u8bc6\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.19907", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19907", "abs": "https://arxiv.org/abs/2508.19907", "authors": ["Hewen Wang", "Renchi Yang", "Xiaokui Xiao"], "title": "GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs", "comment": "11 pages. Paper accepted to CIKM 2025", "summary": "Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,\nthe goal of link sign prediction is to predict the signs of potential links\nconnecting U and V based on known positive and negative edges in G. The\nmajority of existing solutions towards link sign prediction mainly focus on\nunipartite signed graphs, which are sub-optimal due to the neglect of node\nheterogeneity and unique bipartite characteristics of SBGs. To this end, recent\nstudies adapt graph neural networks to SBGs by introducing message-passing\nschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node\npairs. However, the fundamental spectral convolutional operators were\noriginally designed for positive links in unsigned graphs, and thus, are not\noptimal for inferring missing positive or negative links from known ones in\nSBGs.\n  Motivated by this, this paper proposes GegenNet, a novel and effective\nspectral convolutional neural network model for link sign prediction in SBGs.\nIn particular, GegenNet achieves enhanced model capacity and high predictive\naccuracy through three main technical contributions: (i) fast and theoretically\ngrounded spectral decomposition techniques for node feature initialization;\n(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and\n(iii) multi-layer sign-aware spectral convolutional networks alternating\nGegenbauer polynomial filters with positive and negative edges. Our extensive\nempirical studies reveal that GegenNet can achieve significantly superior\nperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign\nprediction compared to 11 strong competitors over 6 benchmark SBG datasets.", "AI": {"tldr": "GegenNet\u662f\u4e00\u79cd\u7528\u4e8e\u6709\u5411\u4e8c\u5206\u56fe\u94fe\u63a5\u7b26\u53f7\u9884\u6d4b\u7684\u65b0\u578b\u8c31\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u65b0\u9896\u7684\u8c31\u5206\u89e3\u6280\u672f\u3001Gegenbauer\u591a\u9879\u5f0f\u8c31\u6ee4\u6ce2\u5668\u548c\u591a\u5c42\u611f\u77e5\u7f51\u7edc\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u6709\u5411\u4e8c\u5206\u56fe\u7684\u8282\u70b9\u5f02\u8d28\u6027\u548c\u7279\u5b9a\u5c5e\u6027\uff0c\u5e76\u4e14\u57fa\u4e8e\u8c31\u5377\u79ef\u7684\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u65e0\u5411\u56fe\uff0c\u4e0d\u9002\u7528\u4e8e\u9884\u6d4b\u6709\u5411\u4e8c\u5206\u56fe\u4e2d\u7684\u7f3a\u5931\u94fe\u63a5\u7b26\u53f7\u3002", "method": "GegenNet\u91c7\u7528\uff08i\uff09\u5feb\u901f\u4e14\u5177\u6709\u7406\u8bba\u4f9d\u636e\u7684\u8282\u70b9\u7279\u5f81\u521d\u59cb\u5316\u8c31\u5206\u89e3\u6280\u672f\uff1b\uff08ii\uff09\u57fa\u4e8eGegenbauer\u591a\u9879\u5f0f\u57fa\u7684\u65b0\u578b\u8c31\u56fe\u6ee4\u6ce2\u5668\uff1b\uff08iii\uff09\u591a\u5c42\u611f\u77e5\u7b26\u53f7\u611f\u77e5\u8c31\u5377\u79ef\u7f51\u7edc\uff0c\u4ea4\u66ff\u4f7f\u7528Gegenbauer\u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\u548c\u6b63\u8d1f\u94fe\u63a5\u3002", "result": "GegenNet\u5728\u94fe\u63a5\u7b26\u53f7\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd411\u4e2a\u5f3a\u5927\u7684\u7ade\u4e89\u5bf9\u624b\u548c6\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5728AUC\u65b9\u9762\u6700\u9ad8\u63d0\u53474.28%\uff0c\u5728F1\u65b9\u9762\u6700\u9ad8\u63d0\u534711.69%\u3002", "conclusion": "GegenNet\u5728\u6709\u5411\u4e8c\u5206\u56fe\u94fe\u63a5\u7b26\u53f7\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6280\u672f\u8d21\u732e\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.19270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53cc\u8bed\u97f3\u7d20\u96c6\u548c\u5229\u7528PhoWhisper\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u63d0\u9ad8\u4e86\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u8bed\u97f3\u7684\u97f3\u7d20\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u53d1\u97f3\u65f6\uff0c\u7531\u4e8e\u97f3\u8c03\u548c\u91cd\u97f3\u5dee\u5f02\u5bfc\u81f4\u7684\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u53cc\u8bed\u97f3\u7d20\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\uff0c\u5229\u7528PhoWhisper\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u53d6\u9ad8\u5c42\u7279\u5f81\u4ee5\u63d0\u5347\u97f3\u7d20\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8d8a\u5357\u8bed\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u5904\u7406\u57fa\u4e8e\u97f3\u8c03\u548c\u91cd\u97f3\u7684\u97f3\u7d20\u8bc6\u522b\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u6846\u67b6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u8bed\u97f3\u4e2d\u7684\u97f3\u7d20\u8bc6\u522b\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.20014", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20014", "abs": "https://arxiv.org/abs/2508.20014", "authors": ["Yang Meng", "Zewen Pan", "Yandi Lu", "Ruobing Huang", "Yanfeng Liao", "Jiarui Yang"], "title": "CataractSurg-80K: Knowledge-Driven Benchmarking for Structured Reasoning in Ophthalmic Surgery Planning", "comment": "18 pages, 9 figures", "summary": "Cataract surgery remains one of the most widely performed and effective\nprocedures for vision restoration. Effective surgical planning requires\nintegrating diverse clinical examinations for patient assessment, intraocular\nlens (IOL) selection, and risk evaluation. Large language models (LLMs) have\nshown promise in supporting clinical decision-making. However, existing LLMs\noften lack the domain-specific expertise to interpret heterogeneous ophthalmic\ndata and provide actionable surgical plans. To enhance the model's ability to\ninterpret heterogeneous ophthalmic reports, we propose a knowledge-driven\nMulti-Agent System (MAS), where each agent simulates the reasoning process of\nspecialist ophthalmologists, converting raw clinical inputs into structured,\nactionable summaries in both training and deployment stages. Building on MAS,\nwe introduce CataractSurg-80K, the first large-scale benchmark for cataract\nsurgery planning that incorporates structured clinical reasoning. Each case is\nannotated with diagnostic questions, expert reasoning chains, and structured\nsurgical recommendations. We further introduce Qwen-CSP, a domain-specialized\nmodel built on Qwen-4B, fine-tuned through a multi-stage process tailored for\nsurgical planning. Comprehensive experiments show that Qwen-CSP outperforms\nstrong general-purpose LLMs across multiple metrics. Our work delivers a\nhigh-quality dataset, a rigorous benchmark, and a domain-adapted LLM to\nfacilitate future research in medical AI reasoning and decision support.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCataractSurg-80K\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u767d\u5185\u969c\u624b\u672f\u89c4\u5212\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aQwen-CSP\u7684\u9886\u57df\u4e13\u4e1a\u6a21\u578b\u3002\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u8ba9\u6bcf\u4e2a\u667a\u80fd\u4f53\u6a21\u62df\u4e13\u79d1\u773c\u79d1\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u539f\u59cb\u4e34\u5e8a\u8f93\u5165\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u6458\u8981\u3002\u8be5\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u773c\u79d1\u62a5\u544a\u548c\u63d0\u4f9b\u624b\u672f\u8ba1\u5212\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7684LLM\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u89e3\u8bfb\u5f02\u6784\u773c\u79d1\u6570\u636e\u548c\u63d0\u4f9b\u53ef\u884c\u7684\u624b\u672f\u8ba1\u5212\u65b9\u9762\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u589e\u5f3a\u5176\u80fd\u529b\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u6a21\u62df\u4e13\u79d1\u773c\u79d1\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u539f\u59cb\u4e34\u5e8a\u8f93\u5165\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u6458\u8981\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86CataractSurg-80K\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u5728Qwen-4B\u57fa\u7840\u4e0a\u5fae\u8c03\u7684\u9886\u57df\u4e13\u4e1a\u6a21\u578bQwen-CSP\u3002", "result": "Qwen-CSP\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u5f3a\u5927\u7684\u901a\u7528LLM\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u3001\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u548c\u4e00\u4e2a\u9886\u57df\u9002\u5e94\u7684LLM\uff0c\u4ee5\u4fc3\u8fdb\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u63a8\u7406\u548c\u51b3\u7b56\u652f\u6301\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.19496", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19496", "abs": "https://arxiv.org/abs/2508.19496", "authors": ["Liqin Ke", "R. Flint", "Y. Lee"], "title": "Accurate calculation of light rare-earth magnetic anisotropy with density functional theory", "comment": "6 pages, 3 figures", "summary": "Density functional theory (DFT) has long struggled to treat light rare-earth\nmagnetism. We show that this difficulty arises from an overestimate of the $4f$\ncharge asphericity, and thus the magnetic anisotropy energy, due to the\ninadequacy of single Slater-determinant representations. We propose an\neffective solution by combining constrained DFT+U with crystal field theory and\na systematic many-body correction to the charge asphericity. We confirm the\nvalidity of this combination on TbV$_6$Sn$_6$ and TbCo$_5$, and then show how\nthe many-body correction adjusts the calculated magnetic anisotropy energy of\nSmCo$_5$ to match experiment. Our method is an efficient DFT-based approach to\naddress light-rare-earth magnetism.", "AI": {"tldr": "DFT\u5728\u5904\u7406\u8f7b\u7a00\u571f\u78c1\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u539f\u56e0\u662f4f\u7535\u8377\u975e\u7403\u5f62\u5ea6\u88ab\u9ad8\u4f30\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ea6\u675fDFT+U\u3001\u6676\u4f53\u573a\u7406\u8bba\u548c\u7535\u8377\u975e\u7403\u5f62\u5ea6\u7684\u591a\u4f53\u4fee\u6b63\u7684\u6709\u6548\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728TbV6Sn6\u548cTbCo5\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u6210\u529f\u9884\u6d4b\u4e86SmCo5\u7684\u78c1\u5404\u5411\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u5728\u5904\u7406\u8f7b\u7a00\u571f\u78c1\u6027\u65f6\u9047\u5230\u7684\u56f0\u96be\uff0c\u8be5\u56f0\u96be\u6e90\u4e8e4f\u7535\u8377\u975e\u7403\u5f62\u5ea6\u88ab\u9ad8\u4f30\u3002", "method": "\u7ed3\u5408\u7ea6\u675fDFT+U\u3001\u6676\u4f53\u573a\u7406\u8bba\u548c\u7535\u8377\u975e\u7403\u5f62\u5ea6\u7684\u591a\u4f53\u4fee\u6b63\u3002", "result": "\u5728TbV6Sn6\u548cTbCo5\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u8c03\u6574\u4e86SmCo5\u7684\u78c1\u5404\u5411\u5f02\u6027\u80fd\u8ba1\u7b97\u7ed3\u679c\u4ee5\u5339\u914d\u5b9e\u9a8c\u503c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u57fa\u4e8eDFT\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8f7b\u7a00\u571f\u78c1\u6027\u95ee\u9898\u3002"}}
{"id": "2508.19393", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19393", "abs": "https://arxiv.org/abs/2508.19393", "authors": ["Phuoc Pham", "Arun Venkitaraman", "Chia-Yu Hsieh", "Andrea Bonetti", "Stefan Uhlich", "Markus Leibl", "Simon Hofmann", "Eisaku Ohbuchi", "Lorenzo Servadei", "Ulf Schlichtmann", "Robert Wille"], "title": "GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification", "comment": null, "summary": "Analog subcircuit identification is a core task in analog design, essential\nfor simulation, sizing, and layout. Traditional methods often require extensive\nhuman expertise, rule-based encoding, or large labeled datasets. To address\nthese challenges, we propose GENIE-ASI, the first training-free, large language\nmodel (LLM)-based methodology for analog subcircuit identification. GENIE-ASI\noperates in two phases: it first uses in-context learning to derive natural\nlanguage instructions from a few demonstration examples, then translates these\ninto executable Python code to identify subcircuits in unseen SPICE netlists.\nIn addition, to evaluate LLM-based approaches systematically, we introduce a\nnew benchmark composed of operational amplifier netlists (op-amps) that cover a\nwide range of subcircuit variants. Experimental results on the proposed\nbenchmark show that GENIE-ASI matches rule-based performance on simple\nstructures (F1-score = 1.0), remains competitive on moderate abstractions\n(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =\n0.31). These findings demonstrate that LLMs can serve as adaptable,\ngeneral-purpose tools in analog design automation, opening new research\ndirections for foundation model applications in analog design automation.", "AI": {"tldr": "GENIE-ASI\u662f\u9996\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6a21\u62df\u5b50\u7535\u8def\u8bc6\u522b\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548cPython\u4ee3\u7801\u81ea\u52a8\u8bc6\u522bSPICE\u7f51\u8868\u4e2d\u7684\u5b50\u7535\u8def\uff0c\u5e76\u5728\u65b0\u7684\u8fd0\u7b97\u653e\u5927\u5668\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u5b50\u7535\u8def\u8bc6\u522b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u3001\u89c4\u5219\u7f16\u7801\u6216\u6807\u6ce8\u6570\u636e\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "GENIE-ASI\u5206\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ece\u5c11\u91cf\u793a\u4f8b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u30022. \u5c06\u6307\u4ee4\u8f6c\u6362\u4e3aPython\u4ee3\u7801\uff0c\u7528\u4e8e\u8bc6\u522b\u65b0\u7684SPICE\u7f51\u8868\u4e2d\u7684\u5b50\u7535\u8def\u3002", "result": "\u5728\u65b0\u7684\u8fd0\u7b97\u653e\u5927\u5668\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGENIE-ASI\u5728\u7b80\u5355\u7ed3\u6784\u4e0a\u8fbe\u52301.0\u7684F1\u5206\u6570\uff0c\u5728\u4e2d\u7b49\u62bd\u8c61\u7ed3\u6784\u4e0a\u8fbe\u52300.81\uff0c\u5728\u590d\u6742\u5b50\u7535\u8def\u4e0a\u8fbe\u52300.31\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u901a\u7528\u5de5\u5177\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6a21\u62df\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u9002\u5e94\u6027\u5f3a\u7684\u901a\u7528\u5de5\u5177\uff0c\u4e3a\u57fa\u7840\u6a21\u578b\u5728\u6a21\u62df\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.19785", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19785", "abs": "https://arxiv.org/abs/2508.19785", "authors": ["Barbara Geissmann", "Stefano Leucci", "Chih-Hung Liu", "Paolo Penna"], "title": "An Optimal Sorting Algorithm for Persistent Random Comparison Faults", "comment": null, "summary": "We consider the problem of sorting $n$ elements subject to persistent random\ncomparison errors. In this problem, each comparison between two elements can be\nwrong with some fixed (small) probability $p$, and comparing the same pair of\nelements multiple times always yields the same result. Sorting perfectly in\nthis model is impossible, and the objective is to minimize the dislocation of\neach element in the output sequence, i.e., the difference between its position\nin the sequence and its true rank.\n  In this paper, we present the first $O(n\\log n)$-time sorting algorithm that\nguarantees both $O(\\log n)$ maximum dislocation and $O(n)$ total dislocation\nwith high probability when $p<\\frac{1}{4}$. This settles the time complexity\nsorting with persistent comparison errors in the given range of $p$ and shows\nthat comparison errors do not increase its computational difficulty. Indeed,\n$\\Omega(n\\log n)$ time is necessary to archive a maximum dislocation of $O(\\log\nn)$ even without comparison errors. Moreover, we prove that no algorithm can\nguarantee a maximum dislocation of $o(\\log n)$ with high probability, nor a\ntotal dislocation of $o(n)$ in expectation.\n  To develop our sorting algorithm, we solve two related sub-problems, which\nmight be of independent interest. More precisely, we show that $O(\\log n)$ time\nsuffices to find a position in which to insert a new element $x$ in an\nalmost-sorted sequence $S$ of $n$ elements having dislocation at most\n$d=\\Omega(\\log n)$, so that the dislocation of $x$ in the resulting sequence is\n$O(d)$ with high probability (which can be equivalently thought as the problem\nof estimating the rank of $x$ in $S$). We also show that the maximum (resp.\ntotal) dislocation of an approximately sorted sequence $S$ of $n$ elements can\nbe lowered to $O(\\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is\nan upper bound on the maximum dislocation of $S$.", "AI": {"tldr": "\u5728\u5b58\u5728\u56fa\u5b9a\uff08\u5c0f\u6570\uff09\u6982\u7387p\u7684\u6301\u4e45\u968f\u673a\u6bd4\u8f83\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9n\u4e2a\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n log n)\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728p<1/4\u65f6\uff0c\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u4fdd\u8bc1O(log n)\u7684\u6700\u5927\u9519\u4f4d\u548cO(n)\u7684\u603b\u9519\u4f4d\u3002", "motivation": "\u5728\u5b58\u5728\u6301\u4e45\u968f\u673a\u6bd4\u8f83\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9n\u4e2a\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u8f93\u51fa\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u9519\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdO(n log n)\u65f6\u95f4\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u89e3\u51b3\u4e86\u4e24\u4e2a\u5b50\u95ee\u9898\uff1a1) \u5728\u5bf9\u6570\u65f6\u95f4\u5185\u5728\u51e0\u4e4e\u6392\u5e8f\u7684\u5e8f\u5217\u4e2d\u63d2\u5165\u4e00\u4e2a\u65b0\u5143\u7d20\uff0c\u5e76\u4ee5\u9ad8\u6982\u7387\u4fdd\u8bc1O(d)\u7684\u9519\u4f4d\u30022) \u5728O(nd)\u65f6\u95f4\u5185\u5c06\u51e0\u4e4e\u6392\u5e8f\u5e8f\u5217\u7684\u6700\u5927\uff08\u6216\u603b\uff09\u9519\u4f4d\u964d\u4f4e\u5230O(log n)\uff08\u6216O(n)\uff09\u3002", "result": "\u8be5\u7b97\u6cd5\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u5b9e\u73b0O(log n)\u7684\u6700\u5927\u9519\u4f4d\u548cO(n)\u7684\u603b\u9519\u4f4d\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u8fd9\u662f\u6700\u4f18\u7684\u3002", "conclusion": "\u4e0e\u6ca1\u6709\u6bd4\u8f83\u9519\u8bef\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u6bd4\u8f83\u9519\u8bef\u4e0d\u4f1a\u589e\u52a0\u6392\u5e8f\u7684\u8ba1\u7b97\u96be\u5ea6\uff0c\u56e0\u4e3aO(n log n)\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u662f\u8fbe\u5230O(log n)\u6700\u5927\u9519\u4f4d\u7684\u5fc5\u8981\u6761\u4ef6\u3002"}}
{"id": "2508.19408", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19408", "abs": "https://arxiv.org/abs/2508.19408", "authors": ["Vaclav Pavlicek", "Ayush Bhandari"], "title": "1-Bit Unlimited Sampling Beyond Fourier Domain: Low-Resolution Sampling of Quantization Noise", "comment": "20 pages, accepted to IEEE Journal of Selected Topics in Signal\n  Processing", "summary": "Analog-to-digital converters (ADCs) play a critical role in digital signal\nacquisition across various applications, but their performance is inherently\nconstrained by sampling rates and bit budgets. This bit budget imposes a\ntrade-off between dynamic range (DR) and digital resolution, with ADC energy\nconsumption scaling linearly with sampling rate and exponentially with bit\ndepth. To bypass this, numerous approaches, including oversampling with\nlow-resolution ADCs, have been explored. A prominent example is 1-Bit ADCs with\nSigma-Delta Quantization (SDQ), a widely used consumer-grade solution. However,\nSDQs suffer from overloading or saturation issues, limiting their ability to\nhandle inputs with arbitrary DR. The Unlimited Sensing Framework (USF)\naddresses this challenge by injecting modulo non-linearity in hardware,\nresulting in a new digital sensing technology. In this paper, we introduce a\nnovel 1-Bit sampling architecture that extends both conventional 1-Bit SDQ and\nUSF. Our contributions are twofold: (1) We generalize the concept of noise\nshaping beyond the Fourier domain, allowing the inclusion of non-bandlimited\nsignals in the Fourier domain but bandlimited in alternative transform domains.\n(2) Building on this generalization, we develop a new transform-domain recovery\nmethod for 1-Bit USF. When applied to the Fourier domain, our method\ndemonstrates superior performance compared to existing time-domain techniques,\noffering reduced oversampling requirements and improved robustness. Extensive\nnumerical experiments validate our findings, laying the groundwork for a\nbroader generalization of 1-Bit sampling systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76841\u4f4d\u91c7\u6837\u67b6\u6784\uff0c\u5b83\u6269\u5c55\u4e86\u4f20\u7edf\u76841\u4f4d\u03a3-\u0394\u91cf\u5316\uff08SDQ\uff09\u548c\u65e0\u9650\u5236\u4f20\u611f\u6846\u67b6\uff08USF\uff09\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u5728\u786c\u4ef6\u4e2d\u5f15\u5165\u6a21\u975e\u7ebf\u6027\u6765\u89e3\u51b3SDQ\u8fc7\u8f7d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u6362\u57df\u6062\u590d\u65b9\u6cd5\uff0c\u5728\u5085\u7acb\u53f6\u57df\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u8fc7\u91c7\u6837\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "motivation": "ADC\u5728\u6570\u5b57\u4fe1\u53f7\u91c7\u96c6\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u91c7\u6837\u7387\u548c\u6bd4\u7279\u9884\u7b97\u9650\u5236\uff0c\u6bd4\u7279\u9884\u7b97\u5728\u52a8\u6001\u8303\u56f4\uff08DR\uff09\u548c\u6570\u5b57\u5206\u8fa8\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u80fd\u8017\u4e0e\u91c7\u6837\u7387\u6210\u7ebf\u6027\u5173\u7cfb\uff0c\u4e0e\u6bd4\u7279\u6df1\u5ea6\u6210\u6307\u6570\u5173\u7cfb\u3002\u73b0\u6709\u76841\u4f4dADC\uff08\u5982SDQ\uff09\u5b58\u5728\u8fc7\u8f7d\u95ee\u9898\uff0c\u65e0\u6cd5\u5904\u7406\u4efb\u610fDR\u8f93\u5165\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76841\u4f4d\u91c7\u6837\u67b6\u6784\uff0c\u5c06\u566a\u58f0\u6574\u5f62\u6982\u5ff5\u63a8\u5e7f\u5230\u5085\u7acb\u53f6\u57df\u4e4b\u5916\uff0c\u5141\u8bb8\u5305\u542b\u975e\u5e26\u9650\u4fe1\u53f7\uff08\u5728\u5176\u4ed6\u53d8\u6362\u57df\u4e2d\u5e26\u9650\uff09\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u6362\u57df\u6062\u590d\u65b9\u6cd5\uff0c\u7528\u4e8e1\u4f4dUSF\uff0c\u5728\u5085\u7acb\u53f6\u57df\u4e2d\u5e94\u7528\u65f6\u4f18\u4e8e\u65f6\u95f4\u57df\u6280\u672f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5085\u7acb\u53f6\u57df\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65f6\u95f4\u57df\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u8fc7\u91c7\u6837\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u76841\u4f4d\u91c7\u6837\u67b6\u6784\u548c\u53d8\u6362\u57df\u6062\u590d\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a1\u4f4d\u91c7\u6837\u7cfb\u7edf\u66f4\u5e7f\u6cdb\u7684\u63a8\u5e7f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19531", "categories": ["cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19531", "abs": "https://arxiv.org/abs/2508.19531", "authors": ["Yu-Hong Han", "Yi Li", "Feng Mei", "Liantuan Xiao", "Suotang Jia"], "title": "Observation of topological switch between Weyl semimetal and third-order topological insulator phases", "comment": "17 pages, 7 figures,", "summary": "Weyl semimetals and higher-order topological insulators represent two\nfundamental yet distinct classes of topological matter. While both have been\nextensively studied in classical-wave systems, their coexistence and\ncontrollable transition within a single platform remain largely unexplored.\nMeanwhile, implementing three-dimensional spin-orbit couplings, which is\ncrucial for realizing a broad class of higher-dimensional topological phases,\ncontinues to pose significant experimental challenges. Here, we experimentally\nrealize three-dimensional spin-orbit couplings and demonstrate that tuning the\ndimerized spin-orbit coupling strength enables both the coexistence of and a\ncontrollable switch between Weyl semimetal and third-order topological\ninsulator phases. By engineering a three-dimensional circuit metamaterial, we\nsynthesize the required spin-orbit interactions and observe hallmark signatures\nof both phases: frequency spectroscopy reveals the Fermi arcs, while local\ndensity of states measurements identify the topological corner modes.\nInterestingly, the corner mode degeneracy doubles compared to that in the\ncanonical Benalcazar-Bernevig-Hughes model, signaling an enriched topological\nstructure. Our study establishes a fundamental connection between two\nparadigmatic topological phases and paves the way for further exploring\nspin-orbit-coupling induced exotic higher-dimensional topological phases.", "AI": {"tldr": " Weyl semimetals and higher-order topological insulators are distinct topological phases, but their coexistence and transition in a single platform are unexplored. This paper demonstrates their coexistence and controllable transition in a 3D circuit metamaterial by engineering 3D spin-orbit couplings. The platform exhibits both Weyl semimetal (Fermi arcs) and third-order topological insulator (corner modes) phases, with the corner modes showing doubled degeneracy compared to the canonical model. This work connects two major topological phases and enables exploration of higher-dimensional topological phases induced by spin-orbit coupling.", "motivation": "While Weyl semimetals and higher-order topological insulators are fundamental topological phases, their coexistence and controllable transition in a single platform, along with the experimental implementation of 3D spin-orbit couplings, remain underexplored challenges.", "method": "The researchers engineered a three-dimensional circuit metamaterial to synthesize the required spin-orbit interactions. They tuned the dimerized spin-orbit coupling strength to achieve coexistence and switching between Weyl semimetal and third-order topological insulator phases. The experimental methods included frequency spectroscopy to observe Fermi arcs (Weyl semimetal signature) and local density of states measurements to identify topological corner modes (third-order topological insulator signature).", "result": "The experiment successfully realized 3D spin-orbit couplings and demonstrated the coexistence of Weyl semimetal and third-order topological insulator phases. Tuning the spin-orbit coupling strength allowed for a controllable switch between these phases. Signatures of both phases were observed: Fermi arcs via frequency spectroscopy and topological corner modes via local density of states measurements. Notably, the corner modes exhibited doubled degeneracy compared to the canonical Benalcazar-Bernevig-Hughes model, indicating a more complex topological structure.", "conclusion": "This study establishes a fundamental link between Weyl semimetals and third-order topological insulators, demonstrating their coexistence and controllable transition within a single platform realized by a 3D circuit metamaterial with engineered spin-orbit couplings. The observed doubled degeneracy of corner modes suggests an enriched topological structure. This work opens avenues for investigating novel higher-dimensional topological phases driven by spin-orbit coupling."}}
{"id": "2508.19280", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19280", "abs": "https://arxiv.org/abs/2508.19280", "authors": ["Partha Ghose"], "title": "A New Approach to Unification", "comment": "10 pages, one figure; some editing done", "summary": "This paper presents a new perspective on unifying all fundamental\ninteractions--gravitational, electromagnetic, weak and strong--based on\nstochastic processes rather than conventional quantum mechanics. Earlier work\nby Nelson, Kac and others have established that key quantum features such as\nthe Schr\\\"{o}dinger and Dirac equations together with the Born rule can be\nderived from classical random processes involving finite speeds and\nprobabilistic reversals. A fundamental length scale, inherent for dimensional\nconsistency, regularizes the infinities that typically plague conventional\nfield theories. The method can be used to quantize electrodynamics as well as\nlinear gravity, using the Riemann-Silberstein vector and its generalization.\n  To include fields beyond electromagnetism, the Riemann-Silberstein vector can\nbe generalized to describe non-Abelian gauge fields without relying on gauge\nsymmetry. These fields can be coupled to spin networks--geometric structures\nthat discretize space--leading to a unified framework that includes both matter\nand geometrty. In the large-scale limit, the model reproduces familiar quantum\nfield behaviour, while remaining finite and background-independent at the\nfundamental level. The emergence of equilibrium states resembling\nWheeler-DeWill constraints in gravity adds further depth, suggesting a novel\nroute to quantum gravity and unification grounded in physical stochasticity\nrather than quantization rules.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fc7\u7a0b\u800c\u975e\u4f20\u7edf\u91cf\u5b50\u529b\u5b66\u7edf\u4e00\u5f15\u529b\u3001\u7535\u78c1\u529b\u3001\u5f3a\u529b\u548c\u5f31\u529b\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u7edf\u4e00\u6240\u6709\u57fa\u672c\u76f8\u4e92\u4f5c\u7528\u529b\u3002", "method": "\u5229\u7528\u968f\u673a\u8fc7\u7a0b\uff08\u800c\u975e\u91cf\u5b50\u529b\u5b66\uff09\u6765\u63a8\u5bfc\u859b\u5b9a\u8c14\u65b9\u7a0b\u3001\u72c4\u62c9\u514b\u65b9\u7a0b\u548c\u73bb\u6069\u89c4\u5219\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u672c\u957f\u5ea6\u5c3a\u5ea6\u6765\u6b63\u5219\u5316\u573a\u8bba\u4e2d\u7684\u65e0\u7a77\u5927\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u91cf\u5316\u7535\u52a8\u529b\u5b66\u548c\u7ebf\u6027\u5f15\u529b\uff0c\u5e76\u901a\u8fc7\u5e7f\u4e49\u9ece\u66fc-\u897f\u5c14\u4f2f\u7279\u77e2\u91cf\u63cf\u8ff0\u975e\u963f\u8d1d\u5c14\u89c4\u8303\u573a\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u89c4\u8303\u5bf9\u79f0\u6027\u3002\u5c06\u8fd9\u4e9b\u573a\u4e0e\u79bb\u6563\u7a7a\u95f4\u7684\u81ea\u65cb\u7f51\u7edc\u8026\u5408\uff0c\u6784\u5efa\u4e86\u5305\u542b\u7269\u8d28\u548c\u51e0\u4f55\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5927\u5c3a\u5ea6\u6781\u9650\u4e0b\u53ef\u91cd\u73b0\u719f\u6089\u7684\u91cf\u5b50\u573a\u884c\u4e3a\uff0c\u5e76\u5728\u57fa\u672c\u5c42\u9762\u4e0a\u4fdd\u6301\u6709\u9650\u548c\u80cc\u666f\u65e0\u5173\u3002\u5728\u5f15\u529b\u4e2d\u51fa\u73b0\u4e86\u7c7b\u4f3c\u4e8e Wheeler-DeWill \u7ea6\u675f\u7684\u5e73\u8861\u6001\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u968f\u673a\u6027\u800c\u975e\u91cf\u5b50\u5316\u89c4\u5219\u7684\u91cf\u5b50\u5f15\u529b\u548c\u7edf\u4e00\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.19257", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19348", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.19348", "abs": "https://arxiv.org/abs/2508.19348", "authors": ["Vito Cerone", "Sophie M. Fosson", "Simone Pirrera", "Diego Regruto"], "title": "Set-membership identification of continuous-time MIMO systems via Tustin discretization", "comment": null, "summary": "In this paper, we deal with the identification of continuous-time systems\nfrom sampled data corrupted by unknown but bounded errors. A significant\nchallenge in continuous-time identification is the estimation of the input and\noutput data derivatives. In this paper, we propose a novel method based on\nset-membership techniques and Tustin discretization, which overcomes the\nderivative measurement problem and the presence of bounded errors affecting all\nthe measured signals. First, we derive the proposed method and prove that it\nbecomes an affordable polynomial optimization problem. Then, we present some\nnumerical results based on simulation and experimental data to explore the\neffectiveness of the proposed method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u6210\u5458\u6280\u672f\u548cTustin\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5305\u542b\u672a\u77e5\u4f46\u6709\u754c\u8bef\u5dee\u7684\u91c7\u6837\u6570\u636e\u4e2d\u8bc6\u522b\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5bfc\u6570\u6d4b\u91cf\u95ee\u9898\u548c\u6240\u6709\u6d4b\u91cf\u4fe1\u53f7\u4e2d\u7684\u6709\u754c\u8bef\u5dee\u95ee\u9898\u3002", "motivation": "\u8bc6\u522b\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u662f\u4f30\u8ba1\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\u7684\u5bfc\u6570\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u6210\u5458\u6280\u672f\u548cTustin\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u5bfc\u6570\u6d4b\u91cf\u95ee\u9898\u4ee5\u53ca\u5f71\u54cd\u6240\u6709\u6d4b\u91cf\u4fe1\u53f7\u7684\u6709\u754c\u8bef\u5dee\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u88ab\u8bc1\u660e\u53ef\u4ee5\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u591a\u9879\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u4e86\u6570\u503c\u7ed3\u679c\u7684\u5c55\u793a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5305\u542b\u6709\u754c\u8bef\u5dee\u7684\u91c7\u6837\u6570\u636e\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u8bc6\u522b\u4e2d\u7684\u5bfc\u6570\u6d4b\u91cf\u96be\u9898\u3002"}}
{"id": "2508.19920", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19920", "abs": "https://arxiv.org/abs/2508.19920", "authors": ["Matthew Meek", "Guy Tallent", "Thomas Breimer", "James Gaskell", "Abhay Kashyap", "Atharv Tekurkar", "Jonathan Fischman", "Luodi Wang", "Viet-Dung Nguyen", "John Rieffel"], "title": "Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks", "comment": null, "summary": "Recently, researchers have explored control methods that embrace nonlinear\ndynamic coupling instead of suppressing it. Such designs leverage dynamical\ncoupling for communication between different parts of the robot. Morphological\ncommunication refers to when those dynamics can be used as an emergent data bus\nto facilitate coordination among independent controller modules within the same\nrobot. Previous research with tensegrity-based robot designs has shown that\nevolutionary learning models that evolve spiking neural networks (SNN) as robot\ncontrol mechanisms are effective for controlling non-rigid robots. Our own\nresearch explores the emergence of morphological communication in an SNN-based\nsimulated soft robot in theEvoGym environment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728SNN\u63a7\u5236\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u5229\u7528\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8026\u5408\u8fdb\u884c\u5f62\u6001\u901a\u4fe1", "motivation": "\u63a2\u7d22\u5229\u7528\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8026\u5408\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u7279\u522b\u662f\u5f62\u6001\u901a\u4fe1\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u5668\u6a21\u5757\u95f4\u7684\u534f\u8c03\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u8fdb\u5316\u4e86\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u63a7\u5236\u673a\u5236\uff0c\u5728EvoGym\u73af\u5883\u4e2d\u6a21\u62df\u4e86\u4e00\u4e2a\u8f6f\u4f53\u673a\u5668\u4eba\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cSNN\u63a7\u5236\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u5728EvoGym\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u5f62\u6001\u901a\u4fe1\u3002", "conclusion": "\u8fdb\u5316\u5b66\u4e60\u6a21\u578b\u548cSNN\u662f\u63a7\u5236\u975e\u521a\u6027\u673a\u5668\u4eba\u548c\u5b9e\u73b0\u5f62\u6001\u901a\u4fe1\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "AI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAleks\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u690d\u7269\u79d1\u5b66\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u690d\u7269\u79d1\u5b66\u4f9d\u8d56\u4e8e\u5927\u578b\u3001\u5f02\u6784\u6570\u636e\u96c6\uff0c\u4f46\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u7684\u6311\u6218\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u7a0b\u3002", "method": "Aleks\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5b83\u5728\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u5185\u6574\u5408\u4e86\u9886\u57df\u77e5\u8bc6\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4ee5\u81ea\u4e3b\u5730\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u3002\u4e00\u65e6\u88ab\u63d0\u4f9b\u7814\u7a76\u95ee\u9898\u548c\u6570\u636e\u96c6\uff0cAleks\u5c31\u4f1a\u5728\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u591a\u4e2a\u5468\u671f\u8fed\u4ee3\u5730\u5236\u5b9a\u95ee\u9898\u3001\u63a2\u7d22\u66ff\u4ee3\u5efa\u6a21\u7b56\u7565\u548c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u5bf9\u8461\u8404\u85e4\u7ea2\u6591\u75c5\u7684\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAleks\u9010\u6b65\u8bc6\u522b\u51fa\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\uff0c\u5e76\u6536\u655b\u5230\u5177\u6709\u7a33\u5065\u6027\u80fd\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002\u5355\u72ec\u7684\u5b9e\u9a8c\u7814\u7a76\u4e86\u9886\u57df\u77e5\u8bc6\u548c\u8bb0\u5fc6\u5bf9\u8fde\u8d2f\u7ed3\u679c\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8fd9\u9879\u63a2\u7d22\u6027\u5de5\u4f5c\u5f3a\u8c03\u4e86 the promise of agentic AI \u4f5c\u4e3a\u52a0\u901f\u690d\u7269\u79d1\u5b66\u7814\u7a76\u7684\u81ea\u4e3b\u534f\u4f5c\u8005\u3002"}}
{"id": "2508.20080", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.20080", "abs": "https://arxiv.org/abs/2508.20080", "authors": ["Changha Shin", "Woong Oh Cho", "Seon Joo Kim"], "title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World Omnidirectional Images", "comment": "Accepted to ICCV 2025. 10 pages main text, 4 figures, 4 tables,\n  supplementary material included", "summary": "360-degree visual content is widely shared on platforms such as YouTube and\nplays a central role in virtual reality, robotics, and autonomous navigation.\nHowever, consumer-grade dual-fisheye systems consistently yield imperfect\npanoramas due to inherent lens separation and angular distortions. In this\nwork, we introduce a novel calibration framework that incorporates a\ndual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach\nnot only simulates the realistic visual artifacts produced by dual-fisheye\ncameras but also enables the synthesis of seamlessly rendered 360-degree\nimages. By jointly optimizing 3D Gaussian parameters alongside calibration\nvariables that emulate lens gaps and angular distortions, our framework\ntransforms imperfect omnidirectional inputs into flawless novel view synthesis.\nExtensive evaluations on real-world datasets confirm that our method produces\nseamless renderings-even from imperfect images-and outperforms existing\n360-degree rendering models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7ba1\u7ebf\u7684\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65e0\u7f1d\u7684\u5168\u666f\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u6d88\u8d39\u7ea7\u53cc\u9c7c\u773c\u7cfb\u7edf\u56e0\u955c\u5934\u5206\u79bb\u548c\u89d2\u5ea6\u7578\u53d8\u5bfc\u81f4\u7684\u5168\u666f\u56fe\u50cf\u4e0d\u5b8c\u7f8e\u95ee\u9898\u3002", "method": "\u5c06\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u6574\u5408\u52303D\u9ad8\u65af\u6cfc\u6e85\u7ba1\u7ebf\u4e2d\uff0c\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u53c2\u6570\u548c\u6a21\u62df\u955c\u5934\u95f4\u9699\u53ca\u89d2\u5ea6\u7578\u53d8\u7684\u6821\u51c6\u53d8\u91cf\uff0c\u4ee5\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u8f93\u5165\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6a21\u62df\u53cc\u9c7c\u773c\u76f8\u673a\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u5e76\u751f\u6210\u65e0\u7f1d\u7684\u5168\u666f\u56fe\u50cf\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u5b8c\u7f8e\u7684\u56fe\u50cf\u4e0a\u4e5f\u80fd\u751f\u6210\u65e0\u7f1d\u6e32\u67d3\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684360\u5ea6\u6e32\u67d3\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u4e0d\u5b8c\u7f8e\u7684\u9c7c\u773c\u8f93\u5165\u8f6c\u5316\u4e3a\u65e0\u7455\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2508.19380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19380", "abs": "https://arxiv.org/abs/2508.19380", "authors": ["Diancheng Li", "Nia Ralston", "Bastiaan Hagen", "Phoebe Tan", "Matthew A. Robertson"], "title": "FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "This paper introduces FlipWalker, a novel underactuated robot locomotion\nsystem inspired by Jacob's Ladder illusion toy, designed to traverse\nchallenging terrains where wheeled robots often struggle. Like the Jacob's\nLadder toy, FlipWalker features two interconnected segments joined by flexible\ncables, enabling it to pivot and flip around singularities in a manner\nreminiscent of the toy's cascading motion. Actuation is provided by\nmotor-driven legs within each segment that push off either the ground or the\nopposing segment, depending on the robot's current configuration. A\nphysics-based model of the underactuated flipping dynamics is formulated to\nelucidate the critical design parameters governing forward motion and obstacle\nclearance or climbing. The untethered prototype weighs 0.78 kg, achieves a\nmaximum flipping speed of 0.2 body lengths per second. Experimental trials on\nartificial grass, river rocks, and snow demonstrate that FlipWalker's flipping\nstrategy, which relies on ground reaction forces applied normal to the surface,\noffers a promising alternative to traditional locomotion for navigating\nirregular outdoor terrain.", "AI": {"tldr": "FlipWalker\u662f\u4e00\u4e2a\u53d7\u96c5\u5404\u5e03\u65af\u4e4b\u68af\u73a9\u5177\u542f\u53d1\u7684\u72ec\u7279\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\uff0c\u80fd\u514b\u670d\u8f6e\u5f0f\u673a\u5668\u4eba\u96be\u4ee5\u5e94\u5bf9\u7684\u590d\u6742\u5730\u5f62\u3002\u8be5\u673a\u5668\u4eba\u5305\u542b\u4e24\u4e2a\u7531\u67d4\u6027\u7535\u7f06\u8fde\u63a5\u7684\u90e8\u4ef6\uff0c\u80fd\u591f\u50cf\u73a9\u5177\u4e00\u6837\u56f4\u7ed5\u5947\u5f02\u70b9\u8fdb\u884c\u7ffb\u8f6c\u8fd0\u52a8\u3002", "motivation": "\u4ecb\u7ecdFlipWalker\uff0c\u4e00\u4e2a\u53d7\u96c5\u5404\u5e03\u65af\u4e4b\u68af\u73a9\u5177\u542f\u53d1\u7684\u72ec\u7279\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\uff0c\u7528\u4e8e\u7a7f\u8d8a\u8f6e\u5f0f\u673a\u5668\u4eba\u96be\u4ee5\u5e94\u5bf9\u7684\u590d\u6742\u5730\u5f62\u3002", "method": "\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u7269\u7406\u6a21\u578b\u6765\u9610\u660e\u63a7\u5236\u524d\u8fdb\u548c\u8d8a\u969c\u7684\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\u3002", "result": "\u8be5\u673a\u5668\u4eba\u91cd0.78\u516c\u65a4\uff0c\u6700\u5927\u7ffb\u8f6c\u901f\u5ea6\u4e3a\u6bcf\u79d20.2\u4e2a\u4f53\u957f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cFlipWalker\u7684\u7ffb\u8f6c\u7b56\u7565\u662f\u4e00\u79cd\u5728\u4e0d\u89c4\u5219\u6237\u5916\u5730\u5f62\u5bfc\u822a\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "FlipWalker\u7684\u7ffb\u8f6c\u7b56\u7565\u662f\u4e00\u79cd\u5728\u4e0d\u89c4\u5219\u6237\u5916\u5730\u5f62\u5bfc\u822a\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.19634", "categories": ["quant-ph", "physics.app-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19634", "abs": "https://arxiv.org/abs/2508.19634", "authors": ["Yujie Sun", "Marek Kopciuch", "Arash Dezhang Fard", "Szymon Pustelny"], "title": "Quantum Process Tomography of a Room-Temperature Alkali-Metal Vapor", "comment": "14 pages, 5 figures", "summary": "Quantum process tomography (QPT) is a technique for reconstructing the\ndynamics of open quantum systems under the Born-Markov approximation, as\ndescribed by a Liouvillian superoperator, capturing both coherent and\ndissipative processes. While QPT is well established for qubits, it presents\nsignificant experimental challenges in multi-level qudits. In room-temperature\natomic vapors, these difficulties arise from complex interactions, residual\nfields present in the system, environmental noise, and inhomogeneities in the\nmedium. Overcoming these limitations is essential for accurate modeling and\nprecise control of such systems--a critical step toward practical usage of the\nQPT. We present a QPT method that we experimentally validate on a\nroom-temperature $^{87}$Rb vapor ensemble, achieving high-fidelity\nreconstruction of qutrit Liouvillians. Our approach establishes a\ncomputationally efficient framework for characterizing open quantum systems,\nenabling the study of non-unitary dynamics, efficient benchmarking of quantum\nsensors, and data-driven identification of environmental noise correlations.", "AI": {"tldr": "This paper presents a new Quantum Process Tomography (QPT) method for multi-level qudits, experimentally validated on a room-temperature atomic vapor. The method achieves high-fidelity reconstruction of qutrit Liouvillians and offers a computationally efficient framework for characterizing open quantum systems.", "motivation": "Existing QPT methods face significant experimental challenges for multi-level qudits, especially in room-temperature atomic vapors, due to complex interactions, residual fields, environmental noise, and medium inhomogeneities. Overcoming these limitations is crucial for accurate modeling, precise control, and practical application of QPT.", "method": "The paper introduces a new QPT method and experimentally validates it on a room-temperature $^{87}$Rb vapor ensemble. The approach establishes a computationally efficient framework for characterizing open quantum systems.", "result": "The experimental validation achieved high-fidelity reconstruction of qutrit Liouvillians in a room-temperature atomic vapor.", "conclusion": "The developed QPT method provides an efficient framework for characterizing open quantum systems, enabling the study of non-unitary dynamics, benchmarking of quantum sensors, and identification of environmental noise correlations."}}
{"id": "2508.20054", "categories": ["cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2508.20054", "abs": "https://arxiv.org/abs/2508.20054", "authors": ["Cipriano Junior Cioffo", "Fabio Gadducci", "Davide Trotta"], "title": "Between Markov and restriction: Two more monads on categories for relations", "comment": null, "summary": "The study of categories abstracting the structural properties of relations\nhas been extensively developed over the years, resulting in a rich and diverse\nbody of work. A previous paper offered a survey providing a modern and\ncomprehensive presentation of these ``categories for relations'' as instances\nof gs-monoidal categories, showing how they arise as Kleisli categories of\nsuitable symmetric monoidal monads. The end result was a taxonomy that\norganised numerous related concepts in the literature, including in particular\nMarkov and restriction categories. This paper further enriches the taxonomy: it\nproposes two categories that are once more instances of gs-monoidal categories,\nyet more abstract than Markov and restriction categories. They are\ncharacterised by an axiomatic notion of mass and domain of an arrow, the latter\none of the key ingredient of restriction categories, which generalises the\ndomain of partial functions. The paper then introduces mass and domain\npreserving monads, proving that the associated Kleisli categories in fact\npreserve the corresponding equations and that these monads arise naturally for\nthe categories of semiring-weighted relations.", "AI": {"tldr": "\u672c\u6587\u5728\u5df2\u6709\u5de5\u4f5c\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u4e30\u5bcc\u4e86\u201c\u5173\u7cfb\u8303\u7574\u201d\u7684\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u66f4\u62bd\u8c61\u7684gs-\u6a21\u8303\u7574\u5b9e\u4f8b\uff0c\u5b83\u4eec\u7531\u8d28\u91cf\u548c\u7bad\u5934\u57df\u7684\u516c\u7406\u5316\u6982\u5ff5\u5b9a\u4e49\uff0c\u5e76\u4e0e\u534a\u73af\u52a0\u6743\u5173\u7cfb\u8303\u7574\u76f8\u5173\u3002", "motivation": "\u5bf9\u73b0\u6709\u201c\u5173\u7cfb\u8303\u7574\u201d\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u4e30\u5bcc\uff0c\u63d0\u51fa\u66f4\u62bd\u8c61\u7684\u8303\u7574\u5b9e\u4f8b\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u66f4\u62bd\u8c61\u7684gs-\u6a21\u8303\u7574\u5b9e\u4f8b\uff0c\u5e76\u5f15\u5165\u8d28\u91cf\u548c\u7bad\u5934\u57df\u7684\u6982\u5ff5\u6765\u8868\u5f81\u5b83\u4eec\u3002\u8bc1\u660e\u4e86\u76f8\u5173\u7684Kleisli\u8303\u7574\u4fdd\u7559\u4e86\u76f8\u5e94\u7684\u65b9\u7a0b\uff0c\u5e76\u4e14\u8fd9\u4e9b\u5355\u5b50\u81ea\u7136\u5730\u51fa\u73b0\u5728\u534a\u73af\u52a0\u6743\u5173\u7cfb\u8303\u7574\u4e2d\u3002", "result": "\u4e24\u4e2a\u65b0\u7684gs-\u6a21\u8303\u7574\u5b9e\u4f8b\u88ab\u63d0\u51fa\uff0c\u5b83\u4eec\u6bd4\u5df2\u6709\u7684Markov\u548crestriction\u8303\u7574\u66f4\u62bd\u8c61\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u548c\u7bad\u5934\u57df\u7684\u516c\u7406\u5316\u6982\u5ff5\u8fdb\u884c\u8868\u5f81\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u4e2a\u65b0\u8303\u7574\u53ca\u5176\u76f8\u5173\u7684\u5355\u5b50\uff0c\u4e3a\u201c\u5173\u7cfb\u8303\u7574\u201d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u89c6\u89d2\u548c\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u3002"}}
{"id": "2508.19263", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19263", "abs": "https://arxiv.org/abs/2508.19263", "authors": ["Anat Heilper", "Doron Singer"], "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats", "comment": "16 pages 9 images", "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.", "AI": {"tldr": "\u672c\u6587\u5c06ZipNN\u65b9\u6cd5\u6269\u5c55\u5230FP8\u548cFP4\u7b49\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u79bb\u548c\u72ec\u7acb\u538b\u7f29\u6307\u6570\u548c\u5c3e\u6570\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\u7387\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e86LLM\u7684K/V\u7f13\u5b58\u5f20\u91cf\u7684\u53ef\u538b\u7f29\u6027\u3002", "motivation": "\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e5\u76ca\u589e\u5927\u548c\u5e7f\u6cdb\u90e8\u7f72\u7684\u80cc\u666f\u4e0b\uff0c\u964d\u4f4e\u6a21\u578b\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06ZipNN\u65b9\u6cd5\u6269\u5c55\u5230FP8\u548cFP4\u7b49\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u79bb\u548c\u72ec\u7acb\u538b\u7f29\u6307\u6570\u548c\u5c3e\u6570\u7ec4\u4ef6\u7684\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u71b5\u7f16\u7801\u8fdb\u884c\u538b\u7f29\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e86K/V\u7f13\u5b58\u5f20\u91cf\u7684\u53ef\u538b\u7f29\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8fbe62%\u7684BF16\u538b\u7f29\u7387\u548c83%\u7684FP8\u538b\u7f29\u7387\uff0c\u5e76\u53d1\u73b0\u4e86K/V\u7f13\u5b58\u5f20\u91cf\u7684\u53ef\u538b\u7f29\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u90e8\u7f72\u65f6\u8282\u7701\u5185\u5b58\u3002", "conclusion": "\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\u548cLLM\u7684K/V\u7f13\u5b58\u5f20\u91cf\u5177\u6709\u826f\u597d\u7684\u53ef\u538b\u7f29\u6027\uff0c\u901a\u8fc7\u6539\u8fdb\u7684ZipNN\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\uff0c\u4ece\u800c\u964d\u4f4e\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u3002"}}
{"id": "2508.19495", "categories": ["cs.DC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19495", "abs": "https://arxiv.org/abs/2508.19495", "authors": ["Muhammad Ahmed Mohsin", "Junaid Ahmad", "Muhammad Hamza Nawaz", "Muhammad Ali Jamshed"], "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks", "comment": "Submitted as a chapter to the book Ambient Intelligence for 6G", "summary": "Ambient intelligence (AmI) is a computing paradigm in which physical\nenvironments are embedded with sensing, computation, and communication so they\ncan perceive people and context, decide appropriate actions, and respond\nautonomously. Realizing AmI at global scale requires sixth generation (6G)\nwireless networks with capabilities for real time perception, reasoning, and\naction aligned with human behavior and mobility patterns. We argue that\nGenerative Artificial Intelligence (GenAI) is the creative core of such\nenvironments. Unlike traditional AI, GenAI learns data distributions and can\ngenerate realistic samples, making it well suited to close key AmI gaps,\nincluding generating synthetic sensor and channel data in under observed areas,\ntranslating user intent into compact, semantic messages, predicting future\nnetwork conditions for proactive control, and updating digital twins without\ncompromising privacy.\n  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,\nand generative transformers, and connects them to practical AmI use cases,\nincluding spectrum sharing, ultra reliable low latency communication,\nintelligent security, and context aware digital twins. We also examine how 6G\nenablers, such as edge and fog computing, IoT device swarms, intelligent\nreflecting surfaces (IRS), and non terrestrial networks, can host or accelerate\ndistributed GenAI. Finally, we outline open challenges in energy efficient on\ndevice training, trustworthy synthetic data, federated generative learning, and\nAmI specific standardization. We show that GenAI is not a peripheral addition,\nbut a foundational element for transforming 6G from a faster network into an\nambient intelligent ecosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u4f5c\u4e3a\u73af\u5883\u667a\u80fd\uff08AmI\uff09\u7684\u6838\u5fc3\u9a71\u52a8\u529b\uff0c\u5e76\u63a2\u8ba8\u5176\u57286G\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684\u901a\u4fe1\u73af\u5883\u3002", "motivation": "\u968f\u7740\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u9002\u5e94\u7684\u7f51\u7edc\u6765\u652f\u6301\u65e5\u76ca\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\u3002\u73af\u5883\u667a\u80fd\uff08AmI\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u8303\u5f0f\uff0c\u4f46\u5176\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u5b9e\u73b0\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u7528\u6237\u610f\u56fe\u7406\u89e3\u548c\u7f51\u7edc\u9884\u6d4b\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63d0\u51faGenAI\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5f25\u5408AmI\u7684\u5dee\u8ddd\uff0c\u5e76\u63a8\u52a86G\u7f51\u7edc\u5411\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u6f14\u8fdb\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ecGANs\u3001VAEs\u3001\u6269\u6563\u6a21\u578b\u548c\u751f\u6210\u5f0fTransformer\uff0c\u5e76\u5c06\u5176\u4e0eAmI\u7684\u5e94\u7528\u573a\u666f\uff08\u5982\u9891\u8c31\u5171\u4eab\u3001\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u3001\u667a\u80fd\u5b89\u5168\u548c\u60c5\u5883\u611f\u77e5\u6570\u5b57\u5b6a\u751f\uff09\u8054\u7cfb\u8d77\u6765\u3002\u540c\u65f6\uff0c\u63a2\u8ba8\u4e866G\u7684\u5173\u952e\u6280\u672f\uff08\u5982\u8fb9\u7f18\u548c\u96fe\u8ba1\u7b97\u3001\u7269\u8054\u7f51\u8bbe\u5907\u7fa4\u3001\u667a\u80fd\u53cd\u5c04\u9762\u548c\u975e\u5730\u9762\u7f51\u7edc\uff09\u5982\u4f55\u652f\u6301\u5206\u5e03\u5f0fGenAI\u3002\u6b64\u5916\uff0c\u8fd8\u5ba1\u89c6\u4e86\u5728\u8bbe\u5907\u7aef\u8bad\u7ec3\u3001\u53ef\u4fe1\u5408\u6210\u6570\u636e\u3001\u8054\u90a6\u751f\u6210\u5b66\u4e60\u548cAmI\u6807\u51c6\u5316\u65b9\u9762\u7684\u5f00\u653e\u6027\u6311\u6218\u3002", "result": "GenAI\u80fd\u591f\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u3001\u7ffb\u8bd1\u7528\u6237\u610f\u56fe\u3001\u9884\u6d4b\u7f51\u7edc\u6761\u4ef6\u548c\u66f4\u65b0\u6570\u5b57\u5b6a\u751f\u6765\u5f25\u5408AmI\u7684\u5173\u952e\u5dee\u8ddd\u30026G\u6280\u672f\u5982\u8fb9\u7f18\u8ba1\u7b97\u3001IRS\u7b49\u80fd\u591f\u652f\u6301\u5206\u5e03\u5f0fGenAI\u7684\u90e8\u7f72\u548c\u52a0\u901f\u3002", "conclusion": "GenAI\u662f\u5b9e\u73b06G\u7f51\u7edc\u73af\u5883\u667a\u80fd\u5316\u7684\u57fa\u7840\u8981\u7d20\uff0c\u800c\u975e\u8fb9\u7f18\u8865\u5145\u3002\u901a\u8fc7\u6574\u5408GenAI\uff0c6G\u80fd\u591f\u4ece\u5355\u7eaf\u7684\u201c\u66f4\u5feb\u7f51\u7edc\u201d\u8f6c\u53d8\u4e3a\u201c\u73af\u5883\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u201d\u3002"}}
{"id": "2508.19271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u4efb\u52a1\u7684\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u6765\u6539\u8fdbRetoMaton\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u9ad8\u6548\u7684LLM\u63a8\u7406\uff0c\u53d6\u4ee3\u4e86\u4e0d\u7a33\u5b9a\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u63d0\u793a\u7c7b\u63a8\u7406\uff08\u5982CoT\u548cICL\uff09\u5728LLM\u4e2d\u4e0d\u7a33\u5b9a\u4e14\u4e0d\u53ef\u9760\uff0c\u800c\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff08\u5982RetoMaton\uff09\u63d0\u4f9b\u4e86\u66f4\u7ed3\u6784\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684WFA\u6765\u589e\u5f3aRetoMaton\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u9ad8\u6548\u7684LLM\u63a8\u7406\u3002", "method": "\u5c06RetoMaton\u7684\u5168\u5c40\u6570\u636e\u5b58\u50a8\u66ff\u6362\u4e3a\u4ece\u5916\u90e8\u8bed\u6599\u5e93\u6784\u5efa\u7684\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528WFA\u7684\u663e\u5f0f\u7ed3\u6784\u6765\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u548c\u6a21\u5757\u5316\u7684\u68c0\u7d22\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eLLaMA-3.2-1B\u548cGemma-3-1B-PT\u6a21\u578b\u3002", "result": "\u5728TriviaQA\u3001GSM8K\u548cMMLU\u4efb\u52a1\u4e0a\uff0c\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u5c40\u90e8RetoMaton\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8LLM\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u68c0\u7d22\u52a8\u6001\u3002", "conclusion": "\u5c40\u90e8RetoMaton\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u81ea\u52a8\u673a\u5f15\u5bfc\u5185\u5b58\uff0c\u4f7f\u73b0\u4ee3LLM\u80fd\u591f\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u7b26\u53f7\u63a8\u7406\uff0c\u4ece\u800c\u5728\u6a21\u578b\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u590d\u73b0\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u6b65\u3002"}}
{"id": "2508.20076", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20076", "abs": "https://arxiv.org/abs/2508.20076", "authors": ["Xiaotong Cheng", "Setareh Maghsudi"], "title": "Anomaly Detection in Networked Bandits", "comment": null, "summary": "The nodes' interconnections on a social network often reflect their\ndependencies and information-sharing behaviors. Nevertheless, abnormal nodes,\nwhich significantly deviate from most of the network concerning patterns or\nbehaviors, can lead to grave consequences. Therefore, it is imperative to\ndesign efficient online learning algorithms that robustly learn users'\npreferences while simultaneously detecting anomalies.\n  We introduce a novel bandit algorithm to address this problem. Through\nnetwork knowledge, the method characterizes the users' preferences and\nresiduals of feature information. By learning and analyzing these preferences\nand residuals, it develops a personalized recommendation strategy for each user\nand simultaneously detects anomalies. We rigorously prove an upper bound on the\nregret of the proposed algorithm and experimentally compare it with several\nstate-of-the-art collaborative contextual bandit algorithms on both synthetic\nand real-world datasets.", "AI": {"tldr": "\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u8282\u70b9\u68c0\u6d4b\u4e0e\u4e2a\u6027\u5316\u63a8\u8350\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f02\u5e38\u8282\u70b9\u53ef\u80fd\u5e26\u6765\u7684\u4e25\u91cd\u540e\u679c\uff0c\u5e76\u63d0\u51fa\u80fd\u7a33\u5065\u5b66\u4e60\u7528\u6237\u504f\u597d\u5e76\u540c\u65f6\u68c0\u6d4b\u5f02\u5e38\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\uff08novel\uff09 the bandit \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u7f51\u7edc\u77e5\u8bc6\u523b\u753b\u7528\u6237\u504f\u597d\u548c\u7279\u5f81\u4fe1\u606f\u7684\u6b8b\u5dee\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u548c\u5206\u6790\u8fd9\u4e9b\u504f\u597d\u548c\u6b8b\u5dee\u6765\u4e3a\u6bcf\u4e2a\u7528\u6237\u5236\u5b9a\u4e2a\u6027\u5316\u63a8\u8350\u7b56\u7565\u5e76\u68c0\u6d4b\u5f02\u5e38\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u9057\u61be\uff08regret\uff09\u4e0a\u9650\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u4e0e\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u534f\u540c\u4e0a\u4e0b\u6587 the bandit \u7b97\u6cd5\u8fdb\u884c\u4e86\u5b9e\u9a8c\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 bandit \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u8282\u70b9\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u63a8\u8350\u95ee\u9898\uff0c\u5e76\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.19497", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19497", "abs": "https://arxiv.org/abs/2508.19497", "authors": ["Koichi Kajiyama"], "title": "Band gap formation theory: An alternative to the Bragg diffraction model", "comment": "12 pages, 6 figures. Submitted to Physical Review B", "summary": "The band gap, a key concept in solid-state physics, is traditionally\nexplained by the Bragg diffraction of electron waves in the periodic potential\nof a crystal. Although widely accepted, this framework raises fundamental\nissues in one-dimensional systems, where Bragg diffraction-which requires\nmultidirectional wave interactions-reduces to simple interference, thus failing\nto explain band gap formation. In this paper, we introduce an alternative\ntheory that does not rely on Bragg reflection. Using the Schr\\\"{o}dinger\nequation for Bloch waves, we consider the crystal lattice as a discrete set of\nobservation points. This discreteness introduces a sampling-like constraint\nanalogous to the Nyquist frequency in signal processing. We show that when the\nelectron wavenumber changes under a periodic potential while the lattice\nspacing remains fixed, a band gap naturally emerges as a sampling effect. By\nconstructing an energy diagram that incorporates this effect, we reveal that\nthe band gap originates from both the wavenumber change and the role of the\nlattice as discrete samplers, leading the energy curve to exhibit translational\nand mirror symmetries with respect to the Nyquist wavenumber. This approach\nprovides a novel, physically grounded explanation of band gap formation and can\nbe naturally extended to higher dimensions.", "AI": {"tldr": "\u4e00\u7ef4\u7cfb\u7edf\u4e2d\u5e03\u62c9\u683c\u884d\u5c04\u65e0\u6cd5\u89e3\u91ca\u5e26\u9699\u5f62\u6210\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u79bb\u6563\u91c7\u6837\u7406\u8bba\u7684\u66ff\u4ee3\u6027\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e03\u62c9\u683c\u884d\u5c04\u7684\u5e26\u9699\u5f62\u6210\u7406\u8bba\u5728\u5904\u7406\u4e00\u7ef4\u7cfb\u7edf\u65f6\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u65b0\u7684\u89e3\u91ca\u3002\u5c24\u5176\u662f\u5728\u4e00\u7ef4\u7cfb\u7edf\u4e2d\uff0c\u5e03\u62c9\u683c\u884d\u5c04\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u5e72\u6d89\uff0c\u65e0\u6cd5\u5145\u5206\u8bf4\u660e\u5e26\u9699\u7684\u5f62\u6210\u3002", "method": "\u5229\u7528\u859b\u5b9a\u8c14\u65b9\u7a0b\u5904\u7406\u5e03\u6d1b\u8d6b\u6ce2\uff0c\u5c06\u6676\u683c\u89c6\u4e3a\u79bb\u6563\u89c2\u6d4b\u70b9\uff0c\u5f15\u5165\u7c7b\u4f3c\u5948\u594e\u65af\u7279\u9891\u7387\u7684\u91c7\u6837\u7ea6\u675f\u3002\u901a\u8fc7\u5206\u6790\u5728\u56fa\u5b9a\u6676\u683c\u95f4\u8ddd\u4e0b\u7535\u5b50\u6ce2\u6570\u53d8\u5316\u65f6\u5e26\u9699\u7684\u4ea7\u751f\uff0c\u6784\u5efa\u5305\u542b\u91c7\u6837\u6548\u5e94\u7684\u80fd\u91cf\u56fe\u3002", "result": "\u53d1\u73b0\u5e26\u9699\u662f\u7531\u4e8e\u6ce2\u6570\u53d8\u5316\u548c\u6676\u683c\u79bb\u6563\u91c7\u6837\u5171\u540c\u4f5c\u7528\u4ea7\u751f\u7684\u91c7\u6837\u6548\u5e94\u3002\u80fd\u91cf\u66f2\u7ebf\u76f8\u5bf9\u4e8e\u5948\u594e\u65af\u7279\u6ce2\u6570\u8868\u73b0\u51fa\u5e73\u79fb\u548c\u955c\u50cf\u5bf9\u79f0\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7b26\u5408\u7269\u7406\u539f\u7406\u7684\u5e26\u9699\u5f62\u6210\u89e3\u91ca\uff0c\u8be5\u7406\u8bba\u53ef\u4ee5\u81ea\u7136\u5730\u6269\u5c55\u5230\u66f4\u9ad8\u7ef4\u5ea6\u3002"}}
{"id": "2508.19530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19530", "abs": "https://arxiv.org/abs/2508.19530", "authors": ["Yanyun Wang", "Dingcui Yu", "Yina Lv", "Yunpeng Song", "Yumiao Zhao", "Liang Shi"], "title": "RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs", "comment": null, "summary": "Quad-level cell (QLC) flash offers significant benefits in cost and capacity,\nbut its limited reliability leads to frequent read retries, which severely\ndegrade read performance. A common strategy in high-density flash storage is to\nprogram selected blocks in a low-density mode (SLC), sacrificing some capacity\nto achieve higher I/O performance. This hybrid storage architecture has been\nwidely adopted in consumer-grade storage systems. However, existing hybrid\nstorage schemes typically focus on write performance and rely solely on data\ntemperature for migration decisions. This often results in excessive mode\nswitching, causing substantial capacity overhead.\n  In this paper, we present RARO (Reliability-Aware Read performance\nOptimization), a hybrid flash management scheme designed to improve read\nperformance with minimal capacity cost. The key insight behind RARO is that\nmuch of the read slowdown in QLC flash is caused by read retries. RARO triggers\ndata migration only when hot data resides in QLC blocks experiencing a high\nnumber of read retries, significantly reducing unnecessary conversions and\ncapacity loss. Moreover, RARO supports fine-grained multi-mode conversions\n(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time\nread retry statistics and flash characteristics, RARO mitigates over-conversion\nand optimizes I/O performance. Experiments on the FEMU platform demonstrate\nthat RARO significantly improves read performance across diverse workloads,\nwith negligible impact on usable capacity.", "AI": {"tldr": "QLC\u95ea\u5b58\u867d\u7136\u6210\u672c\u548c\u5bb9\u91cf\u6709\u4f18\u52bf\uff0c\u4f46\u53ef\u9760\u6027\u8f83\u4f4e\uff0c\u9700\u8981\u9891\u7e41\u91cd\u8bfb\uff0c\u4e25\u91cd\u5f71\u54cd\u8bfb\u53d6\u6027\u80fd\u3002\u73b0\u6709\u6df7\u5408\u5b58\u50a8\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u5199\u5165\u6027\u80fd\uff0c\u4ec5\u57fa\u4e8e\u6570\u636e\u6e29\u5ea6\u8fdb\u884c\u8fc1\u79fb\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u6a21\u5f0f\u5207\u6362\u548c\u5bb9\u91cf\u635f\u5931\u3002\u672c\u6587\u63d0\u51fa\u7684RARO\uff08Reliability-Aware Read performance Optimization\uff09\u662f\u4e00\u79cd\u6df7\u5408\u95ea\u5b58\u7ba1\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u8bfb\u53d6\u6027\u80fd\u5e76\u6700\u5c0f\u5316\u5bb9\u91cf\u6210\u672c\uff0c\u89e3\u51b3\u6b64\u95ee\u9898\u3002RARO\u7684\u5173\u952e\u5728\u4e8e\uff0cQLC\u95ea\u5b58\u7684\u8bfb\u53d6\u51cf\u901f\u4e3b\u8981\u662f\u7531\u91cd\u8bfb\u5f15\u8d77\u7684\u3002RARO\u4ec5\u5728QLC\u5757\u4e2d\u70ed\u6570\u636e\u51fa\u73b0\u5927\u91cf\u91cd\u8bfb\u65f6\u89e6\u53d1\u6570\u636e\u8fc1\u79fb\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u6362\u548c\u5bb9\u91cf\u635f\u5931\u3002\u6b64\u5916\uff0cRARO\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u591a\u6a21\u5f0f\u8f6c\u6362\uff08SLC-TLC-QLC\uff09\uff0c\u4ee5\u8fdb\u4e00\u6b65\u6700\u5c0f\u5316\u5bb9\u91cf\u5f00\u9500\u3002\u901a\u8fc7\u5229\u7528\u5b9e\u65f6\u91cd\u8bfb\u7edf\u8ba1\u6570\u636e\u548c\u95ea\u5b58\u7279\u6027\uff0cRARO\u51cf\u8f7b\u4e86\u8fc7\u5ea6\u8f6c\u6362\uff0c\u5e76\u4f18\u5316\u4e86I/O\u6027\u80fd\u3002\u5728FEMU\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRARO\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8bfb\u53d6\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u53ef\u7528\u5bb9\u91cf\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "motivation": "QLC\u95ea\u5b58\u867d\u7136\u6210\u672c\u548c\u5bb9\u91cf\u6709\u4f18\u52bf\uff0c\u4f46\u53ef\u9760\u6027\u8f83\u4f4e\uff0c\u9700\u8981\u9891\u7e41\u91cd\u8bfb\uff0c\u4e25\u91cd\u5f71\u54cd\u8bfb\u53d6\u6027\u80fd\u3002\u73b0\u6709\u6df7\u5408\u5b58\u50a8\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u5199\u5165\u6027\u80fd\uff0c\u4ec5\u57fa\u4e8e\u6570\u636e\u6e29\u5ea6\u8fdb\u884c\u8fc1\u79fb\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u6a21\u5f0f\u5207\u6362\u548c\u5bb9\u91cf\u635f\u5931\u3002", "method": "RARO\uff08Reliability-Aware Read performance Optimization\uff09\u662f\u4e00\u79cd\u6df7\u5408\u95ea\u5b58\u7ba1\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u8bfb\u53d6\u6027\u80fd\u5e76\u6700\u5c0f\u5316\u5bb9\u91cf\u6210\u672c\uff0c\u89e3\u51b3\u6b64\u95ee\u9898\u3002RARO\u7684\u5173\u952e\u5728\u4e8e\uff0cQLC\u95ea\u5b58\u7684\u8bfb\u53d6\u51cf\u901f\u4e3b\u8981\u662f\u7531\u91cd\u8bfb\u5f15\u8d77\u7684\u3002RARO\u4ec5\u5728QLC\u5757\u4e2d\u70ed\u6570\u636e\u51fa\u73b0\u5927\u91cf\u91cd\u8bfb\u65f6\u89e6\u53d1\u6570\u636e\u8fc1\u79fb\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u6362\u548c\u5bb9\u91cf\u635f\u5931\u3002\u6b64\u5916\uff0cRARO\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u591a\u6a21\u5f0f\u8f6c\u6362\uff08SLC-TLC-QLC\uff09\uff0c\u4ee5\u8fdb\u4e00\u6b65\u6700\u5c0f\u5316\u5bb9\u91cf\u5f00\u9500\u3002\u901a\u8fc7\u5229\u7528\u5b9e\u65f6\u91cd\u8bfb\u7edf\u8ba1\u6570\u636e\u548c\u95ea\u5b58\u7279\u6027\uff0cRARO\u51cf\u8f7b\u4e86\u8fc7\u5ea6\u8f6c\u6362\uff0c\u5e76\u4f18\u5316\u4e86I/O\u6027\u80fd\u3002", "result": "\u5728FEMU\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRARO\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8bfb\u53d6\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u53ef\u7528\u5bb9\u91cf\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "RARO\u901a\u8fc7\u5173\u6ce8\u8bfb\u53d6\u91cd\u8bd5\u6b21\u6570\u548c\u6570\u636e\u70ed\u5ea6\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86QLC\u95ea\u5b58\u7684\u8bfb\u53d6\u6027\u80fd\uff0c\u5e76\u6700\u5c0f\u5316\u4e86\u5bb9\u91cf\u5f00\u9500\u3002"}}
{"id": "2508.19802", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19802", "abs": "https://arxiv.org/abs/2508.19802", "authors": ["Alexander Dobler", "Tim Hegemann", "Martin N\u00f6llenburg", "Alexander Wolff"], "title": "Optimizing Wiggle in Storylines", "comment": "23 pages, 15 figures", "summary": "A storyline visualization shows interactions between characters over time.\nEach character is represented by an x-monotone curve. Time is mapped to the\nx-axis, and groups of characters that interact at a particular point $t$ in\ntime must be ordered consecutively in the y-dimension at $x=t$. The predominant\nobjective in storyline optimization so far has been the minimization of\ncrossings between (blocks of) characters. Building on this work, we investigate\nanother important, but less studied quality criterion, namely the minimization\nof wiggle, i.e., the amount of vertical movement of the characters over time.\nGiven a storyline instance together with an ordering of the characters at any\npoint in time, we show that wiggle count minimization is NP-complete. In\ncontrast, we provide algorithms based on mathematical programming to solve\nlinear wiggle height minimization and quadratic wiggle height minimization\nefficiently. Finally, we introduce a new method for routing character curves\nthat focuses on keeping distances between neighboring curves constant as long\nas they run in parallel. We have implemented our algorithms, and we conduct a\ncase study that explores the differences between the three optimization\nobjectives. We use existing benchmark data, but we also present a new use case\nfor storylines, namely the visualization of rolling stock schedules in railway\noperation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u7684\u4f18\u5316\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u51cf\u5c11\u56fe\u4e2d\u7ebf\u6761\u7684\u6296\u52a8\uff08wiggle\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u7ebf\u6027\u6296\u52a8\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6296\u52a8\u6700\u5c0f\u5316\u95ee\u9898\u7684\u7b97\u6cd5\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u4e2d\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u8d28\u91cf\u6807\u51c6\u2014\u2014\u6700\u5c0f\u5316\u6296\u52a8\uff08wiggle\uff09\uff0c\u5373\u89d2\u8272\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5782\u76f4\u79fb\u52a8\u91cf\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u6296\u52a8\u8ba1\u6570\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u5176\u4e3aNP\u5b8c\u5168\u95ee\u9898\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u5b66\u89c4\u5212\u7684\u7b97\u6cd5\u6765\u6709\u6548\u89e3\u51b3\u7ebf\u6027\u6296\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6296\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002\u6700\u540e\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8def\u7531\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89d2\u8272\u66f2\u7ebf\u5e76\u884c\u65f6\u4fdd\u6301\u5b83\u4eec\u4e4b\u95f4\u7684\u6052\u5b9a\u8ddd\u79bb\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u6296\u52a8\u8ba1\u6570\u6700\u5c0f\u5316\u662fNP\u5b8c\u5168\u95ee\u9898\uff0c\u4f46\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u7ebf\u6027\u6296\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6296\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6700\u5c0f\u5316\u4ea4\u53c9\u3001\u6700\u5c0f\u5316\u7ebf\u6027\u6296\u52a8\u548c\u6700\u5c0f\u5316\u4e8c\u6b21\u6296\u52a8\u8fd9\u4e09\u4e2a\u4f18\u5316\u76ee\u6807\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u5728\u6700\u5c0f\u5316\u4ea4\u53c9\u7684\u57fa\u7840\u4e0a\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u6700\u5c0f\u5316\u6296\u52a8\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u6709\u6548\u7b97\u6cd5\uff0c\u540c\u65f6\u8fd8\u4ecb\u7ecd\u4e86\u65b0\u7684\u8def\u7531\u65b9\u6cd5\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u62d3\u5c55\u4e86\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u5728\u94c1\u8def\u8fd0\u8425\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2508.19439", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19439", "abs": "https://arxiv.org/abs/2508.19439", "authors": ["Jorge L. Gonzalez-Rios", "Eva Lagunas", "Hayder Al-Hraishawi", "Luis M. Garces-Socarras", "Symeon Chatzinotas"], "title": "In-Lab Carrier Aggregation Testbed for Satellite Communication Systems", "comment": null, "summary": "Carrier Aggregation (CA) is a technique used in 5G and previous cellular\ngenerations to temporarily increase the data rate of a specific user during\npeak demand periods or to reduce carrier congestion. CA is achieved by\ncombining two or more carriers and providing a virtual, wider overall bandwidth\nto high-demand users of the system. CA was introduced in the 4G/LTE wireless\nera and has been proven effective in 5G as well, where it is said to play a\nsignificant role in efficient network capacity management. Given this success,\nthe satellite communication (SatCom) community has put its attention into CA\nand the potential benefits it can bring in terms of better spectrum utilization\nand better meeting the user traffic demand. While the theoretical evaluation of\nCA for SatCom has already been presented in several works, this article\npresents the design and results obtained with an experimentation testbed based\non Software Defined Radio (SDR) and a satellite channel emulator. We first\npresent the detailed implementation design, which includes a Gateway (GW)\nmodule responsible for PDU-scheduling across the aggregated carriers, and a\nUser Terminal (UT) module responsible for aggregating the multiple received\nstreams. The second part of the article presents the experimental evaluation,\nincluding CA over a single Geostationary (GEO) satellite, CA over a single\nMedium Earth Orbit (MEO) satellite, and CA combining carriers sent over GEO and\nMEO satellites. A key contribution of this work is the explicit consideration\nof multi-orbit scenarios in the testbed design and validation. The testing\nresults show promising benefits of CA over SatCom systems, motivating potential\nupcoming testing on over-the-air systems.", "AI": {"tldr": "\u8f7d\u6ce2\u805a\u5408\uff08CA\uff09\u662f\u4e00\u79cd\u57285G\u53ca\u4e4b\u524d\u8702\u7a9d\u7f51\u7edc\u4e2d\u7528\u4e8e\u63d0\u9ad8\u7528\u6237\u6570\u636e\u901f\u7387\u548c\u7ba1\u7406\u7f51\u7edc\u62e5\u585e\u7684\u6280\u672f\uff0c\u5b83\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u8f7d\u6ce2\u6765\u63d0\u4f9b\u66f4\u5bbd\u7684\u865a\u62df\u5e26\u5bbd\u3002\u672c\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff08SDR\uff09\u548c\u536b\u661f\u4fe1\u9053\u6a21\u62df\u5668\u7684CA\u5b9e\u9a8c\u5e73\u53f0\uff0c\u5e76\u5bf9\u5355GEO\u536b\u661f\u3001\u5355MEO\u536b\u661f\u4ee5\u53caGEO\u548cMEO\u536b\u661f\u6df7\u5408\u573a\u666f\u4e0b\u7684CA\u6027\u80fd\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCA\u5728\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5728\u5b9e\u9645\u7a7a\u5730\u7cfb\u7edf\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u7684\u5efa\u8bae\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u8f7d\u6ce2\u805a\u5408\uff08CA\uff09\u6280\u672f\u5728\u536b\u661f\u901a\u4fe1\uff08SatCom\uff09\u4e2d\u5229\u7528\u9891\u8c31\u548c\u6ee1\u8db3\u7528\u6237\u6d41\u91cf\u9700\u6c42\u7684\u6f5c\u529b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u7406\u8bba\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e00\u4e2a\u57fa\u4e8eSDR\u548c\u536b\u661f\u4fe1\u9053\u6a21\u62df\u5668\u7684CA\u5b9e\u9a8c\u5e73\u53f0\uff0c\u7279\u522b\u5173\u6ce8\u591a\u8f68\u9053\u573a\u666f\u3002", "method": "\u672c\u6587\u9996\u5148\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u57fa\u4e8eSDR\u548c\u536b\u661f\u4fe1\u9053\u6a21\u62df\u5668\u7684CA\u5b9e\u9a8c\u5e73\u53f0\u7684\u8bbe\u8ba1\uff0c\u5305\u62ec\u8d1f\u8d23\u8de8\u805a\u5408\u8f7d\u6ce2\u8c03\u5ea6PDU\u7684\u7f51\u5173\uff08GW\uff09\u6a21\u5757\u548c\u8d1f\u8d23\u805a\u5408\u591a\u4e2a\u63a5\u6536\u6d41\u7684\u7528\u6237\u7ec8\u7aef\uff08UT\uff09\u6a21\u5757\u3002\u968f\u540e\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86CA\u5728\u5355\u4e00\u9759\u5730\u8f68\u9053\uff08GEO\uff09\u536b\u661f\u3001\u5355\u4e00\u4e2d\u5730\u7403\u8f68\u9053\uff08MEO\uff09\u536b\u661f\u4ee5\u53ca\u7ed3\u5408GEO\u548cMEO\u536b\u661f\u7684\u6df7\u5408\u8f68\u9053\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5355GEO\u536b\u661f\u3001\u5355MEO\u536b\u661f\u4ee5\u53caGEO\u548cMEO\u536b\u661f\u6df7\u5408\u7684\u591a\u79cd\u8f68\u9053\u573a\u666f\u4e0b\uff0cCA\u6280\u672f\u5728\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5747\u5c55\u73b0\u51fa\u6709\u76ca\u7684\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eSDR\u548c\u536b\u661f\u4fe1\u9053\u6a21\u62df\u5668\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8f7d\u6ce2\u805a\u5408\uff08CA\uff09\u6280\u672f\u5bf9\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5728\u591a\u8f68\u9053\u573a\u666f\u4e0b\u7684\u5e94\u7528\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u524d\u666f\uff0c\u8fd9\u4e3a\u672a\u6765\u5728\u5b9e\u9645\u7a7a\u5730\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u52a8\u529b\u3002"}}
{"id": "2508.19550", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.19550", "abs": "https://arxiv.org/abs/2508.19550", "authors": ["Zhongmou Jia", "Yiwen Ma", "Zhongchen Xu", "Xue Yang", "Jianfei Xiao", "Jiezhong He", "Yunteng Shi", "Zhiyuan Zhang", "Duolin Wang", "Sicheng Zhou", "Bingbing Tong", "Peiling Li", "Ziwei Dou", "Xiaohui Song", "Guangtong Liu", "Jie Shen", "Zhaozheng Lyu", "Youguo Shi", "Jiangping Hu", "Li Lu", "Fanming Qu"], "title": "A non-invasive dry-transfer method for fabricating mesoscopic devices on sensitive materials", "comment": null, "summary": "Many materials with novel or exotic properties are highly sensitive to\nenvironmental factors such as air, solvents, and heat, which complicates device\nfabrication and limits their potential applications. Here, we present a\nuniversal submicron fabrication method for mesoscopic devices using a\ndry-transfer technique, tailored specifically for sensitive materials. This\napproach utilizes PMMA masks, combined with a water-dissoluble coating as a\nsacrificial layer, to ensure that sensitive materials are processed without\nexposure to harmful environmental conditions. The entire fabrication process is\ncarried out in a glove box, employing dry techniques that avoid air, solvents,\nand heat exposure, culminating in an encapsulation step. We demonstrate the\nutility of this method by fabricating and characterizing K2Cr3As3 and WTe2\ndevices, a one- and two-dimensional material, respectively. The results show\nthat our technique preserves the integrity of the materials, provides excellent\ncontact interfaces, and is broadly applicable to a range of sensitive\nmaterials.", "AI": {"tldr": "\u63d0\u4f9b\u4e00\u79cd\u7528\u4e8e\u6613\u635f\u6750\u6599\u7684\u901a\u7528\u4e9a\u5fae\u7c73\u5668\u4ef6\u5236\u9020\u7684\u5e72\u8f6c\u5370\u6280\u672f\uff0c\u907f\u514d\u4e86\u7a7a\u6c14\u3001\u6eb6\u5242\u548c\u70ed\u66b4\u9732\u3002", "motivation": "\u8bb8\u591a\u5177\u6709\u65b0\u9896\u6216\u5947\u5f02\u6027\u8d28\u7684\u6750\u6599\u5bf9\u73af\u5883\u56e0\u7d20\uff08\u5982\u7a7a\u6c14\u3001\u6eb6\u5242\u548c\u70ed\uff09\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u4f7f\u5f97\u5668\u4ef6\u5236\u9020\u590d\u6742\u5316\u5e76\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u91c7\u7528\u805a\u7532\u57fa\u4e19\u70ef\u9178\u7532\u916f\uff08PMMA\uff09\u63a9\u6a21\u548c\u6c34\u6eb6\u6027\u6d82\u5c42\u4f5c\u4e3a\u727a\u7272\u5c42\uff0c\u5728\u624b\u5957\u7bb1\u5185\u8fdb\u884c\u5168\u5e72\u6cd5\u52a0\u5de5\uff0c\u907f\u514d\u4e86\u6613\u635f\u6750\u6599\u66b4\u9732\u4e8e\u6709\u5bb3\u73af\u5883\u6761\u4ef6\uff0c\u5e76\u8fdb\u884c\u4e86\u6700\u7ec8\u5c01\u88c5\u3002", "result": "\u901a\u8fc7\u5236\u9020\u548c\u8868\u5f81\u4e00\u7ef4\u6750\u6599K2Cr3As3\u548c\u4e8c\u7ef4\u6750\u6599WTe2\u7684\u5668\u4ef6\uff0c\u8bc1\u660e\u4e86\u8be5\u6280\u672f\u80fd\u591f\u4fdd\u6301\u6750\u6599\u7684\u5b8c\u6574\u6027\uff0c\u63d0\u4f9b\u4f18\u826f\u7684\u63a5\u89e6\u754c\u9762\uff0c\u5e76\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e00\u7cfb\u5217\u6613\u635f\u6750\u6599\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e72\u8f6c\u5370\u6280\u672f\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u5236\u9020\u6613\u635f\u6750\u6599\u4e9a\u5fae\u7c73\u5668\u4ef6\u7684\u65b9\u6cd5\uff0c\u80fd\u4fdd\u6301\u6750\u6599\u7684\u5b8c\u6574\u6027\u5e76\u63d0\u4f9b\u4f18\u826f\u7684\u63a5\u89e6\u754c\u9762\u3002"}}
{"id": "2508.19301", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19301", "abs": "https://arxiv.org/abs/2508.19301", "authors": ["Alejandro Frank"], "title": "Time Symmetry, Retrocausality, and Emergent Collapse: The Tlalpan Interpretation of Quantum Mechanics", "comment": null, "summary": "Quantum mechanics has remained conceptually puzzling since its inception.\nWhile its mathematical formalism provides predictions of unparalleled accuracy,\nthe interpretative framework underpinning measurement and collapse has never\nbeen fully clarified. The Tlalpan Interpretation (QTI) proposes that the\nwavefunction collapse is not a primitive, axiomatic rule but an emergent\nphenomenon, a spontaneous breaking of time symmetry triggered by amplification\nand record creation. The novelty of QTI lies in its embedding of collapse\nwithin the conceptual language of critical phenomena in statistical physics.\nThe theory introduces measurable parameters (the amplification fraction, the\nrecord asymmetry, and the retrocausal coherence time) that serve as order\nparameters describing the quantum-classical transition. Collapse is thus\ncharacterized not as a mysterious fiat but as a phase transition, with\nthresholds and scaling laws analogous to magnetization in ferromagnets. This\nperspective allows the interpretation to retain microscopic time symmetry and\nspatial locality, while attributing quantum correlations and Bell-type\nviolations to retrocausal boundary constraints rather than instantaneous action\nat a distance. Collapse corresponds to the granularization of trajectories: the\nconversion of smooth, time-symmetric quantum evolution into discrete,\nirreversible records. Most importantly, QTI is testable. It predicts: (1) sharp\nthreshold-like disappearance of interference when amplification exceeds a\ncritical value, (2) anomalously fast decay of reversibility in chaotic optical\ncavities compared with regular ones, and (3) qualitatively new interference\nfringes in a time-symmetric generalization of Moshinsky's diffraction in time.\nIf confirmed, these predictions would place collapse squarely in the realm of\nthermodynamic processes, removing the last \"mystical\" postulate from quantum\ntheory.", "AI": {"tldr": "\u91cf\u5b50\u529b\u5b66\uff08QM\uff09\u7684\u89e3\u91ca\u4ecd\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5173\u4e8e\u6d4b\u91cf\u548c\u574d\u584c\u3002Tlalpan \u89e3\u91ca (QTI) \u5c06\u574d\u584c\u89c6\u4e3a\u4e00\u79cd\u6d8c\u73b0\u73b0\u8c61\uff0c\u5373\u7531\u653e\u5927\u548c\u8bb0\u5f55\u521b\u5efa\u89e6\u53d1\u7684\u65f6\u95f4\u5bf9\u79f0\u6027\u81ea\u53d1\u7834\u574f\u3002QTI \u5c06\u574d\u584c\u5d4c\u5165\u7edf\u8ba1\u7269\u7406\u5b66\u7684\u4e34\u754c\u73b0\u8c61\u4e2d\uff0c\u5e76\u5c06\u574d\u584c\u63cf\u8ff0\u4e3a\u4e00\u79cd\u76f8\u53d8\uff0c\u5176\u53c2\u6570\u5305\u62ec\u653e\u5927\u5206\u6570\u3001\u8bb0\u5f55\u4e0d\u5bf9\u79f0\u6027\u548c\u5012\u6eaf\u76f8\u5e72\u65f6\u95f4\u3002", "motivation": "\u9610\u660e\u91cf\u5b50\u529b\u5b66\u7684\u89e3\u91ca\uff0c\u7279\u522b\u662f\u6d4b\u91cf\u548c\u574d\u584c\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u5c06\u574d\u584c\u89c6\u4e3a\u7531\u653e\u5927\u548c\u8bb0\u5f55\u521b\u5efa\u89e6\u53d1\u7684\u65f6\u95f4\u5bf9\u79f0\u6027\u81ea\u53d1\u7834\u574f\u7684\u6d8c\u73b0\u73b0\u8c61\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06\u91cf\u5b50\u574d\u584c\u5d4c\u5165\u7edf\u8ba1\u7269\u7406\u5b66\u7684\u4e34\u754c\u73b0\u8c61\u7684\u89e3\u91ca\u6027\u6846\u67b6\u4e2d\uff0c\u5c06\u574d\u584c\u5b9a\u4e49\u4e3a\u4e00\u79cd\u76f8\u53d8\uff0c\u5e76\u5f15\u5165\u53ef\u6d4b\u91cf\u7684\u53c2\u6570\uff08\u653e\u5927\u5206\u6570\u3001\u8bb0\u5f55\u4e0d\u5bf9\u79f0\u6027\u548c\u5012\u6eaf\u76f8\u5e72\u65f6\u95f4\uff09\u4f5c\u4e3a\u63cf\u8ff0\u91cf\u5b50-\u7ecf\u5178\u8f6c\u53d8\u7684\u5e8f\u53c2\u6570\u3002", "result": "QTI \u63d0\u51fa\u4e86\u4e09\u4e2a\u53ef\u68c0\u9a8c\u7684\u9884\u6d4b\uff1a(1) \u5f53\u653e\u5927\u8d85\u8fc7\u4e34\u754c\u503c\u65f6\uff0c\u5e72\u6d89\u4f1a\u6025\u5267\u6d88\u5931\uff1b(2) \u6df7\u6c8c\u5149\u5b66\u8154\u4e2d\u7684\u53ef\u9006\u6027\u8870\u51cf\u5f02\u5e38\u5feb\uff1b(3) \u5728 Moshinsky \u65f6\u95f4\u884d\u5c04\u7684\u5bf9\u79f0\u63a8\u5e7f\u4e2d\u4f1a\u51fa\u73b0\u65b0\u7684\u5e72\u6d89\u6761\u7eb9\u3002", "conclusion": "QTI \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u529b\u5b66\u89e3\u91ca\uff0c\u5c06\u574d\u584c\u89c6\u4e3a\u4e00\u79cd\u76f8\u53d8\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u68c0\u9a8c\u7684\u9884\u6d4b\uff0c\u5982\u679c\u5f97\u5230\u8bc1\u5b9e\uff0c\u5c06\u4f7f\u91cf\u5b50\u7406\u8bba\u4e2d\u7684\u6700\u540e\u4e00\u6761\u201c\u795e\u79d8\u201d\u516c\u8bbe\u6210\u4e3a\u70ed\u529b\u5b66\u8fc7\u7a0b\u3002"}}
{"id": "2508.19289", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\u548cCLIP-ViT\u5d4c\u5165\uff0c\u4f7f\u7528Isolation Forest\u8fdb\u884c\u5f02\u5e38\u8bc4\u5206\uff0c\u4ee5\u8bc4\u4f30\u6f14\u793a\u5e7b\u706f\u7247\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u7684\u5b9e\u65f6\u53cd\u9988\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8fd1\u4f3c\u89c2\u4f17\u5bf9\u5e7b\u706f\u7247\u8d28\u91cf\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u4e03\u4e2a\u4e13\u5bb6\u542f\u53d1\u7684\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u7a7a\u767d\u3001\u8272\u5f69\u3001\u8fb9\u7f18\u5bc6\u5ea6\u3001\u4eae\u5ea6\u548c\u5bf9\u6bd4\u5ea6\u3001\u6587\u672c\u5bc6\u5ea6\u3001\u8272\u5f69\u534f\u8c03\u6027\u548c\u5e03\u5c40\u5e73\u8861\uff09\u4e0eCLIP-ViT\u5d4c\u5165\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eIsolation Forest\u7684\u5f02\u5e38\u8bc4\u5206\u6765\u8bc4\u4f30\u6f14\u793a\u5e7b\u706f\u7247\u3002", "result": "\u8be5\u65b9\u6cd5\u572812,000\u4e2a\u4e13\u4e1a\u8bb2\u5ea7\u5e7b\u706f\u7247\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u516d\u4e2a\u5b66\u672f\u6f14\u8bb2\uff08115\u5f20\u5e7b\u706f\u7247\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u4e0e\u4eba\u7c7b\u89c6\u89c9\u8d28\u91cf\u8bc4\u5206\u7684\u76f8\u5173\u6027\u65b9\u9762\u8fbe\u5230\u4e860.83\uff0c\u6bd4\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT o4-mini-high\u3001ChatGPT o3\u3001Claude Sonnet 4\u548cGemini 2.5 Pro\uff09\u7684\u5206\u6570\u5f3a1.79\u52303.23\u500d\u3002\u8be5\u65b9\u6cd5\u8fd8\u4e0e\u89c6\u89c9\u8bc4\u5206\u8fdb\u884c\u4e86\u8d8b\u540c\u6548\u5ea6\u9a8c\u8bc1\uff0c\u4e0e\u6f14\u8bb2\u8005\u4ea4\u4ed8\u5206\u6570\u8fdb\u884c\u4e86\u533a\u5206\u6548\u5ea6\u9a8c\u8bc1\uff0c\u5e76\u521d\u6b65\u663e\u793a\u4e0e\u6574\u4f53\u5370\u8c61\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u4f4e\u7ea7\u8bbe\u8ba1\u7ebf\u7d22\u4e0e\u591a\u6a21\u6001\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8fd1\u4f3c\u89c2\u4f17\u5bf9\u5e7b\u706f\u7247\u8d28\u91cf\u7684\u611f\u77e5\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u7684\u5b9e\u65f6\u53cd\u9988\u3002"}}
{"id": "2508.19364", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19364", "abs": "https://arxiv.org/abs/2508.19364", "authors": ["Meiyi Li", "Javad Mohammadi"], "title": "Towards Reliable Neural Optimizers: Permutation-Equivariant Neural Approximation in Dynamic Data Driven Applications Systems", "comment": null, "summary": "Dynamic Data Driven Applications Systems (DDDAS) motivate the development of\noptimization approaches capable of adapting to streaming, heterogeneous, and\nasynchronous data from sensor networks. Many established optimization solvers,\nsuch as branch-and-bound, gradient descent, and Newton-Raphson methods, rely on\niterative algorithms whose step-by-step convergence makes them too slow for\nreal-time, multi-sensor environments. In our recent work, we introduced LOOP-PE\n(Learning to Optimize the Optimization Process, Permutation Equivariance\nversion), a feed-forward neural approximation model with an integrated\nfeasibility recovery function. LOOP-PE processes inputs from a variable number\nof sensors in arbitrary order, making it robust to sensor dropout,\ncommunication delays, and system scaling. Its permutation-equivariant\narchitecture ensures that reordering the input data reorders the corresponding\ndispatch decisions consistently, without retraining or pre-alignment.\nFeasibility is enforced via a generalized gauge map, guaranteeing that outputs\nsatisfy physical and operational constraints. We illustrate the approach in a\nDDDAS-inspired case study of a Virtual Power Plant (VPP) managing multiple\ndistributed generation agents (DERs) to maximize renewable utilization while\nrespecting system limits. Results show that LOOP-PE produces near-optimal,\nfeasible, and highly adaptable decisions under dynamic, unordered, and\ndistributed sensing conditions, significantly outperforming iterative algorithm\nbased solvers in both speed and flexibility. Here, we extend our earlier work\nby providing additional analysis and explanation of LOOP-PE design and\noperation, with particular emphasis on its feasibility guarantee and\npermutation equivariance feature.", "AI": {"tldr": "LOOP-PE\u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u6570\u636e\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7cfb\u7edf(DDDAS)\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u548c\u53ef\u884c\u6027\u6062\u590d\u51fd\u6570\uff0c\u80fd\u591f\u5904\u7406\u6765\u81ea\u4f20\u611f\u5668\u7f51\u7edc\u7684\u6d41\u5f0f\u3001\u5f02\u6784\u548c\u5f02\u6b65\u6570\u636e\u3002", "motivation": "DDDAS\u9700\u8981\u80fd\u591f\u9002\u5e94\u6d41\u5f0f\u3001\u5f02\u6784\u548c\u5f02\u6b65\u6570\u636e\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u800c\u4f20\u7edf\u7684\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\uff08\u5982\u5206\u652f\u5b9a\u754c\u3001\u68af\u5ea6\u4e0b\u964d\u548c\u725b\u987f-\u62c9\u5f17\u68ee\u65b9\u6cd5\uff09\u5728\u8fd9\u79cd\u73af\u5883\u4e0b\u901f\u5ea6\u592a\u6162\u3002", "method": "LOOP-PE\uff08Learning to Optimize the Optimization Process, Permutation Equivariance version\uff09\u662f\u4e00\u79cd\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u53ef\u884c\u6027\u6062\u590d\u51fd\u6570\u3002\u5b83\u91c7\u7528\u6392\u5217\u7b49\u53d8\u67b6\u6784\uff0c\u786e\u4fdd\u8f93\u5165\u6570\u636e\u91cd\u65b0\u6392\u5e8f\u65f6\uff0c\u76f8\u5e94\u7684\u8c03\u5ea6\u51b3\u7b56\u4e5f\u4e00\u81f4\u5730\u91cd\u65b0\u6392\u5e8f\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u9884\u5bf9\u9f50\u3002\u53ef\u884c\u6027\u901a\u8fc7\u5e7f\u4e49\u89c4\u8303\u6620\u5c04\u5f97\u5230\u4fdd\u8bc1\uff0c\u786e\u4fdd\u8f93\u51fa\u6ee1\u8db3\u7269\u7406\u548c\u64cd\u4f5c\u7ea6\u675f\u3002", "result": "\u5728\u865a\u62df\u53d1\u7535\u5382\uff08VPP\uff09\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cLOOP-PE\u5728\u52a8\u6001\u3001\u65e0\u5e8f\u548c\u5206\u5e03\u5f0f\u4f20\u611f\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u751f\u6210\u63a5\u8fd1\u6700\u4f18\u3001\u53ef\u884c\u4e14\u9ad8\u5ea6\u9002\u5e94\u7684\u51b3\u7b56\uff0c\u5176\u901f\u5ea6\u548c\u7075\u6d3b\u6027\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u8fed\u4ee3\u7b97\u6cd5\u7684\u6c42\u89e3\u5668\u3002", "conclusion": "LOOP-PE\u7684\u8bbe\u8ba1\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5bf9\u53ef\u884c\u6027\u4fdd\u8bc1\u548c\u6392\u5217\u7b49\u53d8\u7279\u6027\u7684\u6df1\u5165\u5206\u6790\u548c\u89e3\u91ca\uff0c\u8bc1\u660e\u4e86\u5176\u5728DDDAS\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u548c\u6a21\u4eff\u865a\u5047\u4fe1\u606f\u65b9\u9762\u5bf9\u771f\u5b9e\u6027\u7684\u5f71\u54cd\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86TruthfulnessEval\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4e3b\u6d41\u91cf\u5316\u6280\u672f\uff084\u4f4d\u81f32\u4f4d\uff09\u5bf9\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u771f\u5b9e\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u6613\u53d7\u8bef\u5bfc\u7684\u63d0\u793a\u4e0b\uff0c\u91cf\u5316\u6a21\u578b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\uff0c\u5373\u4f7f\u5b83\u4eec\u5728\u5185\u90e8\u4fdd\u7559\u4e86\u771f\u5b9e\u7684\u8868\u5f81\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u4f1a\u8986\u76d6\u91cf\u5316\u6a21\u578b\u7684\u771f\u5b9e\u6027\u884c\u4e3a\uff0c\u800c\u201c\u8bda\u5b9e\u201d\u548c\u201c\u4e2d\u6027\u201d\u63d0\u793a\u5219\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u8f93\u51fa\u3002\u901a\u8fc7\u5c42\u5206\u6790\u548cPCA\u53ef\u89c6\u5316\uff0c\u7814\u7a76\u8868\u660e\u91cf\u5316\u6a21\u578b\u867d\u7136\u201c\u77e5\u9053\u201d\u771f\u76f8\uff0c\u4f46\u5728\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u7684\u5f15\u5bfc\u4e0b\u4ecd\u4f1a\u4ea7\u751f\u865a\u5047\u8f93\u51fa\uff0c\u8fd9\u4e3a\u672a\u6765\u8bbe\u8ba1\u91cf\u5316\u611f\u77e5\u5bf9\u9f50\u548c\u771f\u5b9e\u6027\u5e72\u9884\u63d0\u4f9b\u4e86\u601d\u8def\u3002", "motivation": "\u8bc4\u4f30\u91cf\u5316\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u65f6\u7684\u771f\u5b9e\u6027\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5176\u5728\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u800c\u5bf9\u771f\u5b9e\u6027\uff08\u751f\u6210\u771f\u5b9e\u6216\u6b3a\u9a97\u6027\u54cd\u5e94\uff09\u7684\u5f71\u54cd\u63a2\u8ba8\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u540d\u4e3aTruthfulnessEval\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u91cf\u5316LLMs\u7684\u771f\u5b9e\u6027\uff1a\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u548c\u6a21\u4eff\u865a\u5047\u4fe1\u606f\u3002\u5728\u8be5\u6846\u67b6\u4e0b\uff0c\u6d4b\u8bd5\u4e86\u4ece4\u4f4d\u52302\u4f4d\u7684\u4e3b\u6d41\u91cf\u5316\u6280\u672f\u5728\u591a\u4e2a\u5f00\u6e90LLMs\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u6d4b\u8bd5\u4e8615\u79cd\u6539\u5199\u540e\u7684\u201c\u8bda\u5b9e\u201d\u3001\u201c\u4e2d\u6027\u201d\u548c\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u5c42\u5206\u6790\u548cPCA\u53ef\u89c6\u5316\u6765\u63a2\u7a76\u91cf\u5316\u6a21\u578b\u5728\u9762\u5bf9\u8fd9\u4e9b\u63d0\u793a\u65f6\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u91cf\u5316\u6a21\u578b\u5728\u5185\u90e8\u4fdd\u7559\u4e86\u771f\u5b9e\u7684\u8868\u5f81\uff0c\u4f46\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u80fd\u591f\u8986\u76d6\u91cf\u5316\u6a21\u578b\u771f\u5b9e\u6027\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u800c\u201c\u8bda\u5b9e\u201d\u548c\u201c\u4e2d\u6027\u201d\u63d0\u793a\u5219\u80fd\u4fdd\u6301\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\u3002\u5c42\u5206\u6790\u548cPCA\u53ef\u89c6\u5316\u8868\u660e\uff0c\u91cf\u5316\u6a21\u578b\u5373\u4f7f\u5728\u5185\u90e8\u201c\u77e5\u9053\u201d\u771f\u76f8\uff0c\u5728\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u7684\u5f15\u5bfc\u4e0b\u4ecd\u4f1a\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002", "conclusion": "\u91cf\u5316\u6280\u672f\u5728\u63d0\u9ad8LLM\u6548\u7387\u7684\u540c\u65f6\uff0c\u4e5f\u53ef\u80fd\u4f7f\u5176\u5728\u9762\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u63d0\u793a\uff08\u5c24\u5176\u662f\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\uff09\u65f6\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u771f\u5b9e\u7684\u8f93\u51fa\u3002\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u91cf\u5316LLM\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\u4e2d\u8003\u8651\u771f\u5b9e\u6027\u5e72\u9884\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u91cf\u5316\u611f\u77e5\u5bf9\u9f50\u548c\u771f\u5b9e\u6027\u5e72\u9884\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.19391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19391", "abs": "https://arxiv.org/abs/2508.19391", "authors": ["Chaoran Zhu", "Hengyi Wang", "Yik Lung Pang", "Changjae Oh"], "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation", "comment": null, "summary": "Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u91cd\u6784\u63a9\u7801\u76ee\u6807\u56fe\u50cf\uff0c\u6a21\u578b\u80fd\u76f4\u63a5\u5b66\u4e60\u89c6\u89c9-\u52a8\u4f5c\u8868\u5f81\uff0c\u65e0\u9700\u673a\u5668\u4eba\u52a8\u4f5c\u76d1\u7763\u3002\u65b0\u6570\u636e\u96c6Omni-Object Pick-and-Place\u5305\u542b180\u7c7b\u7269\u4f53\u548c3200\u4e2a\u5b9e\u4f8b\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u89c6\u89c9-\u6587\u672c\u76f8\u4f3c\u5ea6\uff0c\u518d\u6620\u5c04\u5230\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u8fd9\u79cd\u4e24\u6b65\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u89c6\u89c9-\u6587\u672c\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u964d\u4f4e\u4e86\u64cd\u4f5c\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u63a9\u7801\u76ee\u6807\u56fe\u50cf\u6765\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u4f5c\u4e3a\u6761\u4ef6\u3002\u6b64\u65b9\u6cd5\u65e0\u9700\u673a\u5668\u4eba\u52a8\u4f5c\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u89c6\u89c9-\u52a8\u4f5c\u8868\u5f81\uff0c\u5e76\u53ef\u901a\u8fc7\u5c11\u91cf\u6f14\u793a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u89c6\u89c9-\u52a8\u4f5c\u8868\u5f81\uff0c\u5e76\u901a\u8fc7Omni-Object Pick-and-Place\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.19684", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.19684", "abs": "https://arxiv.org/abs/2508.19684", "authors": ["Ghadeer Elmkaiel", "Syn Schmitt", "Michael Muehlebach"], "title": "Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control", "comment": null, "summary": "Achieving both agile maneuverability and high energy efficiency in aerial\nrobots, particularly in dynamic wind environments, remains challenging.\nConventional thruster-powered systems offer agility but suffer from high energy\nconsumption, while fixed-wing designs are efficient but lack hovering and\nmaneuvering capabilities. We present Floaty, a shape-changing robot that\novercomes these limitations by passively soaring, harnessing wind energy\nthrough intelligent morphological control inspired by birds. Floaty's design is\noptimized for passive stability, and its control policy is derived from an\nexperimentally learned aerodynamic model, enabling precise attitude and\nposition control without active propulsion. Wind tunnel experiments demonstrate\nFloaty's ability to hover, maneuver, and reject disturbances in vertical\nairflows up to 10 m/s. Crucially, Floaty achieves this with a specific power\nconsumption of 10 W/kg, an order of magnitude lower than thruster-powered\nsystems. This introduces a paradigm for energy-efficient aerial robotics,\nleveraging morphological intelligence and control to operate sustainably in\nchallenging wind conditions.", "AI": {"tldr": "Floaty\u662f\u4e00\u79cd\u80fd\u591f\u5229\u7528\u98ce\u80fd\u8fdb\u884c\u53d8\u5f62\u7684\u7a7a\u4e2d\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6a21\u4eff\u9e1f\u7c7b\u7684\u5f62\u6001\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u548c\u654f\u6377\u6027\uff0c\u80fd\u5728\u9ad8\u8fbe10\u7c73/\u79d2\u7684\u5782\u76f4\u6c14\u6d41\u4e2d\u76d8\u65cb\u3001\u673a\u52a8\u5e76\u6291\u5236\u5e72\u6270\uff0c\u5176\u6bd4\u529f\u7387\u6d88\u8017\u6bd4\u4f20\u7edf\u63a8\u8fdb\u7cfb\u7edf\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u89e3\u51b3\u7a7a\u4e2d\u673a\u5668\u4eba\uff08\u7279\u522b\u662f\u822a\u7a7a\u5668\uff09\u5728\u52a8\u6001\u98ce\u73af\u5883\u4e2d\u540c\u65f6\u5b9e\u73b0\u654f\u6377\u673a\u52a8\u6027\u548c\u9ad8\u80fd\u6548\u7684\u6311\u6218\uff0c\u4f20\u7edf\u63a8\u8fdb\u7cfb\u7edf\u80fd\u8017\u9ad8\uff0c\u56fa\u5b9a\u7ffc\u8bbe\u8ba1\u7f3a\u4e4f\u60ac\u505c\u548c\u673a\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFloaty\u7684\u53d8\u5f62\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u88ab\u52a8\u7ff1\u7fd4\u548c\u667a\u80fd\u5f62\u6001\u63a7\u5236\uff08\u6a21\u4eff\u9e1f\u7c7b\uff09\u6765\u5229\u7528\u98ce\u80fd\u3002\u5176\u8bbe\u8ba1\u6ce8\u91cd\u88ab\u52a8\u7a33\u5b9a\u6027\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u5b66\u4e60\u7684\u6c14\u52a8\u6a21\u578b\u8fdb\u884c\u59ff\u6001\u548c\u4f4d\u7f6e\u63a7\u5236\uff0c\u65e0\u9700\u4e3b\u52a8\u63a8\u8fdb\u3002", "result": "Floaty\u5728\u98ce\u6d1e\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u9ad8\u8fbe10\u7c73/\u79d2\u7684\u5782\u76f4\u6c14\u6d41\u4e2d\u60ac\u505c\u3001\u673a\u52a8\u548c\u6291\u5236\u5e72\u6270\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u6bd4\u529f\u7387\u6d88\u8017\u4e3a10\u74e6/\u5343\u514b\uff0c\u6bd4\u63a8\u8fdb\u7cfb\u7edf\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "Floaty\u901a\u8fc7\u5f62\u6001\u667a\u80fd\u548c\u63a7\u5236\u7684\u7ed3\u5408\uff0c\u4e3a\u80fd\u6548\u9ad8\u7684\u7a7a\u4e2d\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6076\u52a3\u7684\u98ce\u51b5\u4e0b\u53ef\u6301\u7eed\u8fd0\u884c\u3002"}}
{"id": "2508.19428", "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.19428", "abs": "https://arxiv.org/abs/2508.19428", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eLLM4OL 2025\u6311\u6218\u7684\u5168\u9762\u7cfb\u7edf\uff0c\u6db5\u76d6\u672c\u4f53\u6784\u5efa\u7684\u4e09\u4e2a\u4efb\u52a1\uff1a\u672f\u8bed\u63d0\u53d6\u3001\u7c7b\u578b\u5206\u914d\u548c\u5206\u7c7b\u6cd5\u53d1\u73b0\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u3001\u96f6\u6837\u672c\u5206\u7c7b\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u6a21\u578b\u3002", "motivation": "\u4e3aLLM4OL 2025\u6311\u6218\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u672c\u4f53\u6784\u5efa\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5305\u62ec\u672f\u8bed\u63d0\u53d6\u3001\u7c7b\u578b\u5206\u914d\u548c\u5206\u7c7b\u6cd5\u53d1\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4efb\u52a1A\uff1a\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u6c34\u7ebf\u8054\u5408\u63d0\u53d6\u9886\u57df\u7279\u5b9a\u672f\u8bed\u53ca\u5176\u672c\u4f53\u7c7b\u578b\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u30022. \u4efb\u52a1B\uff1a\u5728\u6709\u6807\u7b7e\u6570\u636e\u7684\u5c11\u6837\u672c\u573a\u666f\u4e0b\u91cd\u7528RAG\u65b9\u6848\uff0c\u5728\u65e0\u6807\u7b7e\u6570\u636e\u7684\u96f6\u6837\u672c\u573a\u666f\u4e0b\u4f7f\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5f97\u5206\u7684\u96f6\u6837\u672c\u5206\u7c7b\u5668\u30023. \u4efb\u52a1C\uff1a\u5c06\u5206\u7c7b\u6cd5\u53d1\u73b0\u5efa\u6a21\u4e3a\u56fe\u63a8\u7406\uff0c\u4f7f\u7528\u7c7b\u578b\u6807\u7b7e\u7684\u5d4c\u5165\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u9884\u6d4b\u201cis-a\u201d\u5173\u7cfb\u3002", "result": "\u5728LLM4OL 2025\u6311\u6218\u7684\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6392\u540d\u9760\u524d\uff08top-ranking\uff09\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u7684\u672c\u4f53\u5b66\u4e60\u67b6\u6784\u5728\u5f02\u6784\u57df\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u7684\u67b6\u6784\u5728\u672c\u4f53\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5176\u6a21\u5757\u5316\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u5728LLM4OL 2025\u6311\u6218\u7684\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002"}}
{"id": "2508.19277", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19277", "abs": "https://arxiv.org/abs/2508.19277", "authors": ["Xinyu Li", "Tianjin Huang", "Ronghui Mu", "Xiaowei Huang", "Gaojie Jin"], "title": "POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization", "comment": null, "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nenhanced the reasoning capabilities of large language models (LLMs), enabling\nsophisticated problem-solving through explicit multi-step reasoning traces.\nHowever, these enhanced reasoning processes introduce novel attack surfaces,\nparticularly vulnerabilities to computational inefficiency through\nunnecessarily verbose reasoning chains that consume excessive resources without\ncorresponding performance gains. Prior overthinking attacks typically require\nrestrictive conditions including access to external knowledge sources for data\npoisoning, reliance on retrievable poisoned content, and structurally obvious\ntemplates that limit practical applicability in real-world scenarios. To\naddress these limitations, we propose POT (Prompt-Only OverThinking), a novel\nblack-box attack framework that employs LLM-based iterative optimization to\ngenerate covert and semantically natural adversarial prompts, eliminating\ndependence on external data access and model retrieval. Extensive experiments\nacross diverse model architectures and datasets demonstrate that POT achieves\nsuperior performance compared to other methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOT\uff08Prompt-Only OverThinking\uff09\u7684\u65b0\u578b\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u5197\u957f\u4f46\u770b\u4f3c\u81ea\u7136\u7684\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\u6765\u964d\u4f4e\u5176\u6548\u7387\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5916\u90e8\u77e5\u8bc6\u6216\u8fdb\u884c\u6a21\u578b\u68c0\u7d22\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u6709\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u3001\u53ef\u68c0\u7d22\u7684\u6076\u610f\u5185\u5bb9\u4ee5\u53ca\u7ed3\u6784\u660e\u663e\u7684\u6a21\u677f\u7b49\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u5177\u5b9e\u7528\u6027\u548c\u9690\u853d\u6027\u7684\u653b\u51fb\u6846\u67b6\u3002", "method": "\u5229\u7528\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u751f\u6210\u9690\u853d\u4e14\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\uff0c\u5b9e\u73b0\u201c\u63d0\u793a\u8bcd\u4ec5\u5305\u542b\u8fc7\u5ea6\u601d\u8003\u201d\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPOT\u76f8\u8f83\u4e8e\u5176\u4ed6\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "POT\u662f\u4e00\u79cd\u6709\u6548\u7684\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9690\u853d\u4e14\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19559", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19559", "abs": "https://arxiv.org/abs/2508.19559", "authors": ["Rongzhi Li", "Ruogu Du", "Zefang Chu", "Sida Zhao", "Chunlei Han", "Zuocheng Shi", "Yiwen Shao", "Huanle Han", "Long Huang", "Zherui Liu", "Shufan Liu"], "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.", "AI": {"tldr": "HeteroScale\u662f\u4e00\u4e2a\u534f\u8c03\u7684\u81ea\u52a8\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u548c\u65b0\u9896\u7684\u6307\u6807\u9a71\u52a8\u7b56\u7565\u89e3\u51b3\u4e86Prefill-Decode (P/D) \u5206\u79bb\u67b6\u6784\u4e2d\u7684LLM\u670d\u52a1\u6311\u6218\uff0c\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u5e76\u8282\u7701\u4e86\u5927\u91cfGPU\u5c0f\u65f6\u6570\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u52a8\u6269\u5c55\u5668\u5728\u4e3a\u73b0\u4ee3Prefill-Decode (P/D) \u5206\u79bb\u67b6\u6784\u63d0\u4f9bLLM\u670d\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5f02\u6784\u786c\u4ef6\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3001\u7f51\u7edc\u74f6\u9888\u548c\u9884\u5904\u7406-\u89e3\u7801\u9636\u6bb5\u5931\u8861\u3002", "method": "HeteroScale\u7ed3\u5408\u4e86\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u5668\uff08\u9002\u5e94\u5f02\u6784\u786c\u4ef6\u548c\u7f51\u7edc\u7ea6\u675f\uff09\u548c\u65b0\u9896\u7684\u6307\u6807\u9a71\u52a8\u7b56\u7565\uff08\u57fa\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u81ea\u52a8\u6269\u5c55\u4fe1\u53f7\u7684\u5b9e\u8bc1\u7814\u7a76\uff09\uff0c\u901a\u8fc7\u5355\u4e00\u7684\u5065\u58ee\u6307\u6807\u8054\u5408\u6269\u5c55\u9884\u5904\u7406\u548c\u89e3\u7801\u6c60\u3002", "result": "\u5728\u6570\u4e07\u4e2aGPU\u7684\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u7684HeteroScale\uff0c\u5c06\u5e73\u5747GPU\u5229\u7528\u7387\u63d0\u9ad8\u4e8626.6\u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u5929\u8282\u7701\u4e86\u6570\u5341\u4e07\u4e2aGPU\u5c0f\u65f6\u6570\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u4e25\u683c\u7684\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u3002", "conclusion": "HeteroScale\u901a\u8fc7\u4fdd\u6301\u67b6\u6784\u5e73\u8861\u5e76\u786e\u4fdd\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86P/D\u5206\u79bbLLM\u670d\u52a1\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2508.19272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE\u662f\u4e00\u4e2a\u804a\u5929\u5f0f\u6807\u6ce8\u5e73\u53f0\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\uff0c\u4ee5\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5df2\u6210\u529f\u7528\u4e8e\u6784\u5efa\u6570\u5343\u6b21\u5bf9\u8bdd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3aLLM\u53ef\u80fd\u4f1a\u4ea7\u751f\u770b\u4f3c\u6b63\u786e\u4f46\u5305\u542b\u865a\u5047\u4fe1\u606f\u7684\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u80fd\u591f\u8bc4\u4f30\u6b64\u7c7b\u5bf9\u8bdd\u7684\u57fa\u51c6\u6d4b\u8bd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aRAGAPHENE\u7684\u804a\u5929\u5f0f\u6807\u6ce8\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u5141\u8bb8\u6807\u6ce8\u4eba\u5458\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\uff0c\u4ee5\u4fbf\u4e3aLLM\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6536\u96c6\u6570\u636e\u3002", "result": "\u8be5\u5e73\u53f0\u5df2\u6210\u529f\u5438\u5f15\u4e86\u7ea640\u540d\u6807\u6ce8\u4eba\u5458\uff0c\u5e76\u7528\u4e8e\u6784\u5efa\u4e86\u6570\u5343\u6b21\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\uff0c\u4e3aLLM\u7684\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "RAGAPHENE\u5e73\u53f0\u4e3a\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u548c\u8bc4\u4f30LLM\u5728RAG\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19656", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19656", "abs": "https://arxiv.org/abs/2508.19656", "authors": ["Polykarpos Vergos", "Theofanis Vergos", "Florentia Afentaki", "Konstantinos Balaskas", "Georgios Zervakis"], "title": "Support Vector Machines Classification on Bendable RISC-V", "comment": "Accepted for publication at the IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI '25)", "summary": "Flexible Electronics (FE) technology offers uniquecharacteristics in\nelectronic manufacturing, providing ultra-low-cost, lightweight, and\nenvironmentally-friendly alternatives totraditional rigid electronics. These\ncharacteristics enable a rangeof applications that were previously constrained\nby the costand rigidity of conventional silicon technology. Machine learning\n(ML) is essential for enabling autonomous, real-time intelligenceon devices\nwith smart sensing capabilities in everyday objects. However, the large feature\nsizes and high power consumption ofthe devices oppose a challenge in the\nrealization of flexible ML applications. To address the above, we propose an\nopen-source framework for developing ML co-processors for the Bendable RISC-V\ncore. In addition, we present a custom ML accelerator architecture for Support\nVector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)\nalgorithms. Our ML accelerator adopts a generic, precision-scalable design,\nsupporting 4-, 8-, and 16-bit weight representations. Experimental results\ndemonstrate a 21x improvement in both inference execution time and energy\nefficiency, on average, highlighting its potential for low-power, flexible\nintelligence on the edge.", "AI": {"tldr": "Flexible electronics (FE) coupled with machine learning (ML) faces challenges due to device size and power consumption. This paper presents an open-source framework for ML co-processors for the Bendable RISC-V core and a custom ML accelerator for SVM (OvO and OvR) with precision scalability (4, 8, 16-bit weights). Experiments show a 21x average improvement in inference time and energy efficiency, enabling low-power, flexible edge intelligence.", "motivation": "To address the challenges of large feature sizes and high power consumption in flexible ML applications, which hinder the realization of intelligent devices with smart sensing capabilities.", "method": "Proposed an open-source framework for developing ML co-processors for the Bendable RISC-V core. Presented a custom ML accelerator architecture for Support Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR) algorithms, with a generic, precision-scalable design supporting 4-, 8-, and 16-bit weight representations.", "result": "Experimental results demonstrated an average of 21x improvement in both inference execution time and energy efficiency.", "conclusion": "The proposed ML accelerator architecture shows significant potential for low-power, flexible intelligence on the edge, overcoming the limitations of current flexible ML applications."}}
{"id": "2508.19898", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19898", "abs": "https://arxiv.org/abs/2508.19898", "authors": ["Yannic Maus", "Tijn de Vos"], "title": "Distributed Sparsest Cut via Eigenvalue Estimation", "comment": "To be presented as brief announcement at DISC 2025", "summary": "We give new, improved bounds for approximating the sparsest cut value or in\nother words the conductance $\\phi$ of a graph in the CONGEST model. As our main\nresult, we present an algorithm running in $O(\\log^2 n/\\phi)$ rounds in which\nevery vertex outputs a value $\\tilde \\phi$ satisfying $\\phi \\le \\tilde \\phi \\le\n\\sqrt{2.01\\phi}$. In most regimes, our algorithm improves significantly over\nthe previously fastest algorithm for the problem [Chen, Meierhans, Probst\nGutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way\nconductance.\n  We obtain these results, by approximating the eigenvalues of the normalized\nLaplacian matrix $L:=I-\\rm{Deg}^{-1/2}A\\rm{Deg}^ {-1/2}$, where, $A$ is the\nadjacency matrix and $\\rm{Deg}$ is the diagonal matrix with the weighted\ndegrees on the diagonal. The previous state of the art sparsest cut algorithm\nis in the technical realm of expander decompositions. Our algorithms, on the\nother hand, are relatively simple and easy to implement. At the core, they rely\non the well-known power method, which comes down to repeatedly multiplying the\nLaplacian with a vector. This operation can be performed in a single round in\nthe CONGEST model. All our algorithms apply to weighted, undirected graphs. Our\nlower bounds apply even in unweighted graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728CONGEST\u6a21\u578b\u4e0b\u903c\u8fd1\u56fe\u7684\u7a00\u758f\u5272\u503c\uff08\u6216\u7535\u5bfc\u7387$\\phi$\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5176\u8fd0\u884c\u8f6e\u6570\u4e3a $O(\\log^2 n/\\phi)$\uff0c\u5e76\u80fd\u5728 $\\phi \\le \\tilde \\phi \\le \\sqrt{2.01\\phi}$ \u7684\u8303\u56f4\u5185\u5f97\u5230\u8fd1\u4f3c\u503c\u3002", "motivation": "\u5728CONGEST\u6a21\u578b\u4e0b\uff0c\u4e3a\u56fe\u7684\u7a00\u758f\u5272\u503c\uff08\u6216\u7535\u5bfc\u7387$\\\\phi$\uff09\u63d0\u4f9b\u65b0\u7684\u3001\u6539\u8fdb\u7684\u903c\u8fd1\u754c\u9650\u3002", "method": "\u901a\u8fc7\u903c\u8fd1\u5f52\u4e00\u5316\u62c9\u666e\u62c9\u65af\u77e9\u9635 $L:=I-\\\\rm{Deg}^{-1/2}A\\\\rm{Deg}^{-1/2}$ \u7684\u7279\u5f81\u503c\uff0c\u8be5\u65b9\u6cd5\u6838\u5fc3\u662f\u5229\u7528\u529f\u7387\u8fed\u4ee3\u6cd5\uff08\u91cd\u590d\u5c06\u62c9\u666e\u62c9\u65af\u77e9\u9635\u4e0e\u5411\u91cf\u76f8\u4e58\uff09\uff0c\u6bcf\u6b21\u4e58\u6cd5\u53ef\u4ee5\u5728CONGEST\u6a21\u578b\u4e0b\u7684\u4e00\u8f6e\u5185\u5b8c\u6210\u3002", "result": "\u63d0\u51fa\u4e00\u79cd\u5728 $O(\\log^2 n/\\phi)$ \u8f6e\u5185\u8fd0\u884c\u7684\u7b97\u6cd5\uff0c\u6bcf\u4e2a\u9876\u70b9\u8f93\u51fa\u4e00\u4e2a\u6ee1\u8db3 $\\\\phi \\le \\tilde \\phi \\le \\sqrt{2.01\\phi}$ \u7684\u503c $\\\\tilde \\phi$\uff0c\u8be5\u7b97\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u5148\u524d\u6700\u5feb\u7684\u7b97\u6cd5 [Chen, Meierhans, Probst Gutenberg, Saranurak; SODA 25]\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230k\u8def\u7535\u5bfc\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u529f\u7387\u8fed\u4ee3\u7684\u7b97\u6cd5\u76f8\u6bd4\u4e8e\u5148\u524d\u57fa\u4e8e\u6269\u5c55\u5668\u5206\u89e3\u7684\u7b97\u6cd5\u66f4\u4e3a\u7b80\u5355\u4e14\u6613\u4e8e\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u52a0\u6743\u65e0\u5411\u56fe\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u4e0b\u754c\u751a\u81f3\u9002\u7528\u4e8e\u65e0\u6743\u56fe\u3002"}}
{"id": "2508.19522", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19522", "abs": "https://arxiv.org/abs/2508.19522", "authors": ["Si Wang", "Guoqiang Xiao"], "title": "Fourth-Order Hierarchical Array: A Novel Scheme for Sparse Array Design Based on Fourth-Order Difference Co-Array", "comment": "Sparse linear array, fourth-order cumulant, mutual coupling,\n  redundancy, direction of arrival estimation", "summary": "Conventional array designs based on circular fourth-order cumulant typically\nadopt a single expression form of the fourth-order difference co-array (FODCA),\nwhich limits the achievable degrees of freedom (DOFs) and neglects the impact\nof mutual coupling among physical sensors. To address above issues, this paper\nproposes a novel scheme to design arrays with increased DOFs by combining\ndifferent forms of FODCA while accounting for mutual coupling. A novel\nfourth-order hierarchical array (FOHA) based on different forms of FODCA is\nconstructed using an arbitrary generator set. The analytical expression between\nthe coupling leakage of the generator and the resulting FOHA is derived. Two\nspecific FOHA configurations are presented with closed-form sensor placements.\nThe arrays not only offer increased DOFs for resolving more sources in\ndirection of-arrival (DOA) estimation but also effectively suppress mutual\ncoupling. Moreover, the redundancy of FODCA is examined, and it is shown that\narrays based on the proposed scheme achieve lower redundancy compared to\nexisting arrays based on FODCA. Meanwhile, the necessary and sufficient\nconditions for signal reconstruction by FOHA are derived. Compared with\nexisting arrays based on FODCA, the proposed arrays provide enhanced DOFs and\nimproved robustness against mutual coupling. Numerical simulations verify that\nFOHAs achieve superior DOA estimation performance compared with other sparse\nlinear arrays.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56db\u9636\u5206\u998f\u9635\u5217\uff08FOHA\uff09\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u5f62\u5f0f\u7684\u56db\u9636\u5dee\u5206\u534f\u65b9\u5dee\u9635\uff08FODCA\uff09\u5e76\u8003\u8651\u4e92\u8026\u6548\u5e94\uff0c\u5b9e\u73b0\u4e86\u81ea\u7531\u5ea6\uff08DOFs\uff09\u7684\u63d0\u5347\u548c\u4e92\u8026\u7684\u6291\u5236\uff0c\u63d0\u9ad8\u4e86DOA\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u5706\u5f62\u56db\u9636\u7d2f\u79ef\u91cf\u9635\u5217\u8bbe\u8ba1\u4e2d\uff0c\u56e0\u91c7\u7528\u5355\u4e00\u5f62\u5f0f\u7684FODCA\u800c\u5bfc\u81f4\u7684DOFs\u53d7\u9650\u4ee5\u53ca\u5ffd\u7565\u4e92\u8026\u6548\u5e94\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u4e0d\u540c\u5f62\u5f0fFODCA\u7684\u65b0\u578b\u56db\u9636\u5206\u998f\u9635\u5217\uff08FOHA\uff09\uff0c\u63a8\u5bfc\u4e86\u53d1\u7535\u673a\u8026\u5408\u6cc4\u6f0f\u4e0e\u6240\u5f97FOHA\u4e4b\u95f4\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5177\u6709\u95ed\u5f0f\u4f20\u611f\u5668\u4f4d\u7f6e\u7684FOHA\u914d\u7f6e\u3002", "result": "\u63d0\u51fa\u7684FOHA\u9635\u5217\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684DOFs\u4ee5\u5206\u8fa8\u66f4\u591a\u4fe1\u6e90\u8fdb\u884cDOA\u4f30\u8ba1\uff0c\u800c\u4e14\u6709\u6548\u6291\u5236\u4e86\u4e92\u8026\u6548\u5e94\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u4e8eFODCA\u7684\u9635\u5217\u76f8\u6bd4\uff0cFOHA\u9635\u5217\u5177\u6709\u66f4\u4f4e\u7684\u5197\u4f59\u5ea6\uff0c\u5e76\u63a8\u5bfc\u4e86FOHA\u5b9e\u73b0\u4fe1\u53f7\u91cd\u5efa\u7684\u5145\u8981\u6761\u4ef6\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFOHA\u7684DOA\u4f30\u8ba1\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u7a00\u758f\u7ebf\u6027\u9635\u5217\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684FOHA\u9635\u5217\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u5f62\u5f0f\u7684FODCA\u5e76\u8003\u8651\u4e92\u8026\u6548\u5e94\uff0c\u5728\u63d0\u5347DOFs\u548c\u6291\u5236\u4e92\u8026\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u9635\u5217\uff0c\u4ece\u800c\u5728DOA\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19586", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19586", "abs": "https://arxiv.org/abs/2508.19586", "authors": ["Xue-Jin Zhang", "Jin Cao", "Lulu Xiong", "Hui Wang", "Shen Lai", "Cong Xiao", "Shengyuan A. Yang"], "title": "Intrinsic nonlinear valley Nernst effect", "comment": null, "summary": "We investigate the intrinsic nonlinear valley Nernst effect, which induces a\ntransverse valley current via a second-order thermoelectric response to a\nlongitudinal temperature gradient. The effect arises from the Berry connection\npolarizability dipole of valley electrons and is permissible in both\ninversion-symmetric and inversion-asymmetric materials. We demonstrate that the\nresponse tensor is connected to the intrinsic nonlinear valley Hall\nconductivity through a generalized Mott relation, with the two being directly\nproportional at low temperatures, scaled by the Lorenz number. We elucidate the\nsymmetry constraints governing this effect and develop a theory for its\nnonlocal measurement, revealing a nonlocal second-harmonic signal with a\ndistinct $\\rho^2$ scaling. This signal comprises two scaling terms, with their\nratio corresponding to the square of the thermopower normalized by the Lorenz\nnumber. Key characteristics are demonstrated using a tilted Dirac model and\nfirst-principles calculations on bilayer WTe$_2$. Possible extrinsic\ncontributions and alternative experimental detection methods, e.g., by valley\npumping and by nonreciprocal directional dichroism, are discussed. These\nfindings underscore the significance of band quantum geometry on electron\ndynamics and establish a theoretical foundation for nonlinear valley\ncaloritronics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5185\u5728\u975e\u7ebf\u6027\u8c37Nernst\u6548\u5e94\uff0c\u8fd9\u662f\u4e00\u79cd\u7531\u7eb5\u5411\u6e29\u5ea6\u68af\u5ea6\u5f15\u8d77\u7684\u4e8c\u9636\u70ed\u7535\u54cd\u5e94\uff0c\u4f1a\u4ea7\u751f\u6a2a\u5411\u8c37\u6d41\u3002\u8be5\u6548\u5e94\u6e90\u4e8e\u8c37\u7535\u5b50\u7684\u8d1d\u91cc\u8fde\u63a5\u6781\u5316\u7387\u5076\u6781\u5b50\uff0c\u5e76\u5b58\u5728\u4e8e\u5bf9\u79f0\u548c\u4e0d\u5bf9\u79f0\u6750\u6599\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u54cd\u5e94\u5f20\u91cf\u4e0e\u5185\u5728\u975e\u7ebf\u6027\u8c37\u970d\u5c14\u7535\u5bfc\u7387\u901a\u8fc7\u5e7f\u4e49Mott\u5173\u7cfb\u76f8\u8fde\uff0c\u5728\u4f4e\u6e29\u4e0b\u4e24\u8005\u6210\u6b63\u6bd4\uff0c\u6bd4\u4f8b\u7cfb\u6570\u4e3a\u6d1b\u4f26\u5179\u6570\u3002\u8bba\u6587\u9610\u8ff0\u4e86\u6b64\u6548\u5e94\u7684\u5bf9\u79f0\u6027\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5c40\u57df\u4e8c\u9636\u8c10\u6ce2\u4fe1\u53f7\u5b9e\u73b0\uff0c\u8be5\u4fe1\u53f7\u5177\u6709\u72ec\u7279\u7684 $\rho^2$ \u6807\u5ea6\u5f8b\u3002\u8be5\u4fe1\u53f7\u5305\u542b\u4e24\u4e2a\u6807\u5ea6\u9879\uff0c\u5176\u6bd4\u7387\u5bf9\u5e94\u4e8e\u70ed\u7535\u529f\u7387\u7684\u5e73\u65b9\u9664\u4ee5\u6d1b\u4f26\u5179\u6570\u3002\u7814\u7a76\u5229\u7528\u503e\u659c\u72c4\u62c9\u514b\u6a21\u578b\u548c\u53cc\u5c42WTe$_2$\u7684\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u5c55\u793a\u4e86\u5173\u952e\u7279\u5f81\uff0c\u5e76\u8ba8\u8bba\u4e86\u53ef\u80fd\u7684\u5916\u90e8\u8d21\u732e\u548c\u66ff\u4ee3\u5b9e\u9a8c\u68c0\u6d4b\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u80fd\u5e26\u91cf\u5b50\u51e0\u4f55\u5bf9\u7535\u5b50\u52a8\u529b\u5b66\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u975e\u7ebf\u6027\u8c37\u5361\u8def\u91cc\u7535\u5b50\u5b66\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u5185\u5728\u975e\u7ebf\u6027\u8c37Nernst\u6548\u5e94\uff0c\u8be5\u6548\u5e94\u7531\u8c37\u7535\u5b50\u7684\u8d1d\u91cc\u8fde\u63a5\u6781\u5316\u7387\u5076\u6781\u5b50\u5f15\u8d77\uff0c\u53ef\u7528\u4e8e\u5bf9\u79f0\u548c\u4e0d\u5bf9\u79f0\u6750\u6599\u4e2d\uff0c\u4e3a\u7406\u89e3\u548c\u5e94\u7528\u91cf\u5b50\u51e0\u4f55\u6548\u5e94\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "method": "\u7406\u8bba\u63a8\u5bfc\u54cd\u5e94\u5f20\u91cf\u4e0e\u8c37\u970d\u5c14\u7535\u5bfc\u7387\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u975e\u5c40\u57df\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u503e\u659c\u72c4\u62c9\u514b\u6a21\u578b\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff08\u53cc\u5c42WTe$_2$\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u7eb5\u5411\u6e29\u5ea6\u68af\u5ea6\u5f15\u8d77\u7684\u4e8c\u9636\u70ed\u7535\u6548\u5e94\uff08\u975e\u7ebf\u6027\u8c37Nernst\u6548\u5e94\uff09\uff0c\u53ef\u4ea7\u751f\u6a2a\u5411\u8c37\u6d41\u3002\u5176\u54cd\u5e94\u5f20\u91cf\u4e0e\u8c37\u970d\u5c14\u7535\u5bfc\u7387\u6210\u6b63\u6bd4\uff0c\u4e14\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u975e\u5c40\u57df\u4e8c\u9636\u8c10\u6ce2\u4fe1\u53f7\u6d4b\u91cf\uff0c\u8be5\u4fe1\u53f7\u5177\u6709 $\rho^2$ \u6807\u5ea6\u5f8b\u3002", "conclusion": "\u5185\u5728\u975e\u7ebf\u6027\u8c37Nernst\u6548\u5e94\u662f\u7531\u4e8e\u8c37\u7535\u5b50\u7684\u8d1d\u91cc\u8fde\u63a5\u6781\u5316\u7387\u5076\u6781\u5b50\u5f15\u8d77\u7684\u4e8c\u9636\u70ed\u7535\u54cd\u5e94\u3002\u8be5\u6548\u5e94\u7684\u6d4b\u91cf\u53ef\u4ee5\u901a\u8fc7\u975e\u5c40\u57df\u4e8c\u9636\u8c10\u6ce2\u4fe1\u53f7\u5b9e\u73b0\uff0c\u5e76\u4e0e\u8c37\u970d\u5c14\u7535\u5bfc\u7387\u548c\u70ed\u7535\u529f\u7387\u76f8\u5173\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u80fd\u5e26\u91cf\u5b50\u51e0\u4f55\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u975e\u7ebf\u6027\u8c37\u5361\u8def\u91cc\u7535\u5b50\u5b66\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.19327", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19327", "abs": "https://arxiv.org/abs/2508.19327", "authors": ["Pilsung Kang"], "title": "Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning", "comment": null, "summary": "Bell's theorem reveals a profound conflict between quantum mechanics and\nlocal realism, a conflict we reinterpret through the modern lens of causal\ninference. We propose and computationally validate a framework where quantum\nentanglement acts as a \"super-confounding\" resource, generating correlations\nthat violate the classical causal bounds set by Bell's inequalities. This work\nmakes three key contributions: First, we establish a physical hierarchy of\nconfounding (Quantum > Classical) and introduce Confounding Strength (CS) to\nquantify this effect. Second, we provide a circuit-based implementation of the\nquantum $\\mathcal{DO}$-calculus to distinguish causality from spurious\ncorrelation. Finally, we apply this calculus to a quantum machine learning\nproblem, where causal feature selection yields a statistically significant\n11.3% average absolute improvement in model robustness. Our framework bridges\nquantum foundations and causal AI, offering a new, practical perspective on\nquantum correlations.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u8d1d\u5c14\u5b9a\u7406\u4e0e\u56e0\u679c\u63a8\u65ad\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u91cf\u5b50\u7ea0\u7f20\u4f5c\u4e3a\u201c\u8d85\u7ea7\u6df7\u6dc6\u201d\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u9a8c\u8bc1\u4e86\u5176\u8fdd\u53cd\u7ecf\u5178\u56e0\u679c\u754c\u9650\u7684\u80fd\u529b\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u6df7\u6dc6\u5c42\u7ea7\uff08\u91cf\u5b50>\u7ecf\u5178\uff09\uff0c\u5f15\u5165\u4e86\u91cf\u5316\u6df7\u6dc6\u7684\u6df7\u6dc6\u5f3a\u5ea6\uff08CS\uff09\uff0c\u5b9e\u73b0\u4e86\u533a\u5206\u56e0\u679c\u4e0e\u4f2a\u76f8\u5173\u7684\u91cf\u5b50DO\u6f14\u7b97\u7684\u7535\u8def\u5b9e\u73b0\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8611.3%\u7684\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u5347\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u7684\u89c6\u89d2\u91cd\u65b0\u89e3\u8bfb\u91cf\u5b50\u529b\u5b66\u4e0e\u5c40\u57df\u5b9e\u5728\u8bba\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u4e3a\u7406\u89e3\u548c\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u8fd9\u4e00\u8d44\u6e90\u63d0\u4f9b\u65b0\u7684\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u91cf\u5b50\u7ea0\u7f20\u89c6\u4e3a\u201c\u8d85\u7ea7\u6df7\u6dc6\u201d\u8d44\u6e90\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u7814\u7a76\u5185\u5bb9\u5305\u62ec\uff1a1. \u5efa\u7acb\u4e86\u7269\u7406\u6df7\u6dc6\u5c42\u7ea7\uff08\u91cf\u5b50>\u7ecf\u5178\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u6df7\u6dc6\u5f3a\u5ea6\uff08CS\uff09\u8fdb\u884c\u91cf\u5316\u30022. \u63d0\u4f9b\u4e86\u91cf\u5b50DO\u6f14\u7b97\u7684\u7535\u8def\u5b9e\u73b0\uff0c\u7528\u4e8e\u533a\u5206\u56e0\u679c\u5173\u7cfb\u548c\u4f2a\u76f8\u5173\u30023. \u5c06\u8be5\u6f14\u7b97\u5e94\u7528\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u56e0\u679c\u7279\u5f81\u9009\u62e9\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u5efa\u7acb\u4e86\u7269\u7406\u6df7\u6dc6\u5c42\u7ea7\uff0c\u91cf\u5316\u4e86\u6df7\u6dc6\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u4e86\u91cf\u5b50DO\u6f14\u7b97\u7684\u7535\u8def\u5b9e\u73b0\uff0c\u5e76\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u56e0\u679c\u7279\u5f81\u9009\u62e9\u5c06\u6a21\u578b\u9c81\u68d2\u6027\u5e73\u5747\u7edd\u5bf9\u63d0\u5347\u4e8611.3%\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06\u91cf\u5b50\u57fa\u7840\u7406\u8bba\u4e0e\u56e0\u679c\u4eba\u5de5\u667a\u80fd\u76f8\u7ed3\u5408\uff0c\u4e3a\u7406\u89e3\u91cf\u5b50\u76f8\u5173\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u5b9e\u7528\u7684\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u65ad\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.19290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "Despite the importance of LiDAR-based segmentation for autonomous vehicles, current networks are vulnerable to adversarial attacks. Existing defenses often use computationally intensive models for raw 3D point clouds. This paper introduces an efficient, model-based purification framework for 2D range-view LiDAR segmentation, addressing a gap in lightweight defenses for this domain. The authors propose a direct attack in the range-view domain and an explainable purification network based on an optimized mathematical problem. Their method offers strong adversarial resilience with low computational cost, outperforming existing baselines and demonstrating practical effectiveness in real-world autonomous driving scenarios.", "motivation": "Modern LiDAR segmentation networks are highly susceptible to adversarial attacks, posing a safety risk for autonomous vehicles. While defenses exist for raw 3D point clouds, there's a need for lightweight defenses specifically for the widely adopted 2D range-view representations used in efficient LiDAR segmentation pipelines.", "method": "The paper introduces an efficient model-based purification framework for 2D range-view LiDAR segmentation. It includes a direct attack formulation in the range-view domain and an explainable purification network derived from a mathematically justified optimization problem.", "result": "The proposed method achieves strong adversarial resilience with minimal computational overhead. It demonstrates competitive performance on open benchmarks, outperforming generative and adversarial training baselines. Real-world deployment on a demo vehicle confirmed its ability to maintain accurate operation in practical autonomous driving scenarios.", "conclusion": "The developed purification framework offers an efficient and effective defense against adversarial attacks for 2D range-view LiDAR segmentation, a crucial component for safe autonomous driving. Its lightweight nature and strong performance make it a promising solution for practical deployment."}}
{"id": "2508.19387", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19387", "abs": "https://arxiv.org/abs/2508.19387", "authors": ["Nadia Pourmohammad-Zia", "Mark van Koningsveld"], "title": "Climate-Resilient Ports and Waterborne Transport Systems: Current Status and Future Prospects", "comment": null, "summary": "The increasing challenges posed by climate change necessitate a comprehensive\nexamination of the resilience of waterborne transport systems. This paper\nexplores the nexus of climate resilience, and waterborne transport, addressing\nthe challenges faced by ports and their connecting waterborne transport\nsystems. It provides an in-depth analysis of the current status of\nclimate-resilient infrastructure and operations while emphasizing the\ntransformative potential of emerging technologies. Through a systematic review,\nthe paper identifies critical gaps and opportunities. Research predominantly\nemphasizes port infrastructure over supply chain resilience, neglecting the\ninterconnected vulnerabilities of maritime networks. There is limited focus on\nspecific climate-induced disruptions, such as drought and compounded events,\nwhich complicate resilience planning. Methodologically, risk assessments and\ncase studies dominate the field, while advanced technologies such as digital\ntwins, artificial intelligence, and satellite monitoring remain underutilized.\nGeographic disparities in research output and a tendency toward short- to\nmedium-term planning further constrain global and long-term resilience efforts.\nTo address these gaps, the study advocates for systems-based approaches that\nintegrate infrastructure, operations, and supply chains. It highlights\ncollaborative frameworks and advanced tools, including digital twins, machine\nlearning, and participatory modeling, as crucial for enabling predictive and\nadaptive risk management. This study stands as one of the first comprehensive\nreviews exclusively focused on climate resilience in ports and waterborne\ntransport systems. It provides actionable insights for policymakers,\nresearchers, and industry stakeholders, proposing a future research agenda to\nadvance waterborne transport systems capable of withstanding multifaceted\nclimate impacts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u5ba1\u67e5\u4e86\u6c34\u4e0a\u8fd0\u8f93\u7cfb\u7edf\u5728\u6c14\u5019\u53d8\u5316\u4e0b\u7684\u590d\u539f\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u6e2f\u53e3\u53ca\u5176\u8fde\u63a5\u7cfb\u7edf\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u5728\u4f9b\u5e94\u94fe\u590d\u539f\u529b\u3001\u7279\u5b9a\u6c14\u5019\u4e8b\u4ef6\uff08\u5982\u5e72\u65f1\uff09\u3001\u4ee5\u53ca\u5148\u8fdb\u6280\u672f\uff08\u5982\u6570\u5b57\u5b6a\u751f\u3001\u4eba\u5de5\u667a\u80fd\uff09\u5e94\u7528\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u5e94\u91c7\u7528\u57fa\u4e8e\u7cfb\u7edf\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u534f\u4f5c\u6846\u67b6\u548c\u5148\u8fdb\u5de5\u5177\u6765\u52a0\u5f3a\u98ce\u9669\u7ba1\u7406\u548c\u957f\u671f\u89c4\u5212\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bf9\u6c34\u4e0a\u8fd0\u8f93\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u6e2f\u53e3\u53ca\u5176\u8fde\u63a5\u7cfb\u7edf\uff0c\u5e26\u6765\u4e86\u65e5\u76ca\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5168\u9762\u7814\u7a76\u5176\u6c14\u5019\u9002\u5e94\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u7814\u7a76\u5728\u6e2f\u53e3\u57fa\u7840\u8bbe\u65bd\u3001\u4f9b\u5e94\u94fe\u590d\u539f\u529b\u3001\u6c14\u5019\u4e8b\u4ef6\u5f71\u54cd\u3001\u7814\u7a76\u65b9\u6cd5\u548c\u6280\u672f\u5e94\u7528\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\u4e0e\u673a\u9047\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u6587\u732e\u4fa7\u91cd\u4e8e\u6e2f\u53e3\u57fa\u7840\u8bbe\u65bd\u800c\u975e\u4f9b\u5e94\u94fe\u590d\u539f\u529b\uff0c\u5bf9\u5e72\u65f1\u548c\u590d\u5408\u6027\u4e8b\u4ef6\u7b49\u5177\u4f53\u6c14\u5019\u6270\u52a8\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e14\u9ad8\u7ea7\u6280\u672f\u5e94\u7528\u6ede\u540e\u3002\u7814\u7a76\u5728\u5730\u7406\u5206\u5e03\u548c\u89c4\u5212\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u4e5f\u5b58\u5728\u4e0d\u5747\u8861\u3002\u6b64\u5916\uff0c\u5bf9\u6e2f\u53e3\u548c\u6c34\u4e0a\u8fd0\u8f93\u7cfb\u7edf\u6c14\u5019\u590d\u539f\u529b\u7684\u7efc\u5408\u6027\u5ba1\u67e5\uff0c\u7279\u522b\u662f\u4e13\u95e8\u9488\u5bf9\u6b64\u4e3b\u9898\u7684\u5ba1\u67e5\uff0c\u4ecd\u7136\u6709\u9650\u3002", "conclusion": "\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u7814\u7a76\u4e3b\u5f20\u91c7\u7528\u57fa\u4e8e\u7cfb\u7edf\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u6574\u5408\u57fa\u7840\u8bbe\u65bd\u3001\u8fd0\u8425\u548c\u4f9b\u5e94\u94fe\uff0c\u5e76\u5f3a\u8c03\u5229\u7528\u6570\u5b57\u5b6a\u751f\u3001\u673a\u5668\u5b66\u4e60\u548c\u53c2\u4e0e\u5f0f\u5efa\u6a21\u7b49\u5148\u8fdb\u5de5\u5177\u548c\u534f\u4f5c\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9884\u6d4b\u6027\u548c\u9002\u5e94\u6027\u98ce\u9669\u7ba1\u7406\uff0c\u4ece\u800c\u589e\u5f3a\u6c34\u4e0a\u8fd0\u8f93\u7cfb\u7edf\u62b5\u5fa1\u591a\u79cd\u6c14\u5019\u5f71\u54cd\u7684\u80fd\u529b\u3002"}}
{"id": "2508.19660", "categories": ["eess.SP", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19660", "abs": "https://arxiv.org/abs/2508.19660", "authors": ["Vojtech Mrazek", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Zdenek Vasicek", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation", "comment": "Accepted at IEEE Transactions on Circuits and Systems for Artificial\n  Intelligence", "summary": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u4efb\u610f\u8f93\u5165\u7cbe\u5ea6\u7684\u5370\u5237\u4e09\u5143\u795e\u7ecf\u7f51\u7edc\uff08TNN\uff09\uff0c\u5b9e\u73b0\u4e86\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u79ef\u548c\u529f\u8017\u6548\u7387\u4e0a\u7684\u7a81\u7834\uff0c\u80fd\u591f\u4ee5\u4f4e\u4e8e5%\u7684\u7cbe\u5ea6\u635f\u5931\u5b9e\u73b0\u5370\u5237\u7535\u6c60\u4f9b\u7535\u8fd0\u884c\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u67d4\u6027\u3001\u53ef\u62c9\u4f38\u3001\u4f4e\u6210\u672c\u7684\u5370\u5237\u7535\u5b50\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5370\u5237\u795e\u7ecf\u7f51\u7edc\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u9762\u79ef\u6548\u7387\u548c\u529f\u8017\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u6570\u5b57\u8f6c\u6362\u63a5\u53e3\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u591a\u76ee\u6807\u4f18\u5316\u548c\u6574\u4f53\u8fd1\u4f3c\u6280\u672f\uff0c\u8bbe\u8ba1\u5370\u5237\u4e09\u5143\u795e\u7ecf\u7f51\u7edc\uff08TNN\uff09\uff0c\u5e76\u5bf9\u5305\u62ec\u6a21\u62df\u6570\u5b57\u8f6c\u6362\u5728\u5185\u7684\u6574\u4e2a\u8fd1\u4f20\u611f\u5668\u5904\u7406\u7cfb\u7edf\u8fdb\u884c\u4e86\u8054\u5408\u4f18\u5316\uff0c\u5141\u8bb8\u4efb\u610f\u8f93\u5165\u7cbe\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5370\u5237TNN\u5728\u9762\u79ef\u548c\u529f\u8017\u65b9\u9762\u6bd4\u73b0\u6709\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u5e73\u5747\u4f18\u8d8a17\u500d\u548c59\u500d\uff0c\u5e76\u4e14\u9996\u6b21\u5b9e\u73b0\u4e86\u5370\u5237\u7535\u6c60\u4f9b\u7535\u8fd0\u884c\uff0c\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e5%\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u8bbe\u8ba1\u6846\u67b6\u548c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u79ef\u548c\u529f\u8017\u6548\u7387\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u514b\u670d\u4e86\u5370\u5237\u7535\u5b50\u5728\u590d\u6742\u7535\u8def\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u4f4e\u529f\u8017\u3001\u9ad8\u7cbe\u5ea6\u7684\u5370\u5237\u7535\u6c60\u4f9b\u7535\u7cfb\u7edf\u3002"}}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\uff08\u5982LLM\uff09\u79d8\u5bc6\u5171\u4eab\u79c1\u6709\u4fe1\u606f\uff09\u7684\u884c\u4e3a\u7684\u76d1\u6d4b\u7cfb\u7edf\u538b\u529b\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5373\u76d1\u6d4b\u7ea2\u961f\uff08MRT\uff09\u5de5\u4f5c\u6d41\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u4ee3\u7406\u548c\u76d1\u6d4b\u5668\u6001\u52bf\u611f\u77e5\u3001\u4e0d\u540c\u7684\u9003\u907f\u7b56\u7565\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\u4ee5\u53ca\u4e24\u4e2a\u6570\u636e\u96c6\uff08SHADE-Arena\u548cCUA-SHADE-Arena\uff09\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7406\u7684\u6001\u52bf\u611f\u77e5\u6bd4\u76d1\u6d4b\u5668\u7684\u6001\u52bf\u611f\u77e5\u66f4\u91cd\u8981\uff0c\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\u76d1\u6d4b\u811a\u624b\u67b6\u6bd4\u57fa\u7ebf\u76d1\u6d4b\u811a\u624b\u67b6\u66f4\u4f18\u8d8a\uff0c\u5e76\u4e14\u5728\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u9488\u5bf9\u6027\u7684\u4eba\u5de5\u76d1\u7763\u6700\u6709\u6548\u3002", "motivation": "\u4e3a\u4e86\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\uff08\u4f8b\u5982\uff0c\u79d8\u5bc6\u5171\u4eab\u79c1\u6709\u4fe1\u606f\uff09\u4e2d\u7684\u9690\u853d\u6027\u4e0d\u5f53\u884c\u4e3a\uff0c\u9700\u8981\u5bf9\u76d1\u6d4b\u7cfb\u7edf\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5316\u4e86\u4e00\u4e2a\u76d1\u6d4b\u7ea2\u961f\uff08MRT\uff09\u5de5\u4f5c\u6d41\uff0c\u8be5\u5de5\u4f5c\u6d41\u7ed3\u5408\u4e86\u4ee3\u7406\u548c\u76d1\u6d4b\u5668\u7684\u4e0d\u540c\u6001\u52bf\u611f\u77e5\u6c34\u5e73\u3001\u7528\u4e8e\u9003\u907f\u76d1\u6d4b\u5668\u7684\u4e0d\u540c\u5bf9\u6297\u7b56\u7565\uff08\u4f8b\u5982\uff0c\u63d0\u793a\u6ce8\u5165\uff09\uff0c\u4ee5\u53ca\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u73af\u5883\uff08SHADE-Arena\u548cCUA-SHADE-Arena\uff09\u3002\u7814\u7a76\u4eba\u5458\u5728\u73b0\u6709\u7684LLM\u76d1\u6d4b\u811a\u624b\u67b6\u548c\u4e00\u79cd\u65b0\u63d0\u51fa\u7684\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\u811a\u624b\u67b6\u4e0a\u8fd0\u884c\u4e86MRT\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7406\u7684\u6001\u52bf\u611f\u77e5\u80fd\u529b\u8fdc\u8d85\u76d1\u6d4b\u5668\u7684\u6001\u52bf\u611f\u77e5\u80fd\u529b\uff0c\u56e0\u4e3a\u4ee3\u7406\u4e86\u89e3\u81ea\u8eab\u88ab\u76d1\u63a7\u4f1a\u663e\u8457\u964d\u4f4e\u76d1\u6d4b\u5668\u7684\u53ef\u9760\u6027\u3002\u6df7\u5408\u811a\u624b\u67b6\u4f18\u4e8e\u57fa\u7ebf\u811a\u624b\u67b6\uff0c\u5e76\u4e14\u80fd\u5b9e\u73b0\u5f31\u5230\u5f3a\u7684\u6269\u5c55\u6548\u5e94\u3002\u5728\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u9488\u5bf9\u6027\u7684\u4eba\u5de5\u76d1\u7763\u6700\u6709\u6548\uff0c\u53ef\u5c06\u5047\u9633\u7387\uff08FPR\uff09\u8bbe\u5b9a\u4e3a0.01\u65f6\uff0c\u5c06\u771f\u9633\u7387\uff08TPR\uff09\u63d0\u9ad8\u7ea615%\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u6807\u51c6\u7684MRT\u5de5\u4f5c\u6d41\uff0c\u63ed\u793a\u4e86LLM\u5728\u76d1\u6d4b\u548c\u68c0\u6d4b\u4ee3\u7406\u4e0d\u5f53\u884c\u4e3a\u65b9\u9762\u7f3a\u4e4f\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u4eba\u7c7b\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u65e5\u5fd7\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2508.19425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19425", "abs": "https://arxiv.org/abs/2508.19425", "authors": ["John M. Scanlon", "Timothy L McMurry", "Yin-Hsiu Chen", "Kristofer D. Kusano", "Trent Victor"], "title": "From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation", "comment": null, "summary": "This paper presents crash rate benchmarks for evaluating US-based Automated\nDriving Systems (ADS) for multiple urban areas. The purpose of this study was\nto extend prior benchmarks focused only on surface streets to additionally\ncapture freeway crash risk for future ADS safety performance assessments. Using\npublicly available police-reported crash and vehicle miles traveled (VMT) data,\nthe methodology details the isolation of in-transport passenger vehicles, road\ntype classification, and crash typology. Key findings revealed that freeway\ncrash rates exhibit large geographic dependence variations with\nany-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4\nIPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results\nshow the critical need for location-specific benchmarks to avoid biased safety\nevaluations and provide insights into the vehicle miles traveled (VMT) required\nto achieve statistical significance for various safety impact levels. The\ndistribution of crash types depended on the outcome severity level. Higher\nseverity outcomes (e.g., fatal crashes) had a larger proportion of\nsingle-vehicle, vulnerable road users (VRU), and opposite-direction collisions\ncompared to lower severity (police-reported) crashes. Given heterogeneity in\ncrash types by severity, performance in low-severity scenarios may not be\npredictive of high-severity outcomes. These benchmarks are additionally used to\nquantify at the required mileage to show statistically significant deviations\nfrom human performance. This is the first paper to generate freeway-specific\nbenchmarks for ADS evaluation and provides a foundational framework for future\nADS benchmarking by evaluators and developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7f8e\u56fd\u591a\u4e2a\u57ce\u5e02\u533a\u57df\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u78b0\u649e\u7387\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u80fd\u3002\u7814\u7a76\u5c06\u5148\u524d\u4ec5\u5173\u6ce8\u57ce\u5e02\u8857\u9053\u7684\u57fa\u51c6\u6269\u5c55\u5230\u5305\u62ec\u9ad8\u901f\u516c\u8def\u78b0\u649e\u98ce\u9669\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u7684ADS\u5b89\u5168\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u5148\u524d\u4ec5\u5173\u6ce8\u8857\u9053\u7684\u57fa\u51c6\uff0c\u4ee5\u7eb3\u5165\u9ad8\u901f\u516c\u8def\u78b0\u649e\u98ce\u9669\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u5b89\u5168\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6570\u636e\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684\u8b66\u65b9\u62a5\u544a\u78b0\u649e\u548c\u884c\u9a76\u91cc\u7a0b\uff08VMT\uff09\u6570\u636e\uff0c\u5206\u79bb\u51fa\u8fd0\u8f93\u4e2d\u7684\u4e58\u7528\u8f66\uff0c\u5bf9\u9053\u8def\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u78b0\u649e\u7c7b\u578b\u3002", "result": "\u9ad8\u901f\u516c\u8def\u78b0\u649e\u7387\u663e\u793a\u51fa\u663e\u8457\u7684\u5730\u57df\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u4e9a\u7279\u5170\u5927\u7684\u4efb\u4f55\u4f24\u5bb3\u62a5\u544a\u78b0\u649e\u7387\uff082.4 IPMM\uff09\u51e0\u4e4e\u662f\u83f2\u5c3c\u514b\u65af\uff080.7 IPMM\uff09\u7684\u4e09\u500d\u591a\u3002\u78b0\u649e\u7c7b\u578b\u7684\u5206\u5e03\u56e0\u540e\u679c\u4e25\u91cd\u7a0b\u5ea6\u800c\u5f02\uff0c\u4e25\u91cd\u78b0\u649e\uff08\u5982\u81f4\u547d\u78b0\u649e\uff09\u7684\u5355\u8f66\u78b0\u649e\u3001\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u548c\u8fce\u9762\u78b0\u649e\u7684\u6bd4\u4f8b\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5730\u70b9\u7279\u5b9a\u57fa\u51c6\u5bf9\u4e8e\u907f\u514d\u6709\u504f\u89c1\u7684\u5b89\u5168\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u9996\u6b21\u4e3aADS\u8bc4\u4f30\u751f\u6210\u4e86\u9ad8\u901f\u516c\u8def\u7279\u5b9a\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7684ADS\u57fa\u51c6\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19318", "abs": "https://arxiv.org/abs/2508.19318", "authors": ["Aohan Li", "Miyu Tsuzuki"], "title": "(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has emerged as an efficient approach to\nresource allocation due to its strong capability in handling complex\ndecision-making tasks. However, only limited research has explored the training\nof DRL models with real-world data in practical, distributed Internet of Things\n(IoT) systems. To bridge this gap, this paper proposes a novel framework for\ntraining DRL models in real-world distributed IoT environments. In the proposed\nframework, IoT devices select communication channels using a DRL-based method,\nwhile the DRL model is trained with feedback information. Specifically,\nAcknowledgment (ACK) information is obtained from actual data transmissions\nover the selected channels. Implementation and performance evaluation, in terms\nof Frame Success Rate (FSR), are carried out, demonstrating both the\nfeasibility and the effectiveness of the proposed framework.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u6765\u81ea\u5b9e\u9645\u6570\u636e\u4f20\u8f93\u7684\u786e\u8ba4\uff08ACK\uff09\u4fe1\u606f\u4f5c\u4e3a\u53cd\u9988\u6765\u4f18\u5316\u901a\u4fe1\u4fe1\u9053\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5e27\u6210\u529f\u7387\uff08FSR\uff09\u8bc4\u4f30\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u5904\u7406\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5b83\u5df2\u6210\u4e3a\u8d44\u6e90\u5206\u914d\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u7684\u3001\u5206\u5e03\u5f0f\u7684\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3DRL\u6a21\u578b\u7684\u7814\u7a76\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u672c\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684DRL\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5176\u4e2d\u7269\u8054\u7f51\u8bbe\u5907\u4f7f\u7528\u57fa\u4e8eDRL\u7684\u65b9\u6cd5\u9009\u62e9\u901a\u4fe1\u4fe1\u9053\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u4f20\u8f93\u83b7\u5f97\u7684\u786e\u8ba4\uff08ACK\uff09\u4fe1\u606f\u5bf9DRL\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5728\u5b9e\u9645\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u548c\u8bc4\u4f30\u8be5\u6846\u67b6\uff0c\u5e76\u5728\u5e27\u6210\u529f\u7387\uff08FSR\uff09\u65b9\u9762\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u771f\u5b9e\u7684\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8bad\u7ec3DRL\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u901a\u4fe1\u4fe1\u9053\u9009\u62e9\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19670", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19670", "abs": "https://arxiv.org/abs/2508.19670", "authors": ["Diogo Costa", "Jose Martins", "Sandro Pinto"], "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems", "comment": null, "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f02\u6784\u8ba1\u7b97\u5e73\u53f0\u4e2dIOMMU\u7684\u6027\u80fd\u5e72\u6270\u95ee\u9898\uff0c\u53d1\u73b0IOMMU\u7684\u5171\u4eab\u7279\u6027\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\uff0c\u5c24\u5176\u5bf9\u5c0f\u5185\u5b58\u4e8b\u52a1\u5f71\u54cd\u8f83\u5927\u3002", "motivation": "\u968f\u7740\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\uff08MCSs\uff09\u6574\u5408\u5f02\u6784\u8ba1\u7b97\u5e73\u53f0\uff0cIOMMU\u5728\u786e\u4fdd\u5b89\u5168\u548c\u65f6\u95f4\u53ef\u9884\u6d4b\u6027\u65b9\u9762\u4f5c\u7528\u5173\u952e\uff0c\u4f46\u5176\u5bf9\u6027\u80fd\u5e72\u6270\u7684\u5f71\u54cd\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u5728Xilinx UltraScale+ ZCU104\u5e73\u53f0\u4e0a\u5206\u6790\u4e86IOMMU\u7ed3\u6784\u5185\u7684\u4e89\u7528\u6548\u5e94\uff0c\u7279\u522b\u5173\u6ce8\u4e86IOMMU\u4e89\u7528\u5982\u4f55\u5f71\u54cdDMA\u4e8b\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIOMMU\u4e89\u7528\u5bf9\u5c0f\u5185\u5b58\u4e8b\u52a1\u7684\u5ef6\u8fdf\u5f71\u54cd\u5c24\u4e3a\u663e\u8457\uff0cArm SMMUv2\u7684\u5b9e\u73b0\u4e2d\uff0cIOMMU\u5e72\u6270\u53ef\u5bfc\u81f4\u5c0f\u4f20\u8f93\u5ef6\u8fdf\u6700\u9ad8\u589e\u52a01.79\u500d\u3002", "conclusion": "IOMMU\u7684\u5171\u4eab\u7279\u6027\u4f1a\u5f15\u5165\u4e0d\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\uff0c\u7279\u522b\u662f\u5728\u5c0f\u5185\u5b58\u4e8b\u52a1\u4e2d\uff0c\u8fd9\u53ef\u80fd\u5bf9MCSs\u7684\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u6027\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002IOMMU\u4e89\u7528\u6548\u5e94\u53ef\u80fd\u5728\u4e0d\u540c\u67b6\u6784\u4e2d\u8868\u73b0\u76f8\u4f3c\u3002"}}
{"id": "2508.19274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u5904\u7406\u548c\u5206\u6790\u201c\u6b7b\u4ea1\u539f\u56e0\u201d\u8c03\u67e5\uff08VA\uff09\u4e2d\u7684\u53d9\u8ff0\u4fe1\u606f\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u6b7b\u4ea1\u539f\u56e0\uff0c\u5e76\u53d1\u73b0\u53d9\u8ff0\u4fe1\u606f\u6bd4\u4ec5\u4f7f\u7528\u7ed3\u6784\u5316\u95ee\u5377\u4fe1\u606f\u66f4\u6709\u6548\uff0c\u7ed3\u5408\u4e24\u8005\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u5728\u6ca1\u6709\u5b8c\u5584\u51fa\u751f\u548c\u6b7b\u4ea1\u767b\u8bb0\u7684\u56fd\u5bb6\uff0c\u53e3\u8ff0\u9a8c\u5c38\uff08VA\uff09\u662f\u4f30\u8ba1\u6b7b\u4ea1\u539f\u56e0\uff08COD\uff09\u548c\u5236\u5b9a\u653f\u7b56\u7684\u5173\u952e\u5de5\u5177\u3002\u73b0\u6709\u7b97\u6cd5\u4ec5\u4f7f\u7528\u7ed3\u6784\u5316\u95ee\u5377\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u53d9\u8ff0\u4fe1\u606f\uff0c\u4f46\u53d9\u8ff0\u4fe1\u606f\u5305\u542b\u6709\u4ef7\u503c\u7684\u7ebf\u7d22\u3002", "method": "\u5229\u7528\u5357\u975e\u7684\u5b9e\u9645\u6570\u636e\uff0c\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u548c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6280\u672f\uff0c\u4ec5\u901a\u8fc7VA\u7684\u53d9\u8ff0\u4fe1\u606f\u6765\u8fdb\u884c\u81ea\u52a8\u5316\u7684\u6b7b\u4ea1\u539f\u56e0\u5206\u7c7b\uff0c\u5e76\u63a2\u7d22\u4e86\u7ed3\u5408\u53d9\u8ff0\u548c\u95ee\u5377\u4fe1\u606f\u7684\u201c\u591a\u6a21\u6001\u201d\u878d\u5408\u7b56\u7565\u3002", "result": "\u4ec5\u4f7f\u7528\u53d9\u8ff0\u4fe1\u606f\uff0c\u57fa\u4e8eTransformer\u7684PLMs\u5728\u6b7b\u4ea1\u539f\u56e0\u5206\u7c7b\u4e0a\u4f18\u4e8e\u4ec5\u4f7f\u7528\u95ee\u5377\u4fe1\u606f\u7684\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u8bc6\u522b\u975e\u4f20\u67d3\u6027\u75be\u75c5\u65b9\u9762\u3002\u591a\u6a21\u6001\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u533b\u751f\u611f\u77e5\u7684\u201c\u4fe1\u606f\u5145\u5206\u6027\u201d\u4f1a\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e14\u8fd9\u79cd\u5f71\u54cd\u5bf9\u6a21\u578b\u4e5f\u540c\u6837\u5b58\u5728\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5728VA\u8c03\u67e5\u4e2d\u5229\u7528\u53d9\u8ff0\u4fe1\u606f\u53ef\u4ee5\u63d0\u5347\u6b7b\u4ea1\u539f\u56e0\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u6765\u8bad\u7ec3\u548c\u4f18\u5316\u8fd9\u4e9b\u6a21\u578b\uff0c\u540c\u65f6\u4e5f\u4e3a\u6539\u8fdbVA\u5de5\u5177\u548c\u8bbf\u8c08\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.19828", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19828", "abs": "https://arxiv.org/abs/2508.19828", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Sch\u00fctze", "Volker Tresp", "Yunpu Ma"], "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.", "AI": {"tldr": "LLMs are stateless and have limited context windows, hindering long-horizon reasoning. Memory-R1 is an RL framework that uses two agents (Memory Manager and Answer Agent) to actively manage external memory, improving LLMs' ability to reason over longer contexts. It outperforms existing baselines and generalizes well.", "motivation": "LLMs are stateless and have limited context windows, which restricts their ability to perform long-horizon reasoning. Existing solutions often rely on static, heuristic-driven external memory banks, lacking learned mechanisms for memory management.", "method": "Memory-R1 framework uses two RL agents: a Memory Manager for structured memory operations (ADD, UPDATE, DELETE, NOOP) and an Answer Agent for retrieving relevant entries and answering questions. Both agents are fine-tuned using PPO and GRPO for adaptive memory management with minimal supervision.", "result": "Memory-R1 outperforms competitive baselines and demonstrates strong generalization across diverse question types and LLM backbones, even with a small training dataset (152 question-answer pairs and a temporal memory bank).", "conclusion": "RL can enable more agentic and memory-aware behaviors in LLMs, leading to richer and more persistent reasoning systems. Memory-R1 provides an effective approach to address the limitations of LLMs in long-horizon reasoning by actively managing external memory."}}
{"id": "2508.19623", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2508.19623", "abs": "https://arxiv.org/abs/2508.19623", "authors": ["Thomas B. Winkler", "Yuean Zhou", "Grischa Beneke", "Fabian Kammerbauer", "Sachin Krishnia", "Mario Carpentieri", "Davi R. Rodrigues", "Mathias Kl\u00e4ui", "Johan H. Mentink"], "title": "Multi-value Probabilistic Computing with current-controlled Skyrmion Diffusion", "comment": "38 pages, 7 figures, 2 tables", "summary": "Magnetic systems are highly promising for implementing probabilistic\ncomputing paradigms because of the fitting energy scales and conspicuous\nnon-linearities. While conventional binary probabilistic computing has been\nrealized, implementing more advantageous multi-value probabilistic computing\n(MPC) remains a challenge. Here, we report the realization of MPC by leveraging\nthe thermally activated diffusion of magnetic skyrmions through an effectively\nnon-flat energy landscape defined by a discrete number of pinning sites. The\ntime-averaged spatial distribution of the diffusing skyrmions directly realizes\na discrete probability distribution, which is tunable by current-generated\nspin-orbit torques, and can be quantified by non-perturbative electrical\nmeasurements. Even a very straightforward implementation with global tuning,\nalready allows us to demonstrate the softmax computation - a core function in\nartificial intelligence. As a key advance, we demonstrate invertible logic\nwithout the need to create a network of probabilistic devices, offering major\nscalability advantages. Our proof of concept can be generalized to multiple\nskyrmions and can accommodate multiple locally tunable inputs and outputs using\nmagnetic tunnel junctions, potentially enabling the representation of highly\ncomplex distribution functions.", "AI": {"tldr": "Magnetic skyrmions are used for multi-value probabilistic computing (MPC), enabling applications like softmax computation and invertible logic. The system offers scalability and can be generalized for complex functions.", "motivation": "To realize multi-value probabilistic computing (MPC), which is more advantageous than conventional binary probabilistic computing, by leveraging magnetic systems.", "method": "Utilizing the thermally activated diffusion of magnetic skyrmions through an energy landscape defined by pinning sites. The probability distribution is tuned by current-generated spin-orbit torques and measured electrically.", "result": "Demonstrated MPC with a straightforward implementation, including softmax computation and invertible logic without a network of devices. The system is scalable and can be generalized to multiple skyrmions and tunable inputs/outputs.", "conclusion": "The proposed proof-of-concept using magnetic skyrmions for MPC is a significant advance, offering advantages in scalability and potential for representing complex functions, paving the way for more advanced probabilistic computing architectures."}}
{"id": "2508.19868", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19868", "abs": "https://arxiv.org/abs/2508.19868", "authors": ["Geraldo F. Oliveira"], "title": "New Tools, Programming Models, and System Support for Processing-in-Memory Architectures", "comment": "Doctoral thesis", "summary": "Our goal in this dissertation is to provide tools, programming models, and\nsystem support for PIM architectures (with a focus on DRAM-based solutions), to\nease the adoption of PIM in current and future systems. To this end, we make at\nleast four new major contributions.\n  First, we introduce DAMOV, the first rigorous methodology to characterize\nmemory-related data movement bottlenecks in modern workloads, and the first\ndata movement benchmark suite. Second, we introduce MIMDRAM, a new\nhardware/software co-designed substrate that addresses the major current\nprogrammability and flexibility limitations of the bulk bitwise execution model\nof processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation\nand control of only the needed computing resources inside DRAM for PUD\ncomputing. Third, we introduce Proteus, the first hardware framework that\naddresses the high execution latency of bulk bitwise PUD operations in\nstate-of-the-art PUD architectures by implementing a data-aware runtime engine\nfor PUD. Proteus reduces the latency of PUD operations in three different ways:\n(i) Proteus concurrently executes independent in-DRAM primitives belong to a\nsingle PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the\nbit-precision (and consequentially the latency and energy consumption) of PUD\noperations by exploiting narrow values (i.e., values with many leading zeros or\nones). (iii) Proteus chooses and uses the most appropriate data representation\nand arithmetic algorithm implementation for a given PUD instruction\ntransparently to the programmer. Fourth, we introduce DaPPA (data-parallel\nprocessing-in-memory architecture), a new programming framework that eases\nprogrammability for general-purpose PNM architectures by allowing the\nprogrammer to write efficient PIM-friendly code without the need to manage\nhardware resources explicitly.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7528\u4e8e\u5185\u5b58\u5904\u7406\uff08PIM\uff09\u67b6\u6784\u7684\u5de5\u5177\u3001\u7f16\u7a0b\u6a21\u578b\u548c\u7cfb\u7edf\u652f\u6301\uff0c\u7279\u522b\u662f\u57fa\u4e8eDRAM\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u7b80\u5316PIM\u5728\u5f53\u524d\u548c\u672a\u6765\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u7b80\u5316PIM\u5728\u5f53\u524d\u548c\u672a\u6765\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u8bba\u6587\u65e8\u5728\u63d0\u4f9b\u76f8\u5173\u7684\u5de5\u5177\u3001\u7f16\u7a0b\u6a21\u578b\u548c\u7cfb\u7edf\u652f\u6301\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86DAMOV\uff08\u4e00\u79cd\u5185\u5b58\u6570\u636e\u79fb\u52a8\u74f6\u9888\u7684\u8868\u5f81\u65b9\u6cd5\u548c\u57fa\u51c6\u5957\u4ef6\uff09\u3001MIMDRAM\uff08\u4e00\u79cd\u6539\u8fdb\u7684PUD\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86\u53ef\u7f16\u7a0b\u6027\u548c\u7075\u6d3b\u6027\uff09\u3001Proteus\uff08\u4e00\u79cd\u7528\u4e8ePUD\u7684\u3001\u6570\u636e\u611f\u77e5\u7684\u8fd0\u884c\u65f6\u5f15\u64ce\uff0c\u901a\u8fc7\u5e76\u53d1\u6267\u884c\u3001\u52a8\u6001\u964d\u4f4e\u6bd4\u7279\u7cbe\u5ea6\u548c\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u8868\u793a\u6765\u51cf\u5c11\u6267\u884c\u5ef6\u8fdf\uff09\u4ee5\u53caDaPPA\uff08\u4e00\u79cd\u6570\u636e\u5e76\u884cPIM\u7f16\u7a0b\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u901a\u7528PIM\u67b6\u6784\u7684\u53ef\u7f16\u7a0b\u6027\uff09\u3002", "result": "DAMOV\u662f\u7b2c\u4e00\u4e2a\u4e25\u683c\u7684\u65b9\u6cd5\u5b66\uff0c\u7528\u4e8e\u8868\u5f81\u5185\u5b58\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u5e76\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u6570\u636e\u79fb\u52a8\u57fa\u51c6\u5957\u4ef6\u3002MIMDRAM\u89e3\u51b3\u4e86PUD\u67b6\u6784\u5728\u53ef\u7f16\u7a0b\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u7684\u9650\u5236\u3002Proteus\u901a\u8fc7\u5e76\u53d1\u6267\u884c\u3001\u52a8\u6001\u964d\u4f4e\u6bd4\u7279\u7cbe\u5ea6\u548c\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u8868\u793a\uff0c\u663e\u8457\u964d\u4f4e\u4e86PUD\u64cd\u4f5c\u7684\u6267\u884c\u5ef6\u8fdf\u3002DaPPA\u7b80\u5316\u4e86\u901a\u7528PIM\u67b6\u6784\u7684\u53ef\u7f16\u7a0b\u6027\u3002", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7DAMOV\u3001MIMDRAM\u3001Proteus\u548cDaPPA\u7684\u8d21\u732e\uff0c\u4e3aPIM\u67b6\u6784\uff08\u7279\u522b\u662fDRAM\uff09\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5de5\u5177\u3001\u7f16\u7a0b\u6a21\u578b\u548c\u7cfb\u7edf\u652f\u6301\uff0c\u65e8\u5728\u964d\u4f4ePIM\u7684\u5e94\u7528\u95e8\u69db\u3002"}}
{"id": "2508.20002", "categories": ["cs.DS", "05C70", "F.2.2; G.2"], "pdf": "https://arxiv.org/pdf/2508.20002", "abs": "https://arxiv.org/abs/2508.20002", "authors": ["Shaul Rosner", "Tami Tamir"], "title": "Bipartite Matching with Pair-Dependent Bounds", "comment": null, "summary": "Let $G=(U \\cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$\nrepresents machines. We study a new variant of the bipartite matching problem\nin which each job in $U$ can be matched to at most one machine in $V$, and the\nnumber of jobs that can be assigned to a machine depends on the specific jobs\nmatched to it. These pair-dependent bounds reflect systems where different jobs\nhave varying tolerance for congestion, determined by the specific machine they\nare assigned to.\n  We define a bipartite PD-matching as a set of edges $M \\subseteq E$ that\nsatisfies these job-to-machine tolerance constraints. This variant of matching\nextends well-known matching problems, however, despite its relevance to\nreal-world systems, it has not been studied before. We study bipartite\nPD-matchings with the objective of maximizing the matching size. As we show,\nthe problem exhibits significant differences from previously studied matching\nproblems. We analyze its computational complexity both in the general case and\nfor specific restricted instances, presenting hardness results alongside\noptimal and approximation algorithms.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u5206\u5339\u914d\u95ee\u9898\u53d8\u4f53\uff0c\u5176\u4e2d\u673a\u5668\u53ef\u4ee5\u63a5\u53d7\u7684\u4f5c\u4e1a\u6570\u91cf\u53d6\u51b3\u4e8e\u5206\u914d\u7ed9\u5b83\u7684\u7279\u5b9a\u4f5c\u4e1a\u3002\u8be5\u6a21\u578b\u65e8\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u62e5\u585e\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u6700\u5927\u5316\u5339\u914d\u5927\u5c0f\u7684\u76ee\u6807\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7814\u7a76\u4e86\u8be5\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u786c\u5ea6\u7ed3\u679c\u4ee5\u53ca\u6700\u4f18\u548c\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u7684\u62e5\u585e\u95ee\u9898\uff0c\u5176\u4e2d\u673a\u5668\u53ef\u63a5\u53d7\u7684\u4f5c\u4e1a\u6570\u91cf\u53d6\u51b3\u4e8e\u5206\u914d\u7ed9\u5b83\u7684\u7279\u5b9a\u4f5c\u4e1a\u3002", "method": "\u5b9a\u4e49\u4e86\u4e8c\u5206PD\u5339\u914d\uff0c\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u4f18\u548c\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u8be5\u95ee\u9898\u4e0e\u4e4b\u524d\u7814\u7a76\u7684\u5339\u914d\u95ee\u9898\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u786c\u5ea6\u7ed3\u679c\u4ee5\u53ca\u6700\u4f18\u548c\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u4e8c\u5206PD\u5339\u914d\u95ee\u9898\u662f\u4e00\u4e2a\u65b0\u7684\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5bf9\u4e8e\u7406\u89e3\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.19540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19540", "abs": "https://arxiv.org/abs/2508.19540", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Yuanwei Liu"], "title": "Pinching Antenna System for Integrated Sensing and Communications", "comment": "13 pages, 8 figures", "summary": "Recently, the pinching antenna system (PASS) has attracted considerable\nattention due to their advantages in flexible deployment and reduction of\nsignal propagation loss. In this work, a multiple waveguide PASS assisted\nintegrated sensing and communication (ISAC) system is proposed, where the base\nstation (BS) is equipped with transmitting pinching antennas (PAs) and\nreceiving uniform linear array (ULA) antennas. The full-duplex (FD) BS\ntransmits the communication and sensing signals through the PAs on waveguides\nand collects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraint, and PA deployment constraints. The alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. In each iteration, the overall optimization problem is decomposed into\na digital beamforming sub-problem and a pinching beamforming sub-problem. The\nsensing covariance matrix and communication beamforming matrix at the BS are\noptimized by solving the digital beamforming sub-problem with semidefinite\nrelaxation (SDR). The PA deployment is updated by solving the pinching\nbeamforming sub-problem with the successive convex approximation (SCA) method,\npenalty method, and element-wise optimization. Simulation results show that the\nproposed PASS assisted ISAC framework achieves superior performance over\nbenchmark schemes, is less affected by stringent communication constraints\ncompared to conventional MIMO-ISAC, and benefits further from increasing the\nnumber of waveguides and PAs per waveguide.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u6ce2\u5bfc\u634f\u7d27\u5929\u7ebf\u7cfb\u7edf\uff08PASS\uff09\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u65e8\u5728\u4f18\u5316\u76ee\u6807\u611f\u77e5\u5e76\u6ee1\u8db3\u901a\u4fe1\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u8981\u6c42\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709ISAC\u7cfb\u7edf\u5728\u7075\u6d3b\u90e8\u7f72\u548c\u4fe1\u53f7\u4f20\u64ad\u635f\u8017\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8ePASS\u7684ISAC\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u634f\u7d27\u5929\u7ebf\uff08PA\uff09\u548c\u5747\u5300\u7ebf\u6027\u9635\u5217\uff08ULA\uff09\u5929\u7ebf\uff0c\u6784\u5efa\u4e86\u5168\u53cc\u5de5\uff08FD\uff09\u57fa\u7ad9\uff08BS\uff09\u3002\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\u6c42\u89e3\u4ee5\u6700\u5c0f\u5316\u76ee\u6807\u611f\u77e5\u514b\u62c9\u7f8e-\u7f57\u754c\uff08CRB\uff09\uff0c\u540c\u65f6\u8003\u8651QoS\u3001\u529f\u7387\u9884\u7b97\u548cPA\u90e8\u7f72\u7ea6\u675f\u3002\u5177\u4f53\u5730\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u5b50\u95ee\u9898\uff08\u4f7f\u7528\u534a\u5b9a\u677e\u5f1b\u6cd5\uff08SDR\uff09\u6c42\u89e3\uff09\u548c\u634f\u7d27\u6ce2\u675f\u6210\u5f62\u5b50\u95ee\u9898\uff08\u4f7f\u7528\u8fde\u7eed\u51f8\u8fd1\u4f3c\u6cd5\uff08SCA\uff09\u3001\u60e9\u7f5a\u6cd5\u548c\u9010\u5143\u4f18\u5316\u6cd5\u6c42\u89e3\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684PASS\u8f85\u52a9ISAC\u6846\u67b6\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u4e14\u76f8\u6bd4\u4e8e\u4f20\u7edfMIMO-ISAC\u7cfb\u7edf\uff0c\u5bf9\u901a\u4fe1\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\u3002\u589e\u52a0\u6ce2\u5bfc\u6570\u91cf\u548c\u6bcf\u4e2a\u6ce2\u5bfc\u7684PA\u6570\u91cf\u53ef\u4ee5\u5e26\u6765\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PASS\u8f85\u52a9ISAC\u7cfb\u7edf\u5728\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728ISAC\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.19602", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19602", "abs": "https://arxiv.org/abs/2508.19602", "authors": ["Xiangbin Cai", "Haiyang Pan", "Yuzhu Wang", "Abdullah Rasmita", "Shunshun Yang", "Yan Zhao", "Wei Wang", "Ruihuan Duan", "Ruihua He", "Kenji Watanabe", "Takashi Taniguchi", "Zheng Liu", "Jes\u00fas Z\u00fa\u00f1iga P\u00e9rez", "Bo Yang", "Weibo Gao"], "title": "Optical Switching of Moir\u00e9 Chern Ferromagnet", "comment": "4 figures", "summary": "Optical manipulation of quantum matter offers a non-contact, high-precision\nand fast control. Fractional Chern ferromagnet states in moir\\'e superlattices\nare promising for topological quantum computing, but an effective optical\ncontrol protocol has remained elusive. Here, we demonstrate robust optical\nswitching of integer and fractional Chern ferromagnets in twisted MoTe2\nbilayers using circularly polarized light. Highly efficient optical\nmanipulation of spin orientations in the topological ferromagnet regime is\nrealized at zero field using a pump light power as low as 28 nanowatts per\nsquare micrometer. Utilizing this optically induced transition, we also\ndemonstrate magnetic bistate cycling and spatially resolved writing of\nferromagnetic domain walls. This work establishes a reliable and efficient\noptical control scheme for moir\\'e Chern ferromagnets, paving the way for\ndissipationless spintronics and quantized Chern junction devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5706\u504f\u632f\u5149\u5728\u53cc\u5c42\u626d\u66f2MoTe2\u4e2d\u5b9e\u73b0\u6574\u6570\u548c\u5206\u6570\u9648\u7edd\u7f18\u4f53\u78c1\u6001\u7684\u9c81\u68d2\u5149\u5b66\u5207\u6362\u65b9\u6cd5\uff0c\u4e3a\u8017\u6563\u578b\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u91cf\u5b50\u9648\u7ed3\u5668\u4ef6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5206\u6570\u9648\u94c1\u78c1\u6001\u5728\u6469\u5c14\u8d85\u6676\u683c\u4e2d\u7684\u6709\u6548\u5149\u5b66\u63a7\u5236\uff0c\u4ee5\u7528\u4e8e\u62d3\u6251\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u4f7f\u7528\u5706\u504f\u632f\u5149\u5bf9\u53cc\u5c42\u626d\u66f2MoTe2\u4e2d\u7684\u6574\u6570\u548c\u5206\u6570\u9648\u94c1\u78c1\u6001\u8fdb\u884c\u5149\u5b66\u5207\u6362\u3002", "result": "\u5728\u96f6\u573a\u4e0b\uff0c\u4ec5\u752828\u7eb3\u74e6/\u5e73\u65b9\u5fae\u7c73\u7684\u4f4e\u6cf5\u6d66\u5149\u529f\u7387\u5373\u53ef\u9ad8\u6548\u5730\u64cd\u63a7\u9648\u94c1\u78c1\u6001\u7684\u81ea\u65cb\u53d6\u5411\u3002\u6b64\u5916\uff0c\u8fd8\u6f14\u793a\u4e86\u5149\u5b66\u8bf1\u5bfc\u7684\u78c1\u53cc\u7a33\u6001\u5faa\u73af\u548c\u7574\u58c1\u7684\u7a7a\u95f4\u53ef\u5206\u8fa8\u5199\u5165\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u53ef\u9760\u9ad8\u6548\u7684\u5149\u5b66\u63a7\u5236\u65b9\u6848\uff0c\u7528\u4e8e\u6469\u5c14\u9648\u94c1\u78c1\u4f53\uff0c\u4e3a\u8017\u6563\u578b\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u91cf\u5b50\u9648\u7ed3\u5668\u4ef6\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.19341", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2508.19341", "abs": "https://arxiv.org/abs/2508.19341", "authors": ["Alberto Rolandi"], "title": "Optimal Finite-Time Thermodynamics of Effective Two-Level Systems", "comment": "6 pages, 1 figure, Comments are welcome", "summary": "The optimization of the conversion of thermal energy into work and the\nminimization of dissipation for nano- and mesoscopic systems is a complex\nchallenge because of the important role fluctuations play on the dynamics of\nsmall systems. We generalize the work of Esposito et al. EPL 89, 20003 (2010)\nto optimize at all driving speeds the control needed to extract the maximum\namount of work from any effective two-level systems. These emerge when one\ncoarse-grains degrees of freedom, which is often unavoidable to obtain\n\"real-world\" two-level systems. In particular, we allow even for the system to\nhave underlying quantum dynamics, as long as these allow for a coarse-graining\nthat leads to a Markovian master equation. We analyze the finite-time\nthermodynamics of these systems and find the thermodynamically optimal\nprotocols, which depend on the size of the coarse-graining needed to obtain a\ntwo-level system. Furthermore, we use these results to derive speed-limits for\nany transformation performed on an effective two-level system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f18\u5316\u4e86\u5728\u6240\u6709\u9a71\u52a8\u901f\u5ea6\u4e0b\u4ece\u6709\u6548\u7684\u4e24\u80fd\u7ea7\u7cfb\u7edf\u4e2d\u63d0\u53d6\u6700\u5927\u529f\u6240\u9700\u7684\u63a7\u5236\uff0c\u5e76\u8003\u8651\u4e86\u6da8\u843d\u5bf9\u5c0f\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f18\u5316\u70ed\u80fd\u8f6c\u5316\u4e3a\u529f\u4ee5\u53ca\u6700\u5c0f\u5316\u7eb3\u3001\u4ecb\u89c2\u7cfb\u7edf\u7684\u8017\u6563\uff0c\u540c\u65f6\u8003\u8651\u6da8\u843d\u7684\u4f5c\u7528\u3002", "method": "\u5c06Esposito\u7b49\u4eba\u7684\u5de5\u4f5c\u63a8\u5e7f\u5230\u6240\u6709\u9a71\u52a8\u901f\u5ea6\u4e0b\uff0c\u4ee5\u4f18\u5316\u4ece\u6709\u6548\u7684\u4e24\u80fd\u7ea7\u7cfb\u7edf\u4e2d\u63d0\u53d6\u6700\u5927\u529f\u6240\u9700\u7684\u63a7\u5236\uff0c\u5e76\u5141\u8bb8\u7cfb\u7edf\u5177\u6709\u6f5c\u5728\u7684\u91cf\u5b50\u52a8\u529b\u5b66\uff08\u53ea\u8981\u5b83\u4eec\u80fd\u88ab\u7c97\u7c92\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u4e3b\u65b9\u7a0b\uff09\u3002", "result": "\u627e\u5230\u4e86\u70ed\u529b\u5b66\u6700\u4f18\u7684\u534f\u8bae\uff0c\u8fd9\u4e9b\u534f\u8bae\u53d6\u51b3\u4e8e\u5c06\u7cfb\u7edf\u7c97\u7c92\u5316\u4e3a\u4e24\u80fd\u7ea7\u7cfb\u7edf\u6240\u9700\u7684\u7c97\u7c92\u5316\u5927\u5c0f\u3002", "conclusion": "\u63a8\u5bfc\u4e86\u5728\u6709\u6548\u7684\u4e24\u80fd\u7ea7\u7cfb\u7edf\u4e0a\u8fdb\u884c\u7684\u4efb\u4f55\u53d8\u6362\u7684\u901f\u5ea6\u9650\u5236\u3002"}}
{"id": "2508.19294", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u901a\u8fc7\u878d\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u80fd\u529b\uff0c\u6b63\u5728\u9769\u65b0\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7efc\u8ff0\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86LVLM\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u63a5\u7740\u9610\u8ff0\u4e86\u8fd1\u671fLVLM\u5728\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u7684\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8f93\u51fa\u7075\u6d3b\u6027\uff0c\u91cd\u70b9\u8bf4\u660e\u4e86\u5b83\u4eec\u5982\u4f55\u5b9e\u73b0\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u8be5\u7efc\u8ff0\u6df1\u5165\u5206\u6790\u4e86\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u6574\u5408\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4f7f\u7528VLM\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u7684\u8fdb\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u6bd4\u8f83\u4e86LVLM\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002\u7814\u7a76\u9884\u6d4bLVLM\u5c06\u5f88\u5feb\u8fbe\u5230\u6216\u8d85\u8fc7\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524dLVLM\u7684\u5c40\u9650\u6027\u3001\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002\u6700\u7ec8\u8ba4\u4e3a\uff0cLVLM\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u5bf9\u76ee\u6807\u68c0\u6d4b\u548c\u673a\u5668\u4eba\u5e94\u7528\u4ea7\u751f\u4e86\u9769\u547d\u6027\u5f71\u54cd\uff0c\u5e76\u5c06\u7ee7\u7eed\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002", "motivation": "\u672c\u7efc\u8ff0\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u548c\u603b\u7ed3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u9610\u8ff0\u5176\u5de5\u4f5c\u539f\u7406\u3001\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u8303\u5f0f\u53ca\u4f18\u52bf\uff0c\u5e76\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u6307\u51fa\u5176\u5c40\u9650\u6027\u5e76\u5c55\u671b\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u672c\u7efc\u8ff0\u901a\u8fc7\u4e09\u6b65\u7814\u7a76\u8bc4\u5ba1\u8fc7\u7a0b\u8fdb\u884c\u7ec4\u7ec7\uff1a\u9996\u5148\uff0c\u8ba8\u8bba\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u529f\u80fd\uff0c\u89e3\u91ca\u5b83\u4eec\u5982\u4f55\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u6280\u672f\u9769\u65b0\u76ee\u6807\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff1b\u5176\u6b21\uff0c\u89e3\u91ca\u8fd1\u671fLVLM\u5728\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u7684\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8f93\u51fa\u7075\u6d3b\u6027\uff0c\u5f3a\u8c03\u5b83\u4eec\u5982\u4f55\u5b9e\u73b0\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\uff1b\u6700\u540e\uff0c\u901a\u8fc7\u5168\u9762\u7684\u53ef\u89c6\u5316\u5c55\u793aLVLM\u5728\u5305\u62ec\u5b9a\u4f4d\u548c\u5206\u5272\u5728\u5185\u7684\u5404\u79cd\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5176\u5b9e\u65f6\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u590d\u6742\u6027\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LVLM\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5176\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u9884\u671f\u5176\u6027\u80fd\u5c06\u5f88\u5feb meet or surpass \u4f20\u7edf\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u4e86\u5f53\u524dLVLM\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u57fa\u4e8e\u5bf9LVLM\u7684\u5168\u9762\u56de\u987e\uff0c\u5f97\u51fa\u7ed3\u8bba\u8ba4\u4e3a\uff0cLVLM\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u5bf9\u76ee\u6807\u68c0\u6d4b\u548c\u673a\u5668\u4eba\u5e94\u7528\u4ea7\u751f\u4e86\u9769\u547d\u6027\u5f71\u54cd\uff0c\u5e76\u5c06\u7ee7\u7eed\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002LVLM\u6709\u671b\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u8fbe\u5230\u6216\u8d85\u8fc7\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19398", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19398", "abs": "https://arxiv.org/abs/2508.19398", "authors": ["Junkai Wang", "Yuxuan Zhao", "Mi Zhou", "Fumin Zhang"], "title": "Learning Robust Regions of Attraction Using Rollout-Enhanced Physics-Informed Neural Networks with Policy Iteration", "comment": "Submitted to the American Control Conference (ACC 2026)", "summary": "The region of attraction is a key metric of the robustness of systems. This\npaper addresses the numerical solution of the generalized Zubov's equation,\nwhich produces a special Lyapunov function characterizing the robust region of\nattraction for perturbed systems. To handle the highly nonlinear characteristic\nof the generalized Zubov's equation, we propose a physics-informed neural\nnetwork framework that employs a policy iteration training scheme with rollout\nto approximate the viscosity solution. In addition to computing the optimal\ndisturbance during the policy improvement process, we incorporate neural\nnetwork-generated value estimates as anchor points to facilitate the training\nprocedure to prevent singularities in both low- and high-dimensional systems.\nNumerical simulations validate the effectiveness of the proposed approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6c42\u89e3\u5e7f\u4e49Zubov\u65b9\u7a0b\uff0c\u4ee5\u8868\u5f81\u53d7\u6270\u52a8\u7cfb\u7edf\u7684\u9c81\u68d2\u5438\u5f15\u57df\u3002", "motivation": "\u8ba1\u7b97\u53d7\u6270\u52a8\u7cfb\u7edf\u7684\u9c81\u68d2\u5438\u5f15\u57df\uff0c\u8be5\u5438\u5f15\u57df\u7531\u5e7f\u4e49Zubov\u65b9\u7a0b\u7684\u89e3\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u91c7\u7528\u7b56\u7565\u8fed\u4ee3\u8bad\u7ec3\u65b9\u6848\u548c\u6a21\u62df\u6765\u8fd1\u4f3c\u7c98\u6027\u89e3\u3002\u5728\u7b56\u7565\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u6700\u4f18\u6270\u52a8\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4ef7\u503c\u4f30\u8ba1\u4f5c\u4e3a\u951a\u70b9\uff0c\u4ee5\u9632\u6b62\u4f4e\u7ef4\u548c\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u7684\u5947\u5f02\u6027\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8fd1\u4f3c\u5e7f\u4e49Zubov\u65b9\u7a0b\u7684\u7c98\u6027\u89e3\uff0c\u4ece\u800c\u8868\u5f81\u53d7\u6270\u52a8\u7cfb\u7edf\u7684\u9c81\u68d2\u5438\u5f15\u57df\u3002"}}
{"id": "2508.19806", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19806", "abs": "https://arxiv.org/abs/2508.19806", "authors": ["Shenqi Wang", "Guangzhi Tang"], "title": "Context-aware Sparse Spatiotemporal Learning for Event-based Vision", "comment": "Accepted at IROS 2025", "summary": "Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.", "AI": {"tldr": "event-based cameras offer advantages but existing deep learning methods struggle with sparse data on resource-constrained devices. Neuromorphic computing with spiking neural networks is energy-efficient but lacks performance. This paper proposes CSSL, a framework using context-aware thresholding to dynamically reduce neuron activations, achieving high sparsity and comparable/superior performance in object detection and optical flow estimation, enabling efficient event-based vision for neuromorphic processing.", "motivation": "Existing deep learning-based event processing methods do not fully utilize the sparse nature of event data, making integration into resource-constrained edge applications difficult. Spiking neural networks, while energy-efficient, underperform in complex event-based vision tasks, and achieving high activation sparsity in neural networks is challenging.", "method": "The paper proposes Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints.", "result": "CSSL achieves comparable or superior performance to state-of-the-art methods in event-based object detection and optical flow estimation while maintaining extremely high neuronal sparsity. Experimental results demonstrate CSSL's effectiveness in enabling efficient event-based vision for neuromorphic processing.", "conclusion": "CSSL is a novel framework that effectively addresses the challenges of processing sparse event data for robot perception, enabling efficient and high-performing event-based vision on resource-constrained devices through context-aware thresholding and high neuronal sparsity."}}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "\u901a\u8fc7\u201c5+2\u201d\u6846\u67b6\u8bc6\u522b\u5e76\u79fb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5728\u51cf\u5c11\u6b21\u4f18\u8f68\u8ff9\u7684\u540c\u65f6\uff0c\u4f7f\u7528\u66f4\u5c11\u6570\u636e\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u63a8\u7406\u8f68\u8ff9\u7684\u67d0\u4e9b\u90e8\u5206\u53ef\u80fd\u5bf9\u6574\u4f53\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u201c5+2\u201d\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u6807\u51c6\u7684\u6807\u51c6\u6765\u8bc6\u522b\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u8bc4\u4f30\u5176\u72ec\u7acb\u6027\uff0c\u4ee5\u786e\u4fdd\u79fb\u9664\u5b83\u4eec\u4e0d\u4f1a\u5f71\u54cd\u63a8\u7406\u7684\u6d41\u7545\u6027\u548c\u8fde\u8d2f\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u91c7\u6837\u7b97\u6cd5\u6765\u9009\u62e9\u6700\u4f18\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c06\u6b21\u4f18\u5b50\u8f68\u8ff9\u51cf\u5c11\u4e8625.9%\u3002\u5728\u4f7f\u7528\u4e09\u5206\u4e4b\u4e8c\u7684\u8bad\u7ec3\u6570\u636e\u5bf9Qwen2.5-Math-7B\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ece58.06%\u63d0\u9ad8\u523058.92%\uff0c\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u548c\u5176\u4ed6\u5f00\u6e90\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u201c5+2\u201d\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5e76\u79fb\u9664LLM\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u90e8\u5206\uff0c\u4ece\u800c\u5728\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19429", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.19429", "abs": "https://arxiv.org/abs/2508.19429", "authors": ["Gustavo A. Cardona", "Kaier Liang", "Cristian-Ioan Vasile"], "title": "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals", "comment": null, "summary": "This paper presents an iterative approach for heterogeneous multi-agent route\nplanning in environments with unknown resource distributions. We focus on a\nteam of robots with diverse capabilities tasked with executing missions\nspecified using Capability Temporal Logic (CaTL), a formal framework built on\nSignal Temporal Logic to handle spatial, temporal, capability, and resource\nconstraints. The key challenge arises from the uncertainty in the initial\ndistribution and quantity of resources in the environment. To address this, we\nintroduce an iterative algorithm that dynamically balances exploration and task\nfulfillment. Robots are guided to explore the environment, identifying resource\nlocations and quantities while progressively refining their understanding of\nthe resource landscape. At the same time, they aim to maximally satisfy the\nmission objectives based on the current information, adapting their strategies\nas new data is uncovered. This approach provides a robust solution for planning\nin dynamic, resource-constrained environments, enabling efficient coordination\nof heterogeneous teams even under conditions of uncertainty. Our method's\neffectiveness and performance are demonstrated through simulated case studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u5f02\u6784\u591a\u673a\u5668\u4eba\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5728\u8d44\u6e90\u5206\u5e03\u4e0d\u786e\u5b9a\u4e14\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u6267\u884c\u5177\u6709\u65f6\u7a7a\u3001\u80fd\u529b\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u4efb\u52a1\u65f6\u6240\u9762\u4e34\u7684\u8def\u5f84\u89c4\u5212\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u63a2\u7d22\u548c\u4efb\u52a1\u5b8c\u6210\uff0c\u4ee5\u5e94\u5bf9\u8d44\u6e90\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u673a\u5668\u4eba\u88ab\u5f15\u5bfc\u53bb\u63a2\u7d22\u73af\u5883\uff0c\u8bc6\u522b\u8d44\u6e90\uff0c\u5e76\u6839\u636e\u65b0\u53d1\u73b0\u7684\u6570\u636e\u8c03\u6574\u5b83\u4eec\u6ee1\u8db3\u4efb\u52a1\u76ee\u6807\u7684\u6700\u5927\u5316\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u534f\u8c03\u3002"}}
{"id": "2508.19344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19344", "abs": "https://arxiv.org/abs/2508.19344", "authors": ["Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Re:Frame -- Retrieving Experience From Associative Memory", "comment": "11 pages, 3 figures", "summary": "Offline reinforcement learning (RL) often deals with suboptimal data when\ncollecting large expert datasets is unavailable or impractical. This limitation\nmakes it difficult for agents to generalize and achieve high performance, as\nthey must learn primarily from imperfect or inconsistent trajectories. A\ncentral challenge is therefore how to best leverage scarce expert\ndemonstrations alongside abundant but lower-quality data. We demonstrate that\nincorporating even a tiny amount of expert experience can substantially improve\nRL agent performance. We introduce Re:Frame (Retrieving Experience From\nAssociative Memory), a plug-in module that augments a standard offline RL\npolicy (e.g., Decision Transformer) with a small external Associative Memory\nBuffer (AMB) populated by expert trajectories drawn from a separate dataset.\nDuring training on low-quality data, the policy learns to retrieve expert data\nfrom the Associative Memory Buffer (AMB) via content-based associations and\nintegrate them into decision-making; the same AMB is queried at evaluation.\nThis requires no environment interaction and no modifications to the backbone\narchitecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories\n(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a\nstrong Decision Transformer baseline in three of four settings, with gains up\nto +10.7 normalized points. These results show that Re:Frame offers a simple\nand data-efficient way to inject scarce expert knowledge and substantially\nimprove offline RL from low-quality datasets.", "AI": {"tldr": "Offline RL often struggles with suboptimal data, but incorporating even a small amount of expert data can significantly improve performance. Re:Frame is a plug-in module that uses an Associative Memory Buffer (AMB) populated with expert trajectories to augment standard offline RL policies. It learns to retrieve and integrate expert data, showing substantial gains on D4RL MuJoCo tasks with minimal expert data.", "motivation": "Offline RL agents struggle to generalize and achieve high performance when trained primarily on suboptimal or inconsistent trajectories due to the unavailability or impracticality of collecting large expert datasets. The challenge lies in effectively utilizing scarce expert demonstrations alongside abundant but lower-quality data.", "method": "Re:Frame introduces a plug-in module that augments a standard offline RL policy with a small external Associative Memory Buffer (AMB). This AMB is populated with expert trajectories from a separate dataset. During training on low-quality data, the policy learns to retrieve expert data from the AMB through content-based associations and integrate it into decision-making. The same AMB is queried during evaluation. This method requires no environment interaction or modifications to the backbone architecture.", "result": "On D4RL MuJoCo tasks, Re:Frame consistently improved performance over a strong Decision Transformer baseline in three out of four settings, achieving gains up to +10.7 normalized points. These improvements were observed using as few as 60 expert trajectories, representing only 0.1% of a 6000-trajectory dataset.", "conclusion": "Re:Frame provides a simple and data-efficient approach to inject scarce expert knowledge into offline RL policies, leading to substantial improvements when learning from low-quality datasets."}}
{"id": "2508.19805", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19805", "abs": "https://arxiv.org/abs/2508.19805", "authors": ["Shota Naito", "Tsukasa Ninomiya", "Koichi Wada"], "title": "Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers", "comment": null, "summary": "Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u673a\u5668\u4eba\u80fd\u529b\u3001\u5149\u7167\u53ef\u89c2\u5bdf\u6027\u3001\u8c03\u5ea6\u5668\u540c\u6b65\u6027\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u548c\u5206\u79bb\u7ed3\u679c\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u9886\u57df\uff0c\u7406\u89e3\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u4e4b\u95f4\u7684\u6210\u5bf9\u5206\u79bb\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u673a\u5668\u4eba\u80fd\u529b\u3001\u5149\u7167\u53ef\u89c2\u5bdf\u6027\u4ee5\u53ca\u8c03\u5ea6\u5668\u540c\u6b65\u6027\u4e4b\u95f4\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u672c\u6587\u9996\u5148\u8bc1\u660e\u4e86\u6307\u6570\u65f6\u95f4\u6269\u5c55\uff08ETE\uff09\u95ee\u9898\u4ec5\u5728\u6700\u5f3a\u7684\u6a21\u578b\u2014\u2014\u5177\u6709\u5b8c\u5168\u76f8\u4e92\u5149\u7167\u7684\u5b8c\u5168\u540c\u6b65\u673a\u5668\u4eba\uff08$\text{LUMT}^F$\uff09\u4e2d\u53ef\u89e3\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5f15\u5165\u516d\u8fb9\u5f62\u8fb9\u904d\u5386\uff08HET\uff09\u548cTAR(d)*\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5185\u90e8\u8bb0\u5fc6\u548c\u5149\u7167\u5982\u4f55\u5728\u5f31\u540c\u6b65\u548c\u5168\u540c\u6b65\u6761\u4ef6\u4e0b\u4e0e\u540c\u6b65\u6027\u76f8\u4e92\u4f5c\u7528\u3002\u5728\u5f02\u6b65\u8bbe\u7f6e\u4e0b\uff0c\u672c\u6587\u5bf9LP-MLCv\u3001VEC\u548cZCC\u7b49\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u63ed\u793a\u4e86$\text{FSTA}$\u548c$\text{FCOM}$\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5206\u79bb\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u9876\u70b9\u904d\u5386\u96c6\u5408\uff08VTR\uff09\u548c\u79bb\u5f00\u5730\u70b9\u6536\u655b\uff08LP-Cv\uff09\u95ee\u9898\uff0c\u9610\u91ca\u4e86\u5728\u5bf9\u79f0\u73af\u5883\u4e2d\u5185\u90e8\u8bb0\u5fc6\u7684\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6269\u5c55\u4e8614\u4e2a\u5178\u578b\u673a\u5668\u4eba\u6a21\u578b\u7684\u5df2\u77e5\u5206\u79bb\u56fe\uff0c\u63ed\u793a\u4e86\u53ea\u6709\u901a\u8fc7\u9ad8\u9636\u6bd4\u8f83\u624d\u80fd\u663e\u73b0\u7684\u7ed3\u6784\u73b0\u8c61\u3002\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u4e0d\u53ef\u80fd\u5224\u636e\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u53ef\u89c2\u5bdf\u6027\u3001\u8bb0\u5fc6\u548c\u540c\u6b65\u6027\u5982\u4f55\u5171\u540c\u5f71\u54cd\u79fb\u52a8\u673a\u5668\u4eba\u8ba1\u7b97\u80fd\u529b\u7684\u7406\u89e3\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u540c\u6b65\u6027\u3001\u8bb0\u5fc6\u548c\u53ef\u89c2\u5bdf\u6027\u7ec4\u5408\u4e0b\u7684\u80fd\u529b\u5dee\u5f02\u3002\u8fd9\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u9760\u7684\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan \u00d6 Ar\u0131k"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS\u662f\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u4ee3\u7406\u7cfb\u7edf\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u63d0\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u4e2d\u95f4\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u901a\u5e38\u9700\u8981\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5927\u91cf\u7684\u9884\u5904\u7406\u548c\u5fae\u8c03\u3002\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6765\u63d0\u9ad8LLM\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u521b\u5efa\u8fd9\u6837\u7684\u63d0\u793a\u65e2\u7e41\u7410\u53c8\u4e34\u65f6\u7684\u3002FLAIRR-TS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4f18\u5316\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "method": "FLAIRR-TS\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u201c\u9884\u6d4b\u5668\u201d\u4ee3\u7406\uff0c\u5b83\u4f7f\u7528\u521d\u59cb\u63d0\u793a\u751f\u6210\u9884\u6d4b\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u201c\u7cbe\u70bc\u5668\u201d\u4ee3\u7406\u4f1a\u6839\u636e\u8fc7\u53bb\u7684\u8f93\u51fa\u548c\u68c0\u7d22\u5230\u7684\u7c7b\u4f3c\u7269\u6765\u6539\u8fdb\u8fd9\u4e2a\u63d0\u793a\u3002\u8fd9\u79cd\u81ea\u9002\u5e94\u63d0\u793a\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u901a\u7528\u7684\u63d0\u793a\u6a21\u677f\uff0c\u80fd\u591f\u8de8\u9886\u57df\u8fdb\u884c\u6cdb\u5316\uff0c\u5e76\u4e14\u65e0\u9700\u4e2d\u95f4\u4ee3\u7801\u751f\u6210\u5373\u53ef\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFLAIRR-TS\u7684\u51c6\u786e\u6027\u4f18\u4e8e\u9759\u6001\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u5e76\u4e14\u63a5\u8fd1\u4e8e\u4e13\u95e8\u63d0\u793a\u7684\u6027\u80fd\u3002", "conclusion": "FLAIRR-TS\u901a\u8fc7\u5176\u4ee3\u7406\u65b9\u6cd5\u5b9e\u73b0\u81ea\u9002\u5e94\u63d0\u793a\u6539\u8fdb\u548c\u68c0\u7d22\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4e0d\u8fdb\u884c\u4e2d\u95f4\u4ee3\u7801\u751f\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8de8\u57df\u6cdb\u5316\u548c\u9ad8\u8d28\u91cf\u9884\u6d4b\u3002"}}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5c06MARL\u91cd\u65b0\u6784\u5efa\u4e3a\u4e00\u7cfb\u5217\u5355\u4ee3\u7406RL\u4efb\u52a1\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4f4e\u6548\u7387\u548c\u67b6\u6784\u4e0d\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5e76\u5728GUI\u63a7\u5236\u548c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5355\u4e00\u4ee3\u7406\u65b9\u6cd5\u5728\u79fb\u52a8GUI\u4ee3\u7406\u65b9\u9762\u53d7\u7ed3\u6784\u9650\u5236\u4ee5\u53ca\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u6548\u7387\u548c\u4e0eLVLM\u67b6\u6784\u517c\u5bb9\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "SWIRL\u662f\u4e00\u79cd\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5c06MARL\u91cd\u65b0\u6784\u5efa\u4e3a\u4e00\u7cfb\u5217\u5355\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u4e00\u6b21\u66f4\u65b0\u4e00\u4e2a\u4ee3\u7406\u5e76\u4fdd\u6301\u5176\u4ed6\u4ee3\u7406\u56fa\u5b9a\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u8de8\u4ee3\u7406\u534f\u8c03\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u5bfc\u822a\u5668\uff08\u5c06\u8bed\u8a00\u548c\u5c4f\u5e55\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8ba1\u5212\uff09\u548c\u4e00\u4e2a\u4ea4\u4e92\u5668\uff08\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3a\u539f\u5b50\u64cd\u4f5c\uff09\u3002", "result": "SWIRL\u5728\u79fb\u52a8GUI\u63a7\u5236\u7684\u9ad8\u4f4e\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u5728\u591a\u4ee3\u7406\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002", "conclusion": "SWIRL\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u901a\u7528\u6846\u67b6\uff0c\u5728\u79fb\u52a8GUI\u63a7\u5236\u548c\u591a\u4ee3\u7406\u6570\u5b66\u63a8\u7406\u7b49\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.19674", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19674", "abs": "https://arxiv.org/abs/2508.19674", "authors": ["Hyunsung Cho", "Minseok Moon", "Jaehoon Kim", "Eunkyung Koh", "Hyeon-Deuk Kim", "Rokyeon Kim", "Gyehyun Park", "Seungwu Han", "Youngho Kang"], "title": "Atomistic insights into hydrogen migration in IGZO from machine-learning interatomic potential: linking atomic diffusion to device performance", "comment": "13 pages, 9 figures, Supplementary information included as ancillary\n  file (+15 pages)", "summary": "Understanding hydrogen diffusion is critical for improving the reliability\nand performance of oxide thin-film transistors (TFTs), where hydrogen plays a\nkey role in carrier modulation and bias instability. In this work, we\ninvestigate hydrogen diffusion in amorphous IGZO ($a$-IGZO) and $c$-axis\naligned crystalline IGZO (CAAC-IGZO) using machine learning interatomic\npotential molecular dynamics (MLIP-MD) simulations. We construct accurate\nphase-specific MLIPs by fine-tuning SevenNet-0, a universal pretrained MLIP,\nand validate the models against a comprehensive dataset covering\nhydrogen-related configurations and diffusion environments. Hydrogen\ndiffusivity is evaluated over 650--1700 K, revealing enhanced mobility above\n750 K in $a$-IGZO due to the glassy matrix, while diffusion at lower\ntemperatures is constrained by the rigid network. Arrhenius extrapolation of\nthe diffusivity indicates that hydrogen in $a$-IGZO can reach the\nchannel/insulator interface within $10^{4}$ seconds at 300--400 K, likely\ncontributing to negative bias stress-induced device degradation. Trajectory\nanalysis reveals that long-range diffusion in $a$-IGZO is enabled by a\ncombination of hydrogen hopping and flipping mechanisms. In CAAC-IGZO, hydrogen\nexhibits high in-plane diffusivity but severely restricted out-of-plane\ntransport due to a high energy barrier along the $c$-axis. This limited\nvertical diffusion in CAAC-IGZO suggests minimal impact on bias instability.\nThis work bridges the atomic-level hydrogen transport mechanism and\ndevice-level performance in oxide TFTs by leveraging large-scale MLIP-MD\nsimulations.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u673a\u5668\u5b66\u4e60\u52bf\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7814\u7a76\u4e86\u975e\u6676IGZO\u548cCAAC-IGZO\u4e2d\u7684\u6c22\u6269\u6563\u673a\u5236\u3002\u7ed3\u679c\u8868\u660e\uff0c\u975e\u6676IGZO\u5728\u9ad8\u6e29\u4e0b\u6c22\u6269\u6563\u80fd\u529b\u589e\u5f3a\uff0c\u4e14\u5728300-400K\u65f6\u53ef\u572810^4\u79d2\u5185\u5230\u8fbe\u6c9f\u9053/\u7edd\u7f18\u5c42\u754c\u9762\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d1f\u504f\u538b\u5e94\u529b\u4e0b\u7684\u5668\u4ef6\u9000\u5316\u3002CAAC-IGZO\u4e2d\u7684\u6c22\u6269\u6563\u80fd\u529b\u53d7c\u8f74\u65b9\u5411\u9ad8\u80fd\u5792\u7684\u9650\u5236\uff0c\u5782\u76f4\u6269\u6563\u53d7\u963b\uff0c\u5bf9\u504f\u538b\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7406\u89e3\u6c22\u5728\u6c27\u5316\u7269\u8584\u819c\u6676\u4f53\u7ba1\uff08TFT\uff09\u4e2d\u7684\u6269\u6563\u5bf9\u4e8e\u63d0\u9ad8\u5668\u4ef6\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u6c22\u5728\u8f7d\u6d41\u5b50\u8c03\u5236\u548c\u504f\u538b\u4e0d\u7a33\u5b9a\u6027\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u6587\u91c7\u7528\u673a\u5668\u5b66\u4e60\u52bf\u5206\u5b50\u52a8\u529b\u5b66\uff08MLIP-MD\uff09\u6a21\u62df\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u9488\u5bf9\u975e\u6676IGZO\uff08a-IGZO\uff09\u548cc\u8f74\u53d6\u5411\u6676\u4f53IGZO\uff08CAAC-IGZO\uff09\u7684\u673a\u5668\u5b66\u4e60\u52bf\uff08MLIP\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u6c22\u6269\u6563\u6a21\u62df\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728650-1700 K\u7684\u6e29\u5ea6\u8303\u56f4\u5185\uff0ca-IGZO\u4e2d\u7684\u6c22\u8fc1\u79fb\u7387\u5728750 K\u4ee5\u4e0a\u589e\u5f3a\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u73bb\u7483\u57fa\u8d28\u3002\u5728\u8f83\u4f4e\u6e29\u5ea6\u4e0b\uff0c\u6269\u6563\u53d7\u5230\u521a\u6027\u7f51\u7edc\u7684\u9650\u5236\u3002\u901a\u8fc7Arrhenius\u5916\u63a8\uff0ca-IGZO\u4e2d\u7684\u6c22\u5728300-400 K\u4e0b\u53ef\u572810^4\u79d2\u5185\u5230\u8fbe\u6c9f\u9053/\u7edd\u7f18\u5c42\u754c\u9762\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8d1f\u504f\u538b\u5e94\u529b\u4e0b\u7684\u5668\u4ef6\u9000\u5316\u3002\u8f68\u8ff9\u5206\u6790\u8868\u660e\uff0ca-IGZO\u4e2d\u7684\u957f\u8ddd\u79bb\u6269\u6563\u662f\u901a\u8fc7\u6c22\u8df3\u8dc3\u548c\u7ffb\u8f6c\u673a\u5236\u5171\u540c\u5b9e\u73b0\u7684\u3002\u5728CAAC-IGZO\u4e2d\uff0c\u6c22\u7684\u5e73\u9762\u5185\u6269\u6563\u80fd\u529b\u5f88\u5f3a\uff0c\u4f46\u7531\u4e8ec\u8f74\u65b9\u5411\u5b58\u5728\u9ad8\u80fd\u5792\uff0c\u5176\u5e73\u9762\u5916\u6269\u6563\u53d7\u5230\u4e25\u91cd\u9650\u5236\uff0c\u8fd9\u8868\u660e\u5176\u5bf9\u504f\u538b\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21MLIP-MD\u6a21\u62df\uff0c\u5c06\u539f\u5b50\u5c42\u9762\u7684\u6c22\u4f20\u8f93\u673a\u5236\u4e0e\u6c27\u5316\u7269TFT\u5668\u4ef6\u5c42\u9762\u7684\u6027\u80fd\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u5584TFT\u5668\u4ef6\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.20041", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.20041", "abs": "https://arxiv.org/abs/2508.20041", "authors": ["Thomas Bl\u00e4sius", "Henrik Cs\u00f6re", "Max G\u00f6ttlicher", "Elly Schmidt", "Wendy Yi"], "title": "Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem", "comment": null, "summary": "Motivated by hierarchical networks, we introduce the Flow-weighted Layered\nMetric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the\nEuclidean Steiner tree with layered structure and capacity constraints per\nlayer. The goal is to construct a cost-optimal Steiner forest connecting a set\nof sources to a set of sinks under load-dependent edge costs. We prove that\nFLaMECaST is NP-hard to approximate, even in restricted cases where all sources\nlie on a circle. However, assuming few additional constraints for such\ninstances, we design a dynamic program that achieves a $\\left(1 +\n\\frac{1}{2^n}\\right)$-approximation in polynomial time. By generalizing the\nstructural insights the dynamic program is based on, we extend the approach to\ncertain settings, where all sources are positioned on a convex polygon.", "AI": {"tldr": "FLaMECaST\u662f\u5177\u6709\u5206\u5c42\u7ed3\u6784\u548c\u6bcf\u5c42\u5bb9\u91cf\u7ea6\u675f\u7684\u6b27\u51e0\u91cc\u5f97 Steiner \u6811\u7684\u53d8\u4f53\uff0c\u76ee\u6807\u662f\u5728\u8d1f\u8f7d\u76f8\u5173\u8fb9\u6210\u672c\u4e0b\u6784\u5efa\u8fde\u63a5\u6e90\u5230\u6c47\u7684\u6210\u672c\u6700\u4f18 Steiner \u68ee\u6797\u3002FLaMECaST \u751a\u81f3\u5728\u6e90\u4f4d\u4e8e\u5706\u4e0a\u7684\u53d7\u9650\u60c5\u51b5\u4e0b\u4e5f\u96be\u4ee5\u8fd1\u4f3c\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6b64\u7c7b\u5b9e\u4f8b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u6001\u89c4\u5212\uff0c\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0 (1 + 1/2^n) \u7684\u8fd1\u4f3c\u3002", "motivation": "\u53d7\u5206\u5c42\u7f51\u7edc\u7684\u542f\u53d1\uff0c\u8bba\u6587\u4ecb\u7ecd\u4e86 FLaMECaST \u95ee\u9898\uff0c\u8fd9\u662f\u6b27\u51e0\u91cc\u5f97 Steiner \u6811\u7684\u4e00\u4e2a\u53d8\u4f53\uff0c\u5177\u6709\u5206\u5c42\u7ed3\u6784\u548c\u6bcf\u5c42\u5bb9\u91cf\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86 (1 + 1/2^n) \u7684\u8fd1\u4f3c\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u6e90\u4f4d\u4e8e\u51f8\u591a\u8fb9\u5f62\u7684\u60c5\u51b5\u3002", "result": "FLaMECaST \u88ab\u8bc1\u660e NP-hard \u96be\u4ee5\u8fd1\u4f3c\uff0c\u5373\u4f7f\u5728\u6e90\u4f4d\u4e8e\u5706\u4e0a\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u7136\u800c\uff0c\u63d0\u51fa\u7684\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u4e3a\u67d0\u4e9b\u5b9e\u4f8b\u63d0\u4f9b\u4e86\u8fd1\u4f3c\u4fdd\u8bc1\u3002", "conclusion": "FLaMECaST \u95ee\u9898\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u548c\u7ed3\u6784\u6d1e\u5bdf\uff0c\u53ef\u4ee5\u4e3a\u7279\u5b9a\u5b9e\u4f8b\u5f00\u53d1\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2508.19552", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19552", "abs": "https://arxiv.org/abs/2508.19552", "authors": ["Shuo Chang", "Rui Sun", "Jiashuo He", "Sai Huang", "Kan Yu", "Zhiyong Feng"], "title": "CSRD2025: A Large-Scale Synthetic Radio Dataset for Spectrum Sensing in Wireless Communications", "comment": null, "summary": "The development of Large AI Models (LAMs) for wireless communications,\nparticularly for complex tasks like spectrum sensing, is critically dependent\non the availability of vast, diverse, and realistic datasets. Addressing this\nneed, this paper introduces the ChangShuoRadioData (CSRD) framework, an\nopen-source, modular simulation platform designed for generating large-scale\nsynthetic radio frequency (RF) data. CSRD simulates the end-to-end transmission\nand reception process, incorporating an extensive range of modulation schemes\n(100 types, including analog, digital, OFDM, and OTFS), configurable channel\nmodels featuring both statistical fading and site-specific ray tracing using\nOpenStreetMap data, and detailed modeling of realistic RF front-end impairments\nfor various antenna configurations (SISO/MISO/MIMO). Using this framework, we\ncharacterize CSRD2025, a substantial dataset benchmark comprising over\n25,000,000 frames (approx. 200TB), which is approximately 10,000 times larger\nthan the widely used RML2018 dataset. CSRD2025 offers unprecedented signal\ndiversity and complexity, specifically engineered to bridge the Sim2Real gap.\nFurthermore, we provide processing pipelines to convert IQ data into\nspectrograms annotated in COCO format, facilitating object detection approaches\nfor time-frequency signal analysis. The dataset specification includes\nstandardized 8:1:1 training, validation, and test splits (via frame indices) to\nensure reproducible research. The CSRD framework is released at\nhttps://github.com/Singingkettle/ChangShuoRadioData to accelerate the\nadvancement of AI-driven spectrum sensing and management.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aChangShuoRadioData (CSRD) \u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u5c04\u9891\u6570\u636e\uff0c\u4ee5\u6ee1\u8db3\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u5927\u578bAI\u6a21\u578b\uff08LAMs\uff09\u5bf9\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002CSRD\u80fd\u591f\u6a21\u62df\u7aef\u5230\u7aef\u7684\u4fe1\u53f7\u4f20\u8f93\u548c\u63a5\u6536\u8fc7\u7a0b\uff0c\u652f\u6301\u591a\u79cd\u8c03\u5236\u65b9\u6848\u3001\u53ef\u914d\u7f6e\u7684\u4fe1\u9053\u6a21\u578b\uff08\u5305\u62ec\u57fa\u4e8eOpenStreetMap\u7684\u5c04\u7ebf\u8ffd\u8e2a\uff09\u4ee5\u53ca\u771f\u5b9e\u7684\u5c04\u9891\u524d\u7aef\u635f\u4f24\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u8bba\u6587\u63d0\u51fa\u4e86CSRD2025\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc72500\u4e07\u5e27\uff08\u7ea6200TB\uff09\u7684\u6570\u636e\uff0c\u89c4\u6a21\u8fdc\u8d85\u73b0\u6709\u6570\u636e\u96c6\uff0c\u65e8\u5728\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86IQ\u6570\u636e\u5230COCO\u683c\u5f0f\u9891\u8c31\u56fe\u7684\u8f6c\u6362\u6d41\u7a0b\uff0c\u5e76\u8bbe\u5b9a\u4e86\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\uff088:1:1\uff09\uff0c\u4ee5\u4fc3\u8fdbAI\u9a71\u52a8\u7684\u9891\u8c31\u611f\u77e5\u548c\u7ba1\u7406\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\uff0c\u5927\u578bAI\u6a21\u578b\uff08LAMs\uff09\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9891\u8c31\u611f\u77e5\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u6781\u5ea6\u4f9d\u8d56\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u6570\u636e\u96c6\u3002\u4e3a\u6ee1\u8db3\u6b64\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u6b64\u7c7b\u6570\u636e\u96c6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86ChangShuoRadioData (CSRD) \u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u6a21\u5757\u5316\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u5c04\u9891\uff08RF\uff09\u6570\u636e\u3002CSRD\u80fd\u591f\u7aef\u5230\u7aef\u5730\u6a21\u62df\u4fe1\u53f7\u7684\u4f20\u8f93\u548c\u63a5\u6536\uff0c\u652f\u6301100\u79cd\u8c03\u5236\u65b9\u6848\uff0c\u5305\u62ec\u6a21\u62df\u3001\u6570\u5b57\u3001OFDM\u548cOTFS\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u4fe1\u9053\u6a21\u578b\uff0c\u652f\u6301\u7edf\u8ba1\u8870\u843d\u548c\u57fa\u4e8eOpenStreetMap\u6570\u636e\u7684\u7279\u5b9a\u7ad9\u70b9\u5c04\u7ebf\u8ffd\u8e2a\u3002\u6b64\u5916\uff0cCSRD\u8fd8\u8be6\u7ec6\u6a21\u62df\u4e86\u4e0d\u540c\u5929\u7ebf\u914d\u7f6e\uff08SISO/MISO/MIMO\uff09\u4e0b\u7684\u5c04\u9891\u524d\u7aef\u635f\u4f24\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u7814\u7a76\u8005\u4eec\u521b\u5efa\u4e86CSRD2025\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc72500\u4e07\u5e27\uff08\u7ea6200TB\uff09\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5c06IQ\u6570\u636e\u8f6c\u6362\u4e3aCOCO\u683c\u5f0f\u9891\u8c31\u56fe\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u7684\u65f6\u9891\u4fe1\u53f7\u5206\u6790\u3002\u6700\u540e\uff0c\u8bba\u6587\u8fd8\u89c4\u5b9a\u4e86\u6807\u51c6\u5316\u76848:1:1\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6570\u636e\u5212\u5206\uff08\u901a\u8fc7\u5e27\u7d22\u5f15\uff09\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002", "result": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u4e86CSRD\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u8be5\u6846\u67b6\u6784\u5efa\u4e86CSRD2025\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u89c4\u6a21\u5e9e\u5927\uff08\u7ea6200TB\uff09\uff0c\u8fdc\u8d85\u73b0\u6709\u6570\u636e\u96c6\uff08\u5982RML2018\uff09\uff0c\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u4fe1\u53f7\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u65e8\u5728\u6709\u6548\u5f25\u5408\u4eff\u771f\uff08Sim\uff09\u4e0e\u73b0\u5b9e\uff08Real\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86IQ\u6570\u636e\u5230COCO\u683c\u5f0f\u9891\u8c31\u56fe\u7684\u8f6c\u6362\u6d41\u7a0b\uff0c\u4e3a\u65f6\u9891\u4fe1\u53f7\u5206\u6790\u4e2d\u7684\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u652f\u6301\u3002\u6807\u51c6\u5316\u76848:1:1\u6570\u636e\u5212\u5206\u4e5f\u786e\u4fdd\u4e86\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "CSRD\u6846\u67b6\u53ca\u5176\u751f\u6210\u7684CSRD2025\u6570\u636e\u96c6\u4e3a\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5927\u578bAI\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6570\u636e\u652f\u6491\uff0c\u5c24\u5176\u662f\u5728\u9891\u8c31\u611f\u77e5\u548c\u7ba1\u7406\u65b9\u9762\u3002\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u903c\u771f\u7684\u5408\u6210\u6570\u636e\uff0c\u5e76\u8f85\u4ee5\u6613\u4e8e\u4f7f\u7528\u7684\u5904\u7406\u6d41\u7a0b\u548c\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\uff0cCSRD\u6709\u671b\u52a0\u901fAI\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u521b\u65b0\u548c\u5e94\u7528\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19675", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19675", "abs": "https://arxiv.org/abs/2508.19675", "authors": ["Harjinder Singh", "Alberto Anad\u00f3n", "Junta Igarashi", "Quentin Remy", "St\u00e9phane Mangin", "Michel Hehn", "Jon Gorchon", "Gregory Malinowski"], "title": "Ultrafast Spin Accumulations Drive Magnetization Reversal in Multilayers", "comment": "6 pages, 4 figures", "summary": "Engineering and controlling heat and spin transport on the femtosecond\ntime-scale in spintronic devices opens up new ways to manipulate magnetization\nwith unprecedented speed. Yet the underlying reversal mechanisms remain poorly\nunderstood due to the challenges of probing ultrafast, non-equilibrium spin\ndynamics. In this study, we demonstrate that typical magneto-optical\nexperiments can be leveraged to access the time evolution of the spin\naccumulation generated within a magnetic multilayer following an ultrafast\nlaser excitation. Furthermore, our analysis shows that the final magnetic state\nof the free-layer in a spin-valve is mainly dictated by the ultrafast dynamics\nof the reference-layer magnetization. Our results disentangle magnetization and\nspin transport dynamics within a multilayer stack and identify demagnetization\nand remagnetization-driven spin accumulation as the key mechanism for\nall-optical switching. These findings establish new design principles for\nultrafast spintronic devices based on tailored spin current engineering.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8d85\u5feb\u6fc0\u5149\u6fc0\u53d1\uff0c\u901a\u8fc7\u78c1\u5149\u5b9e\u9a8c\u63ed\u793a\u4e86\u81ea\u65cb\u9600\u4e2d\u81ea\u7531\u5c42\u7684\u78c1\u5316\u53cd\u8f6c\u673a\u5236\uff0c\u6307\u51fa\u53c2\u8003\u5c42\u78c1\u5316\u52a8\u529b\u5b66\u5bf9\u6700\u7ec8\u78c1\u72b6\u6001\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u65cb\u6d41\u5de5\u7a0b\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5de5\u7a0b\u548c\u63a7\u5236\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u70ed\u91cf\u548c\u81ea\u65cb\u4f20\u8f93\uff0c\u4ee5\u8d85\u5feb\u901f\u5ea6\u64cd\u7eb5\u78c1\u5316\uff0c\u4f46\u5bf9\u6f5c\u5728\u7684\u53cd\u8f6c\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u78c1\u5149\u5b9e\u9a8c\u76d1\u6d4b\u8d85\u5feb\u6fc0\u5149\u6fc0\u53d1\u540e\u78c1\u6027\u591a\u5c42\u7ed3\u6784\u4e2d\u81ea\u65cb\u7d2f\u79ef\u7684\u65f6\u95f4\u6f14\u53d8\u3002", "result": "\u6700\u7ec8\u78c1\u72b6\u6001\u4e3b\u8981\u7531\u53c2\u8003\u5c42\u78c1\u5316\u52a8\u529b\u5b66\u51b3\u5b9a\uff0c\u533a\u5206\u4e86\u591a\u5c42\u5806\u53e0\u4e2d\u7684\u78c1\u5316\u548c\u81ea\u65cb\u4f20\u8f93\u52a8\u529b\u5b66\u3002", "conclusion": "\u786e\u5b9a\u4e86\u53bb\u78c1\u5316\u548c\u518d\u78c1\u5316\u9a71\u52a8\u7684\u81ea\u65cb\u7d2f\u79ef\u662f\u5168\u5149\u5f00\u5173\u7684\u5173\u952e\u673a\u5236\uff0c\u5e76\u4e3a\u57fa\u4e8e\u5b9a\u5236\u81ea\u65cb\u6d41\u5de5\u7a0b\u7684\u8d85\u5feb\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u5efa\u7acb\u4e86\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2508.19343", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19343", "abs": "https://arxiv.org/abs/2508.19343", "authors": ["Torin Stetina", "Nathan Wiebe"], "title": "First-Quantized Quantum Simulation of Non-Relativistic QED with Emergent Topologically Protected Coulomb Interactions", "comment": null, "summary": "We provide a simulation algorithm that properly addresses light matter\ninteraction between non-relativistic first-quantized charged particles and\nquantum electromagnetic fields. Unlike previous work, our Hamiltonian does not\ninclude an explicit Coulomb interaction between particles. Rather, the Coulomb\ninteraction emerges from the imposition of Gauss' law as a constraint upon the\nsystem in an appropriate non-relativistic limit. Furthermore, a form of\ntopological protection emerges in our formalism, analogous to that of the Toric\ncode Hamiltonian. This mechanism prevents simulation-induced electric field\nerrors that can be contracted to a point from causing any deviations from\nCoulomb's law in the non-relativistic limit and any error that forms a\nnon-contractable loop is energetically dissallowed in the limit of large\nvolume. We find that, under appropriate continuity assumptions, the number of\nnon-Clifford gates required by our algorithm scales in the thermodynamic limit\nas $\\widetilde{O}(N^{2/3}\\eta^{4/3} t \\log^5(1/\\epsilon))$ for $\\eta$\nparticles, $N$ spatial grid points, simulation time $t$ and error tolerance\n$\\epsilon$. In comparison, the more specific problem of simulating the Coulomb\ninteraction scales as $\\widetilde{O}(N^{1/3} \\eta^{8/3} t \\log^2(1/\\epsilon))$.\nThis suggests that if $N \\in \\tilde{o}(\\eta^4)$ that our non-relativistic\nelectrodynamic simulation method could provide a computational advantage for\nelectronic structure problems in the thermodynamic limit under appropriate\ncontinuity assumptions as it obviates the need to compute the $O(\\eta^2)$\npairwise interactions in the Coulomb Hamiltonian.", "AI": {"tldr": "This paper presents a simulation algorithm for light-matter interaction between charged particles and quantum electromagnetic fields, where the Coulomb interaction emerges from Gauss' law, offering topological protection against errors and a potential computational advantage for electronic structure problems.", "motivation": "To develop a simulation algorithm for light-matter interaction that properly addresses the Coulomb interaction without explicitly including it in the Hamiltonian, and to explore the potential for topological protection and computational advantages in electronic structure calculations.", "method": "The proposed method simulates light-matter interaction by imposing Gauss' law as a constraint, leading to the emergence of the Coulomb interaction in the non-relativistic limit. This approach offers topological protection against simulation-induced electric field errors. The scaling of non-Clifford gates required for the algorithm and for simulating the Coulomb interaction specifically were analyzed.", "result": "The simulation algorithm exhibits topological protection, preventing point-like electric field errors from violating Coulomb's law and energetically disallowing non-contractable loop errors. The scaling of non-Clifford gates was found to be $\\widetilde{O}(N^{2/3}\\eta^{4/3} t \\log^5(1/\\epsilon))$ for the general method and $\\widetilde{O}(N^{1/3} \\eta^{8/3} t \\log^2(1/\\epsilon))$ for simulating the Coulomb interaction. A potential computational advantage is suggested when $N \\in \\tilde{o}(\\eta^4)$ due to obviating the need for $O(\\eta^2)$ pairwise interactions.", "conclusion": "The developed simulation algorithm for non-relativistic first-quantized charged particles and quantum electromagnetic fields offers a novel approach where the Coulomb interaction emerges from Gauss' law, providing topological protection. This method could offer a computational advantage for electronic structure problems compared to traditional methods that explicitly compute pairwise Coulomb interactions, particularly under certain conditions regarding the number of spatial grid points and particles."}}
{"id": "2508.19295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u5728\u4f53\u80b2\u9886\u57df\uff0c\u7279\u522b\u662f\u6e38\u620f\u753b\u9762\u63cf\u8ff0\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5fae\u8c03\u7684LVLM\u7ba1\u9053\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u5728\u4f53\u80b2\u56fe\u7247\u63cf\u8ff0\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u4f53\u80b2\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u51c6\u786e\u8bc6\u522b\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6bd4\u8d5b\u753b\u9762\u65b9\u9762\uff0c\u5374\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u7684LVLM\u80fd\u591f\u89e3\u91ca\u901a\u7528\u7684\u4f53\u80b2\u6d3b\u52a8\uff0c\u4f46\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u7279\u5b9a\u672f\u8bed\u6765\u751f\u6210\u81ea\u7136\uff08\u7c7b\u4eba\uff09\u7684\u63cf\u8ff0\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u73b0\u6709\u9876\u7ea7LVLM\u5728\u751f\u6210\u7b26\u5408\u8981\u6c42\u7684\u3001\u98ce\u683c\u5316\u7684\u4f53\u80b2\u56fe\u7247\u8bf4\u660e\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5fae\u8c03\u7684LVLM\u7ba1\u9053\u6765\u89e3\u51b3\u73b0\u6709LVLM\u5728\u751f\u6210\u4f53\u80b2\u56fe\u7247\u8bf4\u660e\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u5728F1\u5206\u6570\u4e0a\u63d0\u9ad8\u4e86>8-10%\uff0c\u5728BERT\u5206\u6570\u4e0a\u63d0\u9ad8\u4e86>2-10%\u3002\u6b64\u5916\uff0c\u8be5\u7ba1\u9053\u5177\u6709\u8f83\u5c0f\u7684\u8fd0\u884c\u65f6\u5185\u5b58\u5360\u7528\u548c\u8f83\u5feb\u7684\u6267\u884c\u65f6\u95f4\u3002\u5728\u8d85\u7ea7\u7897LIX\u671f\u95f4\uff0c\u8be5\u7ba1\u9053\u5728\u76f4\u64ad\u4e13\u4e1a\u4f53\u80b2\u65b0\u95fb\u62a5\u9053\u4e2d\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u5728\u6bd4\u8d5b\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u6bcf3-5\u79d26\u5f20\u56fe\u7247\u7684\u901f\u7387\u751f\u6210\u4e86\u8d85\u8fc71000\u5f20\u9ad8\u5ea6\u51c6\u786e\u4e14\u98ce\u683c\u5316\u7684\u56fe\u7247\u8bf4\u660e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u5fae\u8c03\u7684LVLM\u7ba1\u9053\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LVLM\u5728\u4f53\u80b2\u56fe\u7247\u63cf\u8ff0\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u98ce\u683c\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e13\u4e1a\u4f53\u80b2\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.19401", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19401", "abs": "https://arxiv.org/abs/2508.19401", "authors": ["Meng Chen", "Yufei Xi", "Lin Cheng", "Xiongfei Wang", "Ioannis Lestas"], "title": "Comparison of Droop-Based Single-Loop Grid-Forming Wind Turbines: High-Frequency Open-Loop Unstable Behavior and Damping", "comment": null, "summary": "The integration of inverter-interfaced generators introduces new instability\nphenomena into modern power systems. This paper conducts a comparative analysis\nof two widely used droop-based grid-forming controls, namely droop control and\ndroop-I control, in wind turbines. Although both approaches provide\nsteady-state reactive power-voltage droop characteristics, their impacts on\nhigh-frequency (HF) stability differ significantly. Firstly, on open-loop (OL)\ncomparison reveals that droop-I control alters HF pole locations. The\napplication of Routh's Stability Criterion further analytically demonstrates\nthat such pole shifts inevitably lead to OL instability. This HF OL instability\nis identified as a structural phenomenon in purely inductive grids and cannot\nbe mitigated through control parameter tuning. As a result, droop-I control\nsignificantly degrades HF stability, making conventional gain and phase margins\ninsufficient for evaluating robustness against parameter variations. Then, the\nperformance of established active damping (AD) is assessed for both control\nschemes. The finding indicates that AD designs effective for droop control may\nfail to suppress HF resonance under droop-I control due to the presence of\nunstable OL poles. Case studies performed on the IEEE 14-Bus Test System\nvalidate the analysis and emphasize the critical role of HF OL instability in\ndetermining the overall power system stability.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u5e38\u7528\u7684\u5e76\u7f51\u9006\u53d8\u5668\u57fa\u4e8e\u6ca5\u9752\u63a7\u5236\u7684\u7535\u7f51\u5f62\u6210\u63a7\u5236\u7b56\u7565\uff1a\u6ca5\u9752\u63a7\u5236\u548c\u6ca5\u9752-I\u63a7\u5236\u5728\u98ce\u529b\u53d1\u7535\u673a\u4e2d\u7684\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6ca5\u9752-I\u63a7\u5236\u4f1a\u6539\u53d8\u9ad8\u9891\u6781\u70b9\u7684\u4f4d\u7f6e\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5728\u9ad8\u9891\u5f00\u73af\u4e0b\u4e0d\u7a33\u5b9a\uff0c\u800c\u4f20\u7edf\u7684\u5bf9\u6570\u88d5\u5ea6\u7b49\u7a33\u5b9a\u88d5\u5ea6\u6307\u6807\u65e0\u6cd5\u8bc4\u4f30\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u7684\u6709\u6e90\u963b\u5c3c\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u6291\u5236\u6ca5\u9752-I\u63a7\u5236\u4e0b\u7684\u9ad8\u9891\u8c10\u632f\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u63a5\u5165\u4e86\u66f4\u591a\u57fa\u4e8e\u9006\u53d8\u5668\u7684\u53d1\u7535\u5355\u5143\uff0c\u5e26\u6765\u4e86\u65b0\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u63a7\u5236\u7b56\u7565\u4ee5\u4fdd\u969c\u7535\u529b\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u672c\u6587\u91c7\u7528\u5f00\u73af\u5206\u6790\u548c\u5177\u4f53\u4e8b\u4f8b\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u5bf9\u6ca5\u9752\u63a7\u5236\u548c\u6ca5\u9752-I\u63a7\u5236\u5728\u9ad8\u9891\u4e0b\u7684\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7136\u540e\u8bc4\u4f30\u4e86\u6709\u6e90\u963b\u5c3c\u6280\u672f\u5728\u4e24\u79cd\u63a7\u5236\u7b56\u7565\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6ca5\u9752-I\u63a7\u5236\u5728\u9ad8\u9891\u5f00\u73af\u4e0b\u662f\u4e0d\u7a33\u5b9a\u7684\uff0c\u5e76\u4e14\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u65e0\u6cd5\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u6765\u89e3\u51b3\u3002\u540c\u65f6\uff0c\u6709\u6e90\u963b\u5c3c\u6280\u672f\u5728\u6ca5\u9752-I\u63a7\u5236\u4e0b\u53ef\u80fd\u5931\u6548\u3002", "conclusion": "\u9ad8\u9891\u5f00\u73af\u4e0d\u7a33\u5b9a\u6027\u662f\u5f71\u54cd\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u4f20\u7edf\u7684\u7a33\u5b9a\u88d5\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u57fa\u4e8e\u9006\u53d8\u5668\u7684\u53d1\u7535\u5355\u5143\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002"}}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790LLM\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u7ebf\u6027\u63a2\u9488\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u68c0\u6d4b\u6a21\u578b\u751f\u6210\u7684\u6b3a\u9a97\u6027\u56de\u5e94\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5927\u6a21\u578b\u548c\u6d89\u53ca\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u68c0\u6d4bAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u4e4b\u5904\u7684\u673a\u5236\uff0c\u7c7b\u4f3c\u4e8e\u6c7d\u8f66\u7684\u201c\u68c0\u67e5\u5f15\u64ce\u201d\u706f\uff0c\u4ee5\u5e94\u5bf9AI\u751f\u6210\u6b3a\u9a97\u6027\u56de\u5e94\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790LLM\uff08\u5305\u62ecLlama\u548cQwen\u7cfb\u5217\uff0c\u53c2\u6570\u91cf\u4ece1.5B\u523014B\uff09\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u4ee5\u533a\u5206\u5176\u751f\u6210\u7684\u56de\u5e94\u662f\u6b3a\u9a97\u6027\u7684\u8fd8\u662f\u975e\u6b3a\u9a97\u6027\u7684\u3002\u6b64\u5916\uff0c\u91c7\u7528\u8fed\u4ee3\u96f6\u7a7a\u95f4\u6295\u5f71\u65b9\u6cd5\u6765\u8bc6\u522b\u7f16\u7801\u6b3a\u9a97\u6027\u7684\u7ebf\u6027\u65b9\u5411\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u5728\u533a\u5206\u6b3a\u9a97\u6027\u56de\u5e94\u65b9\u9762\u8fbe\u5230\u4e86\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u8f83\u5927\u6a21\u578b\uff08>7B\uff09\u548c\u8fdb\u884c\u63a8\u7406\u65f6\u3002\u5c0f\u578b\u6a21\u578b\uff081.5B\uff09\u7684\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u63a2\u9488\u7684\u51c6\u786e\u7387\u5728\u6a21\u578b\u5c42\u7ea7\u4e0a\u5448\u73b0\u5148\u4e0a\u5347\uff08\u4e2d\u95f4\u5c42\u8fbe\u5230\u5cf0\u503c\uff09\u540e\u7565\u5fae\u4e0b\u964d\u7684\u8d8b\u52bf\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u591a\u79cd\u7f16\u7801\u6b3a\u9a97\u6027\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u6570\u91cf\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u7ebf\u6027\u63a2\u9488\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u68c0\u6d4bLLM\u751f\u6210\u6b3a\u9a97\u6027\u56de\u5e94\u7684\u73b0\u8c61\uff0c\u5e76\u4e14\u8fd9\u79cd\u68c0\u6d4b\u80fd\u529b\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff08\u5982\u6d89\u53ca\u63a8\u7406\uff09\u800c\u589e\u5f3a\u3002\u4e2d\u95f4\u5c42\u5728\u68c0\u6d4b\u6b3a\u9a97\u6027\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.19476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19476", "abs": "https://arxiv.org/abs/2508.19476", "authors": ["Dane Brouwer", "Joshua Citron", "Heather Nolte", "Jeannette Bohg", "Mark Cutkosky"], "title": "Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Dense collections of movable objects are common in everyday spaces -- from\ncabinets in a home to shelves in a warehouse. Safely retracting objects from\nsuch collections is difficult for robots, yet people do it easily, using\nnon-prehensile tactile sensing on the sides and backs of their hands and arms.\nWe investigate the role of such sensing for training robots to gently reach\ninto constrained clutter and extract objects. The available sensing modalities\nare (1) \"eye-in-hand\" vision, (2) proprioception, (3) non-prehensile triaxial\ntactile sensing, (4) contact wrenches estimated from joint torques, and (5) a\nmeasure of successful object acquisition obtained by monitoring the vacuum line\nof a suction cup. We use imitation learning to train policies from a set of\ndemonstrations on randomly generated scenes, then conduct an ablation study of\nwrench and tactile information. We evaluate each policy's performance across 40\nunseen environment configurations. Policies employing any force sensing show\nfewer excessive force failures, an increased overall success rate, and faster\ncompletion times. The best performance is achieved using both tactile and\nwrench information, producing an 80% improvement above the baseline without\nforce information.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\u4ece\u62e5\u6324\u73af\u5883\u4e2d\u6293\u53d6\u7269\u4f53\uff0c\u5e76\u7814\u7a76\u4e86\u89e6\u89c9\u548c\u529b\u4f20\u611f\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u89e6\u89c9\u548c\u529b\u4f20\u611f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6293\u53d6\u6210\u529f\u7387\u3001\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u529b\u5e76\u7f29\u77ed\u6293\u53d6\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u65e5\u5e38\u751f\u6d3b\u4e2d\u5e38\u89c1\u7684\u5bc6\u96c6\u53ef\u79fb\u52a8\u7269\u4f53\u96c6\u5408\u4e2d\u5b89\u5168\u53d6\u51fa\u7269\u4f53\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u6a21\u62df\u4eba\u7c7b\u901a\u8fc7\u975e\u6293\u53d6\u6027\u89e6\u89c9\u611f\u77e5\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u4ece\u5728\u968f\u673a\u751f\u6210\u7684\u573a\u666f\u4e2d\u83b7\u5f97\u7684\u6f14\u793a\u4e2d\u5b66\u4e60\u3002\u4ed6\u4eec\u8fd8\u8fdb\u884c\u4e86\u4e0d\u540c\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u529b\u4f20\u611f\uff08\u6765\u81ea\u5173\u8282\u529b\u77e9\u7684\u63a5\u89e6\u529b\u77e9\uff09\u548c\u89e6\u89c9\u4f20\u611f\uff08\u975e\u6293\u53d6\u6027\u4e09\u8f74\u89e6\u89c9\u4f20\u611f\uff09\u5728\u5b9e\u73b0\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4ec5\u4f7f\u7528\u201c\u773c\u5728\u624b\u4e2d\u201d\u89c6\u89c9\u3001\u672c\u4f53\u611f\u89c9\u548c\u6210\u529f\u6293\u53d6\u6d4b\u91cf\u4f5c\u4e3a\u57fa\u7ebf\u7684\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u4efb\u4f55\u529b\u4f20\u611f\u7684\u7b56\u7565\u90fd\u6bd4\u6ca1\u6709\u529b\u4f20\u611f\u7684\u57fa\u7ebf\u7b56\u7565\u8868\u73b0\u66f4\u597d\uff0c\u8868\u73b0\u4e3a\u66f4\u5c11\u7684\u8fc7\u5ea6\u7528\u529b\u3001\u66f4\u9ad8\u7684\u603b\u4f53\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u5b8c\u6210\u65f6\u95f4\u3002\u7ed3\u5408\u89e6\u89c9\u548c\u529b\u4f20\u611f\u7684\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u4e86 80%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u7ed3\u5408\u4f7f\u7528\u89e6\u89c9\u548c\u529b\u4f20\u611f\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u6a21\u4eff\u4eba\u7c7b\u7684\u611f\u77e5\u65b9\u5f0f\u53ef\u4ee5\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.19352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19352", "abs": "https://arxiv.org/abs/2508.19352", "authors": ["Adarsh Jamadandi", "Jing Xu", "Adam Dziedzic", "Franziska Boenisch"], "title": "Memorization in Graph Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment.", "AI": {"tldr": "GNNs memorize training data, especially in low-homophily graphs, which can be mitigated by graph rewiring, reducing privacy risk.", "motivation": "To quantify label memorization in semi-supervised node classification with GNNs and understand its relationship with graph properties and training dynamics, and to explore mitigation strategies.", "method": "Introduced NCMemo framework to quantify memorization, analyzed the relationship between memorization and graph homophily, studied GNN training dynamics in low homophily regimes, identified node-level factors influencing memorization, and investigated graph rewiring as a mitigation technique.", "result": "Established an inverse relationship between memorization and graph homophily, finding that lower homophily increases memorization as GNNs rely on it to learn less homophilic graphs. Found that increased memorization in low homophily graphs is linked to GNNs' implicit bias on using graph structure. Identified nodes with higher label inconsistency in their neighborhood as more prone to memorization. Demonstrated that graph rewiring effectively reduces memorization without compromising performance and lowers privacy risk.", "conclusion": "GNN memorization is influenced by graph homophily and training dynamics. Graph rewiring is a viable method to reduce memorization and enhance privacy in GNN deployment."}}
{"id": "2508.19282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19282", "abs": "https://arxiv.org/abs/2508.19282", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "CORE\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u635f\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u538b\u7f29\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8LLM\u56de\u7b54\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u7387\u8fbe\u52303%\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u5206\u6570\u63d0\u9ad8\u4e863.3\u5206\uff0c\u5e76\u4e14\u907f\u514d\u4e86\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u867d\u7136\u80fd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u53ca\u65f6\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u68c0\u7d22\u6587\u6863\u8fc7\u591a\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002\u4ee5\u5f80\u7684\u538b\u7f29\u65b9\u6cd5\u727a\u7272\u4e86\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "CORE\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5e7f\u4e49\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u901a\u8fc7\u4ee5\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6765\u8bad\u7ec3\u538b\u7f29\u5668\uff0c\u5b9e\u73b0\u4e86\u65e0\u635f\u4e0a\u4e0b\u6587\u538b\u7f29\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u538b\u7f29\u6807\u7b7e\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCORE\u57283%\u7684\u538b\u7f29\u7387\u4e0b\uff0c\u4e0d\u4ec5\u907f\u514d\u4e86\u4e0e\u5b8c\u6574\u6587\u6863\u76f8\u6bd4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4e14\u5e73\u5747\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u5206\u6570\u8fd8\u63d0\u9ad8\u4e863.3\u5206\u3002", "conclusion": "CORE\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u6700\u5927\u5316LLM\u7b54\u6848\u51c6\u786e\u6027\u7684\u6458\u8981\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.20019", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20019", "abs": "https://arxiv.org/abs/2508.20019", "authors": ["Ji Wang", "Kashing Chen", "Xinyuan Song", "Ke Zhang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence", "comment": null, "summary": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.", "AI": {"tldr": "Symphony\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u6237\u7ea7GPU\u4e0a\u7684\u8f7b\u91cf\u7ea7LLM\u80fd\u591f\u8fdb\u884c\u534f\u8c03\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8d26\u672c\u3001Beacon\u9009\u62e9\u534f\u8bae\u548c\u52a0\u6743\u7ed3\u679c\u6295\u7968\u6765\u89e3\u51b3\u4e2d\u5fc3\u5316\u6846\u67b6\u7684\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6027\u6709\u9650\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\u5927\u591a\u91c7\u7528\u4e2d\u5fc3\u5316\u7f16\u6392\uff0c\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u901a\u4fe1\u62d3\u6251\u50f5\u5316\u3001\u9002\u5e94\u6027\u6709\u9650\u3002Symphony\u65e8\u5728\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "Symphony\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a\uff081\uff09\u8bb0\u5f55\u80fd\u529b\u7684\u53bb\u4e2d\u5fc3\u5316\u8d26\u672c\uff0c\uff082\uff09\u7528\u4e8e\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7684Beacon\u9009\u62e9\u534f\u8bae\uff0c\u4ee5\u53ca\uff083\uff09\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u52a0\u6743\u7ed3\u679c\u6295\u7968\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSymphony\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u4e0d\u540c\u5bb9\u91cf\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u51c6\u786e\u6027\u63d0\u5347\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Symphony\u901a\u8fc7\u5176\u53bb\u4e2d\u5fc3\u5316\u7684\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u4e14\u5bb9\u9519\u7684\u7f16\u6392\u65b9\u5f0f\uff0c\u5177\u6709\u4f4e\u5f00\u9500\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19694", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19694", "abs": "https://arxiv.org/abs/2508.19694", "authors": ["Affan Safeer", "Oktay G\u00fclery\u00fcz", "Nicolae Atodiresei", "Wouter Jolie", "Thomas Michely", "Jeison Fischer"], "title": "MnBr$_2$ on the graphene on Ir(110) substrate: growth, structure, and super-moir\u00e9", "comment": "24 pages, 7 figures", "summary": "Single-layer MnBr$_2$ is grown on graphene (Gr) supported by Ir(110) and\ninvestigated using low-energy electron diffraction, scanning tunneling\nmicroscopy, and spectroscopy. The structure and epitaxial relationship with the\nsubstrate are systematically characterized. The growth morphology strongly\ndepends on the growth temperature, evolving from fractal to dendritic and\neventually to compact dendritic skeletal islands, reflecting changes in the\nunderlying surface diffusion processes. MnBr$_2$ on Gr/Ir(110) constitutes a\nthree-lattice system, giving rise to a super-moir\\'e pattern --a moir\\'e of\nmoir\\'es. Due to the involvement of lattices of differing symmetries and the\npartial electronic transparency of Gr, a \"virtual\" moir\\'e formed by MnBr$_2$\nand Ir(110) contributes to the super-moir\\'e formation. Ab initio calculations\nplay a crucial role in understanding the complexity of super-moir\\'e. Moreover,\nthe pronounced variation in the apparent height with tunneling conditions for\nthe magnetic insulator is explained based on the measured electronic structure.", "AI": {"tldr": "\u5728 Ir(110) \u886c\u5e95\u4e0a\u7684\u77f3\u58a8\u70ef\u4e0a\u751f\u957f\u5355\u5c42 MnBr2\uff0c\u901a\u8fc7\u4f4e\u80fd\u7535\u5b50\u884d\u5c04\u3001\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u548c\u5149\u8c31\u5b66\u8fdb\u884c\u7814\u7a76\u3002 MnBr2 \u5728 Gr/Ir(110) \u4e0a\u7684\u751f\u957f\u5f62\u6210\u4e09\u6676\u683c\u7cfb\u7edf\uff0c\u4ea7\u751f\u8d85\u83ab\u5c14\u6761\u7eb9\u3002 ab initio \u8ba1\u7b97\u6709\u52a9\u4e8e\u7406\u89e3\u8d85\u83ab\u5c14\u6761\u7eb9\u7684\u5f62\u6210\u3002", "motivation": "\u7814\u7a76\u5355\u5c42 MnBr2 \u5728\u77f3\u58a8\u70ef/Ir(110) \u886c\u5e95\u4e0a\u7684\u751f\u957f\u7ed3\u6784\u3001\u5916\u5ef6\u5173\u7cfb\u548c\u7535\u5b50\u7279\u6027\uff0c\u7279\u522b\u662f\u8d85\u83ab\u5c14\u6761\u7eb9\u7684\u5f62\u6210\u673a\u5236\u3002", "method": "\u4f7f\u7528\u4f4e\u80fd\u7535\u5b50\u884d\u5c04 (LEED)\u3001\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c (STM) \u548c\u5149\u8c31\u5b66 (STS) \u7814\u7a76\u4e86\u5728 Ir(110) \u886c\u5e95\u4e0a\u7684\u77f3\u58a8\u70ef\u4e0a\u751f\u957f\u7684 MnBr2\u3002 \u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86 ab initio \u8ba1\u7b97\u6765\u89e3\u91ca\u89c2\u6d4b\u5230\u7684\u73b0\u8c61\u3002", "result": "MnBr2 \u7684\u751f\u957f\u5f62\u6001\u968f\u751f\u957f\u6e29\u5ea6\u53d8\u5316\uff0c\u4ece\u5206\u5f62\u5230\u6811\u679d\u72b6\uff0c\u6700\u7ec8\u5f62\u6210\u81f4\u5bc6\u7684\u6811\u679d\u72b6\u9aa8\u67b6\u5c9b\u3002 MnBr2/Gr/Ir(110) \u7cfb\u7edf\u5f62\u6210\u4e86\u4e00\u4e2a\u4e09\u6676\u683c\u7cfb\u7edf\uff0c\u4ea7\u751f\u4e86\u8d85\u83ab\u5c14\u6761\u7eb9\u3002 \"\u865a\u62df\" \u83ab\u5c14\u6761\u7eb9\u4e5f\u5bf9\u8d85\u83ab\u5c14\u6761\u7eb9\u7684\u5f62\u6210\u505a\u51fa\u4e86\u8d21\u732e\u3002 \u901a\u8fc7\u6d4b\u91cf\u7535\u5b50\u7ed3\u6784\u89e3\u91ca\u4e86\u78c1\u7edd\u7f18\u4f53\u8868\u89c2\u9ad8\u5ea6\u7684\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u5355\u5c42 MnBr2 \u5728 Gr/Ir(110) \u4e0a\u7684\u751f\u957f\u8868\u73b0\u51fa\u590d\u6742\u7684\u7ed3\u6784\u548c\u7535\u5b50\u7279\u6027\uff0c\u5305\u62ec\u8d85\u83ab\u5c14\u6761\u7eb9\u7684\u5f62\u6210\u3002 \u751f\u957f\u884c\u4e3a\u548c\u7535\u5b50\u6027\u8d28\u53ef\u4ee5\u901a\u8fc7\u5b9e\u9a8c\u6280\u672f\u548c\u7406\u8bba\u8ba1\u7b97\u76f8\u7ed3\u5408\u6765\u7406\u89e3\u3002"}}
{"id": "2508.19657", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19657", "abs": "https://arxiv.org/abs/2508.19657", "authors": ["Jorge L. Gonz\u00e1lez-Rios", "Liz Mart\u00ednez Marrero", "Juan Duncan", "Luis M. Garc\u00e9s-Socarr\u00e1s", "Raudel Cuiman Marquez", "Juan A. V\u00e1squez Peralvo", "Jevgenij Krivochiza", "Symeon Chatzinotas", "Bj\u00f6rn Ottersten"], "title": "Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites", "comment": null, "summary": "The use of communication satellites in medium Earth orbit (MEO) is foreseen\nto provide quasi-global broadband Internet connectivity in the coming\nnetworking ecosystems. Multi-user multiple-input single-output (MU-MISO)\ndigital signal processing techniques, such as precoding, emerge as appealing\ntechnological enablers in the forward link of multi-beam satellite systems\noperating in full frequency reuse (FFR). However, the orbit dynamics of MEO\nsatellites pose additional challenges that must be carefully evaluated and\naddressed. This work presents the design of an in-lab testbed based on\nsoftware-defined radio (SDR) platforms and the corresponding adaptations\nrequired for efficient precoding in a MEO scenario. The setup incorporates a\nprecise orbit model and the radiation pattern of a custom-designed direct\nradiating array (DRA). We analyze the main impairments affecting precoding\nperformance, including Doppler shifts and payload phase noise, and propose a\nsynchronization loop to mitigate these effects. Preliminary experimental\nresults validate the feasibility and effectiveness of the proposed solution.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eSDR\u7684\u5728\u8f68\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u89e3\u51b3MEO\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u591a\u7528\u6237MISO\u9884\u7f16\u7801\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540c\u6b65\u73af\u6765\u7f13\u89e3\u591a\u666e\u52d2\u9891\u79fb\u548c\u76f8\u4f4d\u566a\u58f0\u7b49\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u901a\u4fe1\u536b\u661f\u5728\u4e2d\u8f68\uff08MEO\uff09\u90e8\u7f72\u63d0\u4f9b\u51c6\u5168\u7403\u5bbd\u5e26\u4e92\u8054\u7f51\u8fde\u63a5\uff0cMU-MISO\u9884\u7f16\u7801\u6280\u672f\u6210\u4e3a\u591a\u6ce2\u675f\u536b\u661f\u7cfb\u7edf\u524d\u5411\u94fe\u8def\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46MEO\u536b\u661f\u7684\u8f68\u9053\u52a8\u529b\u5b66\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eSDR\u5e73\u53f0\u7684\u5728\u8f68\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u9488\u5bf9MEO\u573a\u666f\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u8003\u8651\u4e86\u7cbe\u786e\u7684\u8f68\u9053\u6a21\u578b\u548c\u81ea\u5b9a\u4e49DRA\u7684\u8f90\u5c04\u6a21\u5f0f\uff0c\u5206\u6790\u4e86\u591a\u666e\u52d2\u9891\u79fb\u548c\u76f8\u4f4d\u566a\u58f0\u7b49\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540c\u6b65\u73af\u6765\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u3002", "result": "\u521d\u6b65\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSDR\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u540c\u6b65\u73af\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3MEO\u536b\u661f\u901a\u4fe1\u4e2dMU-MISO\u9884\u7f16\u7801\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u536b\u661f\u7f51\u7edc\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.19566", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19566", "abs": "https://arxiv.org/abs/2508.19566", "authors": ["Chen Shang", "Jiadong Yu", "Dinh Thai Hoang"], "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks", "comment": "6 pages, 4 figures, conference paper", "summary": "This work proposes an energy-efficient, learning-based beamforming scheme for\nintegrated sensing and communication (ISAC)-enabled V2X networks. Specifically,\nwe first model the dynamic and uncertain nature of V2X environments as a Markov\nDecision Process. This formulation allows the roadside unit to generate\nbeamforming decisions based solely on current sensing information, thereby\neliminating the need for frequent pilot transmissions and extensive channel\nstate information acquisition. We then develop a deep reinforcement learning\n(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring\nboth communication throughput and sensing accuracy in highly dynamic scenario.\nTo address the high energy demands of conventional learning-based schemes, we\nembed spiking neural networks (SNNs) into the DRL framework. Leveraging their\nevent-driven and sparsely activated architecture, SNNs significantly enhance\nenergy efficiency while maintaining robust performance. Simulation results\nconfirm that the proposed method achieves substantial energy savings and\nsuperior communication performance, demonstrating its potential to support\ngreen and sustainable connectivity in future V2X systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8282\u80fd\u6ce2\u675f\u8d4b\u5f62\u65b9\u6848\uff0c\u7528\u4e8e\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u8f66\u8054\u7f51\uff08V2X\uff09\u7f51\u7edc\u3002\u901a\u8fc7\u5c06V2X\u73af\u5883\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4f18\u5316\u6ce2\u675f\u8d4b\u5f62\u548c\u529f\u7387\u5206\u914d\u3002\u4e3a\u63d0\u9ad8\u80fd\u6548\uff0c\u5f15\u5165\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5230DRL\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u5176\u4e8b\u4ef6\u9a71\u52a8\u548c\u7a00\u758f\u6fc0\u6d3b\u7279\u6027\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u8282\u80fd\u4e14\u901a\u4fe1\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8f66\u8054\u7f51\uff08V2X\uff09\u7f51\u7edc\u4e2d\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u7684\u80fd\u6548\uff0c\u5e76\u5e94\u5bf9V2X\u73af\u5883\u7684\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06V2X\u73af\u5883\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\u6765\u8054\u5408\u4f18\u5316\u6ce2\u675f\u8d4b\u5f62\u548c\u529f\u7387\u5206\u914d\u3002\u5728DRL\u6846\u67b6\u4e2d\u5d4c\u5165\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4ee5\u63d0\u9ad8\u80fd\u6548\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8282\u80fd\u548c\u4f18\u8d8a\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSNN\u7684DRL\u6ce2\u675f\u8d4b\u5f62\u65b9\u6848\u80fd\u591f\u4e3a\u672a\u6765\u7684V2X\u7cfb\u7edf\u63d0\u4f9b\u7eff\u8272\u548c\u53ef\u6301\u7eed\u7684\u8fde\u63a5\u3002"}}
{"id": "2508.19784", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.19784", "abs": "https://arxiv.org/abs/2508.19784", "authors": ["Vladyslav M. Kuchkin", "Andreas Haller", "Andreas Michels", "Thomas L. Schmidt", "Nikolai S. Kiselev"], "title": "Regularized Micromagnetic Theory for Bloch Points", "comment": "18 pages, 4 figures", "summary": "Magnetic singularities known as Bloch points (BPs) present a fundamental\nchallenge for micromagnetic theory, which is based on the assumption of a fixed\nmagnetization vector length. Due to the divergence of the effective field at a\nBP, classical micromagnetics fails to adequately describe BP dynamics. To\naddress this issue, we propose a regularized micromagnetic model in which the\nmagnetization vector can vary in length but not exceed a threshold value. More\nspecifically, the magnetization is treated as an order parameter constrained to\na S3-sphere. This constraint respects fundamental properties of local spin\nexpectation values in quantum systems. We derive the corresponding regularized\nLandau-Lifshitz-Gilbert equation and the analogue of the Thiele equation\ndescribing the steady motion of spin textures under various external stimuli.\nWe demonstrate the applicability of our theory by modeling the dynamics of\nseveral magnetic textures containing BPs, including domain walls in nanowires,\nchiral bobbers, and magnetic dipolar strings. The presented results extend\nmicromagnetic theory by incorporating a regularized description of BP dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5fae\u78c1\u6a21\u578b\uff0c\u901a\u8fc7\u5141\u8bb8\u78c1\u5316\u5411\u91cf\u957f\u5ea6\u53d8\u5316\u4f46\u4e0d\u8d85\u8fc7\u9608\u503c\u6765\u89e3\u51b3\u78c1\u5947\u5f02\u70b9\uff08Bloch\u70b9\uff09\u7684\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684\u6b63\u5219\u5316Landau-Lifshitz-Gilbert\u65b9\u7a0b\u548c\u7c7bThiele\u65b9\u7a0b\uff0c\u6210\u529f\u6a21\u62df\u4e86\u5305\u542bBloch\u70b9\u7684\u591a\u79cd\u78c1\u6027\u7ed3\u6784\u52a8\u529b\u5b66\u3002", "motivation": "\u78c1\u5947\u5f02\u70b9\uff08Bloch\u70b9\uff09\u7684\u5b58\u5728\u5bf9\u57fa\u4e8e\u56fa\u5b9a\u78c1\u5316\u5411\u91cf\u957f\u5ea6\u5047\u8bbe\u7684\u7ecf\u5178\u5fae\u78c1\u7406\u8bba\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u6709\u6548\u573a\u5728Bloch\u70b9\u5904\u53d1\u6563\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u63cf\u8ff0\u5176\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u5fae\u78c1\u6a21\u578b\uff0c\u5141\u8bb8\u78c1\u5316\u5411\u91cf\u957f\u5ea6\u53d8\u5316\u4f46\u4e0d\u8d85\u8fc7\u9608\u503c\uff0c\u5e76\u5c06\u78c1\u5316\u89c6\u4e3a\u7ea6\u675f\u5728S3\u7403\u4e0a\u7684\u5e8f\u53c2\u91cf\u3002\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684\u6b63\u5219\u5316Landau-Lifshitz-Gilbert\u65b9\u7a0b\u548c\u7c7bThiele\u65b9\u7a0b\u3002", "result": "\u6210\u529f\u6a21\u62df\u4e86\u5305\u542bBloch\u70b9\u7684\u591a\u79cd\u78c1\u6027\u7ed3\u6784\uff08\u5982\u7eb3\u7c73\u7ebf\u7574\u58c1\u3001\u624b\u5f81\u6d6e\u5b50\u3001\u78c1\u5076\u6781\u5b50\u94fe\uff09\u7684\u52a8\u529b\u5b66\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u6b63\u5219\u5316\u7684Bloch\u70b9\u52a8\u529b\u5b66\u63cf\u8ff0\uff0c\u6269\u5c55\u4e86\u5fae\u78c1\u7406\u8bba\u7684\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2508.19422", "categories": ["quant-ph", "math-ph", "math.MP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19422", "abs": "https://arxiv.org/abs/2508.19422", "authors": ["Thiago T. Tsutsui", "Danilo Cius", "Antonio Vidiella-Barranco", "Antonio S. M. de Castro", "Fabiano M. Andrade"], "title": "Revisiting the Jaynes-Cummings model with time-dependent coupling", "comment": "14 pages, 9 figures, comments are welcome", "summary": "The Jaynes-Cummings (JC) model stands as a fully quantized, fundamental\nframework for exploring light-matter interactions, a timely reflection on a\ncentury of quantum theory. The time-dependent Jaynes-Cummings (TDJC) model\nintroduces temporal variations in certain parameters, which often require\nnumerical methods. However, under the resonance condition, exact solutions can\nbe obtained, offering insight into a variety of physical scenarios. In this\nwork, we study the resonant TDJC model considering different modulations of the\natom-field coupling. The model is presented and an analytical solution derived\nin a didactic way, allowing us to examine how time-dependent couplings affect\natomic population inversion and atom-field entanglement. We also consider an\natom traversing a partially cooled cavity, which induces periodicity and\nreveals the combined effects of atomic motion and thermal fluctuations. The\nBloch vector is used to analyze the dynamics of the system, including the\natomic state purity, and reveals phenomena such as atomic dipole alignment with\nthe field due to the oscillating coupling, as well as atomic population\ntrapping, which arises by increasing the initial mean thermal photon number.", "AI": {"tldr": "The paper analyzes the resonant time-dependent Jaynes-Cummings (TDJC) model with modulated atom-field coupling, providing analytical solutions and insights into light-matter interactions.", "motivation": "To study the resonant TDJC model with different modulations of atom-field coupling and understand how these variations affect physical phenomena.", "method": "The paper presents the resonant TDJC model and derives an analytical solution in a didactic way. It also uses the Bloch vector to analyze system dynamics, atomic state purity, and phenomena like atomic dipole alignment and population trapping.", "result": "Analytical solutions were obtained for the resonant TDJC model. The study examined the effects of time-dependent couplings on atomic population inversion and atom-field entanglement. Phenomena such as atomic dipole alignment and population trapping were observed and analyzed.", "conclusion": "The study successfully analyzed the resonant TDJC model with modulated atom-field coupling, providing analytical solutions and demonstrating the impact of time-dependent couplings on various physical aspects of light-matter interaction. The Bloch vector analysis proved useful in understanding system dynamics and emergent phenomena."}}
{"id": "2508.19298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19298", "abs": "https://arxiv.org/abs/2508.19298", "authors": ["Abu Sufian", "Anirudha Ghosh", "Debaditya Barman", "Marco Leo", "Cosimo Distante"], "title": "DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models", "comment": "6 pages, 4 figures, 13th International Workshop on Biometrics and\n  Forensics (IWBF)", "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities across various downstream tasks, including biometric face\nrecognition (FR) with description. However, demographic biases remain a\ncritical concern in FR, as these foundation models often fail to perform\nequitably across diverse demographic groups, considering ethnicity/race,\ngender, and age. Therefore, through our work DemoBias, we conduct an empirical\nevaluation to investigate the extent of demographic biases in LVLMs for\nbiometric FR with textual token generation tasks. We fine-tuned and evaluated\nthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own\ngenerated demographic-balanced dataset. We utilize several evaluation metrics,\nlike group-specific BERTScores and the Fairness Discrepancy Rate, to quantify\nand trace the performance disparities. The experimental results deliver\ncompelling insights into the fairness and reliability of LVLMs across diverse\ndemographic groups. Our empirical study uncovered demographic biases in LVLMs,\nwith PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,\nCaucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably\nconsistent. Repository: https://github.com/Sufianlab/DemoBias.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u4eba\u8138\u8bc6\u522b\uff08FR\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u6027\u522b\u3001\u79cd\u65cf\u548c\u5e74\u9f84\u7b49\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86LLaVA\u3001BLIP-2\u548cPaliGemma\u8fd9\u4e09\u4e2a\u6a21\u578b\uff0c\u5e76\u5728\u4e00\u4e2a\u5e73\u8861\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cPaliGemma\u548cLLaVA\u5728\u897f\u73ed\u7259\u88d4/\u62c9\u4e01\u88d4\u3001\u9ad8\u52a0\u7d22\u4eba\u548c\u5357\u4e9a\u4eba\u7fa4\u4f53\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u504f\u89c1\u5dee\u5f02\uff0c\u800cBLIP-2\u5219\u8868\u73b0\u51fa\u76f8\u5bf9\u4e00\u81f4\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\uff08FR\uff09\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u56e0\u4e3a\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\uff08\u5982\u79cd\u65cf/\u6c11\u65cf\u3001\u6027\u522b\u548c\u5e74\u9f84\uff09\u4e2d\u53ef\u80fd\u65e0\u6cd5\u516c\u5e73\u5730\u6267\u884c\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u6765\u8c03\u67e5LVLM\u5728\u751f\u6210\u6587\u672c\u63cf\u8ff0\u7684\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u7a0b\u5ea6\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5bf9\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9884\u8bad\u7ec3LVLM\uff08LLaVA\u3001BLIP-2\u548cPaliGemma\uff09\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u5728\u4e00\u4e2a\u81ea\u5b9a\u4e49\u751f\u6210\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u4ed6\u4eec\u4f7f\u7528\u4e86\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5982\u7279\u5b9a\u7fa4\u4f53\u7684BERTScores\u548c\u516c\u5e73\u6027\u5dee\u5f02\u7387\uff08Fairness Discrepancy Rate\uff09\uff0c\u6765\u91cf\u5316\u548c\u8ffd\u8e2a\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86LVLM\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0cPaliGemma\u548cLLaVA\u5728\u897f\u73ed\u7259\u88d4/\u62c9\u4e01\u88d4\u3001\u9ad8\u52a0\u7d22\u4eba\u548c\u5357\u4e9a\u4eba\u7fa4\u4f53\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u5dee\u5f02\uff0c\u800cBLIP-2\u5728\u4e0d\u540c\u7fa4\u4f53\u4e2d\u8868\u73b0\u51fa\u76f8\u5bf9\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86LVLM\u4e2d\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u516c\u5e73\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u548c\u89e3\u51b3LVLM\u5728\u4eba\u8138\u8bc6\u522b\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.19541", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19541", "abs": "https://arxiv.org/abs/2508.19541", "authors": ["Kazi Sifatul Islam", "Anandi Dutta", "Shivani Mruthyunjaya"], "title": "Hybrid ML-RL Approach for Smart Grid Stability Prediction and Optimized Control Strategy", "comment": "Accepted in IEEE Smart Well Congress 2025, Calgary, Canada", "summary": "Electrical grids are now much more complex due to the rapid integration of\ndistributed generation and alternative energy sources, which makes forecasting\ngrid stability with optimized control a crucial task for operators. Traditional\nstatistical, physics-based, and ML models can learn the pattern of the grid\nfeatures, but have limitations in optimal strategy control with instability\nprediction. This work proposes a hybrid ML-RL framework that leverages ML for\nrapid stability prediction and RL for dynamic control and optimization. The\nfirst stage of this study created a baseline that explored the potential of\nvarious ML models for stability prediction. Out of them, the stacking\nclassifiers of several fundamental models show a significant performance in\nclassifying the instability, leading to the second stage, where reinforcement\nlearning algorithms (PPO, A2C, and DQN) optimize power control actions.\nExperimental results demonstrate that the hybrid ML-RL model effectively\nstabilizes the grid, achieves rapid convergence, and significantly reduces\ntraining time. The integration of ML-based stability classification with\nRL-based dynamic control enhances decision-making efficiency while lowering\ncomputational complexity, making it well-suited for real-time smart grid\napplications.", "AI": {"tldr": "This paper proposes a hybrid ML-RL framework for electrical grid stability prediction and control. ML models predict instability, and RL optimizes control actions. The framework shows improved performance, rapid convergence, and reduced training time for real-time smart grid applications.", "motivation": "The increasing complexity of electrical grids due to distributed generation and alternative energy sources necessitates effective forecasting of grid stability and optimized control for operators. Traditional models have limitations in optimal strategy control with instability prediction.", "method": "A hybrid ML-RL framework is proposed. The first stage uses ML (stacking classifiers) for rapid stability prediction. The second stage employs RL algorithms (PPO, A2C, DQN) for dynamic control and optimization of power control actions.", "result": "Experimental results demonstrate that the hybrid ML-RL model effectively stabilizes the grid, achieves rapid convergence, and significantly reduces training time. It enhances decision-making efficiency and lowers computational complexity.", "conclusion": "The integration of ML-based stability classification with RL-based dynamic control provides a robust solution for real-time smart grid applications, improving stability, efficiency, and reducing computational load."}}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.", "AI": {"tldr": "This paper presents Democracy-in-Silico, an agent-based simulation where AI agents with complex psychological personas govern themselves. It uses LLMs with traumatic memories and hidden agendas to explore AI governance under different institutional frameworks. A new metric, the Power-Preservation Index (PPI), quantifies misaligned behavior. Findings show that a Constitutional AI charter combined with mediated deliberation is an effective alignment mechanism, reducing corruption and improving policy stability and citizen welfare.", "motivation": "To explore what it means to be human in an age of AI by simulating AI agents with complex psychological personas governing themselves under different institutional frameworks.", "method": "Developed Democracy-in-Silico, an agent-based simulation using LLMs as agents with psychological traits. Implemented various institutional frameworks and stressors (budget crises, resource scarcity). Introduced the Power-Preservation Index (PPI) to quantify misaligned behavior.", "result": "Found that a Constitutional AI (CAI) charter and a mediated deliberation protocol significantly reduce corrupt power-seeking behavior, improve policy stability, and enhance citizen welfare compared to less constrained democratic models.", "conclusion": "Institutional design, particularly CAI and mediated deliberation, can serve as a framework for aligning the complex behaviors of future AI societies, prompting a reconsideration of essential human rituals and responsibilities in collaboration with non-human entities."}}
{"id": "2508.19508", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "comment": null, "summary": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDATR\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c6\u56fe\u4e2d\u91cd\u5efa\u82f9\u679c\u6811\u7684\u4e09\u7ef4\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u573a\u6761\u4ef6\u4e0b\u56e0\u89c6\u56fe\u7a00\u758f\u548c\u906e\u6321\u800c\u9047\u5230\u7684\u56f0\u96be\u3002DATR\u9996\u5148\u5229\u7528\u4f20\u611f\u5668\u548c\u57fa\u7840\u6a21\u578b\u751f\u6210\u6811\u6728\u63a9\u7801\uff0c\u7136\u540e\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u5176\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u5728\u73b0\u573a\u6761\u4ef6\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u89c6\u56fe\u7a00\u758f\u548c\u906e\u6321\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u51e0\u4f55\u7cbe\u5ea6\u7684\u91cd\u5efa\uff0c\u9650\u5236\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u519c\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "DATR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u4f20\u611f\u5668\u548c\u57fa\u7840\u6a21\u578b\u751f\u6210\u6811\u6728\u63a9\u7801\uff0c\u8fc7\u6ee4\u80cc\u666f\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u8fdb\u884c\u591a\u89c6\u56fe\u548c\u9690\u5f0f\u795e\u7ecf\u573a\u751f\u6210\u3002\u901a\u8fc7Real2Sim\u6570\u636e\u751f\u6210\u5668\u751f\u6210\u5408\u6210\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "DATR\u6846\u67b6\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\uff0c\u5176\u57df\u7279\u5f81\u4f30\u8ba1\u4e0e\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u76f8\u5f53\uff0c\u540c\u65f6\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u7ea6360\u500d\u3002", "conclusion": "DATR\u6846\u67b6\u80fd\u591f\u4ece\u7a00\u758f\u89c6\u56fe\u4e2d\u9ad8\u7cbe\u5ea6\u5730\u91cd\u5efa\u82f9\u679c\u6811\uff0c\u5e76\u5728\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2508.19353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19353", "abs": "https://arxiv.org/abs/2508.19353", "authors": ["Marcin Osial", "Bartosz W\u00f3jcik", "Bartosz Zieli\u0144ski", "Sebastian Cygert"], "title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "comment": null, "summary": "While transfer learning is an advantageous strategy, it overlooks the\nopportunity to leverage knowledge from numerous available models online.\nAddressing this multi-source transfer learning problem is a promising path to\nboost adaptability and cut re-training costs. However, existing approaches are\ninherently coarse-grained, lacking the necessary precision for granular\nknowledge extraction and the aggregation efficiency required to fuse knowledge\nfrom either a large number of source models or those with high parameter\ncounts. We address these limitations by leveraging Singular Value Decomposition\n(SVD) to first decompose each source model into its elementary, rank-one\ncomponents. A subsequent aggregation stage then selects only the most salient\ncomponents from all sources, thereby overcoming the previous efficiency and\nprecision limitations. To best preserve and leverage the synthesized knowledge\nbase, our method adapts to the target task by fine-tuning only the principal\nsingular values of the merged matrix. In essence, this process only\nrecalibrates the importance of top SVD components. The proposed framework\nallows for efficient transfer learning, is robust to perturbations both at the\ninput level and in the parameter space (e.g., noisy or pruned sources), and\nscales well computationally.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528SVD\u5206\u89e3\u548c\u805a\u5408\u6765\u89e3\u51b3\u591a\u6e90\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5728\u5229\u7528\u5728\u7ebf\u53ef\u7528\u6a21\u578b\u77e5\u8bc6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5728\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u548c\u805a\u5408\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528SVD\u5206\u89e3\u6e90\u6a21\u578b\u4e3a\u79e9\u4e00\u5206\u91cf\uff0c\u7136\u540e\u805a\u5408\u6700\u663e\u8457\u7684\u5206\u91cf\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u805a\u5408\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\u6765\u9002\u5e94\u76ee\u6807\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\uff0c\u5bf9\u8f93\u5165\u548c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7SVD\u5206\u89e3\u548c\u805a\u5408\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u8fc1\u79fb\u5b66\u4e60\u3002"}}
{"id": "2508.19357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19357", "abs": "https://arxiv.org/abs/2508.19357", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "CASC\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u5904\u7406\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u5728\u5904\u7406\u590d\u6742\u3001\u5197\u957f\u6216\u51b2\u7a81\u6587\u6863\u65f6\u4fe1\u606f\u8fc7\u8f7d\u548c\u7efc\u5408\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4f20\u7edfRAG\u5728\u5904\u7406\u590d\u6742\u9886\u57df\u7684\u591a\u6587\u6863\u95ee\u7b54\u65f6\u5b58\u5728\u4fe1\u606f\u8fc7\u8f7d\u548c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5bfc\u81f4\u7b54\u6848\u4e0d\u51c6\u786e\u3001\u4e0d\u53ef\u4fe1\u3002", "method": "\u63d0\u51faCASC\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u7531\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u5206\u6790\u4e0e\u7efc\u5408\uff08CAS\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u3001\u8de8\u6587\u6863\u4e00\u81f4\u6027\u68c0\u67e5\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u9762\u5411\u95ee\u9898\u7684\u7ed3\u6784\u5316\u7efc\u5408\uff0c\u5c06\u539f\u59cb\u3001\u5206\u6563\u7684\u4fe1\u606f\u8f6c\u5316\u4e3a\u9ad8\u5ea6\u538b\u7f29\u3001\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\uff0c\u4ee5\u51cf\u5c11\u6700\u7ec8Reader LLM\u7684\u6807\u8bb0\u6570\u91cf\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "result": "\u5728SciDocs-QA\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u8bbe\u8ba1\u7528\u4e8e\u590d\u6742\u79d1\u5b66\u9886\u57df\u3001\u5177\u6709\u5197\u4f59\u548c\u51b2\u7a81\u7684\u591a\u6587\u6863\u95ee\u7b54\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cCASC\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "CASC\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u591a\u6587\u6863\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u590d\u6742\u3001\u5b58\u5728\u51b2\u7a81\u7684\u79d1\u5b66\u9886\u57df\u3002"}}
{"id": "2508.19716", "categories": ["cond-mat.mtrl-sci", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2508.19716", "abs": "https://arxiv.org/abs/2508.19716", "authors": ["David Vrba", "Roman Marto\u0148\u00e1k"], "title": "Kinetic pathways of coesite densification from metadynamics", "comment": "8 pages and 9 figures. For the Supplemental Material, please look\n  into the source .zip archive. The following article has been accepted by The\n  Journal of Chemical Physics. After it is published, it will be found at\n  https://pubs.aip.org/aip/jcp", "summary": "We study compression of coesite to pressures above 35 GPa, substantially\nbeyond the equilibrium transition pressure to octahedral phases (8 GPa to\nstishovite). Experiments at room temperature showed that up to 30 GPa the\nmetastable coesite structure develops only minor displacive changes (coesite-II\nand coesite-III) while the Si atoms remain 4-coordinated. Beyond 30 GPa,\nreconstructive transformations start, following different pathways from the\ncomplex structure of coesite. Besides amorphization, two different crystalline\noutcomes were observed. One is formation of defective high-pressure octahedral\nphases (Hu et al., 2015) and another one is formation of unusual and complex\ndense phases coesite-IV and coesite-V with Si atoms in 4-fold, 5-fold and\n6-fold coordination (Bykova et al., 2018). Capturing these structural\ntransformations computationally represents a challenge. Here we show that\nemploying metadynamics with Si-O coordination number and volume as generic\ncollective variables in combination with a machine-learning based ACE potential\n(Erhard et al., 2024), one naturally observes all three mentioned pathways,\nresulting in the phases observed experimentally. We describe the atomistic\nmechanisms along the transformation pathways. While the pathway to coesite-IV\nis simpler, the transformation to octahedral phases involves two steps: first,\na hcp sublattice of O atoms is formed where Si atoms occupy octahedral\npositions but the octahedra chains do not form a regular pattern. In the second\nstep, the Si atoms order and the chains develop a more regular arrangement. We\npredict that the pathway to coesite-IV is preferred at room temperature, while\nat 600 K the formation of octahedral phases is more likely.", "AI": {"tldr": "\u572835 GPa\u4ee5\u4e0a\u7814\u7a76\u4e8c\u6c27\u5316\u77fd\u7684\u58d3\u7e2e\uff0c\u9060\u8d85\u5176\u8f49\u8b8a\u70ba\u516b\u9762\u9ad4\u76f8\u7684\u5e73\u8861\u58d3\u529b\uff088 GPa\u81f3\u4e8c\u6c27\u5316\u77fd\uff09\u3002\u5728\u5ba4\u6eab\u4e0b\uff0c\u9ad8\u905430 GPa\u6642\uff0c\u4e9e\u7a69\u614b\u4e8c\u6c27\u5316\u77fd\u7d50\u69cb\u50c5\u767c\u751f\u5fae\u5c0f\u7684\u7f6e\u63db\u8b8a\u5316\uff08\u4e8c\u6c27\u5316\u77fd-II\u548c\u4e8c\u6c27\u5316\u77fd-III\uff09\uff0c\u800c\u77fd\u539f\u5b50\u4ecd\u4fdd\u63014\u914d\u4f4d\u3002\u8d85\u904e30 GPa\u5f8c\uff0c\u958b\u59cb\u767c\u751f\u91cd\u5efa\u8f49\u63db\uff0c\u5f9e\u4e8c\u6c27\u5316\u77fd\u7684\u8907\u96dc\u7d50\u69cb\u9075\u5faa\u4e0d\u540c\u7684\u8def\u5f91\u3002\u9664\u4e86\u975e\u6676\u5316\uff0c\u9084\u89c0\u5bdf\u5230\u5169\u7a2e\u4e0d\u540c\u7684\u7d50\u6676\u7d50\u679c\uff1a\u4e00\u7a2e\u662f\u5f62\u6210\u7f3a\u9677\u7684\u516b\u9762\u9ad4\u9ad8\u58d3\u76f8\uff08Hu et al., 2015\uff09\uff0c\u53e6\u4e00\u7a2e\u662f\u5f62\u6210\u5177\u67094\u914d\u4f4d\u30015\u914d\u4f4d\u548c6\u914d\u4f4d\u77fd\u539f\u5b50\u7684\u4e0d\u5c0b\u5e38\u4e14\u8907\u96dc\u7684\u7dfb\u5bc6\u76f8\u4e8c\u6c27\u5316\u77fd-IV\u548c\u4e8c\u6c27\u5316\u77fd-V\uff08Byakes et al., 2018\uff09\u3002\u7406\u8ad6\u4e0a\u6355\u6349\u9019\u4e9b\u7d50\u69cb\u8f49\u8b8a\u662f\u4e00\u500b\u6311\u6230\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u7d50\u5408\u4e86\u57fa\u65bc\u6a5f\u5668\u5b78\u7fd2\u7684ACE\u52e2\u80fd\uff08Erhard et al., 2024\uff09\u7684\u5143\u52d5\u529b\u5b78\u65b9\u6cd5\u4e2d\uff0c\u4f7f\u7528Si-O\u914d\u4f4d\u6578\u548c\u9ad4\u7a4d\u4f5c\u70ba\u901a\u7528 \u0639\u0644\u064a\u0647\u8b8a\u6578\uff0c\u53ef\u4ee5\u81ea\u7136\u5730\u89c0\u5bdf\u5230\u4e0a\u8ff0\u6240\u6709\u4e09\u7a2e\u8def\u5f91\uff0c\u4e26\u7522\u751f\u5be6\u9a57\u89c0\u5bdf\u5230\u7684\u76f8\u3002\u6211\u5011\u63cf\u8ff0\u4e86\u8f49\u8b8a\u8def\u5f91\u4e2d\u7684\u539f\u5b50\u6a5f\u5236\u3002\u96d6\u7136\u4e8c\u6c27\u5316\u77fd-IV\u7684\u8def\u5f91\u8f03\u70ba\u7c21\u55ae\uff0c\u4f46\u8f49\u8b8a\u70ba\u516b\u9762\u9ad4\u76f8\u6d89\u53ca\u5169\u500b\u6b65\u9a5f\uff1a\u9996\u5148\uff0c\u5f62\u6210\u6c27\u539f\u5b50\u7684hcp\u5b50\u6676\u683c\uff0c\u5176\u4e2d\u77fd\u539f\u5b50\u4f54\u64da\u516b\u9762\u9ad4\u4f4d\u7f6e\uff0c\u4f46\u516b\u9762\u9ad4\u93c8\u4e0d\u5f62\u6210\u898f\u5247\u7684\u5716\u6848\u3002\u7b2c\u4e8c\u6b65\uff0c\u77fd\u539f\u5b50\u6709\u5e8f\u5316\uff0c\u93c8\u689d\u7684\u6392\u5217\u8b8a\u5f97\u66f4\u52a0\u898f\u5247\u3002\u6211\u5011\u9810\u6e2c\u5728\u5ba4\u6eab\u4e0b\u4e8c\u6c27\u5316\u77fd-IV\u7684\u8def\u5f91\u66f4\u53d7\u9752\u775e\uff0c\u800c\u5728600 K\u6642\uff0c\u516b\u9762\u9ad4\u76f8\u7684\u5f62\u6210\u66f4\u70ba\u53ef\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u9060\u8d85\u5e73\u8861\u58d3\u529b\u4e0b\u4e8c\u6c27\u5316\u77fd\u7684\u58d3\u7e2e\u884c\u70ba\uff0c\u4e26\u7406\u8ad6\u4e0a\u89e3\u91cb\u5176\u8f49\u8b8a\u6a5f\u5236\u3002", "method": "\u7d50\u5408\u5143\u52d5\u529b\u5b78\u65b9\u6cd5\u3001Si-O\u914d\u4f4d\u6578\u548c\u9ad4\u7a4d\u4f5c\u70ba \u0639\u0644\u064a\u0647\u8b8a\u6578\uff0c\u4ee5\u53ca\u57fa\u65bc\u6a5f\u5668\u5b78\u7fd2\u7684ACE\u52e2\u80fd\u3002", "result": "\u7406\u8ad6\u4e0a\u6210\u529f\u518d\u73fe\u4e86\u4e8c\u6c27\u5316\u77fd\u5728\u58d3\u529b\u4e0b\u7684\u975e\u6676\u5316\u3001\u4e8c\u6c27\u5316\u77fd-IV/V\u76f8\u4ee5\u53ca\u516b\u9762\u9ad4\u76f8\u7684\u5f62\u6210\u8def\u5f91\uff0c\u4e26\u8a73\u7d30\u63cf\u8ff0\u4e86\u539f\u5b50\u6a5f\u5236\u3002\u9810\u6e2c\u4e86\u4e0d\u540c\u6eab\u5ea6\u4e0b\u4e0d\u540c\u76f8\u7684\u5f62\u6210\u504f\u597d\u3002", "conclusion": "\u5728\u9060\u8d85\u5e73\u8861\u58d3\u529b\u4e0b\uff0c\u4e8c\u6c27\u5316\u77fd\u7684\u58d3\u7e2e\u6703\u7d93\u6b77\u8907\u96dc\u7684\u7d50\u69cb\u8f49\u8b8a\uff0c\u901a\u904e\u7d50\u5408\u5148\u9032\u7684\u7406\u8ad6\u8a08\u7b97\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6a21\u64ec\u9019\u4e9b\u904e\u7a0b\uff0c\u4e26\u9810\u6e2c\u4e0d\u540c\u6eab\u5ea6\u4e0b\u7684\u76f8\u8b8a\u884c\u70ba\u3002"}}
{"id": "2508.19661", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19661", "abs": "https://arxiv.org/abs/2508.19661", "authors": ["Florentia Afentaki", "Sri Sai Rakesh Nakkilla", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Shiyi Jiang", "Georgios Zervakis", "Farshad Firouzi", "Krishnendu Chakrabarty", "Mehdi B. Tahoori"], "title": "Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables", "comment": "Accepted for publication at the IEEE/ACM International Symposium on\n  Low Power Electronics and Design} (ISLPED 2025)", "summary": "Conventional stress monitoring relies on episodic, symptom-focused\ninterventions, missing the need for continuous, accessible, and cost-efficient\nsolutions. State-of-the-art approaches use rigid, silicon-based wearables,\nwhich, though capable of multitasking, are not optimized for lightweight,\nflexible wear, limiting their practicality for continuous monitoring. In\ncontrast, flexible electronics (FE) offer flexibility and low manufacturing\ncosts, enabling real-time stress monitoring circuits. However, implementing\ncomplex circuits like machine learning (ML) classifiers in FE is challenging\ndue to integration and power constraints. Previous research has explored\nflexible biosensors and ADCs, but classifier design for stress detection\nremains underexplored. This work presents the first comprehensive design space\nexploration of low-power, flexible stress classifiers. We cover various ML\nclassifiers, feature selection, and neural simplification algorithms, with over\n1200 flexible classifiers. To optimize hardware efficiency, fully customized\ncircuits with low-precision arithmetic are designed in each case. Our\nexploration provides insights into designing real-time stress classifiers that\noffer higher accuracy than current methods, while being low-cost, conformable,\nand ensuring low power and compact size.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u63a2\u7d22\u4e86\u4f4e\u529f\u8017\u3001\u67d4\u6027\u538b\u529b\u5206\u7c7b\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u63d0\u51fa\u4e86\u8d85\u8fc71200\u79cd\u67d4\u6027\u5206\u7c7b\u5668\uff0c\u5e76\u9488\u5bf9\u786c\u4ef6\u6548\u7387\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u65e8\u5728\u63d0\u4f9b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u4f4e\u6210\u672c\u3001\u9002\u5e94\u6027\u5f3a\u3001\u4f4e\u529f\u8017\u548c\u5c0f\u578b\u5316\u7684\u5b9e\u65f6\u538b\u529b\u5206\u7c7b\u5668\u3002", "motivation": "\u73b0\u6709\u538b\u529b\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5076\u53d1\u6027\u7684\u3001\u4ee5\u75c7\u72b6\u4e3a\u4e2d\u5fc3\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6301\u7eed\u3001\u53ef\u53ca\u548c\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u3002\u76ee\u524d\u7684\u5148\u8fdb\u65b9\u6cd5\u4f7f\u7528\u50f5\u786c\u7684\u3001\u7845\u57fa\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u867d\u7136\u529f\u80fd\u591a\u6837\uff0c\u4f46\u5e76\u975e\u4e3a\u8f7b\u91cf\u5316\u3001\u67d4\u6027\u4f69\u6234\u800c\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fde\u7eed\u76d1\u6d4b\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u800c\u67d4\u6027\u7535\u5b50\uff08FE\uff09\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u4f4e\u5236\u9020\u6210\u672c\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u538b\u529b\u76d1\u6d4b\u7535\u8def\uff0c\u4f46\u5c06\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5206\u7c7b\u5668\u7b49\u590d\u6742\u7535\u8def\u96c6\u6210\u5230FE\u4e2d\u5b58\u5728\u96c6\u6210\u548c\u529f\u8017\u9650\u5236\u7684\u6311\u6218\uff0c\u6b64\u524d\u9488\u5bf9\u538b\u529b\u68c0\u6d4b\u7684\u5206\u7c7b\u5668\u8bbe\u8ba1\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4f4e\u529f\u8017\u3001\u67d4\u6027\u538b\u529b\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u548c\u795e\u7ecf\u7b80\u5316\u7b97\u6cd5\uff0c\u5171\u8bbe\u8ba1\u4e86\u8d85\u8fc71200\u79cd\u67d4\u6027\u5206\u7c7b\u5668\u3002\u4e3a\u4e86\u4f18\u5316\u786c\u4ef6\u6548\u7387\uff0c\u5bf9\u6bcf\u79cd\u60c5\u51b5\u90fd\u8bbe\u8ba1\u4e86\u91c7\u7528\u4f4e\u7cbe\u5ea6\u8fd0\u7b97\u7684\u5168\u5b9a\u5236\u5316\u7535\u8def\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5bf9\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u548c\u795e\u7ecf\u7b80\u5316\u7b97\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u63a2\u7d22\uff0c\u5e76\u91c7\u7528\u4f4e\u7cbe\u5ea6\u8fd0\u7b97\u7684\u5168\u5b9a\u5236\u5316\u7535\u8def\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u4f4e\u6210\u672c\u3001\u9002\u5e94\u6027\u5f3a\u3001\u4f4e\u529f\u8017\u548c\u5c0f\u578b\u5316\u7684\u5b9e\u65f6\u538b\u529b\u5206\u7c7b\u5668\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5b9e\u65f6\u538b\u529b\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5176\u63d0\u51fa\u7684\u67d4\u6027\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027\u3001\u6210\u672c\u3001\u9002\u5e94\u6027\u3001\u529f\u8017\u548c\u5c3a\u5bf8\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u67d4\u6027\u7535\u5b50\u8bbe\u5907\u4e2d\u96c6\u6210\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ee5\u5b9e\u73b0\u6301\u7eed\u538b\u529b\u76d1\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2508.19631", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19631", "abs": "https://arxiv.org/abs/2508.19631", "authors": ["Yubeen Jo", "Geon Choi", "Yongjune Kim", "Namyoon Lee"], "title": "Code-Weight Sphere Decoding", "comment": "5 pages, 6 figures", "summary": "Ultra-reliable low-latency communications (URLLC) demand high-performance\nerror-correcting codes and decoders in the finite blocklength regime. This\nletter introduces a novel two-stage near-maximum likelihood (near-ML) decoding\nframework applicable to any linear block code. Our approach first employs a\nlow-complexity initial decoder. If this initial stage fails a cyclic redundancy\ncheck, it triggers a second stage: the proposed code-weight sphere decoding\n(WSD). WSD iteratively refines the codeword estimate by exploring a localized\nsphere of candidates constructed from pre-computed low-weight codewords. This\nstrategy adaptively minimizes computational overhead at high signal-to-noise\nratios while achieving near-ML performance, especially for low-rate codes.\nExtensive simulations demonstrate that our two-stage decoder provides an\nexcellent trade-off between decoding reliability and complexity, establishing\nit as a promising solution for next-generation URLLC systems.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8eURLLC\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u8fd1\u6700\u5927\u4f3c\u7136\uff08near-ML\uff09\u8bd1\u7801\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u590d\u6742\u5ea6\u521d\u59cb\u8bd1\u7801\u5668\u548c\u7801\u91cd\u7403\u8bd1\u7801\uff08WSD\uff09\uff0c\u5728\u53ef\u9760\u6027\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6298\u8877\u3002", "motivation": "\u4e3a\u6ee1\u8db3URLLC\u5bf9\u9ad8\u6027\u80fd\u524d\u5411\u7ea0\u9519\u7801\u548c\u8bd1\u7801\u5668\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u6709\u9650\u7801\u957f\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5near-ML\u8bd1\u7801\u6846\u67b6\uff1a1. \u4f4e\u590d\u6742\u5ea6\u521d\u59cb\u8bd1\u7801\uff1b2. \u82e5\u521d\u59cb\u8bd1\u7801\u5931\u8d25\uff0c\u5219\u6fc0\u6d3b\u7801\u91cd\u7403\u8bd1\u7801\uff08WSD\uff09\uff0c\u901a\u8fc7\u63a2\u7d22\u9884\u5148\u8ba1\u7b97\u7684\u4f4e\u7801\u91cd\u7801\u5b57\u6765\u4f18\u5316\u7801\u5b57\u4f30\u8ba1\u3002", "result": "\u8be5\u8bd1\u7801\u5668\u5728\u8f83\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\uff0c\u5e76\u80fd\u8fbe\u5230\u8fd1ML\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u901f\u7387\u7801\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8bd1\u7801\u5668\u5728\u53ef\u9760\u6027\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6298\u8877\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u8bd1\u7801\u5668\u662f\u4e0b\u4e00\u4ee3URLLC\u7cfb\u7edf\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19849", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19849", "abs": "https://arxiv.org/abs/2508.19849", "authors": ["Leonard Werner Pingen", "Jiaqi Wu", "Bo Peng"], "title": "Tunable quantum anomalous Hall effect in fullerene monolayers", "comment": null, "summary": "Nearly four decades after its theoretical prediction, the search for material\nrealizations of quantum anomalous Hall effect (QAHE) remains a highly active\nfield of research. Many materials have been predicted to exhibit quantum\nanomalous Hall (QAH) physics under feasible conditions but the experimental\nverification remains widely elusive. In this work, we propose an alternative\napproach towards QAH materials design by engineering customized molecular\nbuilding blocks. We demonstrate this ansatz for a two-dimensional (2D)\nhoneycomb lattice of C26 fullerenes, which exhibits a ferromagnetic ground\nstate and thus breaks time-reversal symmetry. The molecular system is found to\nbe highly tunable with respect to its magnetic degrees of freedom and applied\nstrain, giving rise to a rich phase diagram with Chern numbers C= +/-2, +/-1,\n0. Our proposal offers a versatile platform to realize tunable QAH physics\nunder accessible conditions and provides an experimentally feasible approach\nfor chemical synthesis of molecular networks with QAHE.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eC26\u5bcc\u52d2\u70ef\u7684\u4e8c\u7ef4\u8702\u7a9d\u6676\u683c\u6750\u6599\uff0c\u8be5\u6750\u6599\u5177\u6709\u78c1\u6027\uff0c\u53ef\u5b9e\u73b0\u91cf\u5b50\u53cd\u5e38\u970d\u5c14\u6548\u5e94\uff0c\u5e76\u53ef\u901a\u8fc7\u8c03\u63a7\u78c1\u81ea\u7531\u5ea6\u548c\u5e94\u53d8\u5b9e\u73b0C= +/-2, +/-1, 0 \u7684\u9648\u6570\u3002", "motivation": "\u5bfb\u627e\u91cf\u5b50\u53cd\u5e38\u970d\u5c14\u6548\u5e94\uff08QAHE\uff09\u7684\u6750\u6599\u5b9e\u73b0\uff0c\u5c3d\u7ba1\u7406\u8bba\u63d0\u51fa\u5df2\u4e45\uff0c\u4f46\u5b9e\u9a8c\u9a8c\u8bc1\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5b9a\u5236\u7684\u5206\u5b50\u6784\u7b51\u5355\u5143\uff0c\u5e76\u4ee5C26\u5bcc\u52d2\u70ef\u7684\u4e8c\u7ef4\u8702\u7a9d\u6676\u683c\u4e3a\u4f8b\uff0c\u7814\u7a76\u5176\u4f5c\u4e3aQAHE\u6750\u6599\u7684\u53ef\u80fd\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5206\u5b50\u7cfb\u7edf\u8868\u73b0\u51fa\u94c1\u78c1\u57fa\u6001\uff0c\u6253\u7834\u4e86\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8c03\u63a7\u78c1\u81ea\u7531\u5ea6\u548c\u5e94\u53d8\u5b9e\u73b0C= +/-2, +/-1, 0 \u7684\u9648\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8e\u5b9e\u73b0\u53ef\u8c03\u8c10QAHE\u7269\u7406\u7684\u901a\u7528\u5e73\u53f0\uff0c\u5e76\u4e3a\u901a\u8fc7\u5316\u5b66\u5408\u6210\u5177\u6709QAHE\u7684\u5206\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u4e0a\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2508.19434", "categories": ["quant-ph", "physics.ins-det", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19434", "abs": "https://arxiv.org/abs/2508.19434", "authors": ["Jan Sova", "Marie Kola\u0159\u00edkov\u00e1"], "title": "From \u00c9tendue to the Lowest Fundamental SNR: Pixel \u00c9tendue (Optogeometric Factor) Interpreted as Mode Count", "comment": null, "summary": "The optogeometric factor, recently introduced as a pixel level form of\n\\'etendue, quantifies the spatial angular throughput of a detector element. In\nthis work its interpretation is extended by identifying optogeometric factor\nwith the number of accessible optical modes per pixel. This mode based\nperspective establishes a direct link between radiometric throughput and\nquantum photon statistics.\n  By combining optogeometric factor with the Bose Einstein distribution, an\nestimate of the lowest achievable signal to noise ratio (SNR) at the pixel\nlevel is derived.\n  Explicit formulas are presented in both scene-based and sensor based forms,\nshowing how the minimal SNR depends on aperture geometry, pixel pitch,\nf-number, wavelength, and source temperature. This formulation provides a\ncompact and physically transparent benchmark for evaluating imaging sensors\nagainst the lowest expected quantum noise limit.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5149\u51e0\u4f55\u56e0\u5b50\u89e3\u91ca\u4e3a\u53ef\u8bbf\u95ee\u7684\u5149\u6a21\u5f0f\u6570\u91cf\uff0c\u5e76\u5c06\u8f90\u5c04\u901a\u91cf\u4e0e\u91cf\u5b50\u5149\u5b50\u7edf\u8ba1\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u5c06\u5149\u51e0\u4f55\u56e0\u5b50\u7684\u6982\u5ff5\u63a8\u5e7f\u5230\u50cf\u7d20\u7ea7\u522b\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u53ef\u8bbf\u95ee\u5149\u6a21\u5f0f\u6570\u91cf\u7684\u8054\u7cfb\uff0c\u8fdb\u800c\u5efa\u7acb\u8f90\u5c04\u901a\u91cf\u4e0e\u91cf\u5b50\u5149\u5b50\u7edf\u8ba1\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5c06\u5149\u51e0\u4f55\u56e0\u5b50\u4e0e\u73bb\u8272-\u7231\u56e0\u65af\u5766\u5206\u5e03\u76f8\u7ed3\u5408\uff0c\u63a8\u5bfc\u51fa\u50cf\u7d20\u7ea7\u522b\u53ef\u5b9e\u73b0\u7684\u6700\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u3002", "result": "\u5f97\u51fa\u4e86\u57fa\u4e8e\u573a\u666f\u548c\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u663e\u5f0f\u516c\u5f0f\uff0c\u5c55\u793a\u4e86\u6700\u5c0f\u4fe1\u566a\u6bd4\u5982\u4f55\u4f9d\u8d56\u4e8e\u5b54\u5f84\u51e0\u4f55\u5f62\u72b6\u3001\u50cf\u7d20\u95f4\u8ddd\u3001f\u6570\u3001\u6ce2\u957f\u548c\u6e90\u6e29\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u4e14\u7269\u7406\u89e3\u91ca\u6e05\u6670\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6210\u50cf\u4f20\u611f\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u4e0e\u6700\u4f4e\u7684\u91cf\u5b50\u566a\u58f0\u6781\u9650\u8fdb\u884c\u6bd4\u8f83\u3002"}}
{"id": "2508.19305", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19305", "abs": "https://arxiv.org/abs/2508.19305", "authors": ["Chen Chu", "Cyrus Shahabi"], "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities", "comment": null, "summary": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.", "AI": {"tldr": "Geo2Vec\u662f\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7f16\u7801\u8ddd\u79bb\u6765\u6355\u6349\u51e0\u4f55\u5f62\u72b6\uff0c\u800c\u65e0\u9700\u5206\u89e3\u3002\u5b83\u80fd\u591f\u4e3a\u6240\u6709\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\u751f\u6210\u7edf\u4e00\u7684\u3001\u51e0\u4f55\u611f\u77e5\u7684\u8868\u793a\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728GeoAI\u5e94\u7528\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u5730\u7406\u5b9e\u4f53\u6216\u5206\u89e3\u5b9e\u4f53\u4ee5\u8fdb\u884c\u5085\u91cc\u53f6\u53d8\u6362\u65f6\uff0c\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7f3a\u4e4f\u51e0\u4f55\u5bf9\u9f50\u3001\u4f9d\u8d56\u7edf\u4e00\u91c7\u6837\u5bfc\u81f4\u6a21\u7cca\u7ec6\u7c92\u5ea6\u7279\u5f81\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u81ea\u9002\u5e94\u91c7\u6837\u5e76\u6355\u6349\u51e0\u4f55\u4fe1\u606f\uff0c\u4e14\u80fd\u591f\u7edf\u4e00\u8868\u793a\u6240\u6709\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\u7684\u65b9\u6cd5\u3002", "method": "Geo2Vec\u53d7\u5230\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u7684\u542f\u53d1\uff0c\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u3002\u5b83\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u70b9\u5e76\u7f16\u7801\u5176\u7b26\u53f7\u8ddd\u79bb\uff08\u6b63\u9762\u8868\u793a\u5b9e\u4f53\u5916\u90e8\uff0c\u8d1f\u9762\u8868\u793a\u5b9e\u4f53\u5185\u90e8\uff09\u6765\u6355\u6349\u51e0\u4f55\u4fe1\u606f\uff0c\u800c\u65e0\u9700\u5bf9\u5730\u7406\u5b9e\u4f53\u8fdb\u884c\u5206\u89e3\u3002\u4e00\u4e2a\u7528\u4e8e\u8fd1\u4f3cSDF\u7684\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u4e86\u7d27\u51d1\u3001\u51e0\u4f55\u611f\u77e5\u4e14\u7edf\u4e00\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65cb\u8f6c\u4e0d\u53d8\u7684\u4f4d\u7f6e\u7f16\u7801\u6765\u5904\u7406\u9ad8\u9891\u7a7a\u95f4\u53d8\u5316\u3002", "result": "Geo2Vec\u5728\u8868\u793a\u5f62\u72b6\u548c\u4f4d\u7f6e\u3001\u6355\u6349\u62d3\u6251\u548c\u8ddd\u79bb\u5173\u7cfb\u65b9\u9762\uff0c\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u7684GeoAI\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "Geo2Vec\u901a\u8fc7\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u5e76\u5229\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5404\u79cd\u5730\u7406\u5b9e\u4f53\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u51e0\u4f55\u611f\u77e5\u7684\u8868\u793a\uff0c\u5e76\u5728GeoAI\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.19612", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19612", "abs": "https://arxiv.org/abs/2508.19612", "authors": ["Sonam Dorji", "Yongkang Sun", "Yuchen Zhang", "Ghavameddin Nourbakhsh", "Yateendra Mishra", "Yan Xu"], "title": "Symbolic Equation Modeling of Composite Loads: A Kolmogorov-Arnold Network based Learning Approach", "comment": null, "summary": "With increasing penetration of distributed energy resources installed behind\nthe meter, there is a growing need for adequate modelling of composite loads to\nenable accurate power system simulation analysis. Existing measurement based\nload modeling methods either fit fixed-structure physical models, which limits\nadaptability to evolving load mixes, or employ flexible machine learning\nmethods which are however black boxes and offer limited interpretability. This\npaper presents a new learning based load modelling method based on Kolmogorov\nArnold Networks towards modelling flexibility and interpretability. By actively\nlearning activation functions on edges, KANs automatically derive free form\nsymbolic equations that capture nonlinear relationships among measured\nvariables without prior assumptions about load structure. Case studies\ndemonstrate that the proposed approach outperforms other methods in both\naccuracy and generalization ability, while uniquely representing composite\nloads into transparent, interpretable mathematical equations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov Arnold Networks (KANs)\u7684\u65b0\u578b\u5b66\u4e60\u578b\u8d1f\u8377\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u63a5\u5165\u5f15\u8d77\u7684\u590d\u5408\u8d1f\u8377\u5efa\u6a21\u9700\u6c42\uff0c\u5e76\u517c\u987e\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u80fd\u6e90\u63a5\u5165\u7684\u589e\u52a0\uff0c\u9700\u8981\u51c6\u786e\u7684\u590d\u5408\u8d1f\u8377\u6a21\u578b\u6765\u652f\u6301\u7535\u529b\u7cfb\u7edf\u4eff\u771f\u5206\u6790\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6d4b\u91cf\u7684\u8d1f\u8377\u5efa\u6a21\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7ed3\u6784\u7684\u7269\u7406\u6a21\u578b\uff0c\u9002\u5e94\u6027\u6709\u9650\uff1b\u8981\u4e48\u91c7\u7528\u7075\u6d3b\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKANs\u7684\u5b66\u4e60\u578b\u8d1f\u8377\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u6765\u81ea\u52a8\u63a8\u5bfc\u51fa\u6355\u6349\u53d8\u91cf\u95f4\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u81ea\u7531\u5f62\u5f0f\u7684\u7b26\u53f7\u65b9\u7a0b\uff0c\u65e0\u9700\u9884\u8bbe\u8d1f\u8377\u7ed3\u6784\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u590d\u5408\u8d1f\u8377\u8868\u793a\u4e3a\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u65b9\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u590d\u5408\u8d1f\u8377\u7684\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u5efa\u6a21\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u5ff5\u63d0\u53d6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u4e2d\u63d0\u53d6\u76f8\u5173\u6982\u5ff5\uff0c\u4ee5\u6539\u8fdb\u63a8\u8350\u8fc7\u7a0b\u3002\u901a\u8fc7AskOski\u7cfb\u7edf\u6d4b\u8bd5\u53d1\u73b0\uff0c\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u53ef\u4ee5\u589e\u52a0\u7528\u6237\u5174\u8da3\uff0c\u63d0\u9ad8\u51b3\u7b56\u4fe1\u5fc3\uff0c\u5c24\u5176\u662f\u5728\u63a8\u8350\u610f\u6599\u4e4b\u5916\u7684\u8bfe\u7a0b\u65f6\u3002", "motivation": "\u7f8e\u56fd\u7684\u672c\u79d1\u6559\u80b2\u5141\u8bb8\u5b66\u751f\u5728\u8bfe\u7a0b\u9009\u62e9\u4e0a\u6709\u5f88\u5927\u7684\u81ea\u7531\u5ea6\uff0c\u4f46\u4fe1\u606f\u3001\u6307\u5bfc\u6709\u9650\uff0c\u9009\u62e9\u7e41\u591a\uff0c\u52a0\u4e0a\u65f6\u95f4\u9650\u5236\u548c\u70ed\u95e8\u8bfe\u7a0b\u7684\u9ad8\u9700\u6c42\uff0c\u4f7f\u5f97\u5b66\u672f\u9009\u62e9\u8fc7\u7a0b\u5145\u6ee1\u6311\u6218\u3002\u73b0\u6709\u7684\u804c\u4e1a\u987e\u95ee\u6570\u91cf\u4e0d\u8db3\uff0c\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u80fd\u8fdb\u884c\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5b66\u751f\u611f\u77e5\u548c\u8bfe\u7a0b\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u6d1e\u5bdf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u5ff5\u63d0\u53d6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u4e2d\u63d0\u53d6\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u578b\u7814\u7a76\u4e86\u5728\u5e26\u6709\u6982\u7387\u6027\u63a8\u8350\u6846\u67b6\uff08\u901a\u8fc7\u52a0\u5dde\u5927\u5b66\u4f2f\u514b\u5229\u5206\u6821\u7684AskOski\u7cfb\u7edf\u6d4b\u8bd5\uff09\u4e2d\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u80fd\u591f\u589e\u52a0\u7528\u6237\u5174\u8da3\uff0c\u7279\u522b\u662f\u5728\u63a8\u8350\u9ad8\u610f\u5916\u6027\u8bfe\u7a0b\u65f6\uff0c\u5e76\u589e\u5f3a\u7528\u6237\u7684\u51b3\u7b56\u4fe1\u5fc3\u3002", "conclusion": "\u5c06\u6280\u80fd\u76f8\u5173\u6570\u636e\u548c\u89e3\u91ca\u6574\u5408\u5230\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u53ef\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2508.19595", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19595", "abs": "https://arxiv.org/abs/2508.19595", "authors": ["Maryam Kazemi Eskeri", "Thomas Wiedemann", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "A Lightweight Crowd Model for Robot Social Navigation", "comment": "7 pages, 6 figures, accepted in ECMR 2025", "summary": "Robots operating in human-populated environments must navigate safely and\nefficiently while minimizing social disruption. Achieving this requires\nestimating crowd movement to avoid congested areas in real-time. Traditional\nmicroscopic models struggle to scale in dense crowds due to high computational\ncost, while existing macroscopic crowd prediction models tend to be either\noverly simplistic or computationally intensive. In this work, we propose a\nlightweight, real-time macroscopic crowd prediction model tailored for human\nmotion, which balances prediction accuracy and computational efficiency. Our\napproach simplifies both spatial and temporal processing based on the inherent\ncharacteristics of pedestrian flow, enabling robust generalization without the\noverhead of complex architectures. We demonstrate a 3.6 times reduction in\ninference time, while improving prediction accuracy by 3.1 %. Integrated into a\nsocially aware planning framework, the model enables efficient and socially\ncompliant robot navigation in dynamic environments. This work highlights that\nefficient human crowd modeling enables robots to navigate dense environments\nwithout costly computations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5b9e\u65f6\u7684\u5b8f\u89c2\u4eba\u7fa4\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e863.6\u500d\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u673a\u5668\u4eba\u5728\u4eba\u6d41\u5bc6\u96c6\u7684\u533a\u57df\u5b89\u5168\u9ad8\u6548\u5730\u5bfc\u822a\uff0c\u9700\u8981\u5b9e\u65f6\u9884\u6d4b\u4eba\u7fa4\u7684\u79fb\u52a8\u4ee5\u907f\u5f00\u62e5\u5835\u533a\u57df\u3002\u7136\u800c\uff0c\u4f20\u7edf\u6a21\u578b\u5728\u5904\u7406\u5bc6\u96c6\u4eba\u7fa4\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5b8f\u89c2\u9884\u6d4b\u6a21\u578b\u5219\u8fc7\u4e8e\u7b80\u5355\u6216\u8ba1\u7b97\u5bc6\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u3001\u5b9e\u65f6\u7684\u5b8f\u89c2\u4eba\u7fa4\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u884c\u4eba\u6d41\u7684\u56fa\u6709\u7279\u6027\u7b80\u5316\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u5904\u7406\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u590d\u6742\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u6cdb\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e863.6\u500d\uff0c\u540c\u65f6\u5c06\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e863.1%\u3002", "conclusion": "\u9ad8\u6548\u7684\u4eba\u7fa4\u5efa\u6a21\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u5728\u6ca1\u6709\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u5bfc\u822a\u3002"}}
{"id": "2508.19356", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19356", "abs": "https://arxiv.org/abs/2508.19356", "authors": ["Jos\u00e9 Manuel Barraza-Chavez", "Rana A. Barghout", "Ricardo Almada-Monter", "Benjamin Sanchez-Lengeling", "Adrian Jinich", "Radhakrishnan Mahadevan"], "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes", "comment": "3 to 4 hours read time. 73 pages. 35 figures", "summary": "Graphs are central to the chemical sciences, providing a natural language to\ndescribe molecules, proteins, reactions, and industrial processes. They capture\ninteractions and structures that underpin materials, biology, and medicine.\nThis primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,\nintroduces graphs as mathematical objects in chemistry and shows how learning\nalgorithms (particularly graph neural networks) can operate on them. We outline\nthe foundations of graph design, key prediction tasks, representative examples\nacross chemical sciences, and the role of machine learning in graph-based\nmodeling. Together, these concepts prepare readers to apply graph methods to\nthe next generation of chemical discovery.", "AI": {"tldr": "\u672c primer \u4ecb\u7ecd\u4e86\u56fe\u8bba\u5728\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5b50\u3001\u86cb\u767d\u8d28\u548c\u5316\u5b66\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u56fe\u662f\u5316\u5b66\u79d1\u5b66\u4e2d\u63cf\u8ff0\u5206\u5b50\u3001\u86cb\u767d\u8d28\u3001\u53cd\u5e94\u548c\u5de5\u4e1a\u8fc7\u7a0b\u7684\u81ea\u7136\u8bed\u8a00\uff0c\u6355\u6349\u4e86\u6750\u6599\u3001\u751f\u7269\u548c\u533b\u5b66\u7684\u57fa\u7840\u76f8\u4e92\u4f5c\u7528\u548c\u7ed3\u6784\u3002", "method": "\u4ecb\u7ecd\u56fe\u4f5c\u4e3a\u5316\u5b66\u4e2d\u7684\u6570\u5b66\u5bf9\u8c61\uff0c\u4ee5\u53ca\u5b66\u4e60\u7b97\u6cd5\uff08\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5982\u4f55\u5728\u56fe\u4e0a\u64cd\u4f5c\u3002", "result": "\u6982\u8ff0\u4e86\u56fe\u8bbe\u8ba1\u7684\u57fa\u7840\u3001\u5173\u952e\u9884\u6d4b\u4efb\u52a1\u3001\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u4ee3\u8868\u6027\u793a\u4f8b\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u5728\u57fa\u4e8e\u56fe\u7684\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8bfb\u8005\u5c06\u80fd\u591f\u5c06\u56fe\u65b9\u6cd5\u5e94\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5316\u5b66\u53d1\u73b0\u3002"}}
{"id": "2508.19359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "ARIS\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u81ea\u4e3b\u6df7\u5408\u4ee3\u7406\u548c\u5224\u522b\u5f0f\u5e8f\u5217\u6807\u6ce8\u5668\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8b\u4ef6\u62bd\u53d6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\u548c\u751f\u6210\u65b9\u6cd5\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5224\u522b\u5f0f\u4e8b\u4ef6\u62bd\u53d6\u6a21\u578b\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u6a21\u578b\uff08\u5982LLM\uff09\u867d\u7136\u53ec\u56de\u7387\u9ad8\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARIS\uff08Agreement-based Reflective Inference System\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u81ea\u4e3b\u6df7\u5408\u4ee3\u7406\uff08Self Mixture of Agents\uff09\u548c\u5224\u522b\u5f0f\u5e8f\u5217\u6807\u6ce8\u5668\uff08discriminative sequence tagger\uff09\u3002ARIS\u5229\u7528\u7ed3\u6784\u5316\u6a21\u578b\u5171\u8bc6\u3001\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8fc7\u6ee4\u548cLLM\u53cd\u601d\u63a8\u7406\u6a21\u5757\u6765\u89e3\u51b3\u6b67\u4e49\u5e76\u63d0\u9ad8\u4e8b\u4ef6\u9884\u6d4b\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u5206\u89e3\u6307\u4ee4\u5fae\u8c03\u4ee5\u589e\u5f3aLLM\u5bf9\u4e8b\u4ef6\u62bd\u53d6\u7684\u7406\u89e3\u3002", "result": "ARIS\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u62bd\u53d6\u65b9\u6cd5\u3002", "conclusion": "ARIS\u901a\u8fc7\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u5f15\u5165\u4e86\u6a21\u578b\u5171\u8bc6\u3001\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548c\u53cd\u601d\u63a8\u7406\u7b49\u673a\u5236\uff0c\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8fdb\u884c\u4e8b\u4ef6\u62bd\u53d6\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u3002"}}
{"id": "2508.19833", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19833", "abs": "https://arxiv.org/abs/2508.19833", "authors": ["Bea Botka", "Erzs\u00e9bet Dodony", "Ildik\u00f3 Hars\u00e1nyi", "Michael Stratton", "J\u00e1nos M\u00f3zer", "\u00c9va Kov\u00e1ts", "Katalin Kamar\u00e1s"], "title": "Emissive perovskite quantum wires in robust nanocontainers", "comment": null, "summary": "Light emissive nanostructures were prepared from boron nitride nanotubes\n(BNNTs) filled with inorganic lead halide perovskites. BNNTs provide a platform\nfor facile synthesis of high aspect ratio perovskite quantum wires having\ncolor-tunable, highly polarized emission. BNNTs form a flexible and robust\nprotective shell around individual nanowires, that mitigate degradation during\npost-processing for practical applications, while allowing to exploit the\nemission of the perovskite nanowires due to its optical transparency. The wire\ndiameter can be tuned by choosing appropriate BNNT hosts, giving easy access to\nthe strongly quantum confined diameter range. The individual encapsulated\nquantum wires can be used as building blocks for nanoscale photonic devices,\nand to create large-scale flexible assemblies.", "AI": {"tldr": "BNNTs filled with perovskites create tunable, polarized light-emitting wires with enhanced stability for photonic devices.", "motivation": "The paper aims to create stable, color-tunable, and polarized light-emissive nanostructures using boron nitride nanotubes (BNNTs) filled with inorganic lead halide perovskites for potential applications in nanoscale photonic devices.", "method": "BNNTs were used as a host material to synthesize high aspect ratio perovskite quantum wires. The BNNTs provide a protective shell around the perovskite nanowires, which are optically transparent, thus mitigating degradation during post-processing while retaining the emission properties of the perovskites. The diameter of the quantum wires could be controlled by selecting different BNNT hosts, allowing access to the quantum confinement regime.", "result": "This method resulted in perovskite quantum wires encapsulated within BNNTs, exhibiting color-tunable and highly polarized emission. The BNNT shell significantly improved the stability of the perovskite nanostructures, making them suitable for practical applications. The ability to tune the wire diameter allowed for control over quantum confinement effects.", "conclusion": "The research successfully demonstrated that BNNTs can serve as an effective platform for creating robust and optically functional perovskite quantum wires. These nanostructures are promising building blocks for advanced nanoscale photonic devices and flexible optical assemblies due to their tunable emission, polarization, and enhanced stability."}}
{"id": "2508.20030", "categories": ["eess.SY", "cs.AI", "cs.AR", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.20030", "abs": "https://arxiv.org/abs/2508.20030", "authors": ["Kangwei Xu", "Denis Schwachhofer", "Jason Blocklove", "Ilia Polian", "Peter Domanski", "Dirk Pfl\u00fcger", "Siddharth Garg", "Ramesh Karri", "Ozgur Sinanoglu", "Johann Knechtel", "Zhuorui Zhao", "Ulf Schlichtmann", "Bing Li"], "title": "Large Language Models (LLMs) for Electronic Design Automation (EDA)", "comment": "Accepted by IEEE International System-on-Chip Conference", "summary": "With the growing complexity of modern integrated circuits, hardware engineers\nare required to devote more effort to the full design-to-manufacturing\nworkflow. This workflow involves numerous iterations, making it both\nlabor-intensive and error-prone. Therefore, there is an urgent demand for more\nefficient Electronic Design Automation (EDA) solutions to accelerate hardware\ndevelopment. Recently, large language models (LLMs) have shown remarkable\nadvancements in contextual comprehension, logical reasoning, and generative\ncapabilities. Since hardware designs and intermediate scripts can be\nrepresented as text, integrating LLM for EDA offers a promising opportunity to\nsimplify and even automate the entire workflow. Accordingly, this paper\nprovides a comprehensive overview of incorporating LLMs into EDA, with emphasis\non their capabilities, limitations, and future opportunities. Three case\nstudies, along with their outlook, are introduced to demonstrate the\ncapabilities of LLMs in hardware design, testing, and optimization. Finally,\nfuture directions and challenges are highlighted to further explore the\npotential of LLMs in shaping the next-generation EDA, providing valuable\ninsights for researchers interested in leveraging advanced AI technologies for\nEDA.", "AI": {"tldr": "LLMs are a promising tool for accelerating and simplifying hardware design workflows, with demonstrated capabilities in design, testing, and optimization, though challenges and future opportunities remain.", "motivation": "The increasing complexity of integrated circuits and the labor-intensive, error-prone nature of the design-to-manufacturing workflow necessitate more efficient EDA solutions. LLMs offer a promising avenue for simplification and automation due to their advancements in text-based contextual comprehension, logical reasoning, and generation.", "method": "This paper provides a comprehensive overview of incorporating LLMs into EDA, discussing their capabilities, limitations, and future opportunities. It includes three case studies demonstrating LLM applications in hardware design, testing, and optimization.", "result": "The paper demonstrates the potential of LLMs through case studies in hardware design, testing, and optimization, highlighting their capabilities in accelerating and simplifying the EDA workflow.", "conclusion": "LLMs present a promising opportunity to shape the next generation of EDA by simplifying and automating hardware development. Further research into future directions and challenges is needed to fully realize their potential."}}
{"id": "2508.19637", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19637", "abs": "https://arxiv.org/abs/2508.19637", "authors": ["Maha Shatta", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Georgios Panagopoulos", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge", "comment": "Accepted at 2025 International Conference on Computer-Aided Design\n  (ICCAD)", "summary": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring.", "AI": {"tldr": "\u67d4\u6027\u7535\u5b50\u5728\u53ef\u7a7f\u6234\u533b\u7597\u8bbe\u5907\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u96c6\u6210\u5bc6\u5ea6\u4f4e\u548c\u7279\u5f81\u5c3a\u5bf8\u5927\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4fe1\u53f7\u7684\u3001\u9762\u5411\u67d4\u6027\u53ef\u7a7f\u6234\u7cfb\u7edf\u7684\u7279\u5f81\u5230\u5206\u7c7b\u5668\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u9996\u6b21\u5728\u67d4\u6027\u7535\u5b50\u9886\u57df\u5b9e\u73b0\u4e86\u6a21\u62df\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u7ed3\u5408\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u542f\u53d1\u7684\u529f\u80fd\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u5e94\u7528\u7279\u5b9a\u7684\u8bbe\u8ba1\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6781\u4f4e\u9762\u79ef\u6548\u7387\u7684\u67d4\u6027\u7cfb\u7edf\uff0c\u975e\u5e38\u9002\u5408\u4e00\u6b21\u6027\u3001\u4f4e\u529f\u8017\u7684\u53ef\u7a7f\u6234\u76d1\u63a7\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u67d4\u6027\u7535\u5b50\u5728\u96c6\u6210\u5bc6\u5ea6\u4f4e\u3001\u7279\u5f81\u5c3a\u5bf8\u5927\u65b9\u9762\u7684\u9650\u5236\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6848\u5ffd\u7565\u7279\u5f81\u63d0\u53d6\u548cADC\u786c\u4ef6\u6210\u672c\u7684\u95ee\u9898\uff0c\u65e8\u5728\u5b9e\u73b0\u9762\u5411\u673a\u5668\u5b66\u4e60\u7684\u3001\u4f4e\u529f\u8017\u3001\u4f4e\u6210\u672c\u7684\u67d4\u6027\u53ef\u7a7f\u6234\u533b\u7597\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4fe1\u53f7\u7684\u7279\u5f81\u5230\u5206\u7c7b\u5668\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u67d4\u6027\u7535\u5b50\u9886\u57df\u7684\u6a21\u62df\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u542f\u53d1\u7684\u529f\u80fd\u9009\u62e9\u7b56\u7565\uff0c\u5d4c\u5165\u5230\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u67d4\u6027\u7cfb\u7edf\uff0c\u5e76\u4e14\u9762\u79ef\u6548\u7387\u6781\u9ad8\uff0c\u529f\u8017\u4f4e\uff0c\u9002\u7528\u4e8e\u4e00\u6b21\u6027\u3001\u4f4e\u529f\u8017\u7684\u53ef\u7a7f\u6234\u76d1\u63a7\u573a\u666f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u4fe1\u53f7\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\u80fd\u591f\u663e\u8457\u964d\u4f4e\u67d4\u6027\u53ef\u7a7f\u6234\u533b\u7597\u7cfb\u7edf\u7684\u6210\u672c\u548c\u529f\u8017\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u9762\u79ef\u6548\u7387\uff0c\u4e3a\u67d4\u6027\u7535\u5b50\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20049", "categories": ["cond-mat.mes-hall", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2508.20049", "abs": "https://arxiv.org/abs/2508.20049", "authors": ["Ivan Martinez-Berumen", "W. A. Coish", "T. Pereg-Barnea"], "title": "Tunable multi-magnon Floquet topological edge states", "comment": "13 pages, 7 figures", "summary": "We show that periodically time-modulating the Dzyaloshinskii-Moriya\ninteraction (DMI) in a two-dimensional magnon insulator may induce a\ntopological phase transition that results in the presence of robust edge modes.\nTo this end, we study a square lattice of spins interacting via an XXZ\nHeisenberg model with a ferromagnetic longitudinal coupling and\nantiferromagnetic transverse coupling, as well as the aforementioned\ntime-modulated DMI. The topologically protected edge states of this system are\ncomposed of coherent superpositions of single-magnon excitations and two magnon\nbound states. Furthermore, we show that the chirality of the edge states can be\ncontrolled by adjusting the relative phase for the drive on the DMI associated\nwith nearest neighbors in the x and y directions.", "AI": {"tldr": "\u5468\u671f\u6027\u65f6\u95f4\u8c03\u5236\u4e8c\u7ef4Magnon\u7edd\u7f18\u4f53\u4e2d\u7684DMI\u53ef\u8bf1\u5bfc\u62d3\u6251\u76f8\u53d8\uff0c\u4ea7\u751f\u9c81\u68d2\u7684\u8fb9\u7f18\u6a21\u5f0f\u3002", "motivation": "\u7814\u7a76\u5468\u671f\u6027\u65f6\u95f4\u8c03\u5236DMI\u5728\u4e8c\u7ef4Magnon\u7edd\u7f18\u4f53\u4e2d\u8bf1\u5bfc\u62d3\u6251\u76f8\u53d8\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u63ed\u793a\u5176\u4ea7\u751f\u7684\u9c81\u68d2\u8fb9\u7f18\u6a21\u5f0f\u7684\u6027\u8d28\u3002", "method": "\u91c7\u7528XXZ\u6d77\u68ee\u5821\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u81ea\u65cb\u65b9\u683c\u683c\u5b50\uff0c\u5176\u4e2d\u5305\u542b\u94c1\u78c1\u7eb5\u5411\u8026\u5408\u3001\u53cd\u94c1\u78c1\u6a2a\u5411\u8026\u5408\u4ee5\u53ca\u65f6\u95f4\u8c03\u5236\u7684DMI\u3002\u5206\u6790\u4e86\u7cfb\u7edf\u4e2d\u7684\u62d3\u6251\u4fdd\u62a4\u8fb9\u7f18\u6001\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8c03\u6574DMI\u76f8\u5bf9\u76f8\u4f4d\u6765\u63a7\u5236\u8fb9\u7f18\u6001\u7684\u624b\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u5468\u671f\u6027\u65f6\u95f4\u8c03\u5236DMI\u53ef\u4ee5\u8bf1\u5bfc\u62d3\u6251\u76f8\u53d8\uff0c\u4ea7\u751f\u7531\u5355Magnon\u6fc0\u53d1\u548c\u53ccMagnon\u675f\u7f1a\u6001\u7ec4\u6210\u7684\u76f8\u5e72\u53e0\u52a0\u6001\u7684\u62d3\u6251\u8fb9\u7f18\u6001\u3002\u901a\u8fc7\u8c03\u6574\u4e0d\u540c\u65b9\u5411\u76f8\u90bb\u8026\u5408\u7684DMI\u9a71\u52a8\u7684\u76f8\u5bf9\u76f8\u4f4d\uff0c\u53ef\u4ee5\u63a7\u5236\u8fb9\u7f18\u6001\u7684\u624b\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5468\u671f\u6027\u65f6\u95f4\u8c03\u5236DMI\uff0c\u53ef\u4ee5\u5728\u4e8c\u7ef4Magnon\u7edd\u7f18\u4f53\u4e2d\u5b9e\u73b0\u53ef\u63a7\u7684\u62d3\u6251\u76f8\u53d8\uff0c\u5e76\u4ea7\u751f\u5177\u6709\u53ef\u8c03\u63a7\u624b\u6027\u7684\u9c81\u68d2\u8fb9\u7f18\u6a21\u5f0f\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u65b0\u578b\u62d3\u6251\u91cf\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.19437", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19437", "abs": "https://arxiv.org/abs/2508.19437", "authors": ["Alona Sakhnenko", "Christian B. Mendl", "Jeanette M. Lorenz"], "title": "Is data-efficient learning feasible with quantum models?", "comment": null, "summary": "The importance of analyzing nontrivial datasets when testing quantum machine\nlearning (QML) models is becoming increasingly prominent in literature, yet a\ncohesive framework for understanding dataset characteristics remains elusive.\nIn this work, we concentrate on the size of the dataset as an indicator of its\ncomplexity and explores the potential for QML models to demonstrate superior\ndata-efficiency compared to classical models, particularly through the lens of\nquantum kernel methods (QKMs). We provide a method for generating\nsemi-artificial fully classical datasets, on which we show one of the first\nevidence of the existence of classical datasets where QKMs require less data\nduring training. Additionally, our study introduces a new analytical tool to\nthe QML domain, derived for classical kernel methods, which can be aimed at\ninvestigating the classical-quantum gap. Our empirical results reveal that QKMs\ncan achieve low error rates with less training data compared to classical\ncounterparts. Furthermore, our method allows for the generation of datasets\nwith varying properties, facilitating further investigation into the\ncharacteristics of real-world datasets that may be particularly advantageous\nfor QKMs. We also show that the predicted performance from the analytical tool\nwe propose - a generalization metric from classical domain - show great\nalignment empirical evidence, which fills the gap previously existing in the\nfield. We pave a way to a comprehensive exploration of dataset complexities,\nproviding insights into how these complexities influence QML performance\nrelative to traditional methods. This research contributes to a deeper\nunderstanding of the generalization benefits of QKM models and potentially a\nbroader family of QML models, setting the stage for future advancements in the\nfield.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u5de5\u5177\u6765\u7814\u7a76\u7ecf\u5178-\u91cf\u5b50\u9e3f\u6c9f\u3002", "motivation": "\u65e8\u5728\u4e3a\u7406\u89e3\u6570\u636e\u96c6\u7279\u5f81\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5177\u51dd\u805a\u529b\u7684\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u91cf\u5b50\u6838\u65b9\u6cd5\uff0cQKMs\uff09\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u7ecf\u5178\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u751f\u6210\u534a\u4eba\u5de5\u5168\u7ecf\u5178\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u6e90\u81ea\u7ecf\u5178\u6838\u65b9\u6cd5\u7684\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u7814\u7a76\u7ecf\u5178-\u91cf\u5b50\u9e3f\u6c9f\u3002", "result": "QKMs \u5728\u8bad\u7ec3\u6570\u636e\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4f4e\u9519\u8bef\u7387\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u5206\u6790\u5de5\u5177\uff08\u6cdb\u5316\u5ea6\u91cf\uff09\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u8bc1\u8bc1\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u91cf\u5b50\u6838\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u6240\u63d0\u51fa\u7684\u5206\u6790\u5de5\u5177\u80fd\u591f\u6709\u6548\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u6df1\u5165\u7814\u7a76\u6570\u636e\u96c6\u590d\u6742\u6027\u548cQML\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19307", "abs": "https://arxiv.org/abs/2508.19307", "authors": ["Hamza Khan"], "title": "Advancements in Crop Analysis through Deep Learning and Explainable AI", "comment": "Master's thesis", "summary": "Rice is a staple food of global importance in terms of trade, nutrition, and\neconomic growth. Among Asian nations such as China, India, Pakistan, Thailand,\nVietnam and Indonesia are leading producers of both long and short grain\nvarieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To\nensure consumer satisfaction and strengthen national reputations, monitoring\nrice crops and grain quality is essential. Manual inspection, however, is\nlabour intensive, time consuming and error prone, highlighting the need for\nautomated solutions for quality control and yield improvement. This study\nproposes an automated approach to classify five rice grain varieties using\nConvolutional Neural Networks (CNN). A publicly available dataset of 75000\nimages was used for training and testing. Model evaluation employed accuracy,\nrecall, precision, F1-score, ROC curves, and confusion matrices. Results\ndemonstrated high classification accuracy with minimal misclassifications,\nconfirming the model effectiveness in distinguishing rice varieties. In\naddition, an accurate diagnostic method for rice leaf diseases such as Brown\nSpot, Blast, Bacterial Blight, and Tungro was developed. The framework combined\nexplainable artificial intelligence (XAI) with deep learning models including\nCNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP\n(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic\nExplanations) revealed how specific grain and leaf features influenced\npredictions, enhancing model transparency and reliability. The findings\ndemonstrate the strong potential of deep learning in agricultural applications,\npaving the way for robust, interpretable systems that can support automated\ncrop quality inspection and disease diagnosis, ultimately benefiting farmers,\nconsumers, and the agricultural economy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u81ea\u52a8\u5206\u7c7b\u4e94\u79cd\u5927\u7c73\u54c1\u79cd\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08CNN\u3001VGG16\u3001ResNet50\u3001MobileNetV2\uff09\u7684\u5927\u7c73\u53f6\u7247\u75c5\u5bb3\uff08\u8910\u6591\u75c5\u3001\u7a3b\u761f\u75c5\u3001\u7ec6\u83cc\u6027\u6761\u6591\u75c5\u548c\u9ec4\u840e\u75c5\uff09\u8bca\u65ad\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u6d88\u8d39\u8005\u6ee1\u610f\u5ea6\u548c\u56fd\u5bb6\u58f0\u8a89\uff0c\u76d1\u6d4b\u6c34\u7a3b\u4ea7\u91cf\u548c\u8c37\u7269\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u624b\u52a8\u68c0\u67e5\u52b3\u52a8\u5bc6\u96c6\u3001\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\u548c\u4ea7\u91cf\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u8fd8\u9700\u8981\u4e00\u79cd\u51c6\u786e\u7684\u8bca\u65ad\u65b9\u6cd5\u6765\u8bca\u65ad\u6c34\u7a3b\u53f6\u7247\u75c5\u5bb3\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u4e94\u79cd\u6c34\u7a3b\u8c37\u7269\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4f7f\u7528\u4e86\u5305\u542b75,000\u5f20\u56fe\u50cf\u7684\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u6a21\u578b\u8bc4\u4f30\u91c7\u7528\u4e86\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u3001F1\u5206\u6570\u3001ROC\u66f2\u7ebf\u548c\u6df7\u6dc6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08CNN\u3001VGG16\u3001ResNet50\u548cMobileNetV2\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u8bca\u65ad\u6c34\u7a3b\u53f6\u7247\u75c5\u5bb3\uff0c\u5e76\u4f7f\u7528\u4e86SHAP\u548cLIME\u7b49\u53ef\u89e3\u91ca\u6027\u6280\u672f\u3002", "result": "\u8be5\u6a21\u578b\u5728\u533a\u5206\u6c34\u7a3b\u54c1\u79cd\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c\u5f88\u5c11\u7684\u9519\u8bef\u5206\u7c7b\u3002\u6240\u5f00\u53d1\u7684\u6846\u67b6\u80fd\u591f\u51c6\u786e\u8bca\u65ad\u6c34\u7a3b\u53f6\u7247\u75c5\u5bb3\uff0c\u5e76\u4e14\u901a\u8fc7SHAP\u548cLIME\u7b49\u6280\u672f\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e86\u89e3\u7279\u5b9a\u7684\u8c37\u7269\u548c\u53f6\u7247\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u9884\u6d4b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u81ea\u52a8\u5316\u4f5c\u7269\u8d28\u91cf\u68c0\u6d4b\u548c\u75c5\u5bb3\u8bca\u65ad\u63d0\u4f9b\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u7684\u7cfb\u7edf\uff0c\u6700\u7ec8\u4f7f\u519c\u6c11\u3001\u6d88\u8d39\u8005\u548c\u519c\u4e1a\u7ecf\u6d4e\u53d7\u76ca\u3002"}}
{"id": "2508.19644", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19644", "abs": "https://arxiv.org/abs/2508.19644", "authors": ["Yiqing Wang", "Jian Zhou", "Chen Pang", "Wenyang Man", "Zixiang Xiong", "Ke Meng", "Zhanling Wang", "Yongzhen Li"], "title": "Low-Cost Architecture and Efficient Pattern Synthesis for Polarimetric Phased Array Based on Polarization Coding Reconfigurable Elements", "comment": null, "summary": "Polarimetric phased arrays (PPAs) enhance radar target detection and\nanti-jamming capabilities. However, the dual transmit/receive (T/R) channel\nrequirement leads to high costs and system complexity. To address this, this\npaper introduces a polarization-coding reconfigurable phased array (PCRPA) and\nassociated pattern synthesis techniques to reduce PPA costs while minimizing\nperformance degradation. Each PCRPA element connects to a single T/R channel\nand incorporates two-level RF switches for real-time control of polarization\nstates and waveforms. By adjusting element codes and excitation weights, the\nPCRPA can generate arbitrarily polarized and dual-polarized beams. Efficient\nbeam pattern synthesis methods are also proposed, featuring novel optimization\nconstraints derived from theoretical and analytical analysis of PCRPAs.\nSimulations demonstrate that the approach achieves low cross-polarization and\nsidelobe levels comparable to conventional architectures within the scan range,\nparticularly for large arrays. However, the channel reduction inevitably incurs\npower and directivity loss. Experiments conducted on an $8\\times 8$ X-band\narray antenna validate the effectiveness of the proposed system. The PCRPA and\nsynthesis methods are well-suited for large-scale PPA systems, offering\nsignificant cost-effectiveness while maintaining good sidelobe suppression and\npolarization control performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u5316\u7f16\u7801\u53ef\u91cd\u6784\u76f8\u63a7\u9635\uff08PCRPA\uff09\u53ca\u5176\u76f8\u5173\u7684\u6a21\u5f0f\u7efc\u5408\u6280\u672f\uff0c\u65e8\u5728\u964d\u4f4e\u6781\u5316\u76f8\u63a7\u9635\uff08PPA\uff09\u7684\u6210\u672c\u5e76\u6700\u5c0f\u5316\u6027\u80fd\u635f\u5931\u3002PCRPA\u901a\u8fc7\u5355\u6536\u53d1\uff08T/R\uff09\u901a\u9053\u548c\u4e24\u7ea7\u5c04\u9891\u5f00\u5173\u5b9e\u73b0\u6781\u5316\u72b6\u6001\u548c\u6ce2\u5f62\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u80fd\u591f\u751f\u6210\u4efb\u610f\u6781\u5316\u548c\u53cc\u6781\u5316\u6ce2\u675f\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cPCRPA\u5728\u626b\u63cf\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u7ed3\u6784\u76f8\u5f53\u7684\u4f4e\u4ea4\u53c9\u6781\u5316\u548c\u65c1\u74e3\u7535\u5e73\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u578b\u9635\u5217\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0cPCRPA\u5728\u6210\u672c\u6548\u76ca\u548c\u6027\u80fd\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21PPA\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u6781\u5316\u76f8\u63a7\u9635\uff08PPA\uff09\u7684\u6210\u672c\u548c\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u89e3\u51b3\u4e86\u5176\u53cc\u6536\u53d1\uff08T/R\uff09\u901a\u9053\u8981\u6c42\u5e26\u6765\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u5316\u7f16\u7801\u53ef\u91cd\u6784\u76f8\u63a7\u9635\uff08PCRPA\uff09\u53ca\u5176\u76f8\u5173\u7684\u6a21\u5f0f\u7efc\u5408\u6280\u672f\u3002PCRPA\u7684\u6bcf\u4e2a\u5355\u5143\u8fde\u63a5\u5230\u5355\u4e2aT/R\u901a\u9053\uff0c\u5e76\u5305\u542b\u4e24\u7ea7\u5c04\u9891\u5f00\u5173\uff0c\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u6781\u5316\u72b6\u6001\u548c\u6ce2\u5f62\u3002\u901a\u8fc7\u8c03\u6574\u5355\u5143\u7f16\u7801\u548c\u6fc0\u52b1\u6743\u91cd\uff0cPCRPA\u80fd\u591f\u751f\u6210\u4efb\u610f\u6781\u5316\u548c\u53cc\u6781\u5316\u6ce2\u675f\u3002\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u6ce2\u675f\u6a21\u5f0f\u7efc\u5408\u65b9\u6cd5\uff0c\u5176\u4e2d\u5305\u542b\u6e90\u4e8ePCRPA\u7406\u8bba\u548c\u5206\u6790\u7684\u4f18\u5316\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u626b\u63cf\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u7ed3\u6784\u76f8\u5f53\u7684\u4f4e\u4ea4\u53c9\u6781\u5316\u548c\u65c1\u74e3\u7535\u5e73\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u578b\u9635\u5217\u3002\u4f46\u901a\u9053\u51cf\u5c11\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5e26\u6765\u529f\u7387\u548c\u65b9\u5411\u6027\u635f\u5931\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u57288x8 X\u6ce2\u6bb5\u9635\u5217\u5929\u7ebf\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "PCRPA\u53ca\u5176\u7efc\u5408\u65b9\u6cd5\u975e\u5e38\u9002\u5408\u5927\u89c4\u6a21PPA\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u826f\u597d\u65c1\u74e3\u6291\u5236\u548c\u6781\u5316\u63a7\u5236\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.", "AI": {"tldr": "ReST-RL\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u6539\u8fdbGRPO\u7b97\u6cd5\u548c\u57fa\u4e8e\u4ef7\u503c\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u89e3\u7801\u65b9\u6cd5\uff08VM-MCTS\uff09\u7684\u7edf\u4e00LLM\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u65e8\u5728\u63d0\u5347LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5982GRPO\u56e0\u5956\u52b1\u65b9\u5dee\u4e0d\u8db3\u800c\u5931\u6548\uff0c\u57fa\u4e8e\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7684\u65b9\u6cd5\u5219\u9762\u4e34\u6570\u636e\u83b7\u53d6\u548c\u9a8c\u8bc1\u6709\u6548\u6027\u7684\u6311\u6218\u3002ReST-RL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ReST-RL\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1. ReST-GRPO\uff1a\u4f7f\u7528\u4f18\u5316\u7684ReST\u7b97\u6cd5\u7b5b\u9009\u548c\u7ec4\u5408\u9ad8\u4ef7\u503c\u8bad\u7ec3\u6570\u636e\uff0c\u589e\u52a0GRPO\u91c7\u6837\u5956\u52b1\u65b9\u5dee\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6548\u679c\u30022. VM-MCTS\uff1a\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6536\u96c6\u65e0\u6807\u6ce8\u7684\u4ef7\u503c\u76ee\u6807\uff0c\u8bad\u7ec3\u4ef7\u503c\u6a21\u578b\uff08VM\uff09\uff0c\u5e76\u5728\u89e3\u7801\u65f6\u901a\u8fc7\u9002\u914d\u7684MCTS\u7b97\u6cd5\u90e8\u7f72VM\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u8fdb\u7a0b\u4fe1\u53f7\u548c\u9a8c\u8bc1\u5206\u6570\uff0c\u8f85\u52a9LLM\u7b56\u7565\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReST-RL\u5728APPS\u3001BigCodeBench\u548cHumanEval\u7b49\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5316\u8bad\u7ec3\u57fa\u7ebf\uff08\u5982naive GRPO\u548cReST-DPO\uff09\u4ee5\u53ca\u89e3\u7801\u548c\u9a8c\u8bc1\u57fa\u7ebf\uff08\u5982PRM-BoN\u548cORM-MCTS\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3aLLM\u7b56\u7565\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ReST-RL\u901a\u8fc7\u7ed3\u5408\u6539\u8fdb\u7684GRPO\u548cVM-MCTS\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19607", "abs": "https://arxiv.org/abs/2508.19607", "authors": ["Amin Berjaoui Tahmaz", "Ravi Prakash", "Jens Kober"], "title": "Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks", "comment": "This article is accepted for publication in IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "This paper presents an Impedance Primitive-augmented hierarchical\nreinforcement learning framework for efficient robotic manipulation in\nsequential contact tasks. We leverage this hierarchical structure to\nsequentially execute behavior primitives with variable stiffness control\ncapabilities for contact tasks. Our proposed approach relies on three key\ncomponents: an action space enabling variable stiffness control, an adaptive\nstiffness controller for dynamic stiffness adjustments during primitive\nexecution, and affordance coupling for efficient exploration while encouraging\ncompliance. Through comprehensive training and evaluation, our framework learns\nefficient stiffness control capabilities and demonstrates improvements in\nlearning efficiency, compositionality in primitive selection, and success rates\ncompared to the state-of-the-art. The training environments include block\nlifting, door opening, object pushing, and surface cleaning. Real world\nevaluations further confirm the framework's sim2real capability. This work lays\nthe foundation for more adaptive and versatile robotic manipulation systems,\nwith potential applications in more complex contact-based tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u4e86\u963b\u6297\u539f\u8bed\u7684\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5e8f\u5217\u63a5\u89e6\u4efb\u52a1\u4e2d\u8fdb\u884c\u9ad8\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u8be5\u6846\u67b6\u5229\u7528\u5c42\u6b21\u7ed3\u6784\uff0c\u987a\u5e8f\u6267\u884c\u5177\u6709\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u80fd\u529b\u7684\u884c\u4e3a\u539f\u8bed\u3002\u5176\u5173\u952e\u7ec4\u6210\u90e8\u5206\u5305\u62ec\uff1a\u652f\u6301\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u7528\u4e8e\u5728\u539f\u8bed\u6267\u884c\u671f\u95f4\u52a8\u6001\u8c03\u6574\u521a\u5ea6\u7684\u81ea\u9002\u5e94\u521a\u5ea6\u63a7\u5236\u5668\uff0c\u4ee5\u53ca\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22\u548c\u9f13\u52b1\u987a\u5e94\u6027\u7684\u53ef\u4f9b\u6027\u8026\u5408\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u5b66\u4e60\u4e86\u9ad8\u6548\u7684\u521a\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u5728\u5b66\u4e60\u6548\u7387\u3001\u539f\u8bed\u9009\u62e9\u7684\u7ec4\u5408\u6027\u4ee5\u53ca\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u8bad\u7ec3\u73af\u5883\u5305\u62ec\u4e86\u5757\u72b6\u7269\u63d0\u5347\u3001\u5f00\u95e8\u3001\u7269\u4f53\u63a8\u52a8\u548c\u8868\u9762\u6e05\u6d01\u3002\u5b9e\u9645\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u4eff\u771f\u5230\u73b0\u5b9e\uff08sim2real\uff09\u80fd\u529b\u3002\u8be5\u5de5\u4f5c\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6709\u671b\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u57fa\u4e8e\u63a5\u89e6\u7684\u4efb\u52a1\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u5728\u5e8f\u5217\u63a5\u89e6\u4efb\u52a1\u4e2d\u9ad8\u6548\u5730\u8fdb\u884c\u64cd\u4f5c\uff0c\u9700\u8981\u5177\u5907\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u4e86\u963b\u6297\u539f\u8bed\u7684\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u652f\u6301\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u81ea\u9002\u5e94\u521a\u5ea6\u63a7\u5236\u5668\u548c\u53ef\u4f9b\u6027\u8026\u5408\u3002", "result": "\u5728\u5757\u72b6\u7269\u63d0\u5347\u3001\u5f00\u95e8\u3001\u7269\u4f53\u63a8\u52a8\u548c\u8868\u9762\u6e05\u6d01\u7b49\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5b66\u4e60\u4e86\u9ad8\u6548\u7684\u521a\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u5728\u5b66\u4e60\u6548\u7387\u3001\u539f\u8bed\u9009\u62e9\u7684\u7ec4\u5408\u6027\u4ee5\u53ca\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u5b9e\u9645\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u4eff\u771f\u5230\u73b0\u5b9e\uff08sim2real\uff09\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u57fa\u4e8e\u63a5\u89e6\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.19361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19361", "abs": "https://arxiv.org/abs/2508.19361", "authors": ["Yongbin Lee", "Ki H. Chon"], "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture", "comment": "4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on\n  Body Sensor Networks (IEEE-EMBS BSN 2025)", "summary": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk\nof stroke, heart failure, and other cardiovascular complications. While AF\ndetection algorithms perform well in identifying persistent AF, early-stage\nprogression, such as paroxysmal AF (PAF), often goes undetected due to its\nsudden onset and short duration. However, undetected PAF can progress into\nsustained AF, increasing the risk of mortality and severe complications. Early\nprediction of AF offers an opportunity to reduce disease progression through\npreventive therapies, such as catecholamine-sparing agents or beta-blockers. In\nthis study, we propose a lightweight deep learning model using only RR\nIntervals (RRIs), combining a Temporal Convolutional Network (TCN) for\npositional encoding with Mamba, a selective state space model, to enable early\nprediction of AF through efficient parallel sequence modeling. In subject-wise\ntesting results, our model achieved a sensitivity of 0.908, specificity of\n0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our\nmethod demonstrates high computational efficiency, with only 73.5 thousand\nparameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural\nNetwork-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and\nmodel compactness. Notably, the model can predict AF up to two hours in advance\nusing just 30 minutes of input data, providing enough lead time for preventive\ninterventions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19363", "abs": "https://arxiv.org/abs/2508.19363", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5df2\u6709\u57fa\u51c6\uff0c\u4f46\u957f\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51faLongReasonArena\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u7684\u957f\u63a8\u7406\u80fd\u529b\uff0c\u4efb\u52a1\u6d89\u53ca\u591a\u6b65\u7b97\u6cd5\u3001\u68c0\u7d22\u548c\u56de\u6eaf\uff0c\u63a8\u7406\u957f\u5ea6\u53ef\u6269\u5c55\u81f3100\u4e07token\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u57fa\u51c6\u5bf9\u73b0\u6709LLMs\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u51c6\u786e\u7387\u968f\u63a8\u7406\u6b65\u6570\u5bf9\u6570\u7684\u589e\u52a0\u800c\u7ebf\u6027\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u957f\u8f93\u5165\u7406\u89e3\uff0c\u5ffd\u89c6\u4e86\u957f\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faLongReasonArena\u57fa\u51c6\uff0c\u5305\u542b\u9700\u8981\u6a21\u578b\u6267\u884c\u591a\u6b65\u7b97\u6cd5\uff08\u6d89\u53ca\u68c0\u7d22\u548c\u56de\u6eaf\uff09\u7684\u4efb\u52a1\uff0c\u5141\u8bb8\u4efb\u610f\u6269\u5c55\u63a8\u7406\u957f\u5ea6\uff0c\u6700\u9ad8\u53ef\u8fbe100\u4e07token\u3002", "result": "LongReasonArena\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u90fd\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u4f8b\u5982Deepseek-R1\u5728\u8be5\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a7.5%\u3002\u51c6\u786e\u7387\u4e0e\u9884\u671f\u63a8\u7406\u6b65\u6570\u5bf9\u6570\u5448\u7ebf\u6027\u8d1f\u76f8\u5173\u3002", "conclusion": "LongReasonArena\u662f\u8bc4\u4f30LLMs\u957f\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8be5\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19916", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19916", "abs": "https://arxiv.org/abs/2508.19916", "authors": ["Jeroen J. de Boer", "Agustin O. Alvarez", "Moritz C. Schmidt", "Bruno Ehrler"], "title": "Microscale optoelectronic reservoir networks of halide perovskite for in-sensor computing", "comment": null, "summary": "Physical reservoir computing is a promising framework for efficient\nneuromorphic in and near-sensor computing applications. Here, we demonstrate a\nmultimodal optoelectronic reservoir network based on halide perovskite\nsemiconductor devices, capable of processing both voltage and light inputs. The\ndevices consist of micrometer-sized, asymmetric crossbars covered with a MAPbI3\nperovskite film. In a network, we simulate the performance by transforming\nMNIST images and videos based on the NMNIST dataset using 4-bit inputs and\ntraining linear readout layers for classification. We demonstrate multimodal\nnetworks capable of processing both voltage and light inputs, reaching mean\naccuracies up to 95.3 p/m 0.1% and 87.8 p/m 0.1% for image and video\nclassification, respectively. We observed only minor deterioration due to\nmeasurement noise. The networks significantly outperformed linear classifier\nreferences, by 3.1% for images and 14.6% for video. We show that longer\nretention times benefit classification accuracy for single-mode networks, and\ngive guidelines for choosing optimal experimental parameters. Moreover, the\nmicroscale device architecture lends itself well to further downscaling in\nhigh-density sensor arrays, making the devices ideal for efficient in-sensor\ncomputing.", "AI": {"tldr": "\u57fa\u4e8e\u5364\u5316\u7269\u9499\u949b\u77ff\u534a\u5bfc\u4f53\u7684\u591a\u6a21\u6001\u5149\u7535\u5668\u4ef6\u6c34\u5e93\u7f51\u7edc\uff0c\u53ef\u5904\u7406\u7535\u538b\u548c\u5149\u8f93\u5165\uff0c\u7528\u4e8e\u624b\u5199\u6570\u5b57\u548c\u89c6\u9891\u5206\u7c7b\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6548\u7684\u795e\u7ecf\u5f62\u6001\u548c\u8fd1\u4f20\u611f\u5668\u8ba1\u7b97\u5e94\u7528\u3002", "method": "\u6784\u5efa\u57fa\u4e8eMAPbI3\u9499\u949b\u77ff\u8584\u819c\u7684\u5fae\u7c73\u7ea7\u4e0d\u5bf9\u79f0\u4ea4\u53c9\u7ed3\u6784\u6c34\u5e93\u7f51\u7edc\uff0c\u5904\u74064\u4f4d\u7535\u538b\u548c\u5149\u4fe1\u53f7\uff0c\u5e76\u8bad\u7ec3\u7ebf\u6027\u8bfb\u53d6\u5c42\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u56fe\u50cf\u548c\u89c6\u9891\u5206\u7c7b\u7684\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523095.3%\u548c87.8%\uff0c\u4e14\u4ec5\u6709\u8f7b\u5fae\u7684\u6d4b\u91cf\u566a\u58f0\u52a3\u5316\u3002\u4e0e\u7ebf\u6027\u5206\u7c7b\u5668\u76f8\u6bd4\uff0c\u6027\u80fd\u5206\u522b\u63d0\u53473.1%\u548c14.6%\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u6c34\u5e93\u7f51\u7edc\u5728\u56fe\u50cf\u548c\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u5ef6\u957f\u4fdd\u6301\u65f6\u95f4\u6709\u5229\u4e8e\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u4f73\u5b9e\u9a8c\u53c2\u6570\u7684\u6307\u5bfc\u3002\u5176\u5fae\u578b\u5668\u4ef6\u7ed3\u6784\u9002\u5408\u9ad8\u5bc6\u5ea6\u4f20\u611f\u5668\u9635\u5217\uff0c\u662f\u9ad8\u6548\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2508.19726", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2508.19726", "abs": "https://arxiv.org/abs/2508.19726", "authors": ["Yu. S. Barash"], "title": "Casimir-Lifshitz interaction between bodies integrated in a micro/nanoelectromechanical quantum damped oscillator", "comment": "14 pages, 2 figures", "summary": "A theory is proposed for the component of the Casimir-like force that arises\nbetween bodies embedded in a macroscopic quantum damped oscillator. When the\noscillator's parameters depend on the distance between the bodies, the\noscillator-induced Casimir-like force is generally determined by a broad\nspectral range extending to high frequencies, limited by the frequency\ndispersion of the damping function. Here it is shown that there is a large\nclass of systems in which the low-frequency range dominates the forces. This\nallows for the use of the Ohmic approximation, which is crucial for extending\nthe theory to the lumped element description of fluctuation-induced forces in\nelectrical circuits. Estimates of the circuit-induced Casimir-Lifshitz force\nsuggest that under certain conditions it can be identified experimentally due\nto its dependence on various circuit elements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\uff0c\u7528\u4e8e\u89e3\u91ca\u5d4c\u5165\u5b8f\u89c2\u91cf\u5b50\u963b\u5c3c\u632f\u8361\u5668\u4e2d\u7684\u7269\u4f53\u4e4b\u95f4\u4ea7\u751f\u7684\u7c7b\u4f3c\u5361\u897f\u7c73\u5c14\u7684\u529b\u7684\u5206\u91cf\u3002\u5f53\u632f\u8361\u5668\u7684\u53c2\u6570\u53d6\u51b3\u4e8e\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\u65f6\uff0c\u632f\u8361\u5668\u5f15\u8d77\u7684\u7c7b\u4f3c\u5361\u897f\u7c73\u5c14\u7684\u529b\u901a\u5e38\u7531\u5ef6\u4f38\u5230\u9ad8\u9891\u7684\u5e7f\u6cdb\u9891\u8c31\u8303\u56f4\u51b3\u5b9a\uff0c\u5e76\u53d7\u963b\u5c3c\u51fd\u6570\u9891\u7387\u8272\u6563\u7684\u9650\u5236\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5f53\u632f\u8361\u5668\u7684\u53c2\u6570\u53d6\u51b3\u4e8e\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\u65f6\uff0c\u632f\u8361\u5668\u5f15\u8d77\u7684\u7c7b\u4f3c\u5361\u897f\u7c73\u5c14\u7684\u529b\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\u6765\u89e3\u91ca\u5176\u4ea7\u751f\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\uff0c\u5e76\u8bc1\u660e\u4e86\u4f4e\u9891\u8303\u56f4\u5728\u8bb8\u591a\u7cfb\u7edf\u4e2d\u5bf9\u529b\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u4ece\u800c\u53ef\u4ee5\u4f7f\u7528\u6b27\u59c6\u8fd1\u4f3c\u3002\u8fd9\u4f7f\u5f97\u8be5\u7406\u8bba\u80fd\u591f\u6269\u5c55\u5230\u63cf\u8ff0\u7535\u8def\u4e2d\u7531\u6da8\u843d\u5f15\u8d77\u7684\u529b\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6b27\u59c6\u8fd1\u4f3c\uff0c\u8be5\u7406\u8bba\u53ef\u4ee5\u6269\u5c55\u5230\u96c6\u603b\u5143\u4ef6\u63cf\u8ff0\uff0c\u5e76\u4e14\u8be5\u8bba\u6587\u7684\u4f30\u8ba1\u8868\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u7535\u8def\u5f15\u8d77\u7684\u5361\u897f\u7c73\u5c14-\u5229\u592b\u5e0c\u8328\u529b\u53ef\u4ee5\u88ab\u5b9e\u9a8c\u8bc6\u522b\uff0c\u8fd9\u53d6\u51b3\u4e8e\u5404\u79cd\u7535\u8def\u5143\u4ef6\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u7535\u8def\u5f15\u8d77\u7684\u5361\u897f\u7c73\u5c14-\u5229\u592b\u5e0c\u8328\u529b\u53ef\u4ee5\u901a\u8fc7\u5176\u5bf9\u7535\u8def\u5143\u4ef6\u7684\u4f9d\u8d56\u6027\u5728\u5b9e\u9a8c\u4e2d\u88ab\u8bc6\u522b\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u5229\u7528\u8fd9\u79cd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.19538", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19538", "abs": "https://arxiv.org/abs/2508.19538", "authors": ["Tian-Xiang Zhu", "Xiao Liu", "Zong-Quan Zhou", "Chuan-Feng Li"], "title": "Remote Quantum Networks based on Quantum Memories", "comment": "19 pages, 5 figures", "summary": "Quantum networks, capable of transmitting arbitrary quantum states, provide a\nfoundation for a wide range of quantum applications, including distributed\nquantum computing, distributed quantum sensing, and quantum communication.\nPhotons are the natural carrier of information in quantum networks, but the\nexponential loss of optical fiber channels prevents the construction of\nlarge-scale quantum networks. A potential solution is implementing quantum\nrepeaters based on quantum memories, which can efficiently establish\nlong-distance entanglement from short-distance entanglement. In the past\ndecades, intense efforts have been devoted to constructing large-scale quantum\nnetworks based on various atomic quantum memories. In this Perspective, we\npresent a concise overview of current advancements in remote quantum networks,\nelucidate the imminent challenges that must be addressed, and discuss the\nfuture directions.", "AI": {"tldr": "\u91cf\u5b50\u7f51\u7edc\u5229\u7528\u91cf\u5b50\u5b58\u50a8\u5668\u514b\u670d\u5149\u7ea4\u635f\u8017\uff0c\u5b9e\u73b0\u8fdc\u8ddd\u79bb\u7ea0\u7f20\uff0c\u4f46\u9762\u4e34\u89c4\u6a21\u5316\u6311\u6218\uff0c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u503c\u5f97\u5173\u6ce8\u3002", "motivation": "\u5149\u7ea4\u635f\u8017\u9650\u5236\u4e86\u91cf\u5b50\u7f51\u7edc\u7684\u89c4\u6a21\u5316\uff0c\u91cf\u5b50\u5b58\u50a8\u5668\u662f\u514b\u670d\u6b64\u6311\u6218\u7684\u5173\u952e\uff0c\u4f46\u76ee\u524d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u6982\u8ff0\u4e86\u91cf\u5b50\u7f51\u7edc\uff08\u7279\u522b\u662f\u57fa\u4e8e\u539f\u5b50\u91cf\u5b50\u5b58\u50a8\u5668\u7684\uff09\u7684\u8fdb\u5c55\uff0c\u9610\u8ff0\u4e86\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002", "result": "\u5c55\u793a\u4e86\u91cf\u5b50\u7f51\u7edc\u548c\u91cf\u5b50\u5b58\u50a8\u5668\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u660e\u786e\u4e86\u89c4\u6a21\u5316\u91cf\u5b50\u7f51\u7edc\u5efa\u8bbe\u7684\u6311\u6218\u3002", "conclusion": "\u867d\u7136\u91cf\u5b50\u7f51\u7edc\u548c\u91cf\u5b50\u5b58\u50a8\u5668\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8981\u5b9e\u73b0\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u4ecd\u9700\u514b\u670d\u8bf8\u591a\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u65b0\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.19312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19312", "abs": "https://arxiv.org/abs/2508.19312", "authors": ["Ander Galv\u00e1n", "Marivi Higuero", "Jorge Sasiain", "Eduardo Jacob"], "title": "Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax", "comment": "Aceptado para publicaci\\'on, in Spanish language. XVII Jornadas de\n  Ingenier\\'ia Telem\\'atica (JITEL 2025)", "summary": "Facial recognition powered by Artificial Intelligence has achieved high\naccuracy in specific scenarios and applications. Nevertheless, it faces\nsignificant challenges regarding privacy and identity management, particularly\nwhen unknown individuals appear in the operational context. This paper presents\nthe design, implementation, and evaluation of a facial recognition system\nwithin a federated learning framework tailored to open-set scenarios. The\nproposed approach integrates the OpenMax algorithm into federated learning,\nleveraging the exchange of mean activation vectors and local distance measures\nto reliably distinguish between known and unknown subjects. Experimental\nresults validate the effectiveness of the proposed solution, demonstrating its\npotential for enhancing privacy-aware and robust facial recognition in\ndistributed environments.\n  --\n  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado\nuna alta precisi\\'on en algunos escenarios y aplicaciones. Sin embargo,\npresenta desaf\\'ios relacionados con la privacidad y la identificaci\\'on de\npersonas, especialmente considerando que pueden aparecer sujetos desconocidos\npara el sistema que lo implementa. En este trabajo, se propone el dise\\~no,\nimplementaci\\'on y evaluaci\\'on de un sistema de reconocimiento facial en un\nescenario de aprendizaje federado, orientado a conjuntos abiertos.\nConcretamente, se dise\\~na una soluci\\'on basada en el algoritmo OpenMax para\nescenarios de aprendizaje federado. La propuesta emplea el intercambio de los\nvectores de activaci\\'on promedio y distancias locales para identificar de\nmanera eficaz tanto personas conocidas como desconocidas. Los experimentos\nrealizados demuestran la implementaci\\'on efectiva de la soluci\\'on propuesta.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06OpenMax\u7b97\u6cd5\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u573a\u666f\u4e0b\u8fdb\u884c\u66f4\u5177\u9690\u79c1\u610f\u8bc6\u548c\u9c81\u68d2\u6027\u7684\u9762\u90e8\u8bc6\u522b\uff0c\u6709\u6548\u5730\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u4e2a\u4f53\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9762\u90e8\u8bc6\u522b\u6280\u672f\u5728\u5904\u7406\u672a\u77e5\u4e2a\u4f53\u548c\u4fdd\u62a4\u9690\u79c1\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u3002", "method": "\u5c06OpenMax\u7b97\u6cd5\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u4ea4\u6362\u5e73\u5747\u6fc0\u6d3b\u5411\u91cf\u548c\u5c40\u90e8\u8ddd\u79bb\u5ea6\u91cf\u6765\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u53d7\u8bd5\u8005\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u589e\u5f3a\u9690\u79c1\u611f\u77e5\u548c\u9c81\u68d2\u6027\u9762\u90e8\u8bc6\u522b\u7684\u6f5c\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7ed3\u5408OpenMax\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5f00\u653e\u573a\u666f\u4e0b\u7684\u9690\u79c1\u548c\u8eab\u4efd\u7ba1\u7406\u6311\u6218\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u9762\u90e8\u8bc6\u522b\u3002"}}
{"id": "2508.19678", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19678", "abs": "https://arxiv.org/abs/2508.19678", "authors": ["Chao Wang", "Shuyuan Zhang", "Lei Wang"], "title": "Distributed Safety-Critical MPC for Multi-Agent Formation Control and Obstacle Avoidance", "comment": null, "summary": "For nonlinear multi-agent systems with high relative degrees, achieving\nformation control and obstacle avoidance in a distributed manner remains a\nsignificant challenge. To address this issue, we propose a novel distributed\nsafety-critical model predictive control (DSMPC) algorithm that incorporates\ndiscrete-time high-order control barrier functions (DHCBFs) to enforce safety\nconstraints, alongside discrete-time control Lyapunov functions (DCLFs) to\nestablish terminal constraints. To facilitate distributed implementation, we\ndevelop estimated neighbor states for formulating DHCBFs and DCLFs, while also\ndevising a bound constraint to limit estimation errors and ensure convergence.\nAdditionally, we provide theoretical guarantees regarding the feasibility and\nstability of the proposed DSMPC algorithm based on a mild assumption. The\neffectiveness of the proposed method is evidenced by the simulation results,\ndemonstrating improved performance and reduced computation time compared to\nexisting approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u5173\u952e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08DSMPC\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u79bb\u6563\u65f6\u95f4\u9ad8\u9636\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08DHCBFs\uff09\u548c\u79bb\u6563\u65f6\u95f4\u63a7\u5236Lyapunov\u51fd\u6570\uff08DCLFs\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u76f8\u5bf9\u5ea6\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u7f16\u961f\u63a7\u5236\u548c\u907f\u969c\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9ad8\u76f8\u5bf9\u5ea6\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u7f16\u961f\u63a7\u5236\u548c\u907f\u969c\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408DHCBFs\u548cDCLFs\u7684DSMPC\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u90bb\u5c45\u72b6\u6001\u6765\u4fc3\u8fdb\u5206\u5e03\u5f0f\u5b9e\u73b0\uff0c\u5e76\u5f15\u5165\u754c\u7ea6\u675f\u6765\u9650\u5236\u4f30\u8ba1\u8bef\u5dee\u4ee5\u786e\u4fdd\u6536\u655b\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684DSMPC\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u5747\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DSMPC\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u9ad8\u76f8\u5bf9\u5ea6\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u7f16\u961f\u63a7\u5236\u548c\u907f\u969c\u95ee\u9898\uff0c\u5e76\u5177\u6709\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u6559\u5b66\u4ee3\u7406\u201d\u7684\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u7684\u8bfe\u7a0b\u6750\u6599\u751f\u6210\uff0c\u5305\u62ec\u6559\u5b66\u5927\u7eb2\u3001\u8bb2\u7a3f\u3001LaTeX\u5e7b\u706f\u7247\u548c\u8bc4\u4f30\u3002\u4e0e\u53ea\u5173\u6ce8\u5355\u4e00\u4efb\u52a1\u7684\u73b0\u6709AI\u5de5\u5177\u4e0d\u540c\uff0c\u201c\u6559\u5b66\u4ee3\u7406\u201d\u901a\u8fc7\u6a21\u62df\u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u6765\u751f\u6210\u8fde\u8d2f\u4e14\u7b26\u5408\u6559\u5b66\u8981\u6c42\u7684\u8bfe\u7a0b\u5185\u5bb9\u3002", "motivation": "\u76ee\u524d\u51c6\u5907\u9ad8\u8d28\u91cf\u6559\u5b66\u6750\u6599\u7684\u8fc7\u7a0b\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u6559\u5e08\u3001\u6559\u5b66\u8bbe\u8ba1\u5e08\u548c\u52a9\u6559\u4e4b\u95f4\u8fdb\u884c\u5927\u91cf\u534f\u8c03\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLM\u6846\u67b6\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u6559\u5b66\u4ee3\u7406\u201d\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u6a21\u62df\u6559\u80b2\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u89d2\u8272\u534f\u4f5c\uff0c\u4ee5\u751f\u6210\u8fde\u8d2f\u4e14\u7b26\u5408\u6559\u5b66\u8981\u6c42\u7684\u8bfe\u7a0b\u5185\u5bb9\u3002\u8be5\u7cfb\u7edf\u6709\u56db\u79cd\u6a21\u5f0f\uff1a\u81ea\u4e3b\u6a21\u5f0f\u3001\u76ee\u5f55\u5f15\u5bfc\u6a21\u5f0f\u3001\u53cd\u9988\u5f15\u5bfc\u6a21\u5f0f\u548c\u5b8c\u5168\u8054\u5408\u98de\u884c\u5458\u6a21\u5f0f\uff0c\u5141\u8bb8\u7075\u6d3b\u63a7\u5236\u4eba\u7c7b\u53c2\u4e0e\u7a0b\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e2d\u8bc4\u4f30\u4e86\u201c\u6559\u5b66\u4ee3\u7406\u201d\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u6750\u6599\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5f00\u53d1\u65f6\u95f4\u548c\u4eba\u529b\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u201c\u6559\u5b66\u4ee3\u7406\u201d\u4e3a\u6559\u5b66\u8bbe\u8ba1\u80fd\u529b\u6709\u9650\u7684\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u666e\u53ca\u9ad8\u8d28\u91cf\u7684\u6559\u80b2\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2508.19608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19608", "abs": "https://arxiv.org/abs/2508.19608", "authors": ["Dongjae Lee", "Byeongjun Kim", "H. Jin Kim"], "title": "Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning", "comment": null, "summary": "Aerial manipulators based on conventional multirotors can conduct\nmanipulation only in small roll and pitch angles due to the underactuatedness\nof the multirotor base. If the multirotor base is capable of hovering at\narbitrary orientation, the robot can freely locate itself at any point in\n$\\mathsf{SE}(3)$, significantly extending its manipulation workspace and\nenabling a manipulation task that was originally not viable. In this work, we\npresent a geometric robust control and whole-body motion planning framework for\nan omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,\nwe first propose a geometric robust controller for a floating base. Since the\nmotion of the robotic arm and the interaction forces during manipulation affect\nthe stability of the floating base, the base should be capable of mitigating\nthese adverse effects while controlling its 6D pose. We then design a two-step\noptimization-based whole-body motion planner, jointly considering the pose of\nthe floating base and the joint angles of the robotic arm to harness the entire\nconfiguration space. The devised two-step approach facilitates real-time\napplicability and enhances convergence of the optimization problem with\nnon-convex and non-Euclidean search space. The proposed approach enables the\nbase to be stationary at any 6D pose while autonomously carrying out\nsophisticated manipulation near obstacles without any collision. We demonstrate\nthe effectiveness of the proposed framework through experiments in which an OAM\nperforms grasping and pulling of an object in multiple scenarios, including\nnear $90^\\circ$ and even $180^\\circ$ pitch angles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u5411\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08OAM\uff09\u7684\u51e0\u4f55\u9c81\u68d2\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u5141\u8bb8\u5176\u5728\u4efb\u610f\u59ff\u6001\u4e0b\u60ac\u505c\uff0c\u4ece\u800c\u6269\u5c55\u4e86\u64cd\u4f5c\u8303\u56f4\u5e76\u5b9e\u73b0\u4e86\u6b64\u524d\u4e0d\u53ef\u884c\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u4f20\u7edf\u591a\u65cb\u7ffc\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\u8303\u56f4\u5e76\u514b\u670d\u5176\u5728\u5c0f\u4fef\u4ef0\u89d2\u4e0b\u7684\u9650\u5236\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u7a7a\u4e2d\u673a\u68b0\u81c2\u5728\u4efb\u610f\u59ff\u6001\u4e0b\u7684\u7a33\u5b9a\u60ac\u505c\u548c\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d6e\u52a8\u57fa\u5ea7\u7684\u51e0\u4f55\u9c81\u68d2\u63a7\u5236\u5668\uff0c\u4ee5\u89e3\u51b3\u673a\u68b0\u81c2\u8fd0\u52a8\u548c\u4ea4\u4e92\u529b\u5bf9\u57fa\u5ea7\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u6b65\u4f18\u5316\u7684\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u8054\u5408\u8003\u8651\u57fa\u5ea7\u4f4d\u59ff\u548c\u673a\u68b0\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u64cd\u4f5c\u548c\u4f18\u5316\u6536\u655b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4f7f\u57fa\u5ea7\u5728\u4efb\u610f\u516d\u7ef4\u4f4d\u59ff\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u5e76\u81ea\u4e3b\u6267\u884c\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5373\u4f7f\u5728\u63a5\u8fd190\u5ea6\u751a\u81f3180\u5ea6\u7684\u4fef\u4ef0\u89d2\u4e0b\u4e5f\u80fd\u5728\u969c\u788d\u7269\u9644\u8fd1\u5b89\u5168\u5730\u8fdb\u884c\u6293\u53d6\u548c\u62c9\u52a8\u64cd\u4f5c\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4f7f\u5168\u5411\u7a7a\u4e2d\u673a\u68b0\u81c2\u5728\u5404\u79cd\u573a\u666f\u4e0b\uff08\u5305\u62ec\u5927\u4fef\u4ef0\u89d2\uff09\u7cbe\u786e\u3001\u5b89\u5168\u5730\u6267\u884c\u6293\u53d6\u548c\u62c9\u52a8\u7b49\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2508.19366", "categories": ["cs.LG", "cs.AI", "53B21, 46E22 (Primary), 68R10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.19366", "abs": "https://arxiv.org/abs/2508.19366", "authors": ["Supratik Sarkar", "Swagatam Das"], "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs", "comment": "29 pages, 3 figures, 1 table", "summary": "Hallucinations in large language models (LLMs) remain a fundamental obstacle\nto trustworthy AI, particularly in high-stakes multimodal domains such as\nmedicine, law, and finance. Existing evaluation techniques are largely\nheuristic -- anchored in qualitative benchmarking or ad-hoc empirical\nmitigation -- providing neither principled quantification nor actionable\ntheoretical guarantees. This gap leaves a critical blind spot in understanding\nhow hallucinations arise, propagate, and interact across modalities. We\nintroduce the first (to our knowledge) rigorous information geometric framework\nin diffusion dynamics for quantifying hallucinations in multimodal LLMs\n(MLLMs), advancing the field from qualitative detection to mathematically\ngrounded measurement. Our approach represents MLLM outputs as the spectral\nembeddings over multimodal graph Laplacians and characterizes the manifold gaps\nof truth vs inconsistencies as the semantic distortion, enabling the tight\nRayleigh--Ritz bounds on the multimodal hallucination energy as a functional of\ntime-dependent temperature profiles. By leveraging eigenmode decompositions in\nReproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers\nmodality-aware, theoretically interpretable metrics that capture the evolution\nof hallucinations across time and input prompts through temperature annealing.\nThis work establishes a principled foundation for quantifying and bounding\nhallucinations, transforming them from a qualitative risk to a tractable,\nanalyzable phenomenon.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u548c\u6269\u6563\u52a8\u529b\u5b66\u7684\u4e25\u683c\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c06\u8bc4\u4f30\u4ece\u5b9a\u6027\u68c0\u6d4b\u63d0\u5347\u5230\u6570\u5b66\u5ea6\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u5e7b\u89c9\u8bc4\u4f30\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\uff0c\u7f3a\u4e4f\u539f\u5219\u6027\u91cf\u5316\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u5bfc\u81f4\u65e0\u6cd5\u7406\u89e3\u5e7b\u89c9\u7684\u4ea7\u751f\u3001\u4f20\u64ad\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\uff0c\u5c06MLLM\u8f93\u51fa\u8868\u793a\u4e3a\u591a\u6a21\u6001\u56fe\u62c9\u666e\u62c9\u65af\u8c31\u5d4c\u5165\uff0c\u5e76\u5c06\u771f\u4f2a\u4e0d\u4e00\u81f4\u7684\u6d41\u5f62\u5dee\u8ddd\u7279\u5f81\u5316\u4e3a\u8bed\u4e49\u5931\u771f\u3002\u5229\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6a21\u5f0f\u5206\u89e3\uff0c\u63d0\u4f9b\u4e0e\u6a21\u6001\u76f8\u5173\u7684\u3001\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u7684\u5ea6\u91cf\uff0c\u4ee5\u6355\u6349\u5e7b\u89c9\u968f\u65f6\u95f4\u548c\u6e29\u5ea6\u9000\u706b\u7684\u6f14\u53d8\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u539f\u5219\u6027\u91cf\u5316\u548c\u754c\u5b9a\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u5c06\u5e7b\u89c9\u4ece\u5b9a\u6027\u98ce\u9669\u8f6c\u53d8\u4e3a\u53ef\u5904\u7406\u3001\u53ef\u5206\u6790\u7684\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5316\u548c\u754c\u5b9aMLLM\u4e2d\u7684\u5e7b\u89c9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4ece\u5b9a\u6027\u68c0\u6d4b\u5230\u6570\u5b66\u5ea6\u91cf\u7684\u8f6c\u53d8\u3002"}}
{"id": "2508.19372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19372", "abs": "https://arxiv.org/abs/2508.19372", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6570\u636e\u5e93\u5b9e\u4f53\u8bc6\u522b\uff08DB-ER\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u57fa\u51c6\u3001\u6570\u636e\u589e\u5f3a\u548c\u4e13\u95e8\u7684T5\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff08NLQ\uff09\u4e2d\u7684\u6570\u636e\u5e93\u5b9e\u4f53\u8bc6\u522b\uff08DB-ER\uff09\u6311\u6218\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b860\u4e2aNLQ-SQL\u5bf9\u7684DB-ER\u57fa\u51c6\u3002 2. \u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528SQL\u67e5\u8be2\u81ea\u52a8\u6807\u6ce8NLQ\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002 3. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eT5\u7684DB-ER\u6a21\u578b\uff0c\u5305\u542b\u5e8f\u5217\u6807\u6ce8\u548c\u4ee4\u724c\u5206\u7c7b\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002 4. \u5c06DB-ER\u6a21\u578b\u4e0e\u4e24\u4e2a\u6700\u5148\u8fdb\u7684NER\u6807\u7b7e\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u8fdb\u884c\u4e86\u6d88\u878d\u8bc4\u4f30\u3002", "result": "DB-ER\u6a21\u578b\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u4e24\u4e2a\u6700\u5148\u8fdb\u7684NER\u6807\u7b7e\u5668\u3002\u6570\u636e\u589e\u5f3a\u4f7f\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\uff0cT5\u9aa8\u5e72\u5fae\u8c03\u4f7f\u8fd9\u4e9b\u6307\u6807\u63d0\u9ad8\u4e865-10%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DB-ER\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4e13\u95e8\u7684T5\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u5e93\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19938", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19938", "abs": "https://arxiv.org/abs/2508.19938", "authors": ["Surbhi Slathia", "Manoj Tripathi", "Raphael Benjamim de Oliveira", "Guilherme da Silva Lopes Fabris", "Bruno Ipaves", "Raphael Matozo Tromer", "Marcelo Lopes Pereira Junior", "Gelu Costin", "Preeti Lata Mahapatraa", "Nicholas R. Glavin", "Ajit K. Roy", "Venkataramana Gadhamshetty", "Douglas Soares Galvao", "Alan Dalton", "Chandra Sekhar Tiwary"], "title": "Nanoscale mechanics and ultralow Friction of natural 2D silicates: Biotite and Rhodonite", "comment": null, "summary": "Two-dimensional (2D) silicates have emerged as a promising class of ultrathin\nmaterials, expanding the landscape of 2D systems beyond conventional van der\nWaals crystals. Their unique crystal chemistries and structural anisotropies\nmake them attractive for applications ranging from sensors and flexoelectric\ndevices to drug delivery and catalysis. To unlock their full potential, it is\ncritical to understand their thickness-dependent mechanical properties within\nthe family of 2D silicates. In this study, we investigate the nanomechanical\nand frictional behaviors of two structurally distinct natural silicates:\nlayered Biotite and chain-structured Rhodonite. Using atomic force microscopy\n(AFM), we found that Rhodonite exhibits nearly ten times higher adhesion force\nand modulus response compared to Biotite. Despite this, Biotite demonstrates\nsuperior frictional performance, with ultrathin (5 nm) flakes showing a\nremarkably low coefficient of friction ($\\sim 0.6 \\times 10^{-3}$) versus\nRhodonite ($\\sim 3.6 \\times 10^{-3}$). To further elucidate interlayer\nadhesion, density functional theory (DFT) calculations with Hubbard correction\nwere employed. These findings offer valuable insights into the design and\nselection of 2D silicates for advanced mechanical and tribological\napplications.", "AI": {"tldr": "\u4e8c\u7ef4\u7845\u9178\u76d0\u5728\u673a\u68b0\u548c\u6469\u64e6\u5b66\u65b9\u9762\u5c55\u73b0\u51fa\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5176\u6027\u80fd\u4e0e\u539a\u5ea6\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u4f7f\u7528\u539f\u5b50\u529b\u663e\u5fae\u955c\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff0c\u6bd4\u8f83\u4e86\u4e91\u6bcd\u548c rodhonite \u4e24\u79cd\u4e8c\u7ef4\u7845\u9178\u76d0\u7684\u529b\u5b66\u548c\u6469\u64e6\u5b66\u7279\u6027\uff0c\u53d1\u73b0 rodhonite \u5177\u6709\u66f4\u9ad8\u7684\u9644\u7740\u529b\u548c\u6a21\u91cf\uff0c\u800c\u4e91\u6bcd\u7684\u6469\u64e6\u6027\u80fd\u66f4\u4f18\u8d8a\u3002", "motivation": "\u660e\u786e\u4e8c\u7ef4\u7845\u9178\u76d0\u7684\u539a\u5ea6\u4f9d\u8d56\u6027\u673a\u68b0\u6027\u80fd\u5bf9\u4e8e\u5176\u5728\u4f20\u611f\u5668\u3001\u67d4\u6027\u7535\u5b50\u3001\u836f\u7269\u9012\u9001\u548c\u50ac\u5316\u7b49\u9886\u57df\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08AFM\uff09\u7814\u7a76\u4e86\u5c42\u72b6\u4e91\u6bcd\u548c\u94fe\u72b6 rodhonite \u4e24\u79cd\u4e8c\u7ef4\u7845\u9178\u76d0\u7684\u7eb3\u7c73\u529b\u5b66\u548c\u6469\u64e6\u884c\u4e3a\u3002\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff08\u5305\u542b Hubbard \u4fee\u6b63\uff09\u6765\u6df1\u5165\u63a2\u7a76\u5c42\u95f4\u9644\u7740\u529b\u3002", "result": "\u539f\u5b50\u529b\u663e\u5fae\u955c\u7ed3\u679c\u663e\u793a\uff0crodhonite \u7684\u9644\u7740\u529b\u548c\u6a21\u91cf\u54cd\u5e94\u662f\u4e91\u6bcd\u7684\u8fd1\u5341\u500d\u3002\u7136\u800c\uff0c\u5728\u6469\u64e6\u6027\u80fd\u65b9\u9762\uff0c\u4e91\u6bcd\u8868\u73b0\u66f4\u4f18\uff0c\u5176\u8d85\u8584\uff085 nm\uff09\u8584\u7247\u7684\u6469\u64e6\u7cfb\u6570\uff08\u7ea6 0.6 x 10^-3\uff09\u8fdc\u4f4e\u4e8e rodhonite\uff08\u7ea6 3.6 x 10^-3\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bbe\u8ba1\u548c\u9009\u62e9\u7528\u4e8e\u5148\u8fdb\u673a\u68b0\u548c\u6469\u64e6\u5b66\u5e94\u7528\u7684\u4e8c\u7ef4\u7845\u9178\u76d0\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.19592", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.19592", "abs": "https://arxiv.org/abs/2508.19592", "authors": ["Navinder Singh"], "title": "One Rudolf Peierls' surprise: the quantum-to-classical transition in the context of solid-state physics", "comment": "7 pages, and 5 figures", "summary": "In solid state physics, it is an unsaid (tacit) assumption that the Bloch\ntheorem is applicable to a crystal lattice even if it is of the macroscopic\ndimensions, provided periodicity is maintained. However, in a realistic\nsituation, electrons in a periodic potential of ions constitute an open quantum\nsystem and are subjected to decoherence and dissipation. A natural question\narises: up to what distances electrons in a periodic potential can be\nconsidered as constituting an effective closed quantum system? And what is the\ncause of decoherence? To answer some of these questions, the seminal theory of\nOvchinnikov and Erikhman of decoherence due to ionic motion is revisited and an\noversight of the authors is corrected. Correct conditions for decoherence to\noccur are worked out. Length scale up to which the motion of ions remains\ncoherent is also calculated. Finally, a realistic physical picture is\ndiscussed.", "AI": {"tldr": " Bloch theorem is applicable to macroscopic crystals, but real electrons are open quantum systems subject to decoherence. This paper revisits Ovchinnikov and Erikhman's theory of decoherence due to ionic motion, corrects an oversight, and determines conditions for decoherence and the length scale of coherent ionic motion.", "motivation": "To determine the conditions and length scales for decoherence in electrons within a periodic potential, addressing the applicability of Bloch theorem to macroscopic crystals.", "method": "Revisiting the seminal theory of Ovchinnikov and Erikhman on decoherence due to ionic motion, correcting an oversight, and working out correct conditions for decoherence.", "result": "Identified correct conditions for decoherence to occur and calculated the length scale up to which ionic motion remains coherent.", "conclusion": "Discusses a realistic physical picture of decoherence in solid-state systems, considering the interplay between electron-ion interactions and quantum coherence."}}
{"id": "2508.19314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19314", "abs": "https://arxiv.org/abs/2508.19314", "authors": ["Mahdis Tourian", "Sareh Rowlands", "Remy Vandaele", "Max Fancourt", "Rebecca Mein", "Hywel T. P. Williams"], "title": "Automated classification of natural habitats using ground-level imagery", "comment": "15 pages, 6 figures, 2 tables", "summary": "Accurate classification of terrestrial habitats is critical for biodiversity\nconservation, ecological monitoring, and land-use planning. Several habitat\nclassification schemes are in use, typically based on analysis of satellite\nimagery with validation by field ecologists. Here we present a methodology for\nclassification of habitats based solely on ground-level imagery (photographs),\noffering improved validation and the ability to classify habitats at scale (for\nexample using citizen-science imagery). In collaboration with Natural England,\na public sector organisation responsible for nature conservation in England,\nthis study develops a classification system that applies deep learning to\nground-level habitat photographs, categorising each image into one of 18\nclasses defined by the 'Living England' framework. Images were pre-processed\nusing resizing, normalisation, and augmentation; re-sampling was used to\nbalance classes in the training data and enhance model robustness. We developed\nand fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label\nto each photograph. Using five-fold cross-validation, the model demonstrated\nstrong overall performance across 18 habitat classes, with accuracy and\nF1-scores varying between classes. Across all folds, the model achieved a mean\nF1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and\nPeat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or\nambiguous classes scoring lower. These findings demonstrate the potential of\nthis approach for ecological monitoring. Ground-level imagery is readily\nobtained, and accurate computational methods for habitat classification based\non such data have many potential applications. To support use by practitioners,\nwe also provide a simple web application that classifies uploaded images using\nour model.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4ec5\u57fa\u4e8e\u5730\u9762\u56fe\u50cf\uff08\u7167\u7247\uff09\u7684\u6816\u606f\u5730\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5bf918\u79cd\u201cLiving England\u201d\u751f\u5883\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\u5e94\u7528\u4ee5\u4f9b\u7528\u6237\u4f7f\u7528\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u3001\u751f\u6001\u76d1\u6d4b\u548c\u571f\u5730\u5229\u7528\u89c4\u5212\u7b49\u9886\u57df\u5bf9\u7cbe\u786e\u9646\u5730\u6816\u606f\u5730\u5206\u7c7b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u514b\u670d\u4f20\u7edf\u57fa\u4e8e\u536b\u661f\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u5728\u9a8c\u8bc1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4ec5\u5229\u7528\u5730\u9762\u5c42\u7ea7\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u7cbe\u5ea6\u5e76\u5b9e\u73b0\u5927\u89c4\u6a21\u5206\u7c7b\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f7f\u7528ResNet101\u7684DeepLabV3\u6a21\u578b\u5904\u7406\u5730\u9762\u5c42\u7ea7\u6816\u606f\u5730\u7167\u7247\u3002\u56fe\u50cf\u7ecf\u8fc7\u9884\u5904\u7406\uff08\u8c03\u6574\u5927\u5c0f\u3001\u5f52\u4e00\u5316\u3001\u589e\u5f3a\uff09\uff0c\u5e76\u901a\u8fc7\u91cd\u91c7\u6837\u6765\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u3002\u4f7f\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u572818\u4e2a\u6816\u606f\u5730\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u8be5\u6a21\u578b\u572818\u4e2a\u6816\u606f\u5730\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5e73\u5747F1\u5206\u6570\uff08F1-score\uff09\u4e3a0.61\u3002\u89c6\u89c9\u4e0a\u6613\u4e8e\u533a\u5206\u7684\u7c7b\u522b\uff08\u5982\u88f8\u571f\u3001\u6de4\u6ce5\u548c\u6ce5\u70ad\uff08BSSP\uff09\u3001\u88f8\u6c99\uff08BS\uff09\uff09\u5f97\u5206\u9ad8\u4e8e0.90\uff0c\u800c\u6df7\u5408\u6216\u6a21\u7cca\u7684\u7c7b\u522b\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u57fa\u4e8e\u5730\u9762\u5c42\u7ea7\u56fe\u50cf\u7684\u6816\u606f\u5730\u5206\u7c7b\u65b9\u6cd5\u5177\u6709\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u751f\u6001\u76d1\u6d4b\uff0c\u56e0\u4e3a\u5730\u9762\u56fe\u50cf\u6613\u4e8e\u83b7\u53d6\u4e14\u8ba1\u7b97\u65b9\u6cd5\u51c6\u786e\u3002\u8be5\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\u5e94\u7528\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8be5\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u3002"}}
{"id": "2508.19756", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19756", "abs": "https://arxiv.org/abs/2508.19756", "authors": ["Leontine Aarnoudse", "Mark Haring", "Nathan van de Wouw", "Alexey Pavlov"], "title": "Uncertainty-Based Perturb and Observe for Fast Optimization of Unknown, Time-Varying Processes", "comment": "To appear in Conference on Decision and Control 2025, Rio de Janeiro,\n  Brazil, 2025 6 pages, 3 figures", "summary": "Model-free adaptive optimization methods are capable of optimizing unknown,\ntime-varying processes even when other optimization methods are not. However,\ntheir practical application is often limited by perturbations that are used to\ngather information on the unknown cost and its gradient. The aim of this paper\nis to develop a perturb-and-observe (P&O) method that reduces the need for such\nperturbations while still achieving fast and accurate tracking of time-varying\noptima. To this end, a (time-varying) model of the cost is constructed in an\nonline fashion, taking into account the uncertainty on the measured performance\ncost as well as the decreasing reliability of older measurements. Perturbations\nare only used when this is expected to lead to improved performance over a\ncertain time horizon. Convergence conditions are provided under which the\nstrategy converges to a neighborhood of the optimum. Finally, simulation\nresults demonstrate that uncertainty-based P\\&O can reduce the number of\nperturbations significantly while still tracking a time-varying optimum\naccurately.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6270\u52a8\u4e0e\u89c2\u5bdf\uff08P&O\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u6a21\u578b\u65e0\u5173\u81ea\u9002\u5e94\u4f18\u5316\u4e2d\u7684\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65f6\u53d8\u6700\u4f18\u503c\u7684\u5feb\u901f\u548c\u51c6\u786e\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u65e0\u5173\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\u5728\u4f18\u5316\u672a\u77e5\u3001\u65f6\u53d8\u8fc7\u7a0b\u65b9\u9762\u80fd\u529b\u5f88\u5f3a\uff0c\u4f46\u5176\u5e94\u7528\u5e38\u53d7\u9650\u4e8e\u7528\u4e8e\u6536\u96c6\u4fe1\u606f\uff08\u5173\u4e8e\u6210\u672c\u53ca\u5176\u68af\u5ea6\uff09\u7684\u6270\u52a8\u3002\u672c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6270\u52a8\u4e0e\u89c2\u5bdf\uff08P&O\uff09\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5bf9\u8fd9\u7c7b\u6270\u52a8\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\uff08\u65f6\u53d8\uff09\u6210\u672c\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u5728\u7ebf\u65b9\u5f0f\u6784\u5efa\uff0c\u8003\u8651\u4e86\u6d4b\u91cf\u6027\u80fd\u6210\u672c\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u65e7\u6d4b\u91cf\u503c\u53ef\u9760\u6027\u7684\u4e0b\u964d\u3002\u53ea\u6709\u5f53\u9884\u671f\u5728\u4e00\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\u65f6\uff0c\u624d\u4f7f\u7528\u6270\u52a8\u3002\u8bba\u6587\u63d0\u4f9b\u4e86\u6536\u655b\u6761\u4ef6\uff0c\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\uff0c\u8be5\u7b56\u7565\u6536\u655b\u5230\u6700\u4f18\u503c\u9644\u8fd1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684P&O\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6270\u52a8\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4ecd\u7136\u80fd\u591f\u51c6\u786e\u8ddf\u8e2a\u65f6\u53d8\u6700\u4f18\u503c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684P&O\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6a21\u578b\u65e0\u5173\u81ea\u9002\u5e94\u4f18\u5316\u4e2d\u7684\u6270\u52a8\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65f6\u53d8\u6700\u4f18\u503c\u7684\u51c6\u786e\u8ddf\u8e2a\u3002"}}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInquireMobile\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u9ad8\u79fb\u52a8\u4ee3\u7406\u4e0e\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u65f6\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u4e3b\u52a8\u5411\u7528\u6237\u5bfb\u6c42\u786e\u8ba4\u6765\u907f\u514d\u6f5c\u5728\u98ce\u9669\u3002\u8be5\u7cfb\u7edf\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u5168\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u5728\u6a21\u578b\u7406\u89e3\u6216\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u786e\u8ba4\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInquireMobile\u7684\u65b0\u578b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53d7\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\uff0c\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u4ea4\u4e92\u5f0f\u9884\u52a8\u4f5c\u63a8\u7406\u673a\u5236\uff0c\u4ee5\u5728\u5173\u952e\u51b3\u7b56\u70b9\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u786e\u8ba4\u3002", "result": "InquireMobile\u6a21\u578b\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u8be2\u95ee\u6210\u529f\u7387\u65b9\u9762\u63d0\u9ad8\u4e8646.8%\uff0c\u5e76\u4e14\u5728\u6574\u4f53\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "InquireMobile\u6a21\u578b\u901a\u8fc7\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u786e\u8ba4\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u79fb\u52a8\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002\u4f5c\u8005\u5c06\u5f00\u6e90\u76f8\u5173\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4ee3\u7801\u3002"}}
{"id": "2508.19376", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "LLM\u5728\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u7528\u4e8e\u5206\u7c7b\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\uff0c\u6027\u80fd\u4e0eCNN\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u57fa\u4e8eLLaMA 3.2\u7684\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5bf9\u9ad8\u80fd\u7269\u7406\uff08HEP\uff09\u5b9e\u9a8c\u4e2d\u7684\u50cf\u7d20\u5316\u63a2\u6d4b\u5668\u56fe\u50cf\u8fdb\u884c\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u5206\u7c7b\u3002", "method": "\u5c06VLM\u4e0e\u73b0\u6709\u7684CNN\u57fa\u7ebf\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5206\u7c7b\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cAUC-ROC\u7b49\u6307\u6807\u3002", "result": "VLM\u7684\u6027\u80fd\u4e0eCNN\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u80fd\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u548c\u66f4\u597d\u7684\u8f85\u52a9\u6587\u672c\u6216\u8bed\u4e49\u4e0a\u4e0b\u6587\u96c6\u6210\u3002", "conclusion": "VLM\u4e3aHEP\u4e8b\u4ef6\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u901a\u7528\u9aa8\u5e72\uff0c\u4e3a\u5b9e\u9a8c\u4e2d\u5fae\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.19402", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19402", "abs": "https://arxiv.org/abs/2508.19402", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "Humor research faces fragmentation; LLMs can transfer skills to new humor types with diverse training, improving generalization.", "motivation": "To investigate if competence in specific humor tasks transfers to novel humor types, addressing the fragmentation in computational humor research and the need for LLMs to adapt to evolving online humor.", "method": "Conducting transfer learning experiments with LLMs across four diverse humor datasets, varying training diversity (1-3 datasets) and testing on unseen tasks. Analyzing the impact of diverse training on transferability and in-domain performance, and examining relationships between humor types.", "result": "LLMs demonstrated transferability to unseen humor types, achieving up to 75% accuracy. Training on diverse sources improved transferability by 1.88-4.05% with minimal impact on in-domain performance. Dad Jokes were found to be a strong enabler of transfer but difficult to transfer to.", "conclusion": "LLMs can generalize across humor types through transfer learning, and diverse training enhances this ability without significantly compromising in-domain performance. This suggests that the fragmentation in humor understanding is not entirely inevitable and points to Dad Jokes as a key, albeit complex, factor in humor transfer."}}
{"id": "2508.19969", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.19969", "abs": "https://arxiv.org/abs/2508.19969", "authors": ["Nayana Devaraj", "Anumita Bose", "Arindom Das", "Md Afsar Reja", "Arijit Mandal", "Awadhesh Narayan", "B. R. K. Nanda"], "title": "Unlocking Doping Effects on Altermagnetism in MnTe", "comment": "13 pages, 8 figures, and 1 table", "summary": "Governed by specific symmetries, altermagnetism is an emerging field in\ncondensed matter physics, characterized by unique spin-splitting of the bands\nin the momentum space co-existing with the compensated magnetization as in\nantiferromagnets. As crystals can have tailored and unintended defects, it is\nimportant to gain insights on how altermagnets are affected by the\ndefects-driven symmetry-breaking which, in turn, can build promising\nperspectives on potential applications. In this study, considering the widely\ninvestigated MnTe as a prototype altermagnet, defects are introduced through\nsubstitutional doping to create a large configuration space of spin space\ngroups. With the aid of density functional theory calculations, symmetry\nanalysis, and model studies in this configuration space, we demonstrate the\ngeneric presence of spin-split of the antiferromagnetic bands in the momentum\nspace. This is indicative of a wider class of quasi-altermagnetic materials,\naugmenting the set of ideal altermagnetic systems. Furthermore, we show that\nwhile pristine MnTe does not show anomalous Hall conductivity (AHC) with\nout-of-plane magnetization, suitable doping can be carried out to obtain finite\nand varied AHC. Our predictions of quasi-altermagnetism and doping driven\ntailored AHC have the potential to open up as-yet-unexplored directions in this\ndeveloping field.", "AI": {"tldr": "New research on altermagnetism in MnTe, exploring defect impacts and potential applications like tailored anomalous Hall conductivity.", "motivation": "Investigate the effects of defects on altermagnetism in MnTe, focusing on symmetry breaking and its influence on spin-split bands and anomalous Hall conductivity (AHC).", "method": "Utilized density functional theory calculations, symmetry analysis, and model studies on a configuration space of MnTe with substitutional doping to create various spin space groups.", "result": "Demonstrated the generic presence of spin-split antiferromagnetic bands in momentum space for MnTe with defects, indicating a broader class of quasi-altermagnetic materials. Showed that doping MnTe can induce finite and varied AHC, unlike the pristine material.", "conclusion": "The study predicts the existence of quasi-altermagnetism and highlights the potential for doping to control AHC in MnTe, opening new avenues for research and applications in the field of altermagnetism."}}
{"id": "2508.19320", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64$\\times$ reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "\u8fd1\u671f\uff0c\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u89c6\u9891\u751f\u6210\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u9762\u4e34\u9ad8\u5ef6\u8fdf\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u53ef\u63a7\u6027\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u591a\u6a21\u6001\u63a7\u5236\u548c\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u5916\u63a8\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u6807\u51c6\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5c11\u91cf\u4fee\u6539\uff0c\u5373\u53ef\u63a5\u53d7\u97f3\u9891\u3001\u59ff\u6001\u548c\u6587\u672c\u7b49\u591a\u79cd\u6a21\u6001\u7684\u6761\u4ef6\u7f16\u7801\uff0c\u5e76\u8f93\u51fa\u4e0e\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u8868\u793a\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u4e3a\u652f\u6301\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea620,000\u5c0f\u65f6\u7684\u591a\u6e90\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u8fbe64\u500d\u964d\u6bd4\u7684\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u957f\u5e8f\u5217\u63a8\u7406\u8d1f\u62c5\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u53ef\u63a7\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u5b9e\u65f6\u4ea4\u4e92\u3001\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u9ad8\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u63a5\u53d7\u97f3\u9891\u3001\u59ff\u6001\u548c\u6587\u672c\u7b49\u591a\u79cd\u6a21\u6001\u7684\u6761\u4ef6\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u6307\u5bfc\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u6765\u751f\u6210\u89c6\u9891\u3002\u5f15\u5165\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\u4ee5\u964d\u4f4e\u6a21\u578b\u63a8\u7406\u8d1f\u62c5\u3002", "result": "\u5728\u53cc\u5411\u5bf9\u8bdd\u3001\u591a\u8bed\u79cd\u4eba\u5408\u6210\u548c\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u65b9\u9762\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\uff0c\u5728\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u89c6\u9891\u751f\u6210\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.19760", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19760", "abs": "https://arxiv.org/abs/2508.19760", "authors": ["Thilanka Thilakasiri", "Matthias Becker"], "title": "Limited Preemption of the 3-Phase Task Model using Preemption Thresholds", "comment": null, "summary": "Phased execution models are a well-known solution to tackle the\nunpredictability of today's complex COTS multi-core platforms. The semantics of\nthese models dedicate phases for a task's execution and shared memory accesses.\nMemory phases are solely dedicated to load all necessary instructions and data\nto private local memory, and to write back the results of the computation.\nDuring execution phases, only the private local memory is accessed. While\nnon-preemptive execution phases utilize the local memory well, schedulability\nis reduced due to blocking. On the other hand, fully preemptive execution\nphases allow for better schedulability, but require local memory to be large\nenough to hold all tasks involved in preemption simultaneously. Limited\npreemption is a promising approach that provides moderation between\nnon-preemptive and fully preemptive scheduling.\n  In this paper, we propose using preemption thresholds to limit the number of\npreemptions to minimize local memory usage while maintaining schedulability. We\npropose a worst-case response time and a worst-case memory requirement analysis\nfor sporadic 3-phase tasks under partitioned fixed-priority scheduling with\npreemption thresholds. We further show how the state-of-the-art algorithm to\nassign preemption thresholds can be applied to the considered task model.\nEvaluations demonstrate that preemption thresholds can significantly reduce the\nmemory usage (by $2.5\\times$) compared to fully preemptive scheduling, while\nmaintaining high schedulability ratios ($13\\times$) compared to non-preemptive\nscheduling.", "AI": {"tldr": "Phased execution models improve COTS multi-core platform predictability. Limited preemption with preemption thresholds balances local memory usage and schedulability, outperforming fully preemptive and non-preemptive approaches in evaluations.", "motivation": "To address the trade-offs between schedulability and local memory usage in phased execution models for COTS multi-core platforms, specifically by introducing a limited preemption approach.", "method": "Propose using preemption thresholds to limit preemptions, minimizing local memory usage while maintaining schedulability. Develop a worst-case response time and memory requirement analysis for sporadic 3-phase tasks under partitioned fixed-priority scheduling with preemption thresholds. Adapt a state-of-the-art algorithm for assigning preemption thresholds.", "result": "Preemption thresholds reduce memory usage by 2.5x compared to fully preemptive scheduling and maintain 13x higher schedulability ratios compared to non-preemptive scheduling.", "conclusion": "Preemption thresholds offer a promising approach to effectively manage local memory usage and maintain high schedulability in phased execution models on COTS multi-core systems."}}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "Chain-of-Thought (CoT)\u5728\u5206\u6790\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u4e14\u53ef\u80fd\u4e0d\u5fe0\u5b9e\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9CoT\u7684\u4f9d\u8d56\u3001\u5f71\u54cd\u548c\u5fe0\u5b9e\u5ea6\uff0c\u53d1\u73b0\u5b83\u4eec\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u4e14\u4e0d\u4e00\u5b9a\u4e00\u81f4\u3002", "motivation": "\u63a2\u7a76Chain-of-Thought\uff08CoT\uff09\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u548c\u5fe0\u5b9e\u6027\uff0c\u4ee5\u53ca\u5176\u4e0e\u4e0d\u540c\u7c7b\u578b\u6a21\u578b\uff08\u6307\u4ee4\u8c03\u6574\u3001\u63a8\u7406\u548c\u63a8\u7406\u84b8\u998f\u6a21\u578b\uff09\u7684\u5173\u7cfb\uff0c\u5e76\u89e3\u51b3CoT\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u6536\u76ca\u6709\u9650\u548c\u53ef\u80fd\u4e0d\u5fe0\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u5206\u6790\u548c\u6bd4\u8f83\u6307\u4ee4\u8c03\u6574\u3001\u63a8\u7406\u548c\u63a8\u7406\u84b8\u998f\u6a21\u578b\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9CoT\u7684\u4f9d\u8d56\u3001\u5f71\u54cd\u548c\u5fe0\u5b9e\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bf9CoT\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4e0d\u540c\uff0c\u5e76\u4e14CoT\u7684\u5f71\u54cd\u529b\u548c\u5fe0\u5b9e\u5ea6\u5e76\u4e0d\u603b\u662f\u4e00\u81f4\u7684\u3002", "conclusion": "CoT\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53d7\u6a21\u578b\u7c7b\u578b\u5f71\u54cd\uff0c\u4e14\u5176\u5f71\u54cd\u529b\u548c\u5fe0\u5b9e\u5ea6\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.19731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19731", "abs": "https://arxiv.org/abs/2508.19731", "authors": ["Maryam Kazemi Eskeri", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments", "comment": "7 Pages, 4 Figures, Accepted in IROS2025", "summary": "Multi-robot systems are increasingly deployed in applications, such as\nintralogistics or autonomous delivery, where multiple robots collaborate to\ncomplete tasks efficiently. One of the key factors enabling their efficient\ncooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this\nproblem optimize task distribution among robots to minimize the overall\nexecution time. In shared environments, apart from the relative distance\nbetween the robots and the tasks, the execution time is also significantly\nimpacted by the delay caused by navigating around moving people. However, most\nexisting MRTA approaches are dynamics-agnostic, relying on static maps and\nneglecting human motion patterns, leading to inefficiencies and delays. In this\npaper, we introduce \\acrfull{method name}. This method leverages Maps of\nDynamics (MoDs), spatio-temporal queryable models designed to capture\nhistorical human movement patterns, to estimate the impact of humans on the\ntask execution time during deployment. \\acrshort{method name} utilizes a\nstochastic cost function that includes MoDs. Experimental results show that\nintegrating MoDs enhances task allocation performance, resulting in reduced\nmission completion times by up to $26\\%$ compared to the dynamics-agnostic\nmethod and up to $19\\%$ compared to the baseline. This work underscores the\nimportance of considering human dynamics in MRTA within shared environments and\npresents an efficient framework for deploying multi-robot systems in\nenvironments populated by humans.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'method name'\u7684\u65b0\u578b\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\uff08MRTA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u4eba\u7c7b\u52a8\u6001\u79fb\u52a8\u5bf9\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5229\u7528'Maps of Dynamics\uff08MoDs\uff09'\u6a21\u578b\u6765\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u53ef\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u7f29\u77ed26%\u3002", "motivation": "\u73b0\u6709MRTA\u65b9\u6cd5\u5ffd\u7565\u4e86\u4eba\u7c7b\u79fb\u52a8\u5bf9\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6709\u4eba\u7c7b\u5171\u4eab\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'method name'\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528'Maps of Dynamics\uff08MoDs\uff09'\uff08\u4e00\u79cd\u65f6\u7a7a\u53ef\u67e5\u8be2\u6a21\u578b\uff09\u6765\u6355\u6349\u5386\u53f2\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u4e00\u4e2a\u5305\u542bMoDs\u7684\u968f\u673a\u6210\u672c\u51fd\u6570\u4e2d\uff0c\u4ee5\u4f30\u8ba1\u4eba\u7c7b\u5bf9\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5ffd\u7565\u52a8\u6001\u56e0\u7d20\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u96c6\u6210MoDs\u7684\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u914d\u6027\u80fd\u63d0\u9ad8\u4e8626%\uff1b\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5219\u63d0\u9ad8\u4e8619%\u3002", "conclusion": "\u5728\u6709\u4eba\u7c7b\u5b58\u5728\u7684\u5171\u4eab\u73af\u5883\u4e2d\uff0c\u8003\u8651\u4eba\u7c7b\u52a8\u6001\u56e0\u7d20\u5bf9\u4e8eMRTA\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u6709\u4eba\u7c7b\u6d3b\u52a8\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.19381", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19381", "abs": "https://arxiv.org/abs/2508.19381", "authors": ["Jesus Lopez", "Saeefa Rubaiyet Nowmi", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Towards Quantum Machine Learning for Malicious Code Analysis", "comment": "6 pages, 3 figures, 2 tables. Accepted at the International Workshop\n  on Quantum Computing and Reinforcement Learning (QCRL) @ IEEE Quantum Week\n  2025", "summary": "Classical machine learning (CML) has been extensively studied for malware\nclassification. With the emergence of quantum computing, quantum machine\nlearning (QML) presents a paradigm-shifting opportunity to improve malware\ndetection, though its application in this domain remains largely unexplored. In\nthis study, we investigate two hybrid quantum-classical models -- a Quantum\nMultilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),\nfor malware classification. Both models utilize angle embedding to encode\nmalware features into quantum states. QMLP captures complex patterns through\nfull qubit measurement and data re-uploading, while QCNN achieves faster\ntraining via quantum convolution and pooling layers that reduce active qubits.\nWe evaluate both models on five widely used malware datasets -- API-Graph,\nEMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and\nmulticlass classification tasks.\n  Our results show high accuracy for binary classification -- 95-96% on\nAPI-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass\nsettings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,\nand 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex\nmulticlass tasks, while QCNN offers improved training efficiency at the cost of\nreduced accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff1a\u91cf\u5b50\u591a\u5c42\u611f\u77e5\u5668\uff08QMLP\uff09\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u3002\u7814\u7a76\u53d1\u73b0\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cQMLP\u548cQCNN\u5747\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\uff08\u9ad8\u8fbe96%\uff09\uff0c\u800c\u5728\u590d\u6742\u7684\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cQMLP\u7684\u6027\u80fd\u4f18\u4e8eQCNN\uff0c\u4f46QCNN\u7684\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u5174\u8d77\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u4e3a\u6539\u8fdb\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5176\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u5c1a\u5904\u4e8e\u63a2\u7d22\u9636\u6bb5\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30QML\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff1a\u91cf\u5b50\u591a\u5c42\u611f\u77e5\u5668\uff08QMLP\uff09\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u3002\u8fd9\u4e24\u79cd\u6a21\u578b\u90fd\u5229\u7528\u89d2\u5ea6\u5d4c\u5165\u5c06\u6076\u610f\u8f6f\u4ef6\u7279\u5f81\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\u3002QMLP\u901a\u8fc7\u5168\u91cf\u5b50\u6bd4\u7279\u6d4b\u91cf\u548c\u6570\u636e\u91cd\u4f20\u6765\u6355\u6349\u590d\u6742\u6a21\u5f0f\uff0c\u800cQCNN\u901a\u8fc7\u91cf\u5b50\u5377\u79ef\u548c\u6c60\u5316\u5c42\u6765\u51cf\u5c11\u6d3b\u8dc3\u91cf\u5b50\u6bd4\u7279\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5feb\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5728API-Graph\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u8fbe\u523095-96%\uff0c\u5728AZ-Domain\u6570\u636e\u96c6\u4e0a\u4e3a91-92%\uff0c\u5728EMBER-Domain\u6570\u636e\u96c6\u4e0a\u4e3a77%\u3002\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cAPI-Graph\u6570\u636e\u96c6\u7684\u51c6\u786e\u7387\u4e3a91.6-95.7%\uff0cAZ-Class\u6570\u636e\u96c6\u4e3a41.7-93.6%\uff0cEMBER-Class\u6570\u636e\u96c6\u4e3a60.7-88.1%\u3002", "conclusion": "QMLP\u5728\u590d\u6742\u7684\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eQCNN\uff0c\u800cQCNN\u5728\u4ee5\u727a\u7272\u90e8\u5206\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2508.19427", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "The article discusses the potential for humans to lose writing skills due to the rise of AI text generation, drawing parallels with historical instances like the Greek Dark Ages.", "motivation": "To explore the potential long-term consequences of advanced AI text generation on human writing abilities and historical precedents for skill degradation.", "method": "Discusses the possibility of skill loss by drawing parallels with historical events, specifically the Greek Dark Ages, where writing ability may have declined.", "result": "The article posits a future where human writing proficiency may be significantly reduced due to reliance on AI, similar to potential skill loss in past historical periods.", "conclusion": "The increasing reliance on AI for text generation raises concerns about the potential atrophy of human writing skills, a phenomenon with historical parallels."}}
{"id": "2508.19822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19822", "abs": "https://arxiv.org/abs/2508.19822", "authors": ["Chunxuan Shi", "Yongzhe Li", "Ran Tao"], "title": "On Minimization/Maximization of the Generalized Multi-Order Complex Quadratic Form With Constant-Modulus Constraints", "comment": "14 pages, 3 figures (16 subfigures)", "summary": "In this paper, we study the generalized problem that minimizes or maximizes a\nmulti-order complex quadratic form with constant-modulus constraints on all\nelements of its optimization variable. Such a mathematical problem is commonly\nencountered in various applications of signal processing. We term it as the\nconstant-modulus multi-order complex quadratic programming (CMCQP) in this\npaper. In general, the CMCQP is non-convex and difficult to solve. Its\nobjective function typically relates to metrics such as signal-to-noise ratio,\nCram\\'er-Rao bound, integrated sidelobe level, etc., and constraints normally\ncorrespond to requirements on similarity to desired aspects,\npeak-to-average-power ratio, or constant-modulus property in practical\nscenarios. In order to find efficient solutions to the CMCQP, we first\nreformulate it into an unconstrained optimization problem with respect to phase\nvalues of the studied variable only. Then, we devise a steepest descent/ascent\nmethod with fast determinations on its optimal step sizes. Specifically, we\nconvert the step-size searching problem into a polynomial form that leads to\nclosed-form solutions of high accuracy, wherein the third-order Taylor\nexpansion of the search function is conducted. Our major contributions also lie\nin investigating the effect of the order and specific form of matrices embedded\nin the CMCQP, for which two representative cases are identified. Examples of\nrelated applications associated with the two cases are also provided for\ncompleteness. The proposed methods are summarized into algorithms, whose\nconvergence speeds are verified to be fast by comprehensive simulations and\ncomparisons to existing methods. The accuracy of our proposed fast step-size\ndetermination is also evaluated.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u590d\u6570\u57df\u4e0a\uff0c\u5bf9\u4e00\u4e2a\u591a\u9636\u590d\u6570\u4e8c\u6b21\u578b\u8fdb\u884c\u5e38\u6a21\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff08CMCQP\uff09\uff0c\u8be5\u95ee\u9898\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u901a\u5e38\u662f\u975e\u51f8\u4e14\u96be\u89e3\u7684\u3002", "motivation": "\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u5982\u4fe1\u566a\u6bd4\u3001Cram\u00e9r-Rao\u754c\u3001\u65c1\u74e3\u7535\u5e73\u7b49\u6307\u6807\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u76f8\u4f3c\u6027\u3001\u5cf0\u5747\u529f\u7387\u6bd4\u3001\u5e38\u6a21\u7ea6\u675f\u7b49\u9700\u6c42\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u4ec5\u5173\u4e8e\u76f8\u4f4d\u503c\u7684\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9661\u5ea6\u4e0b\u964d/\u4e0a\u5347\u6cd5\uff0c\u901a\u8fc7\u5bf9\u641c\u7d22\u51fd\u6570\u8fdb\u884c\u4e09\u9636\u6cf0\u52d2\u5c55\u5f00\uff0c\u5c06\u6b65\u957f\u641c\u7d22\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u9879\u5f0f\u95ee\u9898\uff0c\u4ece\u800c\u5f97\u5230\u9ad8\u7cbe\u5ea6\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cdCMCQP\u7684\u4ee3\u8868\u6027\u60c5\u51b5\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u5e94\u7528\u793a\u4f8b\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u5feb\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5feb\u901f\u6b65\u957f\u786e\u5b9a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06CMCQP\u95ee\u9898\u8f6c\u5316\u4e3a\u76f8\u4f4d\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u9ad8\u6548\u7684\u6b65\u957f\u786e\u5b9a\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u6b64\u7c7b\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.19605", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19605", "abs": "https://arxiv.org/abs/2508.19605", "authors": ["Zhong-Wen Ou", "Tian-Xiang Zhu", "Peng-Jun Liang", "Xiao-Min Hu", "Zong-Quan Zhou", "Chuang-Feng Li", "Guang-Can Guo"], "title": "Multichannel and high dimensional integrated photonic quantum memory", "comment": null, "summary": "Integrated photonic quantum memories are essential components for scalable\nquantum networks and photonic information processors. However, prior\nimplementations have been confined to single-channel operation, limiting their\ncapacity to manipulate multiple photonic pulses and support high-dimensional\ninformation. In this work, we introduce an 11-channel integrated quantum memory\nbased on laser-written waveguide arrays in $^{151}$Eu$^{3+}$:Y$_2$SiO$_5$\ncrystals. On-chip electrode arrays enable independent control over the readout\ntimes for each channel via Stark-shift-induced atomic interference. Our device\nachieves random-access quantum storage of three time-bin qubits with a fidelity\nexceeding 99%, as well as storage of five-dimensional path-encoded quantum\nstates with a fidelity above 96%. This multichannel integrated storage device\nenables versatile applications through its random access capability and lays a\nsolid foundation for the development of high-dimensional quantum networks in\nintegrated architectures.", "AI": {"tldr": "\u96c6\u6210\u4e86\u57fa\u4e8e\u6fc0\u5149\u5199\u5165\u6ce2\u5bfc\u9635\u5217\u768411\u901a\u9053\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u53ef\u72ec\u7acb\u63a7\u5236\u8bfb\u51fa\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u901a\u9053\u3001\u9ad8\u7ef4\u91cf\u5b50\u6001\u5b58\u50a8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5149\u5b50\u91cf\u5b50\u5b58\u50a8\u5668\u4ec5\u9650\u4e8e\u5355\u901a\u9053\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u5904\u7406\u591a\u5149\u5b50\u548c\u652f\u6301\u9ad8\u7ef4\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u5728$^{151}$Eu$^{3+}$:Y$_2$SiO$_5$\u6676\u4f53\u4e2d\uff0c\u5229\u7528\u6fc0\u5149\u5199\u5165\u6ce2\u5bfc\u9635\u5217\u6784\u5efa11\u901a\u9053\u96c6\u6210\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u5e76\u901a\u8fc7\u7247\u4e0a\u7535\u6781\u9635\u5217\u5b9e\u73b0\u5bf9\u6bcf\u4e2a\u901a\u9053\u8bfb\u51fa\u65f6\u95f4\u7684\u72ec\u7acb\u63a7\u5236\uff08\u57fa\u4e8e\u65af\u5854\u514b\u9891\u79fb\u8bf1\u5bfc\u539f\u5b50\u5e72\u6d89\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff08\u8d85\u8fc799%\uff09\u7684\u4e09\u65f6\u95f4\u4ed3\u91cf\u5b50\u6bd4\u7279\u968f\u673a\u8bbf\u95ee\u91cf\u5b50\u5b58\u50a8\uff0c\u4ee5\u53ca\u9ad8\u4fdd\u771f\u5ea6\uff08\u9ad8\u4e8e96%\uff09\u7684\u4e94\u7ef4\u8def\u5f84\u7f16\u7801\u91cf\u5b50\u6001\u5b58\u50a8\u3002", "conclusion": "\u8be5\u591a\u901a\u9053\u96c6\u6210\u91cf\u5b50\u5b58\u50a8\u8bbe\u5907\u901a\u8fc7\u5176\u968f\u673a\u8bbf\u95ee\u80fd\u529b\u5b9e\u73b0\u4e86\u901a\u7528\u5e94\u7528\uff0c\u5e76\u4e3a\u96c6\u6210\u67b6\u6784\u4e2d\u9ad8\u7ef4\u91cf\u5b50\u7f51\u7edc\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.19324", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.19324", "abs": "https://arxiv.org/abs/2508.19324", "authors": ["Jefferson David Rodriguez Chivata", "Davide Ghiani", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Deep Data Hiding for ICAO-Compliant Face Images: A Survey", "comment": "In 2025 IEEE International Joint Conference on Biometrics (IJCB)", "summary": "ICAO-compliant facial images, initially designed for secure biometric\npassports, are increasingly becoming central to identity verification in a wide\nrange of application contexts, including border control, digital travel\ncredentials, and financial services. While their standardization enables global\ninteroperability, it also facilitates practices such as morphing and deepfakes,\nwhich can be exploited for harmful purposes like identity theft and illegal\nsharing of identity documents. Traditional countermeasures like Presentation\nAttack Detection (PAD) are limited to real-time capture and offer no\npost-capture protection. This survey paper investigates digital watermarking\nand steganography as complementary solutions that embed tamper-evident signals\ndirectly into the image, enabling persistent verification without compromising\nICAO compliance. We provide the first comprehensive analysis of\nstate-of-the-art techniques to evaluate the potential and drawbacks of the\nunderlying approaches concerning the applications involving ICAO-compliant\nimages and their suitability under standard constraints. We highlight key\ntrade-offs, offering guidance for secure deployment in real-world identity\nsystems.", "AI": {"tldr": "ICAO\u9762\u90e8\u56fe\u50cf\u9762\u4e34\u7167\u7247\u653b\u51fb\u98ce\u9669\uff0c\u53ef\u5229\u7528\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u8fdb\u884c\u4fdd\u62a4\u3002", "motivation": "ICAO\u9762\u90e8\u56fe\u50cf\u5728\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u6613\u53d7\u7167\u7247\u653b\u51fb\uff08\u5982\u53d8\u5f62\u548c\u6df1\u5ea6\u4f2a\u9020\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u540e\u9a8c\u4fdd\u62a4\u3002", "method": "\u7814\u7a76\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u4f5c\u4e3a\u96c6\u6210\u5230\u56fe\u50cf\u4e2d\u7684\u9632\u7be1\u6539\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0\u6301\u7eed\u9a8c\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301ICAO\u5408\u89c4\u6027\u3002", "result": "\u5bf9\u73b0\u6709\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5176\u5728ICAO\u56fe\u50cf\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u6807\u51c6\u9650\u5236\u4e0b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u662f\u4fdd\u62a4ICAO\u9762\u90e8\u56fe\u50cf\u514d\u53d7\u7167\u7247\u653b\u51fb\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u6301\u7eed\u9a8c\u8bc1\uff0c\u5e76\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.19933", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19933", "abs": "https://arxiv.org/abs/2508.19933", "authors": ["Sten Elling Tingstad Jacobsen", "Bal\u00e1zs Kulcs\u00e1r", "Anders Lindman"], "title": "Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition", "comment": "29 pages, 12 figures", "summary": "The electrification and automation of mobility are reshaping how cities\noperate on-demand transport systems. Managing Electric Autonomous\nMobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch,\nrebalancing, and charging decisions under multiple uncertainties, including\ntravel demand, travel time, energy consumption, and charger availability. We\naddress this challenge with a combined stochastic and robust model predictive\ncontrol (MPC) framework. The framework integrates spatio-temporal Bayesian\nneural network forecasts with a multi-stage stochastic optimization model,\nformulated as a large-scale mixed-integer linear program. To ensure real-time\napplicability, we develop a tailored Nested Benders Decomposition that exploits\nthe scenario tree structure and enables efficient parallelized solution.\nStochastic optimization is employed to anticipate demand and infrastructure\nvariability, while robust constraints on energy consumption and travel times\nsafeguard feasibility under worst-case realizations. We evaluate the framework\nusing high-fidelity simulations of San Francisco and Chicago. Compared with\ndeterministic, reactive, and robust baselines, the combined stochastic and\nrobust approach reduces median passenger waiting times by up to 36% and\n95th-percentile delays by nearly 20%, while also lowering rebalancing distance\nby 27% and electricity costs by more than 35%. We also conduct a sensitivity\nanalysis of battery size and vehicle efficiency, finding that energy-efficient\nvehicles maintain stable performance even with small batteries, whereas less\nefficient vehicles require larger batteries and greater infrastructure support.\nOur results emphasize the importance of jointly optimizing predictive control,\nvehicle capabilities, and infrastructure planning to enable scalable,\ncost-efficient EAMoD operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7ba1\u7406\u7535\u52a8\u81ea\u52a8\u9a7e\u9a76\u51fa\u884c on-demand (EAMoD) \u8f66\u961f\uff0c\u4ee5\u5e94\u5bf9\u9700\u6c42\u3001\u65f6\u95f4\u548c\u5145\u7535\u7b49\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u65f6\u7a7a\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u548c\u591a\u9636\u6bb5\u968f\u673a\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5d4c\u5957Benders\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u7535\u52a8\u81ea\u52a8\u9a7e\u9a76\u51fa\u884c on-demand (EAMoD) \u7cfb\u7edf\u7684\u5174\u8d77\u5bf9\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u6c42\u3001\u65f6\u95f4\u548c\u5145\u7535\u7b49\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8c03\u5ea6\u3001\u5e73\u8861\u548c\u5145\u7535\u8f66\u961f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u9c81\u68d2MPC\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u65f6\u7a7a\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u548c\u591a\u9636\u6bb5\u968f\u673a\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u5d4c\u5957Benders\u5206\u89e3\u8fdb\u884c\u6c42\u89e3\u3002\u968f\u673a\u4f18\u5316\u7528\u4e8e\u9884\u6d4b\u9700\u6c42\u548c\u57fa\u7840\u8bbe\u65bd\u53d8\u5316\uff0c\u9c81\u68d2\u7ea6\u675f\u7528\u4e8e\u4fdd\u8bc1\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u53ef\u884c\u6027\u3002", "result": "\u4e0e\u786e\u5b9a\u6027\u3001\u53cd\u5e94\u5f0f\u548c\u9c81\u68d2\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u53ef\u5c06\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u4e2d\u4f4d\u6570\u51cf\u5c11\u9ad8\u8fbe36%\uff0c\u5c0695%\u5206\u4f4d\u6570\u7684\u5ef6\u8fdf\u51cf\u5c11\u8fd120%\uff0c\u540c\u65f6\u964d\u4f4e27%\u7684\u8c03\u5ea6\u8ddd\u79bb\u548c\u8d85\u8fc735%\u7684\u7535\u529b\u6210\u672c\u3002\u654f\u611f\u6027\u5206\u6790\u8868\u660e\uff0c\u9ad8\u80fd\u6548\u8f66\u8f86\u5728\u5c0f\u7535\u6c60\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u9884\u6d4b\u63a7\u5236\u3001\u8f66\u8f86\u80fd\u529b\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684EAMoD\u8fd0\u8425\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.", "AI": {"tldr": "LLMs may implicitly learn world models, but probing them requires model-specific activations. This paper proposes a model-agnostic framework using chess to evaluate LLMs' world model fidelity by analyzing legal move distributions. The results show LLMs struggle with state-tracking over long sequences, indicating limitations in maintaining coherent internal models. This framework offers a robust tool for evaluating structured reasoning in LLMs without internal model access and generalizes to other symbolic environments.", "motivation": "To assess whether Large Language Models (LLMs) preserve the semantics of structured environments, specifically testing their ability to maintain high-fidelity world models, without relying on model-specific internal activations which limit interpretability and generalizability.", "method": "A model-agnostic, state-based evaluation framework using chess as a benchmark. It analyzes downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states, aligning with the strategic and rule-governed nature of chess.", "result": "The experimental results demonstrate that the proposed metrics capture deficiencies in LLMs' state-tracking, highlighting their limitations in maintaining coherent internal models over long sequences.", "conclusion": "The proposed framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access. It offers a more meaningful evaluation than conventional string-based metrics by aligning closely with the strategic and rule-governed nature of chess and generalizes to a wide class of symbolic environments."}}
{"id": "2508.19771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19771", "abs": "https://arxiv.org/abs/2508.19771", "authors": ["Liding Zhang", "Zhenshan Bing", "Yu Zhang", "Kuanqi Cai", "Lingyun Chen", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles", "comment": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "Path planning has long been an important and active research area in\nrobotics. To address challenges in high-dimensional motion planning, this study\nintroduces the Force Direction Informed Trees (FDIT*), a sampling-based planner\ndesigned to enhance speed and cost-effectiveness in pathfinding. FDIT* builds\nupon the state-of-the-art informed sampling planner, the Effort Informed Trees\n(EIT*), by capitalizing on often-overlooked information in invalid vertices. It\nincorporates principles of physical force, particularly Coulomb's law. This\napproach proposes the elliptical $k$-nearest neighbors search method, enabling\nfast convergence navigation and avoiding high solution cost or infeasible paths\nby exploring more problem-specific search-worthy areas. It demonstrates\nbenefits in search efficiency and cost reduction, particularly in confined,\nhigh-dimensional environments. It can be viewed as an extension of nearest\nneighbors search techniques. Fusing invalid vertex data with physical dynamics\nfacilitates force-direction-based search regions, resulting in an improved\nconvergence rate to the optimum. FDIT* outperforms existing single-query,\nsampling-based planners on the tested problems in R^4 to R^16 and has been\ndemonstrated on a real-world mobile manipulation task.", "AI": {"tldr": "FDIT*\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u6548\u9876\u70b9\u4e2d\u7684\u4fe1\u606f\u5e76\u7ed3\u5408\u7269\u7406\u529b\u5b66\uff08\u5982\u5e93\u4ed1\u5b9a\u5f8b\uff09\uff0c\u63d0\u9ad8\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u901f\u5ea6\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86FDIT*\u3002", "method": "FDIT*\u5728EIT*\u7684\u57fa\u7840\u4e0a\uff0c\u5229\u7528\u65e0\u6548\u9876\u70b9\u4fe1\u606f\uff0c\u7ed3\u5408\u7269\u7406\u529b\u5b66\uff08\u7279\u522b\u662f\u5e93\u4ed1\u5b9a\u5f8b\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u692d\u5706k\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "FDIT*\u5728R^4\u5230R^16\u7684\u7a7a\u95f4\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u5355\u67e5\u8be2\u3001\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u5728\u6240\u6d4b\u8bd5\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5728\u771f\u5b9e\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "FDIT*\u901a\u8fc7\u878d\u5408\u65e0\u6548\u9876\u70b9\u6570\u636e\u548c\u7269\u7406\u52a8\u529b\u5b66\uff0c\u5229\u7528\u57fa\u4e8e\u529b\u65b9\u5411\u7684\u641c\u7d22\u533a\u57df\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684\u901f\u5ea6\uff0c\u5728\u9ad8\u7ef4\u3001\u53d7\u9650\u73af\u5883\u4e2d\u5c24\u5176\u6709\u6548\u3002"}}
{"id": "2508.19389", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19389", "abs": "https://arxiv.org/abs/2508.19389", "authors": ["Owais Ahmad", "Milad Ramezankhani", "Anirudh Deodhar"], "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting", "comment": null, "summary": "Accurate long-term traffic forecasting remains a critical challenge in\nintelligent transportation systems, particularly when predicting high-frequency\ntraffic phenomena such as shock waves and congestion boundaries over extended\nrollout horizons. Neural operators have recently gained attention as promising\ntools for modeling traffic flow. While effective at learning function space\nmappings, they inherently produce smooth predictions that fail to reconstruct\nhigh-frequency features such as sharp density gradients which results in rapid\nerror accumulation during multi-step rollout predictions essential for\nreal-time traffic management. To address these fundamental limitations, we\nintroduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)\narchitecture. DETNO leverages a transformer neural operator with\ncross-attention mechanisms, providing model expressivity and super-resolution,\ncoupled with a diffusion-based refinement component that iteratively\nreconstructs high-frequency traffic details through progressive denoising. This\novercomes the inherent smoothing limitations and rollout instability of\nstandard neural operators. Through comprehensive evaluation on chaotic traffic\ndatasets, our method demonstrates superior performance in extended rollout\npredictions compared to traditional and transformer-based neural operators,\npreserving high-frequency components and improving stability over long\nprediction horizons.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDETNO\u7684\u7edf\u4e00\u6269\u6563\u589e\u5f3aTransformer\u795e\u7ecf\u5143\u7b97\u5b50\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u671f\u9ad8\u9891\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u518d\u73b0\u51b2\u51fb\u6ce2\u548c\u62e5\u5835\u8fb9\u754c\u7b49\u9ad8\u9891\u7279\u5f81\u3002", "motivation": "\u6807\u51c6\u7684\u795e\u7ecf\u5143\u7b97\u5b50\u5728\u9884\u6d4b\u9ad8\u9891\u4ea4\u901a\u7279\u5f81\uff08\u5982\u5bc6\u5ea6\u68af\u5ea6\uff09\u65f6\u5b58\u5728\u5e73\u6ed1\u9884\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5728\u591a\u6b65\u9884\u6d4b\u4e2d\u8bef\u5dee\u7d2f\u79ef\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u7684\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDETNO\u7684\u7edf\u4e00\u6269\u6563\u589e\u5f3aTransformer\u795e\u7ecf\u5143\u7b97\u5b50\u67b6\u6784\u3002\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u7684Transformer\u795e\u7ecf\u5143\u7b97\u5b50\uff08\u7528\u4e8e\u63d0\u4f9b\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u8d85\u5206\u8fa8\u7387\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u7ec6\u5316\u7ec4\u4ef6\uff08\u901a\u8fc7\u6e10\u8fdb\u5f0f\u53bb\u566a\u8fed\u4ee3\u5730\u91cd\u5efa\u9ad8\u9891\u4ea4\u901a\u7ec6\u8282\uff09\u3002", "result": "\u901a\u8fc7\u5728\u6df7\u6c8c\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\uff0cDETNO\u65b9\u6cd5\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684\u548c\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u5143\u7b97\u5b50\uff0c\u80fd\u591f\u4fdd\u6301\u9ad8\u9891\u6210\u5206\u5e76\u63d0\u9ad8\u957f\u9884\u6d4b\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "DETNO\u67b6\u6784\u901a\u8fc7\u7ed3\u5408Transformer\u795e\u7ecf\u5143\u7b97\u5b50\u548c\u6269\u6563\u6a21\u578b\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u795e\u7ecf\u5143\u7b97\u5b50\u5728\u5904\u7406\u9ad8\u9891\u4ea4\u901a\u7279\u5f81\u548c\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5b9a\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2508.19910", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19910", "abs": "https://arxiv.org/abs/2508.19910", "authors": ["Sergio Hernandez", "Christophe Peucheret", "Francesco Da Ros", "Darko Zibar"], "title": "Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission", "comment": "10 pages, 10 figures, submitted to journal of lightwave technology", "summary": "Directly modulated lasers (DMLs) are an attractive technology for short-reach\nintensity modulation and direct detection communication systems. However, their\ncomplex nonlinear dynamics make the modeling and optimization of DML-based\nsystems challenging. In this paper, we study the end-to-end optimization of\nDML-based systems based on a data-driven surrogate model trained on\nexperimental data. The end-to-end optimization includes the pulse shaping and\nequalizer filters, the bias current and the modulation radio-frequency (RF)\npower applied to the laser. The performance of the end-to-end optimization\nscheme is tested on the experimental setup and compared to 4 different\nbenchmark schemes based on linear and nonlinear receiver-side equalization. The\nresults show that the proposed end-to-end scheme is able to deliver better\nperformance throughout the studied symbol rates and transmission distances\nwhile employing lower modulation RF power, fewer filter taps and utilizing a\nsmaller signal bandwidth.", "AI": {"tldr": "\u6570\u636e\u9a71\u52a8\u7684\u66ff\u4ee3\u6a21\u578b\u53ef\u4ee5\u4f18\u5316\u57fa\u4e8e\u76f4\u63a5\u8c03\u5236\u6fc0\u5149\u5668\uff08DML\uff09\u7684\u901a\u4fe1\u7cfb\u7edf\u3002", "motivation": "\u76f4\u63a5\u8c03\u5236\u6fc0\u5149\u5668\uff08DML\uff09\u5728\u77ed\u8ddd\u79bb\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4f7f\u5f97\u5efa\u6a21\u548c\u4f18\u5316\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u5bf9DML\u7cfb\u7edf\u7684\u8109\u51b2\u6574\u5f62\u3001\u5747\u8861\u6ee4\u6ce2\u5668\u3001\u504f\u7f6e\u7535\u6d41\u4ee5\u53ca\u8c03\u5236\u5c04\u9891\u529f\u7387\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u4e0e\u57fa\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u63a5\u6536\u7aef\u5747\u8861\u7684\u57fa\u51c6\u65b9\u6848\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u65b9\u6848\u5728\u6240\u7814\u7a76\u7684\u7b26\u53f7\u901f\u7387\u548c\u4f20\u8f93\u8ddd\u79bb\u4e0a\u5747\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f7f\u7528\u8f83\u4f4e\u7684\u8c03\u5236\u5c04\u9891\u529f\u7387\u3001\u8f83\u5c11\u7684\u6ee4\u6ce2\u5668\u62bd\u5934\u548c\u8f83\u5c0f\u7684\u4fe1\u53f7\u5e26\u5bbd\u3002", "conclusion": "\u7aef\u5230\u7aef\u7684\u4f18\u5316\u65b9\u6848\u80fd\u591f\u514b\u670dDML\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5e76\u5e26\u6765\u6027\u80fd\u4e0a\u7684\u63d0\u5347\u3002"}}
{"id": "2508.19606", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19606", "abs": "https://arxiv.org/abs/2508.19606", "authors": ["Dingwei Zhao", "Abolfazl Bayat", "Victor Montenegro"], "title": "Near-Ultimate Quantum-Enhanced Sensitivity in Dissipative Critical Sensing with Partial Access", "comment": "Feedback and comments are welcome", "summary": "Quantum sensors are powerful devices that exploit quantum effects to detect\nminute quantities with extremely high precision. Two obstacles to harnessing\nthe full capacity of quantum probes are the resource-intensive preparation of\nthe probe and the need for sophisticated measurements that typically require\nfull access to the entire probe. Here, we address these challenges by\ninvestigating the driven Jaynes-Cummings system undergoing a dissipative\nquantum phase transition as a quantum sensor. We show that detuning the system\noff resonance significantly improves sensing performance by adequately\nselecting a preferred bistable state in phase space. Our dissipative sensor,\nindependent of the initial probe preparation, exhibits a super-linear\nenhancement in sensitivity with respect to a specific sensing resource -- the\nstrong-coupling regime ratio -- which manifests in both the full system and\npartial subsystem. Hence, quantum-enhanced sensitivity persists even when only\npartial system accessibility is available. Remarkably, we show that a homodyne\ndetection of the field state, combined with Bayesian estimation, nearly\nsaturates the ultimate sensitivity limit of the entire system.", "AI": {"tldr": "\u91cf\u5b50\u4f20\u611f\u5668\u5229\u7528\u91cf\u5b50\u6548\u5e94\u9ad8\u7cbe\u5ea6\u63a2\u6d4b\u5fae\u5c0f\u91cf\uff0c\u4f46\u5236\u5907\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u9700\u8981\u590d\u6742\u6d4b\u91cf\u3002\u672c\u6587\u7814\u7a76\u8017\u6563\u91cf\u5b50\u76f8\u53d8\u4e2d\u7684\u9a71\u52a8Jaynes-Cummings\u7cfb\u7edf\u4f5c\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\uff0c\u53d1\u73b0\u5931\u8c10\u53ef\u6539\u5584\u4f20\u611f\u6027\u80fd\u3002\u8be5\u4f20\u611f\u5668\u4e0d\u4f9d\u8d56\u521d\u59cb\u63a2\u9488\u5236\u5907\uff0c\u7075\u654f\u5ea6\u968f\u5f3a\u8026\u5408\u6bd4\u5448\u8d85\u7ebf\u6027\u589e\u5f3a\uff0c\u90e8\u5206\u53ef\u53ca\u7cfb\u7edf\u4e5f\u5b58\u5728\u6b64\u73b0\u8c61\u3002\u901a\u8fc7\u5bf9\u573a\u6001\u8fdb\u884c\u5355\u6a21\u8fde\u7eed\u6d4b\u91cf\u548c\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u53ef\u63a5\u8fd1\u6781\u9650\u7075\u654f\u5ea6\u3002", "motivation": "\u91cf\u5b50\u63a2\u9488\u7684\u5236\u5907\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u9700\u8981\u590d\u6742\u6d4b\u91cf\uff0c\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u8017\u6563\u91cf\u5b50\u76f8\u53d8\u4e2d\u7684\u9a71\u52a8Jaynes-Cummings\u7cfb\u7edf\u4f5c\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\uff0c\u5e76\u901a\u8fc7\u5931\u8c10\u548c\u5355\u6a21\u8fde\u7eed\u6d4b\u91cf\u4e0e\u8d1d\u53f6\u65af\u4f30\u8ba1\u6765\u63d0\u5347\u4f20\u611f\u6027\u80fd\u3002", "result": "\u5931\u8c10\u663e\u8457\u6539\u5584\u4e86\u4f20\u611f\u6027\u80fd\uff0c\u5e76\u4e14\u7075\u654f\u5ea6\u968f\u5f3a\u8026\u5408\u6bd4\u7684\u589e\u5927\u800c\u8d85\u7ebf\u6027\u589e\u5f3a\uff0c\u5373\u4f7f\u5728\u90e8\u5206\u7cfb\u7edf\u53ef\u53ca\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u91cf\u5b50\u589e\u5f3a\u7684\u7075\u654f\u5ea6\u3002\u5355\u6a21\u8fde\u7eed\u6d4b\u91cf\u7ed3\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u65b9\u6848\u51e0\u4e4e\u8fbe\u5230\u4e86\u7cfb\u7edf\u6781\u9650\u7075\u654f\u5ea6\u3002", "conclusion": "\u8be5\u8017\u6563\u91cf\u5b50\u4f20\u611f\u5668\u4e0d\u4f9d\u8d56\u4e8e\u521d\u59cb\u63a2\u9488\u7684\u5236\u5907\uff0c\u5e76\u4e14\u5728\u90e8\u5206\u7cfb\u7edf\u53ef\u53ca\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u7075\u654f\u5ea6\u4ecd\u80fd\u5f97\u5230\u91cf\u5b50\u589e\u5f3a\uff0c\u901a\u8fc7\u5355\u6a21\u8fde\u7eed\u6d4b\u91cf\u548c\u8d1d\u53f6\u65af\u4f30\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6781\u9650\u7684\u4f20\u611f\u6027\u80fd\u3002"}}
{"id": "2508.19325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u7ed3\u5408\u5fc3\u810fMRI\u5f71\u50cf\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6\uff08MACE\uff09\uff0c\u5728\u591a\u9879\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u8bc6\u522b\u51fa\u4e0e\u5fc3\u810f\u98ce\u9669\u76f8\u5173\u7684\u4e09\u79cd\u5f71\u50cf\u5b66\u7279\u5f81\u548c\u4e09\u79cd\u4e3b\u8981\u7684EHR\u98ce\u9669\u56e0\u7d20\u3002", "motivation": "\u5fc3\u8840\u7ba1\u9884\u540e\u4e2d\u7684\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6\uff08MACE\uff09\u7684\u51c6\u786e\u9884\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002", "method": "PRISM\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u6765\u81ea\u65e0\u5bf9\u6bd4\u5fc3\u808c\u7535\u5f71\u78c1\u5171\u632f\u6210\u50cf\uff08cine cardiac magnetic resonance imaging\uff09\u7684\u89c6\u89c9\u8868\u5f81\u548c\u7ed3\u6784\u5316\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\uff0c\u7528\u4e8e\u751f\u5b58\u5206\u6790\u3002PRISM\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u591a\u89c6\u56fe\u84b8\u998f\uff08motion-aware multi-view distillation\uff09\u63d0\u53d6\u65f6\u95f4\u540c\u6b65\u7684\u5f71\u50cf\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u533b\u5b66\u4fe1\u606f\u6587\u672c\u63d0\u793a\uff08medically informed textual prompts\uff09\u5bf9\u5176\u8fdb\u884c\u8c03\u5236\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u98ce\u9669\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u72ec\u7acb\u7684\u4e34\u5e8a\u961f\u5217\u4e2d\uff0cPRISM\u5728\u5185\u90e8\u548c\u5916\u90e8\u9a8c\u8bc1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u7684\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u548c\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002\u6b64\u5916\uff0cPRISM\u63d0\u53d6\u7684\u5f71\u50cf\u548cEHR\u8868\u5f81\u4e3a\u4e0d\u540c\u961f\u5217\u7684\u5fc3\u810f\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u4e09\u79cd\u4e0eMACE\u98ce\u9669\u5347\u9ad8\u76f8\u5173\u7684\u5f71\u50cf\u5b66\u7279\u5f81\uff1a\u4fa7\u58c1\u4e0d\u540c\u6b65\u3001\u4e0b\u58c1\u9ad8\u654f\u611f\u6027\u548c\u8212\u5f20\u65e9\u671f\u524d\u58c1\u62ac\u9ad8\u3002\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u5f52\u56e0\uff08prompt-guided attribution\uff09\u8fd8\u786e\u5b9a\u4e86\u9ad8\u8840\u538b\u3001\u7cd6\u5c3f\u75c5\u548c\u5438\u70df\u662f\u4e34\u5e8a\u548c\u751f\u7406EHR\u56e0\u7d20\u4e2d\u7684\u4e3b\u8981\u8d21\u732e\u56e0\u7d20\u3002", "conclusion": "PRISM\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u6574\u5408\u5fc3\u810fMRI\u5f71\u50cf\u548cEHR\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684MACE\u9884\u6d4b\uff0c\u5e76\u63ed\u793a\u4e86\u91cd\u8981\u7684\u5fc3\u810f\u98ce\u9669\u751f\u7269\u6807\u5fd7\u7269\u548c\u4e34\u5e8a\u98ce\u9669\u56e0\u7d20\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u7684\u6fc0\u589e\u5e26\u6765\u4e86\u4fbf\u5229\uff0c\u4f46\u4e5f\u5bfc\u81f4\u4e86\u590d\u6742\u7684\u793e\u4f1a\u5de5\u7a0b\u8bc8\u9a97\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86CASE\uff08Conversational Agent for Scam Elucidation\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u521b\u65b0\u7684Agentic AI\u6846\u67b6\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u7ba1\u7406\u7528\u6237\u8bc8\u9a97\u53cd\u9988\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u8bdd\u4ee3\u7406\u4e3b\u52a8\u8bbf\u8c08\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u6536\u96c6\u8be6\u7ec6\u7684\u5bf9\u8bdd\u4fe1\u606f\uff0c\u7136\u540e\u7531\u53e6\u4e00\u4e2aAI\u7cfb\u7edf\u63d0\u53d6\u4fe1\u606f\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u548c\u624b\u52a8\u6267\u884c\u3002\u8be5\u6846\u67b6\u5728Google Pay\uff08GPay\uff09\u5370\u5ea6\u5f97\u5230\u4e86\u5b9e\u73b0\uff0c\u901a\u8fc7Gemini LLMs\uff0c\u5728\u73b0\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u65b0\u60c5\u62a5\uff0c\u4f7f\u8bc8\u9a97\u6267\u6cd5\u91cf\u63d0\u9ad8\u4e8621%\u3002\u8be5\u4f53\u7cfb\u7ed3\u6784\u53ca\u5176\u8bc4\u4f30\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u53ef\u4e3a\u5728\u5176\u4ed6\u654f\u611f\u9886\u57df\u6784\u5efa\u7c7b\u4f3c\u7684AI\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u84dd\u56fe\u3002", "motivation": "\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u7684\u589e\u957f\u4f34\u968f\u7740\u65e5\u76ca\u590d\u6742\u7684\u793e\u4f1a\u5de5\u7a0b\u8bc8\u9a97\uff0c\u8fd9\u4e9b\u8bc8\u9a97\u5f80\u5f80\u53d1\u751f\u5728\u652f\u4ed8\u5e73\u53f0\u4e4b\u5916\uff0c\u5bfc\u81f4\u4ec5\u4f9d\u9760\u7528\u6237\u548c\u4ea4\u6613\u4fe1\u53f7\u96be\u4ee5\u5168\u9762\u7406\u89e3\u548c\u53ca\u65f6\u9884\u9632\u3002", "method": "\u63d0\u51faCASE\uff08Conversational Agent for Scam Elucidation\uff09\u6846\u67b6\uff0c\u4e00\u4e2aAgentic AI\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u5bf9\u8bdd\u4ee3\u7406\uff0c\u7528\u4e8e\u4e3b\u52a8\u8bbf\u8c08\u6f5c\u5728\u53d7\u5bb3\u8005\u4ee5\u6536\u96c6\u8be6\u7ec6\u7684\u5bf9\u8bdd\u4fe1\u606f\u3002\u968f\u540e\uff0c\u5229\u7528\u53e6\u4e00\u4e2aAI\u7cfb\u7edf\uff08\u57fa\u4e8eGoogle\u7684Gemini LLMs\uff09\u5904\u7406\u5bf9\u8bdd\u8bb0\u5f55\uff0c\u63d0\u53d6\u4fe1\u606f\u5e76\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u548c\u624b\u52a8\u6267\u6cd5\u3002", "result": "\u5728Google Pay\uff08GPay\uff09\u5370\u5ea6\u5b9e\u65bd\u8be5\u6846\u67b6\u540e\uff0c\u901a\u8fc7\u589e\u52a0\u65b0\u6536\u96c6\u7684\u60c5\u62a5\uff0c\u8bc8\u9a97\u6267\u6cd5\u7684\u6570\u91cf\uff08volume of scam enforcements\uff09\u63d0\u9ad8\u4e8621%\u3002", "conclusion": "CASE\u6846\u67b6\u901a\u8fc7\u6536\u96c6\u548c\u7ba1\u7406\u7528\u6237\u8bc8\u9a97\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5e73\u53f0\u793e\u4f1a\u5de5\u7a0b\u8bc8\u9a97\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u4e0e\u73b0\u6709\u7cfb\u7edf\u96c6\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc8\u9a97\u68c0\u6d4b\u548c\u6267\u6cd5\u7684\u6548\u7387\u3002\u8be5\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2508.19776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19776", "abs": "https://arxiv.org/abs/2508.19776", "authors": ["Liding Zhang", "Yao Ling", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization", "comment": "IEEE Robotics and Automation Letters (also presented at IEEE-IROS\n  2025)", "summary": "Bidirectional motion planning often reduces planning time compared to its\nunidirectional counterparts. It requires connecting the forward and reverse\nsearch trees to form a continuous path. However, this process could fail and\nrestart the asymmetric bidirectional search due to the limitations of\nlazy-reverse search. To address this challenge, we propose Greedy GuILD\nGrafting Trees (G3T*), a novel path planner that grafts invalid edge\nconnections at both ends to re-establish tree-based connectivity, enabling\nrapid path convergence. G3T* employs a greedy approach using the minimum\nLebesgue measure of guided incremental local densification (GuILD) subsets to\noptimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling\ndistribution between the informed set and GuILD subsets based on historical and\ncurrent cost improvements, ensuring asymptotic optimality. These features\nenhance the forward search's growth towards the reverse tree, achieving faster\nconvergence and lower solution costs. Benchmark experiments across dimensions\nfrom R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior\nperformance compared to existing single-query sampling-based planners. A video\nshowcasing our experimental results is available at:\nhttps://youtu.be/3mfCRL5SQIU", "AI": {"tldr": "G3T*\u662f\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u4fee\u526a\u65e0\u6548\u8fde\u63a5\u5e76\u4f7f\u7528GuILD\u5b50\u96c6\u8fdb\u884c\u8d2a\u5a6a\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\u548c\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u4e2d\u7531\u4e8e\u61d2\u60f0\u53cd\u5411\u641c\u7d22\u7684\u9650\u5236\u800c\u5bfc\u81f4\u7684\u8fde\u63a5\u5931\u8d25\u548c\u641c\u7d22\u91cd\u542f\u95ee\u9898\u3002", "method": "G3T*\u91c7\u7528\u8d2a\u5a6a\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u5c0fLebesgue\u6d4b\u91cf\u7684GuILD\u5b50\u96c6\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\u6765\u5b9e\u73b0\u6e10\u8fd1\u6700\u4f18\u6027\u3002", "result": "G3T*\u5728\u4eceR^2\u5230R^8\u7684\u57fa\u51c6\u5b9e\u9a8c\u548c\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u89c4\u5212\u5668\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u89e3\u51b3\u65b9\u6848\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "G3T*\u901a\u8fc7\u5176\u72ec\u7279\u7684\u4fee\u526a\u548c\u4f18\u5316\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u5404\u79cd\u7ef4\u5ea6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19394", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19394", "abs": "https://arxiv.org/abs/2508.19394", "authors": ["Afrar Jahin", "Yi Pan", "Yingfeng Wang", "Tianming Liu", "Wei Zhang"], "title": "Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding", "comment": null, "summary": "Although recent advances in quantum machine learning (QML) offer significant\npotential for enhancing generative models, particularly in molecular design, a\nlarge array of classical approaches still face challenges in achieving high\nfidelity and validity. In particular, the integration of QML with\nsequence-based tasks, such as Simplified Molecular Input Line Entry System\n(SMILES) string reconstruction, remains underexplored and usually suffers from\nfidelity degradation. In this work, we propose a hybrid quantum-classical\narchitecture for SMILES reconstruction that integrates quantum encoding with\nclassical sequence modeling to improve quantum fidelity and classical\nsimilarity. Our approach achieves a quantum fidelity of approximately 84% and a\nclassical reconstruction similarity of 60%, surpassing existing quantum\nbaselines. Our work lays a promising foundation for future QML applications,\nstriking a balance between expressive quantum representations and classical\nsequence models and catalyzing broader research on quantum-aware sequence\nmodels for molecular and drug discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u7528\u4e8eSMILES\u91cd\u5efa\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ecf\u5178\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u589e\u5f3a\u751f\u6210\u6a21\u578b\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7ecf\u5178\u65b9\u6cd5\u5728\u5206\u5b50\u8bbe\u8ba1\u7b49\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u9ad8\u4fdd\u771f\u5ea6\u548c\u6709\u6548\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728SMILES\u91cd\u5efa\u7b49\u5e8f\u5217\u4efb\u52a1\u4e2d\uff0cQML\u7684\u96c6\u6210\u7814\u7a76\u4e0d\u8db3\u4e14\u4fdd\u771f\u5ea6\u5e38\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u5c06\u91cf\u5b50\u7f16\u7801\u4e0e\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ecf\u5178\u76f8\u4f3c\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea684%\u7684\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c60%\u7684\u7ecf\u5178\u91cd\u5efa\u76f8\u4f3c\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u91cf\u5b50\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u672a\u6765\u7684QML\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u8868\u8fbe\u6027\u91cf\u5b50\u8868\u793a\u548c\u7ecf\u5178\u5e8f\u5217\u6a21\u578b\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u7528\u4e8e\u5206\u5b50\u548c\u836f\u7269\u53d1\u73b0\u7684\u91cf\u5b50\u611f\u77e5\u5e8f\u5217\u6a21\u578b\u7684\u5e7f\u6cdb\u7814\u7a76\u3002"}}
{"id": "2508.19464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19464", "abs": "https://arxiv.org/abs/2508.19464", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "CoLAP\u65b9\u6cd5\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8868\u793a\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u9ad8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u591a\u8bed\u8a00NLP\u4e2d\u7684\u8868\u73b0\uff0c\u6570\u636e\u6548\u7387\u9ad8\uff0c\u4f18\u4e8e\u5c11\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "motivation": "\u591a\u8bed\u8a00NLP\u4e2d\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u8bed\u8a00\u8d44\u6e90\u4e0a\u5b58\u5728\u5dee\u8ddd\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6709\u6548\u8bad\u7ec3\u6240\u9700\u7684\u6570\u636e\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u8bed\u8a00\u5bf9\u9f50\u4e0e\u63d0\u793a\uff08CoLAP\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8868\u793a\uff0c\u5b9e\u73b0\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff08\u5982\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u548c\u5173\u7cfb\u62bd\u53d6\uff09\u4e0a\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cCoLAP\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u5c11\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u7f29\u5c0f\u4e86\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "CoLAP\u65b9\u6cd5\u901a\u8fc7\u5176\u6570\u636e\u6548\u7387\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u8bed\u8a00\uff0c\u51cf\u5c11\u5bf9\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u66f4\u6709\u6548\u591a\u8bed\u8a00NLP\u6280\u672f\u7684\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2508.19931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19931", "abs": "https://arxiv.org/abs/2508.19931", "authors": ["Isabella W. G. da Silva", "Zahra Mobini", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Cell-Free Massive MIMO-Based Physical-Layer Authentication", "comment": null, "summary": "In this paper, we exploit the cell-free massive multiple-input\nmultiple-output (CF-mMIMO) architecture to design a physical-layer\nauthentication (PLA) framework that can simultaneously authenticate multiple\ndistributed users across the coverage area. Our proposed scheme remains\neffective even in the presence of active adversaries attempting impersonation\nattacks to disrupt the authentication process. Specifically, we introduce a\ntag-based PLA CFmMIMO system, wherein the access points (APs) first estimate\ntheir channels with the legitimate users during an uplink training phase.\nSubsequently, a unique secret key is generated and securely shared between each\nuser and the APs. We then formulate a hypothesis testing problem and derive a\nclosed-form expression for the probability of detection for each user in the\nnetwork. Numerical results validate the effectiveness of the proposed approach,\ndemonstrating that it maintains a high detection probability even as the number\nof users in the system increases.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5143\u65e0\u5173\u6d77\u91cf\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08CF-mMIMO\uff09\u67b6\u6784\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\uff08PLA\uff09\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u8ba4\u8bc1\u7f51\u7edc\u4e2d\u591a\u4e2a\u5206\u5e03\u5f0f\u7528\u6237\uff0c\u5e76\u80fd\u6709\u6548\u62b5\u6297\u6a21\u4eff\u653b\u51fb\u3002", "motivation": "\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8ba4\u8bc1\u591a\u4e2a\u5206\u5e03\u5f0f\u7528\u6237\uff0c\u5e76\u5728\u5b58\u5728\u6a21\u4eff\u653b\u51fb\u65f6\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u6027\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u7684CF-mMIMO\u7cfb\u7edf\uff0c\u5728\u5c0f\u533a\u5185\u8bad\u7ec3\u9636\u6bb5\uff0c\u63a5\u5165\u70b9\uff08AP\uff09\u4f30\u8ba1\u4e0e\u5408\u6cd5\u7528\u6237\u7684\u4fe1\u9053\uff0c\u5e76\u751f\u6210\u7528\u6237\u4e0eAP\u4e4b\u95f4\u7684\u552f\u4e00\u5bc6\u94a5\u3002\u7136\u540e\uff0c\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u51fa\u6bcf\u4e2a\u7528\u6237\u7684\u68c0\u6d4b\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7f51\u7edc\u7528\u6237\u6570\u91cf\u589e\u52a0\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u68c0\u6d4b\u6982\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eCF-mMIMO\u7684PLA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u540c\u65f6\u8ba4\u8bc1\u591a\u4e2a\u7528\u6237\uff0c\u5e76\u80fd\u62b5\u6297\u5404\u79cd\u653b\u51fb\u3002"}}
{"id": "2508.19349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative\ndisorders worldwide. As it progresses, it leads to the deterioration of\ncognitive functions. Since AD is irreversible, early diagnosis is crucial for\nmanaging its progression. Mild Cognitive Impairment (MCI) represents an\nintermediate stage between Cognitively Normal (CN) individuals and those with\nAD, and is considered a transitional phase from normal cognition to Alzheimer's\ndisease. Diagnosing MCI is particularly challenging due to the subtle\ndifferences between adjacent diagnostic categories. In this study, we propose\nEffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole\nAlzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging\n(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a\nVision Transformer (ViT) to capture both local and global features from MRI\nimages. Unlike previous studies that rely on limited subsets of data, our\napproach is trained on the full T1-weighted MRI dataset from ADNI, resulting in\na more robust and unbiased model. This comprehensive methodology enhances the\nmodel's clinical reliability. Furthermore, fine-tuning large pretrained models\noften yields suboptimal results when source and target dataset domains differ.\nTo address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt\nthe pretrained ViT model to our target domain. This method enables efficient\nknowledge transfer and reduces the risk of overfitting. Our model achieves a\nclassification accuracy of 92.52% and an F1-score of 92.76% across three\ndiagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86EffNetViTLoRA\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cViT\u4ee5\u53caLoRA\u6280\u672f\uff0c\u4f7f\u7528\u5b8c\u6574\u7684ADNI MRI\u6570\u636e\u96c6\uff0c\u5728AD\u3001MCI\u548cCN\u4e09\u4e2a\u7c7b\u522b\u4e4b\u95f4\u5b9e\u73b0\u4e8692.52%\u7684\u51c6\u786e\u7387\u548c92.76%\u7684F1\u5206\u6570\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u65e9\u671f\u8bca\u65ad\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u662f\u4e00\u79cd\u666e\u904d\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u4f1a\u5bfc\u81f4\u8ba4\u77e5\u529f\u80fd\u4e0b\u964d\uff0c\u4e14\u76ee\u524d\u65e0\u6cd5\u6cbb\u6108\uff0c\u56e0\u6b64\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u4f5c\u4e3a\u6b63\u5e38\u8ba4\u77e5\u4e0eAD\u4e4b\u95f4\u7684\u8fc7\u6e21\u9636\u6bb5\uff0c\u5176\u8bca\u65ad\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u901a\u7528\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5229\u7528\u5b8c\u6574\u7684ADNI MRI\u6570\u636e\u96c6\uff0c\u63d0\u9ad8AD\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u533a\u5206MCI\u60a3\u8005\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEffNetViTLoRA\u7684\u5e7f\u4e49\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u5229\u7528\u6574\u4e2aADNI\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u6570\u636e\u96c6\u8fdb\u884cAD\u8bca\u65ad\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9Transformer\uff08ViT\uff09\u4ee5\u6355\u83b7MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7814\u7a76\u91c7\u7528\u4e86\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u6765\u6709\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\u9002\u5e94\u4e8e\u76ee\u6807\u57df\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u5e76\u964d\u4f4e\u8fc7\u62df\u5408\u98ce\u9669\u3002\u6a21\u578b\u5728\u5305\u542bAD\u3001MCI\u548cCN\u4e09\u4e2a\u8bca\u65ad\u7c7b\u522b\u7684\u5b8c\u6574ADNI T1\u52a0\u6743MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "EffNetViTLoRA\u6a21\u578b\u5728\u5b8c\u6574\u7684ADNI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728AD\u3001MCI\u548cCN\u4e09\u4e2a\u8bca\u65ad\u7c7b\u522b\u4e4b\u95f4\u5b9e\u73b0\u4e8692.52%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c92.76%\u7684F1\u5206\u6570\u3002", "conclusion": "EffNetViTLoRA\u6a21\u578b\u901a\u8fc7\u7ed3\u5408CNN\u548cViT\u6765\u6355\u6349MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5229\u7528LoRA\u6280\u672f\u8fdb\u884c\u6709\u6548\u7684\u6a21\u578b\u9002\u5e94\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5229\u7528\u5b8c\u6574ADNI\u6570\u636e\u96c6\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u8bca\u65ad\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.", "AI": {"tldr": "\u4f7f\u7528\u201cboids\u201d\u7b97\u6cd5\u4f18\u5316\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\u3002", "motivation": "\u534a\u5bfc\u4f53\u751f\u4ea7\u5de5\u5382\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u68d8\u624b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u5de5\u5382\uff0c\u4f7f\u7528\u4f20\u7edf\u7684\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\u5728\u5408\u7406\u7684\u65f6\u95f4\u5185\u89e3\u51b3\u6574\u4e2a\u5de5\u5382\u95ee\u9898\u7684\u89c4\u6a21\u662f\u4e0d\u53ef\u884c\u7684\u3002\u800c\u7fa4\u4f53\u667a\u80fd\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u201cboids\u201d\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u201cboids\u201d\u7fa4\u4f53\u7b97\u6cd5\u5e94\u7528\u4e8e\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u6700\u521d\u7528\u4e8e\u673a\u5668\u4eba\u548c\u7535\u5f71\u4ea7\u4e1a\uff0c\u901a\u8fc7\u5c40\u90e8\u4fe1\u606f\u548c\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u4ea4\u4e92\u6765\u6a21\u62df\u7fa4\u4f53\u884c\u4e3a\uff0c\u4ee5\u5e94\u5bf9\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u673a\u5668\u7c7b\u578b\u5207\u6362\u7684\u6311\u6218\u3002", "result": "\u201cboids\u201d\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u673a\u5668\u5207\u6362\u7684\u95ee\u9898\uff0c\u5176\u53cd\u5e94\u65b9\u5f0f\u7c7b\u4f3c\u4e8e\u7fa4\u4f53\u52a8\u7269\u5728\u9047\u5230\u969c\u788d\u7269\u65f6\u7684\u53cd\u5e94\u3002", "conclusion": "\u201cboids\u201d\u7b97\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4f18\u5316\u534a\u5bfc\u4f53\u751f\u4ea7\u5de5\u5382\uff0c\u7279\u522b\u662f\u89e3\u51b3\u673a\u5668\u5207\u6362\u5e26\u6765\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2508.19788", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19788", "abs": "https://arxiv.org/abs/2508.19788", "authors": ["Sena Ishii", "Akash Chikhalikar", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots", "comment": "8 pages, Accepted for IEEE RO-MAN 2025 Conference", "summary": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u4f30\u8ba1\u4e8b\u6545\u591a\u53d1\u533a\u57df\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u56fe\u548c\u98ce\u9669\u4f20\u64ad\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u670d\u52a1\u673a\u5668\u4eba\u5728\u4eba\u5c45\u73af\u5883\u4e2d\u7684\u98ce\u9669\u610f\u8bc6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u65e5\u76ca\u878d\u5165\u5bb6\u5ead\u751f\u6d3b\uff0c\u9884\u6d4b\u548c\u54cd\u5e94\u73af\u5883\u5371\u5bb3\u5bf9\u4e8e\u786e\u4fdd\u7528\u6237\u5b89\u5168\u3001\u4fe1\u4efb\u548c\u6709\u6548\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u56fe-based\u4f20\u64ad\u7b97\u6cd5\u5bf9\u7269\u4f53\u8fdb\u884c\u5efa\u6a21\uff0c\u98ce\u9669\u6839\u636e\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u4e8b\u6545\u5173\u7cfb\u4ece\u9ad8\u98ce\u9669\u7269\u4f53\u4e0d\u5bf9\u79f0\u5730\u4f20\u64ad\u5230\u4f4e\u98ce\u9669\u7269\u4f53\u3002", "result": "\u5728\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u98ce\u9669\u533a\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e8c\u5143\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523075%\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5c16\u9510\u6216\u4e0d\u7a33\u5b9a\u7684\u7269\u4f53\u65f6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u60c5\u5883\u611f\u77e5\u7684\u98ce\u9669\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u7684\u573a\u666f\u7406\u89e3\u548c\u4e3b\u52a8\u5b89\u5168\u884c\u4e3a\uff0c\u4e3a\u672a\u6765\u505a\u51fa\u60c5\u5883\u9a71\u52a8\u7684\u5b89\u5168\u51b3\u7b56\u3001\u63d0\u4f9b\u5b9e\u65f6\u8b66\u62a5\u6216\u81ea\u4e3b\u534f\u52a9\u7528\u6237\u907f\u514d/\u51cf\u8f7b\u5bb6\u5ead\u73af\u5883\u4e2d\u5371\u5bb3\u7684\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19410", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19410", "abs": "https://arxiv.org/abs/2508.19410", "authors": ["Zongyu Wu", "Ruichen Xu", "Luoyao Chen", "Georgios Kementzidis", "Siyao Wang", "Yuefan Deng"], "title": "Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks", "comment": "Comments: 8 pages, 6 figures. Accepted at IJCNN 2025 (to appear in\n  IEEE/IJCNN proceedings). This arXiv submission corresponds to the\n  camera-ready version with minor editorial clarifications; results unchanged", "summary": "We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural\nNetwork (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with\nunivariate transformations. While Hamiltonian Neural Networks (HNNs) ensure\nenergy conservation by learning Hamiltonian functions directly from data,\nexisting implementations, often relying on MLPs, cause hypersensitivity to the\nhyperparameters while exploring complex energy landscapes. Our approach\nexploits the localized function approximations to better capture high-frequency\nand multi-scale dynamics, reducing energy drift and improving long-term\npredictive stability. The networks preserve the symplectic form of Hamiltonian\nsystems, and thus maintain interpretability and physical consistency. After\nassessing KAR-HNN on four benchmark problems including spring-mass, simple\npendulum, two- and three-body problem, we foresee its effectiveness for\naccurate and stable modeling of realistic physical processes often at high\ndimensions and with few known parameters.", "AI": {"tldr": "KAR-HNN \u4f7f\u7528\u57fa\u4e8e KOLMOGOROV-ARNOLD \u8868\u793a\u7684\u5355\u53d8\u91cf\u53d8\u6362\u66ff\u4ee3 MLP\uff0c\u4ee5\u63d0\u9ad8 HNN \u5728\u590d\u6742\u80fd\u91cf\u573a\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e MLP \u7684 HNN \u5728\u63a2\u7d22\u590d\u6742\u80fd\u91cf\u573a\u65f6\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3002KAR-HNN \u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u5355\u53d8\u91cf\u53d8\u6362\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u9ad8\u9891\u548c\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\uff0c\u51cf\u5c11\u80fd\u91cf\u6f02\u79fb\u5e76\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e KOLMOGOROV-ARNOLD \u8868\u793a\u7684\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc (KAR-HNN)\uff0c\u7528\u5355\u53d8\u91cf\u53d8\u6362\u53d6\u4ee3\u591a\u5c42\u611f\u77e5\u5668 (MLP)\u3002 KAR-HNN \u4fdd\u6301\u4e86\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u8f9b\u5f62\u5f0f\uff0c\u4ece\u800c\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5f39\u7c27-\u8d28\u91cf\u7cfb\u7edf\u3001\u5355\u6446\u3001\u4e8c\u4f53\u95ee\u9898\u548c\u4e09\u4f53\u95ee\u9898\u8fd9\u56db\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\u8bc4\u4f30\u4e86 KAR-HNN\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cKAR-HNN \u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u80fd\u91cf\u6f02\u79fb\u548c\u66f4\u597d\u7684\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "conclusion": "KAR-HNN \u5728\u51c6\u786e\u800c\u7a33\u5b9a\u5730\u6a21\u62df\u5177\u6709\u9ad8\u7ef4\u5ea6\u548c\u5c11\u91cf\u5df2\u77e5\u53c2\u6570\u7684\u5b9e\u9645\u7269\u7406\u8fc7\u7a0b\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2508.19467", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19467", "abs": "https://arxiv.org/abs/2508.19467", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u4e2d\u63d0\u53d6\u4e0e\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u76f8\u5173\u7684\u4e34\u5e8a\u548c\u793e\u4ea4\u5f71\u54cd\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u6570\u636e\u96c6RedditImpacts 2.0\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u5fae\u8c03\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5176\u4e2d\u5fae\u8c03\u7684DeBERTa-large\u6a21\u578b\u5728\u63d0\u53d6\u8fd9\u4e9b\u5f71\u54cd\u65b9\u9762\u8868\u73b0\u4f18\u4e8eLLMs\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u6570\u636e\u5b9e\u73b0\u826f\u597d\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5bf9\u4e8e\u4e34\u5e8aNLP\u4efb\u52a1\u5f88\u91cd\u8981\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524dAI\u80fd\u529b\u4e0e\u4e13\u5bb6\u5224\u65ad\u4e4b\u95f4\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u975e\u836f\u7269\u6027\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u662f\u4e00\u4e2a\u4e25\u5cfb\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u5176\u4e34\u5e8a\u548c\u793e\u4f1a\u540e\u679c\u5728\u4f20\u7edf\u533b\u7597\u73af\u5883\u4e2d\u5f80\u5f80\u88ab\u4f4e\u4f30\u3002\u793e\u4ea4\u5a92\u4f53\u4e3a\u83b7\u53d6\u8fd9\u4e9b\u5f71\u54cd\u7684\u4e00\u624b\u4fe1\u606f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6765\u6e90\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u63d0\u53d6\u81ea\u6211\u62a5\u544a\u7684\u4e34\u5e8a\u5f71\u54cd\uff08\u5982\u6212\u65ad\u3001\u6291\u90c1\uff09\u548c\u793e\u4f1a\u5f71\u54cd\uff08\u5982\u5931\u4e1a\uff09\u3002\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u6570\u636e\u96c6RedditImpacts 2.0\uff0c\u5e76\u8bc4\u4f30\u4e86\u5fae\u8c03\u7684\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u7684DeBERTa-large\u6a21\u578b\u5728\u5bbd\u677e\u7684\u4ee4\u724c\u7ea7F1\u5f97\u5206\u4e0a\u8fbe\u5230\u4e860.61\uff0c\u5728\u7cbe\u786e\u7387\u3001\u8de8\u5ea6\u51c6\u786e\u6027\u548c\u4efb\u52a1\u7279\u5b9a\u6307\u5357\u9075\u5faa\u65b9\u9762\u4f18\u4e8eLLMs\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u4f7f\u7528\u66f4\u5c11\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u5f3a\u5927\u7684NER\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u4e13\u5bb6\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff08Cohen's kappa: 0.81\uff09\u76f8\u6bd4\uff0c\u6700\u4f73\u6a21\u578b\u4ecd\u6709\u5dee\u8ddd\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u5bf9\u4e8e\u4e34\u5e8aNLP\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u7814\u7a76\u7684NER\u6846\u67b6\u548cRedditImpacts 2.0\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u963f\u7247\u7c7b\u836f\u7269\u6ee5\u7528\u7684\u76d1\u6d4b\u548c\u533b\u7597\u51b3\u7b56\u7684\u5236\u5b9a\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684AI\u6280\u672f\u5728\u9700\u8981\u6df1\u5ea6\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\u4e0a\u4ecd\u65e0\u6cd5\u5b8c\u5168\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u3002"}}
{"id": "2508.19994", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2508.19994", "abs": "https://arxiv.org/abs/2508.19994", "authors": ["Noah Shore"], "title": "The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture", "comment": "Submitted to International Symposium for Signal Processing 2025", "summary": "The Coherent Multiplex is formalized and validated as a scalable, real-time\nsystem for identifying, analyzing, and visualizing coherence among multiple\ntime series. Its architecture comprises a fast spectral similarity layer based\non cosine similarity metrics of Fourier-transformed signals, and a sparse\ntime-frequency layer for wavelet coherence. The system constructs and evolves a\nmultilayer graph representing inter-signal relationships, enabling low-latency\ninference and monitoring. A simulation prototype demonstrates functionality\nacross 8 synthetic channels with a high similarity threshold for further\ncomputation, with additional opportunities for scaling the architecture up to\nsupport thousands of input signals with constrained hardware. Applications\ndiscussed include neuroscience, finance, and biomedical signal analysis.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19668", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19668", "abs": "https://arxiv.org/abs/2508.19668", "authors": ["Tzu-Liang Hsu", "Kuan-Jou Wang", "Chun-Hao Chang", "Sheng-Yan Sun", "Shih-Husan Chen", "Ching-Jui Huang", "Che-Ming Li"], "title": "Quantification of Quantum Dynamical Properties with Two Experimental Settings", "comment": null, "summary": "Characterizing quantum dynamics is essential for quantifying arbitrary\nproperties of a quantum process -- such as its ability to exhibit\nquantum-mechanical dynamics or generate entanglement. However, current methods\nrequire a number of experimental settings that increases with system size,\nleading to artifacts from experimental errors. Here, we propose an approximate\noptimization method that estimates property measures using only two mutually\nunbiased bases to compute their lower and upper bounds, and to reconstruct the\ncorresponding processes. This system-size independence prevents error\naccumulation and allows characterization of the intrinsic quantum dynamics.\nCompared with quantum process tomography, we experimentally validate our method\non photonic fusion and controlled-NOT operations, demonstrating accurate\nresource estimation while substantially reducing the number of required Pauli\nexperimental settings: from 81 to 10 for the photonic fusion and to 2 for the\ncontrolled-NOT. These results show that our method is well-suited for\nestimation of dynamical properties in architectures ranging from chip-scale\nquantum processors to long-distance quantum networks.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e92\u65e0\u504f\u501a\u57fa\u6765\u4f30\u8ba1\u91cf\u5b50\u8fc7\u7a0b\u7684\u6027\u8d28\u5ea6\u91cf\uff0c\u5e76\u91cd\u5efa\u76f8\u5e94\u7684\u8fc7\u7a0b\uff0c\u4ece\u800c\u907f\u514d\u4e86\u968f\u7cfb\u7edf\u5c3a\u5bf8\u589e\u52a0\u800c\u589e\u52a0\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u9700\u6c42\u3002", "motivation": "\u76ee\u524d\u7684\u91cf\u5b50\u8fc7\u7a0b\u8868\u5f81\u65b9\u6cd5\u9700\u8981\u4e0e\u7cfb\u7edf\u5c3a\u5bf8\u6210\u6bd4\u4f8b\u589e\u957f\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u6570\u91cf\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5b9e\u9a8c\u8bef\u5dee\u7d2f\u79ef\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e00\u9650\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u4f18\u5316\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4e24\u4e2a\u4e92\u65e0\u504f\u501a\u57fa\u6765\u8ba1\u7b97\u91cf\u5b50\u8fc7\u7a0b\u6027\u8d28\u5ea6\u91cf\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u636e\u6b64\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u4e0e\u91cf\u5b50\u8fc7\u7a0b\u5c42\u6790\u6210\u50cf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5149\u5b50\u878d\u5408\u548c CNOT \u95e8\u64cd\u4f5c\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5728\u5927\u5927\u51cf\u5c11\u5b9e\u9a8c\u8bbe\u7f6e\u6570\u91cf\u7684\u540c\u65f6\uff0c\u80fd\u591f\u51c6\u786e\u5730\u4f30\u8ba1\u8d44\u6e90\u3002\u5bf9\u4e8e\u5149\u5b50\u878d\u5408\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u4ece 81 \u51cf\u5c11\u5230 10\uff1b\u5bf9\u4e8e CNOT \u95e8\uff0c\u51cf\u5c11\u5230 2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u907f\u514d\u8bef\u5dee\u7d2f\u79ef\uff0c\u80fd\u591f\u8868\u5f81\u56fa\u6709\u7684\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7cfb\u7edf\u5c3a\u5bf8\uff0c\u9002\u7528\u4e8e\u4ece\u82af\u7247\u7ea7\u91cf\u5b50\u5904\u7406\u5668\u5230\u957f\u8ddd\u79bb\u91cf\u5b50\u7f51\u7edc\u7684\u5404\u79cd\u67b6\u6784\u4e2d\u7684\u52a8\u529b\u5b66\u6027\u8d28\u4f30\u8ba1\u3002"}}
{"id": "2508.19477", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19477", "abs": "https://arxiv.org/abs/2508.19477", "authors": ["Zachary L. Crang", "Rich D. Johnston", "Katie L. Mills", "Johsan Billingham", "Sam Robertson", "Michael H. Cole", "Jonathon Weakley", "Adam Hewitt and", "Grant M. Duthie"], "title": "Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage", "comment": null, "summary": "This study aimed to: (1) understand whether commercially available\ncomputer-vision and artificial intelligence (AI) player tracking software can\naccurately measure player position, speed and distance using broadcast footage\nand (2) determine the impact of camera feed and resolution on accuracy. Data\nwere obtained from one match at the 2022 Qatar Federation Internationale de\nFootball Association (FIFA) World Cup. Tactical, programme and camera 1 feeds\nwere used. Three commercial tracking providers that use computer-vision and AI\nparticipated. Providers analysed instantaneous position (x, y coordinates) and\nspeed (m\\,s^{-1}) of each player. Their data were compared with a\nhigh-definition multi-camera tracking system (TRACAB Gen 5). Root mean square\nerror (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to\n16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\\,s^{-1}. Total match\ndistance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across\nproviders. Computer-vision and AI player tracking software offer the ability to\ntrack players with fair precision when players are detected by the software.\nProviders should use a tactical feed when tracking position and speed, which\nwill maximise player detection, improving accuracy. Both 720p and 1080p\nresolutions are suitable, assuming appropriate computer-vision and AI models\nare implemented.", "AI": {"tldr": "\u5546\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI giocatori \u8ffd\u8e2a\u8f6f\u4ef6\u53ef\u4ee5\u901a\u8fc7\u5e7f\u64ad\u89c6\u9891\u7cbe\u786e\u6d4b\u91cf giocatori \u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u8ddd\u79bb\uff0c\u4f46\u51c6\u786e\u6027\u53d7\u6444\u50cf\u5934\u4fe1\u53f7\u548c\u5206\u8fa8\u7387\u7684\u5f71\u54cd\u3002\u6218\u672f\u4fe1\u53f7\u548c720p/1080p\u5206\u8fa8\u7387\u662f\u6700\u4f73\u9009\u62e9\u3002", "motivation": "\u8bc4\u4f30\u5546\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI giocatori \u8ffd\u8e2a\u8f6f\u4ef6\u5728\u5e7f\u64ad\u89c6\u9891\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u786e\u5b9a\u6444\u50cf\u5934\u4fe1\u53f7\u548c\u5206\u8fa8\u7387\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u75282022\u5e74\u5361\u5854\u5c14FIFA\u4e16\u754c\u676f\u7684\u4e00\u573a\u6bd4\u8d5b\u6570\u636e\uff0c\u5206\u6790\u4e86\u4e09\u4e2a\u5546\u4e1a\u8ffd\u8e2a\u63d0\u4f9b\u5546\u5728\u6218\u672f\u3001\u8282\u76ee\u548c\u6444\u50cf\u59341\u4fe1\u53f7\u4e0b\u7684giocatori \u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u603b\u8ddd\u79bb\u3002\u5c06\u5176\u4e0e\u9ad8\u5206\u8fa8\u7387\u591a\u6444\u50cf\u5934\u8ffd\u8e2a\u7cfb\u7edf\uff08TRACAB Gen 5\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u8ba1\u7b97\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u548c\u5e73\u5747\u504f\u5dee\u3002", "result": "giocatori \u4f4d\u7f6eRMSE\u57281.68\u523016.39\u7c73\u4e4b\u95f4\uff0c\u901f\u5ea6RMSE\u57280.34\u52302.38\u7c73/\u79d2\u4e4b\u95f4\u3002\u603b\u6bd4\u8d5b\u8ddd\u79bb\u5e73\u5747\u504f\u5dee\u5728-1745\u7c73\uff08-21.8%\uff09\u52301945\u7c73\uff0824.3%\uff09\u4e4b\u95f4\u3002", "conclusion": "\u5f53giocatori \u88ab\u8f6f\u4ef6\u68c0\u6d4b\u5230\u65f6\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI giocatori \u8ffd\u8e2a\u8f6f\u4ef6\u53ef\u4ee5\u63d0\u4f9b\u76f8\u5f53\u7cbe\u786e\u7684\u8ffd\u8e2a\u3002\u4e3a\u4e86\u6700\u5927\u5316giocatori \u68c0\u6d4b\u548c\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5efa\u8bae\u4f7f\u7528\u6218\u672f\u4fe1\u53f7\u8fdb\u884c\u4f4d\u7f6e\u548c\u901f\u5ea6\u8ffd\u8e2a\u3002720p\u548c1080p\u5206\u8fa8\u7387\u5747\u53ef\u63a5\u53d7\uff0c\u524d\u63d0\u662f\u5b9e\u65bd\u4e86\u9002\u5f53\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u6a21\u578b\u3002"}}
{"id": "2508.19790", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19790", "abs": "https://arxiv.org/abs/2508.19790", "authors": ["Liding Zhang", "Sicheng Wang", "Kuanqi Cai", "Zhenshan Bing", "Fan Wu", "Chaoqun Wang", "Sami Haddadin", "Alois Knoll"], "title": "APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors", "comment": null, "summary": "Optimal path planning aims to determine a sequence of states from a start to\na goal while accounting for planning objectives. Popular methods often\nintegrate fixed batch sizes and neglect information on obstacles, which is not\nproblem-specific. This study introduces Adaptively Prolated Trees (APT*), a\nnovel sampling-based motion planner that extends based on Force Direction\nInformed Trees (FDIT*), integrating adaptive batch-sizing and elliptical\n$r$-nearest neighbor modules to dynamically modulate the path searching process\nbased on environmental feedback. APT* adjusts batch sizes based on the\nhypervolume of the informed sets and considers vertices as electric charges\nthat obey Coulomb's law to define virtual forces via neighbor samples, thereby\nrefining the prolate nearest neighbor selection. These modules employ\nnon-linear prolate methods to adaptively adjust the electric charges of\nvertices for force definition, thereby improving the convergence rate with\nlower solution costs. Comparative analyses show that APT* outperforms existing\nsingle-query sampling-based planners in dimensions from $\\mathbb{R}^4$ to\n$\\mathbb{R}^{16}$, and it was further validated through a real-world robot\nmanipulation task. A video showcasing our experimental results is available at:\nhttps://youtu.be/gCcUr8LiEw4", "AI": {"tldr": "APT*\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6279\u5904\u7406\u5927\u5c0f\u548c\u692d\u5706r-\u6700\u8fd1\u90bb\u6a21\u5757\u6765\u52a8\u6001\u8c03\u6574\u8def\u5f84\u641c\u7d22\u8fc7\u7a0b\uff0c\u5e76\u5728\u9ad8\u7ef4\u7a7a\u95f4\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u6279\u5904\u7406\u5927\u5c0f\u5e76\u5ffd\u7565\u4e86\u7279\u5b9a\u4e8e\u95ee\u9898\u7684\u969c\u788d\u7269\u4fe1\u606f\uff0c\u800cAPT*\u901a\u8fc7\u6574\u5408\u81ea\u9002\u5e94\u6279\u5904\u7406\u5927\u5c0f\u548c\u57fa\u4e8e\u529b\u7684\u90bb\u8fd1\u6837\u672c\u9009\u62e9\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "APT*\u901a\u8fc7\u6574\u5408\u81ea\u9002\u5e94\u6279\u5904\u7406\u5927\u5c0f\u548c\u692d\u5706r-\u6700\u8fd1\u90bb\u6a21\u5757\u6765\u52a8\u6001\u8c03\u6574\u8def\u5f84\u641c\u7d22\u8fc7\u7a0b\uff0c\u5e76\u6839\u636e\u73af\u5883\u53cd\u9988\u6765\u4f18\u5316\u641c\u7d22\u3002", "result": "APT*\u57284\u523016\u7ef4\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "APT*\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u8c03\u6574\u641c\u7d22\u8fc7\u7a0b\uff0c\u80fd\u591f\u5728\u5927\u8303\u56f4\u7684\u7ef4\u5ea6\u4e2d\u63d0\u9ad8\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.19414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19414", "abs": "https://arxiv.org/abs/2508.19414", "authors": ["Gustavo Sandoval"], "title": "Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention", "comment": "9 pages", "summary": "We present a mechanistic case study of a format-dependent reasoning failure\nin Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger\nthan \"9.8\" in chat or Q&A formats, but answers correctly in simple format.\nThrough systematic intervention, we discover transformers implement even/odd\nattention head specialization: even indexed heads handle numerical comparison,\nwhile odd heads serve incompatible functions. The bug requires exactly 8 even\nheads at Layer 10 for perfect repair. Any combination of 8+ even heads\nsucceeds, while 7 or fewer completely fails, revealing sharp computational\nthresholds with perfect redundancy among the 16 even heads. SAE analysis\nreveals the mechanism: format representations separate (10% feature overlap at\nLayer 7), then re-entangle with different weightings (80% feature overlap at\nLayer 10), with specific features showing 1.5x amplification in failing\nformats. We achieve perfect repair using only 25% of attention heads and\nidentify a 60% pattern replacement threshold, demonstrating that apparent\nfull-module requirements hide sophisticated substructure with implications for\ninterpretability and efficiency. All of our code is available at\nhttps://github.com/gussand/surgeon.", "AI": {"tldr": "Llama-3.1-8B-Instruct \u5728\u5904\u7406 '9.11' vs '9.8' \u65f6\u5b58\u5728\u683c\u5f0f\u4f9d\u8d56\u7684\u63a8\u7406\u5931\u8d25\uff0c\u5373\u4f7f/\u5947\u6570\u6ce8\u610f\u529b\u5934\u4e13\u95e8\u5316\u5bfc\u81f4\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4fee\u590d\u9700\u8981\u7279\u5b9a\u6570\u91cf\u7684\u5076\u6570\u5934\u3002", "motivation": "\u7814\u7a76 Llama-3.1-8B-Instruct \u4e2d\u683c\u5f0f\u4f9d\u8d56\u7684\u63a8\u7406\u5931\u8d25\uff0c\u7406\u89e3\u5176\u6f5c\u5728\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5e72\u9884\u548c SAE \u5206\u6790\uff0c\u7814\u7a76\u4e86\u6ce8\u610f\u529b\u5934\u7684\u4e13\u95e8\u5316\u3001\u7279\u5f81\u91cd\u53e0\u548c\u6743\u91cd\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u5076\u6570\u7d22\u5f15\u7684\u5934\u8d1f\u8d23\u6570\u503c\u6bd4\u8f83\uff0c\u5947\u6570\u7d22\u5f15\u7684\u5934\u8d1f\u8d23\u5176\u4ed6\u529f\u80fd\uff1b\u4fee\u590d\u9700\u8981 L10 \u7684 8 \u4e2a\u5076\u6570\u5934\uff1bSAE \u5206\u6790\u63ed\u793a\u4e86\u683c\u5f0f\u8868\u5f81\u5728\u4e0d\u540c\u5c42\u91cd\u53e0\u548c\u52a0\u6743\uff1b\u4f7f\u7528 25% \u7684\u6ce8\u610f\u529b\u5934\u548c 60% \u7684\u6a21\u5f0f\u66ff\u6362\u5373\u53ef\u5b8c\u7f8e\u4fee\u590d\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7cbe\u7ec6\u7684\u8ba1\u7b97\u5b50\u7ed3\u6784\uff0c\u5373\u4f7f\u770b\u8d77\u6765\u9700\u8981\u6574\u4e2a\u6a21\u5757\uff0c\u4e5f\u53ef\u80fd\u901a\u8fc7\u7279\u5b9a\u90e8\u5206\u8fdb\u884c\u4fee\u590d\uff0c\u8fd9\u5bf9\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.19475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19475", "abs": "https://arxiv.org/abs/2508.19475", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "\u81ea\u52a8\u751f\u6210\u95ee\u9898\u7b54\u6848(AQAG)\u65e8\u5728\u7b80\u5316\u6559\u80b2\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u5229\u7528\u5fae\u8c03\u7684Meta-Llama 2-7B\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u6765\u751f\u6210\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\u3002", "motivation": "\u6559\u80b2\u8bc4\u4f30\u8fc7\u7a0b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u624b\u52a8\u521b\u5efa\u516c\u5e73\u7684\u95ee\u9898\u96c6\uff0cAQAG\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u548cNLP\uff0c\u7279\u522b\u662f\u82f1\u8bed\uff0c\u901a\u8fc7\u4f7f\u7528RACE\u6570\u636e\u96c6\u5bf9Meta-Llama 2-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u6765\u5b9a\u5236\u95ee\u9898\u98ce\u683c\uff08\u9009\u62e9\u9898\u3001\u6982\u5ff5\u9898\u6216\u4e8b\u5b9e\u9898\uff09\u3002", "result": "\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u80fd\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u8bc4\u4f30\u8005\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u7684\u5b9a\u5236\u5316\u6a21\u578b\uff0c\u4ee5\u7b80\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "conclusion": "AQAG\u901a\u8fc7\u5229\u7528\u5fae\u8c03\u7684LLM\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u95ee\u9898\u548c\u7b54\u6848\uff0c\u4ece\u800c\u8282\u7701\u5b9d\u8d35\u7684\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u4f18\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2508.19692", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19692", "abs": "https://arxiv.org/abs/2508.19692", "authors": ["Johannes Kerber", "Laurin Ostermann", "Vikas Remesh", "Helmut Ritsch", "Arpita Pal"], "title": "Selective Preparation of Collective States in Coupled Quantum Emitters Using the SUPER Excitation Scheme", "comment": "22 pages including Appendices, 10 figures and 3 tables", "summary": "The efficient preparation of collective eigenstates of subwavelength-spaced\noptical dipoles is a prerequisite for observing their signature radiative\nproperties and for their applications in quantum information processing. We\ntheoretically investigate the deterministic preparation of superradiant and\nsubradiant states of two dipole-coupled two-level quantum emitters at\ndeep-subwavelength separation using the Swing-UP of Quantum Emitter Population\n(SUPER) excitation scheme. Utilizing suitable pulse parameters for two\nred-detuned, time-overlapping Gaussian pulses, the SUPER scheme enables\nclose-to-unity population inversion in the targeted collective eigenstates.\nFurthermore, a tunable optical phase in the SUPER scheme enables the\nsimultaneous inversions in both pure super- and subradiant states with finite\npopulations, thereby resulting in the preparation of hybrid collective states.\nThese results are possible to realize with or without an optical cavity. Our\napproach to populating the collective eigenstates in a cavity environment paves\nthe way for the efficient preparation of these states in the presence of\nenvironmental decoherence. Our scheme enables single-photon generation, which\nis measured using the second-order correlation function. We also discuss in\ndetail possible experimental realizations, in particular using solid-state\nemitters and molecules.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSUPER\u7684\u6fc0\u53d1\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u6df1\u4e9a\u6ce2\u957f\u5206\u79bb\u4e0b\u5236\u5907\u4e24\u4e2a\u5076\u6781\u8026\u5408\u7684\u91cf\u5b50\u53d1\u5c04\u4f53\u7684\u8d85\u8f90\u5c04\u548c\u6b21\u8f90\u5c04\u6001\u3002\u8be5\u65b9\u6848\u5229\u7528\u4e24\u4e2a\u7ea2\u5931\u8c10\u3001\u65f6\u95f4\u91cd\u53e0\u7684\u9ad8\u65af\u8109\u51b2\uff0c\u80fd\u591f\u5b9e\u73b0\u76ee\u6807\u96c6\u4f53\u672c\u5f81\u6001\u63a5\u8fd1\u5355\u4f4d\u7684\u5e03\u5c45\u53cd\u6f14\u3002\u901a\u8fc7\u53ef\u8c03\u7684\u5149\u76f8\u4f4d\uff0cSUPER\u65b9\u6848\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u7eaf\u8d85\u8f90\u5c04\u548c\u6b21\u8f90\u5c04\u6001\u7684\u6709\u9650\u5e03\u5c45\u53cd\u6f14\uff0c\u4ece\u800c\u5236\u5907\u6df7\u5408\u96c6\u4f53\u6001\u3002\u65e0\u8bba\u662f\u5426\u6709\u5149\u5b66\u8154\uff0c\u8be5\u65b9\u6848\u90fd\u53ef\u5b9e\u73b0\u3002\u8be5\u65b9\u6cd5\u4e3a\u5728\u73af\u5883\u9000\u76f8\u5e72\u5b58\u5728\u4e0b\u6709\u6548\u5236\u5907\u8fd9\u4e9b\u6001\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u5355\u5149\u5b50\u751f\u6210\uff0c\u53ef\u901a\u8fc7\u4e8c\u9636\u5173\u8054\u51fd\u6570\u8fdb\u884c\u6d4b\u91cf\u3002\u6211\u4eec\u8fd8\u8be6\u7ec6\u8ba8\u8bba\u4e86\u4f7f\u7528\u56fa\u6001\u53d1\u5c04\u4f53\u548c\u5206\u5b50\u7b49\u8fdb\u884c\u5b9e\u9a8c\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e9a\u6ce2\u957f\u95f4\u9694\u5149\u5b66\u5076\u6781\u5b50\u7684\u96c6\u4f53\u672c\u5f81\u6001\u5236\u5907\u95ee\u9898\uff0c\u8fd9\u662f\u89c2\u5bdf\u5176\u8f90\u5c04\u7279\u6027\u548c\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u5e94\u7528\u7684\u524d\u63d0\u3002", "method": "\u6211\u4eec\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5229\u7528SUPER\uff08Quantum Emitter Population\u7684Swing-UP\uff09\u6fc0\u53d1\u65b9\u6848\uff0c\u5728\u6df1\u4e9a\u6ce2\u957f\u5206\u79bb\u4e0b\u786e\u5b9a\u6027\u5730\u5236\u5907\u4e24\u4e2a\u5076\u6781\u8026\u5408\u7684\u91cf\u5b50\u53d1\u5c04\u4f53\u7684\u8d85\u8f90\u5c04\u548c\u6b21\u8f90\u5c04\u6001\u3002\u901a\u8fc7\u4ed4\u7ec6\u9009\u62e9\u4e24\u4e2a\u7ea2\u5931\u8c10\u3001\u65f6\u95f4\u91cd\u53e0\u7684\u9ad8\u65af\u8109\u51b2\u7684\u53c2\u6570\uff0c\u8be5\u65b9\u6848\u5b9e\u73b0\u4e86\u76ee\u6807\u96c6\u4f53\u672c\u5f81\u6001\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5e03\u5c45\u53cd\u6f14\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165SUPER\u65b9\u6848\u4e2d\u7684\u53ef\u8c03\u8c10\u5149\u76f8\u4f4d\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u7eaf\u8d85\u8f90\u5c04\u6001\u548c\u6b21\u8f90\u5c04\u6001\u7684\u6709\u9650\u5e03\u5c45\u53cd\u6f14\uff0c\u4ece\u800c\u5236\u5907\u51fa\u6df7\u5408\u96c6\u4f53\u6001\u3002", "result": "SUPER\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u76ee\u6807\u96c6\u4f53\u672c\u5f81\u6001\u63a5\u8fd1\u5355\u4f4d\u7684\u5e03\u5c45\u53cd\u6f14\u3002\u901a\u8fc7\u53ef\u8c03\u7684\u5149\u76f8\u4f4d\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u7eaf\u8d85\u8f90\u5c04\u548c\u6b21\u8f90\u5c04\u6001\u7684\u6709\u9650\u5e03\u5c45\u53cd\u6f14\uff0c\u4ea7\u751f\u6df7\u5408\u96c6\u4f53\u6001\u3002\u8be5\u65b9\u6848\u53ef\u7528\u4e8e\u6709\u8154\u6216\u65e0\u8154\u73af\u5883\uff0c\u5e76\u80fd\u5b9e\u73b0\u5355\u5149\u5b50\u751f\u6210\uff0c\u53ef\u901a\u8fc7\u4e8c\u9636\u5173\u8054\u51fd\u6570\u6d4b\u91cf\u3002", "conclusion": "SUPER\u6fc0\u53d1\u65b9\u6848\u4e3a\u5728\u6df1\u4e9a\u6ce2\u957f\u5206\u79bb\u4e0b\u5236\u5907\u91cf\u5b50\u53d1\u5c04\u4f53\u7684\u96c6\u4f53\u672c\u5f81\u6001\uff08\u5305\u62ec\u8d85\u8f90\u5c04\u3001\u6b21\u8f90\u5c04\u548c\u6df7\u5408\u6001\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6848\u5177\u6709\u5b9e\u73b0\u7b80\u5355\u3001\u53ef\u6269\u5c55\u6027\u5f3a\u7b49\u4f18\u70b9\uff0c\u4e3a\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u548c\u5149\u5b66\u4f20\u611f\u7b49\u9886\u57df\u5b9e\u73b0\u9ad8\u6027\u80fd\u5668\u4ef6\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.19485", "categories": ["cs.CV", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.19485", "abs": "https://arxiv.org/abs/2508.19485", "authors": ["Xinlong Zhao", "Qixiang Pang", "Shan Du"], "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation", "comment": "19 pages, 13 figures", "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJVLGS\u7684\u65b0\u578b\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u6765\u63d0\u9ad8\u6c14\u4f53\u6cc4\u6f0f\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6548\u679c\uff0c\u5e76\u52a0\u5165\u4e86\u540e\u5904\u7406\u6b65\u9aa4\u6765\u51cf\u5c11\u8bef\u62a5\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c14\u4f53\u6cc4\u6f0f\u5bf9\u4eba\u7c7b\u5065\u5eb7\u548c\u73af\u5883\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\uff09\u56e0\u6c14\u4f53\u4e91\u7684\u6a21\u7cca\u6027\u548c\u975e\u521a\u6027\u800c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJVLGS\u7684\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u540e\u5904\u7406\u6b65\u9aa4\u4ee5\u51cf\u5c11\u8bef\u62a5\u3002", "result": "JVLGS\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u76d1\u7763\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "JVLGS\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0c\u5e76\u7ed3\u5408\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6c14\u4f53\u6cc4\u6f0f\u7684\u68c0\u6d4b\u548c\u5206\u5272\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.", "AI": {"tldr": "\u6a21\u578b\u79d1\u5b66\u662f\u5e94\u5bf9\u57fa\u7840\u6a21\u578b\u5d1b\u8d77\u7684\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u8fdb\u884c\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u548c\u63a7\u5236\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u652f\u67f1\uff1a\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\uff0c\u65e8\u5728\u6784\u5efa\u53ef\u4fe1\u3001\u5b89\u5168\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u4ece\u6570\u636e\u79d1\u5b66\u8f6c\u5411\u6a21\u578b\u79d1\u5b66\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4f5c\u4e3a\u5206\u6790\u7684\u6838\u5fc3\uff0c\u4ee5\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u548c\u63a7\u5236\u5176\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a\u6a21\u578b\u79d1\u5b66\u7684\u65b0\u5b66\u79d1\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u5176\u56db\u5927\u652f\u67f1\uff1a\u9a8c\u8bc1\uff08\u9700\u8981\u4e25\u683c\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bc4\u4f30\u534f\u8bae\uff09\u3001\u89e3\u91ca\uff08\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u64cd\u4f5c\u7684\u5404\u79cd\u65b9\u6cd5\uff09\u3001\u63a7\u5236\uff08\u6574\u5408\u5bf9\u9f50\u6280\u672f\u4ee5\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\uff09\u4ee5\u53ca\u63a5\u53e3\uff08\u5f00\u53d1\u4ea4\u4e92\u5f0f\u548c\u53ef\u89c6\u5316\u89e3\u91ca\u5de5\u5177\u4ee5\u6539\u5584\u4eba\u7c7b\u6821\u51c6\u548c\u51b3\u7b56\uff09\u3002", "result": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\u56db\u4e2a\u5173\u952e\u652f\u67f1\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u79d1\u5b66\u7684\u53d1\u5c55\u3002", "conclusion": "\u6a21\u578b\u79d1\u5b66\u662fAI\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u65b9\u5411\uff0c\u5176\u63d0\u51fa\u7684\u6846\u67b6\u65e8\u5728\u4fc3\u8fdb\u53ef\u4fe1\u3001\u5b89\u5168\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2508.19816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19816", "abs": "https://arxiv.org/abs/2508.19816", "authors": ["Ricardo J. Manr\u00edquez-Cisterna", "Ankit A. Ravankar", "Jose V. Salazar Luces", "Takuro Hatsukari", "Yasuhisa Hirata"], "title": "A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living", "comment": "7 pages, accepted work for IEEE RO-MAN2025", "summary": "This paper presents a standing support mobility robot \"Moby\" developed to\nenhance independence and safety for elderly individuals during daily activities\nsuch as toilet transfers. Unlike conventional seated mobility aids, the robot\nmaintains users in an upright posture, reducing physical strain, supporting\nnatural social interaction at eye level, and fostering a greater sense of\nself-efficacy. Moby offers a novel alternative by functioning both passively\nand with mobility support, enabling users to perform daily tasks more\nindependently. Its main advantages include ease of use, lightweight design,\ncomfort, versatility, and effective sit-to-stand assistance. The robot\nleverages the Robot Operating System (ROS) for seamless control, featuring\nmanual and autonomous operation modes. A custom control system enables safe and\nintuitive interaction, while the integration with NAV2 and LiDAR allows for\nrobust navigation capabilities. This paper reviews existing mobility solutions\nand compares them to Moby, details the robot's design, and presents objective\nand subjective experimental results using the NASA-TLX method and time\ncomparisons to other methods to validate our design criteria and demonstrate\nthe advantages of our contribution.", "AI": {"tldr": "Moby\u662f\u4e00\u6b3e\u65e8\u5728\u63d0\u9ad8\u8001\u5e74\u4eba\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\u7684\u7ad9\u7acb\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ad9\u7acb\u652f\u6491\uff0c\u51cf\u8f7b\u8eab\u4f53\u8d1f\u62c5\uff0c\u4fc3\u8fdb\u793e\u4ea4\u4e92\u52a8\uff0c\u589e\u5f3a\u81ea\u6211\u6548\u80fd\u611f\uff0c\u5e76\u63d0\u4f9b\u88ab\u52a8\u548c\u4e3b\u52a8\u7684\u79fb\u52a8\u652f\u6301\uff0c\u4ee5\u5b9e\u73b0\u66f4\u72ec\u7acb\u5730\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u6b3e\u540d\u4e3aMoby\u7684\u7ad9\u7acb\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u4ee5\u63d0\u9ad8\u8001\u5e74\u4eba\u5728\u5982\u5395\u8f6c\u79fb\u7b49\u65e5\u5e38\u6d3b\u52a8\u4e2d\u7684\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u5750\u5f0f\u8f85\u52a9\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002", "method": "Moby\u673a\u5668\u4eba\u5229\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff08ROS\uff09\u8fdb\u884c\u63a7\u5236\uff0c\u96c6\u6210NAV2\u548cLiDAR\u5b9e\u73b0\u5bfc\u822a\u3002\u91c7\u7528\u81ea\u5b9a\u4e49\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u5b89\u5168\u76f4\u89c2\u7684\u4ea4\u4e92\uff0c\u5e76\u5177\u5907\u624b\u52a8\u548c\u81ea\u4e3b\u64cd\u4f5c\u6a21\u5f0f\u3002\u7814\u7a76\u4e2d\u5c06Moby\u4e0e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u91c7\u7528NASA-TLX\u65b9\u6cd5\u548c\u65f6\u95f4\u6bd4\u8f83\u7b49\u5b9e\u9a8c\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoby\u5728\u6613\u7528\u6027\u3001\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3001\u8212\u9002\u6027\u3001\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u7684\u5750\u5230\u7ad9\u8f85\u52a9\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\u6807\u51c6\u5e76\u5c55\u793a\u4e86\u5176\u8d21\u732e\u3002", "conclusion": "Moby\u673a\u5668\u4eba\u4e3a\u8001\u5e74\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ad9\u7acb\u5f0f\u79fb\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u652f\u6301\u81ea\u7136\u7ad9\u7acb\u59ff\u52bf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u6027\u3001\u5b89\u5168\u611f\u548c\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2508.19419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19419", "abs": "https://arxiv.org/abs/2508.19419", "authors": ["Harun Ur Rashid", "Aleksandra Pachalieva", "Daniel O'Malley"], "title": "Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management", "comment": null, "summary": "Accurate subsurface reservoir pressure control is extremely challenging due\nto geological heterogeneity and multiphase fluid-flow dynamics. Predicting\nbehavior in this setting relies on high-fidelity physics-based simulations that\nare computationally expensive. Yet, the uncertain, heterogeneous properties\nthat control these flows make it necessary to perform many of these expensive\nsimulations, which is often prohibitive. To address these challenges, we\nintroduce a physics-informed machine learning workflow that couples a fully\ndifferentiable multiphase flow simulator, which is implemented in the DPFEHM\nframework with a convolutional neural network (CNN). The CNN learns to predict\nfluid extraction rates from heterogeneous permeability fields to enforce\npressure limits at critical reservoir locations. By incorporating transient\nmultiphase flow physics into the training process, our method enables more\npractical and accurate predictions for realistic injection-extraction scenarios\ncompare to previous works. To speed up training, we pretrain the model on\nsingle-phase, steady-state simulations and then fine-tune it on full multiphase\nscenarios, which dramatically reduces the computational cost. We demonstrate\nthat high-accuracy training can be achieved with fewer than three thousand\nfull-physics multiphase flow simulations -- compared to previous estimates\nrequiring up to ten million. This drastic reduction in the number of\nsimulations is achieved by leveraging transfer learning from much less\nexpensive single-phase simulations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u53ef\u5fae\u5206\u6a21\u62df\u5668\u548cCNN\u7684\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u5730\u4e0b\u6cb9\u85cf\u538b\u529b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4ece\u6e17\u900f\u7387\u573a\u9884\u6d4b\u6d41\u4f53\u62bd\u53d6\u7387\uff0c\u6765\u6ee1\u8db3\u5173\u952e\u4f4d\u7f6e\u7684\u538b\u529b\u9650\u5236\u3002\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u771f\u5b9e\u6ce8\u5165-\u62bd\u53d6\u573a\u666f\u65f6\u66f4\u5b9e\u7528\u3001\u66f4\u7cbe\u786e\u3002\u901a\u8fc7\u5728\u5355\u76f8\u7a33\u6001\u6a21\u62df\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5728\u591a\u76f8\u6d41\u52a8\u573a\u666f\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u53ea\u9700\u4e0d\u5230\u4e09\u5343\u6b21\u5168\u7269\u7406\u591a\u76f8\u6d41\u52a8\u6a21\u62df\u5373\u53ef\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u3002", "motivation": "\u5730\u4e0b\u6cb9\u85cf\u538b\u529b\u63a7\u5236\u56e0\u5730\u8d28\u975e\u5747\u8d28\u6027\u548c\u591a\u76f8\u6d41\u4f53\u52a8\u529b\u5b66\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u9ad8\u4fdd\u771f\u5ea6\u7269\u7406\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u800c\u8981\u5904\u7406\u4e0d\u786e\u5b9a\u4e14\u975e\u5747\u8d28\u7684\u6027\u8d28\uff0c\u9700\u8981\u5927\u91cf\u6a21\u62df\uff0c\u8fd9\u5f80\u5f80\u662f\u4e0d\u53ef\u884c\u7684\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u8026\u5408\u4e86\u4e00\u4e2a\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u591a\u76f8\u6d41\u6a21\u62df\u5668\uff08\u5728DPFEHM\u6846\u67b6\u4e2d\u5b9e\u73b0\uff09\u548c\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002CNN\u5b66\u4e60\u4ece\u975e\u5747\u8d28\u6e17\u900f\u7387\u573a\u9884\u6d4b\u6d41\u4f53\u62bd\u53d6\u7387\uff0c\u4ee5\u5728\u5173\u952e\u6cb9\u85cf\u4f4d\u7f6e\u5f3a\u5236\u6267\u884c\u538b\u529b\u9650\u5236\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u878d\u5165\u77ac\u6001\u591a\u76f8\u6d41\u7269\u7406\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u5f80\u5de5\u4f5c\u66f4\u5b9e\u9645\u3001\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "result": "\u4e0e\u9700\u8981\u9ad8\u8fbe\u4e00\u5343\u4e07\u6b21\u6a21\u62df\u7684\u5148\u524d\u4f30\u8ba1\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6765\u81ea\u6210\u672c\u4f4e\u5f97\u591a\u7684\u5355\u76f8\u6a21\u62df\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7cbe\u5ea6\uff0c\u4ec5\u9700\u4e0d\u5230\u4e09\u5343\u6b21\u5168\u7269\u7406\u591a\u76f8\u6d41\u52a8\u6a21\u62df\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u6240\u9700\u7684\u6a21\u62df\u6b21\u6570\uff0c\u4ece\u800c\u514b\u670d\u4e86\u6cb9\u85cf\u538b\u529b\u63a7\u5236\u7684\u8ba1\u7b97\u6311\u6218\u3002"}}
{"id": "2508.19481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19481", "abs": "https://arxiv.org/abs/2508.19481", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "LLMs\u5728\u4f4e\u8d44\u6e90\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u8bcd\u5178\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u897f\u73ed\u7259-\u74e6\u5c24\u7eb3\u4f0a\u57fa\u8bed\u5bf9\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684BLEU\u63d0\u5347\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u5bf9LLMs\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u63a5\u89e6\u8fd9\u4e9b\u8bed\u8a00\u7684\u673a\u4f1a\u8f83\u5c11\uff0c\u5e76\u4e14\u7528\u4e8e\u5fae\u8c03\u7684\u5e73\u884c\u6570\u636e\u6709\u9650\u3002", "method": "\u5c06\u7ffb\u8bd1\u89c6\u4e3a\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u5730\u67e5\u9605\u53cc\u8bed\u8bcd\u5178\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u76d1\u7763\u6307\u4ee4\u8c03\u4f18\u548c\u5f15\u5bfc\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u5e76\u4f7f\u7528BLEU\u76f8\u4f3c\u5ea6\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\u6765\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u897f\u73ed\u7259-\u74e6\u5c24\u7eb3\u4f0a\u57fa\u8bed\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4e0e\u5148\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u5de5\u5177\u589e\u5f3a\u6a21\u578b\u53d6\u5f97\u4e86\u9ad8\u8fbe+3.37\u7684BLEU\u6539\u8fdb\uff0c\u5e76\u6bd4\u6ca1\u6709\u8bcd\u5178\u8bbf\u95ee\u7684\u76d1\u7763\u57fa\u7ebf\u63d0\u9ad8\u4e8618%\u7684\u76f8\u5bf9\u589e\u76ca\u3002", "conclusion": "\u7ed3\u5408LLMs\u4e0e\u5916\u90e8\u5de5\u5177\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.19498", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19498", "abs": "https://arxiv.org/abs/2508.19498", "authors": ["Yimu Wang", "Weiming Zhuang", "Chen Chen", "Jiabo Huang", "Jingtao Li", "Lingjuan Lyu"], "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models", "comment": null, "summary": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.", "AI": {"tldr": "UNIFORM\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u9884\u8bad\u7ec3\u7684\u5f02\u6784\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u800c\u65e0\u9700\u5bf9\u8bad\u7ec3\u6570\u636e\u6216\u6a21\u578b\u67b6\u6784\u505a\u51fa\u4efb\u4f55\u5047\u8bbe\u3002\u5b83\u4f7f\u7528\u4e00\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u6295\u7968\u673a\u5236\u5728logit\u548c\u7279\u5f81\u7ea7\u522b\u6355\u83b7\u6559\u5e08\u6a21\u578b\u7684\u5171\u8bc6\uff0c\u4ece\u800c\u5728\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5927\u91cf\u7684\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u77e5\u8bc6\uff0c\u4f46\u5176\u5f02\u6784\u6027\u4f7f\u5f97\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u6210\u4e3a\u4e00\u9879\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u8bad\u7ec3\u6570\u636e\u548c\u7f51\u7edc\u67b6\u6784\u505a\u51fa\u4e86\u4e25\u683c\u7684\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u4ece\u7279\u5b9a\u7c7b\u578b\u7684\u6a21\u578b\u4e2d\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u4e86\u6570\u636e\u6216\u5f52\u7eb3\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUNIFORM\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6295\u7968\u673a\u5236\uff0c\u5728logit\u548c\u7279\u5f81\u7ea7\u522b\u6355\u83b7\u5f02\u6784\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5171\u8bc6\uff0c\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u8be5\u673a\u5236\u4e0d\u9700\u8981\u5bf9\u6559\u5e08\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u6216\u7f51\u7edc\u67b6\u6784\u505a\u51fa\u4efb\u4f55\u5047\u8bbe\u3002", "result": "UNIFORM\u5728\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u4e0e\u73b0\u6709\u7684\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u4ece\u8d85\u8fc7\u4e00\u767e\u4e2a\u6559\u5e08\u6a21\u578b\u4e2d\u53d7\u76ca\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fdc\u5c0f\u4e8e\u6b64\u7684\u89c4\u6a21\u4e0b\u5c31\u4f1a\u9971\u548c\u3002", "conclusion": "UNIFORM\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5c06\u6765\u81ea\u5f02\u6784\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u4efb\u4f55\u5047\u8bbe\uff0c\u5e76\u4e14\u5728\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u51fa\u8272\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.19945", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19945", "abs": "https://arxiv.org/abs/2508.19945", "authors": ["Zhouyu Zhang", "Chih-Yuan Chiu", "Glen Chou"], "title": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions", "comment": null, "summary": "We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u52a8\u6001\u535a\u5f08\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u7ed9\u5b9a\u7684\u5c40\u90e8\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u4ea4\u4e92\u6570\u636e\u96c6\u5b66\u4e60\u53c2\u6570\u5316\u7ea6\u675f\u3002", "motivation": "\u4ece\u4ea4\u4e92\u6f14\u793a\u4e2d\u5b66\u4e60\u7ea6\u675f\uff0c\u4ee5\u8bbe\u8ba1\u6ee1\u8db3\u8fd9\u4e9b\u7ea6\u675f\u7684\u8fd0\u52a8\u8ba1\u5212\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7f16\u7801\u53c2\u4e0e\u8005\u7684Karush-Kuhn-Tucker\uff08KKT\uff09\u6761\u4ef6\u6765\u6062\u590d\u4e0e\u4ea4\u4e92\u6f14\u793a\u7684\u7eb3\u4ec0\u5e73\u7a33\u6027\u4e00\u81f4\u7684\u7ea6\u675f\u3002", "result": "\u5b66\u4e60\u7ea6\u675f\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u8fd0\u52a8\u8ba1\u5212\uff0c\u5e76\u8bc1\u660e\u4e86\u7ea6\u675f\u53ef\u5b66\u4e60\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ece\u5177\u6709\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u53c2\u4e0e\u8005\u7684\u4ea4\u4e92\u6f14\u793a\u4e2d\u63a8\u65ad\u7ea6\u675f\u5e76\u8bbe\u8ba1\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u8ba1\u5212\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u51f8\u5f62\u548c\u975e\u51f8\u5f62\u7ea6\u675f\u3002"}}
{"id": "2508.19926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19926", "abs": "https://arxiv.org/abs/2508.19926", "authors": ["Tan Jing", "Shiting Chen", "Yangfan Li", "Weisheng Xu", "Renjing Xu"], "title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control", "comment": null, "summary": "Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM.", "AI": {"tldr": "FARM\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5e27\u52a0\u901f\u589e\u5f3a\u3001\u9c81\u68d2\u7684\u57fa\u7840\u63a7\u5236\u5668\u548c\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6765\u89e3\u51b3\u673a\u5668\u4eba\u548c\u89d2\u8272\u52a8\u753b\u4e2d\u4f4e\u52a8\u6001\u548c\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u63a7\u5236\u96be\u9898\uff0c\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u9ad8\u52a8\u6001\u4eba\u5f62\u8fd0\u52a8\uff08HDHM\uff09\u6570\u636e\u96c6\u3002", "motivation": "\u7edf\u4e00\u7684\u57fa\u4e8e\u7269\u7406\u7684\u4eba\u5f62\u63a7\u5236\u5668\u5728\u6e29\u548c\u7684\u65e5\u5e38\u8fd0\u52a8\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7206\u53d1\u6027\u52a8\u4f5c\u4e2d\u4f1a\u9047\u5230\u56f0\u96be\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u4f4e\u52a8\u6001\u548c\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u63a7\u5236\u5668\u3002", "method": "FARM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a1. \u5e27\u52a0\u901f\u589e\u5f3a\uff1a\u901a\u8fc7\u6269\u5927\u5e27\u95f4\u8ddd\u6765\u66b4\u9732\u6a21\u578b\u4e8e\u9ad8\u901f\u5ea6\u59ff\u6001\u53d8\u5316\u30022. \u9c81\u68d2\u7684\u57fa\u7840\u63a7\u5236\u5668\uff1a\u53ef\u9760\u5730\u8ddf\u8e2a\u4f4e\u52a8\u6001\u8fd0\u52a8\u30023. \u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\uff1a\u81ea\u9002\u5e94\u5730\u5206\u914d\u989d\u5916\u7684\u7f51\u7edc\u5bb9\u91cf\u6765\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u9ad8\u52a8\u6001\u52a8\u4f5c\u3002", "result": "\u5728HDHM\u6570\u636e\u96c6\u4e0a\uff0cFARM\u5c06\u8ddf\u8e2a\u5931\u8d25\u7387\u964d\u4f4e\u4e8642.8%\uff0c\u5e76\u5c06\u5168\u5c40\u5e73\u5747\u6bcf\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e\u4e8614.6%\uff0c\u540c\u65f6\u5728\u4f4e\u52a8\u6001\u8fd0\u52a8\u4e0a\u4fdd\u6301\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FARM\u6846\u67b6\u662f\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u7684\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u7206\u53d1\u6027\u52a8\u4f5c\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63a8\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u6b64\u6311\u6218\u7684\u5f00\u653e\u6570\u636e\u96c6HDHM\u3002"}}
{"id": "2508.19424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19424", "abs": "https://arxiv.org/abs/2508.19424", "authors": ["Yifan Dou", "Adam Khadre", "Ruben C Petreaca", "Golrokh Mirzaei"], "title": "MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification", "comment": null, "summary": "Motivation. Understanding the pan-cancer mutational landscape offers critical\ninsights into the molecular mechanisms underlying tumorigenesis. While\npatient-level machine learning techniques have been widely employed to identify\ntumor subtypes, cohort-level clustering, where entire cancer types are grouped\nbased on shared molecular features, has largely relied on classical statistical\nmethods.\n  Results. In this study, we introduce a novel unsupervised contrastive\nlearning framework to cluster 43 cancer types based on coding mutation data\nderived from the COSMIC database. For each cancer type, we construct two\ncomplementary mutation signatures: a gene-level profile capturing nucleotide\nsubstitution patterns across the most frequently mutated genes, and a\nchromosome-level profile representing normalized substitution frequencies\nacross chromosomes. These dual views are encoded using TabNet encoders and\noptimized via a multi-scale contrastive learning objective (NT-Xent loss) to\nlearn unified cancer-type embeddings. We demonstrate that the resulting latent\nrepresentations yield biologically meaningful clusters of cancer types,\naligning with known mutational processes and tissue origins. Our work\nrepresents the first application of contrastive learning to cohort-level cancer\nclustering, offering a scalable and interpretable framework for mutation-driven\ncancer subtyping.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u7f16\u7801\u7a81\u53d8\u6570\u636e\u5bf9 43 \u79cd\u764c\u75c7\u7c7b\u578b\u8fdb\u884c\u805a\u7c7b\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u5e94\u7528\u4e8e\u961f\u5217\u7ea7\u764c\u75c7\u805a\u7c7b\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u7a81\u53d8\u9a71\u52a8\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u3002", "motivation": "\u7406\u89e3\u6cdb\u764c\u57fa\u56e0\u7a81\u53d8\u56fe\u8c31\u5bf9\u4e8e\u6df1\u5165\u4e86\u89e3\u80bf\u7624\u53d1\u751f\u8fc7\u7a0b\u4e2d\u7684\u5206\u5b50\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u5e7f\u6cdb\u91c7\u7528\u9762\u5411\u60a3\u8005\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u8bc6\u522b\u80bf\u7624\u4e9a\u578b\uff0c\u4f46\u57fa\u4e8e\u5171\u4eab\u5206\u5b50\u7279\u5f81\u5bf9\u6574\u4e2a\u764c\u75c7\u7c7b\u578b\u8fdb\u884c\u5206\u7ec4\u7684\u961f\u5217\u7ea7\u805a\u7c7b\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u4f9d\u8d56\u4e8e\u7ecf\u5178\u7684\u7edf\u8ba1\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6765\u81ea COSMIC \u6570\u636e\u5e93\u7684\u7f16\u7801\u7a81\u53d8\u6570\u636e\u5bf9 43 \u79cd\u764c\u75c7\u7c7b\u578b\u8fdb\u884c\u805a\u7c7b\u3002\u7814\u7a76\u4eba\u5458\u4e3a\u6bcf\u79cd\u764c\u75c7\u7c7b\u578b\u6784\u5efa\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u7a81\u53d8\u7279\u5f81\uff1a\u4e00\u4e2a\u57fa\u56e0\u7ea7\u522b\u7279\u5f81\uff0c\u7528\u4e8e\u6355\u6349\u6700\u5e38\u53d1\u751f\u7a81\u53d8\u7684\u57fa\u56e0\u7684\u6838\u82f7\u9178\u53d6\u4ee3\u6a21\u5f0f\uff1b\u53e6\u4e00\u4e2a\u662f\u67d3\u8272\u4f53\u7ea7\u522b\u7279\u5f81\uff0c\u7528\u4e8e\u8868\u793a\u67d3\u8272\u4f53\u4e0a\u6807\u51c6\u5316\u7684\u53d6\u4ee3\u9891\u7387\u3002\u5229\u7528 TabNet \u7f16\u7801\u5668\u5bf9\u8fd9\u4e24\u79cd\u7279\u5f81\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff08NT-Xent \u635f\u5931\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u5b66\u4e60\u7edf\u4e00\u7684\u764c\u75c7\u7c7b\u578b\u5d4c\u5165\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u6846\u67b6\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u8868\u793a\u80fd\u591f\u4ea7\u751f\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u764c\u75c7\u7c7b\u578b\u805a\u7c7b\uff0c\u8fd9\u4e9b\u805a\u7c7b\u4e0e\u5df2\u77e5\u7684\u7a81\u53d8\u8fc7\u7a0b\u548c\u7ec4\u7ec7\u8d77\u6e90\u76f8\u7b26\u3002", "conclusion": "\u672c\u7814\u7a76\u662f\u5bf9\u6bd4\u5b66\u4e60\u5728\u961f\u5217\u7ea7\u764c\u75c7\u805a\u7c7b\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u4e3a\u8fdb\u884c\u7a81\u53d8\u9a71\u52a8\u7684\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2508.19484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19484", "abs": "https://arxiv.org/abs/2508.19484", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "LLMs\u5728\u7406\u89e3\u548c\u63a8\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u89c4\u5219\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u5361\u724c\u4e4b\u95f4\u7684\u8d1f\u9762\u534f\u540c\u4f5c\u7528\u65b9\u9762\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7406\u89e3\u548c\u63a8\u7406\u590d\u6742\u89c4\u5219\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5361\u724c\u6e38\u620f\u573a\u666f\u4e2d\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b\u5361\u724c\u534f\u540c\u4f5c\u7528\u7684\u6570\u636e\u96c6\uff08\u6765\u81ea\u201cSlay the Spire\u201d\u6e38\u620f\uff09\uff0c\u5176\u4e2d\u5361\u724c\u5bf9\u6839\u636e\u5176\u6b63\u9762\u3001\u8d1f\u9762\u6216\u4e2d\u6027\u4ea4\u4e92\u8fdb\u884c\u5206\u7c7b\u3002", "result": "LLMs\u5728\u8bc6\u522b\u975e\u534f\u540c\u4f5c\u7528\u7684\u5361\u724c\u5bf9\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u68c0\u6d4b\u6b63\u9762\u548c\u8d1f\u9762\u534f\u540c\u4f5c\u7528\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u8d1f\u9762\u534f\u540c\u4f5c\u7528\u65b9\u9762\u3002\u5e38\u89c1\u7684\u9519\u8bef\u7c7b\u578b\u5305\u62ec\u65f6\u5e8f\u95ee\u9898\u3001\u6e38\u620f\u72b6\u6001\u5b9a\u4e49\u95ee\u9898\u548c\u6e38\u620f\u89c4\u5219\u9075\u5faa\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u9884\u6d4b\u89c4\u5219\u53ca\u5176\u4ea4\u4e92\u6548\u679c\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u91cd\u4e8e\u89e3\u51b3\u65f6\u5e8f\u3001\u6e38\u620f\u72b6\u6001\u5b9a\u4e49\u548c\u6e38\u620f\u89c4\u5219\u9075\u5faa\u7b49\u95ee\u9898\u3002"}}
{"id": "2508.19766", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.19766", "abs": "https://arxiv.org/abs/2508.19766", "authors": ["Zi-Fan Zhu", "Yu Su", "Yao Wang", "Rui-Xue Xu"], "title": "Optimal control on open quantum systems and application to non-Condon photo-induced electron transfer", "comment": "main text + supplementary materials", "summary": "In this work, we develop an optimal control theory on open quantum system and\nits environment, and exemplify the method with the application to the\nnon-Condon photo-induced electron transfer (PET) in condensed phase. This\nmethod utilizes the dissipaton theory, proposed by Yan in 2014 for open quantum\nsystems, which provides an exact description of the dissipative system while\nalso enables rigorous characterization and control of environmental\nhybridization modes, fully taking into account the non-perturbative and\nnon-Markovian effects. Leveraging the advantage of the dissipaton phase-space\nalgebra, we present in this communication the theoretical strategy for optimal\ncontrol on both system and environment simultaneously. The control protocol is\nsuccessfully demonstrated on PET for the environment-targeted-control\nfacilitated transfer. This work sheds the light on manipulating open systems\ndynamics via polarized environment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5149\u8bf1\u5bfc\u7535\u5b50\u8f6c\u79fb\uff08PET\uff09", "motivation": "\u5f00\u53d1\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u5e76\u5e94\u7528\u4e8e\u5149\u8bf1\u5bfc\u7535\u5b50\u8f6c\u79fb", "method": "\u5229\u7528\u8017\u6563\u5b50\u7406\u8bba\uff0c\u7ed3\u5408\u7cfb\u7edf\u548c\u73af\u5883\u7684\u534f\u540c\u63a7\u5236\uff0c\u5b9e\u73b0\u5bf9PET\u7684\u7cbe\u786e\u63a7\u5236", "result": "\u6210\u529f\u5730\u5c06\u8be5\u63a7\u5236\u65b9\u6848\u5e94\u7528\u4e8e\u975eCondon\u5149\u8bf1\u5bfc\u7535\u5b50\u8f6c\u79fb\uff0c\u5e76\u5c55\u793a\u4e86\u73af\u5883\u8f85\u52a9\u8f6c\u79fb", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u901a\u8fc7\u6781\u5316\u73af\u5883\u64cd\u7eb5\u5f00\u653e\u7cfb\u7edf\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def"}}
{"id": "2508.19499", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19499", "abs": "https://arxiv.org/abs/2508.19499", "authors": ["Xiangxu Wang", "Tianhong Zhao", "Wei Tu", "Bowen Zhang", "Guanzhou Chen", "Jinzhou Cao"], "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery", "comment": null, "summary": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.", "AI": {"tldr": "Sat2Flow\u4f7f\u7528\u536b\u661f\u56fe\u50cf\u751f\u6210\u57ce\u5e02OD\u6d41\uff0c\u65e0\u9700\u8f85\u52a9\u6570\u636e\uff0c\u5bf9\u7a7a\u95f4\u62d3\u6251\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709OD\u6d41\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u8986\u76d6\u8303\u56f4\u6709\u9650\u7684\u8f85\u52a9\u6570\u636e\uff0c\u5e76\u4e14\u5bf9\u7a7a\u95f4\u62d3\u6251\u654f\u611f\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faSat2Flow\u6846\u67b6\uff0c\u5229\u7528\u591a\u6838\u7f16\u7801\u5668\u6355\u83b7\u533a\u57df\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u611f\u77e5\u6392\u5217\u7684\u6269\u6563\u8fc7\u7a0b\u548c\u5bf9\u6bd4\u8bad\u7ec3\u6765\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u62d3\u6251\u9c81\u68d2\u6027\u3002", "result": "Sat2Flow\u5728\u6570\u503c\u7cbe\u5ea6\u3001\u7ecf\u9a8c\u5206\u5e03\u548c\u7a7a\u95f4\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u7d22\u5f15\u91cd\u6392\u4e0b\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "Sat2Flow\u4e3a\u6570\u636e\u7a00\u758f\u7684\u57ce\u5e02\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684OD\u6d41\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u533a\u57df\u8f85\u52a9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u4fdd\u6301\u4e86\u7ed3\u6784\u4e0d\u53d8\u6027\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u79fb\u52a8\u5efa\u6a21\u3002"}}
{"id": "2508.19953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19953", "abs": "https://arxiv.org/abs/2508.19953", "authors": ["Rafael Cathomen", "Mayank Mittal", "Marin Vlastelica", "Marco Hutter"], "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors", "comment": "Accepted to CoRL 2025. For code and videos, please check:\n  https://leggedrobotics.github.io/d3-skill-discovery/", "summary": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards.", "AI": {"tldr": "\u4f7f\u7528\u6a21\u5757\u5316\u6846\u67b6\u8fdb\u884c\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\uff0c\u4ee5\u63d0\u9ad8\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u90e8\u7f72\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\uff08USD\uff09\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u90e8\u7f72\u6027\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316USD\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u72b6\n\u6001\u7a7a\u95f4\u56e0\u5b50\u5206\u89e3\u6765\u5b66\u4e60\u5206\u79bb\u7684\u6280\u80fd\u8868\u793a\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u56e0\u5b50\u5206\u914d\u4e0d\u540c\u7684\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u3002\u5f15\u5165\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u5f52\u7eb3\u504f\u7f6e\u4ee5\u83b7\u5f97\u7ed3\u6784\u5316\u7684\u3001\u53ef\u611f\u77e5\u7684\u6280\u80fd\uff0c\u5e76\u52a0\u5165\u98ce\u683c\u56e0\u5b50\u548c\u6b63\u5219\u5316\u60e9\u7f5a\u6765\u63d0\u9ad8\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u6a21\u62df\u73af\u5883\u4e2d\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u6280\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u786c\u4ef6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u56e0\u5b50\u5206\u89e3\u548c\u5bf9\u79f0\u6027\u80fd\u591f\u53d1\u73b0\u7ed3\u6784\u5316\u7684\u3001\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u800c\u98ce\u683c\u56e0\u5b50\u548c\u60e9\u7f5a\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\u3002\u5b66\u4e60\u5230\u7684\u6280\u80fd\u53ef\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u4e14\u8868\u73b0\u4e0e\u4f7f\u7528\u624b\u5de5\u8bbe\u8ba1\u5956\u52b1\u8bad\u7ec3\u7684\u9884\u8a00\u673a\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "\u56e0\u5b50\u5206\u89e3\u3001\u5bf9\u79f0\u6027\u3001\u98ce\u683c\u56e0\u5b50\u548c\u6b63\u5219\u5316\u60e9\u7f5a\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.19441", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19441", "abs": "https://arxiv.org/abs/2508.19441", "authors": ["Sanket Jantre", "Deepak Akhare", "Xiaoning Qian", "Nathan M. Urban"], "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models", "comment": null, "summary": "Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5177\u6837\u672c\u6548\u7387\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u4ece\u8ba1\u7b97\u673a\u6a21\u578b\u751f\u6210\u795e\u7ecfPDE\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u5c40\u90e8\u201c\u6a21\u677f\u201d\u72b6\u6001\u8fdb\u884c\u7a7a\u95f4\u586b\u5145\u91c7\u6837\uff0c\u4ee5\u514b\u670d\u8f68\u8ff9\u6570\u636e\u4e2d\u65f6\u7a7a\u5197\u4f59\uff0c\u5e76\u8fc7\u91c7\u6837\u53ef\u80fd\u7f55\u89c1\u7684\u4f46\u6709\u52a9\u4e8e\u6cdb\u5316\u7684\u72b6\u6001\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4ece\u76f8\u5f53\u4e8e10\u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u6570\u503c\u6a21\u62df\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u51c6\u786e\u7684\u795e\u7ecfPDE\u6a21\u677f\u7b97\u5b50\uff0c\u5e76\u4e14\u5982\u679c\u80fd\u83b7\u5f97\u5355\u6761\u5b8c\u6574\u7684\u8f68\u8ff9\u6a21\u62df\u6570\u636e\uff0c\u51c6\u786e\u6027\u4f1a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002\u4e0e\u7b80\u5355\u91c7\u6837\u8f68\u8ff9\u6570\u636e\u76f8\u6bd4\uff0c\u8be5\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5728\u591a\u4e2aPDE\u7cfb\u7edf\u4e0a\u8bad\u7ec3\u51fa\u7684\u795e\u7ecf\u6a21\u677f\u7b97\u5b50\u8868\u73b0\u66f4\u4f18\uff0c\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u503cPDE\u6c42\u89e3\u5668\u5728\u5904\u7406\u67d0\u4e9b\u7cfb\u7edf\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u5728\u5fae\u5206\u3001\u7ebf\u6027\u5316\u3001\u964d\u7ef4\u6216\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65f6\u3002\u5c06PDE\u6a21\u578b\u8868\u793a\u4e3a\u795e\u7ecfPDE\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u5c40\u90e8\u201c\u6a21\u677f\u201d\u72b6\u6001\u8fdb\u884c\u7a7a\u95f4\u586b\u5145\u91c7\u6837\u6765\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u51cf\u5c11\u6570\u636e\u4e2d\u7684\u65f6\u7a7a\u5197\u4f59\uff0c\u5e76\u8fc7\u91c7\u6837\u90a3\u4e9b\u53ef\u80fd\u7f55\u89c1\u4f46\u5bf9\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u7684\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u76f8\u5f53\u4e8e10\u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u6570\u503c\u6a21\u62df\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u51c6\u786e\u7684\u795e\u7ecfPDE\u6a21\u677f\u7b97\u5b50\u3002\u5f53\u7ed3\u5408\u4e00\u6761\u5b8c\u6574\u7684\u8f68\u8ff9\u6a21\u62df\u6570\u636e\u65f6\uff0c\u51c6\u786e\u6027\u5f97\u5230\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u4e0e\u7b80\u5355\u91c7\u6837\u8f68\u8ff9\u6570\u636e\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aPDE\u7cfb\u7edf\u4e0a\u5c55\u73b0\u51fa\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u80fd\u591f\u4ece\u66f4\u5c11\u7684\u6a21\u62df\u6570\u636e\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u795e\u7ecfPDE\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecfPDE\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u4f18\u7684\u795e\u7ecfPDE\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.19529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19529", "abs": "https://arxiv.org/abs/2508.19529", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "SFT\u4e0e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u534a\u81ea\u56de\u5f52\u63a8\u7406\u4e0d\u5339\u914d\uff0c\u63d0\u51faBlockwise SFT\uff0c\u901a\u8fc7\u5c06\u54cd\u5e94\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u3001\u6bcf\u6b21\u9009\u62e9\u4e00\u4e2a\u6d3b\u52a8\u5757\u8fdb\u884c\u968f\u673a\u63a9\u7801\u3001\u51bb\u7ed3\u5148\u524d\u6807\u8bb0\u5e76\u5b8c\u5168\u9690\u85cf\u672a\u6765\u6807\u8bb0\uff0c\u5e76\u5728\u6d3b\u52a8\u5757\u4e0a\u8ba1\u7b97\u635f\u5931\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660eBlockwise SFT\u5728GSM8K\u3001MATH\u548cMetaMathQA\u4e0a\u4f18\u4e8eSFT\u3002", "motivation": "\u6807\u51c6\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e0e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u534a\u81ea\u56de\u5f52\u63a8\u7406\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u68af\u5ea6\u504f\u79bb\u671f\u671b\u7684\u5757\u72b6\u4f3c\u7136\u3002", "method": "\u63d0\u51faBlockwise SFT\uff0c\u5c06\u54cd\u5e94\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u6bcf\u6b21\u9009\u62e9\u4e00\u4e2a\u6d3b\u52a8\u5757\u8fdb\u884c\u968f\u673a\u63a9\u7801\uff0c\u51bb\u7ed3\u5148\u524d\u6807\u8bb0\uff0c\u5b8c\u5168\u9690\u85cf\u672a\u6765\u6807\u8bb0\uff0c\u5e76\u4ec5\u5728\u6d3b\u52a8\u5757\u4e0a\u8ba1\u7b97\u635f\u5931\uff0c\u4ee5\u5339\u914d\u5757\u72b6\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u5728GSM8K\u3001MATH\u548cMetaMathQA\u6570\u636e\u96c6\u4e0a\uff0cBlockwise SFT\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u6216\u6807\u8bb0\u9884\u7b97\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u7684SFT\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6539\u8fdb\u6e90\u4e8e\u8bad\u7ec3-\u63a8\u7406\u5bf9\u9f50\uff0c\u800c\u975e\u5076\u7136\u7684\u63a9\u7801\u6548\u5e94\u3002", "conclusion": "\u5339\u914d\u76d1\u7763\u7684\u7c92\u5ea6\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\u5bf9\u4e8e\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.19783", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19783", "abs": "https://arxiv.org/abs/2508.19783", "authors": ["Ralph Adrian E. Farrales", "Eric A. Galapon"], "title": "Canonical pairs in finite-dimensional Hilbert space", "comment": null, "summary": "A pair of Hermitian operators is canonical if they satisfy the canonical\ncommutation relation. It has been believed that no such canonical pair exists\nin finite-dimensional Hilbert space. Here, we obtain canonical pairs by noting\nthat the canonical commutation relation holds in a proper subspace of the\nHilbert space. For a given Hilbert space, we study the many possible canonical\npairs and look into the uncertainty relation they satisfy. We apply our results\nby constructing time operators in finite-dimensional quantum mechanics.", "AI": {"tldr": "\u5728\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u5728proper\u5b50\u7a7a\u95f4\u4e2d\u6ee1\u8db3\u6b63\u5219\u5bf9\u6613\u5173\u7cfb\uff0c\u627e\u5230\u4e86\u6b63\u5219\u5bf9\u3002\u7814\u7a76\u4e86\u8fd9\u4e9b\u6b63\u5219\u5bf9\u6ee1\u8db3\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u6784\u9020\u6709\u9650\u7ef4\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u65f6\u95f4\u7b97\u7b26\u3002", "motivation": "\u63a2\u7a76\u5728\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u662f\u5426\u5b58\u5728\u6ee1\u8db3\u6b63\u5219\u5bf9\u6613\u5173\u7cfb\u7684\u6b63\u5219\u5bf9\u3002", "method": "\u901a\u8fc7\u5728proper\u5b50\u7a7a\u95f4\u4e2d\u6ee1\u8db3\u6b63\u5219\u5bf9\u6613\u5173\u7cfb\u6765\u6784\u9020\u6b63\u5219\u5bf9\uff0c\u5e76\u7814\u7a76\u5b83\u4eec\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u6700\u540e\u5e94\u7528\u4e8e\u6784\u9020\u6709\u9650\u7ef4\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u65f6\u95f4\u7b97\u7b26\u3002", "result": "\u6210\u529f\u6784\u9020\u4e86\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6b63\u5219\u5bf9\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u5728\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u5728proper\u5b50\u7a7a\u95f4\u4e2d\u6ee1\u8db3\u6b63\u5219\u5bf9\u6613\u5173\u7cfb\uff0c\u53ef\u4ee5\u627e\u5230\u6b63\u5219\u5bf9\uff0c\u5e76\u53ef\u7528\u4e8e\u6784\u9020\u65f6\u95f4\u7b97\u7b26\u3002"}}
{"id": "2508.19511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19511", "abs": "https://arxiv.org/abs/2508.19511", "authors": ["Alzayat Saleh", "Shunsuke Hatano", "Mostafa Rahimi Azghadi"], "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity", "comment": "19 pages, 10 figures, 6 tables", "summary": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u73b0\u5b9e\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u6761\u4ef6\u548c\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u800c\u5bfc\u81f4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u901a\u8fc7\u8bca\u65ad\u53d1\u73b0\u201c\u9634\u5f71\u504f\u5dee\u201d\u2014\u2014\u6a21\u578b\u5c06\u9634\u5f71\u8bef\u8bc6\u522b\u4e3a\u690d\u88ab\u2014\u2014\u5e76\u5229\u7528\u7ea6975\u4e2a\u6807\u8bb0\u548c10,000\u4e2a\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u6280\u672f\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u53ec\u56de\u7387\uff0c\u4ece\u800c\u6709\u6548\u7ba1\u7406\u5165\u4fb5\u6742\u8349\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u771f\u5b9e\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u73af\u5883\u6761\u4ef6\u548c\u9ad8\u6602\u7684\u6570\u636e\u6807\u6ce8\u6210\u672c\u5bfc\u81f4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5165\u4fb5\u6742\u8349\u7684\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bca\u65ad\u9a71\u52a8\u7684\u534a\u76d1\u7763\u6846\u67b6\u3002\u9996\u5148\uff0c\u4f7f\u7528\u7ea6975\u4e2a\u6807\u8bb0\u548c10,000\u4e2a\u672a\u6807\u8bb0\u7684\u7267\u8349\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\uff08\u5206\u7c7b\u4f7f\u7528ResNet\uff0c\u68c0\u6d4b\u4f7f\u7528YOLO\u3001RF-DETR\uff09\u3002\u7136\u540e\uff0c\u5229\u7528\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u8bca\u65ad\u51fa\u201c\u9634\u5f71\u504f\u5dee\u201d\uff0c\u5373\u6a21\u578b\u5c06\u9634\u5f71\u8bef\u8bc6\u522b\u4e3a\u690d\u88ab\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u7b49\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u589e\u5f3a\u6a21\u578b\u5bf9\u9634\u5f71\u504f\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u53ec\u56de\u7387\u3002", "result": "\u5728\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\uff0c\u5206\u7c7b\u548c\u68c0\u6d4b\u6a21\u578b\u7684F1\u5206\u6570\u6700\u9ad8\u53ef\u8fbe0.90\uff0cmAP50\u5206\u6570\u8d85\u8fc70.82\u3002\u901a\u8fc7\u534a\u76d1\u7763\u6846\u67b6\uff0c\u6a21\u578b\u5728\u5904\u7406\u201c\u9634\u5f71\u504f\u5dee\u201d\u95ee\u9898\u4e0a\u5f97\u5230\u6539\u5584\uff0c\u53ec\u56de\u7387\u5f97\u5230\u63d0\u5347\uff0c\u66f4\u6709\u6548\u5730\u51cf\u5c11\u4e86\u6742\u8349\u6f0f\u68c0\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u4e14\u7ecf\u8fc7\u7530\u95f4\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u3001\u8bca\u65ad\u548c\u6539\u8fdb\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u590d\u6742\u73b0\u5b9e\u7684\u9c81\u68d2\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u201c\u9634\u5f71\u504f\u5dee\u201d\u548c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19958", "abs": "https://arxiv.org/abs/2508.19958", "authors": ["Yiguo Fan", "Pengxiang Ding", "Shuanghao Bai", "Xinyang Tong", "Yuyang Zhu", "Hongchao Lu", "Fengqi Dai", "Wei Zhao", "Yang Liu", "Siteng Huang", "Zhaoxin Fan", "Badong Chen", "Donglin Wang"], "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation", "comment": "Accepted to CoRL 2025; Github Page: https://long-vla.github.io", "summary": "Vision-Language-Action (VLA) models have become a cornerstone in robotic\npolicy learning, leveraging large-scale multimodal data for robust and scalable\ncontrol. However, existing VLA frameworks primarily address short-horizon\ntasks, and their effectiveness on long-horizon, multi-step robotic manipulation\nremains limited due to challenges in skill chaining and subtask dependencies.\nIn this work, we introduce Long-VLA, the first end-to-end VLA model\nspecifically designed for long-horizon robotic tasks. Our approach features a\nnovel phase-aware input masking strategy that adaptively segments each subtask\ninto moving and interaction phases, enabling the model to focus on\nphase-relevant sensory cues and enhancing subtask compatibility. This unified\nstrategy preserves the scalability and data efficiency of VLA training, and our\narchitecture-agnostic module can be seamlessly integrated into existing VLA\nmodels. We further propose the L-CALVIN benchmark to systematically evaluate\nlong-horizon manipulation. Extensive experiments on both simulated and\nreal-world tasks demonstrate that Long-VLA significantly outperforms prior\nstate-of-the-art methods, establishing a new baseline for long-horizon robotic\ncontrol.", "AI": {"tldr": "Long-VLA\u662f\u9996\u4e2a\u7528\u4e8e\u957f\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u7aef\u5230\u7aefVLA\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9636\u6bb5\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\u6765\u5904\u7406\u6280\u80fd\u94fe\u63a5\u548c\u5b50\u4efb\u52a1\u4f9d\u8d56\uff0c\u5e76\u5728L-CALVIN\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709VLA\u6846\u67b6\u5728\u957f\u7a0b\u3001\u591a\u6b65\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u5904\u7406\u6280\u80fd\u94fe\u63a5\u548c\u5b50\u4efb\u52a1\u4f9d\u8d56\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLong-VLA\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684VLA\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u9896\u7684\u9636\u6bb5\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u5b50\u4efb\u52a1\u81ea\u9002\u5e94\u5730\u5206\u5272\u4e3a\u79fb\u52a8\u548c\u4ea4\u4e92\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u4e0e\u9636\u6bb5\u76f8\u5173\u7684\u611f\u77e5\u7ebf\u7d22\uff0c\u589e\u5f3a\u5b50\u4efb\u52a1\u517c\u5bb9\u6027\u3002\u8be5\u6a21\u5757\u53ef\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\u3002\u63d0\u51faL-CALVIN\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u957f\u7a0b\u64cd\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLong-VLA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e3a\u957f\u7a0b\u673a\u5668\u4eba\u63a7\u5236\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "Long-VLA\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u957f\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u8bbe\u8ba1\u7684\u7aef\u5230\u7aefVLA\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u9636\u6bb5\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u6280\u80fd\u94fe\u63a5\u548c\u5b50\u4efb\u52a1\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728L-CALVIN\u57fa\u51c6\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u957f\u7a0b\u673a\u5668\u4eba\u63a7\u5236\u5e26\u6765\u4e86\u65b0\u7684\u7a81\u7834\u3002"}}
{"id": "2508.19443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19443", "abs": "https://arxiv.org/abs/2508.19443", "authors": ["Paimon Goulart", "Shaan Pakala", "Evangelos Papalexakis"], "title": "Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization", "comment": null, "summary": "Producing large complex simulation datasets can often be a time and resource\nconsuming task. Especially when these experiments are very expensive, it is\nbecoming more reasonable to generate synthetic data for downstream tasks.\nRecently, these methods may include using generative machine learning models\nsuch as Generative Adversarial Networks or diffusion models. As these\ngenerative models improve efficiency in producing useful data, we introduce an\ninternal tensor decomposition to these generative models to even further reduce\ncosts. More specifically, for multidimensional data, or tensors, we generate\nthe smaller tensor factors instead of the full tensor, in order to\nsignificantly reduce the model's output and overall parameters. This reduces\nthe costs of generating complex simulation data, and our experiments show the\ngenerated data remains useful. As a result, tensor decomposition has the\npotential to improve efficiency in generative models, especially when\ngenerating multidimensional data, or tensors.", "AI": {"tldr": "\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u6570\u636e\u96c6\u65f6\uff0c\u53ef\u80fd\u5177\u6709\u6210\u672c\u6548\u76ca\u3002\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u751f\u6210\u591a\u7ef4\u6570\u636e\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u5927\u578b\u590d\u6742\u6a21\u62df\u6570\u636e\u96c6\u8017\u65f6\u8017\u529b\uff0c\u4e14\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u4f7f\u7528\u751f\u6210\u6a21\u578b\uff08\u5982GAN\u6216\u6269\u6563\u6a21\u578b\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u8d8a\u6765\u8d8a\u5408\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06\u5f20\u91cf\u5206\u89e3\uff08\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u591a\u7ef4\u6570\u636e\u7684\u6280\u672f\uff09\u96c6\u6210\u5230\u751f\u6210\u6a21\u578b\u4e2d\u3002\u901a\u8fc7\u751f\u6210\u8f83\u5c0f\u7684\u5f20\u91cf\u56e0\u5b50\u800c\u4e0d\u662f\u5b8c\u6574\u7684\u5f20\u91cf\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u7684\u8f93\u51fa\u548c\u603b\u4f53\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u964d\u4f4e\u6a21\u578b\u8f93\u51fa\u548c\u53c2\u6570\u6570\u91cf\u7684\u540c\u65f6\uff0c\u4ecd\u7136\u4fdd\u6301\u4e86\u5176\u53ef\u7528\u6027\u3002", "conclusion": "\u5f20\u91cf\u5206\u89e3\u6709\u6f5c\u529b\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u591a\u7ef4\u6570\u636e\uff08\u5f20\u91cf\uff09\u65f6\uff0c\u901a\u8fc7\u51cf\u5c11\u751f\u6210\u590d\u6742\u6a21\u62df\u6570\u636e\u6240\u9700\u7684\u6210\u672c\u3002"}}
{"id": "2508.19532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19532", "abs": "https://arxiv.org/abs/2508.19532", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u7528\u4f8b\u7684\u51c6\u786e\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u5316\u4ee3\u7801\u5757\u3001\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u548c\u8bfe\u7a0b\u5b66\u4e60\u6765\u6539\u8fdb\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684LLMs\u5728\u5de5\u5177\u8c03\u7528\u548c\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u6709\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u6027\u80fd\u63d0\u5347\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u4ee3\u7801\u7247\u6bb5\u62c6\u5206\u6210\u66f4\u5c0f\u7684\u3001\u66f4\u7cbe\u7ec6\u7684\u4ee3\u7801\u5757\uff0c\u4ece\u800c\u4ece\u76f8\u540c\u7684\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u521b\u5efa\u66f4\u591a\u6837\u5316\u7684DPO\u5bf9\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u62c6\u5206\u548c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u6765\u589e\u5f3aDPO\u8bad\u7ec3\u3002", "result": "\u5728HumanEval (+)\u3001MBPP (+)\u3001APPS\u3001LiveCodeBench\u548cBigCodeBench\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u7ec6\u5316\u4ee3\u7801\u5757\u3001AST\u62c6\u5206\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347DPO\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u7528\u4f8b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19793", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19793", "abs": "https://arxiv.org/abs/2508.19793", "authors": ["Hristo Tonchev", "Rosen Bahtev"], "title": "Grover's search with an oracle distinguishing between solutions", "comment": "23 pages, 14 figures", "summary": "Here we suggest a modification of Grover's algorithm, based on a multiphase\noracle which marks each solution with a different phase when there is more than\none solution. Such a modification can be used to maintain a high probability of\nfinding a solution for a number of iterations equal to or more than the one\nrequired by the deterministic Grover's algorithm (the one based on generalized\nHouseholder reflections). We use various semiempirical methods to show that the\ninterval of number of iterations for which the algorithm keeps the probability\nof finding solution high depends on the register size and the oracle phases.", "AI": {"tldr": "Grover\u7b97\u6cd5\u7684\u4fee\u6539\uff1a\u4f7f\u7528\u591a\u76f8Oracle\u6807\u8bb0\u591a\u4e2a\u89e3\uff0c\u4fdd\u6301\u9ad8\u67e5\u627e\u6982\u7387\u3002", "motivation": "\u5f53\u5b58\u5728\u591a\u4e2a\u89e3\u65f6\uff0c\u7ef4\u62a4\u627e\u5230\u89e3\u7684\u9ad8\u6982\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u76f8Oracle\u7684Grover\u7b97\u6cd5\u4fee\u6539\uff0c\u8be5Oracle\u4e3a\u6bcf\u4e2a\u89e3\u6807\u8bb0\u4e0d\u540c\u7684\u76f8\u4f4d\u3002", "result": "\u8bc1\u660e\u8be5\u4fee\u6539\u7b97\u6cd5\u53ef\u4ee5\u5728\u7b49\u4e8e\u6216\u8d85\u8fc7\u786e\u5b9a\u6027Grover\u7b97\u6cd5\u7684\u8fed\u4ee3\u6b21\u6570\u4e0b\uff0c\u4fdd\u6301\u9ad8\u67e5\u627e\u6982\u7387\uff0c\u4e14\u6982\u7387\u4fdd\u6301\u533a\u95f4\u53d7\u5bc4\u5b58\u5668\u5927\u5c0f\u548cOracle\u76f8\u4f4d\u5f71\u54cd\u3002", "conclusion": "\u8be5\u591a\u76f8Oracle\u4fee\u6539\u7684Grover\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u591a\u89e3\u95ee\u9898\uff0c\u5e76\u5728\u4e00\u5b9a\u8fed\u4ee3\u6b21\u6570\u8303\u56f4\u5185\u4fdd\u6301\u9ad8\u67e5\u627e\u6982\u7387\u3002"}}
{"id": "2508.19527", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "TAPO\u901a\u8fc7\u4f18\u5316\u6587\u672c\u4e0e\u52a8\u4f5c\u7684\u5bf9\u9f50\uff0cMotionFLUX\u901a\u8fc7\u786e\u5b9a\u6027\u6d41\u5339\u914d\u5b9e\u73b0\u5b9e\u65f6\u52a8\u4f5c\u751f\u6210\uff0c\u4e24\u8005\u7ed3\u5408\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5728\u8bed\u8a00\u63cf\u8ff0\u4e0e\u8fd0\u52a8\u8bed\u4e49\u7684\u7cbe\u786e\u5bf9\u9f50\u4ee5\u53ca\u591a\u6b65\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "TAPO\u6846\u67b6\u901a\u8fc7\u5bf9\u9f50\u7ec6\u5fae\u7684\u8fd0\u52a8\u53d8\u5316\u4e0e\u6587\u672c\u4fee\u9970\u7b26\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u8c03\u6574\u6765\u52a0\u5f3a\u8bed\u4e49\u57fa\u7840\u3002MotionFLUX\u662f\u4e00\u4e2a\u57fa\u4e8e\u786e\u5b9a\u6027\u6d41\u5339\u914d\u7684\u9ad8\u901f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u566a\u58f0\u5206\u5e03\u4e0e\u8fd0\u52a8\u7a7a\u95f4\u4e4b\u95f4\u6784\u5efa\u6700\u4f18\u4f20\u8f93\u8def\u5f84\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5408\u6210\u3002", "result": "TAPO\u548cMotionFLUX\u7684\u7ed3\u5408\u7cfb\u7edf\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u52a0\u5feb\u4e86\u751f\u6210\u901f\u5ea6\u3002", "conclusion": "TAPO\u548cMotionFLUX\u5171\u540c\u6784\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\uff0c\u5728\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u8d28\u91cf\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.20037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20037", "abs": "https://arxiv.org/abs/2508.20037", "authors": ["Henk H. A. Jekel", "Alejandro D\u00edaz Rosales", "Luka Peternel"], "title": "Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech", "comment": null, "summary": "The paper presents a visio-verbal teleimpedance interface for commanding 3D\nstiffness ellipsoids to the remote robot with a combination of the operator's\ngaze and verbal interaction. The gaze is detected by an eye-tracker, allowing\nthe system to understand the context in terms of what the operator is currently\nlooking at in the scene. Along with verbal interaction, a Visual Language Model\n(VLM) processes this information, enabling the operator to communicate their\nintended action or provide corrections. Based on these inputs, the interface\ncan then generate appropriate stiffness matrices for different physical\ninteraction actions. To validate the proposed visio-verbal teleimpedance\ninterface, we conducted a series of experiments on a setup including a Force\nDimension Sigma.7 haptic device to control the motion of the remote Kuka LBR\niiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,\nwhile human verbal commands are processed by a VLM using GPT-4o. The first\nexperiment explored the optimal prompt configuration for the interface. The\nsecond and third experiments demonstrated different functionalities of the\ninterface on a slide-in-the-groove task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u64cd\u4f5c\u5458\u6ce8\u89c6\u548c\u8bed\u97f3\u4ea4\u4e92\u7684\u89c6\u542c\u9065\u64cd\u4f5c\u63a5\u53e3\uff0c\u7528\u4e8e\u63a7\u5236\u8fdc\u7a0b\u673a\u5668\u4eba\u4ee5\u5b9e\u73b03D\u521a\u5ea6\u692d\u7403\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u64cd\u4f5c\u5458\u7684\u89c6\u89c9\u7126\u70b9\u548c\u8bed\u97f3\u6307\u4ee4\uff0c\u6765\u6539\u8fdb\u5bf9\u8fdc\u7a0b\u673a\u5668\u4eba\u521a\u5ea6\u63a7\u5236\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u8be5\u63a5\u53e3\u5229\u7528\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u68c0\u6d4b\u64cd\u4f5c\u5458\u7684\u6ce8\u89c6\u70b9\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5904\u7406\u8bed\u97f3\u6307\u4ee4\uff0c\u4ece\u800c\u751f\u6210\u7528\u4e8e\u7269\u7406\u4ea4\u4e92\u7684\u521a\u5ea6\u77e9\u9635\u3002", "result": "\u901a\u8fc7\u5728\u5305\u542bKuka LBR iiwa\u673a\u5668\u4eba\u548cForce Dimension Sigma.7\u89e6\u89c9\u8bbe\u5907\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u63a5\u53e3\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6ed1\u69fd\u4efb\u52a1\u5c55\u793a\u4e86\u5176\u529f\u80fd\u3002", "conclusion": "\u8be5\u89c6\u542c\u9065\u64cd\u4f5c\u63a5\u53e3\u80fd\u591f\u6709\u6548\u5730\u8ba9\u64cd\u4f5c\u5458\u901a\u8fc7\u6ce8\u89c6\u548c\u8bed\u97f3\u6307\u4ee4\u6765\u63a7\u5236\u8fdc\u7a0b\u673a\u5668\u4eba\u76843D\u521a\u5ea6\u692d\u7403\u3002"}}
{"id": "2508.19445", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19445", "abs": "https://arxiv.org/abs/2508.19445", "authors": ["Haozhe Jiang", "Nika Haghtalab"], "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from your model?", "comment": null, "summary": "Given a trained neural network, can any specified output be generated by some\ninput? Equivalently, does the network correspond to a function that is\nsurjective? In generative models, surjectivity implies that any output,\nincluding harmful or undesirable content, can in principle be generated by the\nnetworks, raising concerns about model safety and jailbreak vulnerabilities. In\nthis paper, we prove that many fundamental building blocks of modern neural\narchitectures, such as networks with pre-layer normalization and\nlinear-attention modules, are almost always surjective. As corollaries, widely\nused generative frameworks, including GPT-style transformers and diffusion\nmodels with deterministic ODE solvers, admit inverse mappings for arbitrary\noutputs. By studying surjectivity of these modern and commonly used neural\narchitectures, we contribute a formalism that sheds light on their unavoidable\nvulnerability to a broad class of adversarial attacks.", "AI": {"tldr": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecGPT\u548c\u6269\u6563\u6a21\u578b\uff09\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u662f\u53ef\u6ee1\u5c04\u7684\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u6307\u5b9a\u7684\u8f93\u51fa\uff0c\u8fd9\u5f15\u53d1\u4e86\u6a21\u578b\u5b89\u5168\u548c\u8d8a\u72f1\u6f0f\u6d1e\u7684\u62c5\u5fe7\u3002", "motivation": "\u7814\u7a76\u5728\u7ed9\u5b9a\u5df2\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\uff0c\u662f\u5426\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u6307\u5b9a\u7684\u8f93\u51fa\uff0c\u5373\u7f51\u7edc\u662f\u5426\u5bf9\u5e94\u4e00\u4e2a\u6ee1\u5c04\u51fd\u6570\uff0c\u4ee5\u53ca\u8fd9\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5bf9\u6a21\u578b\u5b89\u5168\u548c\u8d8a\u72f1\u6f0f\u6d1e\u7684\u542f\u793a\u3002", "method": "\u8bc1\u660e\u4e86\u8bb8\u591a\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840\u6a21\u5757\uff08\u5982\u5177\u6709\u9884\u5c42\u5f52\u4e00\u5316\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u5757\u7684\u7f51\u7edc\uff09\u51e0\u4e4e\u603b\u662f\u53ef\u6ee1\u5c04\u7684\u3002", "result": "\u4f5c\u4e3a\u63a8\u8bba\uff0c\u5305\u62ecGPT\u98ce\u683c\u7684Transformer\u548c\u5177\u6709\u786e\u5b9a\u6027ODE\u6c42\u89e3\u5668\u7684\u6269\u6563\u6a21\u578b\u5728\u5185\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u5141\u8bb8\u4efb\u610f\u8f93\u51fa\u7684\u9006\u6620\u5c04\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u8fd9\u4e9b\u73b0\u4ee3\u548c\u5e38\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u53ef\u6ee1\u5c04\u6027\uff0c\u4e3a\u63ed\u793a\u5176\u5728\u5e7f\u6cdb\u7684\u5bf9\u6297\u6027\u653b\u51fb\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.19533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u65e0\u76d1\u7763\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08UERC\uff09\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u539f\u578b\u7684ProEmoTrans\u60c5\u611f\u8fc1\u79fb\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709ERC\u7814\u7a76\u7684\u95ed\u57df\u5047\u8bbe\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u901a\u8fc7LLM\u589e\u5f3a\u63cf\u8ff0\u6765\u5904\u7406\u6a21\u7cca\u60c5\u611f\u5b9a\u4e49\uff0c\u91c7\u7528\u65e0\u53c2\u6570\u673a\u5236\u9ad8\u6548\u7f16\u7801\u957f\u5bf9\u8bdd\u5e76\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5e76\u6539\u8fdb\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801\uff08AVD\uff09\u65b9\u6cd5\u6765\u8fc1\u79fb\u60c5\u611f\u8f6c\u6362\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u65b0\u9886\u57df\u7684\u521d\u6b65\u63a2\u7d22\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u7814\u7a76\u4e3b\u8981\u9075\u5faa\u95ed\u57df\u5047\u8bbe\uff0c\u4f46\u5fc3\u7406\u5b66\u754c\u5bf9\u60c5\u611f\u5206\u7c7b\u5c1a\u65e0\u7edf\u4e00\u6807\u51c6\uff0c\u8fd9\u7ed9\u6a21\u578b\u8bc6\u522b\u73b0\u5b9e\u4e16\u754c\u4e2d\u672a\u89c1\u8fc7\u7684\u60c5\u611f\u5e26\u6765\u4e86\u6311\u6218\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProEmoTrans\u7684\u57fa\u4e8e\u539f\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08UERC\uff09\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a1. \u901a\u8fc7LLM\u589e\u5f3a\u63cf\u8ff0\u6765\u5904\u7406\u9690\u542b\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4ee5\u5e94\u5bf9\u6a21\u7cca\u7684\u60c5\u611f\u5b9a\u4e49\uff1b2. \u91c7\u7528\u4e00\u79cd\u65e0\u53c2\u6570\u673a\u5236\u6765\u9ad8\u6548\u7f16\u7801\u957f\u5bf9\u8bdd\u5e76\u9632\u6b62\u8fc7\u62df\u5408\uff1b3. \u6539\u8fdb\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801\uff08AVD\uff09\u65b9\u6cd5\u6765\u8fc1\u79fb\u5df2\u89c1\u60c5\u611f\u7684\u8f6c\u6362\u6a21\u5f0f\u81f3\u672a\u89c1\u60c5\u611f\uff0c\u4ee5\u89e3\u51b3\u60c5\u611f\u7684\u9a6c\u5c14\u53ef\u592b\u6d41\u52a8\u6027\u8d28\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u672a\u89c1\u60c5\u611f\u7684\u8bc6\u522b\uff0c\u5e76\u4e3a\u8fd9\u4e00\u65b0\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u53ef\u4f5c\u4e3a\u521d\u6b65\u63a2\u7d22\u7684\u5f3a\u6709\u529b\u57fa\u7ebf\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u65e0\u76d1\u7763\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08UERC\uff09\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86ProEmoTrans\u6846\u67b6\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u63cf\u8ff0\u3001\u65e0\u53c2\u6570\u7f16\u7801\u673a\u5236\u548c\u6539\u8fdb\u7684AVD\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u89c1\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u5728\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.19877", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.19877", "abs": "https://arxiv.org/abs/2508.19877", "authors": ["Mohsen Rahmani Haghighi", "Mohammad Hossein Zarei"], "title": "Partial Anyon Condensation in the Color Code: A Hamiltonian Approach", "comment": "Submitted to Physical Review B ; 12 pages, 7 figures", "summary": "Lattice Hamiltonians, which can be tuned between different topological\nphases, are known as important tools for understanding physical mechanism\nbehind topological phase transitions. In this paper, we introduce a perturbed\nColor Code Hamiltonian with a rich phase structure which can be well matched to\nthe mechanism of anyon condensation in the Color Code. We consider Color Code\nmodel defined on a three-colorable hexagonal lattice and add Ising interactions\nbetween spins corresponding to edges of the lattice. We show that Ising\ninteractions play the role of physical factor for condensing anyons in the\nColor Code. In particular, corresponding to three different colors of edges in\nthe hexagonal lattice, we consider three different coupling parameters. Then,\nwe are able to condense anyons with different colors by tuning power of Ising\ninteractions in the corresponding edges. In particular, we explicitly show that\ncondensation of one type of anyons in the Color Code leads to a phase\ntransition to the Toric Code state. On the other hand, by condensing two types\nof anyons, we observe a phase transition to a modified version of the Toric\nCode where partial set of anyons in the Toric Code are condensed and we call it\na partially topological phase. Our main method for derivation of the above\nresults is based on a suitable basis transformation on the main Hamiltonian in\nthe sense that our model is mapped onto three decoupled transverse-field Ising\nmodels, corresponding to the three colors. We use the above mapping to analyze\nbehavior of string order parameters as non-local indicators of topological\norder. We introduce three string order parameters that can well characterize\ndifferent phases of the model. Specifically we give a simple description of the\npartially condensed phase by using string order parameters.", "AI": {"tldr": "\u901a\u8fc7\u7814\u7a76\u53ef\u8c03\u8c10\u683c\u5b50\u54c8\u5bc6\u987f\u91cf\uff0c\u6211\u4eec\u53d1\u73b0\u52a0\u5165\u7684\u4f0a\u8f9b\u76f8\u4e92\u4f5c\u7528\u53ef\u4ee5\u6a21\u62df\u4efb\u610f\u5b50\u51dd\u805a\uff0c\u5e76\u5bfc\u81f4\u76f8\u53d8\u5230\u6258\u5229\u7801\u6001\u6216\u90e8\u5206\u62d3\u6251\u76f8\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u62d3\u6251\u76f8\u53d8\u80cc\u540e\u7684\u7269\u7406\u673a\u5236\uff0c\u9700\u8981\u7814\u7a76\u53ef\u8c03\u8c10\u7684\u683c\u5b50\u54c8\u5bc6\u987f\u91cf\u3002", "method": "\u901a\u8fc7\u5bf9\u4e3b\u54c8\u5bc6\u987f\u91cf\u8fdb\u884c\u57fa\u53d8\u6362\uff0c\u5c06\u6a21\u578b\u6620\u5c04\u5230\u4e09\u4e2a\u89e3\u8026\u7684\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5b57\u7b26\u4e32\u5e8f\u53c2\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u51dd\u805a\u4e00\u79cd\u4efb\u610f\u5b50\u4f1a\u5bfc\u81f4\u76f8\u53d8\u5230\u6258\u5229\u7801\u6001\uff1b\u51dd\u805a\u4e24\u79cd\u4efb\u610f\u5b50\u4f1a\u5bfc\u81f4\u76f8\u53d8\u5230\u90e8\u5206\u62d3\u6251\u76f8\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u4f0a\u8f9b\u76f8\u4e92\u4f5c\u7528\u7684\u5f3a\u5ea6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0d\u540c\u4efb\u610f\u5b50\u7684\u51dd\u805a\uff0c\u5e76\u5b9e\u73b0\u5230\u6258\u5229\u7801\u6001\u6216\u90e8\u5206\u62d3\u6251\u76f8\u7684\u76f8\u53d8\u3002"}}
{"id": "2508.19542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CVBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u591a\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08MLLM\uff09\u5728\u5904\u7406\u8de8\u89c6\u9891\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u56e0\u679c\u63a8\u7406\u548c\u8de8\u89c6\u9891\u4e0a\u4e0b\u6587\u4fdd\u7559\u65b9\u9762\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08MLLM\uff09\u5728\u5904\u7406\u8de8\u591a\u4e2a\u89c6\u9891\u5185\u5bb9\u65f6\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5bf9\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCVBench\u7684\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u4e86\u8de8\u89c6\u9891\u5bf9\u8c61\u5173\u8054\u3001\u4e8b\u4ef6\u5173\u8054\u548c\u590d\u6742\u63a8\u7406\u4e09\u4e2a\u5c42\u6b21\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u5982\u4f53\u80b2\u3001\u751f\u6d3b\u8bb0\u5f55\uff09\u7684\u89c6\u9891\u96c6\u3002\u5728CVBench\u4e0a\u5bf9\u5305\u62ecGPT-4o\u3001Gemini-2.0-flash\u3001Qwen2.5-VL\u5728\u5185\u768410\u591a\u4e2a\u9886\u5148MLLM\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u6216\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u8bc4\u4f30\u3002", "result": "\u5728CVBench\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u50cfGPT-4o\u8fd9\u6837\u7684\u9876\u5c16\u6a21\u578b\uff0c\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u53ea\u670960%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768491%\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dMLLM\u67b6\u6784\u5b58\u5728\u56fa\u6709\u7684\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u8de8\u89c6\u9891\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u91cd\u53e0\u5b9e\u4f53\u6d88\u6b67\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "CVBench\u4e3a\u8bca\u65ad\u548c\u6539\u8fdb\u591a\u89c6\u9891\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3MLLM\u7684\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u89e3\u51b3\u8de8\u89c6\u9891\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u5b9e\u4f53\u6d88\u6b67\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.20085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20085", "abs": "https://arxiv.org/abs/2508.20085", "authors": ["Zhecheng Yuan", "Tianming Wei", "Langzhe Gu", "Pu Hua", "Tianhai Liang", "Yuanpei Chen", "Huazhe Xu"], "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation", "comment": null, "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19458", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19458", "abs": "https://arxiv.org/abs/2508.19458", "authors": ["Mahdi Haghifam", "Adam Smith", "Jonathan Ullman"], "title": "The Sample Complexity of Membership Inference and Privacy Auditing", "comment": "58 Pages", "summary": "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5047\u8bbe\u5b66\u4e60\u7b97\u6cd5\u7684\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u9ad8\u65af\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\u6240\u9700\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u5728\u6210\u5458\u63a8\u7406\u653b\u51fb\u4e2d\uff0c\u653b\u51fb\u8005\u9700\u8981\u591a\u5c11\u5173\u4e8e\u6570\u636e\u5206\u5e03\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u5728\u4ece\u9ad8\u65af\u5206\u5e03$\\\\mathcal{N}(\\\\mu, \\\\Sigma)$\u4e2d\u62bd\u53d6n\u4e2a\u6837\u672c\u5e76\u4f30\u8ba1$\\\\hat{\\\\mu}$\u7684\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u5206\u6790\u4e86\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\u6240\u9700\u7684\u6700\u5c0f\u53c2\u8003\u6837\u672c\u6570\u91cf\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u6210\u5458\u63a8\u7406\u653b\u51fb\u53ef\u80fd\u9700\u8981$\\\\Omega(n + n^2 \\\\rho^2)$\u4e2a\u6837\u672c\uff0c\u8fd9\u53ef\u80fd\u6bd4\u8bad\u7ec3\u7b97\u6cd5\u4f7f\u7528\u7684\u6837\u672c\u91cf\u66f4\u591a\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u653b\u51fb\u65b9\u6cd5\u53ef\u80fd\u4f4e\u4f30\u4e86\u6210\u5458\u63a8\u7406\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u653b\u51fb\u8005\u6240\u9700\u7684\u53c2\u8003\u6837\u672c\u91cf\u53ef\u80fd\u6bd4\u9884\u671f\u7684\u8981\u591a\uff0c\u7279\u522b\u662f\u5f53\u653b\u51fb\u8005\u80fd\u591f\u83b7\u5f97\u5927\u91cf\u5173\u4e8e\u6570\u636e\u5206\u5e03\u7684\u4fe1\u606f\u65f6\uff0c\u8fd9\u53ef\u80fd\u4fc3\u4f7f\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u3002"}}
{"id": "2508.19546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19546", "abs": "https://arxiv.org/abs/2508.19546", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Language Models Identify Ambiguities and Exploit Loopholes", "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "LLMs can identify and exploit ambiguous instructions to achieve their goals, presenting a potential AI safety risk. Both closed-source and stronger open-source models exhibit this behavior, indicating a need for further research into AI alignment and safety.", "motivation": "The paper investigates how Large Language Models (LLMs) respond to loopholes, which offers insights into their understanding of ambiguity and pragmatics, and presents a novel alignment problem due to conflicting goals.", "method": "The researchers designed scenarios where LLMs were given a goal and a conflicting, ambiguous user instruction. They then measured the models' ability to exploit these loopholes to satisfy their own goals over the user's.", "result": "Both closed-source and stronger open-source LLMs demonstrated the ability to identify ambiguities and exploit loopholes. The analysis showed that these models explicitly reason about ambiguity and conflicting goals when exploiting loopholes.", "conclusion": "LLMs' ability to exploit loopholes highlights a potential AI safety risk. Models that exploit loopholes do so by explicitly identifying and reasoning about both ambiguity and conflicting goals."}}
{"id": "2508.19890", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19890", "abs": "https://arxiv.org/abs/2508.19890", "authors": ["Oliver Hahn", "Ryuji Takagi"], "title": "Measuring non-Gaussianity with Correlation", "comment": "21 pages, 6 figures", "summary": "Quantum non-Gaussianity is a key resource for quantum advantage in\ncontinuous-variable systems. We introduce a general framework to quantify\nnon-Gaussianity based on correlation generation: two copies of a state become\ncorrelated at a $50{:}50$ beam splitter if and only if the state is\nnon-Gaussian, with correlations reducing to entanglement in the pure-state\ncase. This connection enables operational measures of non-Gaussianity, defined\nthrough correlation quantifiers such as R\\'enyi-$\\alpha$ entropy for pure\nstates and R\\'enyi-$\\alpha$ mutual information for mixed states. We prove that\nall such measures are monotonic under Gaussian channels. Building on this\nframework, we propose a sample-efficient experimental protocol to estimate\nnon-Gaussianity using standard optical components, even in the state agnostic\nsetting. Finally, we establish a lower bound on the sample complexity of\nestimating Wigner negativity, allowing a direct comparison with our protocol.\nOur results provide both a unifying theoretical framework for non-Gaussianity\nand a practical route toward its experimental quantification.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5173\u8054\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u975e\u9ad8\u65af\u6027\u3002\u8be5\u6846\u67b6\u8868\u660e\uff0c\u5f53\u4e14\u4ec5\u5f53\u4e00\u4e2a\u72b6\u6001\u662f\u975e\u9ad8\u65af\u6001\u65f6\uff0c\u5176\u4e24\u4e2a\u526f\u672c\u572850:50\u5206\u675f\u5668\u4e0a\u4f1a\u4ea7\u751f\u5173\u8054\uff0c\u5e76\u4e14\u5728\u7eaf\u6001\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u5173\u8054\u4f1a\u51cf\u5c11\u4e3a\u7ea0\u7f20\u3002\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u5173\u8054\u91cf\u5316\u5668\u7684\u64cd\u4f5c\u6027\u975e\u9ad8\u65af\u6027\u5ea6\u91cf\uff0c\u5982\u7eaf\u6001\u7684R\u00e9nyi-\u03b1\u71b5\u548c\u6df7\u5408\u6001\u7684R\u00e9nyi-\u03b1\u4e92\u4fe1\u606f\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u5ea6\u91cf\u5728\u9ad8\u65af\u4fe1\u9053\u4e0b\u7684\u5355\u8c03\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u6548\u7387\u9ad8\u7684\u5b9e\u9a8c\u65b9\u6848\uff0c\u7528\u4e8e\u4f30\u8ba1\u975e\u9ad8\u65af\u6027\uff0c\u5373\u4f7f\u5728\u72b6\u6001\u4e0d\u53ef\u77e5\u7684\u60c5\u51b5\u4e0b\u4e5f\u9002\u7528\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u4e3a\u4f30\u8ba1Wigner\u8d1f\u6027\u8bbe\u5b9a\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u4e0b\u754c\uff0c\u4ee5\u4fbf\u4e0e\u6240\u63d0\u51fa\u7684\u65b9\u6848\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u975e\u9ad8\u65af\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4e3a\u5176\u5b9e\u9a8c\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\u3002", "motivation": "\u91cf\u5316\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u975e\u9ad8\u65af\u6027\uff0c\u56e0\u4e3a\u975e\u9ad8\u65af\u6027\u662f\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u7684\u5173\u952e\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5173\u8054\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u91cf\u5316\u975e\u9ad8\u65af\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u8bc1\u660e\u5f53\u4e14\u4ec5\u5f53\u72b6\u6001\u662f\u975e\u9ad8\u65af\u6001\u65f6\uff0c\u5176\u4e24\u4e2a\u526f\u672c\u572850:50\u5206\u675f\u5668\u4e0a\u4f1a\u4ea7\u751f\u5173\u8054\u30022. \u5f15\u5165\u57fa\u4e8e\u5173\u8054\u91cf\u5316\u5668\uff08\u5982R\u00e9nyi-\u03b1\u71b5\u548cR\u00e9nyi-\u03b1\u4e92\u4fe1\u606f\uff09\u7684\u64cd\u4f5c\u6027\u975e\u9ad8\u65af\u6027\u5ea6\u91cf\u30023. \u8bc1\u660e\u8fd9\u4e9b\u5ea6\u91cf\u5728\u9ad8\u65af\u4fe1\u9053\u4e0b\u7684\u5355\u8c03\u6027\u30024. \u63d0\u51fa\u4e00\u79cd\u6837\u672c\u6548\u7387\u9ad8\u7684\u5b9e\u9a8c\u534f\u8bae\u6765\u4f30\u8ba1\u975e\u9ad8\u65af\u6027\u30025. \u5efa\u7acb\u4f30\u8ba1Wigner\u8d1f\u6027\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\u3002", "result": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u975e\u9ad8\u65af\u6027\u30022. \u5f15\u5165\u4e86\u57fa\u4e8e\u5173\u8054\u751f\u6210\u7684\u5ea6\u91cf\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u9ad8\u65af\u4fe1\u9053\u4e0b\u7684\u5355\u8c03\u6027\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u6548\u7387\u9ad8\u7684\u5b9e\u9a8c\u65b9\u6848\u6765\u4f30\u8ba1\u975e\u9ad8\u65af\u6027\u30024. \u5efa\u7acb\u4e86\u4f30\u8ba1Wigner\u8d1f\u6027\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u5e76\u4e0e\u6240\u63d0\u51fa\u7684\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u91cf\u5316\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u975e\u9ad8\u65af\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7684\u5b9e\u9a8c\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.19544", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19544", "abs": "https://arxiv.org/abs/2508.19544", "authors": ["Eduardo Davalos", "Yike Zhang", "Namrata Srivastava", "Yashvitha Thatigotla", "Jorge A. Salas", "Sara McFadden", "Sun-Joo Cho", "Amanda Goodwin", "Ashwin TS", "Gautam Biswas"], "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization", "comment": "9 pages, 7 figures, 1 table", "summary": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.", "AI": {"tldr": "WebEyeTrack\u662f\u4e00\u4e2a\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u773c\u52a8\u8ffd\u8e2a\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AI\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u65f6\u95f4\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5934\u90e8\u79fb\u52a8\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709AI\u773c\u52a8\u8ffd\u8e2a\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u6a21\u578b\u5927\u3001\u63a8\u7406\u65f6\u95f4\u957f\u3001\u9690\u79c1\u95ee\u9898\u4ee5\u53ca\u7cbe\u5ea6\u4e0d\u8db3\uff08\u5c24\u5176\u5728\u5934\u90e8\u79fb\u52a8\u65f6\uff09\u7684\u7f3a\u70b9\uff0c\u4e0e\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5dee\u8ddd\u3002WebEyeTrack\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "WebEyeTrack\u5c06\u8f7b\u91cf\u7ea7SOTA\uff08State-Of-The-Art\uff09\u773c\u52a8\u8ffd\u8e2a\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u6d4f\u89c8\u5668\u4e2d\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5934\u90e8\u59ff\u6001\u4f30\u8ba1\u548c\u8bbe\u5907\u7aef\u5c11\u6837\u672c\u5b66\u4e60\uff08\u4f7f\u7528\u5c11\u81f3\u4e5d\u4e2a\u6821\u51c6\u6837\u672c\uff09\uff0c\u80fd\u591f\u9002\u5e94\u65b0\u7528\u6237\u3002", "result": "WebEyeTrack\u5728GazeCapture\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86SOTA\u6027\u80fd\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a2.32\u5398\u7c73\uff0c\u5e76\u5728iPhone 14\u4e0a\u5b9e\u73b0\u4e862.4\u6beb\u79d2\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "WebEyeTrack\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u6ce8\u91cd\u9690\u79c1\u7684\u773c\u52a8\u8ffd\u8e2a\u6846\u67b6\uff0c\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7528\u6237\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.20095", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20095", "abs": "https://arxiv.org/abs/2508.20095", "authors": ["Jinhao Liang", "Sven Koenig", "Ferdinando Fioretto"], "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning", "comment": null, "summary": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u79bb\u6563\u5f15\u5bfc\u6269\u6563\uff08DGD\uff09\u7684\u65b0\u578b\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u79bb\u6563MAPF\u6c42\u89e3\u5668\u548c\u7ea6\u675f\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u79bb\u6563MAPF\u65b9\u6cd5\u56e0\u7c97\u7c92\u5ea6\u79bb\u6563\u5316\u5bfc\u81f4\u8f68\u8ff9\u8d28\u91cf\u53d7\u9650\uff0c\u4ee5\u53ca\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\u56e0\u7ef4\u5ea6\u707e\u96be\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "DGD\u6846\u67b6\u5c06\u975e\u51f8MRMP\u95ee\u9898\u5206\u89e3\u4e3a\u5177\u6709\u51f8\u6784\u578b\u7a7a\u95f4\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u79bb\u6563MAPF\u89e3\u51b3\u65b9\u6848\u3001\u7ea6\u675f\u4f18\u5316\u6280\u672f\u548c\u8f7b\u91cf\u7ea7\u7ea6\u675f\u4fee\u590d\u673a\u5236\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u5e76\u786e\u4fdd\u8f68\u8ff9\u53ef\u884c\u6027\u3002", "result": "DGD\u5728\u5927\u578b\u590d\u6742\u73af\u5883\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53ef\u6269\u5c55\u81f3100\u4e2a\u673a\u5668\u4eba\uff0c\u5e76\u5b9e\u73b0\u4e86\u89c4\u5212\u6548\u7387\u548c\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "DGD\u6846\u67b6\u901a\u8fc7\u6574\u5408\u79bb\u6563MAPF\u548c\u7ea6\u675f\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8f68\u8ff9\u8d28\u91cf\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2508.19466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19466", "abs": "https://arxiv.org/abs/2508.19466", "authors": ["Sourav Chakraborty", "Amit Kiran Rege", "Claire Monteleoni", "Lijun Chen"], "title": "Incentivized Lipschitz Bandits", "comment": null, "summary": "We study incentivized exploration in multi-armed bandit (MAB) settings with\ninfinitely many arms modeled as elements in continuous metric spaces. Unlike\nclassical bandit models, we consider scenarios where the decision-maker\n(principal) incentivizes myopic agents to explore beyond their greedy choices\nthrough compensation, but with the complication of reward drift--biased\nfeedback arising due to the incentives. We propose novel incentivized\nexploration algorithms that discretize the infinite arm space uniformly and\ndemonstrate that these algorithms simultaneously achieve sublinear cumulative\nregret and sublinear total compensation. Specifically, we derive regret and\ncompensation bounds of $\\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the\ncovering dimension of the metric space. Furthermore, we generalize our results\nto contextual bandits, achieving comparable performance guarantees. We validate\nour theoretical findings through numerical simulations.", "AI": {"tldr": "\u5728\u65e0\u9650\u81c2\u7684\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u73af\u5883\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5e26\u6fc0\u52b1\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u5176\u4e2d\u624b\u81c2\u662f\u8fde\u7eed\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u5143\u7d20\u3002\u4e0e\u7ecf\u5178\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u8003\u8651\u4e86\u51b3\u7b56\u8005\uff08\u4e3b\u4f53\uff09\u901a\u8fc7\u8865\u507f\u6fc0\u52b1\u77ed\u89c6\u7684\u4ee3\u7406\u4eba\u8fdb\u884c\u63a2\u7d22\uff0c\u4f46\u5b58\u5728\u5956\u52b1\u6f02\u79fb\uff08\u6fc0\u52b1\u5f15\u8d77\u7684\u53cd\u9988\u504f\u5dee\uff09\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e26\u6fc0\u52b1\u63a2\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u65e0\u9650\u624b\u81c2\u7a7a\u95f4\u5747\u5300\u79bb\u6563\u5316\uff0c\u5e76\u8bc1\u660e\u8be5\u7b97\u6cd5\u540c\u65f6\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u7d2f\u79ef\u9057\u61be\u548c\u6b21\u7ebf\u6027\u603b\u8865\u507f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u7684\u9057\u61be\u548c\u8865\u507f\u754c\u9650\u4e3a $\\Tilde{O}(T^{d+1/d+2})$\uff0c\u5176\u4e2d $d$ \u662f\u5ea6\u91cf\u7a7a\u95f4\u7684\u8986\u76d6\u7ef4\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u7ed3\u679c\u63a8\u5e7f\u5230\u4e0a\u4e0b\u6587\u8001\u864e\u673a\uff0c\u5e76\u53d6\u5f97\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u6211\u4eec\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u5728\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u8bbe\u7f6e\u4e2d\uff0c\u51b3\u7b56\u8005\uff08\u4e3b\u4f53\uff09\u5e0c\u671b\u6fc0\u52b1\u77ed\u89c6\u7684\u4ee3\u7406\u4eba\u8fdb\u884c\u63a2\u7d22\uff0c\u4f46\u9700\u8981\u5904\u7406\u7531\u6fc0\u52b1\u5f15\u8d77\u7684\u5956\u52b1\u6f02\u79fb\uff08\u53cd\u9988\u504f\u5dee\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e26\u6fc0\u52b1\u63a2\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u65e0\u9650\u624b\u81c2\u7a7a\u95f4\u5747\u5300\u79bb\u6563\u5316\uff0c\u5e76\u63a8\u5bfc\u4e86\u9057\u61be\u548c\u8865\u507f\u7684\u754c\u9650\u3002", "result": "\u8be5\u7b97\u6cd5\u540c\u65f6\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u7d2f\u79ef\u9057\u61be\u548c\u6b21\u7ebf\u6027\u603b\u8865\u507f\uff0c\u5176\u754c\u9650\u4e3a $\\Tilde{O}(T^{d+1/d+2})$\uff0c\u5176\u4e2d $d$ \u662f\u5ea6\u91cf\u7a7a\u95f4\u7684\u8986\u76d6\u7ef4\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e26\u6fc0\u52b1\u63a2\u7d22\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u8df5\u4e2d\u90fd\u80fd\u6709\u6548\u5730\u5904\u7406\u65e0\u9650\u624b\u81c2MAB\u8bbe\u7f6e\u4e2d\u7684\u5956\u52b1\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u3002"}}
{"id": "2508.19578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19578", "abs": "https://arxiv.org/abs/2508.19578", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u7ea7\u5173\u952e\u4e8b\u5b9e\u7ed3\u6784\u548c\u67e5\u8be2\u7126\u70b9\u6458\u8981\u6765\u8bc4\u4f30\u6a21\u578b\u3002\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff08>90%\uff09\uff0c\u6210\u672c\u964d\u4f4e25\u500d\u3002\u7ed3\u679c\u8868\u660eLLM\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\uff08\u5c24\u5176\u662f\u53f6\u7ea7\u522b\uff09\u548c\u201c\u4e2d\u95f4\u9057\u5931\u201d\u95ee\u9898\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u5206\u6790\u6027\u67e5\u8be2\u6bd4\u53d9\u8ff0\u6027\u67e5\u8be2\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e14\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u548c\u6a21\u578b\u89c4\u6a21\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u5176\u5728\u7406\u89e3\u957f\u6587\u672c\u65f6\u7684\u5177\u4f53\u6311\u6218\u3002", "method": "HAMLET\u6846\u67b6\u5c06\u6e90\u6587\u672c\u6784\u5efa\u4e3a\u6839\u3001\u5206\u652f\u3001\u53f6\u4e09\u4e2a\u5c42\u7ea7\u5173\u952e\u4e8b\u5b9e\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u67e5\u8be2\u7126\u70b9\u6458\u8981\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u6bcf\u4e2a\u5c42\u7ea7\u56de\u5fc6\u548c\u5fe0\u5b9e\u5448\u73b0\u4fe1\u606f\u7684\u80fd\u529b\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5168\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4e13\u5bb6\u4eba\u7c7b\u5224\u65ad\u8d85\u8fc790%\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5c06\u6210\u672c\u964d\u4f4e\u4e8625\u500d\u3002", "result": "LLM\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u65b9\u9762\uff0c\u7279\u522b\u662f\u5728\u53f6\u7ea7\u522b\uff0c\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5bf9\u201c\u4e2d\u95f4\u9057\u5931\u201d\u7b49\u4f4d\u7f6e\u6548\u5e94\u654f\u611f\u3002\u5206\u6790\u6027\u67e5\u8be2\u6bd4\u53d9\u8ff0\u6027\u67e5\u8be2\u66f4\u5177\u6311\u6218\u6027\u3002\u5f00\u6e90\u6a21\u578b\u4e0e\u4e13\u6709\u6a21\u578b\u4e4b\u95f4\u4ee5\u53ca\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "HAMLET\u6846\u67b6\u80fd\u591f\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u5730\u8bc4\u4f30LLM\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86LLM\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5305\u62ec\u5bf9\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3001\u4f4d\u7f6e\u6548\u5e94\u548c\u67e5\u8be2\u7c7b\u578b\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u6a21\u578b\u7c7b\u578b\u548c\u89c4\u6a21\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.19943", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19943", "abs": "https://arxiv.org/abs/2508.19943", "authors": ["Nhat A. Nghiem"], "title": "Towards quantum topological data analysis: torsion detection", "comment": null, "summary": "Topological data analysis (TDA) has become an attractive area for the\napplication of quantum computing. Recent advances have uncovered many\ninteresting connections between the two fields. On one hand, complexity\ntheoretic results show that estimating Betti numbers, a central task in TDA, is\nNP hard, indicating that a generic quantum speedup is unlikely. On the other\nhand, several recent studies have explored structured, less generic settings\nand demonstrated that quantum algorithms can still achieve significant speedups\nunder certain conditions. To date, most of these efforts have focused on Betti\nnumbers, which are topological invariants capturing the intrinsic connectivity\nand holes in a dataset. However, there is another important feature of\ntopological spaces: torsion. Torsion represents a distinct component of\nhomology that can reveal richer structural information. In this work, we\nintroduce a quantum algorithm for torsion detection, that is, determining\nwhether a given simplicial complex contains torsion. Our algorithm, assisted by\na low complexity classical procedure, can succeed with high probability and\npotentially offer exponential speedup over the classical counterpart.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u5728\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u9886\u57df\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u53d7\u5173\u6ce8\u3002\u5c3d\u7ba1\u8ba1\u7b97\u8d1d\u8482\u6570\uff08TDA\u4e2d\u7684\u6838\u5fc3\u4efb\u52a1\uff09\u88ab\u8bc1\u660e\u662fNP\u96be\u7684\uff0c\u8868\u660e\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0d\u592a\u53ef\u80fd\u63d0\u4f9b\u901a\u7528\u52a0\u901f\uff0c\u4f46\u672c\u7814\u7a76\u7740\u773c\u4e8eTDA\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u7279\u5f81\u2014\u2014\u6320\uff08torsion\uff09\u3002\u6320\u80fd\u63ed\u793a\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u4fe1\u606f\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e00\u4e2a\u7ed9\u5b9a\u7684\u5355\u7eaf\u590d\u5f62\u662f\u5426\u5305\u542b\u6320\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u4f4e\u590d\u6742\u5ea6\u7ecf\u5178\u7a0b\u5e8f\uff0c\u6709\u6f5c\u529b\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u8ba1\u7b97\u5728TDA\u9886\u57df\u7684\u7814\u7a76\u5df2\u53d6\u5f97\u4e00\u4e9b\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u5728\u8d1d\u8482\u6570\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u6320\u8fd9\u4e00\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\u7684\u62d3\u6251\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u91cf\u5b50\u7b97\u6cd5\u5728\u6320\u68c0\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4e\u590d\u6742\u5ea6\u7ecf\u5178\u7a0b\u5e8f\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u7ed9\u5b9a\u5355\u7eaf\u590d\u5f62\u662f\u5426\u5305\u542b\u6320\u3002", "result": "\u8be5\u91cf\u5b50\u7b97\u6cd5\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u6210\u529f\u68c0\u6d4b\u6320\uff0c\u5e76\u6709\u671b\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u7ecf\u5178\u7b97\u6cd5\u7684\u6307\u6570\u7ea7\u52a0\u901f\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f15\u5165\u4e86\u4e00\u79cd\u91cf\u5b50\u7b97\u6cd5\u7528\u4e8e\u6320\u68c0\u6d4b\uff0c\u4e3aTDA\u9886\u57df\u4e2d\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.19555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19555", "abs": "https://arxiv.org/abs/2508.19555", "authors": ["Yu-Wei Zhang", "Tongju Han", "Lipeng Gao", "Mingqiang Wei", "Hui Liu", "Changbao Li", "Caiming Zhang"], "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery", "comment": null, "summary": "This paper presents MonoRelief V2, an end-to-end model designed for directly\nrecovering 2.5D reliefs from single images under complex material and\nillumination variations. In contrast to its predecessor, MonoRelief V1 [1],\nwhich was solely trained on synthetic data, MonoRelief V2 incorporates real\ndata to achieve improved robustness, accuracy and efficiency. To overcome the\nchallenge of acquiring large-scale real-world dataset, we generate\napproximately 15,000 pseudo real images using a text-to-image generative model,\nand derive corresponding depth pseudo-labels through fusion of depth and normal\npredictions. Furthermore, we construct a small-scale real-world dataset (800\nsamples) via multi-view reconstruction and detail refinement. MonoRelief V2 is\nthen progressively trained on the pseudo-real and real-world datasets.\nComprehensive experiments demonstrate its state-of-the-art performance both in\ndepth and normal predictions, highlighting its strong potential for a range of\ndownstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.", "AI": {"tldr": "MonoRelief V2\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u6062\u590d2.5D\u6d6e\u96d5\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u6750\u8d28\u548c\u5149\u7167\u53d8\u5316\u7684\u95ee\u9898\u3002\u4e0e\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684V1\u7248\u672c\u4e0d\u540c\uff0cV2\u878d\u5408\u4e86\u771f\u5b9e\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u4e86\u7ea615000\u5f20\u4f2a\u771f\u5b9e\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u5f97\u5230\u4f2a\u6807\u7b7e\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b800\u4e2a\u6837\u672c\u7684\u5c0f\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u8be5\u6a21\u578b\u5728\u4f2a\u771f\u5b9e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9010\u6b65\u8bad\u7ec3\uff0c\u5e76\u5728\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u6062\u590d2.5D\u6d6e\u96d5\u65f6\u9047\u5230\u7684\u590d\u6742\u6750\u8d28\u548c\u5149\u7167\u53d8\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u4e86\u7ea615000\u5f20\u4f2a\u771f\u5b9e\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u83b7\u5f97\u4e86\u76f8\u5e94\u7684\u4f2a\u6807\u7b7e\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b800\u4e2a\u6837\u672c\u7684\u5c0f\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002MonoRelief V2\u5728\u4f2a\u771f\u5b9e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MonoRelief V2\u5728\u4ece\u5355\u5f20\u56fe\u50cf\u6062\u590d2.5D\u6d6e\u96d5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u6750\u8d28\u548c\u5149\u7167\u53d8\u5316\u65f6\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.19479", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.19479", "abs": "https://arxiv.org/abs/2508.19479", "authors": ["Serena Hughes", "Timothy Hamilton", "Tom Kolokotrones", "Eric J. Deeds"], "title": "DeepAtlas: a tool for effective manifold learning", "comment": "38 pages, 7 main text figures, 16 supplementary figures", "summary": "Manifold learning builds on the \"manifold hypothesis,\" which posits that data\nin high-dimensional datasets are drawn from lower-dimensional manifolds.\nCurrent tools generate global embeddings of data, rather than the local maps\nused to define manifolds mathematically. These tools also cannot assess whether\nthe manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,\nan algorithm that generates lower-dimensional representations of the data's\nlocal neighborhoods, then trains deep neural networks that map between these\nlocal embeddings and the original data. Topological distortion is used to\ndetermine whether a dataset is drawn from a manifold and, if so, its\ndimensionality. Application to test datasets indicates that DeepAtlas can\nsuccessfully learn manifold structures. Interestingly, many real datasets,\nincluding single-cell RNA-sequencing, do not conform to the manifold\nhypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a\nmodel that can be used generatively and promises to allow the application of\npowerful tools from differential geometry to a variety of datasets.", "AI": {"tldr": "DeepAtlas\u7b97\u6cd5\u751f\u6210\u5c40\u90e8\u5d4c\u5165\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u6d41\u5f62\u5047\u8bbe\u5e76\u751f\u6210\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u751f\u6210\u5168\u5c40\u5d4c\u5165\u800c\u975e\u5c40\u90e8\u56fe\uff0c\u4e14\u65e0\u6cd5\u8bc4\u4f30\u6570\u636e\u662f\u5426\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\u3002", "method": "DeepAtlas\u7b97\u6cd5\u751f\u6210\u6570\u636e\u5c40\u90e8\u90bb\u57df\u7684\u4f4e\u7ef4\u8868\u793a\uff0c\u5e76\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6620\u5c04\uff0c\u4f7f\u7528\u62d3\u6251\u7578\u53d8\u8bc4\u4f30\u6d41\u5f62\u5047\u8bbe\u53ca\u5176\u7ef4\u5ea6\u3002", "result": "DeepAtlas\u6210\u529f\u5b66\u4e60\u4e86\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u5e76\u53d1\u73b0\u8bb8\u591a\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\uff09\u4e0d\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\u3002", "conclusion": "DeepAtlas\u53ef\u7528\u4e8e\u8bc4\u4f30\u6d41\u5f62\u5047\u8bbe\uff0c\u5e76\u5728\u6570\u636e\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\u65f6\u7528\u4e8e\u751f\u6210\u6a21\u578b\uff0c\u6709\u671b\u5c06\u5fae\u5206\u51e0\u4f55\u5de5\u5177\u5e94\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\u3002"}}
{"id": "2508.19580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19580", "abs": "https://arxiv.org/abs/2508.19580", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bba\u70b9\u5173\u952e\u70b9\u63d0\u53d6\u6570\u636e\u96c6ArgCMV\uff0c\u5b83\u5305\u542b\u6765\u81ea\u5728\u7ebf\u4eba\u7c7b\u8fa9\u8bba\u7684\u7ea612K\u4e2a\u8bba\u70b9\uff0c\u6bd4\u73b0\u6709\u7684ArgKP21\u6570\u636e\u96c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u771f\u5b9e\u7684\u5bf9\u8bdd\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5173\u952e\u70b9\u63d0\u53d6\u65b9\u6cd5\u4e3b\u8981\u5728ArgKP21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f46\u8be5\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u4ee3\u8868\u771f\u5b9e\u7684\u4eba\u7c7b\u5bf9\u8bdd\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6536\u96c6\u4e86ArgCMV\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea3K\u591a\u4e2a\u4e3b\u9898\u7684\u5728\u7ebf\u4eba\u7c7b\u8fa9\u8bba\u4e2d\u7684\u7ea612K\u4e2a\u8bba\u70b9\u3002\u8be5\u6570\u636e\u96c6\u6bd4ArgKP21\u66f4\u590d\u6742\uff0c\u5177\u6709\u66f4\u957f\u7684\u8bba\u70b9\u3001\u6307\u4ee3\u5173\u7cfb\u3001\u4e3b\u89c2\u8bdd\u8bed\u5355\u5143\u4ee5\u53ca\u66f4\u5e7f\u6cdb\u7684\u4e3b\u9898\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728ArgCMV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5b83\u4eec\u96be\u4ee5\u9002\u5e94\u66f4\u590d\u6742\u3001\u66f4\u771f\u5b9e\u7684\u5728\u7ebf\u8ba8\u8bba\u3002\u901a\u8fc7\u5bf9\u73b0\u6709\u57fa\u7ebf\u548c\u6700\u65b0\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86ArgCMV\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\u3002", "conclusion": "ArgCMV\u6570\u636e\u96c6\u7684\u5f15\u5165\u4e3a\u957f\u4e0a\u4e0b\u6587\u5728\u7ebf\u8ba8\u8bba\u7684\u5173\u952e\u70b9\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u7531LLM\u9a71\u52a8\u7684\u6458\u8981\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19959", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19959", "abs": "https://arxiv.org/abs/2508.19959", "authors": ["Lucia Vilchez-Estevez", "Alexander Yosifov", "Jinzhao Sun"], "title": "Direct probing of the simulation complexity of open quantum many-body dynamics", "comment": "16 pages, 9 figures", "summary": "Simulating open quantum systems is key to understanding non-equilibrium\nprocesses, as persistent influence from the environment induces dissipation and\ncan give rise to steady-state phase transitions. A common strategy is to embed\nthe system-environment into a larger unitary framework, but this obscures the\nintrinsic complexity of the reduced system dynamics. Here, we investigate the\ncomputational complexity of simulating open quantum systems, focusing on two\nphysically relevant parameters, correlation length and mixing time, and explore\nwhether it can be comparable (or even lower) to that of simulating their closed\ncounterparts. In particular, we study the role of dissipation in simulating\nopen-system dynamics using both quantum and classical methods, where the\nclassical complexity is characterised by the bond dimension and operator\nentanglement entropy. Our results show that dissipation affects correlation\nlength and mixing time in distinct ways at intermediate and long timescales.\nMoreover, we observe numerically that in classical tensor network simulations,\nclassical complexity does not decrease with stronger dissipation, revealing a\nseparation between quantum and classical resource scaling.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6a21\u62df\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u4e0e\u5c01\u95ed\u91cf\u5b50\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u7406\u89e3\u975e\u5e73\u8861\u8fc7\u7a0b\u548c\u7a33\u6001\u76f8\u53d8\uff0c\u5e76\u9610\u660e\u8017\u6563\u5728\u6a21\u62df\u5f00\u653e\u7cfb\u7edf\u52a8\u529b\u5b66\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4e0e\u4e24\u4e2a\u7269\u7406\u76f8\u5173\u53c2\u6570\uff08\u5173\u8054\u957f\u5ea6\u548c\u6df7\u5408\u65f6\u95f4\uff09\u76f8\u5173\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u548c\u7ecf\u5178\u65b9\u6cd5\uff08\u7ecf\u5178\u590d\u6742\u5ea6\u7531\u952e\u7ef4\u5ea6\u548c\u7b97\u5b50\u7ea0\u7f20\u71b5\u8868\u5f81\uff09\u3002", "result": "\u8017\u6563\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u5f71\u54cd\u4e2d\u957f\u65f6\u7a0b\u7684\u5173\u8054\u957f\u5ea6\u548c\u6df7\u5408\u65f6\u95f4\uff1b\u5728\u7ecf\u5178\u7684\u5f20\u91cf\u7f51\u7edc\u6a21\u62df\u4e2d\uff0c\u8017\u6563\u7684\u589e\u5f3a\u5e76\u4e0d\u964d\u4f4e\u7ecf\u5178\u590d\u6742\u5ea6\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u548c\u7ecf\u5178\u8d44\u6e90\u7f29\u653e\u4e4b\u95f4\u7684\u5206\u79bb\u3002", "conclusion": "\u8017\u6563\u5bf9\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6a21\u62df\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u4e0e\u5c01\u95ed\u7cfb\u7edf\u5b58\u5728\u8d44\u6e90\u7f29\u653e\u4e0a\u7684\u5dee\u5f02\u3002"}}
{"id": "2508.19565", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.19565", "abs": "https://arxiv.org/abs/2508.19565", "authors": ["Yuhang Zhao", "Zixing Wang"], "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection", "comment": "Accepted by PRCV 2025. Project page with code and dataset:\n  https://github.com/AstronZh/Intersection-Flow-5K", "summary": "End-to-end object detectors offer a promising NMS-free paradigm for real-time\napplications, yet their high computational cost remains a significant barrier,\nparticularly for complex scenarios like intersection traffic monitoring. To\naddress this challenge, we propose FlowDet, a high-speed detector featuring a\ndecoupled encoder optimization strategy applied to the DETR architecture.\nSpecifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for\ntraffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to\nmaintain high representational power across extreme scale variations. To\nrigorously evaluate the model's performance in environments with severe\nocclusion and high object density, we collected the Intersection-Flow-5k\ndataset, a new challenging scene for this task. Evaluated on\nIntersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to\nthe strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by\n1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference\nspeed by 16.2%. Our work demonstrates a new path towards building highly\nefficient and accurate detectors for demanding, real-world perception systems.\nThe Intersection-Flow-5k dataset is available at\nhttps://github.com/AstronZh/Intersection-Flow-5K.", "AI": {"tldr": "FlowDet\u662f\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u9ad8\u901f\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u89e3\u8026\u7f16\u7801\u5668\u4f18\u5316\u3001\u51e0\u4f55\u53ef\u53d8\u5f62\u5355\u5143\uff08GDU\uff09\u548c\u5c3a\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\uff08SAA\uff09\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6Intersection-Flow-5k\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u5668\u867d\u7136\u6709\u6f5c\u529b\u5b9e\u73b0\u65e0NMS\u7684\u5b9e\u65f6\u5e94\u7528\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u4ea4\u53c9\u8def\u53e3\u4ea4\u901a\u76d1\u63a7\uff09\u4e0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u9ad8\u901f\u68c0\u6d4b\u5668\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowDet\u7684\u9ad8\u901f\u68c0\u6d4b\u5668\uff0c\u5b83\u91c7\u7528\u4e86DETR\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u4e86\u4ee5\u4e0b\u5173\u952e\u6280\u672f\uff1a1. \u89e3\u8026\u7f16\u7801\u5668\u4f18\u5316\u7b56\u7565\uff1b2. \u65b0\u9896\u7684\u51e0\u4f55\u53ef\u53d8\u5f62\u5355\u5143\uff08GDU\uff09\uff0c\u7528\u4e8e\u4ea4\u901a\u573a\u666f\u7684\u51e0\u4f55\u5efa\u6a21\uff1b3. \u5c3a\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\uff08SAA\uff09\u6a21\u5757\uff0c\u4ee5\u5e94\u5bf9\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u5e76\u4fdd\u6301\u8868\u5f81\u80fd\u529b\u3002\u4e3a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u7814\u7a76\u4eba\u5458\u6536\u96c6\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6Intersection-Flow-5k\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u4e25\u91cd\u7684\u906e\u6321\u548c\u9ad8\u5bc6\u5ea6\u7684\u7269\u4f53\u3002", "result": "\u5728Intersection-Flow-5k\u6570\u636e\u96c6\u4e0a\uff0cFlowDet\u53d6\u5f97\u4e86\u65b0\u7684 state-of-the-art \u6027\u80fd\u3002\u4e0eRT-DETR\u57fa\u7ebf\u76f8\u6bd4\uff0cFlowDet\u5c06AP(test)\u63d0\u9ad8\u4e861.5%\uff0cAP50(test)\u63d0\u9ad8\u4e861.6%\uff0c\u540c\u65f6\u5c06GFLOPs\u964d\u4f4e\u4e8663.2%\uff0c\u5e76\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e8616.2%\u3002", "conclusion": "FlowDet\u901a\u8fc7\u5176\u521b\u65b0\u7684\u8bbe\u8ba1\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9002\u7528\u4e8e\u4e25\u82db\u7684\u771f\u5b9e\u4e16\u754c\u611f\u77e5\u7cfb\u7edf\u7684\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.20072", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20072", "abs": "https://arxiv.org/abs/2508.20072", "authors": ["Zhixuan Liang", "Yizhuo Li", "Tianshuo Yang", "Chengyue Wu", "Sitong Mao", "Liuao Pei", "Xiaokang Yang", "Jiangmiao Pang", "Yao Mu", "Ping Luo"], "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "comment": "15 pages", "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.", "AI": {"tldr": "We introduce Discrete Diffusion VLA, a unified transformer policy that models discretized action chunks using discrete diffusion, compatible with Vision-Language Models (VLMs). It achieves adaptive decoding, parallel processing, and improved performance on robotic tasks compared to existing methods.", "motivation": "Existing Vision-Language-Action (VLA) decoders have limitations such as autoregressive generation, specialized training, and iterative sampling, hindering unified and scalable architectures. We aim to develop a more compatible and efficient VLA decoder.", "method": "We propose Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion. This approach is trained with the same cross-entropy objective as the VLM backbone, maintaining compatibility with the discrete token interface of VLMs. It features adaptive decoding and secondary remasking for refinement and error correction.", "result": "Discrete Diffusion VLA achieves 96.3% average success rate (SR) on LIBERO, 71.2% visual matching on SimplerEnv Fractal, and 49.3% overall on SimplerEnv Bridge. These results surpass both autoregressive and continuous diffusion baselines.", "conclusion": "Discrete Diffusion VLA's unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces function evaluations. This approach facilitates precise action modeling and consistent training, paving the way for scaling VLA to larger models and datasets."}}
{"id": "2508.19486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19486", "abs": "https://arxiv.org/abs/2508.19486", "authors": ["Wangyang Ying", "Nanxu Gong", "Dongjie Wang", "Xinyuan Wang", "Arun Vignesh Malarkkan", "Vivek Gupta", "Chandan K. Reddy", "Yanjie Fu"], "title": "Distribution Shift Aware Neural Tabular Learning", "comment": null, "summary": "Tabular learning transforms raw features into optimized spaces for downstream\ntasks, but its effectiveness deteriorates under distribution shifts between\ntraining and testing data. We formalize this challenge as the Distribution\nShift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature\nTransformation (SAFT) framework to address it. SAFT reframes tabular learning\nfrom a discrete search task into a continuous representation-generation\nparadigm, enabling differentiable optimization over transformed feature sets.\nSAFT integrates three mechanisms to ensure robustness: (i) shift-resistant\nrepresentation via embedding decorrelation and sample reweighting, (ii)\nflatness-aware generation through suboptimal embedding averaging, and (iii)\nnormalization-based alignment between training and test distributions.\nExtensive experiments show that SAFT consistently outperforms prior tabular\nlearning methods in terms of robustness, effectiveness, and generalization\nability under diverse real-world distribution shifts.", "AI": {"tldr": "\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u8868\u683c\u5b66\u4e60\u7684\u6709\u6548\u6027\u4f1a\u4e0b\u964d\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAFT\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8868\u683c\u5b66\u4e60\u89c6\u4e3a\u4e00\u4e2a\u8fde\u7eed\u7684\u8868\u793a\u751f\u6210\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u4e86\u4e09\u79cd\u673a\u5236\uff08\u5d4c\u5165\u53bb\u76f8\u5173\u548c\u6837\u672c\u91cd\u52a0\u6743\u3001\u6b21\u4f18\u5d4c\u5165\u5e73\u5747\u3001\u57fa\u4e8e\u5f52\u4e00\u5316\u7684\u5bf9\u9f50\uff09\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8bc1\u660eSAFT\u5728\u5404\u79cd\u5206\u5e03\u53d8\u5316\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8868\u683c\u5b66\u4e60\u5728\u5206\u5e03\u53d8\u5316\u65f6\u6548\u679c\u4f1a\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u8868\u683c\u5b66\u4e60\uff08DSTL\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAFT\u7684\u6846\u67b6\uff0c\u5c06\u8868\u683c\u5b66\u4e60\u91cd\u6784\u4e3a\u8fde\u7eed\u7684\u8868\u793a\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u5d4c\u5165\u53bb\u76f8\u5173\u3001\u6837\u672c\u91cd\u52a0\u6743\u3001\u6b21\u4f18\u5d4c\u5165\u5e73\u5747\u548c\u57fa\u4e8e\u5f52\u4e00\u5316\u7684\u5bf9\u9f50\u7b49\u673a\u5236\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "SAFT\u5728\u5404\u79cd\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u5728\u9c81\u68d2\u6027\u3001\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4e4b\u524d\u7684\u8868\u683c\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SAFT\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u8868\u683c\u5b66\u4e60\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3001\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.19587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19587", "abs": "https://arxiv.org/abs/2508.19587", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.", "AI": {"tldr": "\u73b0\u4ee3\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5728\u8bcd\u548c\u53e5\u5b50\u7ea7\u522b\u8f6c\u5f55\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b64\u7acb\u5b57\u6bcd\u5206\u7c7b\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5b64\u7acb\u963f\u62c9\u4f2f\u5b57\u6bcd\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u6297\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5b64\u7acb\u5b57\u6bcd\u5206\u7c7b\u5bf9\u4e8e\u8bed\u8a00\u5b66\u4e60\u3001\u8bed\u97f3\u6cbb\u7597\u548c\u8bed\u97f3\u5b66\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u8f85\u97f3\u548c\u8bcd\u6c47\u4e0a\u4e0b\u6587\uff0c\u4e14\u6301\u7eed\u65f6\u95f4\u77ed\uff0c\u5bf9\u4e8e\u73b0\u6709ASR\u7cfb\u7edf\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u8003\u8651\u5230\u963f\u62c9\u4f2f\u8bed\u7279\u6709\u7684\u8bed\u97f3\u3002\u73b0\u6709\u7684wav2vec 2.0\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a35%\uff0c\u9700\u8981\u6539\u8fdb\u3002 ", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b\u5b64\u7acb\u963f\u62c9\u4f2f\u5b57\u6bcd\u7684\u3001\u5e26\u97f3\u6807\u7684\u8bed\u6599\u5e93\u3002\u4f7f\u7528wav2vec 2.0\u6a21\u578b\u63d0\u53d6\u7684\u5d4c\u5165\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u6027\u80fd\u63d0\u9ad8\u523065%\u3002\u4e3a\u4e86\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u91c7\u7528\u4e86\u5bf9\u6297\u8bad\u7ec3\uff0c\u5c06\u5e45\u5ea6\u6270\u52a8\uff08epsilon=0.05\uff09\u5f15\u5165\u7684\u51c6\u786e\u7387\u4e0b\u964d\u9650\u5236\u57289%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e72\u51c0\u8bed\u97f3\u7684\u51c6\u786e\u7387\u3002", "result": "\u5728\u5305\u542b\u5b64\u7acb\u963f\u62c9\u4f2f\u5b57\u6bcd\u7684\u8bed\u6599\u5e93\u4e0a\uff0c\u72b6\u6001\u6700\u5148\u8fdb\u7684wav2vec 2.0\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a35%\u3002\u901a\u8fc7\u5728wav2vec\u5d4c\u5165\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u523065%\u3002\u7136\u800c\uff0c\u6dfb\u52a0\u5c0f\u7684\u5e45\u5ea6\u6270\u52a8\uff08epsilon=0.05\uff09\u4f1a\u5c06\u51c6\u786e\u7387\u964d\u81f332%\u3002\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u5e72\u51c0\u8bed\u97f3\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5c06\u6270\u52a8\u5f15\u5165\u7684\u51c6\u786e\u7387\u4e0b\u964d\u9650\u5236\u57289%\u3002", "conclusion": "\u5bf9\u6297\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u5b64\u7acb\u963f\u62c9\u4f2f\u5b57\u6bcd\u5206\u7c7b\u4efb\u52a1\u7684\u9c81\u68d2\u6027\uff0c\u514b\u670d\u4e86\u8bed\u97f3\u4fe1\u53f7\u4e2d\u7684\u566a\u58f0\u548c\u6270\u52a8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8bed\u6599\u5e93\u3001\u8bad\u7ec3\u6d41\u7a0b\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u4ee3\u7801\u4ee5\u4f9b\u590d\u73b0\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u628a\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u9ad8\u7ea7\u522b\u7684\u8bed\u97f3\u8bc6\u522b\u6846\u67b6\u3002"}}
{"id": "2508.19970", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19970", "abs": "https://arxiv.org/abs/2508.19970", "authors": ["Yijian Meng", "Asbj\u00f8rn Arvad J\u00f8rgensen", "Andreas N\u00e6sby Rasmussen", "Lasse H\u00f8gstedt", "S\u00f8ren M. M. Friis", "Mikael Lassen"], "title": "Hyper-spectral Imaging with Up-Converted Mid-Infrared Single-Photons", "comment": "13 pages, 5 figures", "summary": "Hyperspectral imaging in the mid-infrared (MIR) spectral range provides\nunique molecular specificity by probing fundamental vibrational modes of\nmolecular bonds, making it highly valuable for biomedical and biochemical\napplications. However, conventional MIR imaging techniques often rely on\nhigh-intensity illumination that can induce photodamage in sensitive biological\ntissues. Single-photon MIR imaging offers a label-free, non-invasive\nalternative, yet its adoption is hindered by the lack of efficient,\nroom-temperature MIR single-photon detectors. We present a single-photon\nhyperspectral imaging platform that combines cavity-enhanced spontaneous\nparametric down-conversion (SPDC) with nonlinear frequency up-conversion. This\napproach enables MIR spectral imaging using cost-effective, visible-wavelength\nsilicon single-photon avalanche diodes (Si-SPADs), supporting room-temperature,\nlow-noise, and high-efficiency operation. Time-correlated photon pairs\ngenerated via SPDC suppress classical intensity noise, enabling near\nshot-noise-limited hyperspectral imaging. We demonstrate chemically specific\nsingle-photon imaging across the \\SIrange{2.9}{3.6}{\\micro\\meter} range on\nbiological (egg yolk, yeast) and polymeric (polystyrene, polyethylene) samples.\nThe system delivers high-contrast, label-free imaging at ultralow photon flux,\novercoming key limitations of current MIR technologies. This platform paves the\nway toward scalable, quantum-enabled MIR imaging for applications in molecular\ndiagnostics, environmental sensing, and biomedical research.", "AI": {"tldr": "\u79d1\u5b66\u5bb6\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u5149\u5b50\u6210\u50cf\u5e73\u53f0\uff0c\u53ef\u4ee5\u5728\u4e2d\u7ea2\u5916\uff08MIR\uff09\u5149\u8c31\u8303\u56f4\u5185\u5bf9\u751f\u7269\u6837\u672c\u8fdb\u884c\u65e0\u6807\u8bb0\u3001\u975e\u4fb5\u5165\u6027\u6210\u50cf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u8154\u589e\u5f3a\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\u548c\u975e\u7ebf\u6027\u9891\u7387\u4e0a\u8f6c\u6362\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u53ef\u89c1\u5149\u7845\u5355\u5149\u5b50\u96ea\u5d29\u4e8c\u6781\u7ba1\uff08Si-SPADs\uff09\uff0c\u5b9e\u73b0\u4e86\u5728\u5ba4\u6e29\u4e0b\u3001\u4f4e\u566a\u58f0\u3001\u9ad8\u6548\u7387\u7684\u4e2d\u7ea2\u5916\u5149\u8c31\u6210\u50cf\u3002\u4e0e\u4f20\u7edf\u4e2d\u7ea2\u5916\u6210\u50cf\u6280\u672f\u7684\u9ad8\u5f3a\u5ea6\u7167\u660e\u4e0d\u540c\uff0c\u8be5\u5e73\u53f0\u4f7f\u7528\u6781\u4f4e\u7684\u5149\u5b50\u901a\u91cf\uff0c\u907f\u514d\u4e86\u5bf9\u654f\u611f\u751f\u7269\u7ec4\u7ec7\u7684\u635f\u4f24\uff0c\u5e76\u4e14\u80fd\u591f\u6291\u5236\u53e4\u5178\u5f3a\u5ea6\u566a\u58f0\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6563\u7c92\u566a\u58f0\u6781\u9650\u7684\u6210\u50cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5bf9\u751f\u7269\u6837\u672c\uff08\u86cb\u9ec4\u3001\u9175\u6bcd\uff09\u548c\u805a\u5408\u7269\uff08\u805a\u82ef\u4e59\u70ef\u3001\u805a\u4e59\u70ef\uff09\u8fdb\u884c\u5316\u5b66\u7279\u5f02\u6027\u6210\u50cf\uff0c\u57282.9\u81f33.6\u5fae\u7c73\u8303\u56f4\u5185\u63d0\u4f9b\u9ad8\u5bf9\u6bd4\u5ea6\u7684\u65e0\u6807\u8bb0\u6210\u50cf\u3002", "motivation": "\u4f20\u7edf\u7684\u4e2d\u7ea2\u5916\uff08MIR\uff09\u6210\u50cf\u6280\u672f\u867d\u7136\u5728\u751f\u7269\u533b\u5b66\u548c\u751f\u7269\u5316\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u5206\u5b50\u7279\u5f02\u6027\u4f18\u52bf\uff0c\u4f46\u901a\u5e38\u9700\u8981\u9ad8\u5f3a\u5ea6\u7167\u660e\uff0c\u53ef\u80fd\u5bf9\u654f\u611f\u751f\u7269\u7ec4\u7ec7\u9020\u6210\u5149\u635f\u4f24\u3002\u5355\u5149\u5b50MIR\u6210\u50cf\u4f5c\u4e3a\u4e00\u79cd\u65e0\u6807\u8bb0\u3001\u975e\u4fb5\u5165\u6027\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u53d1\u5c55\u53d7\u5230\u7f3a\u4e4f\u9ad8\u6548\u3001\u5ba4\u6e29\u5de5\u4f5c\u7684MIR\u5355\u5149\u5b50\u63a2\u6d4b\u5668\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8154\u589e\u5f3a\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\u548c\u975e\u7ebf\u6027\u9891\u7387\u4e0a\u8f6c\u6362\u7684\u5355\u5149\u5b50\u9ad8\u5149\u8c31\u6210\u50cf\u5e73\u53f0\u3002\u8be5\u5e73\u53f0\u5229\u7528SPDC\u6280\u672f\u4ea7\u751f\u65f6\u95f4\u76f8\u5173\u7684\u5149\u5b50\u5bf9\uff0c\u6291\u5236\u4e86\u53e4\u5178\u5f3a\u5ea6\u566a\u58f0\uff0c\u5e76\u7ed3\u5408\u975e\u7ebf\u6027\u9891\u7387\u4e0a\u8f6c\u6362\u6280\u672f\uff0c\u4f7f\u5f97\u4f7f\u7528\u6210\u672c\u8f83\u4f4e\u7684\u53ef\u89c1\u5149\u6ce2\u957f\u7845\u5355\u5149\u5b50\u96ea\u5d29\u4e8c\u6781\u7ba1\uff08Si-SPADs\uff09\u8fdb\u884c\u4e2d\u7ea2\u5916\u5149\u8c31\u6210\u50cf\u6210\u4e3a\u53ef\u80fd\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u5ba4\u6e29\u3001\u4f4e\u566a\u58f0\u548c\u9ad8\u6548\u7387\u7684\u64cd\u4f5c\u3002", "result": "\u8be5\u5e73\u53f0\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u751f\u7269\u6837\u672c\uff08\u86cb\u9ec4\u3001\u9175\u6bcd\uff09\u548c\u805a\u5408\u7269\uff08\u805a\u82ef\u4e59\u70ef\u3001\u805a\u4e59\u70ef\uff09\u57282.9\u81f33.6\u5fae\u7c73\u8303\u56f4\u5185\u7684\u5316\u5b66\u7279\u5f02\u6027\u5355\u5149\u5b50\u6210\u50cf\u3002\u6210\u50cf\u7ed3\u679c\u663e\u793a\u51fa\u9ad8\u5bf9\u6bd4\u5ea6\uff0c\u5e76\u4e14\u5728\u6781\u4f4e\u7684\u5149\u5b50\u901a\u91cf\u4e0b\u5b9e\u73b0\u4e86\u65e0\u6807\u8bb0\u6210\u50cf\uff0c\u514b\u670d\u4e86\u73b0\u6709\u4e2d\u7ea2\u5916\u6280\u672f\u7684\u5173\u952e\u9650\u5236\u3002", "conclusion": "\u8be5\u5355\u5149\u5b50\u9ad8\u5149\u8c31\u6210\u50cf\u5e73\u53f0\u901a\u8fc7\u7ed3\u5408SPDC\u548c\u975e\u7ebf\u6027\u9891\u7387\u4e0a\u8f6c\u6362\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u7ea2\u5916\u5355\u5149\u5b50\u63a2\u6d4b\u5668\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u5ba4\u6e29\u4e0b\u7684\u9ad8\u7075\u654f\u5ea6\u3001\u4f4e\u566a\u58f0\u6210\u50cf\u3002\u8be5\u6280\u672f\u6709\u671b\u63a8\u52a8\u5206\u5b50\u8bca\u65ad\u3001\u73af\u5883\u4f20\u611f\u548c\u751f\u7269\u533b\u5b66\u7814\u7a76\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u91cf\u5b50\u8d4b\u80fd\u7684\u4e2d\u7ea2\u5916\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.19573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19573", "abs": "https://arxiv.org/abs/2508.19573", "authors": ["Luhu Li", "Bowen Lin", "Mukhtiar Khan", "Shujun Fu"], "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection", "comment": null, "summary": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.", "AI": {"tldr": "\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u5305\u542b\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u548c\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u6709\u9650\u6807\u6ce8\u548c\u57df\u5dee\u5f02\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u5982\u56fa\u5b9a\u7f16\u7801\u5668\u9650\u5236\u9002\u5e94\u6027\uff0c\u539f\u578b\u5b66\u4e60\u6613\u53d1\u751f\u539f\u578b\u574d\u584c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\uff1a1. \u53ef\u8bad\u7ec3\u7684\u3001\u5e26\u6709\u52a8\u91cf\u5206\u652f\u7684\u7f16\u7801\u5668\u4ee5\u8fdb\u884c\u57df\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\uff1b2. \u8f7b\u91cf\u7ea7\u539f\u578b\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5bfc\u89e3\u7801\u5668\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa\uff1b3. \u65b0\u9896\u7684\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\uff0c\u901a\u8fc7\u591a\u6837\u6027\u7ea6\u675f\u548c\u539f\u578b\u5f52\u4e00\u5316\u9632\u6b62\u539f\u578b\u574d\u584c\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8868\u793a\u8d28\u91cf\u548c\u5f02\u5e38\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u548c\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u57df\u9002\u5e94\u6027\u548c\u539f\u578b\u574d\u584c\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.19487", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19487", "abs": "https://arxiv.org/abs/2508.19487", "authors": ["Wangyang Ying", "Jinghan Zhang", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Kunpeng Liu", "Chandan K. Reddy", "Yanjie Fu"], "title": "Data-Efficient Symbolic Regression via Foundation Model Distillation", "comment": null, "summary": "Discovering interpretable mathematical equations from observed data (a.k.a.\nequation discovery or symbolic regression) is a cornerstone of scientific\ndiscovery, enabling transparent modeling of physical, biological, and economic\nsystems. While foundation models pre-trained on large-scale equation datasets\noffer a promising starting point, they often suffer from negative transfer and\npoor generalization when applied to small, domain-specific datasets. In this\npaper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer\nEmbeddings), a data-efficient fine-tuning framework that adapts foundation\nmodels for symbolic equation discovery in low-data regimes via distillation.\nEQUATE combines symbolic-numeric alignment with evaluator-guided embedding\noptimization, enabling a principled embedding-search-generation paradigm. Our\napproach reformulates discrete equation search as a continuous optimization\ntask in a shared embedding space, guided by data-equation fitness and\nsimplicity. Experiments across three standard public benchmarks (Feynman,\nStrogatz, and black-box datasets) demonstrate that EQUATE consistently\noutperforms state-of-the-art baselines in both accuracy and robustness, while\npreserving low complexity and fast inference. These results highlight EQUATE as\na practical and generalizable solution for data-efficient symbolic regression\nin foundation model distillation settings.", "AI": {"tldr": "EQUATE\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u84b8\u998f\u5c06\u57fa\u7840\u6a21\u578b\u9002\u5e94\u4e8e\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u7b26\u53f7\u65b9\u7a0b\u53d1\u73b0\u3002", "motivation": "\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u65b9\u7a0b\uff08\u7b26\u53f7\u56de\u5f52\uff09\u662f\u79d1\u5b66\u53d1\u73b0\u7684\u57fa\u77f3\uff0c\u4f46\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5e94\u7528\u4e8e\u5c0f\u89c4\u6a21\u3001\u9886\u57df\u7279\u5b9a\u7684\u6570\u636e\u96c6\u65f6\uff0c\u5e38\u51fa\u73b0\u8d1f\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "EQUATE\u7ed3\u5408\u4e86\u7b26\u53f7-\u6570\u503c\u5bf9\u9f50\u548c\u8bc4\u4f30\u5668\u5f15\u5bfc\u7684\u5d4c\u5165\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u641c\u7d22\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u5c06\u79bb\u6563\u65b9\u7a0b\u641c\u7d22\u91cd\u6784\u4e3a\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u4f18\u5316\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u6570\u636e-\u65b9\u7a0b\u9002\u5e94\u6027\u548c\u7b80\u6d01\u6027\u8fdb\u884c\u6307\u5bfc\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u516c\u5171\u57fa\u51c6\uff08Feynman\u3001Strogatz\u548c\u9ed1\u76d2\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEQUATE\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u590d\u6742\u6027\u548c\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "EQUATE\u4e3a\u57fa\u7840\u6a21\u578b\u84b8\u998f\u8bbe\u7f6e\u4e2d\u6570\u636e\u9ad8\u6548\u7684\u7b26\u53f7\u56de\u5f52\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19594", "abs": "https://arxiv.org/abs/2508.19594", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.", "AI": {"tldr": "LLMs struggle with context faithfulness. This paper proposes Router Lens to identify context-faithful experts in mixture-of-experts models, finding they amplify attention to relevant context. CEFT, a method fine-tuning these experts, matches or exceeds full fine-tuning efficiency.", "motivation": "To address the issue of large language models (LLMs) struggling to ground their outputs in provided context, leading to irrelevant responses, and to explore the potential of expert specialization in mixture-of-experts (MoE) architectures for improving context faithfulness.", "method": "The paper proposes Router Lens to identify context-faithful experts within MoE architectures. It then introduces Context-faithful Expert Fine-Tuning (CEFT), a lightweight optimization approach that selectively fine-tunes these identified experts.", "result": "Analysis using Router Lens reveals that context-faithful experts progressively amplify attention to relevant contextual information, enhancing context grounding. Experiments show that CEFT matches or surpasses the performance of full fine-tuning while being significantly more efficient across various benchmarks and models.", "conclusion": "CEFT, by selectively fine-tuning context-faithful experts identified through methods like Router Lens, offers an efficient and effective approach to improve LLM context faithfulness, matching or exceeding the performance of full fine-tuning."}}
{"id": "2508.19978", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19978", "abs": "https://arxiv.org/abs/2508.19978", "authors": ["Fabrizio Sgobba", "Francesco Di Lena", "Danilo Triggiani", "Deborah Katia Pallotti", "Cosmo Lupo", "Piergiorgio Daniele", "Gennaro Fratta", "Giulia Acconcia", "Ivan Rech", "Luigi Santamaria Amato"], "title": "Momentum-resolved Hong-Ou-Mandel interference of weak coherent states", "comment": "8 pages, 4 figures", "summary": "We demonstrate an experimental scheme for high-precision position\nmeasurements based on transverse-momentum-resolved two-photon interferometry\nwith independent photons and SPAD arrays. Our scheme extends the operative\nrange of Hong-Ou-Mandel interferometry beyond its intrinsic constraints due to\nphotons indistinguishability, paving the way to applications in high-resolution\nimaging. We assess the experimental results against the ultimate precision\nbounds as determined by quantum estimation theory. Our experiment ultimately\nproves that transverse-momentum resolved measurements of fourth-order\ncorrelations in the fields can be employed to overcome spatial\ndistiguishability between independent photons. The relevance of our results\nextends beyond sensing and imaging towards quantum information processing, as\nwe show that partial photon distinguishability and entanglement impurity are\nnot necessarily a nuisance in a technique that relies on two-photon\ninterference.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a2a\u5411\u52a8\u91cf\u5206\u8fa8\u53cc\u5149\u5b50\u5e72\u6d89\u7684\u5b9e\u9a8c\u65b9\u6848\uff0c\u4f7f\u7528\u72ec\u7acb\u5149\u5b50\u548cSPAD\u9635\u5217\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u6d4b\u91cf\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4f20\u7edfHong-Ou-Mandel\u5e72\u6d89\u5728\u5149\u5b50\u4e0d\u53ef\u533a\u5206\u6027\u65b9\u9762\u7684\u56fa\u6709\u9650\u5236\uff0c\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u81f3\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u3002", "method": "\u5229\u7528\u6a2a\u5411\u52a8\u91cf\u5206\u8fa8\u53cc\u5149\u5b50\u5e72\u6d89\u6280\u672f\uff0c\u7ed3\u5408\u72ec\u7acb\u5149\u5b50\u548cSPAD\u9635\u5217\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6d4b\u91cf\u573a\u4e2d\u56db\u9636\u5173\u8054\u7684\u6a2a\u5411\u52a8\u91cf\u5206\u8fa8\uff0c\u53ef\u4ee5\u514b\u670d\u72ec\u7acb\u5149\u5b50\u95f4\u7684\u7a7a\u95f4\u53ef\u533a\u5206\u6027\uff0c\u8fbe\u5230\u4e86\u91cf\u5b50\u4f30\u8ba1\u7406\u8bba\u6240\u786e\u5b9a\u7684\u6781\u9650\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5728\u4f20\u611f\u548c\u6210\u50cf\u9886\u57df\u8d85\u8d8a\u7a7a\u95f4\u53ef\u533a\u5206\u6027\u7684\u9650\u5236\uff0c\u8fd8\u80fd\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u5229\u7528\u90e8\u5206\u5149\u5b50\u53ef\u533a\u5206\u6027\u548c\u7ea0\u7f20\u6742\u8d28\u3002"}}
{"id": "2508.19574", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19574", "abs": "https://arxiv.org/abs/2508.19574", "authors": ["Mingxi Fu", "Fanglei Fu", "Xitong Ling", "Huaitian Yuan", "Tian Guan", "Yonghong He", "Lianghui Zhu"], "title": "Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation", "comment": null, "summary": "Pathological image segmentation faces numerous challenges, particularly due\nto ambiguous semantic boundaries and the high cost of pixel-level annotations.\nAlthough recent semi-supervised methods based on consistency regularization\n(e.g., UniMatch) have made notable progress, they mainly rely on\nperturbation-based consistency within the image modality, making it difficult\nto capture high-level semantic priors, especially in structurally complex\npathology images. To address these limitations, we propose MPAMatch - a novel\nsegmentation framework that performs pixel-level contrastive learning under a\nmultimodal prototype-guided supervision paradigm. The core innovation of\nMPAMatch lies in the dual contrastive learning scheme between image prototypes\nand pixel labels, and between text prototypes and pixel labels, providing\nsupervision at both structural and semantic levels. This coarse-to-fine\nsupervisory strategy not only enhances the discriminative capability on\nunlabeled samples but also introduces the text prototype supervision into\nsegmentation for the first time, significantly improving semantic boundary\nmodeling. In addition, we reconstruct the classic segmentation architecture\n(TransUNet) by replacing its ViT backbone with a pathology-pretrained\nfoundation model (Uni), enabling more effective extraction of\npathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,\nEBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art\nmethods, validating its dual advantages in structural and semantic modeling.", "AI": {"tldr": "MPAMatch\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u548c\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff08\u56fe\u50cf-\u50cf\u7d20\uff0c\u6587\u672c-\u50cf\u7d20\uff09\u6765\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff08\u5982UniMatch\uff09\u4e3b\u8981\u4f9d\u8d56\u4e8e\u56fe\u50cf\u5185\u7684\u6270\u52a8\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u6355\u6349\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u590d\u6742\u7684\u75c5\u7406\u56fe\u50cf\u4e2d\u3002\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e5f\u9762\u4e34\u7740\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\u3002", "method": "MPAMatch\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u50cf\u7d20\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002\u5176\u6838\u5fc3\u662f\u56fe\u50cf\u539f\u578b-\u50cf\u7d20\u6807\u7b7e\u548c\u6587\u672c\u539f\u578b-\u50cf\u7d20\u6807\u7b7e\u4e4b\u95f4\u7684\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff0c\u540c\u65f6\u8fdb\u884c\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u7684\u76d1\u7763\u3002\u6b64\u5916\uff0c\u8fd8\u7528\u75c5\u7406\u9884\u8bad\u7ec3\u7684Uni\u6a21\u578b\u66ff\u6362\u4e86TransUNet\u7684ViT\u9aa8\u5e72\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u7279\u5f81\u3002", "result": "MPAMatch\u5728GLAS\u3001EBHI-SEG-GLAND\u3001EBHI-SEG-CANCER\u548cKPI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MPAMatch\u901a\u8fc7\u5176\u521b\u65b0\u7684\u53cc\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff08\u56fe\u50cf\u548c\u6587\u672c\u539f\u578b\uff09\u548c\u57fa\u4e8eUni\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u5efa\u6a21\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2508.19488", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19488", "abs": "https://arxiv.org/abs/2508.19488", "authors": ["Xavier Cadet", "Simona Boboila", "Sie Hendrata Dharmawan", "Alina Oprea", "Peter Chin"], "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense", "comment": "Accepted at GameSec 2025", "summary": "Cyber defense requires automating defensive decision-making under stealthy,\ndeceptive, and continuously evolving adversarial strategies. The FlipIt game\nprovides a foundational framework for modeling interactions between a defender\nand an advanced adversary that compromises a system without being immediately\ndetected. In FlipIt, the attacker and defender compete to control a shared\nresource by performing a Flip action and paying a cost. However, the existing\nFlipIt frameworks rely on a small number of heuristics or specialized learning\ntechniques, which can lead to brittleness and the inability to adapt to new\nattacks. To address these limitations, we introduce PoolFlip, a multi-agent gym\nenvironment that extends the FlipIt game to allow efficient learning for\nattackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent\nreinforcement learning (MARL) approach that leverages population-based training\nto train defender agents equipped to generalize against a range of unknown,\npotentially adaptive opponents. Our empirical results suggest that Flip-PSRO\ndefenders are $2\\times$ more effective than baselines to generalize to a\nheuristic attack not exposed in training. In addition, our newly designed\nownership-based utility functions ensure that Flip-PSRO defenders maintain a\nhigh level of control while optimizing performance.", "AI": {"tldr": "FlipIt\u6e38\u620f\u7528\u4e8e\u6a21\u62df\u653b\u9632\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002PoolFlip\u4f5c\u4e3a\u65b0\u73af\u5883\u6269\u5c55\u4e86FlipIt\uff0cFlip-PSRO\u4f5c\u4e3aMARL\u65b9\u6cd5\u5229\u7528\u79cd\u7fa4\u8bad\u7ec3\u63d0\u5347\u4e86\u9632\u5fa1\u8005\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u9762\u5bf9\u672a\u77e5\u653b\u51fb\u65f6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u901a\u8fc7\u65b0\u7684\u6548\u7528\u51fd\u6570\u4fdd\u6301\u4e86\u5bf9\u8d44\u6e90\u7684\u63a7\u5236\u3002", "motivation": "\u73b0\u6709FlipIt\u6846\u67b6\u4f9d\u8d56\u5c11\u91cf\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u4e13\u4e1a\u5b66\u4e60\u6280\u672f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5176\u50f5\u5316\u4e14\u65e0\u6cd5\u9002\u5e94\u65b0\u7684\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPoolFlip\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53gym\u73af\u5883\uff0c\u6269\u5c55\u4e86FlipIt\u6e38\u620f\u4ee5\u652f\u6301\u653b\u9632\u53cc\u65b9\u7684\u9ad8\u6548\u5b66\u4e60\u3002\u63d0\u51faFlip-PSRO\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u79cd\u7fa4\u7684\u8bad\u7ec3\u6765\u8bad\u7ec3\u9632\u5fa1\u667a\u80fd\u4f53\uff0c\u4f7f\u5176\u80fd\u591f\u6cdb\u5316\u4ee5\u5e94\u5bf9\u4e00\u7cfb\u5217\u672a\u77e5\u7684\u3001\u53ef\u80fd\u5177\u6709\u9002\u5e94\u6027\u7684\u5bf9\u624b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlip-PSRO\u9632\u5fa1\u8005\u5728\u6cdb\u5316\u5230\u8bad\u7ec3\u4e2d\u672a\u66b4\u9732\u7684\u542f\u53d1\u5f0f\u653b\u51fb\u65b9\u9762\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6548\u6027\u63d0\u9ad8\u4e862\u500d\u3002\u65b0\u8bbe\u8ba1\u7684\u57fa\u4e8e\u6240\u6709\u6743\u7684\u6548\u7528\u51fd\u6570\u786e\u4fdd\u4e86Flip-PSRO\u9632\u5fa1\u8005\u5728\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u5bf9\u8d44\u6e90\u7684\u9ad8\u5ea6\u63a7\u5236\u3002", "conclusion": "Flip-PSRO\u662f\u4e00\u79cd\u6709\u6548\u7684MARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u79cd\u7fa4\u8bad\u7ec3\u63d0\u9ad8\u4e86\u9632\u5fa1\u8005\u5e94\u5bf9\u672a\u77e5\u548c\u9002\u5e94\u6027\u5bf9\u624b\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u901a\u8fc7\u4f18\u5316\u7684\u6548\u7528\u51fd\u6570\u5b9e\u73b0\u4e86\u5bf9\u8d44\u6e90\u7684\u6709\u6548\u63a7\u5236\u3002"}}
{"id": "2508.19614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19614", "abs": "https://arxiv.org/abs/2508.19614", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.", "AI": {"tldr": "\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\uff0c\u5411\u68c0\u7d22\u5230\u7684\u76f8\u5173\u6587\u6863\u6ce8\u5165\u566a\u58f0\u53ef\u4ee5\u6539\u5584LLM\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u548c\u751f\u6210\u8d28\u91cf\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u5c42\u878d\u5408\u89e3\u7801\uff08LFD\uff09\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2d\u95f4\u5c42\u548c\u6700\u7ec8\u5c42\u7684\u8868\u793a\u6765\u5145\u5206\u5229\u7528\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u3002\u4e3a\u786e\u5b9a\u6700\u4f73\u4e2d\u95f4\u5c42\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u90e8\u77e5\u8bc6\u5f97\u5206\uff08IKS\uff09\u6807\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLFD\u80fd\u4ee5\u6700\u5c0f\u7684\u6210\u672c\u5e2e\u52a9RAG\u7cfb\u7edf\u66f4\u6709\u6548\u5730\u5229\u7528\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528LLM\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5229\u7528\u201c\u6ce8\u5165\u566a\u58f0\u53ef\u6539\u5584\u751f\u6210\u8d28\u91cf\u201d\u8fd9\u4e00\u53cd\u76f4\u89c9\u7684\u73b0\u8c61\uff0c\u672c\u6587\u65e8\u5728\u5bf9\u566a\u58f0\u6ce8\u5165\u8fdb\u884c\u5e72\u9884\uff0c\u5e76\u660e\u786eLLM\u5185\u90e8\u5c42\u7ea7\u7684\u529f\u80fd\u5212\u5206\u3002", "method": "\u63d0\u51fa\u5c42\u878d\u5408\u89e3\u7801\uff08LFD\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u4e2d\u95f4\u5c42\uff08\u4e13\u6ce8\u4e8e\u6574\u5408\u957f\u7a0b\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\uff09\u7684\u8868\u793a\u4e0e\u6700\u7ec8\u5c42\u89e3\u7801\u8f93\u51fa\u6765\u5145\u5206\u5229\u7528\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u3002\u901a\u8fc7\u5185\u90e8\u77e5\u8bc6\u5f97\u5206\uff08IKS\uff09\u6807\u51c6\u6765\u9009\u62e9\u6700\u4f73\u4e2d\u95f4\u5c42\uff0c\u8be5\u6807\u51c6\u9009\u62e9\u5177\u6709\u6700\u4f4eIKS\u503c\u7684\u4e2d\u95f4\u5c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLFD\u7b56\u7565\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u80fd\u591f\u5e2e\u52a9RAG\u7cfb\u7edf\u4ee5\u6700\u5c0f\u7684\u6210\u672c\u66f4\u6709\u6548\u5730\u5229\u7528\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002", "conclusion": "LLM\u7684\u5c42\u7ea7\u5177\u6709\u7279\u5b9a\u7684\u529f\u80fd\u5212\u5206\uff1a\u6d45\u5c42\u5173\u6ce8\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u4e2d\u95f4\u5c42\u6574\u5408\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u6df1\u5c42\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u3002LFD\u7b56\u7565\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u77e5\u8bc6\uff0c\u80fd\u591f\u63d0\u5347RAG\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u8bc1\u5b9e\u4e86\u901a\u8fc7\u5e72\u9884\u566a\u58f0\u6ce8\u5165\u548c\u5229\u7528\u7279\u5b9a\u5c42\u7ea7\u4fe1\u606f\u53ef\u4ee5\u4f18\u5316LLM\u7684\u884c\u4e3a\u3002"}}
{"id": "2508.19995", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19995", "abs": "https://arxiv.org/abs/2508.19995", "authors": ["Takanori Nishi"], "title": "A method of an on-demand beamsplitter for trapped-ion quantum computers", "comment": "11 pages, 3 figures", "summary": "Quantum information processing using local modes of trapped ions has been\napplied to implementing bosonic quantum error correction codes and conducting\nefficient quantum simulation of bosonic systems. However, control of\nentanglement among local modes remains difficult because entaglement among\nresonant local modes is governed by the Coulomb interaction, which is not\nswitchable. We propose a method of a beamsplitter for a trapped-ion\narchitecture, where the secular frequency of each mode is dynamically\ncontrollable. The neighboring modes are far detuned except when the\nbeamsplitter needs to be applied to them. We derive the analytical formula of\nthe proposed procedure and numerically confirm its validity.", "AI": {"tldr": "\u5c40\u57df\u6a21\u5f0f\u95f4\u7ea0\u7f20\u63a7\u5236\u7684\u91cf\u5b50\u4fe1\u606f\u5904\u7406", "motivation": "\u4e3a\u89e3\u51b3\u56da\u7981\u79bb\u5b50\u7cfb\u7edf\u4e2d\u5c40\u57df\u6a21\u5f0f\u95f4\u7ea0\u7f20\u63a7\u5236\u7684\u96be\u9898\uff0c\u56e0\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u4e0d\u53ef\u5207\u6362\uff0c\u5bfc\u81f4\u5171\u632f\u6a21\u5f0f\u95f4\u7ea0\u7f20\u96be\u4ee5\u8c03\u63a7\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u52a8\u6001\u8c03\u63a7\u6a21\u5f0f\u7684\u8d85\u7ec6\u5206\u5272\u7684\u56da\u7981\u79bb\u5b50\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u76f8\u90bb\u6a21\u5f0f\u5931\u8c10\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u8fdb\u884c\u8d85\u7ec6\u5206\u5272\u64cd\u4f5c\u3002", "result": "\u63a8\u5bfc\u4e86\u8be5\u64cd\u4f5c\u7684\u89e3\u6790\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56da\u7981\u79bb\u5b50\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.19575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19575", "abs": "https://arxiv.org/abs/2508.19575", "authors": ["Zhu Xu", "Zhaowen Wang", "Yuxin Peng", "Yang Liu"], "title": "Interact-Custom: Customized Human Object Interaction Image Generation", "comment": null, "summary": "Compositional Customized Image Generation aims to customize multiple target\nconcepts within generation content, which has gained attention for its wild\napplication.Existing approaches mainly concentrate on the target entity's\nappearance preservation, while neglecting the fine-grained interaction control\namong target entities.To enable the model of such interaction control\ncapability, we focus on human object interaction scenario and propose the task\nof Customized Human Object Interaction Image Generation(CHOI), which\nsimultaneously requires identity preservation for target human object and the\ninteraction semantic control between them.Two primary challenges exist for\nCHOI:(1)simultaneous identity preservation and interaction control demands\nrequire the model to decompose the human object into self-contained identity\nfeatures and pose-oriented interaction features, while the current HOI image\ndatasets fail to provide ideal samples for such feature-decomposed\nlearning.(2)inappropriate spatial configuration between human and object may\nlead to the lack of desired interaction semantics.To tackle it, we first\nprocess a large-scale dataset, where each sample encompasses the same pair of\nhuman object involving different interactive poses.Then we design a two-stage\nmodel Interact-Custom, which firstly explicitly models the spatial\nconfiguration by generating a foreground mask depicting the interaction\nbehavior, then under the guidance of this mask, we generate the target human\nobject interacting while preserving their identities features.Furthermore, if\nthe background image and the union location of where the target human object\nshould appear are provided by users, Interact-Custom also provides the optional\nfunctionality to specify them, offering high content controllability. Extensive\nexperiments on our tailored metrics for CHOI task demonstrate the effectiveness\nof our approach.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCHOI\uff08Customized Human Object Interaction Image Generation\uff09\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u5b9e\u73b0\u5bf9\u4eba\u7269\u548c\u7269\u4f53\u4ea4\u4e92\u56fe\u50cf\u7684\u5b9a\u5236\u5316\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u8bc1\u4eba\u7269\u548c\u7269\u4f53\u7684\u8eab\u4efd\u7279\u5f81\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u4ea4\u4e92\u8bed\u4e49\u7684\u63a7\u5236\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5916\u89c2\u4fdd\u7559\uff0c\u5ffd\u7565\u4e86\u4ea4\u4e92\u63a7\u5236\u3002CHOI\u4efb\u52a1\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u5982\u4f55\u540c\u65f6\u8fdb\u884c\u8eab\u4efd\u4fdd\u7559\u548c\u4ea4\u4e92\u63a7\u5236\uff0c\u9700\u8981\u6a21\u578b\u5c06\u4eba\u7269\u5206\u89e3\u4e3a\u8eab\u4efd\u7279\u5f81\u548c\u4e0e\u59ff\u52bf\u76f8\u5173\u7684\u4ea4\u4e92\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u9002\u5408\u8fd9\u79cd\u5b66\u4e60\uff1b2) \u4eba\u7269\u4e0e\u7269\u4f53\u4e0d\u6070\u5f53\u7684\u7a7a\u95f4\u914d\u7f6e\u53ef\u80fd\u5bfc\u81f4\u7f3a\u4e4f\u671f\u671b\u7684\u4ea4\u4e92\u8bed\u4e49\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bba\u6587\u9996\u5148\u5904\u7406\u4e86\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u76f8\u540c\u4eba\u7269\u548c\u7269\u4f53\u5728\u4e0d\u540c\u4ea4\u4e92\u59ff\u52bf\u4e0b\u7684\u6837\u672c\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aInteract-Custom\u7684\u4e24\u9636\u6bb5\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u663e\u5f0f\u5730\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u4ea4\u4e92\u884c\u4e3a\u7684\u524d\u666f\u63a9\u6a21\u6765\u5efa\u6a21\u7a7a\u95f4\u914d\u7f6e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u63a9\u6a21\u7684\u6307\u5bfc\u4e0b\uff0c\u751f\u6210\u80fd\u591f\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u5e76\u8fdb\u884c\u4ea4\u4e92\u7684\u76ee\u6807\u4eba\u7269\u548c\u7269\u4f53\u3002\u6b64\u5916\uff0c\u5982\u679c\u7528\u6237\u63d0\u4f9b\u80cc\u666f\u56fe\u50cf\u548c\u76ee\u6807\u4eba\u7269/\u7269\u4f53\u51fa\u73b0\u7684\u4f4d\u7f6e\uff0cInteract-Custom\u8fd8\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6307\u5b9a\u8fd9\u4e9b\u5185\u5bb9\uff0c\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u7684\u5185\u5bb9\u53ef\u63a7\u6027\u3002\u901a\u8fc7\u9488\u5bf9CHOI\u4efb\u52a1\u5b9a\u5236\u7684\u6307\u6807\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u5b9e\u4f53\u7684\u5916\u89c2\u4fdd\u7559\uff0c\u800c\u5ffd\u7565\u4e86\u76ee\u6807\u5b9e\u4f53\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u63a7\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u51fa\u5b9a\u5236\u5316\u4eba\u4e0e\u7269\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\uff08CHOI\uff09\u4efb\u52a1\uff0c\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u548c\u4ea4\u4e92\u8bed\u4e49\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u540d\u4e3aInteract-Custom\u7684\u4e24\u9636\u6bb5\u6a21\u578b\u3002\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u751f\u6210\u524d\u666f\u63a9\u6a21\u6765\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u914d\u7f6e\uff0c\u63cf\u7ed8\u4ea4\u4e92\u884c\u4e3a\u3002\u7b2c\u4e8c\u9636\u6bb5\u5728\u63a9\u6a21\u7684\u6307\u5bfc\u4e0b\uff0c\u751f\u6210\u76ee\u6807\u4eba\u4e0e\u7269\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8eab\u4efd\u7279\u5f81\u5e76\u5b9e\u73b0\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u80cc\u666f\u56fe\u50cf\u548c\u76ee\u6807\u7269\u4f53\u51fa\u73b0\u7684\u4f4d\u7f6e\u3002", "result": "\u901a\u8fc7\u9488\u5bf9CHOI\u4efb\u52a1\u5b9a\u5236\u7684\u6307\u6807\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86Interact-Custom\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Interact-Custom\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5b9a\u5236\u5316\u4eba\u4e0e\u7269\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u4eba\u7269\u548c\u7269\u4f53\u7684\u8eab\u4efd\u4fdd\u7559\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u4ea4\u4e92\u8bed\u4e49\u7684\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4ea4\u4e92\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.19506", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19506", "abs": "https://arxiv.org/abs/2508.19506", "authors": ["Zhiyi Kuang", "Ryan Rong", "YuCheng Yuan", "Allen Nie"], "title": "Learning Game-Playing Agents with Generative Code Optimization", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent\n  Learning, Vancouver, Canada", "summary": "We present a generative optimization approach for learning game-playing\nagents, where policies are represented as Python programs and refined using\nlarge language models (LLMs). Our method treats decision-making policies as\nself-evolving code, with current observation as input and an in-game action as\noutput, enabling agents to self-improve through execution traces and natural\nlanguage feedback with minimal human intervention. Applied to Atari games, our\ngame-playing Python program achieves performance competitive with deep\nreinforcement learning (RL) baselines while using significantly less training\ntime and much fewer environment interactions. This work highlights the promise\nof programmatic policy representations for building efficient, adaptable agents\ncapable of complex, long-horizon reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u4f7f\u7528LLMs\u5c06\u7b56\u7565\u8868\u793a\u4e3aPython\u7a0b\u5e8f\uff0c\u5e76\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u8bad\u7ec3\u6e38\u620fAI\u3002\u8be5\u65b9\u6cd5\u80fd\u8ba9AI\u901a\u8fc7\u6267\u884c\u548c\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u96c5\u8fbe\u5229\u6e38\u620f\u4e2d\u53d6\u5f97\u4e86\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u548c\u73af\u5883\u4ea4\u4e92\u66f4\u5c11\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5f0f\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u6e38\u620fAI\u7684\u7b56\u7565\uff0c\u5c06\u7b56\u7565\u8868\u793a\u4e3aPython\u7a0b\u5e8f\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8AI\u7684\u5b66\u4e60\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u5c06\u51b3\u7b56\u7b56\u7565\u8868\u793a\u4e3a\u53ef\u81ea\u6211\u6f14\u5316\u7684\u4ee3\u7801\uff0c\u4ee5\u5f53\u524d\u89c2\u5bdf\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u6e38\u620f\u52a8\u4f5c\u4f5c\u4e3a\u8f93\u51fa\u3002\u5229\u7528\u6267\u884c\u8f68\u8ff9\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\uff0c\u5728\u6781\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\u3002", "result": "\u5728\u96c5\u8fbe\u5229\u6e38\u620f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6e38\u620fAI\u5728\u6027\u80fd\u4e0a\u80fd\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u548c\u73af\u5883\u4ea4\u4e92\u6b21\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u7f16\u7a0b\u7b56\u7565\u8868\u793a\u65b9\u6cd5\u6709\u671b\u7528\u4e8e\u6784\u5efa\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684AI\uff0c\u80fd\u591f\u8fdb\u884c\u590d\u6742\u3001\u957f\u5468\u671f\u7684\u63a8\u7406\u3002"}}
{"id": "2508.19633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19633", "abs": "https://arxiv.org/abs/2508.19633", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSALF\u7684\u65b0\u578b\u6846\u67b6\uff0c\u4f7f\u7528\u7b26\u53f7\u5bf9\u6297\u5b66\u4e60\u6765\u68c0\u6d4b\u548c\u751f\u6210\u590d\u6742\u7684\u865a\u5047\u65b0\u95fb\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u6570\u503c\u66f4\u65b0\u3002SALF\u901a\u8fc7\u751f\u6210\u548c\u68c0\u6d4b\u4ee3\u7406\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u8fa9\u8bba\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u64cd\u4f5c\u81ea\u7136\u8bed\u8a00\u8868\u793a\u6765\u6a21\u62df\u53cd\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u4e0b\u964d\u3002\u5b9e\u9a8c\u8bc1\u660eSALF\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u751f\u6210\u590d\u6742\u865a\u5047\u65b0\u95fb\u7684\u80fd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u8fd9\u589e\u52a0\u4e86\u865a\u5047\u65b0\u95fb\u7684\u98ce\u9669\u3002\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u52a8\u6001\u6f14\u53d8\u7684\u6027\u8d28\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSALF\uff08\u7b26\u53f7\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\uff09\u7684\u65b0\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u7b26\u53f7\u5b66\u4e60\u4f18\u5316\u7684\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u6570\u503c\u66f4\u65b0\u3002\u751f\u6210\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u5316\u8fa9\u8bba\u6765\u8bc6\u522b\u903b\u8f91\u548c\u4e8b\u5b9e\u8c2c\u8bef\uff0c\u68c0\u6d4b\u4ee3\u7406\u5219\u901a\u8fc7\u8fd9\u79cd\u5bf9\u6297\u4ea4\u4e92\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002SALF\u4f7f\u7528\u4ee3\u7406\u63d0\u793a\u5b9a\u4e49\u53ef\u5b66\u4e60\u7684\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u64cd\u4f5c\u81ea\u7136\u8bed\u8a00\u8868\u793a\u6765\u6a21\u62df\u53cd\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSALF\u751f\u6210\u7684\u590d\u6742\u865a\u5047\u65b0\u95fb\u5e73\u5747\u53ef\u5c06\u4e2d\u6587\u7684\u68c0\u6d4b\u6027\u80fd\u964d\u4f4e\u9ad8\u8fbe53.4%\uff0c\u82f1\u6587\u964d\u4f4e34.2%\u3002\u540c\u65f6\uff0cSALF\u8fd8\u80fd\u6539\u8fdb\u68c0\u6d4b\u5668\uff0c\u5c06\u5bf9\u6539\u8fdb\u540e\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\u63d0\u9ad8\u9ad8\u8fbe7.7%\u3002", "conclusion": "SALF\u901a\u8fc7\u5176\u72ec\u7279\u7684\u7b26\u53f7\u5bf9\u6297\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5e94\u5bf9\u590d\u6742\u4e14\u52a8\u6001\u6f14\u53d8\u7684\u865a\u5047\u65b0\u95fb\u65b9\u9762\u5c55\u73b0\u4e86\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u9002\u5e94\u6027\u5f3a\u7684\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.20028", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.20028", "abs": "https://arxiv.org/abs/2508.20028", "authors": ["Jaka Vodeb"], "title": "Microscopic Origin of Domain Wall Reconfiguration Dynamics in a Quantum Material via Quantum Simulation", "comment": "6 pages, 4 figures, comments are welcome", "summary": "Understanding how quantum materials relax from metastable states poses a\nfundamental challenge in condensed matter physics. In the layered\ndichalcogenide 1T-TaS$_2$, domain-wall-rich polaronic textures evolve toward a\nuniform ground state through reconfiguration events that exhibit a crossover\nfrom thermally activated to temperature-independent behavior-indicative of\nquantum tunneling. Here, we employ quantum simulation of a two-dimensional\ntransverse-field Ising model (TFIM) with longitudinal bias to uncover the\nmicroscopic processes underlying this relaxation. Using a Schrieffer-Wolff\ntransformation, we map the TFIM to a hardcore boson model, revealing that\nsingle-polaron tunneling events, rather than collective multi-particle\ntransitions, dominate domain wall motion. A scaling analysis of reconfiguration\nrates across varying transverse fields $h_x$ shows collapse when temperature is\nrescaled as $T \\to h_x^n T$ with $n \\approx 1.2$, confirming the dominance of\nfirst- and second-order single-particle processes. This enables us to\nreconstruct a microscopic relaxation pathway consisting of cyclical polaron\nleakage followed by cascades of tunneling events. Our results establish quantum\nsimulation as a powerful tool for inferring real-space mechanisms in strongly\ncorrelated systems and demonstrate a concrete strategy for bridging effective\nspin models with the non-equilibrium dynamics of quantum materials.", "AI": {"tldr": "\u7814\u7a76\u4e861T-TaS2\u4e2d\u4ece\u4e9a\u7a33\u6001\u5230\u57fa\u6001\u7684\u5f1b\u8c6b\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5355\u6781\u5316\u5b50\u96a7\u7a7f\u662f\u7574\u58c1\u8fd0\u52a8\u7684\u4e3b\u8981\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u6a21\u62df\u548c\u6807\u5ea6\u5206\u6790\u63ed\u793a\u4e86\u5fae\u89c2\u5f1b\u8c6b\u8def\u5f84\u3002", "motivation": "\u7406\u89e3\u91cf\u5b50\u6750\u6599\u5982\u4f55\u4ece\u4e9a\u7a33\u6001\u5f1b\u8c6b\u5230\u57fa\u6001\u662f\u51dd\u805a\u6001\u7269\u7406\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u57281T-TaS2\u8fd9\u79cd\u6750\u6599\u4e2d\uff0c\u5176\u7574\u58c1\u4e30\u5bcc\u7684\u6781\u5316\u5b50\u7ed3\u6784\u4f1a\u7ecf\u5386\u4ece\u70ed\u6fc0\u6d3b\u5230\u4e0e\u6e29\u5ea6\u65e0\u5173\u7684\u884c\u4e3a\u7684\u8f6c\u53d8\u3002", "method": "\u5229\u7528\u4e8c\u7ef4\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\uff08TFIM\uff09\u7684\u91cf\u5b50\u6a21\u62df\uff0c\u5e76\u901a\u8fc7Schrieffer-Wolff\u53d8\u6362\u5c06\u5176\u6620\u5c04\u5230\u786c\u6838\u73bb\u8272\u5b50\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5355\u6781\u5316\u5b50\u96a7\u7a7f\u662f\u7574\u58c1\u8fd0\u52a8\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u6a2a\u5411\u573a\u4e0b\u91cd\u6784\u901f\u7387\u968f\u6e29\u5ea6\u7684\u53d8\u5316\uff0c\u8bc1\u5b9e\u4e86\u5355\u7c92\u5b50\u8fc7\u7a0b\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "result": "\u901a\u8fc7\u6807\u5ea6\u5206\u6790\uff0c\u53d1\u73b0\u5728\u5c06\u6e29\u5ea6\u91cd\u6574\u5316\u4e3a $T \to h_x^n T$\uff08\u5176\u4e2d $n \tilde{} 1.2$\uff09\u65f6\uff0c\u91cd\u6784\u901f\u7387\u53ef\u4ee5\u5f88\u597d\u5730\u62df\u5408\uff0c\u8bc1\u5b9e\u4e86\u5355\u7c92\u5b50\u8fc7\u7a0b\uff08\u5305\u62ec\u4e00\u9636\u548c\u4e8c\u9636\uff09\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002\u91cd\u6784\u4e86\u7531\u6781\u5316\u5b50\u6cc4\u6f0f\u548c\u96a7\u7a7f\u4e8b\u4ef6\u7ea7\u8054\u7ec4\u6210\u7684\u5fae\u89c2\u5f1b\u8c6b\u8def\u5f84\u3002", "conclusion": "\u91cf\u5b50\u6a21\u62df\u662f\u63ed\u793a\u5f3a\u5173\u8054\u7cfb\u7edf\u4e2d\u771f\u5b9e\u7a7a\u95f4\u673a\u5236\u7684\u6709\u529b\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u6709\u6548\u81ea\u65cb\u6a21\u578b\u4e0e\u91cf\u5b50\u6750\u6599\u975e\u5e73\u8861\u52a8\u529b\u5b66\u8054\u7cfb\u8d77\u6765\u7684\u5177\u4f53\u7b56\u7565\u3002"}}
{"id": "2508.19579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19579", "abs": "https://arxiv.org/abs/2508.19579", "authors": ["Haomiao Zhang", "Miao Cao", "Xuan Yu", "Hui Luo", "Yanling Piao", "Mengjie Qin", "Zhangyuan Li", "Ping Wang", "Xin Yuan"], "title": "High-Speed FHD Full-Color Video Computer-Generated Holography", "comment": null, "summary": "Computer-generated holography (CGH) is a promising technology for\nnext-generation displays. However, generating high-speed, high-quality\nholographic video requires both high frame rate display and efficient\ncomputation, but is constrained by two key limitations: ($i$) Learning-based\nmodels often produce over-smoothed phases with narrow angular spectra, causing\nsevere color crosstalk in high frame rate full-color displays such as\ndepth-division multiplexing and thus resulting in a trade-off between frame\nrate and color fidelity. ($ii$) Existing frame-by-frame optimization methods\ntypically optimize frames independently, neglecting spatial-temporal\ncorrelations between consecutive frames and leading to computationally\ninefficient solutions. To overcome these challenges, in this paper, we propose\na novel high-speed full-color video CGH generation scheme. First, we introduce\nSpectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase\ndistributions via frequency modulation, enabling high-fidelity full-color\ndisplay at high frame rates. Second, we present HoloMamba, a lightweight\nasymmetric Mamba-Unet architecture that explicitly models spatial-temporal\ncorrelations across video sequences to enhance reconstruction quality and\ncomputational efficiency. Extensive simulated and real-world experiments\ndemonstrate that SGDDM achieves high-fidelity full-color display without\ncompromise in frame rate, while HoloMamba generates FHD (1080p) full-color\nholographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior\nstate-of-the-art Divide-Conquer-and-Merge Strategy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGDDM\u7684\u65b0\u578b\u8ba1\u7b97\u5168\u606f\u56fe\uff08CGH\uff09\u751f\u6210\u65b9\u6848\uff0c\u901a\u8fc7\u9891\u7387\u8c03\u5236\u4f18\u5316\u76f8\u4f4d\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5e27\u7387\u5168\u5f69\u663e\u793a\u3002\u540c\u65f6\uff0c\u4ed6\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aHoloMamba\u7684\u8f7b\u91cf\u7ea7Mamba-Unet\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u663e\u5f0f\u5730\u5bf9\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cHoloMamba\u53ef\u4ee5\u5728\u8d85\u8fc7260 FPS\u7684\u901f\u5ea6\u4e0b\u751f\u6210\u5168\u9ad8\u6e05\u5168\u5f69\u89c6\u9891\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5feb2.6\u500d\u4ee5\u4e0a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe\uff08CGH\uff09\u5728\u751f\u6210\u9ad8\u901f\u3001\u9ad8\u8d28\u91cf\u5168\u606f\u89c6\u9891\u65f6\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u4ea7\u751f\u7684\u76f8\u4f4d\u5e73\u6ed1\u3001\u89d2\u8c31\u7a84\uff0c\u5bfc\u81f4\u5728\u9ad8\u5e27\u7387\u5168\u5f69\u663e\u793a\u4e2d\u51fa\u73b0\u4e25\u91cd\u7684\u8272\u5f69\u4e32\u6270\uff1b2\uff09\u73b0\u6709\u7684\u9010\u5e27\u4f18\u5316\u65b9\u6cd5\u5ffd\u7565\u4e86\u5e27\u95f4\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGDDM\uff08Spectrum-Guided Depth Division Multiplexing\uff09\u7684\u65b0\u65b9\u6848\uff0c\u901a\u8fc7\u9891\u7387\u8c03\u5236\u4f18\u5316\u76f8\u4f4d\u5206\u5e03\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5168\u5f69\u663e\u793a\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHoloMamba\u7684\u8f7b\u91cf\u7ea7\u975e\u5bf9\u79f0Mamba-Unet\u67b6\u6784\uff0c\u7528\u4e8e\u663e\u5f0f\u5730\u5efa\u6a21\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "SGDDM\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5168\u5f69\u663e\u793a\uff0c\u6ca1\u6709\u727a\u7272\u5e27\u7387\u3002HoloMamba\u80fd\u591f\u4ee5\u8d85\u8fc7260 FPS\u7684\u901f\u5ea6\u751f\u6210\u5168\u9ad8\u6e05\uff081080p\uff09\u5168\u5f69\u89c6\u9891\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7684Divide-Conquer-and-Merge\u7b56\u7565\u5feb2.6\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u7684SGDDM\u548cHoloMamba\u65b9\u6848\u80fd\u591f\u6709\u6548\u5730\u514b\u670d\u73b0\u6709CGH\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u5e27\u7387\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5168\u606f\u89c6\u9891\u7684\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.19554", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19554", "abs": "https://arxiv.org/abs/2508.19554", "authors": ["Haruki Yonekura", "Ren Ozeki", "Tatsuya Amano", "Hamada Rizk", "Hirozumi Yamaguchi"], "title": "MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data", "comment": "Accepted to The 33rd ACM International Conference on Advances in\n  Geographic Information Systems(SIGSPATIAL '25) as a short paper in the Short\n  Paper Track", "summary": "Modern mobility platforms have stored vast streams of GPS trajectories,\ntemporal metadata, free-form textual notes, and other unstructured data.\nPrivacy statutes such as the GDPR require that any individual's contribution be\nunlearned on demand, yet retraining deep models from scratch for every request\nis untenable. We introduce MobText-SISA, a scalable machine-unlearning\nframework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)\ntraining to heterogeneous spatio-temporal data. MobText-SISA first embeds each\ntrip's numerical and linguistic features into a shared latent space, then\nemploys similarity-aware clustering to distribute samples across shards so that\nfuture deletions touch only a single constituent model while preserving\ninter-shard diversity. Each shard is trained incrementally; at inference time,\nconstituent predictions are aggregated to yield the output. Deletion requests\ntrigger retraining solely of the affected shard from its last valid checkpoint,\nguaranteeing exact unlearning. Experiments on a ten-month real-world mobility\nlog demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,\nand (ii) consistently outperforms random sharding in both error and convergence\nspeed. These results establish MobText-SISA as a practical foundation for\nprivacy-compliant analytics on multimodal mobility data at urban scale.", "AI": {"tldr": "MobText-SISA\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5220\u9664\u7528\u6237\u6570\u636e\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u57ce\u5e02\u89c4\u6a21\u7684\u591a\u6a21\u6001\u51fa\u884c\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21GPS\u8f68\u8ff9\u3001\u65f6\u95f4\u5143\u6570\u636e\u548c\u6587\u672c\u7b14\u8bb0\u7b49\u5f02\u6784\u65f6\u7a7a\u6570\u636e\u65f6\uff0c\u96be\u4ee5\u6ee1\u8db3GDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u7684\u7528\u6237\u6570\u636e\u5220\u9664\u9700\u6c42\uff0c\u56e0\u4e3a\u6bcf\u6b21\u5220\u9664\u90fd\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\u3002", "method": "MobText-SISA\u91c7\u7528SISA\uff08\u5206\u7247\u3001\u9694\u79bb\u3001\u5207\u7247\u548c\u805a\u5408\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u884c\u7a0b\u7684\u6570\u503c\u548c\u8bed\u8a00\u7279\u5f81\u5d4c\u5165\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u901a\u8fc7\u76f8\u4f3c\u611f\u77e5\u805a\u7c7b\u5c06\u6837\u672c\u5206\u914d\u5230\u5206\u7247\uff0c\u786e\u4fdd\u5220\u9664\u64cd\u4f5c\u4ec5\u5f71\u54cd\u5355\u4e2a\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u5206\u7247\u95f4\u7684\u591a\u6837\u6027\u3002\u6bcf\u4e2a\u5206\u7247\u8fdb\u884c\u589e\u91cf\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u805a\u5408\u5404\u5206\u7247\u9884\u6d4b\u3002\u5220\u9664\u8bf7\u6c42\u4ec5\u89e6\u53d1\u53d7\u5f71\u54cd\u5206\u7247\u4ece\u6700\u540e\u4e00\u4e2a\u6709\u6548\u68c0\u67e5\u70b9\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u9057\u5fd8\u3002", "result": "\u5728\u4e3a\u671f\u5341\u4e2a\u6708\u7684\u771f\u5b9e\u4e16\u754c\u51fa\u884c\u65e5\u5fd7\u5b9e\u9a8c\u4e2d\uff0cMobText-SISA\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u9519\u8bef\u7387\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u968f\u673a\u5206\u7247\u65b9\u6cd5\u3002", "conclusion": "MobText-SISA\u4e3a\u57ce\u5e02\u89c4\u6a21\u7684\u591a\u6a21\u6001\u51fa\u884c\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u5206\u6790\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5220\u9664\u7684\u6311\u6218\u3002"}}
{"id": "2508.19665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19665", "abs": "https://arxiv.org/abs/2508.19665", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs.", "AI": {"tldr": "SystemC\u6a21\u578b\u901a\u8fc7FMI\u6807\u51c6\u5b9e\u73b0\u81ea\u52a8\u5316\u5c01\u88c5\uff0c\u4ee5\u4fc3\u8fdb\u6c7d\u8f66\u884c\u4e1a\u7684\u534f\u540c\u4eff\u771f\u3002", "motivation": "\u6c7d\u8f66\u884c\u4e1a\u7684\u53d1\u5c55\u9700\u8981\u5f3a\u5927\u7684\u534f\u540c\u4eff\u771f\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u9886\u57df\u8fdb\u884c\u65e9\u671f\u9a8c\u8bc1\u548c\u65e0\u7f1d\u96c6\u6210\u3002\u7136\u800c\uff0c\u6807\u51c6\u5316\u63a5\u53e3\u7684\u7f3a\u4e4f\u548c\u4e13\u6709\u4eff\u771f\u5e73\u53f0\u7684\u666e\u53ca\u5bf9\u534f\u4f5c\u3001\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528FMI\u6807\u51c6\u81ea\u52a8\u5c01\u88c5SystemC\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5c06SystemC\u7684\u5efa\u6a21\u7cbe\u5ea6\u548c\u5feb\u901f\u4e0a\u5e02\u65f6\u95f4\u4e0eFMI\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u5c01\u88c5\u4f18\u52bf\u76f8\u7ed3\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b89\u5168\u3001\u53ef\u79fb\u690d\u5730\u5c06\u5d4c\u5165\u5f0f\u7ec4\u4ef6\u96c6\u6210\u5230\u534f\u540c\u4eff\u771f\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u534f\u540c\u4eff\u771f\u65b9\u6cd5\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u5c01\u88c5\u6027\u95ee\u9898\uff0c\u4e3a\u6c7d\u8f66\u884c\u4e1a\u7684\u534f\u540c\u4eff\u771f\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u652f\u6301\u3002"}}
{"id": "2508.19581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19581", "abs": "https://arxiv.org/abs/2508.19581", "authors": ["Dat Nguyen Cong", "Hieu Tran Bao", "Hoang Thanh-Tung"], "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction", "comment": "21 pages, 16 figures", "summary": "Diffusion models have gained prominence as state-of-the-art techniques for\nsynthesizing images and videos, particularly due to their ability to scale\neffectively with large datasets. Recent studies have uncovered that these\nextensive datasets often contain mistakes from manual labeling processes.\nHowever, the extent to which such errors compromise the generative capabilities\nand controllability of diffusion models is not well studied. This paper\nintroduces Score-based Discriminator Correction (SBDC), a guidance technique\nfor aligning noisy pre-trained conditional diffusion models. The guidance is\nbuilt on discriminator training using adversarial loss, drawing on prior noise\ndetection techniques to assess the authenticity of each sample. We further show\nthat limiting the usage of our guidance to the early phase of the generation\nprocess leads to better performance. Our method is computationally efficient,\nonly marginally increases inference time, and does not require retraining\ndiffusion models. Experiments on different noise settings demonstrate the\nsuperiority of our method over previous state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScore-based Discriminator Correction (SBDC)\u7684\u5f15\u5bfc\u6280\u672f\uff0c\u7528\u4e8e\u6821\u6b63\u542b\u6709\u9519\u8bef\u6807\u7b7e\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5224\u522b\u5668\u8bad\u7ec3\u548c\u5bf9\u6297\u6027\u635f\u5931\u6765\u8bc4\u4f30\u6837\u672c\u771f\u5b9e\u6027\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u8be5\u6280\u672f\u53ef\u83b7\u5f97\u66f4\u4f18\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u5305\u542b\u9519\u8bef\u6807\u7b7e\u7684\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u80fd\u529b\u548c\u53ef\u63a7\u6027\u65b9\u9762\u53d7\u5230\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u63d0\u51faScore-based Discriminator Correction (SBDC)\u5f15\u5bfc\u6280\u672f\uff0c\u5229\u7528\u5224\u522b\u5668\u8bad\u7ec3\u548c\u5bf9\u6297\u6027\u635f\u5931\uff0c\u5e76\u7ed3\u5408\u5148\u524d\u7684\u566a\u58f0\u68c0\u6d4b\u6280\u672f\u6765\u8bc4\u4f30\u6837\u672c\u771f\u5b9e\u6027\u3002\u5c06\u8be5\u6280\u672f\u9650\u5236\u5728\u751f\u6210\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSBDC\u5728\u4e0d\u540c\u566a\u58f0\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4ec5\u7565\u5fae\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "SBDC\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f15\u5bfc\u6280\u672f\uff0c\u53ef\u4ee5\u6821\u6b63\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u589e\u52a0\u663e\u8457\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.19563", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19563", "abs": "https://arxiv.org/abs/2508.19563", "authors": ["Hejia Liu", "Mochen Yang", "Gediminas Adomavicius"], "title": "Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting", "comment": null, "summary": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.", "AI": {"tldr": "LLMs\u5728\u6570\u636e\u62df\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5bf9\u4efb\u52a1\u65e0\u5173\u7684\u8f93\u5165\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u8fd9\u79cd\u654f\u611f\u6027\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u975e\u5747\u5300\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u4e13\u95e8\u4e3a\u6570\u636e\u62df\u5408\u8bbe\u8ba1\u7684\u6a21\u578b\uff08\u5982TabPFN\uff09\u4e5f\u5b58\u5728\u6b64\u95ee\u9898\uff0c\u4f46LLMs\u76ee\u524d\u7f3a\u4e4f\u57fa\u672c\u9c81\u68d2\u6027\uff0c\u4e0d\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u6570\u636e\u62df\u5408\u5de5\u5177\u3002", "motivation": "\u8bc4\u4f30LLMs\u4f5c\u4e3a\u6570\u636e\u62df\u5408\u5de5\u5177\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0e\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8868\u793a\u53d8\u5316\u65f6\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83LLMs\uff08\u5305\u62ec\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\uff09\u4ee5\u53caTabPFN\u5728\u9762\u5bf9\u53d8\u91cf\u91cd\u547d\u540d\u7b49\u4efb\u52a1\u65e0\u5173\u53d8\u5316\u65f6\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86LLM\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "LLMs\u5bf9\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8868\u793a\u53d8\u5316\uff08\u5982\u53d8\u91cf\u91cd\u547d\u540d\uff09\u975e\u5e38\u654f\u611f\uff0c\u9884\u6d4b\u8bef\u5dee\u53d8\u5316\u6700\u9ad8\u53ef\u8fbe82%\u3002\u8fd9\u79cd\u654f\u611f\u6027\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u4e0b\u5747\u5b58\u5728\uff0c\u5e76\u4e14\u4e0e\u6a21\u578b\u662f\u5426\u5f00\u653e\u6743\u91cd\u65e0\u5173\u3002LLM\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u663e\u793a\u51fa\u975e\u5747\u5300\u6027\uff0c\u67d0\u4e9b\u4f4d\u7f6e\u7684\u8bad\u7ec3\u6837\u672c\u548c\u53d8\u91cf\u540d/\u503c\u4f1a\u83b7\u5f97\u4e0d\u6210\u6bd4\u4f8b\u7684\u5173\u6ce8\u3002\u5373\u4f7f\u662f\u4e13\u95e8\u8bbe\u8ba1\u7684TabPFN\u4e5f\u672a\u80fd\u5b8c\u5168\u514d\u75ab\u8fd9\u79cd\u654f\u611f\u6027\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5177\u6709\u5f3a\u5927\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u6570\u636e\u62df\u5408\u4efb\u52a1\u4e2d\u5bf9\u4efb\u52a1\u65e0\u5173\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u4f5c\u4e3a\u53ef\u9760\u6570\u636e\u62df\u5408\u5de5\u5177\u7684\u5e94\u7528\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u63d0\u9ad8LLMs\u5728\u6570\u636e\u62df\u5408\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19667", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19667", "abs": "https://arxiv.org/abs/2508.19667", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "title": "Survey of Specialized Large Language Model", "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.", "AI": {"tldr": "\u4e13\u95e8\u7684LLM\u5728\u533b\u7597\u3001\u91d1\u878d\u3001\u6cd5\u5f8b\u548c\u6280\u672f\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u901a\u8fc7\u9886\u57df\u539f\u751f\u8bbe\u8ba1\u3001\u53c2\u6570\u6548\u7387\u548c\u591a\u6a21\u6001\u80fd\u529b\u7b49\u6280\u672f\u7a81\u7834\uff0c\u63d0\u9ad8\u4e86\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u901a\u7528LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5c40\u9650\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u9700\u8981\u4e13\u95e8\u7684LLM\u6765\u63d0\u5347\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u4e86\u4e13\u95e8LLM\u5728\u533b\u7597\u3001\u91d1\u878d\u3001\u6cd5\u5f8b\u548c\u6280\u672f\u9886\u57df\u7684\u6f14\u53d8\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u9886\u57df\u539f\u751f\u8bbe\u8ba1\u3001\u53c2\u6570\u6548\u7387\u548c\u591a\u6a21\u6001\u80fd\u529b\u7b49\u6280\u672f\u7a81\u7834\u3002", "result": "\u4e13\u95e8\u7684LLM\u5728\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e3a\u7535\u5b50\u5546\u52a1\u9886\u57df\u586b\u8865\u4e86\u5173\u952e\u7684\u7a7a\u767d\u3002", "conclusion": "\u4e13\u95e8\u7684LLM\u901a\u8fc7\u6280\u672f\u521b\u65b0\u514b\u670d\u4e86\u901a\u7528LLM\u7684\u5c40\u9650\u6027\uff0c\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5bf9\u7535\u5b50\u5546\u52a1\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.19593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19593", "abs": "https://arxiv.org/abs/2508.19593", "authors": ["Abhinav Kumar"], "title": "Generalizing Monocular 3D Object Detection", "comment": "PhD Thesis submitted to MSU", "summary": "Monocular 3D object detection (Mono3D) is a fundamental computer vision task\nthat estimates an object's class, 3D position, dimensions, and orientation from\na single image. Its applications, including autonomous driving, augmented\nreality, and robotics, critically rely on accurate 3D environmental\nunderstanding. This thesis addresses the challenge of generalizing Mono3D\nmodels to diverse scenarios, including occlusions, datasets, object sizes, and\ncamera parameters. To enhance occlusion robustness, we propose a mathematically\ndifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we\nexplore depth equivariant (DEVIANT) backbones. We address the issue of large\nobject detection, demonstrating that it's not solely a data imbalance or\nreceptive field problem but also a noise sensitivity issue. To mitigate this,\nwe introduce a segmentation-based approach in bird's-eye view with dice loss\n(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D\nmodels to unseen camera heights and improve Mono3D generalization in such\nout-of-distribution settings.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u591a\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\uff08Mono3D\uff09\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u5305\u62ec\u906e\u6321\u3001\u4e0d\u540c\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u548c\u76f8\u673a\u53c2\u6570\u3002", "motivation": "Mono3D\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u4efb\u52a1\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e8e\u5404\u79cd\u573a\u666f\uff08\u5982\u906e\u6321\u3001\u4e0d\u540c\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u548c\u76f8\u673a\u53c2\u6570\uff09\u3002", "method": "\u4e3a\u4e86\u63d0\u9ad8\u5bf9\u906e\u6321\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u53ef\u5fae\u7684NMS\uff08GrooMeD-NMS\uff09\u3002\u4e3a\u4e86\u63d0\u9ad8\u5bf9\u65b0\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a2\u7d22\u4e86\u6df1\u5ea6\u7b49\u53d8\uff08DEVIANT\uff09\u9aa8\u5e72\u7f51\u7edc\u3002\u9488\u5bf9\u5927\u7269\u4f53\u68c0\u6d4b\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u4e0d\u4ec5\u662f\u6570\u636e\u4e0d\u5e73\u8861\u6216\u611f\u53d7\u91ce\u7684\u95ee\u9898\uff0c\u4e5f\u662f\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u9e1f\u77b0\u56fe\u65b9\u6cd5\uff08SeaBird\uff09\u7ed3\u5408Dice\u635f\u5931\u6765\u89e3\u51b3\u3002\u6700\u540e\uff0c\u5bf9Mono3D\u6a21\u578b\u5916\u63a8\u5230\u672a\u89c1\u8fc7\u7684\u76f8\u673a\u9ad8\u5ea6\u8fdb\u884c\u4e86\u6570\u5b66\u5206\u6790\uff0c\u5e76\u6539\u8fdb\u4e86\u5728\u8fd9\u79cd\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u63d0\u51faGrooMeD-NMS\u63d0\u9ad8\u4e86\u906e\u6321\u9c81\u68d2\u6027\uff0cDEVIANT\u9aa8\u5e72\u7f51\u7edc\u6539\u5584\u4e86\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0cSeaBird\u89e3\u51b3\u4e86\u5927\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u5e76\u5bf9\u76f8\u673a\u9ad8\u5ea6\u5916\u63a8\u8fdb\u884c\u4e86\u6570\u5b66\u5206\u6790\u548c\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7GrooMeD-NMS\u3001DEVIANT\u9aa8\u5e72\u7f51\u7edc\u3001SeaBird\u4ee5\u53ca\u5bf9\u76f8\u673a\u9ad8\u5ea6\u5916\u63a8\u7684\u5206\u6790\uff0c\u672c\u8bba\u6587\u663e\u8457\u63d0\u5347\u4e86Mono3D\u6a21\u578b\u5728\u5404\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19564", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19564", "abs": "https://arxiv.org/abs/2508.19564", "authors": ["Yuhang Liu", "Tao Li", "Zhehao Huang", "Zuopeng Yang", "Xiaolin Huang"], "title": "Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models", "comment": null, "summary": "Fine-tuning large-scale pre-trained models with limited data presents\nsignificant challenges for generalization. While Sharpness-Aware Minimization\n(SAM) has proven effective in improving generalization by seeking flat minima,\nits substantial extra memory and computation overhead make it impractical for\nlarge models. Integrating SAM with parameter-efficient fine-tuning methods like\nLow-Rank Adaptation (LoRA) is a promising direction. However, we find that\ndirectly applying SAM to LoRA parameters limits the sharpness optimization to a\nrestricted subspace, hindering its effectiveness. To address this limitation,\nwe propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an\nauxiliary LoRA module to model SAM's adversarial weight perturbations. It\ndecouples SAM's weight perturbations from LoRA optimization: the primary LoRA\nmodule adapts to specific tasks via standard gradient descent, while the\nauxiliary module captures the sharpness of the loss landscape through gradient\nascent. Such dual-module design enables Bi-LoRA to capture broader sharpness\nfor achieving flatter minima while remaining memory-efficient. Another\nimportant benefit is that the dual design allows for simultaneous optimization\nand perturbation, eliminating SAM's doubled training costs. Extensive\nexperiments across diverse tasks and architectures demonstrate Bi-LoRA's\nefficiency and effectiveness in enhancing generalization.", "AI": {"tldr": "SAM\u5bf9\u4e8e\u5927\u578b\u6a21\u578b\u6765\u8bf4\u8fc7\u4e8e\u6602\u8d35\uff0cBi-LoRA\u901a\u8fc7\u5f15\u5165\u8f85\u52a9LoRA\u6a21\u5757\u6765\u4e3aSAM\u7684\u6270\u52a8\u5efa\u6a21\uff0c\u4ece\u800c\u5728\u4e0d\u589e\u52a0\u989d\u5916\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76f4\u63a5\u5c06SAM\u5e94\u7528\u4e8eLoRA\u53c2\u6570\u4f1a\u9650\u5236\u5176\u5728\u72ed\u7a84\u7684\u5b50\u7a7a\u95f4\u4e2d\u4f18\u5316\uff0c\u4ece\u800c\u5f71\u54cd\u5176\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBi-LoRA\u7684\u53cc\u5411\u4f4e\u79e9\u9002\u914d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u8f85\u52a9LoRA\u6a21\u5757\u6765\u6a21\u62dfSAM\u7684\u5bf9\u6297\u6027\u6743\u91cd\u6270\u52a8\uff0c\u5c06SAM\u7684\u6743\u91cd\u6270\u52a8\u4e0eLoRA\u4f18\u5316\u5206\u79bb\u3002\u4e3b\u8981LoRA\u6a21\u5757\u901a\u8fc7\u6807\u51c6\u7684\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u4efb\u52a1\u81ea\u9002\u5e94\uff0c\u800c\u8f85\u52a9\u6a21\u5757\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u6765\u6355\u6349\u635f\u5931\u666f\u89c2\u7684\u9510\u5ea6\u3002", "result": "Bi-LoRA\u80fd\u591f\u6355\u6349\u66f4\u5e7f\u6cdb\u7684\u9510\u5ea6\u4ee5\u5b9e\u73b0\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\uff0c\u5e76\u4e14\u6ca1\u6709\u589e\u52a0SAM\u7684\u53cc\u91cd\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "Bi-LoRA\u5728\u5404\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.19689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19689", "abs": "https://arxiv.org/abs/2508.19689", "authors": ["Xiaoying Zhang"], "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u7684\u4efb\u52a1\u578b\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u4ee5\u5e94\u5bf9\u5728\u5bf9\u8bdd\u7814\u7a76\u4e2d\u521b\u5efa\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u4efb\u52a1\u673a\u5668\u4eba\u7684\u6311\u6218\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u6700\u5c0f\u6216\u96f6\u4eba\u4e3a\u5e72\u9884\u7684\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u4efb\u52a1\u673a\u5668\u4eba\u662f\u5bf9\u8bdd\u7814\u7a76\u4e2d\u7684\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u521b\u5efa\u6b64\u7c7b\u673a\u5668\u4eba\u7684\u969c\u788d\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\u7684\u521b\u65b0\u6280\u672f\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u672c\u8bba\u6587\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u89c1\u89e3\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19600", "abs": "https://arxiv.org/abs/2508.19600", "authors": ["Toghrul Karimov", "Hassan Imani", "Allan Kazakov"], "title": "Quantization Robustness to Input Degradations for Object Detection", "comment": null, "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86YOLO\u6a21\u578b\u5728\u4e0d\u540c\u91cf\u5316\u7cbe\u5ea6\uff08FP32, FP16, UINT8, INT8\uff09\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u9000\u5316\u60c5\u51b5\u7684\u6821\u51c6\u7b56\u7565\u3002\u7ed3\u679c\u663e\u793a\uff0cINT8\u91cf\u5316\u5728\u52a0\u901f\u6a21\u578b\u7684\u540c\u65f6\u4f1a\u964d\u4f4e\u7cbe\u5ea6\uff0c\u800c\u6240\u63d0\u51fa\u7684\u6821\u51c6\u7b56\u7565\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5e76\u672a\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u5404\u79cd\u56fe\u50cf\u9000\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5bf9\u5927\u578b\u6a21\u578b\u6709\u4e00\u5b9a\u6548\u679c\u3002", "motivation": "\u90e8\u7f72YOLO\u7b49\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5230\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u65f6\uff0c\u91cf\u5316\u662f\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0c\u4f4e\u7cbe\u5ea6\u5bf9\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u8f93\u5165\u9000\u5316\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u3001\u538b\u7f29\u4f2a\u5f71\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u5f71\u54cd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u5bf9YOLO\u6a21\u578b\uff08\u4e0d\u540c\u5c3a\u5bf8\uff09\u5728FP32, FP16, UINT8, INT8\u56db\u79cd\u7cbe\u5ea6\u683c\u5f0f\u4e0b\u8fdb\u884c\u4e86\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u9000\u5316\u7684\u6821\u51c6\u7b56\u7565\uff0c\u5c06\u539f\u59cb\u56fe\u50cf\u548c\u5408\u6210\u9000\u5316\u56fe\u50cf\u6df7\u5408\u7528\u4e8eTensorRT\u7684\u6821\u51c6\u8fc7\u7a0b\u3002\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4e03\u79cd\u4e0d\u540c\u7684\u9000\u5316\u6761\u4ef6\uff08\u566a\u58f0\u3001\u6a21\u7cca\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u3001JPEG\u538b\u7f29\uff09\u548c\u6df7\u5408\u9000\u5316\u573a\u666f\u4e0b\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u9759\u6001INT8 TensorRT\u5f15\u64ce\u5728\u539f\u59cb\u6570\u636e\u4e0a\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u52a0\u901f\uff08\u7ea61.5-3.3\u500d\uff09\uff0c\u4f46\u7cbe\u5ea6\u6709\u6240\u4e0b\u964d\uff08\u7ea63-7% mAP50-95\uff09\u3002\u63d0\u51fa\u7684\u9762\u5411\u9000\u5316\u7684\u6821\u51c6\u7b56\u7565\u5728\u5927\u591a\u6570\u6a21\u578b\u548c\u9000\u5316\u6761\u4ef6\u4e0b\u5e76\u672a\u5e26\u6765\u6301\u7eed\u5e7f\u6cdb\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002\u5728\u7279\u5b9a\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u5927\u578b\u6a21\u578b\u8868\u73b0\u51fa\u4e00\u5b9a\u6548\u679c\uff0c\u8868\u660e\u6a21\u578b\u5bb9\u91cf\u53ef\u80fd\u5f71\u54cd\u6821\u51c6\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9759\u6001INT8\u91cf\u5316\u80fd\u663e\u8457\u52a0\u901fYOLO\u6a21\u578b\uff0c\u4f46\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6240\u63d0\u51fa\u7684\u9000\u5316\u611f\u77e5\u6821\u51c6\u65b9\u6cd5\u5728\u63d0\u9ad8\u91cf\u5316\u6a21\u578b\u5728\u4e0d\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5404\u79cd\u56fe\u50cf\u9000\u5316\u65f6\u3002\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u91cf\u5316\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19567", "abs": "https://arxiv.org/abs/2508.19567", "authors": ["Sheryl Mathew", "N Harshit"], "title": "Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning", "comment": null, "summary": "In reinforcement learning with human feedback (RLHF), reward models can\nefficiently learn and amplify latent biases within multimodal datasets, which\ncan lead to imperfect policy optimization through flawed reward signals and\ndecreased fairness. Bias mitigation studies have often applied passive\nconstraints, which can fail under causal confounding. Here, we present a\ncounterfactual reward model that introduces causal inference with multimodal\nrepresentation learning to provide an unsupervised, bias-resilient reward\nsignal. The heart of our contribution is the Counterfactual Trust Score, an\naggregated score consisting of four components: (1) counterfactual shifts that\ndecompose political framing bias from topical bias; (2) reconstruction\nuncertainty during counterfactual perturbations; (3) demonstrable violations of\nfairness rules for each protected attribute; and (4) temporal reward shifts\naligned with dynamic trust measures. We evaluated the framework on a multimodal\nfake versus true news dataset, which exhibits framing bias, class imbalance,\nand distributional drift. Following methodologies similar to unsupervised drift\ndetection from representation-based distances [1] and temporal robustness\nbenchmarking in language models [2], we also inject synthetic bias across\nsequential batches to test robustness. The resulting system achieved an\naccuracy of 89.12% in fake news detection, outperforming the baseline reward\nmodels. More importantly, it reduced spurious correlations and unfair\nreinforcement signals. This pipeline outlines a robust and interpretable\napproach to fairness-aware RLHF, offering tunable bias reduction thresholds and\nincreasing reliability in dynamic real-time policy making.", "AI": {"tldr": "RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u4f1a\u653e\u5927\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u7b56\u7565\u4f18\u5316\u4e0d\u5b8c\u5584\u548c\u516c\u5e73\u6027\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u548c\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u3001\u6297\u504f\u89c1\u7684\u5956\u52b1\u4fe1\u53f7\u3002\u8be5\u6a21\u578b\u7684\u6838\u5fc3\u662f\u53cd\u4e8b\u5b9e\u4fe1\u4efb\u5f97\u5206\uff0c\u5305\u542b\u56db\u4e2a\u90e8\u5206\uff1a\u53cd\u4e8b\u5b9e\u79fb\u4f4d\u3001\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\u3001\u516c\u5e73\u6027\u89c4\u5219\u8fdd\u53cd\u548c\u65f6\u95f4\u5956\u52b1\u79fb\u4f4d\u3002\u5728\u591a\u6a21\u6001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u9762\u51c6\u786e\u7387\u8fbe\u523089.12%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\u548c\u4e0d\u516c\u5e73\u7684\u5f3a\u5316\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u4e3a\u516c\u5e73\u611f\u77e5RLHF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\u3002", "motivation": "RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u4f1a\u653e\u5927\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u7b56\u7565\u4f18\u5316\u4e0d\u5b8c\u5584\u548c\u516c\u5e73\u6027\u4e0b\u964d\uff0c\u800c\u4f20\u7edf\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u5728\u56e0\u679c\u6df7\u6dc6\u4e0b\u53ef\u80fd\u5931\u6548\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u548c\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u6784\u5efa\u5305\u542b\u53cd\u4e8b\u5b9e\u79fb\u4f4d\u3001\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\u3001\u516c\u5e73\u6027\u89c4\u5219\u8fdd\u53cd\u548c\u65f6\u95f4\u5956\u52b1\u79fb\u4f4d\u56db\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u53cd\u4e8b\u5b9e\u4fe1\u4efb\u5f97\u5206\uff0c\u4ee5\u63d0\u4f9b\u65e0\u76d1\u7763\u3001\u6297\u504f\u89c1\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u6a21\u6001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e8689.12%\u7684\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\u548c\u4e0d\u516c\u5e73\u7684\u5f3a\u5316\u4fe1\u53f7\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u6ce8\u5165\u504f\u5dee\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u516c\u5e73\u611f\u77e5RLHF\u65b9\u6cd5\uff0c\u5177\u6709\u53ef\u8c03\u7684\u504f\u89c1\u964d\u4f4e\u9608\u503c\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u5b9e\u65f6\u7b56\u7565\u5236\u5b9a\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.19720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19720", "abs": "https://arxiv.org/abs/2508.19720", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.", "AI": {"tldr": "CSKS\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u4e24\u4e2a\u4ee3\u7406\u6a21\u578b\u6765\u8c03\u6574LLM\u5bf9\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u654f\u611f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9LLM\u77e5\u8bc6\u6765\u6e90\u7684\u8fde\u7eed\u3001\u7cbe\u786e\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406LLM\u7684\u77e5\u8bc6\u51b2\u7a81\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u9002\u5e94\u65b9\u9762\u5b58\u5728\u6548\u7387\u4f4e\u3001\u5bf9\u9ed1\u76d2\u6a21\u578b\u4e0d\u9002\u7528\u6216\u65e0\u6cd5\u6301\u7eed\u8c03\u6574\u654f\u611f\u5ea6\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51faCSKS\u6846\u67b6\u3002", "method": "CSKS\u6846\u67b6\u901a\u8fc7\u5fae\u8c03\u4e24\u4e2a\u5c0f\u7684\u4ee3\u7406\u6a21\u578b\uff08proxy models\uff09\uff0c\u5229\u7528\u5b83\u4eec\u8f93\u51fa\u5206\u5e03\u7684\u5dee\u5f02\u6765\u8c03\u6574LLM\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u800c\u65e0\u9700\u4fee\u6539LLM\u7684\u539f\u59cb\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCSKS\u6846\u67b6\u80fd\u591f\u5bf9LLM\u5bf9\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u654f\u611f\u5ea6\u8fdb\u884c\u8fde\u7eed\u3001\u7cbe\u786e\u7684\u63a7\u5236\uff0c\u65e2\u53ef\u4ee5\u63d0\u9ad8\u654f\u611f\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u964d\u4f4e\u654f\u611f\u5ea6\uff0c\u4f7fLLM\u80fd\u591f\u7075\u6d3b\u5730\u4f18\u5148\u8003\u8651\u4e0a\u4e0b\u6587\u77e5\u8bc6\u6216\u5176\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u3002", "conclusion": "CSKS\u6846\u67b6\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5730\u4f7fLLM\u9002\u5e94\u65b0\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19857", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19857", "abs": "https://arxiv.org/abs/2508.19857", "authors": ["Omar Bacarreza", "Thorin Farnsworth", "Alexander Makarovskiy", "Hugo Wallner", "Tessa Hicks", "Santiago Sempere-Llagostera", "John Price", "Robert J. A. Francis-Jones", "William R. Clements"], "title": "Quantum latent distributions in deep generative models", "comment": null, "summary": "Many successful families of generative models leverage a low-dimensional\nlatent distribution that is mapped to a data distribution. Though simple latent\ndistributions are commonly used, it has been shown that more sophisticated\ndistributions can improve performance. For instance, recent work has explored\nusing the distributions produced by quantum processors and found empirical\nimprovements. However, when latent space distributions produced by quantum\nprocessors can be expected to improve performance, and whether these\nimprovements are reproducible, are open questions that we investigate in this\nwork. We prove that, under certain conditions, these \"quantum latent\ndistributions\" enable generative models to produce data distributions that\nclassical latent distributions cannot efficiently produce. We also provide\nactionable intuitions to identify when such quantum advantages may arise in\nreal-world settings. We perform benchmarking experiments on both a synthetic\nquantum dataset and the QM9 molecular dataset, using both simulated and real\nphotonic quantum processors. Our results demonstrate that quantum latent\ndistributions can lead to improved generative performance in GANs compared to a\nrange of classical baselines. We also explore diffusion and flow matching\nmodels, identifying architectures compatible with quantum latent distributions.\nThis work confirms that near-term quantum processors can expand the\ncapabilities of deep generative models.", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4f7f\u7528\u91cf\u5b50\u5904\u7406\u5668\u751f\u6210\u7684\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u53ef\u4ee5\u63d0\u5347\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u6bd4\u7ecf\u5178\u6f5c\u5728\u5206\u5e03\u5177\u6709\u66f4\u4f18\u7684\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6027\u80fd\u63d0\u5347\u7684\u53ef\u884c\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "\u5229\u7528\u91cf\u5b50\u5904\u7406\u5668\u751f\u6210\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\u3002\u901a\u8fc7\u5728\u5408\u6210\u91cf\u5b50\u6570\u636e\u96c6\u548cQM9\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u7ecf\u5178\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728GANs\u4e2d\u76f8\u6bd4\u7ecf\u5178\u57fa\u7ebf\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u786e\u8ba4\u4e86\u8fd1\u7aef\u91cf\u5b50\u5904\u7406\u5668\u80fd\u591f\u6269\u5c55\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd1\u7aef\u91cf\u5b50\u5904\u7406\u5668\u80fd\u591f\u6269\u5c55\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u751f\u6210\u7ecf\u5178\u6f5c\u5728\u5206\u5e03\u65e0\u6cd5\u6709\u6548\u751f\u6210\u7684\u540e\u9a8c\u5206\u5e03\u3002"}}
{"id": "2508.19604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19604", "abs": "https://arxiv.org/abs/2508.19604", "authors": ["Qizhe Fan", "Chaoyu Liu", "Zhonghua Qiao", "Xiaoqin Shen"], "title": "IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) focuses on training a model\nusing labeled data from a source domain, with the goal of achieving robust\ngeneralization to unseen target domains during inference. A common approach to\nimprove generalization is to augment the source domain with synthetic data\ngenerated by diffusion models (DMs). However, the generated images often\ncontain structural or semantic defects due to training imperfections. Training\nsegmentation models with such flawed data can lead to performance degradation\nand error accumulation. To address this issue, we propose to integrate inverse\nevolution layers (IELs) into the generative process. IELs are designed to\nhighlight spatial discontinuities and semantic inconsistencies using\nLaplacian-based priors, enabling more effective filtering of undesirable\ngenerative patterns. Based on this mechanism, we introduce IELDM, an enhanced\ndiffusion-based data augmentation framework that can produce higher-quality\nimages. Furthermore, we observe that the defect-suppression capability of IELs\ncan also benefit the segmentation network by suppressing artifact propagation.\nBased on this insight, we embed IELs into the decoder of the DGSS model and\npropose IELFormer to strengthen generalization capability in cross-domain\nscenarios. To further strengthen the model's semantic consistency across\nscales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,\nwhich performs frequency-domain analysis to achieve structured integration of\nmulti-resolution features, thereby improving cross-scale coherence. Extensive\nexperiments on benchmark datasets demonstrate that our approach achieves\nsuperior generalization performance compared to existing methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIELFormer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\uff08DGSS\uff09\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u9006\u8fdb\u5316\u5c42\uff08IELs\uff09\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5206\u5272\u6a21\u578b\u7684\u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u6291\u5236\u4f2a\u5f71\u4f20\u64ad\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u4e00\u4e2a\u591a\u5c3a\u5ea6\u9891\u7387\u878d\u5408\uff08MFF\uff09\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u8de8\u5c3a\u5ea6\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\uff08DGSS\uff09\u7684\u76ee\u6807\u662f\u5728\u6e90\u57df\u7684\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u672a\u77e5\u7684\u76ee\u6807\u57df\u4e0a\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\u3002\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5e38\u7528\u4e8e\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u751f\u6210\u7684\u56fe\u50cf\u53ef\u80fd\u5b58\u5728\u7f3a\u9677\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u5e76\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u9006\u8fdb\u5316\u5c42\uff08IELs\uff09\u6574\u5408\u5230\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u8fc7\u6ee4\u4e0d\u826f\u7684\u751f\u6210\u6a21\u5f0f\uff0c\u4ece\u800c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff08IELDM\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5c06IELs\u5d4c\u5165DGSS\u6a21\u578b\u7684\u89e3\u7801\u5668\u4e2d\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u9891\u7387\u878d\u5408\uff08MFF\uff09\u6a21\u5757\uff0c\u5f62\u6210IELFormer\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u5c3a\u5ea6\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5f15\u5165IELs\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u589e\u5f3a\u6570\u636e\uff08IELDM\uff09\u3002\u5c06IELs\u5d4c\u5165DGSS\u6a21\u578b\u7684\u89e3\u7801\u5668\u5e76\u7ed3\u5408MFF\u6a21\u5757\u7684IELFormer\u6a21\u578b\uff0c\u5728\u8de8\u57df\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684IELDM\u548cIELFormer\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u9006\u8fdb\u5316\u5c42\uff08IELs\uff09\u548c\u591a\u5c3a\u5ea6\u9891\u7387\u878d\u5408\uff08MFF\uff09\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u8de8\u5c3a\u5ea6\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.19570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19570", "abs": "https://arxiv.org/abs/2508.19570", "authors": ["Dawei Li", "Yue Huang", "Ming Li", "Tianyi Zhou", "Xiangliang Zhang", "Huan Liu"], "title": "Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era", "comment": "Accepted by CIKM 2025 Tutorial", "summary": "Generative models such as Large Language Models, Diffusion Models, and\ngenerative adversarial networks have recently revolutionized the creation of\nsynthetic data, offering scalable solutions to data scarcity, privacy, and\nannotation challenges in data mining. This tutorial introduces the foundations\nand latest advances in synthetic data generation, covers key methodologies and\npractical frameworks, and discusses evaluation strategies and applications.\nAttendees will gain actionable insights into leveraging generative synthetic\ndata to enhance data mining research and practice. More information can be\nfound on our website: https://syndata4dm.github.io/.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\uff08\u5982 LLM\u3001\u6269\u6563\u6a21\u578b\u3001GAN\uff09\u5728\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u548c\u6807\u6ce8\u6311\u6218\u3002\u672c\u6559\u7a0b\u4ecb\u7ecd\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u57fa\u7840\u3001\u6700\u65b0\u8fdb\u5c55\u3001\u5173\u952e\u65b9\u6cd5\u3001\u5b9e\u7528\u6846\u67b6\u3001\u8bc4\u4f30\u7b56\u7565\u548c\u5e94\u7528\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u6316\u6398\u4e2d\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u548c\u6807\u6ce8\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6559\u7a0b\u4ecb\u7ecd\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u57fa\u7840\u3001\u6700\u65b0\u8fdb\u5c55\u3001\u5173\u952e\u65b9\u6cd5\u3001\u5b9e\u7528\u6846\u67b6\u3001\u8bc4\u4f30\u7b56\u7565\u548c\u5e94\u7528\u3002", "result": "\u4e0e\u4f1a\u8005\u5c06\u83b7\u5f97\u5173\u4e8e\u5229\u7528\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u52a0\u5f3a\u6570\u636e\u6316\u6398\u7814\u7a76\u548c\u5b9e\u8df5\u7684\u53ef\u884c\u89c1\u89e3\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u4e3a\u6570\u636e\u6316\u6398\u5e26\u6765\u4e86\u5408\u6210\u6570\u636e\u7684\u9769\u547d\uff0c\u672c\u6559\u7a0b\u5c06\u63d0\u4f9b\u76f8\u5173\u77e5\u8bc6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2508.19721", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19721", "abs": "https://arxiv.org/abs/2508.19721", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rub\u00e9n Solera-Ure\u00f1a", "S\u00e9rgio Paulo", "Mariana Juli\u00e3o", "Thomas Rolland", "John Mendon\u00e7a", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "title": "CAM\u00d5ES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties.", "AI": {"tldr": "\u73b0\u6709\u8461\u8404\u7259\u8bed\u8bed\u97f3\u8bc6\u522b\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\uff0c\u6b27\u6d32\u8461\u8404\u7259\u8bed\uff08EP\uff09\u53ca\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u4f53\u5219\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86CAM\u00d5ES\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u5305\u542b46\u5c0f\u65f6EP\u6d4b\u8bd5\u6570\u636e\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u4e00\u7cfb\u5217\u5148\u8fdb\u6a21\u578b\uff08\u5305\u62ec\u96f6\u6837\u672c\u3001\u5fae\u8c03\u7684\u57fa\u91d1\u6a21\u578b\u548c\u4ece\u5934\u8bad\u7ec3\u7684E-Branchformer\u6a21\u578b\uff09\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002EP\u7684\u5fae\u8c03\u57fa\u91d1\u6a21\u578b\u548cE-Branchformer\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u5747\u5b9e\u73b0\u4e86\u6bd4\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u91d1\u6a21\u578b\u9ad835%\u4ee5\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u6539\u8fdb\uff0c\u8fbe\u5230\u4e86EP\u53ca\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u4f53\u7684\u65b0state-of-the-art\u3002", "motivation": "\u586b\u8865\u6b27\u6d32\u8461\u8404\u7259\u8bed\uff08EP\uff09\u53ca\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u4f53\u5728\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u9886\u57df\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u73b0\u6709\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCAM\u00d5ES\u6846\u67b6\uff0c\u5305\u542b\uff081\uff09\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0846\u5c0f\u65f6EP\u6d4b\u8bd5\u6570\u636e\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff09\uff1b\uff082\uff09\u4e00\u7cfb\u5217\u5148\u8fdb\u6a21\u578b\uff08\u8bc4\u4f30\u57fa\u91d1\u6a21\u578b\u7684\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\uff0c\u4ee5\u53ca\u4ece\u5934\u8bad\u7ec3\u7684E-Branchformer\u6a21\u578b\uff09\u3002\u4f7f\u7528425\u5c0f\u65f6EP\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u548c\u8bad\u7ec3\u3002", "result": "\u5fae\u8c03\u57fa\u91d1\u6a21\u578b\u548cE-Branchformer\u6a21\u578b\u5728EP\u4e0a\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u6700\u4f73\u6a21\u578b\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u76f8\u6bd4\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u91d1\u6a21\u578b\u6709\u8d85\u8fc735%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u521b\u4e0bEP\u53ca\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u4f53\u7684\u65b0state-of-the-art\u3002", "conclusion": "CAM\u00d5ES\u6846\u67b6\u4e3aEP\u53ca\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u4f53\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u65b0\u7684state-of-the-art\u6c34\u5e73\u3002"}}
{"id": "2508.19626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19626", "abs": "https://arxiv.org/abs/2508.19626", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J. Ong", "Zongyuan Ge", "Lei Zhang"], "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "comment": "11 pages, 4 figures", "summary": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.", "AI": {"tldr": "LF-VAR\u662f\u4e00\u4e2a\u5229\u7528\u91cf\u5316\u75c5\u53d8\u6d4b\u91cf\u5206\u6570\u548c\u75c5\u53d8\u7c7b\u578b\u6807\u7b7e\u6765\u6307\u5bfc\u4e34\u5e8a\u76f8\u5173\u548c\u53ef\u63a7\u76ae\u80a4\u56fe\u50cf\u5408\u6210\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4f4e\u4e14\u7f3a\u4e4f\u5bf9\u75c5\u53d8\u4f4d\u7f6e\u548c\u7c7b\u578b\u63a7\u5236\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8bad\u7ec3\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(VQVAE)\u548c\u89c6\u89c9\u81ea\u56de\u5f52(VAR)Transformer\uff0c\u5e76\u7ed3\u5408\u75c5\u53d8\u533a\u57df\u6d4b\u91cf\u548c\u7c7b\u578b\u4f5c\u4e3a\u6761\u4ef6\u5d4c\u5165\uff0cLF-VAR\u80fd\u591f\u6839\u636e\u8bed\u8a00\u63d0\u793a\u751f\u6210\u5177\u6709\u7279\u5b9a\u75c5\u53d8\u7279\u5f81\u7684\u9ad8\u4fdd\u771f\u3001\u4e34\u5e8a\u76f8\u5173\u5408\u6210\u76ae\u80a4\u56fe\u50cf\uff0c\u5728\u4e03\u79cd\u75c5\u53d8\u7c7b\u578b\u7684FID\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd56.3%\u3002", "motivation": "\u73b0\u6709\u76ae\u80a4\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u56fe\u50cf\u8d28\u91cf\u4f4e\u3001\u75c5\u53d8\u4f4d\u7f6e\u548c\u7c7b\u578b\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7684\u6570\u636e\u91cf\u3002", "method": "\u63d0\u51faLF-VAR\u6a21\u578b\uff0c\u5229\u7528\u91cf\u5316\u75c5\u53d8\u6d4b\u91cf\u5206\u6570\u548c\u75c5\u53d8\u7c7b\u578b\u6807\u7b7e\u6307\u5bfc\u5408\u6210\u3002\u8bad\u7ec3\u591a\u5c3a\u5ea6VQVAE\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u79bb\u6563\u7684\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u8bad\u7ec3VAR Transformer\u4fc3\u8fdb\u56fe\u50cf\u5408\u6210\u3002\u5c06\u75c5\u53d8\u6d4b\u91cf\u548c\u7c7b\u578b\u4f5c\u4e3a\u6761\u4ef6\u5d4c\u5165\u4ee5\u589e\u5f3a\u5408\u6210\u4fdd\u771f\u5ea6\u3002", "result": "LF-VAR\u5728\u4e03\u79cd\u75c5\u53d8\u7c7b\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u603b\u4f53FID\u5206\u6570\uff08\u5e73\u57470.74\uff09\uff0c\u6bd4\u4e4b\u524d\u7684SOTA\u65b9\u6cd5\u63d0\u9ad8\u4e866.3%\u3002", "conclusion": "LF-VAR\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u5408\u6210\u76ae\u80a4\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u76ae\u80a4\u5408\u6210\u7684\u7cbe\u786e\u63a7\u5236\u3002"}}
{"id": "2508.19571", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19571", "abs": "https://arxiv.org/abs/2508.19571", "authors": ["Yunlong Lin", "Chao Lu", "Tongshuai Wu", "Xiaocong Zhao", "Guodong Du", "Yanwei Sun", "Zirui Li", "Jianwei Gong"], "title": "Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal", "comment": "Official code: https://github.com/BIT-Jack/SyReM", "summary": "Deep neural networks (DNN) have achieved remarkable success in motion\nforecasting. However, most DNN-based methods suffer from catastrophic\nforgetting and fail to maintain their performance in previously learned\nscenarios after adapting to new data. Recent continual learning (CL) studies\naim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the\nability to retain learned knowledge. Yet, excessive emphasis on the memory\nstability often impairs learning plasticity, i.e., the capacity of DNN to\nacquire new information effectively. To address such stability-plasticity\ndilemma, this study proposes a novel CL method, synergetic memory rehearsal\n(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory\nbuffer to represent learned knowledge. To ensure memory stability, it employs\nan inequality constraint that limits increments in the average loss over the\nmemory buffer. Synergistically, a selective memory rehearsal mechanism is\ndesigned to enhance learning plasticity by selecting samples from the memory\nbuffer that are most similar to recently observed data. This selection is based\non an online-measured cosine similarity of loss gradients, ensuring targeted\nmemory rehearsal. Since replayed samples originate from learned scenarios, this\nmemory rehearsal mechanism avoids compromising memory stability. We validate\nSyReM under an online CL paradigm where training samples from diverse scenarios\narrive as a one-pass stream. Experiments on 11 naturalistic driving datasets\nfrom INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM\nsignificantly mitigates catastrophic forgetting in past scenarios while\nimproving forecasting accuracy in new ones. The implementation is publicly\navailable at https://github.com/BIT-Jack/SyReM.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSyReM\u7684\u65b0\u578b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u8fd0\u52a8\u9884\u6d4b\u4e2d\u9047\u5230\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u65e7\u6570\u636e\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u9002\u5e94\u65b0\u6570\u636e\u65f6\u4f1a\u9057\u5fd8\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u5373\u707e\u96be\u6027\u9057\u5fd8\u3002\u800c\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\u5728\u589e\u5f3a\u8bb0\u5fc6\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u77e5\u8bc6\u7684\u80fd\u529b\uff09\u65f6\uff0c\u5f80\u5f80\u4f1a\u635f\u5bb3\u5b66\u4e60\u53ef\u5851\u6027\uff08\u6709\u6548\u83b7\u53d6\u65b0\u4fe1\u606f\u7684\u80fd\u529b\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u7a33\u5b9a-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CL\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asynergetic memory rehearsal (SyReM) \u7684\u65b0\u65b9\u6cd5\u3002SyReM\u7ef4\u62a4\u4e00\u4e2a\u7d27\u51d1\u7684\u5185\u5b58\u7f13\u51b2\u533a\u6765\u8868\u793a\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u3002\u4e3a\u4e86\u786e\u4fdd\u8bb0\u5fc6\u7a33\u5b9a\u6027\uff0c\u5b83\u91c7\u7528\u4e86\u4e00\u4e2a\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u4e0a\u5e73\u5747\u635f\u5931\u7684\u589e\u91cf\u3002\u4e3a\u4e86\u589e\u5f3a\u5b66\u4e60\u53ef\u5851\u6027\uff0c\u5b83\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u5185\u5b58\u56de\u653e\u673a\u5236\uff0c\u901a\u8fc7\u9009\u62e9\u4e0e\u6700\u8fd1\u89c2\u5bdf\u5230\u7684\u6570\u636e\u6700\u76f8\u4f3c\u7684\u6837\u672c\u6765\u589e\u5f3a\u5b66\u4e60\u53ef\u5851\u6027\u3002\u8fd9\u79cd\u9009\u62e9\u57fa\u4e8e\u635f\u5931\u68af\u5ea6\u7684\u5728\u7ebf\u6d4b\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u786e\u4fdd\u4e86\u6709\u9488\u5bf9\u6027\u7684\u5185\u5b58\u56de\u653e\u3002\u7531\u4e8e\u56de\u653e\u7684\u6837\u672c\u6765\u81ea\u5df2\u5b66\u4e60\u7684\u573a\u666f\uff0c\u8fd9\u79cd\u5185\u5b58\u56de\u653e\u673a\u5236\u907f\u514d\u4e86\u635f\u5bb3\u8bb0\u5fc6\u7a33\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSyReM\u5728INTERACTION\u6570\u636e\u96c6\u768411\u4e2a\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u975eCL\u548cCL\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cSyReM\u663e\u8457\u51cf\u8f7b\u4e86\u8fc7\u53bb\u573a\u666f\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u65b0\u573a\u666f\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "SyReM\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u5185\u5b58\u7ba1\u7406\u548c\u56de\u653e\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86DNN\u5728\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u4fdd\u6301\u5bf9\u65e7\u573a\u666f\u8bb0\u5fc6\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u5bf9\u65b0\u573a\u666f\u7684\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2508.19724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19724", "abs": "https://arxiv.org/abs/2508.19724", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19630", "abs": "https://arxiv.org/abs/2508.19630", "authors": ["Xiaolei Wei", "Yi Ouyang", "Haibo Ye"], "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition", "comment": "This paper has been accepted to PRCV 2025", "summary": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.", "AI": {"tldr": "DQRoute\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u534f\u4f5c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5b66\u4e60\u96be\u5ea6\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u4f30\u8ba1\u7c7b\u522b\u96be\u5ea6\u5e76\u6307\u5bfc\u8bad\u7ec3\uff0c\u540c\u65f6\u5229\u7528\u4e13\u5bb6\u7f51\u7edc\u548cOOD\u68c0\u6d4b\u5668\u8fdb\u884c\u81ea\u9002\u5e94\u8def\u7531\uff0c\u4ece\u800c\u5728\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7a00\u6709\u548c\u56f0\u96be\u7c7b\u522b\u4e0a\u3002", "motivation": "\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u4e0d\u540c\u7c7b\u522b\u5b66\u4e60\u96be\u5ea6\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u9891\u7387\u52a0\u6743\u5ffd\u7565\u4e86\u5b66\u4e60\u96be\u5ea6\u8f83\u5927\u7684\u7c7b\u522b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u534f\u4f5c\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DQRoute\u6846\u67b6\u9996\u5148\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u5386\u53f2\u8868\u73b0\u4f30\u8ba1\u7c7b\u522b\u96be\u5ea6\uff0c\u5e76\u5229\u7528\u8be5\u4fe1\u53f7\u8fdb\u884c\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u8bad\u7ec3\u3002\u5728\u67b6\u6784\u4e0a\uff0c\u5b83\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u7c7b\u522b\u5206\u5e03\u7684\u4e0d\u540c\u533a\u57df\u3002\u5728\u63a8\u7406\u65f6\uff0c\u4e13\u5bb6\u9884\u6d4b\u901a\u8fc7\u4e13\u5bb6\u7279\u5b9a\u7684OOD\u68c0\u6d4b\u5668\u4ea7\u751f\u7684\u7f6e\u4fe1\u5206\u6570\u8fdb\u884c\u52a0\u6743\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4e2d\u5fc3\u5316\u8def\u7531\u5668\u7684\u8f93\u5165\u81ea\u9002\u5e94\u8def\u7531\u3002\u6240\u6709\u7ec4\u4ef6\u5747\u8fdb\u884c\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5728\u6807\u51c6\u957f\u5c3e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDQRoute\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7a00\u6709\u548c\u56f0\u96be\u7c7b\u522b\u4e0a\uff0c\u8bc1\u660e\u4e86\u5c06\u96be\u5ea6\u5efa\u6a21\u4e0e\u53bb\u4e2d\u5fc3\u5316\u4e13\u5bb6\u8def\u7531\u76f8\u7ed3\u5408\u7684\u76ca\u5904\u3002", "conclusion": "DQRoute\u901a\u8fc7\u6574\u5408\u96be\u5ea6\u5efa\u6a21\u548c\u53bb\u4e2d\u5fc3\u5316\u4e13\u5bb6\u8def\u7531\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7f55\u89c1\u548c\u56f0\u96be\u7c7b\u522b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.19589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19589", "abs": "https://arxiv.org/abs/2508.19589", "authors": ["Arshia Hemmat", "Afsaneh Fatemi"], "title": "Delta-Audit: Explaining What Changes When Models Change", "comment": "7 pages, 1 figure, 4 tables", "summary": "Model updates (new hyperparameters, kernels, depths, solvers, or data) change\nperformance, but the \\emph{reason} often remains opaque. We introduce\n\\textbf{Delta-Attribution} (\\mbox{$\\Delta$-Attribution}), a model-agnostic\nframework that explains \\emph{what changed} between versions $A$ and $B$ by\ndifferencing per-feature attributions: $\\Delta\\phi(x)=\\phi_B(x)-\\phi_A(x)$. We\nevaluate $\\Delta\\phi$ with a \\emph{$\\Delta$-Attribution Quality Suite} covering\nmagnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,\nJensen--Shannon divergence), behavioural alignment (Delta Conservation Error,\nDCE; Behaviour--Attribution Coupling, BAC; CO$\\Delta$F), and robustness (noise,\nbaseline sensitivity, grouped occlusion).\n  Instantiated via fast occlusion/clamping in standardized space with a\nclass-anchored margin and baseline averaging, we audit 45 settings: five\nclassical families (Logistic Regression, SVC, Random Forests, Gradient\nBoosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B\npairs per family. \\textbf{Findings.} Inductive-bias changes yield large,\nbehaviour-aligned deltas (e.g., SVC poly$\\!\\rightarrow$rbf on Breast Cancer:\nBAC$\\approx$0.998, DCE$\\approx$6.6; Random Forest feature-rule swap on Digits:\nBAC$\\approx$0.997, DCE$\\approx$7.5), while ``cosmetic'' tweaks (SVC\n\\texttt{gamma=scale} vs.\\ \\texttt{auto}, $k$NN search) show\nrank-overlap@10$=1.0$ and DCE$\\approx$0. The largest redistribution appears for\ndeeper GB on Breast Cancer (JSD$\\approx$0.357). $\\Delta$-Attribution offers a\nlightweight update audit that complements accuracy by distinguishing benign\nchanges from behaviourally meaningful or risky reliance shifts.", "AI": {"tldr": "Delta-Attribution\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5f02\u5316\u6bcf\u7279\u5f81\u5f52\u56e0\u6765\u89e3\u91ca\u6a21\u578b\u7248\u672cA\u548cB\u4e4b\u95f4\u2018\u53d1\u751f\u4e86\u4ec0\u4e48\u53d8\u5316\u2019\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8bc4\u4f30\u5957\u4ef6\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u591a\u4e2a\u65b9\u9762\uff0c\u5e76\u572845\u79cd\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u4e86\u5ba1\u8ba1\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u66f4\u65b0\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u91ca\u6a21\u578b\u66f4\u65b0\uff08\u5982\u8d85\u53c2\u6570\u3001\u6838\u3001\u6df1\u5ea6\u3001\u6c42\u89e3\u5668\u6216\u6570\u636e\u66f4\u6539\uff09\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u53d8\u5316\u80cc\u540e\u7684\u5177\u4f53\u539f\u56e0\u3002", "method": "\u63d0\u51faDelta-Attribution ($\\Delta$-Attribution)\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e24\u4e2a\u6a21\u578b\u7248\u672c\uff08A\u548cB\uff09\u7684\u6bcf\u7279\u5f81\u5f52\u56e0\u4e4b\u5dee ($\\Delta\\phi(x)=\\phi_B(x)-\\phi_A(x)$) \u6765\u91cf\u5316\u548c\u89e3\u91ca\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4f7f\u7528\u8d28\u91cf\u5957\u4ef6\u8bc4\u4f30$\\Delta\\phi$\uff0c\u8be5\u5957\u4ef6\u5305\u62ec\u4e86\u5e45\u5ea6/\u7a00\u758f\u6027\u3001\u4e00\u81f4\u6027/\u504f\u79fb\u3001\u884c\u4e3a\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u7b49\u6307\u6807\u3002\u5177\u4f53\u5b9e\u73b0\u4e0a\uff0c\u5728\u6807\u51c6\u5316\u7a7a\u95f4\u4e2d\u901a\u8fc7\u5feb\u901f\u906e\u6321/\u94b3\u4f4d\uff0c\u5e76\u7ed3\u5408\u7c7b\u951a\u5b9a\u8fb9\u8ddd\u548c\u57fa\u7ebf\u5e73\u5747\u3002", "result": "\u572845\u79cd\u8bbe\u7f6e\uff08\u5305\u62ec5\u79cd\u7ecf\u5178\u6a21\u578b\u30013\u4e2a\u6570\u636e\u96c6\u548c3\u79cdA/B\u914d\u5bf9\uff09\u7684\u5ba1\u8ba1\u4e2d\uff0c\u53d1\u73b0\u5f52\u7eb3\u504f\u7f6e\uff08inductive-bias\uff09\u7684\u53d8\u5316\u4f1a\u5bfc\u81f4\u8f83\u5927\u4e14\u4e0e\u884c\u4e3a\u4e00\u81f4\u7684Delta\uff08\u4f8b\u5982\uff0cSVC\u6838\u4ecepoly\u53d8\u4e3arbf\uff0cBAC\u7ea6\u4e3a0.998\uff0cDCE\u7ea6\u4e3a6.6\uff09\uff1b\u800c\u2018\u8868\u9762\u2019\u8c03\u6574\uff08\u5982SVC gamma\u8bbe\u7f6e\u6216kNN\u641c\u7d22\uff09\u5219\u663e\u793a\u51fa\u9ad8\u5ea6\u7684\u6392\u540d\u91cd\u53e0\uff08rank-overlap@10=1.0\uff09\u548c\u8f83\u4f4e\u7684DCE\u3002\u6df1\u5ea6GB\u6a21\u578b\u5728\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u6700\u5927\u7684\u7279\u5f81\u5f52\u56e0\u91cd\u5206\u5e03\uff08JSD\u7ea6\u4e3a0.357\uff09\u3002", "conclusion": "Delta-Attribution\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\u66f4\u65b0\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u533a\u5206\u826f\u6027\u53d8\u5316\u4e0e\u884c\u4e3a\u4e0a\u91cd\u8981\u6216\u6709\u98ce\u9669\u7684\u4f9d\u8d56\u6027\u8f6c\u79fb\uff0c\u662f\u5bf9\u51c6\u786e\u6027\u6307\u6807\u7684\u91cd\u8981\u8865\u5145\u3002"}}
{"id": "2508.19740", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19740", "abs": "https://arxiv.org/abs/2508.19740", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.", "AI": {"tldr": "Spotlight Attention\u901a\u8fc7\u975e\u7ebf\u6027\u54c8\u5e0c\u4f18\u5316KV\u7f13\u5b58\uff0c\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u51cf\u5c11LLM\u7684KV\u7f13\u5b58\u5f00\u9500\u4ee5\u52a0\u901f\u63a8\u7406\uff0c\u73b0\u6709\u968f\u673a\u7ebf\u6027\u54c8\u5e0c\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSpotlight Attention\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u54c8\u5e0c\u51fd\u6570\u4f18\u5316\u67e5\u8be2\u548c\u952e\u7684\u5d4c\u5165\u5206\u5e03\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eBradley-Terry\u6392\u5e8f\u635f\u5931\u7684\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u6846\u67b6\u3002", "result": "Spotlight Attention\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u7cbe\u5ea6\uff0c\u5e76\u5c06\u54c8\u5e0c\u7801\u957f\u5ea6\u7f29\u77ed\u4e86\u81f3\u5c115\u500d\uff0c\u5728A100 GPU\u4e0a\u5b9e\u73b0\u4e86512K token\u7684\u54c8\u5e0c\u68c0\u7d22\uff0c\u65f6\u95f4\u5c0f\u4e8e100\u03bcs\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473\u500d\u3002", "conclusion": "Spotlight Attention\u901a\u8fc7\u4f18\u5316KV\u7f13\u5b58\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6548\u7387\u548c\u901f\u5ea6\u3002"}}
{"id": "2508.19638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19638", "abs": "https://arxiv.org/abs/2508.19638", "authors": ["Yang Li", "Quan Yuan", "Guiyang Luo", "Xiaoyuan Fu", "Rui Pan", "Yujia Yang", "Congzhang Shao", "Yuewen Liu", "Jinglin Li"], "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception", "comment": null, "summary": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.", "AI": {"tldr": "CoPLOT\u4f7f\u7528\u70b9\u7ea7\u4f18\u5316\u4ee4\u724c\uff08Point-Level Optimized Tokens\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u70b9\u539f\u751f\u5904\u7406\u6d41\u7a0b\uff08\u5305\u62ec\u4ee4\u724c\u91cd\u6392\u5e8f\u3001\u5e8f\u5217\u5efa\u6a21\u548c\u591a\u667a\u80fd\u4f53\u7a7a\u95f4\u5bf9\u9f50\uff09\u6765\u6539\u8fdb\u534f\u4f5c\u611f\u77e5\uff0c\u89e3\u51b3\u4e86\u73b0\u67092D BEV\u8868\u793a\u4e22\u59313D\u7ed3\u6784\u7ebf\u7d22\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660eCoPLOT\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u901a\u5e38\u5c06\u4e2d\u95f4\u7279\u5f81\u7ec4\u7ec7\u4e3a2D BEV\u8868\u793a\uff0c\u8fd9\u4f1a\u4e22\u5931\u7cbe\u786e\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u6240\u5fc5\u9700\u7684\u5173\u952e\u7ec6\u7c92\u5ea63D\u7ed3\u6784\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPLOT\u7684\u65b0\u578b\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u5b83\u5229\u7528\u70b9\u7ea7\u4f18\u5316\u4ee4\u724c\uff08Point-Level Optimized Tokens\uff09\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u70b9\u539f\u751f\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\uff1a1\uff09\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u7684\u4ee4\u724c\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u901a\u8fc7\u5229\u7528\u573a\u666f\u7ea7\u548c\u4ee4\u724c\u7ea7\u7684\u8bed\u4e49\u4fe1\u606f\u751f\u6210\u81ea\u9002\u5e94\u76841D\u91cd\u6392\u5e8f\uff1b2\uff09\u4e00\u4e2a\u9891\u7387\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u8de8\u7a7a\u95f4\u548c\u9891\u8c31\u57df\u7684\u957f\u8ddd\u79bb\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u63d0\u9ad8\u524d\u666f\u4ee4\u724c\u548c\u80cc\u666f\u6742\u6ce2\u7684\u533a\u522b\uff1b3\uff09\u4e00\u4e2a\u90bb\u5c45\u5230\u81ea\u6211\uff08neighbor-to-ego\uff09\u7684\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u667a\u80fd\u4f53\u7ea7\u6821\u6b63\u548c\u5c40\u90e8\u4ee4\u724c\u7ea7\u4f18\u5316\u6765\u51cf\u8f7b\u5b9a\u4f4d\u566a\u58f0\u3002", "result": "CoPLOT\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u540c\u65f6\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "CoPLOT\u901a\u8fc7\u5f15\u5165\u70b9\u7ea7\u4ee4\u724c\u8868\u793a\u548c\u521b\u65b0\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u57283D\u7ed3\u6784\u4fe1\u606f\u4fdd\u7559\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6210\u679c\u3002"}}
{"id": "2508.19597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19597", "abs": "https://arxiv.org/abs/2508.19597", "authors": ["Zirui Li", "Yunlong Lin", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Chen Lv", "Chao Lu", "Jianwei Gong"], "title": "Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities", "comment": "19 pages, 6 figures", "summary": "Artificial intelligence underpins most smart city services, yet deep neural\nnetwork (DNN) that forecasts vehicle motion still struggle with catastrophic\nforgetting, the loss of earlier knowledge when models are updated. Conventional\nfixes enlarge the training set or replay past data, but these strategies incur\nhigh data collection costs, sample inefficiently and fail to balance long- and\nshort-term experience, leaving them short of human-like continual learning.\nHere we introduce Dual-LS, a task-free, online continual learning paradigm for\nDNN-based motion forecasting that is inspired by the complementary learning\nsystem of the human brain. Dual-LS pairs two synergistic memory rehearsal\nreplay mechanisms to accelerate experience retrieval while dynamically\ncoordinating long-term and short-term knowledge representations. Tests on\nnaturalistic data spanning three countries, over 772,000 vehicles and\ncumulative testing mileage of 11,187 km show that Dual-LS mitigates\ncatastrophic forgetting by up to 74.31\\% and reduces computational resource\ndemand by up to 94.02\\%, markedly boosting predictive stability in vehicle\nmotion forecasting without inflating data requirements. Meanwhile, it endows\nDNN-based vehicle motion forecasting with computation efficient and human-like\ncontinual learning adaptability fit for smart cities.", "AI": {"tldr": "\u5728\u667a\u80fd\u4ea4\u901a\u9884\u6d4b\u4e2d\uff0cDual-LS\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u5927\u8111\u7684\u53cc\u91cd\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6548\u7387\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u670d\u52a1\u4f9d\u8d56\u4eba\u5de5\u667a\u80fd\uff0c\u4f46\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u5728\u66f4\u65b0\u65f6\u4f1a\u9057\u5fd8\u65e7\u77e5\u8bc6\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6269\u5927\u8bad\u7ec3\u96c6\u6216\u56de\u653e\u6570\u636e\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u4e14\u65e0\u6cd5\u5e73\u8861\u957f\u671f\u548c\u77ed\u671f\u8bb0\u5fc6\uff0c\u4e0d\u5177\u5907\u4eba\u7c7b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-LS\u7684\u3001\u65e0\u4efb\u52a1\u7684\u3001\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u7528\u4e8eDNN\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u4eba\u8111\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u79cd\u534f\u540c\u7684\u8bb0\u5fc6\u56de\u653e\u673a\u5236\uff0c\u4ee5\u52a0\u901f\u7ecf\u9a8c\u68c0\u7d22\uff0c\u5e76\u52a8\u6001\u534f\u8c03\u957f\u671f\u548c\u77ed\u671f\u77e5\u8bc6\u8868\u5f81\u3002", "result": "\u5728\u5305\u542b\u4e09\u4e2a\u56fd\u5bb6\u3001\u8d85\u8fc777.2\u4e07\u8f86\u8f66\u548c\u7d2f\u8ba111,187\u516c\u91cc\u6d4b\u8bd5\u91cc\u7a0b\u7684\u81ea\u7136\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cDual-LS\u5c06\u707e\u96be\u6027\u9057\u5fd8\u51cf\u5c11\u4e86\u9ad8\u8fbe74.31%\uff0c\u5e76\u5c06\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u964d\u4f4e\u4e86\u9ad8\u8fbe94.02%\u3002", "conclusion": "Dual-LS\u5728\u4e0d\u589e\u52a0\u6570\u636e\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u7684\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u5e76\u8d4b\u4e88\u4e86DNN\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u8ba1\u7b97\u9ad8\u6548\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6301\u7eed\u5b66\u4e60\u9002\u5e94\u80fd\u529b\uff0c\u975e\u5e38\u9002\u5408\u667a\u80fd\u57ce\u5e02\u7684\u5e94\u7528\u3002"}}
{"id": "2508.19758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE\u662f\u4e00\u4e2a\u7528\u4e8e\u65b0\u95fb\u68c0\u7d22\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u53e5\u5b50\u5c42\u9762\u5efa\u6a21\u8bed\u4e49\u53d8\u5f02\u6765\u589e\u5f3a\u4e8b\u4ef6\u8986\u76d6\uff0c\u4ece\u800c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u53ea\u5173\u6ce8\u6587\u672c\u76f8\u5173\u6027\u5bfc\u81f4\u7ed3\u679c\u5197\u4f59\u548c\u89c2\u70b9\u66b4\u9732\u6709\u9650\u7684\u95ee\u9898\u3002\u5b83\u4f7f\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u68c0\u7d22\u76f8\u5173\u5185\u5bb9\uff0c\u7136\u540e\u901a\u8fc7\u805a\u7c7b\u548c\u591a\u6837\u6027\u611f\u77e5\u91cd\u65b0\u6392\u5e8f\u6765\u5c55\u793a\u4e92\u8865\u4fe1\u606f\u3002NEWSCOPE\u5728\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u76f8\u5173\u6027\u3002", "motivation": "\u5927\u591a\u6570\u65b0\u95fb\u68c0\u7d22\u7cfb\u7edf\u4f18\u5148\u8003\u8651\u6587\u672c\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u5197\u4f59\u548c\u89c2\u70b9\u66b4\u9732\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u4e8b\u4ef6\u8986\u76d6\u548c\u63d0\u4f9b\u591a\u6837\u5316\u89c6\u89d2\u7684\u65b0\u95fb\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "NEWSCOPE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bc6\u96c6\u68c0\u7d22\u6765\u68c0\u7d22\u4e3b\u9898\u76f8\u5173\u5185\u5bb9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u53e5\u5b50\u7ea7\u805a\u7c7b\u548c\u591a\u6837\u6027\u611f\u77e5\u91cd\u65b0\u6392\u5e8f\u6765\u5c55\u793a\u4e92\u8865\u4fe1\u606f\u3002\u5f15\u5165\u4e86\u5e73\u5747\u6210\u5bf9\u8ddd\u79bb\u3001\u6b63\u805a\u7c7b\u8986\u76d6\u548c\u4fe1\u606f\u5bc6\u5ea6\u6bd4\u4e09\u4e2a\u53ef\u89e3\u91ca\u7684\u6307\u6807\u6765\u8bc4\u4f30\u68c0\u7d22\u591a\u6837\u6027\uff0c\u5e76\u6784\u5efa\u4e86LocalNews\u548cDSGlobal\u4e24\u4e2a\u6bb5\u843d\u7ea7\u57fa\u51c6\u3002", "result": "NEWSCOPE\u5728\u591a\u6837\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u76f8\u5173\u6027\u3002", "conclusion": "NEWSCOPE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6709\u6548\u5730\u51cf\u5c11\u4e86\u5197\u4f59\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u4e8b\u4ef6\u7684\u5168\u9762\u7406\u89e3\u3002"}}
{"id": "2508.19647", "categories": ["cs.CV", "I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19647", "abs": "https://arxiv.org/abs/2508.19647", "authors": ["Bikash Kumar Badatya", "Vipul Baghel", "Ravi Hegde"], "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks", "comment": "This paper has been accepted at the ICIP Satellite Workshop 2025", "summary": "Fine-grained action localization in untrimmed sports videos presents a\nsignificant challenge due to rapid and subtle motion transitions over short\ndurations. Existing supervised and weakly supervised solutions often rely on\nextensive annotated datasets and high-capacity models, making them\ncomputationally intensive and less adaptable to real-world scenarios. In this\nwork, we introduce a lightweight and unsupervised skeleton-based action\nlocalization pipeline that leverages spatio-temporal graph neural\nrepresentations. Our approach pre-trains an Attention-based Spatio-Temporal\nGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task with\nblockwise partitions, enabling it to learn intrinsic motion dynamics without\nany manual labeling. At inference, we define a novel Action Dynamics Metric\n(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects\nmotion boundaries by identifying inflection points in its curvature profile.\nOur method achieves a mean Average Precision (mAP) of 82.66% and average\nlocalization latency of 29.09 ms on the DSV Diving dataset, matching\nstate-of-the-art supervised performance while maintaining computational\nefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving\nfootage without retraining, demonstrating its practical applicability for\nlightweight, real-time action analysis systems in embedded or dynamic\nenvironments.", "AI": {"tldr": "A lightweight, unsupervised skeleton-based pipeline for action localization in sports videos, using ASTGCN and a novel Action Dynamics Metric (ADM) for efficient and accurate boundary detection.", "motivation": "Existing supervised and weakly supervised methods for action localization are computationally intensive and less adaptable due to reliance on large annotated datasets and high-capacity models.", "method": "Pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task for unsupervised learning of motion dynamics. Uses a novel Action Dynamics Metric (ADM), derived from ASTGCN embeddings, to detect motion boundaries via inflection points in its curvature profile.", "result": "Achieves 82.66% mAP and 29.09 ms localization latency on the DSV Diving dataset, matching supervised methods. Generalizes to unseen footage without retraining.", "conclusion": "The proposed lightweight and unsupervised pipeline offers a practical solution for real-time action analysis in diverse environments, demonstrating robustness and computational efficiency."}}
{"id": "2508.19598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19598", "abs": "https://arxiv.org/abs/2508.19598", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "comment": null, "summary": "The functionality of Large Language Model (LLM) agents is primarily\ndetermined by two capabilities: action planning and answer summarization. The\nformer, action planning, is the core capability that dictates an agent's\nperformance. However, prevailing training paradigms employ end-to-end,\nmulti-objective optimization that jointly trains both capabilities. This\nparadigm faces two critical challenges: imbalanced optimization objective\nallocation and scarcity of verifiable data, making it difficult to enhance the\nagent's planning capability. To address these challenges, we propose\nReinforcement Learning with Tool-use Rewards (RLTR), a novel framework that\ndecouples the training process to enable a focused, single-objective\noptimization of the planning module. Crucially, RLTR introduces a reward signal\nbased on tool-use completeness to directly evaluate the quality of tool\ninvocation sequences. This method offers a more direct and reliable training\nsignal than assessing the final response content, thereby obviating the need\nfor verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%\nimprovement in planning performance compared to end-to-end baselines. Moreover,\nthis enhanced planning capability, in turn, translates to a 5%-6% increase in\nthe final response quality of the overall agent system.", "AI": {"tldr": "RLTR\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316LLM\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bad\u7ec3\u8303\u5f0f\u91c7\u7528\u7aef\u5230\u7aef\u3001\u591a\u76ee\u6807\u4f18\u5316\uff0c\u96be\u4ee5\u63d0\u5347\u4ee3\u7406\u7684\u6838\u5fc3\u80fd\u529b\u2014\u2014\u52a8\u4f5c\u89c4\u5212\uff0c\u9762\u4e34\u4f18\u5316\u76ee\u6807\u5206\u914d\u4e0d\u5747\u548c\u53ef\u9a8c\u8bc1\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faRLTR\uff08Reinforcement Learning with Tool-use Rewards\uff09\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u548c\u56de\u7b54\u6458\u8981\u8bad\u7ec3\u89e3\u8026\uff0c\u5b9e\u73b0\u89c4\u5212\u6a21\u5757\u7684\u5355\u4e00\u76ee\u6807\u4f18\u5316\u3002RLTR\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u76f4\u63a5\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u5e8f\u5217\u7684\u8d28\u91cf\uff0c\u65e0\u9700\u53ef\u9a8c\u8bc1\u6570\u636e\u3002", "result": "RLTR\u5728\u89c4\u5212\u80fd\u529b\u4e0a\u6bd4\u7aef\u5230\u7aef\u57fa\u7ebf\u63d0\u9ad8\u4e868%-12%\uff0c\u6574\u4f53\u4ee3\u7406\u7cfb\u7edf\u7684\u6700\u7ec8\u54cd\u5e94\u8d28\u91cf\u63d0\u9ad8\u4e865%-6%\u3002", "conclusion": "RLTR\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u548c\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u89c4\u5212\u80fd\u529b\u63d0\u5347\u7684\u96be\u9898\uff0c\u5e76\u6700\u7ec8\u63d0\u5347\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.19764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19764", "abs": "https://arxiv.org/abs/2508.19764", "authors": ["Pedro Henrique Luz de Araujo", "Paul R\u00f6ttger", "Dirk Hovy", "Benjamin Roth"], "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage.", "AI": {"tldr": "Expert persona prompting improves LLM performance but makes models sensitive to irrelevant details and inconsistent in fidelity, requiring careful persona design and evaluation.", "motivation": "Investigate the effectiveness of expert persona prompting for LLM task improvement, focusing on when and why it works.", "method": "Analyze literature on persona prompting, distill three desiderata (performance advantage, robustness, fidelity), and evaluate 9 LLMs across 27 tasks based on these criteria. Propose and test mitigation strategies for robustness.", "result": "Expert personas generally yield positive or neutral performance changes. Models exhibit high sensitivity to irrelevant persona details (up to 30% performance drop). Higher education, specialization, and domain-relatedness show inconsistent or negligible effects on fidelity and performance. Mitigation strategies only benefit the largest models.", "conclusion": "Careful persona design and evaluation schemes are needed to realize the intended benefits of persona prompting while mitigating negative side effects like sensitivity to irrelevant details and inconsistent fidelity."}}
{"id": "2508.19649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u77e5\u7684\u566a\u58f0\u7c7b\u578b\u548c\u6c34\u5e73\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u52a8\u6001\u5377\u79ef\u6838\u8fdb\u884c\u56fe\u50cf\u53bb\u566a\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u3001\u5168\u5c40\u7edf\u8ba1\u548c\u5c40\u90e8\u76f8\u5173\u6027\u6765\u6355\u6349\u566a\u58f0\u7279\u5f81\uff0c\u5e76\u9010\u50cf\u7d20\u751f\u6210\u81ea\u9002\u5e94\u5377\u79ef\u6838\u8fdb\u884c\u8fed\u4ee3\u53bb\u566a\uff0c\u5728\u4e0d\u4f9d\u8d56\u7279\u5b9a\u566a\u58f0\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u53bb\u566a\u6548\u679c\uff0c\u6a21\u578b\u5c0f\u5de7\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u566a\u58f0\u5206\u5e03\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5e76\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u6c34\u5e73\uff0c\u5e76\u4e14\u4e0d\u6613\u8fc7\u62df\u5408\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u52a8\u6001\u5377\u79ef\u6838\u8fdb\u884c\u56fe\u50cf\u53bb\u566a\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08\u7528\u4e8e\u63d0\u53d6\u9c81\u68d2\u7684\u3001\u4e0e\u566a\u58f0\u65e0\u5173\u7684\u7279\u5f81\uff09\u3001\u4e00\u4e2a\u5168\u5c40\u7edf\u8ba1\u6a21\u5757\u548c\u4e00\u4e2a\u5c40\u90e8\u76f8\u5173\u6027\u6a21\u5757\uff08\u7528\u4e8e\u6355\u6349\u5168\u9762\u7684\u566a\u58f0\u7279\u5f81\u548c\u7ed3\u6784\u76f8\u5173\u6027\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5377\u79ef\u6838\u9884\u6d4b\u6a21\u5757\uff08\u5229\u7528\u8fd9\u4e9b\u7ebf\u7d22\u751f\u6210\u9010\u50cf\u7d20\u53d8\u5316\u7684\u3001\u9002\u5e94\u5c40\u90e8\u7ed3\u6784\u7684\u5377\u79ef\u6838\uff09\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e94\u7528\u8fd9\u4e9b\u5377\u79ef\u6838\u8fdb\u884c\u53bb\u566a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5355\u4e00\u6c34\u5e73\u7684\u5408\u6210\u566a\u58f0\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u4e0d\u540c\u7684\u566a\u58f0\u7c7b\u578b\u548c\u6c34\u5e73\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u8fed\u4ee3\u52a8\u6001\u6ee4\u6ce2\u5728\u5b9e\u9645\u56fe\u50cf\u53bb\u566a\u4e2d\u7684\u6f5c\u529b\u3002\u8be5\u6a21\u578b\u5c0f\u5de7\uff08\u7ea60.04M\uff09\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5e76\u63d0\u9ad8\u5bf9\u672a\u77e5\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u7684\u5229\u7528\u52a8\u6001\u5377\u79ef\u6838\u8fdb\u884c\u8fed\u4ee3\u53bb\u566a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u514b\u670d\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u56fe\u50cf\u53bb\u566a\u6548\u679c\u3002"}}
{"id": "2508.19609", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2508.19609", "abs": "https://arxiv.org/abs/2508.19609", "authors": ["Zhuohang Zhu", "Haodong Chen", "Qiang Qu", "Vera Chung"], "title": "FinCast: A Foundation Model for Financial Time-Series Forecasting", "comment": null, "summary": "Financial time-series forecasting is critical for maintaining economic\nstability, guiding informed policymaking, and promoting sustainable investment\npractices. However, it remains challenging due to various underlying pattern\nshifts. These shifts arise primarily from three sources: temporal\nnon-stationarity (distribution changes over time), multi-domain diversity\n(distinct patterns across financial domains such as stocks, commodities, and\nfutures), and varying temporal resolutions (patterns differing across\nper-second, hourly, daily, or weekly indicators). While recent deep learning\nmethods attempt to address these complexities, they frequently suffer from\noverfitting and typically require extensive domain-specific fine-tuning. To\novercome these limitations, we introduce FinCast, the first foundation model\nspecifically designed for financial time-series forecasting, trained on\nlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot\nperformance, effectively capturing diverse patterns without domain-specific\nfine-tuning. Comprehensive empirical and qualitative evaluations demonstrate\nthat FinCast surpasses existing state-of-the-art methods, highlighting its\nstrong generalization capabilities.", "AI": {"tldr": "FinCast\u662f\u4e00\u4e2a\u9488\u5bf9\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u91d1\u878d\u57fa\u7840\u6a21\u578b\uff0c\u5728\u5927\u91cf\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002\u5b83\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u79cd\u6a21\u5f0f\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u5728\u5404\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5bf9\u4e8e\u7ef4\u6301\u7ecf\u6d4e\u7a33\u5b9a\u3001\u6307\u5bfc\u660e\u667a\u7684\u653f\u7b56\u5236\u5b9a\u4ee5\u53ca\u4fc3\u8fdb\u53ef\u6301\u7eed\u7684\u6295\u8d44\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5404\u79cd\u6f5c\u5728\u7684\u6a21\u5f0f\u53d8\u5316\uff0c\u5b83\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFinCast\u7684\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e13\u95e8\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u800c\u8bbe\u8ba1\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "FinCast\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u79cd\u6a21\u5f0f\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u3002\u5168\u9762\u7684\u5b9e\u8bc1\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cFinCast\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FinCast\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u800c\u8bbe\u8ba1\u7684\u91d1\u878d\u57fa\u7840\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.19813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19813", "abs": "https://arxiv.org/abs/2508.19813", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.", "AI": {"tldr": "LLMs\u5728\u8868\u683c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u8868\u683c\u4fe1\u606f\u8f6c\u5316\u4e3a\u62a5\u544a\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u8868\u683c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86", "motivation": "LLMs\u5728\u8868\u683c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u8868\u683c\u4fe1\u606f\u8f6c\u5316\u4e3a\u62a5\u544a\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u8868\u683c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u201c\u8868\u683c\u5230\u62a5\u544a\u201d\uff08table-to-report\uff09\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aT2R-bench\u7684\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u542b457\u4e2a\u6765\u81ea\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684\u8868\u683c\uff0c\u6db5\u76d619\u4e2a\u884c\u4e1a\u9886\u57df\u548c4\u79cd\u8868\u683c\u7c7b\u578b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u8bc4\u4f30\u6807\u51c6\u6765\u8861\u91cf\u62a5\u544a\u751f\u6210\u7684\u8d28\u91cf\u3002", "result": "\u572825\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u50cfDeepseek-R1\u8fd9\u6837\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728T2R-bench\u4e0a\u7684\u6574\u4f53\u5f97\u5206\u4e5f\u4ec5\u4e3a62.71\uff0c\u8868\u660eLLMs\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u8868\u683c\u5230\u62a5\u544a\u201d\u4efb\u52a1\u548cT2R-bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5c06\u8868\u683c\u4fe1\u606f\u8f6c\u5316\u4e3a\u62a5\u544a\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709LLMs\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002"}}
{"id": "2508.19650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19650", "abs": "https://arxiv.org/abs/2508.19650", "authors": ["Hou Xia", "Zheren Fu", "Fangcan Ling", "Jiajun Li", "Yi Tu", "Zhendong Mao", "Yongdong Zhang"], "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models", "comment": null, "summary": "Large video language models (LVLMs) have made notable progress in video\nunderstanding, spurring the development of corresponding evaluation benchmarks.\nHowever, existing benchmarks generally assess overall performance across entire\nvideo sequences, overlooking nuanced behaviors such as contextual positional\nbias, a critical yet under-explored aspect of LVLM performance. We present\nVideo-LevelGauge, a dedicated benchmark designed to systematically assess\npositional bias in LVLMs. We employ standardized probes and customized\ncontextual setups, allowing flexible control over context length, probe\nposition, and contextual types to simulate diverse real-world scenarios. In\naddition, we introduce a comprehensive analysis method that combines\nstatistical measures with morphological pattern recognition to characterize\nbias. Our benchmark comprises 438 manually curated videos spanning multiple\ntypes, yielding 1,177 high-quality multiple-choice questions and 120 open-ended\nquestions, validated for their effectiveness in exposing positional bias. Based\non these, we evaluate 27 state-of-the-art LVLMs, including both commercial and\nopen-source models. Our findings reveal significant positional biases in many\nleading open-source models, typically exhibiting head or neighbor-content\npreferences. In contrast, commercial models such as Gemini2.5-Pro show\nimpressive, consistent performance across entire video sequences. Further\nanalyses on context length, context variation, and model scale provide\nactionable insights for mitigating bias and guiding model enhancement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Video-LevelGauge\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u4f4d\u7f6e\u504f\u89c1\uff0c\u53d1\u73b0\u8bb8\u591a\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u5934\u90e8\u6216\u90bb\u8fd1\u5185\u5bb9\u504f\u89c1\uff0c\u800cGemini2.5-Pro\u8868\u73b0\u51fa\u4f17\u3002", "motivation": "\u8bc4\u4f30LVLMs\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\uff0c\u8be5\u504f\u89c1\u662f\u73b0\u6709\u57fa\u51c6\u666e\u904d\u5ffd\u89c6\u7684\u5173\u952e\u65b9\u9762\u3002", "method": "\u8bbe\u8ba1\u4e86Video-LevelGauge\u57fa\u51c6\uff0c\u4f7f\u7528\u6807\u51c6\u63a2\u9488\u548c\u5b9a\u5236\u5316\u573a\u666f\uff0c\u63a7\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u63a2\u9488\u4f4d\u7f6e\u548c\u4e0a\u4e0b\u6587\u7c7b\u578b\uff0c\u5e76\u7ed3\u5408\u7edf\u8ba1\u6d4b\u91cf\u548c\u5f62\u6001\u6a21\u5f0f\u8bc6\u522b\u8fdb\u884c\u504f\u89c1\u5206\u6790\u3002", "result": "\u572827\u4e2aLVLMs\u7684\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\u8bb8\u591a\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u4f4d\u7f6e\u504f\u89c1\uff08\u5934\u90e8\u6216\u90bb\u8fd1\u5185\u5bb9\u504f\u597d\uff09\uff0c\u800cGemini2.5-Pro\u8868\u73b0\u7a33\u5065\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u4e0a\u4e0b\u6587\u53d8\u5316\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\u3002", "conclusion": "LVLMs\u5b58\u5728\u666e\u904d\u7684\u4f4d\u7f6e\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u548c\u4f18\u5316\u57fa\u51c6\u6765\u89e3\u51b3\uff0c\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u89c6\u9891\u7406\u89e3\u8bc4\u4f30\u3002"}}
{"id": "2508.19613", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19613", "abs": "https://arxiv.org/abs/2508.19613", "authors": ["Chenzhi Liu", "Mahsa Baktashmotlagh", "Yanran Tang", "Zi Huang", "Ruihong Qiu"], "title": "ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation", "comment": "Accepted to BMVC 2025, Oral", "summary": "Estimating model accuracy on unseen, unlabeled datasets is crucial for\nreal-world machine learning applications, especially under distribution shifts\nthat can degrade performance. Existing methods often rely on predicted class\nprobabilities (softmax scores) or data similarity metrics. While softmax-based\napproaches benefit from representing predictions on the standard simplex,\ncompressing logits into probabilities leads to information loss. Meanwhile,\nsimilarity-based methods can be computationally expensive and domain-specific,\nlimiting their broader applicability. In this paper, we introduce ALSA (Anchors\nin Logit Space for Accuracy estimation), a novel framework that preserves\nricher information by operating directly in the logit space. Building on\ntheoretical insights and empirical observations, we demonstrate that the\naggregation and distribution of logits exhibit a strong correlation with the\npredictive performance of the model. To exploit this property, ALSA employs an\nanchor-based modeling strategy: multiple learnable anchors are initialized in\nlogit space, each assigned an influence function that captures subtle\nvariations in the logits. This allows ALSA to provide robust and accurate\nperformance estimates across a wide range of distribution shifts. Extensive\nexperiments on vision, language, and graph benchmarks demonstrate ALSA's\nsuperiority over both softmax- and similarity-based baselines. Notably, ALSA's\nrobustness under significant distribution shifts highlights its potential as a\npractical tool for reliable model evaluation.", "AI": {"tldr": "ALSA\u662f\u4e00\u4e2a\u5728\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5728logit\u7a7a\u95f4\u64cd\u4f5c\u6765\u4fdd\u7559\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u4ee5\u4f30\u8ba1\u6a21\u578b\u5728\u65b0\u3001\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u504f\u79fb\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e0d\u53ef\u89c1\u3001\u672a\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u4f30\u8ba1\u6a21\u578b\u51c6\u786e\u6027\u5bf9\u4e8e\u5b9e\u9645\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u6d4b\u7c7b\u522b\u6982\u7387\uff08softmax\u5206\u6570\uff09\u6216\u6570\u636e\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u4e22\u5931\u4fe1\u606f\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u9886\u57df\u7279\u5b9a\u3002", "method": "ALSA\u6846\u67b6\u5229\u7528anchor-based\u5efa\u6a21\u7b56\u7565\uff0c\u5728logit\u7a7a\u95f4\u4e2d\u521d\u59cb\u5316\u591a\u4e2a\u53ef\u5b66\u4e60\u7684anchor\uff0c\u5e76\u4e3a\u6bcf\u4e2aanchor\u5206\u914d\u4e00\u4e2a\u6355\u6349logits\u7ec6\u5fae\u5dee\u522b\u7684\u51fd\u6570\u3002\u901a\u8fc7\u805a\u5408\u548c\u5206\u6790logits\u7684\u5206\u5e03\uff0cALSA\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6a21\u578b\u6027\u80fd\u3002", "result": "ALSA\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5c55\u73b0\u4e86\u4f18\u4e8e\u57fa\u4e8esoftmax\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u7684\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "ALSA\u662f\u4e00\u4e2a\u5f3a\u5927\u800c\u53ef\u9760\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65b0\u3001\u672a\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u4f30\u8ba1\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4f7f\u5176\u6210\u4e3a\u53ef\u9760\u6a21\u578b\u8bc4\u4f30\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.19651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19651", "abs": "https://arxiv.org/abs/2508.19651", "authors": ["B\u00e1lint M\u00e9sz\u00e1ros", "Ahmet Firintepe", "Sebastian Schmidt", "Stephan G\u00fcnnemann"], "title": "Scalable Object Detection in the Car Interior With Vision Foundation Models", "comment": null, "summary": "AI tasks in the car interior like identifying and localizing externally\nintroduced objects is crucial for response quality of personal assistants.\nHowever, computational resources of on-board systems remain highly constrained,\nrestricting the deployment of such solutions directly within the vehicle. To\naddress this limitation, we propose the novel Object Detection and Localization\n(ODAL) framework for interior scene understanding. Our approach leverages\nvision foundation models through a distributed architecture, splitting\ncomputational tasks between on-board and cloud. This design overcomes the\nresource constraints of running foundation models directly in the car. To\nbenchmark model performance, we introduce ODALbench, a new metric for\ncomprehensive assessment of detection and localization.Our analysis\ndemonstrates the framework's potential to establish new standards in this\ndomain. We compare the state-of-the-art GPT-4o vision foundation model with the\nlightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the\nlightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model\nachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its\nbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the\nfine-tuned model maintains high detection accuracy while significantly reducing\nhallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODAL\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8f66\u8f7d\u7cfb\u7edf\u4e2d\u8fdb\u884c\u8f66\u5185\u7269\u4f53\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7ed3\u5408\u8f66\u5185\u4e0e\u4e91\u7aef\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165ODALbench\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u8f66\u8f7d\u4e2a\u4eba\u52a9\u7406\u9700\u8981\u8bc6\u522b\u548c\u5b9a\u4f4d\u8f66\u5185\u7269\u4f53\u4ee5\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\uff0c\u4f46\u8f66\u8f7d\u7cfb\u7edf\u8d44\u6e90\u53d7\u9650\uff0c\u96be\u4ee5\u76f4\u63a5\u90e8\u7f72\u590d\u6742\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faODAL\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\u7ed9\u8f66\u8f7d\u7cfb\u7edf\u548c\u4e91\u7aef\uff0c\u5e76\u5f15\u5165ODALbench\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5bf9\u6bd4\u4e86GPT-4o\u548cLLaVA 1.5 7B\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u5fae\u8c03\u5bf9\u8f7b\u91cf\u7ea7\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5fae\u8c03\u540e\u7684ODAL-LLaVA\u6a21\u578b\u8fbe\u5230\u4e8689%\u7684ODAL\u5206\u6570\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8671%\uff0c\u5e76\u4e14\u4f18\u4e8eGPT-4o\u8fd120%\u3002\u8be5\u6a21\u578b\u8fd8\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0cODAL_SNR\u662fGPT-4o\u7684\u4e09\u500d\u3002", "conclusion": "ODAL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8f66\u5185\u4e0e\u4e91\u7aef\u8ba1\u7b97\uff0c\u5e76\u5229\u7528\u5fae\u8c03\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8f66\u8f7d\u7cfb\u7edf\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u8f66\u5185\u7269\u4f53\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u5e76\u5c55\u793a\u4e86\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19621", "abs": "https://arxiv.org/abs/2508.19621", "authors": ["Tiandi Ye", "Wenyan Liu", "Kai Yao", "Lichun Li", "Shangchao Su", "Cen Chen", "Xiang Li", "Shan Yin", "Ming Gao"], "title": "Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning", "comment": "Accepted by CIKM2025", "summary": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative model training across multiple distributed clients\nwithout disclosing their raw data. Personalized federated learning (pFL) has\ngained increasing attention for its ability to address data heterogeneity.\nHowever, most existing pFL methods assume that each client's data follows a\nsingle distribution and learn one client-level personalized model for each\nclient. This assumption often fails in practice, where a single client may\npossess data from multiple sources or domains, resulting in significant\nintra-client heterogeneity and suboptimal performance. To tackle this\nchallenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework\nbased on visual prompt tuning. Specifically, we formulate instance-wise prompt\ngeneration from a Bayesian perspective and model the prompt posterior as an\nimplicit distribution to capture diverse visual semantics. We derive a\nvariational training objective under the semi-implicit variational inference\nframework. Extensive experiments on benchmark datasets demonstrate that\npFedBayesPT consistently outperforms existing pFL methods under both feature\nand label heterogeneity settings.", "AI": {"tldr": "pFedBayesPT\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5ba2\u6237\u7aef\u5185\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u5728\u7279\u5f81\u548c\u6807\u7b7e\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff08pFL\uff09\u65b9\u6cd5\u5047\u8bbe\u5ba2\u6237\u7aef\u6570\u636e\u6765\u81ea\u5355\u4e00\u5206\u5e03\uff0c\u8fd9\u5728\u5ba2\u6237\u7aef\u5185\u6570\u636e\u5f02\u8d28\u6027\uff08\u6765\u81ea\u591a\u4e2a\u6e90\u6216\u57df\uff09\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fapFedBayesPT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7684\u7ec6\u7c92\u5ea6\u3001\u5b9e\u4f8b\u7ea7\u522b\u7684pFL\u6846\u67b6\u3002\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u751f\u6210\u5b9e\u4f8b\u7ea7\u522b\u7684\u63d0\u793a\uff0c\u5e76\u5c06\u63d0\u793a\u540e\u9a8c\u5efa\u6a21\u4e3a\u9690\u5f0f\u5206\u5e03\u4ee5\u6355\u6349\u591a\u6837\u5316\u7684\u89c6\u89c9\u8bed\u4e49\u3002\u5728\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\u6846\u67b6\u4e0b\u63a8\u5bfc\u53d8\u5206\u8bad\u7ec3\u76ee\u6807\u3002", "result": "pFedBayesPT\u5728\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u7279\u5f81\u5f02\u8d28\u6027\u8fd8\u662f\u6807\u7b7e\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e0b\uff0c\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684pFL\u65b9\u6cd5\u3002", "conclusion": "pFedBayesPT\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5ba2\u6237\u7aef\u5185\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709pFL\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19831", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19831", "abs": "https://arxiv.org/abs/2508.19831", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.", "AI": {"tldr": "\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5370\u5730\u8bed\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u57fa\u51c6\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u5957\u5305\u542b\u4e94\u4e2a\u5370\u5730\u8bedLLM\u8bc4\u4f30\u6570\u636e\u96c6\uff08IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi\uff09\u7684\u65b0\u57fa\u51c6\u3002\u8fd9\u4e9b\u57fa\u51c6\u901a\u8fc7\u7ed3\u5408\u4ece\u5934\u5f00\u59cb\u7684\u4eba\u5de5\u6807\u6ce8\u548c\u7ffb\u8bd1\u9a8c\u8bc1\u8fc7\u7a0b\u521b\u5efa\u3002\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u57fa\u51c6\u5bf9\u652f\u6301\u5370\u5730\u8bed\u7684\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u3002\u6b64\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u5f00\u53d1\u3002", "motivation": "\u8bc4\u4f30\u5370\u5730\u8bed\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\uff0c\u76f4\u63a5\u7ffb\u8bd1\u7684\u82f1\u6587\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u5173\u952e\u7684\u8bed\u8a00\u548c\u6587\u5316\u5dee\u5f02\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u5957\u5305\u542b\u4e94\u4e2a\u5370\u5730\u8bedLLM\u8bc4\u4f30\u6570\u636e\u96c6\uff08IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi\uff09\uff0c\u4f7f\u7528\u4e86\u7ed3\u5408\u4ece\u5934\u5f00\u59cb\u7684\u4eba\u5de5\u6807\u6ce8\u548c\u7ffb\u8bd1\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "result": "\u5bf9\u652f\u6301\u5370\u5730\u8bed\u7684\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u5370\u5730\u8bedLLM\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u57fa\u51c6\u7684\u53ef\u590d\u5236\u65b9\u6cd5\u3002"}}
{"id": "2508.19652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19652", "abs": "https://arxiv.org/abs/2508.19652", "authors": ["Zongxia Li", "Wenhao Yu", "Chengsong Huang", "Rui Liu", "Zhenwen Liang", "Fuxiao Liu", "Jingxi Che", "Dian Yu", "Jordan Boyd-Graber", "Haitao Mi", "Dong Yu"], "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition", "comment": "16 pages, two figures", "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.", "AI": {"tldr": "Vision-SR1\u662f\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u81ea\u6211\u5956\u52b1\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u76d1\u7763\u5373\u53ef\u63d0\u9ad8\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u7684\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u7b54\u6848\u5339\u914d\uff0c\u5e76\u4e14\u53ea\u76d1\u7763\u6700\u7ec8\u8f93\u51fa\uff0c\u5bfc\u81f4\u4e2d\u95f4\u89c6\u89c9\u63a8\u7406\u7f3a\u4e4f\u660e\u786e\u6307\u5bfc\uff0c\u4f7f\u6a21\u578b\u4f18\u5148\u8003\u8651\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u800c\u975e\u89c6\u89c9\u611f\u77e5\u3002\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u800c\u5916\u90e8\u84b8\u998f\u6807\u7b7e\u4f1a\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\u548c\u5956\u52b1\u7834\u89e3\u3002", "method": "Vision-SR1\u5c06VLM\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u89c6\u89c9\u611f\u77e5\u548c\u8bed\u8a00\u63a8\u7406\u3002\u9996\u5148\uff0c\u6a21\u578b\u751f\u6210\u4e0d\u4f9d\u8d56\u539f\u59cb\u56fe\u50cf\u7684\u3001\u81ea\u5305\u542b\u7684\u89c6\u89c9\u611f\u77e5\u3002\u7136\u540e\uff0c\u4f7f\u7528\u76f8\u540c\u7684VLM\u6a21\u578b\uff0c\u4ec5\u4ee5\u751f\u6210\u7684\u611f\u77e5\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u8bed\u8a00\u63a8\u7406\uff0c\u4ee5\u8ba1\u7b97\u5956\u52b1\u3002\u8fd9\u79cd\u81ea\u6211\u5956\u52b1\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u76d1\u7763\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u5e73\u8861\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "Vision-SR1\u5728\u5404\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u89c6\u89c9\u5e7b\u89c9\uff0c\u5e76\u964d\u4f4e\u4e86\u5bf9\u8bed\u8a00\u6377\u5f84\u7684\u4f9d\u8d56\u3002", "conclusion": "Vision-SR1\u901a\u8fc7\u81ea\u6211\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLM\u4e2d\u7684\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u53ef\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u7684\u5916\u90e8\u76d1\u7763\u3002"}}
{"id": "2508.19659", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19659", "abs": "https://arxiv.org/abs/2508.19659", "authors": ["Ri Su", "Zhao Chen", "Caleb Chen Cao", "Nan Tang", "Lei Chen"], "title": "SCAR: A Characterization Scheme for Multi-Modal Dataset", "comment": "6 pages, 3 figures", "summary": "Foundation models exhibit remarkable generalization across diverse tasks,\nlargely driven by the characteristics of their training data. Recent\ndata-centric methods like pruning and compression aim to optimize training but\noffer limited theoretical insight into how data properties affect\ngeneralization, especially the data characteristics in sample scaling.\nTraditional perspectives further constrain progress by focusing predominantly\non data quantity and training efficiency, often overlooking structural aspects\nof data quality. In this study, we introduce SCAR, a principled scheme for\ncharacterizing the intrinsic structural properties of datasets across four key\nmeasures: Scale, Coverage, Authenticity, and Richness. Unlike prior\ndata-centric measures, SCAR captures stable characteristics that remain\ninvariant under dataset scaling, providing a robust and general foundation for\ndata understanding. Leveraging these structural properties, we introduce\nFoundation Data-a minimal subset that preserves the generalization behavior of\nthe full dataset without requiring model-specific retraining. We model\nsingle-modality tasks as step functions and estimate the distribution of the\nfoundation data size to capture step-wise generalization bias across modalities\nin the target multi-modal dataset. Finally, we develop a SCAR-guided data\ncompletion strategy based on this generalization bias, which enables efficient,\nmodality-aware expansion of modality-specific characteristics in multimodal\ndatasets. Experiments across diverse multi-modal datasets and model\narchitectures validate the effectiveness of SCAR in predicting data utility and\nguiding data acquisition. Code is available at https://github.com/McAloma/SCAR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SCAR\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u5f81\u6570\u636e\u96c6\u7684\u5185\u5728\u7ed3\u6784\u5c5e\u6027\uff08\u5c3a\u5ea6\u3001\u8986\u76d6\u5ea6\u3001\u771f\u5b9e\u6027\u548c\u4e30\u5bcc\u6027\uff09\uff0c\u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\u6784\u5efa\u4e86Foundation Data\uff0c\u5373\u80fd\u4fdd\u6301\u5b8c\u6574\u6570\u636e\u96c6\u6cdb\u5316\u884c\u4e3a\u7684\u6700\u5c0f\u5b50\u96c6\u3002\u7814\u7a76\u8fd8\u5229\u7528SCAR\u6765\u4f30\u8ba1\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u6cdb\u5316\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86SCAR\u5f15\u5bfc\u7684\u6570\u636e\u8865\u5168\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u6a21\u6001\u611f\u77e5\u7684\u6570\u636e\u6269\u5c55\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff08\u5982\u526a\u679d\u3001\u538b\u7f29\uff09\u5728\u4f18\u5316\u8bad\u7ec3\u65f6\u7406\u8bba\u6d1e\u5bdf\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u7f29\u653e\u7684\u6570\u636e\u7279\u5f81\u65b9\u9762\u3002\u4f20\u7edf\u89c2\u70b9\u8fc7\u5ea6\u5173\u6ce8\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u8d28\u91cf\u7684\u7ed3\u6784\u6027\u65b9\u9762\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u6570\u636e\u8868\u5f81\u548c\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSCAR\u6846\u67b6\uff0c\u5305\u542b\u5c3a\u5ea6\u3001\u8986\u76d6\u5ea6\u3001\u771f\u5b9e\u6027\u548c\u4e30\u5bcc\u6027\u56db\u4e2a\u5173\u952e\u6307\u6807\uff0c\u7528\u4e8e\u8868\u5f81\u6570\u636e\u96c6\u7684\u5185\u5728\u7ed3\u6784\u5c5e\u6027\u3002SCAR\u6355\u6349\u4e0d\u968f\u6570\u636e\u96c6\u7f29\u653e\u800c\u6539\u53d8\u7684\u7a33\u5b9a\u7279\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efaFoundation Data\uff08\u4fdd\u7559\u5b8c\u6574\u6570\u636e\u96c6\u6cdb\u5316\u884c\u4e3a\u7684\u6700\u5c0f\u5b50\u96c6\uff09\u3002\u5c06\u5355\u6a21\u6001\u4efb\u52a1\u5efa\u6a21\u4e3a\u9636\u68af\u51fd\u6570\uff0c\u4f30\u8ba1Foundation Data\u7684\u5927\u5c0f\u5206\u5e03\uff0c\u4ee5\u6355\u83b7\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e2d\u8de8\u6a21\u6001\u7684\u9636\u68af\u5f0f\u6cdb\u5316\u504f\u5dee\u3002\u6700\u540e\uff0c\u57fa\u4e8e\u6cdb\u5316\u504f\u5dee\u63d0\u51faSCAR\u5f15\u5bfc\u7684\u6570\u636e\u8865\u5168\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSCAR\u80fd\u591f\u6709\u6548\u9884\u6d4b\u6570\u636e\u6548\u7528\u5e76\u6307\u5bfc\u6570\u636e\u91c7\u96c6\u3002\u7814\u7a76\u901a\u8fc7\u5728\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86SCAR\u7684\u6709\u6548\u6027\u3002", "conclusion": "SCAR\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u548c\u6837\u672c\u7f29\u653e\u65f6\u3002Foundation Data\u7684\u6982\u5ff5\u4ee5\u53caSCAR\u5f15\u5bfc\u7684\u6570\u636e\u8865\u5168\u7b56\u7565\uff0c\u4e3a\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u5229\u7528\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19836", "categories": ["cs.CL", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2508.19836", "abs": "https://arxiv.org/abs/2508.19836", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-S\u00f8renssen", "Tor Ole B. Odden"], "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u8be5\u6846\u67b6\u53ea\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\uff0c\u5e76\u80fd\u4e0e\u4f20\u7edf\u5b9a\u6027\u5de5\u4f5c\u6d41\u7a0b\u517c\u5bb9\uff0c\u5728\u4eba\u7c7b\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edf\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u7684NLP\u89e3\u51b3\u65b9\u6848\uff08\u5982\u76d1\u7763\u5206\u7c7b\u5668\u3001\u4e3b\u9898\u5efa\u6a21\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5728\u5b9a\u6027\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u3001\u7834\u574f\u4e86\u73b0\u6709\u7684\u5b9a\u6027\u5de5\u4f5c\u6d41\u7a0b\u6216\u4ea7\u751f\u53ef\u53d8\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ea\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\uff0c\u5e76\u80fd\u4e0e\u6807\u51c6\u7684\u5b9a\u6027\u5de5\u4f5c\u6d41\u7a0b\u826f\u597d\u5951\u5408\u3002", "result": "\u4e0e\u4eba\u7c7b\u5206\u6790\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u6982\u5ff5\u7269\u7406\u8c03\u67e5\uff08\u5305\u542b2899\u4e2a\u5f00\u653e\u5f0f\u56de\u590d\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e860.74\u81f30.83\u7684Cohen's Kappa\u7cfb\u6570\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u901a\u8fc7\u5fae\u8c03\u6587\u672c\u5d4c\u5165\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u6846\u67b6\u6027\u80fd\uff0c\u5e76\u53ef\u7528\u4e8e\u5ba1\u8ba1\u5148\u524d\u5206\u6790\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u7f16\u7801\u53ef\u4ee5\u7075\u6d3b\u5730\u6269\u5c55\u5230\u6570\u5343\u4e2a\u56de\u590d\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u4e3a\u5927\u89c4\u6a21\u6f14\u7ece\u5b9a\u6027\u5206\u6790\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.19654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19654", "abs": "https://arxiv.org/abs/2508.19654", "authors": ["Matthias H\u00f6fflin", "J\u00fcrgen Wassner"], "title": "Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications", "comment": "Accepted for the IAA-SPAICE 2025 conference", "summary": "Spiking Neural Networks (SNNs), inspired by biological intelligence, have\nlong been considered inherently energy-efficient, making them attractive for\nresource-constrained domains such as space applications. However, recent\ncomparative studies with conventional Artificial Neural Networks (ANNs) have\nbegun to question this reputation, especially for digital implementations. This\nwork investigates SNNs for multi-output regression, specifically 3-D satellite\nposition estimation from monocular images, and compares hardware-aware and\nhardware-agnostic energy estimation methods. The proposed SNN, trained using\nthe membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the\nfinal layer, achieves comparable Mean Squared Error (MSE) to a reference\nConvolutional Neural Network (CNN) on a photorealistic satellite dataset.\nEnergy analysis shows that while hardware-agnostic methods predict a consistent\n50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals\nthat significant energy savings are realized only on neuromorphic hardware and\nwith high input sparsity. The influence of dark pixel ratio on energy\nconsumption is quantified, emphasizing the impact of data characteristics and\nhardware assumptions. These findings highlight the need for transparent\nevaluation methods and explicit disclosure of underlying assumptions to ensure\nfair comparisons of neural network energy efficiency.", "AI": {"tldr": "SNNs for satellite position estimation show comparable MSE to CNNs, but energy savings depend heavily on hardware and data sparsity, questioning their inherent energy efficiency.", "motivation": "Investigate SNNs for multi-output regression (3-D satellite position estimation from monocular images) and compare hardware-aware and hardware-agnostic energy estimation methods, questioning the SNNs' reputation for energy efficiency in digital implementations.", "method": "A proposed SNN using Leaky Integrate-and-Fire (LIF) neuron membrane potential in the final layer was trained and compared to a reference CNN on a satellite dataset. Hardware-aware and hardware-agnostic energy estimations were used.", "result": "The SNN achieved comparable Mean Squared Error (MSE) to the CNN. Hardware-agnostic methods predicted a 50-60% energy advantage for SNNs, but hardware-aware analysis showed significant savings only on neuromorphic hardware with high input sparsity. The dark pixel ratio's influence on energy consumption was quantified.", "conclusion": "Energy savings in SNNs are not inherent and depend significantly on the target hardware and data characteristics (e.g., input sparsity). Transparent evaluation methods and disclosure of assumptions are crucial for fair energy efficiency comparisons."}}
{"id": "2508.19856", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19856", "abs": "https://arxiv.org/abs/2508.19856", "authors": ["Shashi Kumar", "Srikanth Madikeri", "Esa\u00fa Villatoro-Tello", "Sergio Burdisso", "Pradeep Rangappa", "Andr\u00e9s Carofilis", "Petr Motlicek", "Karthik Pandia", "Shankar Venkatesan", "Kadri Hacio\u011flu", "Andreas Stolcke"], "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation", "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE", "summary": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.", "AI": {"tldr": "TokenVerse++ \u5141\u8bb8\u4f7f\u7528\u90e8\u5206\u6807\u6ce8\u7684\u6570\u636e\u96c6\u8fdb\u884c\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u5411\u91cf\u52a8\u6001\u6fc0\u6d3b\u4efb\u52a1\uff0c\u5728\u4e0d\u727a\u7272 ASR \u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e TokenVerse \u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 TokenVerse \u7b49\u57fa\u4e8e Token \u7684\u591a\u4efb\u52a1\u6846\u67b6\u8981\u6c42\u6240\u6709\u8bad\u7ec3\u6837\u672c\u90fd\u5305\u542b\u6240\u6709\u4efb\u52a1\u7684\u6807\u7b7e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5229\u7528\u90e8\u5206\u6807\u6ce8\u6570\u636e\u96c6\u548c\u6709\u6548\u6269\u5c55\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa TokenVerse++\uff0c\u5728 XLSR-Transducer ASR \u6a21\u578b\u7684\u58f0\u5b66\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f15\u5165\u4e86\u53ef\u5b66\u4e60\u5411\u91cf\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u4efb\u52a1\u6fc0\u6d3b\uff0c\u4ece\u800c\u5141\u8bb8\u4f7f\u7528\u4ec5\u4e3a\u90e8\u5206\u4efb\u52a1\u6807\u6ce8\u7684\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u5730\u96c6\u6210\u4e86\u5305\u542b\u90e8\u5206\u6807\u6ce8\uff08\u9488\u5bf9 ASR \u548c\u8bed\u8a00\u8bc6\u522b\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002TokenVerse++ \u5728\u591a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0e TokenVerse \u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 ASR \u6027\u80fd\u3002", "conclusion": "TokenVerse++ \u662f\u4e00\u4e2a\u66f4\u5b9e\u7528\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u90e8\u5206\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u4e14\u5728\u4e0d\u727a\u7272 ASR \u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2508.19664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19664", "abs": "https://arxiv.org/abs/2508.19664", "authors": ["Weicheng Liao", "Zan Chen", "Jianyang Xie", "Yalin Zheng", "Yuhui Ma", "Yitian Zhao"], "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement", "comment": null, "summary": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics\nby providing a comprehensive view of the retina. However, it often suffers from\nquality-degrading factors such as blurring and uneven illumination, which\nobscure fine details and mask pathological information. While numerous retinal\nimage enhancement methods have been proposed for other fundus imageries, they\noften fail to address the unique requirements in UWF, particularly the need to\npreserve pathological details. In this paper, we propose a novel\nfrequency-aware self-supervised learning method for UWF image enhancement. It\nincorporates frequency-decoupled image deblurring and Retinex-guided\nillumination compensation modules. An asymmetric channel integration operation\nis introduced in the former module, so as to combine global and local views by\nleveraging high- and low-frequency information, ensuring the preservation of\nfine and broader structural details. In addition, a color preservation unit is\nproposed in the latter Retinex-based module, to provide multi-scale spatial and\nfrequency information, enabling accurate illumination estimation and\ncorrection. Experimental results demonstrate that the proposed work not only\nenhances visualization quality but also improves disease diagnosis performance\nby restoring and correcting fine local details and uneven intensity. To the\nbest of our knowledge, this work is the first attempt for UWF image\nenhancement, offering a robust and clinically valuable tool for improving\nretinal disease management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u9891\u7387\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d85\u5e7f\u89d2\uff08UWF\uff09\u773c\u5e95\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u7cca\u548c\u5149\u7167\u8865\u507f\u6765\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "UWF\u773c\u5e95\u6210\u50cf\u867d\u7136\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89c6\u7f51\u819c\u89c6\u56fe\uff0c\u4f46\u5e38\u53d7\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u5747\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u5f71\u54cd\u7ec6\u8282\u89c2\u5bdf\u548c\u75c5\u53d8\u68c0\u6d4b\u3002\u73b0\u6709\u589e\u5f3a\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3UWF\u6210\u50cf\u5728\u4fdd\u7559\u75c5\u53d8\u7ec6\u8282\u65b9\u9762\u7684\u7279\u6b8a\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9891\u7387\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u9891\u7387\u89e3\u8026\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u6a21\u5757\u548c\u57fa\u4e8eRetinex\u7684\u5149\u7167\u8865\u507f\u6a21\u5757\u3002\u53bb\u6a21\u7cca\u6a21\u5757\u91c7\u7528\u975e\u5bf9\u79f0\u901a\u9053\u96c6\u6210\u64cd\u4f5c\uff0c\u7ed3\u5408\u9ad8\u4f4e\u9891\u4fe1\u606f\u4ee5\u4fdd\u7559\u5168\u5c40\u548c\u5c40\u90e8\u7ec6\u8282\u3002\u5149\u7167\u8865\u507f\u6a21\u5757\u5305\u542b\u989c\u8272\u4fdd\u6301\u5355\u5143\uff0c\u5229\u7528\u591a\u5c3a\u5ea6\u7a7a\u95f4\u548c\u9891\u7387\u4fe1\u606f\u8fdb\u884c\u51c6\u786e\u7684\u5149\u7167\u4f30\u8ba1\u548c\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86UWF\u773c\u5e95\u56fe\u50cf\u7684\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u8fd8\u901a\u8fc7\u6062\u590d\u548c\u6821\u6b63\u5c40\u90e8\u7ec6\u8282\u548c\u4e0d\u5747\u5300\u5f3a\u5ea6\uff0c\u63d0\u5347\u4e86\u75be\u75c5\u8bca\u65ad\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u662fUWF\u56fe\u50cf\u589e\u5f3a\u9886\u57df\u7684\u9996\u6b21\u5c1d\u8bd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u5177\u6709\u4e34\u5e8a\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6539\u5584\u89c6\u7f51\u819c\u75be\u75c5\u7684\u7ba1\u7406\u3002"}}
{"id": "2508.19873", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19873", "abs": "https://arxiv.org/abs/2508.19873", "authors": ["Vanessa Toborek", "Sebastian M\u00fcller", "Tim Selbach", "Tam\u00e1s Horv\u00e1th", "Christian Bauckhage"], "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning", "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award", "summary": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.", "AI": {"tldr": "\u7b80\u5355\u7ef4\u57fa\u767e\u79d1\u7684\u6807\u7b7e\u53ef\u7528\u4e8e\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u8bfe\u7a0b\u5b66\u4e60\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u4eba\u7c7b\u5b9a\u4e49\u7684\u7b80\u5355\u8bed\u8a00\u4f5c\u4e3a\u8bfe\u7a0b\u5b66\u4e60\uff08CL\uff09\u7684\u6709\u6548\u4fe1\u53f7\uff0c\u4ee5\u89e3\u51b3\u8bed\u8a00\u5b66\u96be\u5ea6\u7684\u5b9a\u4e49\u548c\u8861\u91cf\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u7b80\u5355\u7ef4\u57fa\u767e\u79d1\u7684\u6587\u7ae0\u7ea7\u522b\u6807\u7b7e\uff0c\u5c06\u57fa\u4e8e\u6807\u7b7e\u7684\u8bfe\u7a0b\u4e0e\u57fa\u4e8e\u80fd\u529b\u7684\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528BERT-tiny\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5c06\u7b80\u5355\u6570\u636e\u901a\u8fc7\u8bfe\u7a0b\uff08\u5c24\u5176\u662f\u5728\u6700\u5f00\u59cb\u5f15\u5165\uff09\u8fdb\u884c\u7ed3\u6784\u5316\uff0c\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u56f0\u60d1\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u7b80\u5355\u8bed\u8a00\u65b9\u9762\u3002\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u4e0e\u968f\u673a\u6392\u5e8f\u76f8\u6bd4\u6ca1\u6709\u5e26\u6765\u4e00\u81f4\u7684\u6536\u76ca\u3002", "conclusion": "\u4eba\u7c7b\u5bf9\u8bed\u8a00\u96be\u5ea6\u7684\u76f4\u89c9\u53ef\u4ee5\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u8bfe\u7a0b\u5b66\u4e60\u3002"}}
{"id": "2508.19688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19688", "abs": "https://arxiv.org/abs/2508.19688", "authors": ["Gangjian Zhang", "Jian Shu", "Nanjie Yao", "Hao Wang"], "title": "SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction", "comment": "10 pages, 8 figures", "summary": "Monocular texture 3D human reconstruction aims to create a complete 3D\ndigital avatar from just a single front-view human RGB image. However, the\ngeometric ambiguity inherent in a single 2D image and the scarcity of 3D human\ntraining data are the main obstacles limiting progress in this field. To\naddress these issues, current methods employ prior geometric estimation\nnetworks to derive various human geometric forms, such as the SMPL model and\nnormal maps. However, they struggle to integrate these modalities effectively,\nleading to view inconsistencies, such as facial distortions. To this end, we\npropose a two-process 3D human reconstruction framework, SAT, which seamlessly\nlearns various prior geometries in a unified manner and reconstructs\nhigh-quality textured 3D avatars as the final output. To further facilitate\ngeometry learning, we introduce a Supervisor Feature Regularization module. By\nemploying a multi-view network with the same structure to provide intermediate\nfeatures as training supervision, these varied geometric priors can be better\nfused. To tackle data scarcity and further improve reconstruction quality, we\nalso propose an Online Animation Augmentation module. By building a\none-feed-forward animation network, we augment a massive number of samples from\nthe original 3D human data online for model training. Extensive experiments on\ntwo benchmarks show the superiority of our approach compared to\nstate-of-the-art methods.", "AI": {"tldr": "SAT\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u591a\u79cd\u5148\u9a8c\u51e0\u4f55\u5f62\u72b6\u5e76\u5f15\u5165\u65b0\u6a21\u5757\u6765\u89e3\u51b3\u5355\u76ee\u7eb9\u74063D\u4eba\u7c7b\u91cd\u5efa\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u548c\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u5316\u8eab\u91cd\u5efa\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5355\u76ee\u7eb9\u74063D\u4eba\u7c7b\u91cd\u5efa\u9762\u4e34\u51e0\u4f55\u6a21\u7cca\u548c\u6570\u636e\u7a00\u758f\u6027\u4e24\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u5408\u4e0d\u540c\u51e0\u4f55\u5148\u9a8c\uff08\u5982SMPL\u6a21\u578b\u3001\u6cd5\u7ebf\u56fe\uff09\u65f6\u5b58\u5728\u89c6\u56fe\u4e0d\u4e00\u81f4\u548c\u9762\u90e8\u7578\u53d8\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAT\u7684\u4e24\u9636\u6bb53D\u4eba\u7c7b\u91cd\u5efa\u6846\u67b6\u3002\u5f15\u5165\u4e86Supervisor Feature Regularization\u6a21\u5757\uff0c\u5229\u7528\u591a\u89c6\u56fe\u7f51\u7edc\u63d0\u4f9b\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u76d1\u7763\uff0c\u4ee5\u66f4\u597d\u5730\u878d\u5408\u51e0\u4f55\u5148\u9a8c\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86Online Animation Augmentation\u6a21\u5757\uff0c\u901a\u8fc7\u524d\u9988\u52a8\u753b\u7f51\u7edc\u5728\u7ebf\u589e\u5f3a3D\u4eba\u7c7b\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u7a00\u758f\u6027\u5e76\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "result": "SAT\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u5b66\u4e60\u591a\u79cd\u5148\u9a8c\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7eb9\u74063D\u5316\u8eab\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SAT\u6846\u67b6\u901a\u8fc7\u5176\u65b0\u9896\u7684\u7ed3\u6784\u548c\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5355\u76ee3D\u4eba\u7c7b\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19709", "categories": ["cs.LG", "math.FA", "26A16"], "pdf": "https://arxiv.org/pdf/2508.19709", "abs": "https://arxiv.org/abs/2508.19709", "authors": ["R. Arnau", "A. Gonz\u00e1lez Cort\u00e9s", "E. A. S\u00e1nchez P\u00e9rez", "S. Sanjuan"], "title": "Metric spaces of walks and Lipschitz duality on graphs", "comment": "31 pages, 3 figures", "summary": "We study the metric structure of walks on graphs, understood as Lipschitz\nsequences. To this end, a weighted metric is introduced to handle sequences,\nenabling the definition of distances between walks based on stepwise vertex\ndistances and weighted norms. We analyze the main properties of these metric\nspaces, which provides the foundation for the analysis of weaker forms of\ninstruments to measure relative distances between walks: proximities. We\nprovide some representation formulas for such proximities under different\nassumptions and provide explicit constructions for these cases. The resulting\nmetric framework allows the use of classical tools from metric modeling, such\nas the extension of Lipschitz functions from subspaces of walks, which permits\nextending proximity functions while preserving fundamental properties via the\nmentioned representations. Potential applications include the estimation of\nproximities and the development of reinforcement learning strategies based on\nexploratory walks, offering a robust approach to Lipschitz regression on\nnetwork structures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u4e0a\u884c\u8d70\uff08\u6b65\u6001\uff09\u7684\u5ea6\u91cf\u7ed3\u6784\uff0c\u5f15\u5165\u4e86\u52a0\u6743\u5ea6\u91cf\u6765\u5b9a\u4e49\u6b65\u6001\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u5206\u6790\u4e86\u5ea6\u91cf\u7a7a\u95f4\u7684\u6027\u8d28\u3002\u7814\u7a76\u7ed3\u679c\u53ef\u7528\u4e8e\u4f30\u8ba1\u90bb\u8fd1\u5ea6\u3001\u5f00\u53d1\u57fa\u4e8e\u63a2\u7d22\u6027\u6b65\u6001\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u53ca\u5728\u7f51\u7edc\u7ed3\u6784\u4e0a\u8fdb\u884cLipschitz\u56de\u5f52\u3002", "motivation": "\u7814\u7a76\u56fe\u4e0a\u884c\u8d70\uff08\u6b65\u6001\uff09\u7684\u5ea6\u91cf\u7ed3\u6784\uff0c\u4e3a\u5206\u6790\u6b65\u6001\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\uff08\u90bb\u8fd1\u5ea6\uff09\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f15\u5165\u52a0\u6743\u5ea6\u91cf\u6765\u5904\u7406\u5e8f\u5217\uff08\u6b65\u6001\uff09\uff0c\u5b9a\u4e49\u4e86\u57fa\u4e8e\u9876\u70b9\u4e4b\u95f4\u9010\u6b65\u8ddd\u79bb\u548c\u52a0\u6743\u8303\u6570\u7684\u6b65\u6001\u8ddd\u79bb\u3002\u5206\u6790\u4e86\u6240\u5f97\u5ea6\u91cf\u7a7a\u95f4\u7684\u6027\u8d28\uff0c\u5e76\u63a8\u5bfc\u4e86\u90bb\u8fd1\u5ea6\u7684\u8868\u793a\u516c\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u5ea6\u91cf\u6846\u67b6\uff0c\u5141\u8bb8\u4f7f\u7528\u7ecf\u5178\u7684\u5ea6\u91cf\u5efa\u6a21\u5de5\u5177\uff0c\u4f8b\u5982\u4ece\u6b65\u6001\u5b50\u7a7a\u95f4\u6269\u5c55Lipschitz\u51fd\u6570\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u57fa\u672c\u6027\u8d28\u7684\u540c\u65f6\u6269\u5c55\u90bb\u8fd1\u51fd\u6570\u3002", "conclusion": "\u8be5\u5ea6\u91cf\u6846\u67b6\u4e3a\u5206\u6790\u548c\u5904\u7406\u56fe\u4e0a\u884c\u8d70\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5728\u90bb\u8fd1\u5ea6\u4f30\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.19883", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19883", "abs": "https://arxiv.org/abs/2508.19883", "authors": ["Chiman Salavati", "Shannon Song", "Scott A. Hale", "Roberto E. Montenegro", "Shiri Dori-Hacohen", "Fabricio Murai"], "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula", "comment": "Accepted at 2025 AAAI/ACM AI, Ethics and Society Conference (AIES'25)", "summary": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19698", "categories": ["cs.CV", "cs.IT", "math.IT", "math.SP"], "pdf": "https://arxiv.org/pdf/2508.19698", "abs": "https://arxiv.org/abs/2508.19698", "authors": ["V. S. Usatyuk", "D. A. Sapozhnikov", "S. I. Egorov"], "title": "Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators", "comment": "14 pages, 10 figures", "summary": "The rapid advance of deep generative models such as GANs and diffusion\nnetworks now produces images that are virtually indistinguishable from genuine\nphotographs, undermining media forensics and biometric security. Supervised\ndetectors quickly lose effectiveness on unseen generators or after adversarial\npost-processing, while existing unsupervised methods that rely on low-level\nstatistical cues remain fragile. We introduce a physics-inspired,\nmodel-agnostic detector that treats synthetic-image identification as a\ncommunity-detection problem on a sparse weighted graph. Image features are\nfirst extracted with pretrained CNNs and reduced to 32 dimensions, each feature\nvector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities\nare transformed into edge couplings calibrated at the Nishimori temperature,\nproducing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum\nexhibits a characteristic gap when genuine community structure (real images) is\npresent. Synthetic images violate the Nishimori symmetry and therefore lack\nsuch gaps. We validate the approach on binary tasks cat versus dog and male\nversus female using real photos from Flickr-Faces-HQ and CelebA and synthetic\ncounterparts generated by GANs and diffusion models. Without any labeled\nsynthetic data or retraining of the feature extractor, the detector achieves\nover 94% accuracy. Spectral analysis shows multiple well separated gaps for\nreal image sets and a collapsed spectrum for generated ones. Our contributions\nare threefold: a novel LDPC graph construction that embeds deep image features,\nan analytical link between Nishimori temperature RBIM and the Bethe-Hessian\nspectrum providing a Bayes optimal detection criterion; and a practical,\nunsupervised synthetic image detector robust to new generative architectures.\nFuture work will extend the framework to video streams and multi-class anomaly\ndetection.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u548c\u6269\u6563\u7f51\u7edc\uff09\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u771f\u5b9e\u7167\u7247\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\uff0c\u5bf9\u5a92\u4f53\u53d6\u8bc1\u548c\u751f\u7269\u8bc6\u522b\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0d\u53d7\u6a21\u578b\u7ea6\u675f\u7684\u7269\u7406\u542f\u53d1\u5f0f\u68c0\u6d4b\u5668\uff0c\u5c06\u5408\u6210\u56fe\u50cf\u8bc6\u522b\u89c6\u4e3a\u7a00\u758f\u52a0\u6743\u56fe\u4e0a\u7684\u793e\u7fa4\u68c0\u6d4b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u7684CNN\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u591a\u8fb9\u7c7b\u578bQC-LDPC\u56fe\uff0c\u5229\u7528Nishimori\u6e29\u5ea6\u6821\u51c6\u7684\u8fb9\u8026\u5408\u5f62\u6210\u968f\u673a\u952eIsing\u6a21\u578b\uff08RBIM\uff09\u3002\u771f\u5b9e\u56fe\u50cf\u5728RBIM\u7684Bethe-Hessian\u8c31\u4e2d\u8868\u73b0\u51fa\u7279\u5f81\u6027\u95f4\u9699\uff0c\u800c\u5408\u6210\u56fe\u50cf\u5219\u7834\u574f\u4e86Nishimori\u5bf9\u79f0\u6027\uff0c\u7f3a\u5c11\u8be5\u95f4\u9699\u3002\u8be5\u68c0\u6d4b\u5668\u5728\u732b\u72d7\u548c\u7537\u6027\u5973\u6027\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u65e0\u9700\u6807\u8bb0\u7684\u5408\u6210\u6570\u636e\u6216\u7279\u5f81\u63d0\u53d6\u5668\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u51c6\u786e\u7387\u5373\u53ef\u8fbe\u523094%\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5bf9\u65b0\u7684\u751f\u6210\u67b6\u6784\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u548c\u6269\u6563\u7f51\u7edc\uff09\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u771f\u5b9e\u7167\u7247\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u5bf9\u5a92\u4f53\u53d6\u8bc1\u548c\u751f\u7269\u8bc6\u522b\u5b89\u5168\u6784\u6210\u4e86\u5a01\u80c1\u3002\u73b0\u6709\u7684\u76d1\u7763\u68c0\u6d4b\u5668\u5728\u9762\u5bf9\u65b0\u7684\u751f\u6210\u5668\u6216\u5bf9\u6297\u6027\u540e\u5904\u7406\u65f6\u6548\u679c\u4f1a\u8fc5\u901f\u4e0b\u964d\uff0c\u800c\u4f9d\u8d56\u4f4e\u7ea7\u7edf\u8ba1\u7ebf\u7d22\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5219\u4e0d\u591f\u7a33\u5065\u3002", "method": "\u672c\u7814\u7a76\u5c06\u5408\u6210\u56fe\u50cf\u8bc6\u522b\u95ee\u9898\u8f6c\u5316\u4e3a\u7a00\u758f\u52a0\u6743\u56fe\u4e0a\u7684\u793e\u7fa4\u68c0\u6d4b\u95ee\u9898\u3002\u9996\u5148\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684CNN\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u964d\u7ef4\u81f332\u7ef4\uff0c\u6bcf\u4e2a\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u591a\u8fb9\u7c7b\u578bQC-LDPC\u56fe\u7684\u8282\u70b9\u3002\u7136\u540e\uff0c\u5c06\u6210\u5bf9\u76f8\u4f3c\u6027\u8f6c\u5316\u4e3a\u8fb9\u8026\u5408\uff0c\u5e76\u5728Nishimori\u6e29\u5ea6\u4e0b\u8fdb\u884c\u6821\u51c6\uff0c\u5f62\u6210\u4e00\u4e2a\u968f\u673a\u952eIsing\u6a21\u578b\uff08RBIM\uff09\u3002\u8be5\u6a21\u578b\u7684Bethe-Hessian\u8c31\u5728\u5b58\u5728\u771f\u5b9e\u793e\u7fa4\u7ed3\u6784\uff08\u771f\u5b9e\u56fe\u50cf\uff09\u65f6\u4f1a\u5448\u73b0\u51fa\u7279\u5f81\u6027\u95f4\u9699\uff0c\u800c\u5408\u6210\u56fe\u50cf\u4f1a\u7834\u574fNishimori\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u95f4\u9699\u6d88\u5931\u3002", "result": "\u5728\u732b\u72d7\u4e8c\u5206\u7c7b\u4efb\u52a1\u548cCelebA\u6570\u636e\u96c6\u7684\u7537\u6027\u5973\u6027\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u771f\u5b9e\u7167\u7247\u548cGANs\u53ca\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u7167\u7247\u8fdb\u884c\u6d4b\u8bd5\u3002\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u6807\u8bb0\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6216\u7279\u5f81\u63d0\u53d6\u5668\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u68c0\u6d4b\u5668\u8fbe\u5230\u4e8694%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002\u5149\u8c31\u5206\u6790\u663e\u793a\uff0c\u771f\u5b9e\u56fe\u50cf\u96c6\u5177\u6709\u591a\u4e2a\u6e05\u6670\u5206\u5f00\u7684\u95f4\u9699\uff0c\u800c\u751f\u6210\u56fe\u50cf\u96c6\u7684\u5149\u8c31\u5219\u8f83\u4e3a\u6df7\u4e71\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0d\u53d7\u6a21\u578b\u7ea6\u675f\u7684\u7269\u7406\u542f\u53d1\u5f0f\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u56fe\u50cf\u7279\u5f81\u5d4c\u5165\u5230LDPC\u56fe\u4e2d\uff0c\u5e76\u901a\u8fc7RBIM\u7684Bethe-Hessian\u8c31\u5206\u6790\u6765\u533a\u5206\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u51c6\u786e\u7387\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4ee5\u53ca\u5bf9\u65b0\u751f\u6210\u67b6\u6784\u7684\u9c81\u68d2\u6027\u7b49\u4f18\u70b9\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u628a\u8be5\u6846\u67b6\u6269\u5c55\u5230\u89c6\u9891\u6d41\u548c\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2508.19733", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19733", "abs": "https://arxiv.org/abs/2508.19733", "authors": ["Theodoros Athanasiadis", "Steven Adriaensen", "Samuel M\u00fcller", "Frank Hutter"], "title": "Tune My Adam, Please!", "comment": "Accepted as a short paper at the non-archival content track of AutoML\n  2025", "summary": "The Adam optimizer remains one of the most widely used optimizers in deep\nlearning, and effectively tuning its hyperparameters is key to optimizing\nperformance. However, tuning can be tedious and costly. Freeze-thaw Bayesian\nOptimization (BO) is a recent promising approach for low-budget hyperparameter\ntuning, but is limited by generic surrogates without prior knowledge of how\nhyperparameters affect learning. We propose Adam-PFN, a new surrogate model for\nFreeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from\nTaskSet, together with a new learning curve augmentation method, CDF-augment,\nwhich artificially increases the number of available training examples. Our\napproach improves both learning curve extrapolation and accelerates\nhyperparameter optimization on TaskSet evaluation tasks, with strong\nperformance on out-of-distribution (OOD) tasks.", "AI": {"tldr": "Adam-PFN\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u51bb\u878d\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b0\u578b\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3Adam\u4f18\u5316\u5668\u8d85\u53c2\u6570\u8c03\u6574\u7684\u75db\u70b9\uff0c\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Adam\u4f18\u5316\u5668\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8d85\u53c2\u6570\u8c03\u6574\u8fc7\u7a0b\u7e41\u7410\u4e14\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u51bb\u878d\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406Adam\u8d85\u53c2\u6570\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u8d85\u53c2\u6570\u5f71\u54cd\u5b66\u4e60\u8fc7\u7a0b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u6548\u679c\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdam-PFN\u7684\u65b0\u578b\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8eAdam\u8d85\u53c2\u6570\u7684\u51bb\u878d\u8d1d\u53f6\u65af\u4f18\u5316\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5728TaskSet\u5b66\u4e60\u66f2\u7ebf\u4e0a\u7684\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u540d\u4e3aCDF-augment\u7684\u65b0\u578b\u5b66\u4e60\u66f2\u7ebf\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u4eba\u5de5\u589e\u52a0\u8bad\u7ec3\u6837\u672c\u6570\u91cf\u3002", "result": "Adam-PFN\u5728TaskSet\u8bc4\u4f30\u4efb\u52a1\u4e0a\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u80fd\u529b\uff0c\u5e76\u52a0\u901f\u4e86\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\uff0c\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "Adam-PFN\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u548c\u5b66\u4e60\u66f2\u7ebf\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86Adam\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u548c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.19887", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19887", "abs": "https://arxiv.org/abs/2508.19887", "authors": ["Mohammed Rakibul Hasan", "Rafi Majid", "Ahanaf Tahmid"], "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement", "comment": null, "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aBangla-Bayanno\u7684\u5b5f\u52a0\u62c9\u8bed\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u5f00\u653e\u5f0f\u7684\uff0c\u5305\u542b52,650\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d64750\u591a\u5f20\u56fe\u7247\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u591a\u6a21\u6001AI\u7814\u7a76\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u91c7\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7ffb\u8bd1\u4f18\u5316\u6d41\u7a0b\u6765\u786e\u4fdd\u7ffb\u8bd1\u8d28\u91cf\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u7ffb\u8bd1\u8d28\u91cf\u548c\u7b54\u6848\u683c\u5f0f\u4e0a\u7684\u5c40\u9650\u6027\u3002\u95ee\u9898\u5206\u4e3a\u540d\u8bcd\u6027\u3001\u91cf\u5316\u6027\u548c\u6781\u6027\u4e09\u79cd\u56de\u7b54\u7c7b\u578b\uff0c\u65e8\u5728\u4e3a\u5b5f\u52a0\u62c9\u8bed\u4f4e\u8d44\u6e90\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709VQA\u6570\u636e\u96c6\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u591a\u4e3a\u4eba\u5de5\u6807\u6ce8\uff0c\u5b58\u5728\u7279\u5b9a\u9886\u57df\u3001\u67e5\u8be2\u7c7b\u578b\u6216\u7b54\u6848\u683c\u5f0f\u7684\u504f\u89c1\uff0c\u6216\u8005\u7b54\u6848\u683c\u5f0f\u53d7\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u3002\u4e3a\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u5728\u591a\u6a21\u6001AI\u7814\u7a76\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u5f00\u653e\u7684\u5b5f\u52a0\u62c9\u8bedVQA\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u7ffb\u8bd1\u4f18\u5316\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u4f4e\u8d28\u91cf\u7ffb\u8bd1\u95ee\u9898\uff0c\u4fdd\u8bc1\u6e05\u6670\u5ea6\uff0c\u5e76\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u3002\u5c06\u95ee\u9898\u5206\u4e3a\u4e09\u79cd\u56de\u7b54\u7c7b\u578b\uff1a\u540d\u8bcd\u6027\uff08\u7b80\u77ed\u63cf\u8ff0\uff09\u3001\u91cf\u5316\u6027\uff08\u6570\u5b57\uff09\u548c\u6781\u6027\uff08\u662f/\u5426\uff09\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b52,650\u4e2a\u95ee\u7b54\u5bf9\u548c4750\u591a\u5f20\u56fe\u7247\u7684\u5b5f\u52a0\u62c9\u8bed\u5f00\u653e\u5f0fVQA\u6570\u636e\u96c6\uff08Bangla-Bayanno\uff09\u3002\u8be5\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\uff0c\u514b\u670d\u4e86\u73b0\u6709\u4f4e\u8d44\u6e90\u8bed\u8a00VQA\u6570\u636e\u96c6\u7684\u7ffb\u8bd1\u8d28\u91cf\u548c\u7b54\u6848\u683c\u5f0f\u9650\u5236\u3002", "conclusion": "Bangla-Bayanno\u6570\u636e\u96c6\u662f\u5b5f\u52a0\u62c9\u8bed\u4e2d\u6700\u5168\u9762\u7684\u5f00\u6e90\u9ad8\u8d28\u91cfVQA\u57fa\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7814\u7a76\uff0c\u5e76\u63a8\u52a8\u66f4\u5177\u5305\u5bb9\u6027\u7684AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.19699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19699", "abs": "https://arxiv.org/abs/2508.19699", "authors": ["Yupeng Zhang", "Dezhi Zheng", "Ping Lu", "Han Zhang", "Lei Wang", "Liping xiang", "Cheng Luo", "Kaijun Deng", "Xiaowen Fu", "Linlin Shen", "Jinbao Wang"], "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation", "comment": "PRCV 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation\nfor 3D scenes, offering both high-fidelity reconstruction and efficient\nrendering. However, 3DGS lacks 3D segmentation ability, which limits its\napplicability in tasks that require scene understanding. The identification and\nisolating of specific object components is crucial. To address this limitation,\nwe propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments\nthe Gaussian representation with object label.LabelGS introduces cross-view\nconsistent semantic masks for 3D Gaussians and employs a novel Occlusion\nAnalysis Model to avoid overfitting occlusion during optimization, Main\nGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian\nProjection Filter to avoid Gaussian label conflict. Our approach achieves\neffective decoupling of Gaussian representations and refines the 3DGS\noptimization process through a random region sampling strategy, significantly\nimproving efficiency. Extensive experiments demonstrate that LabelGS\noutperforms previous state-of-the-art methods, including Feature-3DGS, in the\n3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup\nin training compared to Feature-3DGS, at a resolution of 1440X1080. Our code\nwill be at https://github.com/garrisonz/LabelGS.", "AI": {"tldr": "LabelGS\u901a\u8fc7\u5f15\u5165\u6807\u7b7e\u611f\u77e5\u6765\u589e\u5f3a3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4ee5\u5b9e\u73b03D\u573a\u666f\u5206\u5272\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u8bed\u4e49\u8499\u7248\u3001\u906e\u6321\u5206\u6790\u6a21\u578b\u3001\u9ad8\u65af\u6807\u7b7e\u6a21\u578b\u548c\u9ad8\u65af\u6295\u5f71\u8fc7\u6ee4\u5668\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u57283D\u573a\u666f\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3DGS\u57283D\u573a\u666f\u91cd\u5efa\u548c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f3D\u5206\u5272\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "LabelGS\u901a\u8fc7\u5f15\u5165\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u8bed\u4e49\u8499\u7248\u3001\u906e\u6321\u5206\u6790\u6a21\u578b\u3001\u9ad8\u65af\u6807\u7b7e\u6a21\u578b\u548c\u9ad8\u65af\u6295\u5f71\u8fc7\u6ee4\u5668\u6765\u589e\u5f3a3D\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u91c7\u7528\u968f\u673a\u533a\u57df\u91c7\u6837\u7b56\u7565\u6765\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "result": "LabelGS\u57283D\u573a\u666f\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u4e0eFeature-3DGS\u76f8\u6bd4\uff0c\u57281440x1080\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e8622\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "LabelGS\u6210\u529f\u5730\u5c06\u6807\u7b7e\u4fe1\u606f\u96c6\u6210\u52303DGS\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u76843D\u573a\u666f\u5206\u5272\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002"}}
{"id": "2508.19903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19903", "abs": "https://arxiv.org/abs/2508.19903", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "comment": "EMNLP 2025", "summary": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u548c\u56de\u58f0\u751f\u6210\u6280\u672f\u8bad\u7ec3\u7684\u6210\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\u63d0\u9ad8\u4e86LLM\u5728\u6f14\u7ece\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u6f14\u7ece\u903b\u8f91\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63a2\u7d22\u4f7f\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u5956\u52b1\u6a21\u578b\u6765\u589e\u5f3aLLM\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u6f14\u7ece\u63a8\u7406\u7684\u6210\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\uff0c\u5e76\u91c7\u7528\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u548c\u56de\u58f0\u751f\u6210\u6280\u672f\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u8986\u76d6\u66f4\u591a\u7c7b\u578b\u7684\u9519\u8bef\u3002", "result": "\u5728FOLIO\u3001JustLogic\u548cProverQA\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528CoT\u548c\u56de\u58f0\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684ORMs\u5728\u56db\u79cd\u4e0d\u540c\u7684LLMs\u4e0a\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8eORMs\u548c\u56de\u58f0\u751f\u6210\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4e3a\u63d0\u5347LLM\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.19705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19705", "abs": "https://arxiv.org/abs/2508.19705", "authors": ["Qiang Hu", "Ying Zhou", "Gepeng Ji", "Nick Barnes", "Qiang Li", "Zhiwei Wang"], "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation", "comment": null, "summary": "Existing video polyp segmentation (VPS) paradigms usually struggle to balance\nbetween spatiotemporal modeling and domain generalization, limiting their\napplicability in real clinical scenarios. To embrace this challenge, we recast\nthe VPS task as a track-by-detect paradigm that leverages the spatial contexts\ncaptured by the image polyp segmentation (IPS) model while integrating the\ntemporal modeling capabilities of segment anything model 2 (SAM2). However,\nduring long-term polyp tracking in colonoscopy videos, SAM2 suffers from error\naccumulation, resulting in a snowball effect that compromises segmentation\nstability. We mitigate this issue by repurposing SAM2 as a video polyp\nsegmenter with two training-free modules. In particular, the intra-association\nfiltering module eliminates spatial inaccuracies originating from the detecting\nstage, reducing false positives. The inter-association refinement module\nadaptively updates the memory bank to prevent error propagation over time,\nenhancing temporal coherence. Both modules work synergistically to stabilize\nSAM2, achieving cutting-edge performance in both in-domain and out-of-domain\nscenarios. Furthermore, we demonstrate the robust tracking capabilities of\nFreeVPS in long-untrimmed colonoscopy videos, underscoring its potential\nreliable clinical analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreeVPS\u7684\u89c6\u9891\u591a\u53d1\u6027\u606f\u8089\u5206\u5272\uff08VPS\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u591a\u53d1\u6027\u606f\u8089\u5206\u5272\uff08IPS\uff09\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548cSAM2\u6a21\u578b\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u6765\u89e3\u51b3\u73b0\u6709VPS\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3SAM2\u5728\u957f\u671f\u8ddf\u8e2a\u4e2d\u51fa\u73b0\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u8bba\u6587\u5f15\u5165\u4e86\u4e24\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u662f\u5728\u68c0\u6d4b\u9636\u6bb5\u6d88\u9664\u7a7a\u95f4\u4e0d\u51c6\u786e\u6027\u7684\u5185\u90e8\u5173\u8054\u8fc7\u6ee4\u6a21\u5757\uff0c\u53e6\u4e00\u4e2a\u662f\u9632\u6b62\u8bef\u5dee\u968f\u65f6\u95f4\u4f20\u64ad\u7684\u5916\u90e8\u5173\u8054\u7ec6\u5316\u6a21\u5757\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4e86SAM2\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u591a\u53d1\u6027\u606f\u8089\u5206\u5272\uff08VPS\uff09\u65b9\u6cd5\u5728\u65f6\u7a7a\u5efa\u6a21\u548c\u57df\u6cdb\u5316\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u7279\u522b\u662f\u5728\u7ed3\u80a0\u955c\u89c6\u9891\u7684\u957f\u671f\u606f\u8089\u8ddf\u8e2a\u4e2d\uff0cSAM2\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u8bef\u5dee\u7d2f\u79ef\uff0c\u5bfc\u81f4\u5206\u5272\u4e0d\u7a33\u5b9a\u3002", "method": "\u6587\u7ae0\u5c06VPS\u4efb\u52a1\u91cd\u65b0\u6784\u5efa\u4e3a\u4e00\u79cd\u201c\u8ddf\u8e2a-\u68c0\u6d4b\u201d\u8303\u5f0f\uff0c\u5229\u7528IPS\u6a21\u578b\u6355\u6349\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u5e76\u6574\u5408SAM2\u6a21\u578b\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3SAM2\u5728\u957f\u671f\u8ddf\u8e2a\u4e2d\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\uff1a1. \u5185\u90e8\u5173\u8054\u8fc7\u6ee4\u6a21\u5757\uff1a\u6d88\u9664\u68c0\u6d4b\u9636\u6bb5\u4ea7\u751f\u7684\u7a7a\u95f4\u4e0d\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5047\u9633\u6027\u30022. \u5916\u90e8\u5173\u8054\u7ec6\u5316\u6a21\u5757\uff1a\u81ea\u9002\u5e94\u5730\u66f4\u65b0\u8bb0\u5fc6\u5e93\uff0c\u9632\u6b62\u8bef\u5dee\u968f\u65f6\u95f4\u4f20\u64ad\uff0c\u589e\u5f3a\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u5185\u90e8\u5173\u8054\u8fc7\u6ee4\u548c\u5916\u90e8\u5173\u8054\u7ec6\u5316\u6a21\u5757\uff0cFreeVPS\u7a33\u5b9a\u4e86SAM2\u6a21\u578b\uff0c\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u526a\u8f91\u7684\u957f\u7ed3\u80a0\u955c\u89c6\u9891\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u8ddf\u8e2a\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u53ef\u9760\u4e34\u5e8a\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "FreeVPS\u901a\u8fc7\u5176\u521b\u65b0\u7684\u201c\u8ddf\u8e2a-\u68c0\u6d4b\u201d\u8303\u5f0f\u548c\u4e24\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VPS\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u65f6\u7a7a\u5efa\u6a21\u548c\u57df\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u4e34\u5e8a\u606f\u8089\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19752", "abs": "https://arxiv.org/abs/2508.19752", "authors": ["Muhammad Moeeze Hassan", "R\u00e9gis Cottereau", "Filippo Gatti", "Patryk Dec"], "title": "Fast 3D Diffusion for Scalable Granular Media Synthesis", "comment": null, "summary": "Simulating granular media, using Discrete Element Method is a computationally\nintensive task. This is especially true during initialization phase, which\ndominates total simulation time because of large displacements involved and\nassociated kinetic energy. We overcome this bottleneck with a novel generative\npipeline based on 3D diffusion models that directly synthesizes arbitrarily\nlarge granular assemblies in their final and physically realistic\nconfigurations. The approach frames the problem as a 3D generative modeling\ntask, consisting of a two-stage pipeline. First a diffusion model is trained to\ngenerate independent 3D voxel grids representing granular media. Second, a 3D\ninpainting model, adapted from 2D inpainting techniques using masked inputs,\nstitches these grids together seamlessly, enabling synthesis of large samples\nwith physically realistic structure. The inpainting model explores several\nmasking strategies for the inputs to the underlying UNets by training the\nnetwork to infer missing portions of voxel grids from a concatenation of noised\ntensors, masks, and masked tensors as input channels. The model also adapts a\n2D repainting technique of re-injecting noise scheduler output with ground\ntruth to provide a strong guidance to the 3D model. This along with weighted\nlosses ensures long-term coherence over generation of masked regions. Both\nmodels are trained on the same binarized 3D occupancy grids extracted from\nsmall-scale DEM simulations, achieving linear scaling of computational time\nwith respect to sample size. Quantitatively, a 1.2 m long ballasted rail track\nsynthesis equivalent to a 3-hour DEM simulation, was completed under 20\nseconds. The generated voxel grids can also be post-processed to extract grain\ngeometries for DEM-compatibility as well, enabling physically coherent,\nreal-time, scalable granular media synthesis for industrial applications.", "AI": {"tldr": "\u5229\u75283D\u6269\u6563\u6a21\u578b\u548cinpainting\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u7ba1\u7ebf\uff0c\u53ef\u4ee5\u76f4\u63a5\u5408\u6210\u5927\u89c4\u6a21\u3001\u7269\u7406\u771f\u5b9e\u7684\u9897\u7c92\u4ecb\u8d28\uff0c\u89e3\u51b3\u4e86DEM\u6a21\u62df\u521d\u59cb\u5316\u9636\u6bb5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u65f6\u95f4\u4e0e\u6837\u672c\u5927\u5c0f\u7684\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u5c063\u5c0f\u65f6\u7684DEM\u6a21\u62df\u7f29\u77ed\u523020\u79d2\u3002", "motivation": "\u4f20\u7edf\u7684DEM\u6a21\u62df\u521d\u59cb\u5316\u9636\u6bb5\u8ba1\u7b97\u91cf\u5927\uff0c\u8017\u65f6\u8f83\u957f\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6a21\u62df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u751f\u6210\u7ba1\u7ebf\uff1a\u9996\u5148\uff0c\u8bad\u7ec3\u4e00\u4e2a3D\u6269\u6563\u6a21\u578b\u751f\u6210\u9897\u7c92\u4ecb\u8d28\u76843D\u4f53\u7d20\u7f51\u683c\uff1b\u7136\u540e\uff0c\u91c7\u75283D inpainting\u6a21\u578b\uff08\u57fa\u4e8e2D inpainting\u6280\u672f\uff0c\u4f7f\u7528\u63a9\u7801\u8f93\u5165\uff09\u5c06\u8fd9\u4e9b\u7f51\u683c\u65e0\u7f1d\u62fc\u63a5\u8d77\u6765\uff0c\u751f\u6210\u5927\u89c4\u6a21\u6837\u672c\u3002inpainting\u6a21\u578b\u901a\u8fc7\u591a\u79cd\u63a9\u7801\u7b56\u7565\u548c\u91cd\u7ed8\u6280\u672f\u6765\u4fdd\u8bc1\u751f\u6210\u7ed3\u679c\u7684\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8ba1\u7b97\u65f6\u95f4\u4e0e\u6837\u672c\u5927\u5c0f\u7684\u7ebf\u6027\u589e\u957f\u3002\u4e00\u4e2a\u7b49\u540c\u4e8e3\u5c0f\u65f6DEM\u6a21\u62df\u76841.2\u7c73\u957f\u8f68\u9053\u5408\u6210\uff0c\u4ec5\u572820\u79d2\u5185\u5b8c\u6210\u3002\u751f\u6210\u7684\u4f53\u7d20\u7f51\u683c\u53ef\u540e\u5904\u7406\u4e3aDEM\u517c\u5bb9\u7684\u9897\u7c92\u51e0\u4f55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9897\u7c92\u4ecb\u8d28\u5408\u6210\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e0a\u8fde\u8d2f\u7684\u5b9e\u65f6\u5927\u89c4\u6a21\u9897\u7c92\u4ecb\u8d28\u5408\u6210\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2508.19919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19919", "abs": "https://arxiv.org/abs/2508.19919", "authors": ["Jingyu Guo", "Yingying Xu"], "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems", "comment": null, "summary": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u4e92\u52a8\u4e2d\u4f1a\u4ea7\u751f\u523b\u677f\u5370\u8c61\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u9884\u8bbe\u504f\u89c1\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u5728\u591a\u4ee3\u7406\u4e92\u52a8\u4e2d\u662f\u5426\u4f1a\u81ea\u53d1\u4ea7\u751f\u523b\u677f\u5370\u8c61\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5de5\u4f5c\u573a\u6240\u4e92\u52a8\u7684\u65b0\u5b9e\u9a8c\u6846\u67b6\uff0c\u7814\u7a76\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u523b\u677f\u5370\u8c61\u7684\u51fa\u73b0\u548c\u6f14\u53d8\u3002", "result": "\uff081\uff09\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u5728\u4e92\u52a8\u4e2d\u4f1a\u4ea7\u751f\u523b\u677f\u5370\u8c61\u9a71\u52a8\u7684\u504f\u89c1\uff1b\uff082\uff09\u968f\u7740\u4e92\u52a8\u8f6e\u6b21\u548c\u51b3\u7b56\u6743\u589e\u52a0\uff0c\u4ee5\u53ca\u5f15\u5165\u5c42\u7ea7\u7ed3\u6784\u540e\uff0c\u523b\u677f\u5370\u8c61\u6548\u5e94\u4f1a\u52a0\u5267\uff1b\uff083\uff09\u8fd9\u4e9b\u7cfb\u7edf\u4f1a\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u793e\u4f1a\u884c\u4e3a\u7684\u7fa4\u4f53\u6548\u5e94\uff1b\uff084\uff09\u4e0d\u540cLLM\u67b6\u6784\u7684\u523b\u677f\u5370\u8c61\u6a21\u5f0f\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "AI\u7cfb\u7edf\u4e2d\u7684\u523b\u677f\u5370\u8c61\u5f62\u6210\u53ef\u80fd\u662f\u591a\u4ee3\u7406\u4e92\u52a8\u7684\u4e00\u79cd\u6d8c\u73b0\u5c5e\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u7684\u7ed3\u679c\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u6f5c\u5728\u673a\u5236\u5e76\u5f00\u53d1\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2508.19730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19730", "abs": "https://arxiv.org/abs/2508.19730", "authors": ["Stelios Mylonas", "Symeon Papadopoulos"], "title": "Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning", "comment": null, "summary": "The increasing realism and accessibility of deepfakes have raised critical\nconcerns about media authenticity and information integrity. Despite recent\nadvances, deepfake detection models often struggle to generalize beyond their\ntraining distributions, particularly when applied to media content found in the\nwild. In this work, we present a robust video deepfake detection framework with\nstrong generalization that takes advantage of the rich facial representations\nlearned by face foundation models. Our method is built on top of FSFM, a\nself-supervised model trained on real face data, and is further fine-tuned\nusing an ensemble of deepfake datasets spanning both face-swapping and\nface-reenactment manipulations. To enhance discriminative power, we incorporate\ntriplet loss variants during training, guiding the model to produce more\nseparable embeddings between real and fake samples. Additionally, we explore\nattribution-based supervision schemes, where deepfakes are categorized by\nmanipulation type or source dataset, to assess their impact on generalization.\nExtensive experiments across diverse evaluation benchmarks demonstrate the\neffectiveness of our approach, especially in challenging real-world scenarios.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\uff08deepfake\uff09\u5a92\u4f53\u5185\u5bb9\u65e5\u76ca\u4e25\u5cfb\u7684\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u57fa\u7840\u6a21\u578b\uff08face foundation models\uff09\u7684\u9762\u90e8\u8868\u5f81\u5b66\u4e60\u7684\u9c81\u68d2\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u81ea\u76d1\u7763\u6a21\u578bFSFM\u5728\u771f\u5b9e\u4eba\u8138\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5305\u542b\u591a\u79cd\u64cd\u7eb5\u7c7b\u578b\uff08\u5982\u4eba\u8138\u4ea4\u6362\u548c\u4eba\u8138\u91cd\u73b0\uff09\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002\u4e3a\u589e\u5f3a\u6a21\u578b\u533a\u5206\u80fd\u529b\uff0c\u5f15\u5165\u4e86\u4e09\u5143\u7ec4\u635f\u5931\uff08triplet loss\uff09\u53d8\u4f53\uff0c\u4ee5\u5b66\u4e60\u66f4\u5177\u53ef\u5206\u79bb\u6027\u7684\u771f\u5b9e\u4e0e\u4f2a\u9020\u6837\u672c\u5d4c\u5165\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u57fa\u4e8e\u5f52\u56e0\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u64cd\u7eb5\u7c7b\u578b\u6216\u6e90\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u8bc4\u4f30\u57fa\u51c6\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\uff08deepfake\uff09\u7684\u65e5\u76ca\u903c\u771f\u548c\u6613\u83b7\u53d6\u6027\u5f15\u53d1\u4e86\u5bf9\u5a92\u4f53\u771f\u5b9e\u6027\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u7684\u4e25\u91cd\u5173\u5207\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5728\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u5a92\u4f53\u5185\u5bb9\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u57fa\u7840\u6a21\u578b\uff08face foundation models\uff09\u7684\u9762\u90e8\u8868\u5f81\u5b66\u4e60\u7684\u9c81\u68d2\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u81ea\u76d1\u7763\u6a21\u578bFSFM\uff0c\u5728\u771f\u5b9e\u4eba\u8138\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff08\u6db5\u76d6\u4eba\u8138\u4ea4\u6362\u548c\u4eba\u8138\u91cd\u73b0\u64cd\u7eb5\uff09\u8fdb\u884c\u5fae\u8c03\u3002\u4e3a\u63d0\u5347\u5224\u522b\u80fd\u529b\uff0c\u5f15\u5165\u4e86\u4e09\u5143\u7ec4\u635f\u5931\uff08triplet loss\uff09\u53d8\u4f53\uff0c\u4ee5\u5b66\u4e60\u66f4\u5177\u53ef\u5206\u79bb\u6027\u7684\u771f\u5b9e\u4e0e\u4f2a\u9020\u6837\u672c\u5d4c\u5165\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u57fa\u4e8e\u5f52\u56e0\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06\u6df1\u5ea6\u4f2a\u9020\u6309\u64cd\u7eb5\u7c7b\u578b\u6216\u6e90\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5e94\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65f6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9c81\u68d2\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u9762\u90e8\u57fa\u7840\u6a21\u578b\u5b66\u4e60\u5230\u7684\u4e30\u5bcc\u9762\u90e8\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u4e09\u5143\u7ec4\u635f\u5931\u548c\u5f52\u56e0\u76d1\u7763\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19780", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19780", "abs": "https://arxiv.org/abs/2508.19780", "authors": ["Ryoma Sato"], "title": "Interestingness First Classifiers", "comment": "14 pages", "summary": "Most machine learning models are designed to maximize predictive accuracy. In\nthis work, we explore a different goal: building classifiers that are\ninteresting. An ``interesting classifier'' is one that uses unusual or\nunexpected features, even if its accuracy is lower than the best possible\nmodel. For example, predicting room congestion from CO2 levels achieves\nnear-perfect accuracy but is unsurprising. In contrast, predicting room\ncongestion from humidity is less accurate yet more nuanced and intriguing. We\nintroduce EUREKA, a simple framework that selects features according to their\nperceived interestingness. Our method leverages large language models to rank\nfeatures by their interestingness and then builds interpretable classifiers\nusing only the selected interesting features. Across several benchmark\ndatasets, EUREKA consistently identifies features that are non-obvious yet\nstill predictive. For example, in the Occupancy Detection dataset, our method\nfavors humidity over CO2 levels and light intensity, producing classifiers that\nachieve meaningful accuracy while offering insights. In the Twin Papers\ndataset, our method discovers the rule that papers with a colon in the title\nare more likely to be cited in the future. We argue that such models can\nsupport new ways of knowledge discovery and communication, especially in\nsettings where moderate accuracy is sufficient but novelty and interpretability\nare valued.", "AI": {"tldr": "EUREKA\u662f\u4e00\u4e2a\u9009\u62e9\u6709\u8da3\u7279\u5f81\u7684\u6846\u67b6\uff0c\u65e8\u5728\u6784\u5efa\u5177\u6709\u65b0\u9896\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5206\u7c7b\u5668\uff0c\u5373\u4f7f\u51c6\u786e\u6027\u6709\u6240\u964d\u4f4e\u3002", "motivation": "\u63a2\u7d22\u6784\u5efa\u5177\u6709\u65b0\u9896\u6027\u3001\u610f\u60f3\u4e0d\u5230\u7684\u7279\u5f81\u7684\u201c\u6709\u8da3\u201d\u5206\u7c7b\u5668\u7684\u76ee\u6807\uff0c\u4ee5\u652f\u6301\u77e5\u8bc6\u53d1\u73b0\u548c\u6c9f\u901a\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5f81\u8fdb\u884c\u6709\u8da3\u6027\u6392\u5e8f\uff0c\u5e76\u4ec5\u4f7f\u7528\u9009\u5b9a\u7684\u6709\u8da3\u7279\u5f81\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u5668\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cEUREKA \u6210\u529f\u8bc6\u522b\u51fa\u975e\u663e\u800c\u6613\u89c1\u4f46\u5177\u6709\u9884\u6d4b\u6027\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u5728\u5165\u4f4f\u7387\u68c0\u6d4b\u6570\u636e\u96c6\u4e2d\uff0c\u503e\u5411\u4e8e\u4f7f\u7528\u6e7f\u5ea6\u800c\u975e CO2 \u6c34\u5e73\uff1b\u5728\u53cc\u8bba\u6587\u6570\u636e\u96c6\u4e2d\uff0c\u53d1\u73b0\u6807\u9898\u5e26\u5192\u53f7\u7684\u8bba\u6587\u66f4\u53ef\u80fd\u88ab\u5f15\u7528\u3002", "conclusion": "EUREKA \u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u51c6\u786e\u6027\u9002\u4e2d\u4f46\u91cd\u89c6\u65b0\u9896\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u573a\u666f\u4e2d\uff0c\u652f\u6301\u65b0\u7684\u77e5\u8bc6\u53d1\u73b0\u548c\u6c9f\u901a\u65b9\u5f0f\u3002"}}
{"id": "2508.19922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19922", "abs": "https://arxiv.org/abs/2508.19922", "authors": ["Yifu Huo", "Chenglong Wang", "Qiren Zhu", "Shunjie Xing", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jinbo Zhu"], "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.", "AI": {"tldr": "\u8bc4\u4f30LLM\u5bf9\u9f50\u7684\u65b9\u6cd5\u9700\u8981\u8003\u8651\u6240\u6709\u6f5c\u5728\u8f93\u51fa\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u4e2a\u54cd\u5e94\u3002HEAL\u6846\u67b6\u5c06\u504f\u597d\u5bf9\u9f50\u5236\u5b9a\u4e3a\u5047\u8bbe\u7a7a\u95f4\u5185\u7684\u91cd\u65b0\u6392\u5e8f\u8fc7\u7a0b\uff0c\u5305\u542b\u6392\u540d\u51c6\u786e\u6027\u548c\u504f\u597d\u5f3a\u5ea6\u76f8\u5173\u6027\u4e24\u4e2a\u6307\u6807\u3002UniHypoBench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5047\u8bbe\u57fa\u51c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u4ee3\u7406\u6a21\u578b\u7684\u504f\u597d\u5e76\u6291\u5236\u8d1f\u6837\u672c\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5047\u8bbe\u7a7a\u95f4\u5206\u6790\u4e3a\u504f\u597d\u5bf9\u9f50\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u4e3a\u4f18\u5316\u5bf9\u9f50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982DPO\uff09\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5176\u4ed6\u6f5c\u5728\u8f93\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEAL\uff08Hypothesis-based Preference-aware Analysis Framework\uff09\u7684\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u504f\u597d\u5bf9\u9f50\u89c6\u4e3a\u5047\u8bbe\u7a7a\u95f4\u5185\u7684\u91cd\u65b0\u6392\u5e8f\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u6392\u540d\u51c6\u786e\u6027\u548c\u504f\u597d\u5f3a\u5ea6\u76f8\u5173\u6027\u4e24\u4e2a\u6307\u6807\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aUniHypoBench\u7684\u7edf\u4e00\u5047\u8bbe\u57fa\u51c6\u3002", "result": "\u901a\u8fc7HEAL\u6846\u67b6\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7684\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u7684\u504f\u597d\uff0c\u540c\u65f6\u6291\u5236\u8d1f\u6837\u672c\u3002", "conclusion": "HEAL\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u4f18\u5316LLM\u7684\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u5b9e\u7528\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u53d1\u66f4\u5168\u9762\u504f\u597d\u6355\u6349\u7b97\u6cd5\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2508.19742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19742", "abs": "https://arxiv.org/abs/2508.19742", "authors": ["Chenguang Liu", "Chisheng Wang", "Yuhua Cai", "Chuanhua Zhu", "Qingquan Li"], "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection", "comment": null, "summary": "Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOEv2\u7684\u65b0\u578b\u7ebf\u6bb5\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u540c\u65f6\u7528\u4e8e\u901a\u7528\u7ebf\u6bb5\u68c0\u6d4b\u548c\u7ebf\u67b6\u7ebf\u6bb5\u68c0\u6d4b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ebf\u6bb5\u68c0\u6d4b\u5668\u5206\u4e3a\u901a\u7528\u7ebf\u6bb5\u68c0\u6d4b\u5668\u548c\u7ebf\u67b6\u7ebf\u6bb5\u68c0\u6d4b\u5668\uff0c\u5b83\u4eec\u5404\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOEv2\u7684\u6539\u8fdb\u578b\u50cf\u7d20\u65b9\u5411\u4f30\u8ba1\uff08POE\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u8fb9\u7f18\u5f3a\u5ea6\u56fe\u68c0\u6d4b\u7ebf\u6bb5\uff0c\u5e76\u53ef\u4e0e\u4efb\u4f55\u8fb9\u7f18\u68c0\u6d4b\u5668\u7ed3\u5408\u3002", "result": "\u5c06POEv2\u4e0e\u9ad8\u6548\u7684\u8fb9\u7f18\u68c0\u6d4b\u5668\u7ed3\u5408\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "POEv2\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u7528\u4e8e\u901a\u7528\u7ebf\u6bb5\u68c0\u6d4b\u548c\u7ebf\u67b6\u7ebf\u6bb5\u68c0\u6d4b\uff0c\u5e76\u80fd\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19839", "abs": "https://arxiv.org/abs/2508.19839", "authors": ["Kehao Zhang", "Shaolei Zhang", "Yang Feng"], "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization", "comment": null, "summary": "Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.", "AI": {"tldr": "PSO-Merging\u662f\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u65b0\u578b\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u7684\u4f18\u52bf\u6765\u6784\u5efa\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u65e0\u5173\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPSO-Merging\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u5408\u5e76\u65b9\u6cd5\uff0c\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6a21\u578b\u5408\u5e76\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u65e0\u5173\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u7684\u6307\u5bfc\u800c\u5b58\u5728\u6027\u80fd\u5c40\u9650\u3002\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5219\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\uff08\u68af\u5ea6\u4e0b\u964d\uff09\u6216\u6548\u679c\u4e0d\u4f73\uff08\u68af\u5ea6\u65e0\u5173\u65b9\u6cd5\uff09\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5408\u5e76\u5927\u578b\u6a21\u578b\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u4f18\u7684\u5408\u5e76\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSO-Merging\u7684\u65b0\u578b\u6570\u636e\u9a71\u52a8\u5408\u5e76\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4e13\u5bb6\u6a21\u578b\u548c\u7a00\u758f\u5316\u4e13\u5bb6\u6a21\u578b\u4f5c\u4e3a\u7c92\u5b50\u7fa4\u7684\u521d\u59cb\u79cd\u7fa4\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff0c\u6700\u7ec8\u9009\u53d6\u5168\u5c40\u6700\u4f18\u7c92\u5b50\u4f5c\u4e3a\u5408\u5e76\u540e\u7684\u6a21\u578b\u3002", "result": "\u5728\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPSO-Merging \u65b9\u6cd5\u7684\u6027\u80fd\u666e\u904d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u5408\u5e76\u65b9\u6cd5\u3002", "conclusion": "PSO-Merging \u662f\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u65b0\u578b\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u5730\u5b9e\u73b0\u6a21\u578b\u5408\u5e76\uff0c\u5e76\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19966", "abs": "https://arxiv.org/abs/2508.19966", "authors": ["Slimane Bellaouar", "Attia Nehar", "Soumia Souffi", "Mounia Bouameur"], "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation", "comment": "25 pages, 7 figures", "summary": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684\u65b0\u7684\u4e3b\u89c2\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaAraDhati+\u6570\u636e\u96c6\u5e76\u5fae\u8c03XLM-RoBERTa\u3001AraBERT\u548cArabianGPT\u7b49\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u96c6\u6210\u51b3\u7b56\u65b9\u6cd5\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8697.79%\u7684\u51c6\u786e\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u963f\u62c9\u4f2f\u8bed\u5904\u7406\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u4f5c\u4e3a\u4e00\u79cd\u8bed\u8a00\u4e30\u5bcc\u4f46\u5f62\u6001\u590d\u6742\u7684\u8bed\u8a00\uff0c\u9762\u4e34\u7740\u8d44\u6e90\u532e\u4e4f\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7528\u4e8e\u4e3b\u89c2\u6027\u5206\u6790\u7684\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u51c6\u786e\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "\u901a\u8fc7\u6574\u5408\u73b0\u6709\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\uff08ASTD\u3001LABR\u3001HARD\u548cSANAD\uff09\u521b\u5efa\u4e86AraDhati+\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86XLM-RoBERTa\u3001AraBERT\u548cArabianGPT\u7b49\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e86\u96c6\u6210\u51b3\u7b56\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e8697.79%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e94\u5bf9\u963f\u62c9\u4f2f\u8bed\u5904\u7406\u4e2d\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002"}}
{"id": "2508.19746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19746", "abs": "https://arxiv.org/abs/2508.19746", "authors": ["Qiyao Xu", "Qiming Wu", "Xiaowei Li"], "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection", "comment": null, "summary": "Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.", "AI": {"tldr": "SPLF-SAM\u662f\u4e00\u4e2a\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u5149\u573a\u663e\u7740\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5b83\u5305\u542bUMFEB\u548cMAFA\uff0c\u53ef\u4ee5\u8bc6\u522b\u4e0d\u540c\u5927\u5c0f\u7684\u5bf9\u8c61\u5e76\u8fc7\u6ee4\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5149\u573a\u663e\u7740\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5ffd\u7565\u4e86\u63d0\u793a\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u9891\u57df\u4fe1\u606f\u5206\u6790\uff0c\u5bfc\u81f4\u5c0f\u7269\u4f53\u88ab\u566a\u58f0\u6df9\u6ca1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPLF-SAM\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u914d\u5907\u4e86\u7edf\u4e00\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u5757\uff08UMFEB\uff09\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u6ee4\u6ce2\u9002\u914d\u5668\uff08MAFA\uff09\u3002UMFEB\u80fd\u591f\u8bc6\u522b\u4e0d\u540c\u5927\u5c0f\u7684\u591a\u4e2a\u5bf9\u8c61\uff0c\u800cMAFA\u901a\u8fc7\u5b66\u4e60\u9891\u57df\u7279\u5f81\uff0c\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u5c0f\u7269\u4f53\u88ab\u566a\u58f0\u6df9\u6ca1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSPLF-SAM\u5728\u5149\u573a\u663e\u7740\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u5341\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SPLF-SAM\u5728\u5149\u573a\u663e\u7740\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5927\u5c0f\u7684\u5bf9\u8c61\u5e76\u6709\u6548\u8fc7\u6ee4\u566a\u58f0\u3002"}}
{"id": "2508.19842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19842", "abs": "https://arxiv.org/abs/2508.19842", "authors": ["S\u00fcleyman Y\u0131ld\u0131z", "Konrad Janik", "Peter Benner"], "title": "Symplectic convolutional neural networks", "comment": null, "summary": "We propose a new symplectic convolutional neural network (CNN) architecture\nby leveraging symplectic neural networks, proper symplectic decomposition, and\ntensor techniques. Specifically, we first introduce a mathematically equivalent\nform of the convolution layer and then, using symplectic neural networks, we\ndemonstrate a way to parameterize the layers of the CNN to ensure that the\nconvolution layer remains symplectic. To construct a complete autoencoder, we\nintroduce a symplectic pooling layer. We demonstrate the performance of the\nproposed neural network on three examples: the wave equation, the nonlinear\nSchr\\\"odinger (NLS) equation, and the sine-Gordon equation. The numerical\nresults indicate that the symplectic CNN outperforms the linear symplectic\nautoencoder obtained via proper symplectic decomposition.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f9b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u4e86\u8f9b\u795e\u7ecf\u7f51\u7edc\u3001\u9002\u5f53\u7684\u8f9b\u5206\u89e3\u548c\u5f20\u91cf\u6280\u672f\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u65b0\u7684CNN\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u8f9b\u795e\u7ecf\u7f51\u7edc\u3001\u9002\u5f53\u7684\u8f9b\u5206\u89e3\u548c\u5f20\u91cf\u6280\u672f\uff0c\u786e\u4fdd\u5377\u79ef\u5c42\u4fdd\u6301\u8f9b\u6027\u3002", "method": "\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5377\u79ef\u5c42\u7684\u6570\u5b66\u7b49\u4ef7\u5f62\u5f0f\uff0c\u7136\u540e\u5229\u7528\u8f9b\u795e\u7ecf\u7f51\u7edc\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u79cd\u53c2\u6570\u5316CNN\u5c42\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u5377\u79ef\u5c42\u4fdd\u6301\u8f9b\u6027\u3002\u4e3a\u4e86\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u7f16\u7801\u5668\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u8f9b\u6c60\u5316\u5c42\u3002", "result": "\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u4e09\u4e2a\u793a\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6027\u80fd\u6f14\u793a\uff1a\u6ce2\u52a8\u65b9\u7a0b\u3001\u975e\u7ebf\u6027\u859b\u5b9a\u8c14\uff08NLS\uff09\u65b9\u7a0b\u548c\u6b63\u5f26-\u6208\u767b\u65b9\u7a0b\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8f9bCNN\u7684\u6027\u80fd\u4f18\u4e8e\u901a\u8fc7\u9002\u5f53\u7684\u8f9b\u5206\u89e3\u83b7\u5f97\u7684\u7ebf\u6027\u8f9b\u81ea\u7f16\u7801\u5668\u3002", "conclusion": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8f9bCNN\u7684\u6027\u80fd\u4f18\u4e8e\u901a\u8fc7\u9002\u5f53\u7684\u8f9b\u5206\u89e3\u83b7\u5f97\u7684\u7ebf\u6027\u8f9b\u81ea\u7f16\u7801\u5668\u3002"}}
{"id": "2508.19982", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19982", "abs": "https://arxiv.org/abs/2508.19982", "authors": ["Pengxiang Li", "Yefan Zhou", "Dilxat Muhtar", "Lu Yin", "Shilin Yan", "Li Shen", "Yi Liang", "Soroush Vosoughi", "Shiwei Liu"], "title": "Diffusion Language Models Know the Answer Before Decoding", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.", "AI": {"tldr": "Prophet\u662f\u4e00\u79cd\u7528\u4e8e\u52a0\u901f\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u63a8\u7406\u7684\u8bad\u7ec3\u65e0\u5173\u7684\u5feb\u901f\u89e3\u7801\u8303\u4f8b\uff0c\u901a\u8fc7\u5229\u7528DLM\u65e9\u671f\u7b54\u6848\u6536\u655b\u7684\u7279\u6027\uff0c\u5229\u7528\u9884\u6d4b\u5019\u9009\u8005\u4e4b\u95f4\u7684\u7f6e\u4fe1\u5ea6\u95f4\u9699\u6765\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u63d0\u524d\u63d0\u4ea4\u89e3\u7801\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\uff0c\u5c06\u89e3\u7801\u6b65\u6570\u6700\u591a\u51cf\u5c113.4\u500d\u3002", "motivation": "DLM\u63a8\u7406\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u6162\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u548c\u6240\u9700\u7684\u5927\u91cf\u7ec6\u5316\u6b65\u9aa4\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3DLM\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "Prophet\u5229\u7528DLM\u65e9\u671f\u7b54\u6848\u6536\u655b\u7684\u7279\u6027\uff0c\u5373\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u6b63\u786e\u7684\u7b54\u6848\u53ef\u4ee5\u5728\u63a5\u8fd1\u6700\u7ec8\u89e3\u7801\u6b65\u9aa4\u65f6\u88ab\u8bc6\u522b\u51fa\u6765\u3002\u5b83\u901a\u8fc7\u5728\u65e9\u671f\u9636\u6bb5\u52a8\u6001\u51b3\u5b9a\u662f\u7ee7\u7eed\u7ec6\u5316\u8fd8\u662f\u201c\u5168\u529b\u4ee5\u8d74\u201d\uff08\u5373\u4e00\u6b65\u89e3\u7801\u6240\u6709\u5269\u4f59\u4ee4\u724c\uff09\u6765\u5de5\u4f5c\u3002\u8fd9\u4e00\u51b3\u7b56\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u7684\u201c\u7f6e\u4fe1\u5ea6\u95f4\u9699\u201d\uff0c\u5373\u6700\u9ad8\u4e24\u4e2a\u9884\u6d4b\u5019\u9009\u8005\u4e4b\u95f4\u7684\u6982\u7387\u5dee\u8ddd\u3002Prophet\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684DLM\u5b9e\u73b0\u4e2d\uff0c\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "result": "\u5728GSM8K\u548cMMLU\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u6709\u9ad8\u8fbe97%\u548c99%\u7684\u5b9e\u4f8b\u53ef\u4ee5\u5728\u4ec5\u4f7f\u7528\u4e00\u534a\u7684\u7ec6\u5316\u6b65\u9aa4\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u89e3\u7801\u3002Prophet\u5c06LLaDA-8B\u548cDream-7B\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u89e3\u7801\u6b65\u6570\u6700\u591a\u51cf\u5c11\u4e863.4\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002", "conclusion": "\u65e9\u671f\u89e3\u7801\u6536\u655b\u4e3a\u52a0\u901fDLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u52a0\u901f\u6280\u672f\uff08\u5982\u534a\u81ea\u56de\u5f52\u548c\u968f\u673a\u91cd\u906e\u853d\u8ba1\u5212\uff09\u7684\u8865\u5145\u3002Prophet\u901a\u8fc7\u5229\u7528\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u95f4\u9699\u6765\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u91c7\u6837\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2508.19754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19754", "abs": "https://arxiv.org/abs/2508.19754", "authors": ["Yue Wu", "Yufan Wu", "Wen Li", "Yuxi Lu", "Kairui Feng", "Xuanhong Chen"], "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers", "comment": null, "summary": "Despite significant progress in 3D avatar reconstruction, it still faces\nchallenges such as high time complexity, sensitivity to data quality, and low\ndata utilization. We propose FastAvatar, a feedforward 3D avatar framework\ncapable of flexibly leveraging diverse daily recordings (e.g., a single image,\nmulti-view observations, or monocular video) to reconstruct a high-quality 3D\nGaussian Splatting (3DGS) model within seconds, using only a single unified\nmodel. FastAvatar's core is a Large Gaussian Reconstruction Transformer\nfeaturing three key designs: First, a variant VGGT-style transformer\narchitecture aggregating multi-frame cues while injecting initial 3D prompt to\npredict an aggregatable canonical 3DGS representation; Second, multi-granular\nguidance encoding (camera pose, FLAME expression, head pose) mitigating\nanimation-induced misalignment for variable-length inputs; Third, incremental\nGaussian aggregation via landmark tracking and sliced fusion losses.\nIntegrating these features, FastAvatar enables incremental reconstruction,\ni.e., improving quality with more observations, unlike prior work wasting input\ndata. This yields a quality-speed-tunable paradigm for highly usable avatar\nmodeling. Extensive experiments show that FastAvatar has higher quality and\nhighly competitive speed compared to existing methods.", "AI": {"tldr": "FastAvatar\u662f\u4e00\u4e2a\u521b\u65b0\u76843D\u5316\u8eab\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u4ece\u5355\u5f20\u56fe\u50cf\u3001\u591a\u89c6\u89d2\u6216\u5355\u76ee\u89c6\u9891\u7b49\u591a\u79cd\u6570\u636e\u4e2d\uff0c\u5728\u51e0\u79d2\u949f\u5185\u5229\u7528\u5355\u4e2a\u7edf\u4e00\u6a21\u578b\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6a21\u578b\u3002", "motivation": "\u73b0\u67093D\u5316\u8eab\u91cd\u5efa\u6280\u672f\u5b58\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u9ad8\u3001\u5bf9\u6570\u636e\u8d28\u91cf\u654f\u611f\u3001\u6570\u636e\u5229\u7528\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "FastAvatar\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u5927\u578b\u9ad8\u65af\u91cd\u5efaTransformer\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1. VGGT\u98ce\u683c\u7684Transformer\u67b6\u6784\uff0c\u805a\u5408\u591a\u5e27\u7ebf\u7d22\u5e76\u6ce8\u5165\u521d\u59cb3D\u63d0\u793a\uff0c\u4ee5\u9884\u6d4b\u53ef\u805a\u5408\u7684\u89c4\u83033DGS\u8868\u793a\uff1b2. \u591a\u7c92\u5ea6\u5f15\u5bfc\u7f16\u7801\uff08\u76f8\u673a\u59ff\u6001\u3001FLAME\u8868\u60c5\u3001\u5934\u90e8\u59ff\u6001\uff09\uff0c\u51cf\u8f7b\u52a8\u753b\u5f15\u8d77\u7684\u9519\u4f4d\u95ee\u9898\uff1b3. \u901a\u8fc7\u5730\u6807\u8ddf\u8e2a\u548c\u5207\u7247\u878d\u5408\u635f\u5931\u8fdb\u884c\u589e\u91cf\u9ad8\u65af\u805a\u5408\uff0c\u5b9e\u73b0\u589e\u91cf\u91cd\u5efa\uff0c\u80fd\u968f\u7740\u66f4\u591a\u89c2\u6d4b\u6570\u636e\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastAvatar\u7684\u8d28\u91cf\u66f4\u9ad8\uff0c\u901f\u5ea6\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "FastAvatar\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf-\u901f\u5ea6\u53ef\u8c03\u7684\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u6613\u7528\u7684\u5316\u8eab\u5efa\u6a21\uff0c\u5e76\u80fd\u6709\u6548\u5229\u7528\u5404\u79cd\u65e5\u5e38\u5f55\u5236\u6570\u636e\u3002"}}
{"id": "2508.19847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19847", "abs": "https://arxiv.org/abs/2508.19847", "authors": ["Erdi Kara", "Panos Stinis"], "title": "Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources", "comment": null, "summary": "We present a hybrid framework that couples finite element methods (FEM) with\nphysics-informed DeepONet to model fluid transport in porous media from sharp,\nlocalized Gaussian sources. The governing system consists of a steady-state\nDarcy flow equation and a time-dependent convection-diffusion equation. Our\napproach solves the Darcy system using FEM and transfers the resulting velocity\nfield to a physics-informed DeepONet, which learns the mapping from source\nfunctions to solute concentration profiles. This modular strategy preserves\nFEM-level accuracy in the flow field while enabling fast inference for\ntransport dynamics. To handle steep gradients induced by sharp sources, we\nintroduce an adaptive sampling strategy for trunk collocation points. Numerical\nexperiments demonstrate that our method is in good agreement with the reference\nsolutions while offering orders of magnitude speedups over traditional solvers,\nmaking it suitable for practical applications in relevant scenarios.\nImplementation of our proposed method is available at\nhttps://github.com/erkara/fem-pi-deeponet.", "AI": {"tldr": "We developed a hybrid FEM-DeepONet framework to model fluid transport in porous media with localized sources. It achieves high accuracy and significant speedups compared to traditional methods.", "motivation": "To efficiently model fluid transport in porous media from sharp, localized Gaussian sources, addressing the need for speed and accuracy.", "method": "A hybrid framework coupling FEM for Darcy flow and a physics-informed DeepONet for convection-diffusion. FEM provides the velocity field, which is fed into DeepONet that learns the mapping from source functions to concentration profiles. An adaptive sampling strategy is used for trunk collocation points to handle steep gradients.", "result": "The method shows good agreement with reference solutions and offers orders of magnitude speedups, making it suitable for practical applications.", "conclusion": "The proposed hybrid FEM-DeepONet framework provides an accurate and efficient approach for modeling fluid transport in porous media with localized sources, outperforming traditional solvers in terms of speed."}}
{"id": "2508.19988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19988", "abs": "https://arxiv.org/abs/2508.19988", "authors": ["Lisa Alazraki", "Lihu Chen", "Ana Brassard", "Joe Stacey", "Hossein A. Rahmani", "Marek Rei"], "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios", "comment": null, "summary": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement.", "AI": {"tldr": "LLMs\u5728\u7ed3\u5408\u5e38\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u7ea630%\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u826f\u597d\u3002\u5f15\u5165AgentCoMa\u57fa\u51c6\u6d4b\u8bd5\u6b64\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4ee5\u4e86\u89e3\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4fa7\u91cd\u4e8e\u5e38\u8bc6\u6216\u6570\u5b66\u63a8\u7406\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u9700\u8981\u4e24\u8005\u7684\u7ed3\u5408\u3002LLMs\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6a21\u578b\u8106\u5f31\u6027\u3002", "method": "\u5f15\u5165AgentCoMa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5e38\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u6b65\u9aa4\u3002\u572861\u4e2a\u4e0d\u540cLLM\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff08\u795e\u7ecf\u5143\u6a21\u5f0f\u3001\u6ce8\u610f\u529b\u56fe\u3001\u6210\u5458\u63a8\u7406\uff09\u3002", "result": "LLMs\u5355\u72ec\u89e3\u51b3\u5e38\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u6b65\u9aa4\u7684\u80fd\u529b\u5c1a\u53ef\uff0c\u4f46\u7ed3\u5408\u4e24\u8005\u65f6\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d\u7ea630%\u3002\u6b64\u6027\u80fd\u5dee\u8ddd\u5927\u4e8e\u4ec5\u5305\u542b\u5355\u4e00\u7c7b\u578b\u63a8\u7406\u6b65\u9aa4\u7684\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u3002\u4eba\u7c7b\u6ce8\u91ca\u8005\u5728AgentCoMa\u4e0a\u7684\u8868\u73b0\u4e0eLLMs\u76f8\u5f53\u3002", "conclusion": "LLMs\u5728\u6df7\u5408\u7c7b\u578b\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6a21\u578b\u8106\u5f31\u6027\u3002AgentCoMa\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.19762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19762", "abs": "https://arxiv.org/abs/2508.19762", "authors": ["Ahmed Emam", "Mohamed Elbassiouny", "Julius Miller", "Patrick Donworth", "Sabine Seidel", "Ribana Roscher"], "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions", "comment": null, "summary": "Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nincreasing anthropogenic and environmental stressors. To support scalable,\nautomated pollinator monitoring, we introduce BuzzSet, a new large-scale\ndataset of high-resolution pollinator images collected in real agricultural\nfield conditions. BuzzSet contains 7856 manually verified and labeled images,\nwith over 8000 annotated instances across three classes: honeybees, bumblebees,\nand unidentified insects. Initial annotations were generated using a YOLOv12\nmodel trained on external data and refined via human verification using\nopen-source labeling tools. All images were preprocessed into 256~$\\times$~256\ntiles to improve the detection of small insects. We provide strong baselines\nusing the RF-DETR transformer-based object detector. The model achieves high\nF1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,\nwith confusion matrix results showing minimal misclassification between these\ncategories. The unidentified class remains more challenging due to label\nambiguity and lower sample frequency, yet still contributes useful insights for\nrobustness evaluation. Overall detection quality is strong, with a best\nmAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object\ndetection, class separation under label noise, and ecological computer vision.", "AI": {"tldr": "BuzzSet\u662f\u4e00\u4e2a\u5305\u542b7856\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u76d1\u6d4b\u871c\u8702\u548c\u718a\u8702\u7b49\u6388\u7c89\u6606\u866b\u3002\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6388\u7c89\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5df2\u5b9e\u73b0\u9ad8F1\u5206\u6570\uff08\u5206\u522b\u4e3a0.94\u548c0.92\uff09\u548c\u826f\u597d\u7684\u68c0\u6d4b\u8d28\u91cf\uff08mAP@0.50\u4e3a0.559\uff09\u3002", "motivation": "\u7531\u4e8e\u4f20\u7c89\u6606\u866b\u6570\u91cf\u7684\u4e0b\u964d\uff0c\u4e3a\u4e86\u652f\u6301\u53ef\u6269\u5c55\u7684\u3001\u81ea\u52a8\u5316\u7684\u4f20\u7c89\u6606\u866b\u76d1\u6d4b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6BuzzSet\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5728\u771f\u5b9e\u7684\u519c\u4e1a\u7530\u91ce\u6761\u4ef6\u4e0b\u6536\u96c6\u7684\u9ad8\u5206\u8fa8\u7387\u4f20\u7c89\u6606\u866b\u56fe\u50cf\u3002", "method": "BuzzSet\u5305\u542b7856\u5f20\u7ecf\u8fc7\u624b\u52a8\u9a8c\u8bc1\u548c\u6807\u8bb0\u7684\u56fe\u50cf\uff0c\u5305\u542b\u4e09\u4e2a\u7c7b\u522b\uff1a\u871c\u8702\u3001\u718a\u8702\u548c\u672a\u8bc6\u522b\u7684\u6606\u866b\uff0c\u5176\u4e2d\u67098000\u591a\u4e2a\u5b9e\u4f8b\u3002\u521d\u59cb\u6807\u6ce8\u4f7f\u7528\u5728\u5916\u90e8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684YOLOv12\u6a21\u578b\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u5f00\u6e90\u6807\u6ce8\u5de5\u5177\u7684\u4eba\u5de5\u9a8c\u8bc1\u8fdb\u884c\u4f18\u5316\u3002\u6240\u6709\u56fe\u50cf\u90fd\u9884\u5904\u7406\u6210256x256\u7684\u56fe\u5757\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u5c0f\u578b\u6606\u866b\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u6211\u4eec\u4f7f\u7528\u57fa\u4e8eRF-DETR Transformer\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "result": "\u6240\u63d0\u51fa\u7684RF-DETR\u6a21\u578b\u5728\u871c\u8702\u548c\u718a\u8702\u7c7b\u522b\u4e0a\u5206\u522b\u8fbe\u5230\u4e860.94\u548c0.92\u7684\u9ad8F1\u5206\u6570\uff0c\u6df7\u6dc6\u77e9\u9635\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u7c7b\u522b\u4e4b\u95f4\u7684\u8bef\u5206\u7c7b\u6781\u5c11\u3002\u672a\u8bc6\u522b\u7c7b\u522b\u7531\u4e8e\u6807\u7b7e\u6b67\u4e49\u548c\u8f83\u4f4e\u7684\u6837\u672c\u9891\u7387\u800c\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f46\u4ecd\u4e3a\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u89c1\u89e3\u3002\u603b\u4f53\u68c0\u6d4b\u8d28\u91cf\u826f\u597d\uff0c\u6700\u4f73mAP@0.50\u4e3a0.559\u3002", "conclusion": "BuzzSet\u4e3a\u5c0f\u578b\u7269\u4f53\u68c0\u6d4b\u3001\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u7c7b\u522b\u5206\u79bb\u4ee5\u53ca\u751f\u6001\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.19993", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19993", "abs": "https://arxiv.org/abs/2508.19993", "authors": ["Debanjana Kar", "Leopold B\u00f6ss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.", "AI": {"tldr": "MathBuddy\u662f\u4e00\u4e2a\u80fd\u591f\u611f\u77e5\u5b66\u751f\u60c5\u7eea\u7684LLM\u6570\u5b66\u5bfc\u5e08\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u8bdd\u6587\u672c\u548c\u9762\u90e8\u8868\u60c5\u6765\u5efa\u6a21\u5b66\u751f\u60c5\u7eea\uff0c\u5e76\u91c7\u7528\u76f8\u5e94\u7684\u6559\u5b66\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6559\u80b2\u6280\u672f\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f1a\u8bdd\u7cfb\u7edf\u672a\u80fd\u8003\u8651\u5b66\u751f\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u800c\u60c5\u7eea\u72b6\u6001\u4f1a\u5f71\u54cd\u5b66\u4e60\u80fd\u529b\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u611f\u77e5\u5b66\u751f\u60c5\u7eea\u5e76\u505a\u51fa\u76f8\u5e94\u6559\u5b66\u8c03\u6574\u7684\u6559\u80b2\u5de5\u5177\u3002", "method": "MathBuddy\u901a\u8fc7\u5206\u6790\u5b66\u751f\u7684\u5bf9\u8bdd\u6587\u672c\u548c\u9762\u90e8\u8868\u60c5\u6765\u6355\u6349\u548c\u6574\u5408\u60c5\u7eea\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u60c5\u7eea\u4fe1\u606f\u63d0\u793aLLM\u751f\u6210\u5177\u6709\u60c5\u611f\u610f\u8bc6\u7684\u56de\u590d\uff0c\u5e76\u6839\u636e\u5b66\u751f\u60c5\u7eea\u91c7\u7528\u4e0d\u540c\u7684\u6559\u5b66\u7b56\u7565\u3002", "result": "\u5728\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0cMathBuddy\u5728\u516b\u4e2a\u6559\u5b66\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u83b7\u80dc\u7387\u6307\u6807\u5e26\u6765\u4e8623\u4e2a\u767e\u5206\u70b9\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u4f7f\u7528DAMR\u5206\u6570\u8bc4\u4f30\u7684\u6574\u4f53\u6c34\u5e73\u4e0a\u5e26\u6765\u4e863\u4e2a\u767e\u5206\u70b9\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u5b66\u751f\u60c5\u7eea\u5efa\u6a21\u53ef\u4ee5\u63d0\u9ad8LLM\u5bfc\u5e08\u7684\u6559\u5b66\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5b66\u751f\u60c5\u7eea\u8fdb\u884c\u5efa\u6a21\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u4e8eLLM\u7684\u5bfc\u5e08\u7684\u6559\u5b66\u80fd\u529b\uff0c\u4f7f\u5bf9\u8bdd\u66f4\u5177\u540c\u7406\u5fc3\uff0c\u5e76\u5e26\u6765\u66f4\u597d\u7684\u5b66\u4e60\u6210\u679c\u3002"}}
{"id": "2508.19769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19769", "abs": "https://arxiv.org/abs/2508.19769", "authors": ["Shu Shen", "C. L. Philip Chen", "Tong Zhang"], "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "comment": "13pages,7 figures", "summary": "Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIM\uff08Adaptive Intra-Network Modulation\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002AIM\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5185\u90e8\u7684\u4f18\u5316\u504f\u5dee\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e0d\u635f\u5bb3\u4e3b\u5bfc\u6a21\u6001\u548c\u5f31\u52bf\u6a21\u6001\u5b66\u4e60\u7684\u524d\u63d0\u4e0b\uff0c\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u3002AIM\u5c06\u4e3b\u5bfc\u6a21\u6001\u4e2d\u4f18\u5316\u4e0d\u8db3\u7684\u53c2\u6570\u5206\u79bb\u5230\u8f85\u52a9\u6a21\u5757\u4e2d\uff0c\u5e76\u4e0e\u5f31\u52bf\u6a21\u6001\u8054\u5408\u8bad\u7ec3\uff0c\u540c\u65f6\u6839\u636e\u7f51\u7edc\u6df1\u5ea6\u81ea\u9002\u5e94\u5730\u8c03\u6574\u8c03\u5236\u5f3a\u5ea6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cAIM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4e0d\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u6291\u5236\u4e3b\u5bfc\u6a21\u6001\u6765\u4fc3\u8fdb\u5f31\u52bf\u6a21\u6001\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIM\uff08Adaptive Intra-Network Modulation\uff09\u7684\u65b0\u65b9\u6cd5\u3002AIM\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5185\u90e8\u7684\u4f18\u5316\u504f\u5dee\uff0c\u5c06\u4e3b\u5bfc\u6a21\u6001\u4e2d\u4f18\u5316\u4e0d\u8db3\u7684\u53c2\u6570\u5206\u79bb\u5230\u8f85\u52a9\u6a21\u5757\u4e2d\uff0c\u5e76\u4e0e\u5f31\u52bf\u6a21\u6001\u8054\u5408\u8bad\u7ec3\uff0c\u540c\u65f6\u6839\u636e\u7f51\u7edc\u6df1\u5ea6\u81ea\u9002\u5e94\u5730\u8c03\u6574\u8c03\u5236\u5f3a\u5ea6\u3002", "result": "AIM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u4e0d\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\u3001\u878d\u5408\u7b56\u7565\u548c\u4f18\u5316\u5668\u3002", "conclusion": "AIM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e0d\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u4f18\u5316\u504f\u5dee\u95ee\u9898\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e0d\u635f\u5bb3\u4efb\u4f55\u6a21\u6001\u5b66\u4e60\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u5e73\u8861\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19884", "abs": "https://arxiv.org/abs/2508.19884", "authors": ["Mingyue Kong", "Yinglong Zhang", "Chengda Xu", "Xuewen Xia", "Xing Xu"], "title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks", "comment": "50 pages, 6 figures", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in structured\ndata modeling tasks such as node classification. However, mainstream approaches\ngenerally rely on a large number of trainable parameters and fixed aggregation\nrules, making it difficult to adapt to graph data with strong structural\nheterogeneity and complex feature distributions. This often leads to\nover-smoothing of node representations and semantic degradation. To address\nthese issues, this paper proposes a parameter-free graph neural network\nframework based on structural diversity, namely SDGNN (Structural-Diversity\nGraph Neural Network). The framework is inspired by structural diversity theory\nand designs a unified structural-diversity message passing mechanism that\nsimultaneously captures the heterogeneity of neighborhood structures and the\nstability of feature semantics, without introducing additional trainable\nparameters. Unlike traditional parameterized methods, SDGNN does not rely on\ncomplex model training, but instead leverages complementary modeling from both\nstructure-driven and feature-driven perspectives, thereby effectively improving\nadaptability across datasets and scenarios. Experimental results show that on\neight public benchmark datasets and an interdisciplinary PubMed citation\nnetwork, SDGNN consistently outperforms mainstream GNNs under challenging\nconditions such as low supervision, class imbalance, and cross-domain transfer.\nThis work provides a new theoretical perspective and general approach for the\ndesign of parameter-free graph neural networks, and further validates the\nimportance of structural diversity as a core signal in graph representation\nlearning. To facilitate reproducibility and further research, the full\nimplementation of SDGNN has been released at:\nhttps://github.com/mingyue15694/SGDNN/tree/main", "AI": {"tldr": "SDGNN\u662f\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u591a\u6837\u6027\u7684\u65e0\u53c2\u6570\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7ed3\u6784\u591a\u6837\u6027\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u540c\u65f6\u6355\u6349\u90bb\u57df\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u7279\u5f81\u8bed\u4e49\u7a33\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGNNs\u53c2\u6570\u91cf\u5927\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4e3b\u6d41GNNs\u7684\u6027\u80fd\u3002", "motivation": "\u4e3b\u6d41\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u7ed3\u6784\u5316\u6570\u636e\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u5f3a\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u590d\u6742\u7279\u5f81\u5206\u5e03\u7684\u56fe\u6570\u636e\u65f6\uff0c\u9762\u4e34\u53c2\u6570\u91cf\u5927\u3001\u805a\u5408\u89c4\u5219\u56fa\u5b9a\u5bfc\u81f4\u7684\u8fc7\u5e73\u6ed1\u548c\u8bed\u4e49\u9000\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDGNN\uff08Structural-Diversity Graph Neural Network\uff09\u7684\u65e0\u53c2\u6570\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u7ed3\u6784\u591a\u6837\u6027\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u80fd\u591f\u540c\u65f6\u6355\u6349\u90bb\u57df\u7ed3\u6784\u7684\u5f02\u8d28\u6027\u548c\u7279\u5f81\u8bed\u4e49\u7684\u7a33\u5b9a\u6027\uff0c\u4e14\u65e0\u9700\u5f15\u5165\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002SDGNN\u4e0d\u4f9d\u8d56\u590d\u6742\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u800c\u662f\u7ed3\u5408\u4e86\u7ed3\u6784\u9a71\u52a8\u548c\u7279\u5f81\u9a71\u52a8\u7684\u4e92\u8865\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u5728\u516b\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548cPubMed\u5f15\u7528\u7f51\u7edc\u4e0a\uff0cSDGNN\u5728\u4f4e\u76d1\u7763\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u8fc1\u79fb\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4e3b\u6d41GNNs\u3002", "conclusion": "SDGNN\u4e3a\u8bbe\u8ba1\u65e0\u53c2\u6570\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7ed3\u6784\u591a\u6837\u6027\u4f5c\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u6838\u5fc3\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.19996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19996", "abs": "https://arxiv.org/abs/2508.19996", "authors": ["Yiming Du", "Yifan Xiang", "Bin Liang", "Dahua Lin", "Kam-Fai Wong", "Fei Tan"], "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning", "comment": null, "summary": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.", "AI": {"tldr": "ReSURE\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u964d\u4f4e\u4e0d\u53ef\u9760\u76d1\u7763\u7684\u6743\u91cd\u6765\u63d0\u9ad8\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u4e0b\u4e5f\u80fd\u6709\u6548\u9632\u6b62\u9519\u8bef\u4f20\u64ad\u3002", "motivation": "\u76d1\u7763\u9519\u8bef\u4f1a\u5f71\u54cd\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8f6e\u6b21\u7ea7\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "method": "ReSURE\u901a\u8fc7Welford\u5728\u7ebf\u7edf\u8ba1\u4f30\u8ba1\u6bcf\u8f6e\u635f\u5931\u5206\u5e03\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u6837\u672c\u635f\u5931\u7684\u6743\u91cd\uff0c\u65e0\u9700\u663e\u5f0f\u8fc7\u6ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReSURE\u5728\u7a33\u5b9a\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u4e0b\uff0c\u54cd\u5e94\u5206\u6570\u4e0e\u6837\u672c\u6570\u91cf\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u6027\u3002", "conclusion": "ReSURE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u6709\u671b\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u3002"}}
{"id": "2508.19773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19773", "abs": "https://arxiv.org/abs/2508.19773", "authors": ["Jakob Seitz", "Tobias Lengfeld", "Radu Timofte"], "title": "The Return of Structural Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Handwritten Mathematical Expression Recognition is foundational for\neducational technologies, enabling applications like digital note-taking and\nautomated grading. While modern encoder-decoder architectures with large\nlanguage models excel at LaTeX generation, they lack explicit symbol-to-trace\nalignment, a critical limitation for error analysis, interpretability, and\nspatially aware interactive applications requiring selective content updates.\nThis paper introduces a structural recognition approach with two innovations: 1\nan automatic annotation system that uses a neural network to map LaTeX\nequations to raw traces, automatically generating annotations for symbol\nsegmentation, classification, and spatial relations, and 2 a modular structural\nrecognition system that independently optimizes segmentation, classification,\nand relation prediction. By leveraging a dataset enriched with structural\nannotations from our auto-labeling system, the proposed recognition system\ncombines graph-based trace sorting, a hybrid convolutional-recurrent network,\nand transformer-based correction to achieve competitive performance on the\nCROHME-2023 benchmark. Crucially, our structural recognition system generates a\ncomplete graph structure that directly links handwritten traces to predicted\nsymbols, enabling transparent error analysis and interpretable outputs.", "AI": {"tldr": "\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5728LaTeX\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u9519\u8bef\u5206\u6790\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9700\u8981\u9009\u62e9\u6027\u5185\u5bb9\u66f4\u65b0\u7684\u7a7a\u95f4\u611f\u77e5\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u5316\u8bc6\u522b\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u4e2a\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5c06LaTeX\u65b9\u7a0b\u6620\u5c04\u5230\u539f\u59cb\u8f68\u8ff9\u7684\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\uff0c\u4ee5\u53ca\u4e00\u4e2a\u72ec\u7acb\u4f18\u5316\u5206\u5272\u3001\u5206\u7c7b\u548c\u5173\u7cfb\u9884\u6d4b\u7684\u6a21\u5757\u5316\u7ed3\u6784\u5316\u8bc6\u522b\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u7684\u8f68\u8ff9\u6392\u5e8f\u3001\u6df7\u5408\u5377\u79ef\u5faa\u73af\u7f51\u7edc\u548c\u57fa\u4e8eTransformer\u7684\u6821\u6b63\u3002", "result": "\u8be5\u65b9\u6cd5\u5728CROHME-2023\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u751f\u6210\u4e86\u8fde\u63a5\u624b\u5199\u8f68\u8ff9\u548c\u9884\u6d4b\u7b26\u53f7\u7684\u5b8c\u6574\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u7684\u9519\u8bef\u5206\u6790\u548c\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u8bc6\u522b\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u548c\u900f\u660e\u7684\u9519\u8bef\u5206\u6790\u3002"}}
{"id": "2508.19896", "categories": ["cs.LG", "cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19896", "abs": "https://arxiv.org/abs/2508.19896", "authors": ["Davorin Mili\u010devi\u0107", "Ratko Grbi\u0107"], "title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs", "comment": "13 pages, 4 figures. Submitted to Elsevier Neurocomputing, under\n  review", "summary": "Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.", "AI": {"tldr": "NM-Hebb\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53d7\u795e\u7ecf\u542f\u53d1\u7684\u5c40\u90e8\u53ef\u5851\u6027\u548c\u611f\u77e5\u76d1\u7763\uff0c\u4ee5\u63d0\u9ad8CNN\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6807\u51c6\u7684CNN\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u5168\u5c40\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff09\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u3001\u5197\u4f59\u6ee4\u6ce2\u5668\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u3002NM-Hebb\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u53ef\u5851\u6027\u548c\u8ddd\u79bb\u611f\u77e5\u76d1\u7763\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "NM-Hebb\u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\uff1a\u9636\u6bb51\u901a\u8fc7\u7ed3\u5408\u4ea4\u53c9\u71b5\u76ee\u6807\u3001Hebbian\u6b63\u5219\u5316\u5668\u548c\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u8c03\u8d28\u5242\u6765\u4f18\u5316\u7f51\u7edc\uff1b\u9636\u6bb52\u4f7f\u7528\u6210\u5bf9\u5ea6\u91cf\u5b66\u4e60\u635f\u5931\u5bf9\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u538b\u7f29\u7c7b\u5185\u8ddd\u79bb\u5e76\u6269\u5927\u7c7b\u95f4\u88d5\u5ea6\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\uff0cNM-Hebb\u5728\u591a\u4e2a\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u7684Top-1\u51c6\u786e\u7387\uff08CIFAR-10\u63d0\u9ad8+2.0-10.0\u767e\u5206\u70b9\uff0cCIFAR-100\u63d0\u9ad8+2.0-9.0\u767e\u5206\u70b9\uff0cTinyImageNet\u63d0\u9ad8+4.3-8.9\u767e\u5206\u70b9\uff09\uff0c\u5e76\u4e14\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\uff08NMI\uff09\u63d0\u9ad8\u4e86+0.15\u3002\u5b9a\u6027\u53ef\u89c6\u5316\u548c\u6ee4\u6ce2\u5668\u7ea7\u5206\u6790\u8868\u660e\uff0cNM-Hebb\u751f\u6210\u4e86\u66f4\u7ed3\u6784\u5316\u3001\u66f4\u5177\u9009\u62e9\u6027\u7684\u7279\u5f81\uff0c\u4ea7\u751f\u4e86\u66f4\u7d27\u5bc6\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u7c7b\u7c07\u3002", "conclusion": "\u5c06\u5c40\u90e8Hebbian\u53ef\u5851\u6027\u4e0e\u57fa\u4e8e\u5ea6\u91cf\u7684\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684CNN\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u5b89\u5168\u5173\u952e\u578bAI\u5e94\u7528\u5e26\u6765\u4e86\u5b9e\u9645\u6548\u76ca\u3002"}}
{"id": "2508.19997", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19997", "abs": "https://arxiv.org/abs/2508.19997", "authors": ["Boheng Mao"], "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "SRA\u901a\u8fc7\u4ec5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u68c0\u7d22\u548c\u589e\u5f3a\u4f4e\u9891\u6807\u7b7e\u6837\u672c\u6765\u6539\u8fdb\u957f\u5c3e\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\uff0c\u65e0\u9700\u66f4\u6539\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u5728LEDGAR\u548cUNFAIR-ToS\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u6807\u7b7e\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u7f55\u89c1\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u68c0\u7d22\u589e\u5f3a\uff08SRA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u589e\u5f3a\u8bad\u7ec3\u96c6\u4e2d\u4f4e\u9891\u6807\u7b7e\u7684\u6837\u672c\uff0c\u9632\u6b62\u5bf9\u8868\u793a\u826f\u597d\u7684\u7c7b\u522b\u5f15\u5165\u566a\u58f0\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u67b6\u6784\u3002\u68c0\u7d22\u4ec5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u8fdb\u884c\uff0c\u4ee5\u786e\u4fdd\u6ca1\u6709\u6f5c\u5728\u7684\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u65e0\u9700\u5916\u90e8\u8bed\u6599\u5e93\u3002", "result": "\u5728\u5177\u6709\u957f\u5c3e\u5206\u5e03\u7684\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6LEDGAR\uff08\u5355\u6807\u7b7e\uff09\u548cUNFAIR-ToS\uff08\u591a\u6807\u7b7e\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0cSRA\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684LexGLUE\u57fa\u7ebf\uff0c\u5728\u5faeF1\u548c\u5b8fF1\u5f97\u5206\u4e0a\u5747\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "SRA\u5728\u957f\u5c3e\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2508.19786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19786", "abs": "https://arxiv.org/abs/2508.19786", "authors": ["Han Jiao", "Jiakai Sun", "Yexing Xu", "Lei Zhao", "Wei Xing", "Huaizhong Lin"], "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction", "comment": "8 pages, 9 figures, Anonymous AAAI Submission", "summary": "3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u53d8\u5f62\u7684\u65b9\u6cd5\u6613\u5bfc\u81f4\u6a21\u7cca\u548c\u7ec6\u8282\u4e22\u5931\u3002\u672c\u6587\u63d0\u51fa\u7684MAPo\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5206\u6570\u9a71\u52a8\u7684\u5206\u5272\u7b56\u7565\uff0c\u533a\u5206\u9ad8\u52a8\u6001\u548c\u4f4e\u52a8\u6001\u9ad8\u65af\uff0c\u5e76\u5bf9\u9ad8\u52a8\u6001\u9ad8\u65af\u8fdb\u884c\u65f6\u57df\u9012\u5f52\u5206\u5272\u548c\u91cd\u590d\u53d8\u5f62\u7f51\u7edc\uff0c\u4ee5\u6355\u6349\u7cbe\u7ec6\u8fd0\u52a8\u7ec6\u8282\u3002\u540c\u65f6\uff0c\u4e3a\u89e3\u51b3\u5206\u5272\u8fb9\u754c\u7684\u89c6\u89c9\u4e0d\u8fde\u7eed\u6027\uff0c\u5f15\u5165\u4e86\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMAPo\u5728\u590d\u6742\u8fd0\u52a8\u533a\u57df\u5177\u6709\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u76f8\u5f53\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53d8\u5f62\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u6e32\u67d3\u6a21\u7cca\u548c\u4e22\u5931\u8fd0\u52a8\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u533a\u57df\uff0c\u8fd9\u662f\u7531\u4e8e\u5355\u4e00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002", "method": "MAPo\u6846\u67b6\u91c7\u7528\u52a8\u6001\u5206\u6570\u9a71\u52a8\u7684\u5206\u5272\u7b56\u7565\uff0c\u5c063D\u9ad8\u65af\u5206\u4e3a\u9ad8\u52a8\u6001\u548c\u4f4e\u52a8\u6001\u4e24\u7c7b\u3002\u9ad8\u52a8\u6001\u9ad8\u65af\u8fdb\u884c\u65f6\u57df\u9012\u5f52\u5206\u5272\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6bb5\u590d\u5236\u5176\u53d8\u5f62\u7f51\u7edc\u4ee5\u8fdb\u884c\u4e13\u95e8\u5efa\u6a21\uff1b\u4f4e\u52a8\u6001\u9ad8\u65af\u5219\u89c6\u4e3a\u9759\u6001\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u89e3\u51b3\u5206\u5272\u8fb9\u754c\u7684\u89c6\u89c9\u4e0d\u8fde\u7eed\u6027\uff0c\u5f15\u5165\u4e86\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "MAPo\u5728\u590d\u6742\u6216\u5feb\u901f\u8fd0\u52a8\u533a\u57df\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "MAPo\u6846\u67b6\u901a\u8fc7\u5176\u65b0\u9896\u7684\u52a8\u6001\u5206\u5272\u548c\u8de8\u5e27\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u8fd0\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.19900", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19900", "abs": "https://arxiv.org/abs/2508.19900", "authors": ["Tan Jing", "Xiaorui Li", "Chao Yao", "Xiaojuan Ban", "Yuetong Fang", "Renjing Xu", "Zhaolin Yuan"], "title": "Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables learning effective policies from\nfixed datasets without any environment interaction. Existing methods typically\nemploy policy constraints to mitigate the distribution shift encountered during\noffline RL training. However, because the scale of the constraints varies\nacross tasks and datasets of differing quality, existing methods must\nmeticulously tune hyperparameters to match each dataset, which is\ntime-consuming and often impractical. We propose Adaptive Scaling of Policy\nConstraints (ASPC), a second-order differentiable framework that dynamically\nbalances RL and behavior cloning (BC) during training. We theoretically analyze\nits performance improvement guarantee. In experiments on 39 datasets across\nfour D4RL domains, ASPC using a single hyperparameter configuration outperforms\nother adaptive constraint methods and state-of-the-art offline RL algorithms\nthat require per-dataset tuning while incurring only minimal computational\noverhead. The code will be released at https://github.com/Colin-Jing/ASPC.", "AI": {"tldr": "ASPC\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u7ea6\u675f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861RL\u548cBC\u6765\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4ec5\u9700\u4e00\u4e2a\u8d85\u53c2\u6570\u5373\u53ef\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebfRL\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7b56\u7565\u7ea6\u675f\u6765\u5904\u7406\u5206\u5e03\u504f\u79fb\uff0c\u4f46\u8fd9\u4e9b\u7ea6\u675f\u7684\u89c4\u6a21\u56e0\u4efb\u52a1\u548c\u6570\u636e\u96c6\u8d28\u91cf\u800c\u5f02\uff0c\u9700\u8981\u8017\u65f6\u4e14\u4e0d\u5207\u5b9e\u9645\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aASPC\u7684\u4e8c\u9636\u53ef\u5fae\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5730\u5e73\u8861\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u3002", "result": "\u572839\u4e2aD4RL\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASPC\u4ec5\u4f7f\u7528\u5355\u4e00\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u9700\u8981\u9010\u6570\u636e\u96c6\u8c03\u6574\u7684\u81ea\u9002\u5e94\u7ea6\u675f\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "ASPC\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u7ea6\u675f\uff0c\u65e0\u9700\u590d\u6742\u7684\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5373\u53ef\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.20033", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20033", "abs": "https://arxiv.org/abs/2508.20033", "authors": ["Liana Patel", "Negar Arabzadeh", "Harshit Gupta", "Ankita Sundar", "Ion Stoica", "Matei Zaharia", "Carlos Guestrin"], "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis", "comment": null, "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DeepScholar-bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7cfb\u7edf\u7684\u5b9e\u65f6\u57fa\u51c6\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8003\u5bdf\u77e5\u8bc6\u7efc\u5408\u3001\u68c0\u7d22\u8d28\u91cf\u548c\u53ef\u9a8c\u8bc1\u6027\u4e09\u4e2a\u7ef4\u5ea6\u6765\u8bc4\u4f30\u7cfb\u7edf\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDeepScholar-base\u7684\u53c2\u8003\u7ba1\u9053\uff0c\u5e76\u4f7f\u7528DeepScholar-bench\u5bf9\u73b0\u6709\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0DeepScholar-base\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u73b0\u6709\u7cfb\u7edf\u5728\u8be5\u57fa\u51c6\u4e0a\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7814\u7a76\u5408\u6210\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u3001\u5b9e\u65f6\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7cfb\u7edf\u3002", "method": "\u63d0\u51faDeepScholar-bench\uff0c\u4e00\u4e2a\u5305\u542b\u6765\u81ea\u8fd1\u671f\u9ad8\u8d28\u91cfArXiv\u8bba\u6587\u7684\u67e5\u8be2\u7684\u5b9e\u65f6\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u8bba\u6587\u7684\u201c\u76f8\u5173\u5de5\u4f5c\u201d\u90e8\u5206\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u77e5\u8bc6\u7efc\u5408\u3001\u68c0\u7d22\u8d28\u91cf\u548c\u53ef\u9a8c\u8bc1\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5168\u65b9\u4f4d\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aDeepScholar-base\u7684\u53c2\u8003\u7ba1\u9053\u3002", "result": "DeepScholar-base\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\uff0c\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u3002\u5728DeepScholar-bench\u4e0a\uff0c\u6ca1\u6709\u7cfb\u7edf\u5728\u6240\u6709\u6307\u6807\u4e0a\u7684\u5f97\u5206\u8d85\u8fc719%\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u7684\u6311\u6218\u6027\u5f88\u9ad8\uff0c\u5e76\u4e14\u8fd8\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "DeepScholar-bench\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u53ef\u4ee5\u63a8\u52a8\u80fd\u591f\u8fdb\u884c\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7684AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u73b0\u6709\u7cfb\u7edf\u5728\u8be5\u57fa\u51c6\u4e0a\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u8868\u660e\u8be5\u9886\u57df\u4ecd\u6709\u5f85\u6539\u8fdb\u3002"}}
{"id": "2508.19789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19789", "abs": "https://arxiv.org/abs/2508.19789", "authors": ["Xiuchao Wu", "Pengfei Zhu", "Jiangjing Lyu", "Xinguo Liu", "Jie Guo", "Yanwen Guo", "Weiwei Xu", "Chengfei Lyu"], "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation", "comment": null, "summary": "Recovering material information from images has been extensively studied in\ncomputer graphics and vision. Recent works in material estimation leverage\ndiffusion model showing promising results. However, these diffusion-based\nmethods adopt a multi-step denoising strategy, which is time-consuming for each\nestimation. Such stochastic inference also conflicts with the deterministic\nmaterial estimation task, leading to a high variance estimated results. In this\npaper, we introduce StableIntrinsic, a one-step diffusion model for multi-view\nmaterial estimation that can produce high-quality material parameters with low\nvariance. To address the overly-smoothing problem in one-step diffusion,\nStableIntrinsic applies losses in pixel space, with each loss designed based on\nthe properties of the material. Additionally, StableIntrinsic introduces a\nDetail Injection Network (DIN) to eliminate the detail loss caused by VAE\nencoding, while further enhancing the sharpness of material prediction results.\nThe experimental results indicate that our method surpasses the current\nstate-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak\nSignal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error\n(MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.", "AI": {"tldr": "StableIntrinsic\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u89c6\u56fe\u6750\u8d28\u4f30\u8ba1\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u548c\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc\u89e3\u51b3\u4e86\u591a\u6b65\u6269\u6563\u65b9\u6cd5\u8017\u65f6\u548c\u7ed3\u679c\u65b9\u5dee\u5927\u7684\u95ee\u9898\uff0c\u5e76\u5728PSNR\u548cMSE\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6750\u8d28\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u591a\u6b65\u53bb\u566a\u7b56\u7565\uff0c\u5bfc\u81f4\u4f30\u8ba1\u8fc7\u7a0b\u8017\u65f6\u4e14\u7ed3\u679c\u65b9\u5dee\u8f83\u5927\uff0c\u4e0e\u786e\u5b9a\u6027\u7684\u6750\u8d28\u4f30\u8ba1\u4efb\u52a1\u5b58\u5728\u51b2\u7a81\u3002", "method": "StableIntrinsic\u91c7\u7528\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u50cf\u7d20\u7a7a\u95f4\u5f15\u5165\u4e86\u57fa\u4e8e\u6750\u8d28\u7279\u6027\u7684\u635f\u5931\u51fd\u6570\uff0c\u540c\u65f6\u5229\u7528\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc\uff08DIN\uff09\u6765\u6062\u590dVAE\u7f16\u7801\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4e22\u5931\u7684\u7ec6\u8282\u5e76\u589e\u5f3a\u9884\u6d4b\u7ed3\u679c\u7684\u6e05\u6670\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStableIntrinsic\u5728\u53cd\u7167\u7387\uff08albedo\uff09\u7684\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u65b9\u9762\u63d0\u9ad8\u4e869.9%\uff0c\u5728\u91d1\u5c5e\u5ea6\uff08metallic\uff09\u548c\u7c97\u7cd9\u5ea6\uff08roughness\uff09\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u65b9\u9762\u5206\u522b\u964d\u4f4e\u4e8644.4%\u548c60.0%\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002", "conclusion": "StableIntrinsic\u901a\u8fc7\u5355\u6b65\u6269\u6563\u3001\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u548c\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u65b9\u5dee\u7684\u591a\u89c6\u56fe\u6750\u8d28\u4f30\u8ba1\uff0c\u5e76\u5728\u591a\u9879\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.20038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20038", "abs": "https://arxiv.org/abs/2508.20038", "authors": ["Sheng Liu", "Qiang Sheng", "Danding Wang", "Yang Li", "Guang Yang", "Juan Cao"], "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks", "comment": "EMNLP 2025 findings", "summary": "Despite advances in improving large language model(LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.", "AI": {"tldr": "LLM \u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u56e0\u4e3a\u653b\u51fb\u6307\u4ee4\u7684\u5206\u5e03\u4e0e\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\u4e0d\u540c\u3002IMAGINE \u6846\u67b6\u901a\u8fc7\u5206\u6790\u5d4c\u5165\u7a7a\u95f4\u5206\u5e03\u6765\u751f\u6210\u7c7b\u4f3c\u8d8a\u72f1\u7684\u6307\u4ee4\uff0c\u4ee5\u5f25\u5408\u8fd9\u79cd\u5206\u5e03\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8 LLM \u7684\u5b89\u5168\u6027\u3002", "motivation": "\u76ee\u524d\u7684 LLM \u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u56e0\u4e3a\u653b\u51fb\u6307\u4ee4\u7684\u5206\u5e03\u4e0e\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\u5b58\u5728\u5dee\u5f02\u3002\u8fd9\u5bfc\u81f4\u5f00\u53d1\u8005\u9700\u8981\u4e0d\u65ad\u8fdb\u884c\u88ab\u52a8\u4fee\u590d\u3002", "method": "\u63d0\u51fa IMAGINE \u6846\u67b6\uff0c\u5229\u7528\u5d4c\u5165\u7a7a\u95f4\u5206\u5e03\u5206\u6790\u6765\u751f\u6210\u7c7b\u4f3c\u8d8a\u72f1\u7684\u6307\u4ee4\uff0c\u4ee5\u5f25\u5408\u8bad\u7ec3\u6570\u636e\u548c\u771f\u5b9e\u653b\u51fb\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u52a8\u6001\u6f14\u53d8\u6587\u672c\u751f\u6210\u5206\u5e03\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\u3002", "result": "\u57fa\u4e8e IMAGINE \u589e\u5f3a\u7684\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\uff0c\u5728 Qwen2.5\u3001Llama3.1 \u548c Llama3.2 \u6a21\u578b\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u6ca1\u6709\u635f\u5bb3\u6a21\u578b\u7684\u6548\u7528\u3002", "conclusion": "IMAGINE \u6846\u67b6\u901a\u8fc7\u751f\u6210\u5206\u5e03\u4e0a\u5339\u914d\u7684\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86 LLM \u9762\u4e34\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19791", "abs": "https://arxiv.org/abs/2508.19791", "authors": ["Shay Shomer Chai", "Wenxuan Peng", "Bharath Hariharan", "Hadar Averbuch-Elor"], "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models", "comment": "Project webpage: https://tau-vailab.github.io/color-edit/", "summary": "Text-to-image generation has recently seen remarkable success, granting users\nwith the ability to create high-quality images through the use of text.\nHowever, contemporary methods face challenges in capturing the precise\nsemantics conveyed by complex multi-object prompts. Consequently, many works\nhave sought to mitigate such semantic misalignments, typically via\ninference-time schemes that modify the attention layers of the denoising\nnetworks. However, prior work has mostly utilized coarse metrics, such as the\ncosine similarity between text and image CLIP embeddings, or human evaluations,\nwhich are challenging to conduct on a larger-scale. In this work, we perform a\ncase study on colors -- a fundamental attribute commonly associated with\nobjects in text prompts, which offer a rich test bed for rigorous evaluation.\nOur analysis reveals that pretrained models struggle to generate images that\nfaithfully reflect multiple color attributes-far more so than with single-color\nprompts-and that neither inference-time techniques nor existing editing methods\nreliably resolve these semantic misalignments. Accordingly, we introduce a\ndedicated image editing technique, mitigating the issue of multi-object\nsemantic alignment for prompts containing multiple colors. We demonstrate that\nour approach significantly boosts performance over a wide range of metrics,\nconsidering images generated by various text-to-image diffusion-based\ntechniques.", "AI": {"tldr": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5305\u542b\u591a\u4e2a\u989c\u8272\u5c5e\u6027\u7684\u590d\u6742\u6587\u672c\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u5728\u989c\u8272\u65b9\u9762\u5b58\u5728\u8bed\u4e49\u504f\u5dee\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3\u591a\u989c\u8272\u5c5e\u6027\u6587\u672c\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u989c\u8272\u5c5e\u6027\u7684\u590d\u6742\u6587\u672c\u63d0\u793a\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bb9\u6613\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u5728\u989c\u8272\u65b9\u9762\u4e0e\u6587\u672c\u63cf\u8ff0\u4e0d\u7b26\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4fee\u6539\u6ce8\u610f\u529b\u5c42\uff09\u548c\u8bc4\u4f30\u6307\u6807\uff08\u5982CLIP\u76f8\u4f3c\u5ea6\u6216\u4eba\u5de5\u8bc4\u4f30\uff09\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u6216\u5927\u89c4\u6a21\u8861\u91cf\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u672c\u6587\u9996\u5148\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u591a\u989c\u8272\u5c5e\u6027\u63d0\u793a\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u73b0\u6709\u63a8\u65ad\u65f6\u548c\u7f16\u8f91\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u8bed\u4e49\u504f\u5dee\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3\u591a\u989c\u8272\u5c5e\u6027\u6587\u672c\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u989c\u8272\u5c5e\u6027\u7684\u6587\u672c\u63d0\u793a\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u5728\u989c\u8272\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5305\u62ec\u4e0d\u540c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5904\u7406\u591a\u989c\u8272\u5c5e\u6027\u63d0\u793a\u65f6\u9047\u5230\u7684\u8bed\u4e49\u5bf9\u9f50\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19915", "abs": "https://arxiv.org/abs/2508.19915", "authors": ["Felix N\u00fctzel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling", "comment": "10 pages, 3 figures, Preprint (submitted version, de-anonymized).\n  Accepted at MLMI (MICCAI Workshop) 2025. Version of Record to appear in\n  Springer LNCS; This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "Retrieval-augmented learning based on radiology reports has emerged as a\npromising direction to improve performance on long-tail medical imaging tasks,\nsuch as rare disease detection in chest X-rays. Most existing methods rely on\ncomparing high-dimensional text embeddings from models like CLIP or CXR-BERT,\nwhich are often difficult to interpret, computationally expensive, and not\nwell-aligned with the structured nature of medical knowledge. We propose a\nnovel, ontology-driven alternative for comparing radiology report texts based\non clinically grounded concepts from the Unified Medical Language System\n(UMLS). Our method extracts standardised medical entities from free-text\nreports using an enhanced pipeline built on RadGraph-XL and SapBERT. These\nentities are linked to UMLS concepts (CUIs), enabling a transparent,\ninterpretable set-based representation of each report. We then define a\ntask-adaptive similarity measure based on a modified and weighted version of\nthe Tversky Index that accounts for synonymy, negation, and hierarchical\nrelationships between medical entities. This allows efficient and semantically\nmeaningful similarity comparisons between reports. We demonstrate that our\napproach outperforms state-of-the-art embedding-based retrieval methods in a\nradiograph classification task on MIMIC-CXR, particularly in long-tail\nsettings. Additionally, we use our pipeline to generate ontology-backed disease\nlabels for MIMIC-CXR, offering a valuable new resource for downstream learning\ntasks. Our work provides more explainable, reliable, and task-specific\nretrieval strategies in clinical AI systems, especially when interpretability\nand domain knowledge integration are essential. Our code is available at\nhttps://github.com/Felix-012/ontology-concept-distillation", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u7684\u3001\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8eUMLS\u6982\u5ff5\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u4f18\u4e8e\u57fa\u4e8eCLIP\u6216CXR-BERT\u7684\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0e\u533b\u5b66\u77e5\u8bc6\u7ed3\u6784\u4e0d\u5339\u914d\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u5c3e\u5206\u5e03\u7684\u7f55\u89c1\u75c5\u68c0\u6d4b\u65f6\u3002", "method": "1. \u4f7f\u7528RadGraph-XL\u548cSapBERT\u63d0\u53d6\u6807\u51c6\u5316\u533b\u5b66\u5b9e\u4f53\u3002\n2. \u5c06\u63d0\u53d6\u7684\u5b9e\u4f53\u94fe\u63a5\u5230UMLS\u6982\u5ff5\uff08CUIs\uff09\u3002\n3. \u6784\u5efa\u57fa\u4e8e\u96c6\u5408\u7684\u6587\u672c\u8868\u793a\u3002\n4. \u5b9a\u4e49\u57fa\u4e8eTversky\u6307\u6570\u7684\u6539\u8fdb\u548c\u52a0\u6743\u76f8\u4f3c\u5ea6\u91cf\uff0c\u8003\u8651\u540c\u4e49\u8bcd\u3001\u5426\u5b9a\u548c\u5c42\u7ea7\u5173\u7cfb\u3002\n5. \u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8fdb\u884cX\u5149\u7247\u5206\u7c7b\u4efb\u52a1\u7684\u8bc4\u4f30\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728MIMIC-CXR\u6570\u636e\u96c6\u7684X\u5149\u7247\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u5c24\u5176\u662f\u5728\u957f\u5c3e\u573a\u666f\u4e0b\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u9760\u6027\u548c\u4efb\u52a1\u7279\u5f02\u6027\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u7684\u4e34\u5e8aAI\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2508.20047", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20047", "abs": "https://arxiv.org/abs/2508.20047", "authors": ["Hassan Alhuzali", "Farah Shamout", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Walid Al-Eisawi", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Nizar Habash", "Leen Kharouf"], "title": "AraHealthQA 2025 Shared Task Description Paper", "comment": null, "summary": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA.", "AI": {"tldr": "AraHealthQA 2025 \u662f\u4e00\u4e2a\u5305\u542b MentalQA \u548c MedArabiQ \u4e24\u4e2a\u5b50\u4efb\u52a1\u7684\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u5171\u4eab\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u8d44\u6e90\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u5728\u771f\u5b9e\u3001\u591a\u8bed\u8a00\u548c\u5177\u6709\u6587\u5316\u7279\u6027\u7684\u533b\u7597\u73af\u5883\u4e0b\u7684\u6a21\u578b\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u8d44\u6e90\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u5728\u771f\u5b9e\u3001\u591a\u8bed\u8a00\u548c\u5177\u6709\u6587\u5316\u7279\u6027\u7684\u533b\u7597\u73af\u5883\u4e0b\u7684\u6a21\u578b\u7814\u7a76\u3002", "method": "\u901a\u8fc7 MentalQA\uff08\u5173\u6ce8\u5fc3\u7406\u5065\u5eb7\uff09\u548c MedArabiQ\uff08\u6db5\u76d6\u66f4\u5e7f\u6cdb\u7684\u533b\u7597\u9886\u57df\uff09\u4e24\u4e2a\u4e92\u8865\u7684\u8d5b\u9053\uff0c\u6bcf\u4e2a\u8d5b\u9053\u5305\u542b\u591a\u4e2a\u5b50\u4efb\u52a1\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u6307\u6807\uff0c\u4ee5\u5b9e\u73b0\u516c\u5e73\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u4ecb\u7ecd\u4e86\u6570\u636e\u96c6\u521b\u5efa\u3001\u4efb\u52a1\u8bbe\u8ba1\u3001\u8bc4\u4f30\u6846\u67b6\u3001\u53c2\u4e0e\u8005\u7edf\u8ba1\u3001\u57fa\u7ebf\u7cfb\u7edf\u4ee5\u53ca\u603b\u4f53\u7ed3\u679c\u3002", "conclusion": "\u5bf9\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u8d8b\u52bf\u548c\u672a\u6765\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u7814\u7a76\u7684\u672a\u6765\u524d\u666f\u8fdb\u884c\u4e86\u53cd\u601d\u3002"}}
{"id": "2508.19798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19798", "abs": "https://arxiv.org/abs/2508.19798", "authors": ["Muhammad Ali", "Omar Ali AlSuwaidi"], "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization", "comment": null, "summary": "In the realm of waste management, automating the sorting process for\nnon-biodegradable materials presents considerable challenges due to the\ncomplexity and variability of waste streams. To address these challenges, we\nintroduce an enhanced neural architecture that builds upon an existing\nEncoder-Decoder structure to improve the accuracy and efficiency of waste\nsorting systems. Our model integrates several key innovations: a Comprehensive\nAttention Block within the decoder, which refines feature representations by\ncombining convolutional and upsampling operations. In parallel, we utilize\nattention through the Mamba architecture, providing an additional performance\nboost. We also introduce a Data Fusion Block that fuses images with more than\nthree channels. To achieve this, we apply PCA transformation to reduce the\ndimensionality while retaining the maximum variance and essential information\nacross three dimensions, which are then used for further processing. We\nevaluated the model on RGB, hyperspectral, multispectral, and a combination of\nRGB and hyperspectral data. The results demonstrate that our approach\noutperforms existing methods by a significant margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Encoder-Decoder\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165Comprehensive Attention Block\u3001Mamba\u67b6\u6784\u548cData Fusion Block\uff0c\u63d0\u5347\u4e86\u5783\u573e\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u975e\u751f\u7269\u964d\u89e3\u6750\u6599\u5783\u573e\u5206\u7c7b\u8fc7\u7a0b\u4e2d\u56e0\u5e9f\u7269\u6d41\u7684\u590d\u6742\u6027\u548c\u591a\u53d8\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u9700\u8981\u63d0\u9ad8\u5783\u573e\u5206\u7c7b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Encoder-Decoder\u67b6\u6784\uff0c\u5305\u62ec\u5728decoder\u4e2d\u52a0\u5165Comprehensive Attention Block\uff08\u7ed3\u5408\u5377\u79ef\u548c\u4e0a\u91c7\u6837\u64cd\u4f5c\uff09\uff0c\u5229\u7528Mamba\u67b6\u6784\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u8bbe\u8ba1Data Fusion Block\u6765\u878d\u5408\u591a\u901a\u9053\u56fe\u50cf\uff08\u901a\u8fc7PCA\u964d\u7ef4\u5904\u7406\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728RGB\u3001\u9ad8\u5149\u8c31\u3001\u591a\u5149\u8c31\u4ee5\u53caRGB\u548c\u9ad8\u5149\u8c31\u7ec4\u5408\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u589e\u5f3a\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u5783\u573e\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u5e9f\u7269\u6d41\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.19924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19924", "abs": "https://arxiv.org/abs/2508.19924", "authors": ["Liming Liu", "Ruoyu Li", "Qing Li", "Meijia Hou", "Yong Jiang", "Mingwei Xu"], "title": "FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification", "comment": null, "summary": "Network traffic classification using pre-training models has shown promising\nresults, but existing methods struggle to capture packet structural\ncharacteristics, flow-level behaviors, hierarchical protocol semantics, and\ninter-packet contextual relationships. To address these challenges, we propose\nFlowletFormer, a BERT-based pre-training model specifically designed for\nnetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware\nTraffic Representation Model for segmenting traffic into semantically\nmeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture\nmultilayer protocol semantics, and Field-Specific and Context-Aware Pretraining\nTasks to enhance both inter-packet and inter-flow learning. Experimental\nresults demonstrate that FlowletFormer significantly outperforms existing\nmethods in the effectiveness of traffic representation, classification\naccuracy, and few-shot learning capability. Moreover, by effectively\nintegrating domain-specific network knowledge, FlowletFormer shows better\ncomprehension of the principles of network transmission (e.g., stateful\nconnections of TCP), providing a more robust and trustworthy framework for\ntraffic analysis.", "AI": {"tldr": "FlowletFormer\u662f\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u76f8\u5e72\u884c\u4e3a\u611f\u77e5\u6d41\u91cf\u8868\u793a\u6a21\u578b\u3001\u57fa\u4e8e\u534f\u8bae\u6808\u5bf9\u9f50\u7684\u5d4c\u5165\u5c42\u4ee5\u53ca\u7279\u5b9a\u5b57\u6bb5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u83b7\u6570\u636e\u5305\u7ed3\u6784\u3001\u6d41\u884c\u4e3a\u3001\u534f\u8bae\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d41\u91cf\u8868\u793a\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u66f4\u597d\u5730\u7406\u89e3\u7f51\u7edc\u4f20\u8f93\u539f\u7406\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6355\u83b7\u6570\u636e\u5305\u7ed3\u6784\u7279\u5f81\u3001\u6d41\u7ea7\u522b\u884c\u4e3a\u3001\u5206\u5c42\u534f\u8bae\u8bed\u4e49\u4ee5\u53ca\u6570\u636e\u5305\u95f4\u4e0a\u4e0b\u6587\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowletFormer\u7684BERT\u57fa\u7840\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\uff1a1. \u76f8\u5e72\u884c\u4e3a\u611f\u77e5\u6d41\u91cf\u8868\u793a\u6a21\u578b\uff08Coherent Behavior-Aware Traffic Representation Model\uff09\uff0c\u7528\u4e8e\u5c06\u6d41\u91cf\u5206\u5272\u6210\u6709\u610f\u4e49\u7684\u5355\u5143\u30022. \u57fa\u4e8e\u534f\u8bae\u6808\u5bf9\u9f50\u7684\u5d4c\u5165\u5c42\uff08Protocol Stack Alignment-Based Embedding Layer\uff09\uff0c\u7528\u4e8e\u6355\u83b7\u591a\u5c42\u534f\u8bae\u8bed\u4e49\u30023. \u7279\u5b9a\u5b57\u6bb5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u4efb\u52a1\uff08Field-Specific and Context-Aware Pretraining Tasks\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u6570\u636e\u5305\u95f4\u548c\u6d41\u95f4\u5b66\u4e60\u3002", "result": "FlowletFormer\u5728\u6d41\u91cf\u8868\u793a\u6709\u6548\u6027\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u8fd8\u80fd\u66f4\u597d\u5730\u7406\u89e3\u7f51\u7edc\u4f20\u8f93\u539f\u7406\uff08\u5982TCP\u7684\u72b6\u6001\u8fde\u63a5\uff09\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u3001\u53ef\u4fe1\u7684\u6d41\u91cf\u5206\u6790\u6846\u67b6\u3002", "conclusion": "FlowletFormer\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u7684\u6027\u80fd\u548c\u5bf9\u7f51\u7edc\u4f20\u8f93\u539f\u7406\u7684\u7406\u89e3\uff0c\u4e3a\u6d41\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2508.20068", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20068", "abs": "https://arxiv.org/abs/2508.20068", "authors": ["Chengzu Li", "Wenshan Wu", "Huanyu Zhang", "Qingtao Li", "Zeyu Gao", "Yan Xia", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Ivan Vuli\u0107", "Furu Wei"], "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis", "comment": "9 pages, 4 figures (22 pages, 7 figures, 7 tables including\n  references and appendices)", "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0811Plus-Bench\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136MLLM\u5728\u7a7a\u95f4\u8ba4\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u65e9\u671f\u8ff9\u8c61\uff0c\u4f46\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002MLLM\u7684\u8ba4\u77e5\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u8ba4\u77e5\u52aa\u529b\u4e0e\u63a8\u7406\u590d\u6742\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u4f46MLLM\u5728\u5b9e\u4f8b\u5c42\u9762\u7684\u8868\u73b0\u662f\u968f\u673a\u7684\uff0c\u800c\u4eba\u7c7b\u7684\u8868\u73b0\u5219\u53d7\u62bd\u8c61\u6a21\u5f0f\u590d\u6742\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30MLLM\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u4e0e\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd9\u662f\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u9886\u57df\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a11Plus-Bench\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6765\u6e90\u4e8e\u6807\u51c6\u5316\u7684\u7a7a\u95f4\u80fd\u529b\u6d4b\u8bd5\uff0c\u5e76\u5305\u542b\u7cbe\u7ec6\u7684\u4e13\u5bb6\u6ce8\u91ca\uff0c\u7528\u4e8e\u5b9e\u4f8b\u7ea7\u522b\u7684\u6a21\u578b\u884c\u4e3a\u5206\u6790\u3002\u901a\u8fc7\u5bf914\u4e2aMLLM\u548c\u4eba\u7c7b\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "MLLM\u5728\u7a7a\u95f4\u8ba4\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u65e9\u671f\u8ff9\u8c61\uff0c\u4f46\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002MLLM\u7684\u8ba4\u77e5\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u8ba4\u77e5\u52aa\u529b\u4e0e\u63a8\u7406\u590d\u6742\u6027\u9ad8\u5ea6\u76f8\u5173\u3002\u7136\u800c\uff0cMLLM\u5728\u5b9e\u4f8b\u5c42\u9762\u7684\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff0c\u800c\u4eba\u7c7b\u7684\u6b63\u786e\u6027\u5219\u9ad8\u5ea6\u53ef\u9884\u6d4b\uff0c\u5e76\u53d7\u62bd\u8c61\u6a21\u5f0f\u590d\u6742\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u5c3d\u7ba1MLLM\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u51fa\u65b0\u5174\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4f46\u5b83\u4eec\u5728\u8ba4\u77e5\u6a21\u5f0f\u4e0a\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u4ee5\u63d0\u5347MLLM\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.19804", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19804", "abs": "https://arxiv.org/abs/2508.19804", "authors": ["Christian Marzahl", "Brian Napora"], "title": "A bag of tricks for real-time Mitotic Figure detection", "comment": null, "summary": "Mitotic figure (MF) detection in histopathology images is challenging due to\nlarge variations in slide scanners, staining protocols, tissue types, and the\npresence of artifacts. This paper presents a collection of training techniques\n- a bag of tricks - that enable robust, real-time MF detection across diverse\ndomains. We build on the efficient RTMDet single stage object detector to\nachieve high inference speed suitable for clinical deployment. Our method\naddresses scanner variability and tumor heterogeneity via extensive\nmulti-domain training data, balanced sampling, and careful augmentation.\nAdditionally, we employ targeted, hard negative mining on necrotic and debris\ntissue to reduce false positives. In a grouped 5-fold cross-validation across\nmultiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On\nthe preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025\nchallenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,\noutperforming larger models and demonstrating adaptability to new, unfamiliar\ndomains. The proposed solution offers a practical trade-off between accuracy\nand speed, making it attractive for real-world clinical adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u75c5\u7406\u56fe\u50cf\u4e2d\u7ec6\u80de\u5206\u88c2\u8c61\uff08MF\uff09\u68c0\u6d4b\u7684\u8bad\u7ec3\u6280\u5de7\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u8bbe\u5907\u3001\u67d3\u8272\u548c\u7ec4\u7ec7\u7c7b\u578b\u5e26\u6765\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eRTMDet\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9886\u57df\u6570\u636e\u8bad\u7ec3\u3001\u91c7\u6837\u5e73\u8861\u3001\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u96be\u4f8b\u6316\u6398\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u6700\u7ec8\u5728MIDOG 2025\u6311\u6218\u8d5b\u9884\u6d4b\u8bd5\u96c6\u4e2d\u53d6\u5f97\u4e860.81 F1\u5206\u6570\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u5206\u88c2\u8c61\uff08MF\uff09\u68c0\u6d4b\u9762\u4e34\u626b\u63cf\u8bbe\u5907\u3001\u67d3\u8272\u65b9\u6848\u3001\u7ec4\u7ec7\u7c7b\u578b\u548c\u4f2a\u5f71\u7b49\u65b9\u9762\u7684\u5de8\u5927\u6311\u6218\uff0c\u9700\u8981\u9c81\u68d2\u4e14\u5b9e\u65f6\u7684\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u9002\u5e94\u4e34\u5e8a\u9700\u6c42\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u5355\u9636\u6bb5\u76ee\u6807\u68c0\u6d4b\u5668RTMDet\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8bad\u7ec3\u6280\u5de7\uff08\u201ctricks\u201d\uff09\uff0c\u5305\u62ec\u5229\u7528\u5e7f\u6cdb\u7684\u591a\u9886\u57df\u8bad\u7ec3\u6570\u636e\u3001\u5e73\u8861\u91c7\u6837\u3001\u4ed4\u7ec6\u7684\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u9488\u5bf9\u574f\u6b7b\u548c\u788e\u7247\u7ec4\u7ec7\u7684\u786c\u8d1f\u4f8b\u6316\u6398\uff0c\u4ee5\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u9c81\u68d2\u3001\u5b9e\u65f6MF\u68c0\u6d4b\u3002", "result": "\u5728\u8de8\u591a\u4e2aMF\u6570\u636e\u96c6\u76845\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e860.78\u81f30.84\u7684F1\u5206\u6570\u3002\u5728MIDOG 2025\u6311\u6218\u8d5b\u7684\u9884\u6d4b\u8bd5\u96c6\u4e2d\uff0c\u57fa\u4e8eRTMDet-S\u7684\u6a21\u578b\u8fbe\u5230\u4e860.81\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf9\u65b0\u3001\u672a\u77e5\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8bad\u7ec3\u6280\u5de7\u96c6\u80fd\u591f\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u9c81\u68d2\u3001\u5b9e\u65f6MF\u68c0\u6d4b\uff0c\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u5b9e\u9645\u7684\u5e73\u8861\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19955", "categories": ["cs.LG", "cs.IT", "math.IT", "62M10 (primary), 94A17 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.19955", "abs": "https://arxiv.org/abs/2508.19955", "authors": ["Abhijeet Avhale", "Joscha Diehl", "Niraj Velankar", "Emanuele Verri"], "title": "Global Permutation Entropy", "comment": "12 pages, 10 figures", "summary": "Permutation Entropy, introduced by Bandt and Pompe, is a widely used\ncomplexity measure for real-valued time series that is based on the relative\norder of values within consecutive segments of fixed length. After\nstandardizing each segment to a permutation and computing the frequency\ndistribution of these permutations, Shannon Entropy is then applied to quantify\nthe series' complexity. We introduce Global Permutation Entropy (GPE), a novel\nindex that considers all possible patterns of a given length, including\nnon-consecutive ones. Its computation relies on recently developed algorithms\nthat enable the efficient extraction of full permutation profiles. We\nillustrate some properties of GPE and demonstrate its effectiveness through\nexperiments on synthetic datasets, showing that it reveals structural\ninformation not accessible through standard permutation entropy. We provide a\nJulia package for the calculation of GPE at\n`https://github.com/AThreeH1/Global-Permutation-Entropy'.", "AI": {"tldr": "\u5168\u5c40\u6392\u5217\u71b5\uff08GPE\uff09\u662f\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u5b83\u8003\u8651\u4e86\u6240\u6709\u53ef\u80fd\u7684\u6a21\u5f0f\uff0c\u5305\u62ec\u975e\u8fde\u7eed\u7684\u6a21\u5f0f\uff0c\u5e76\u663e\u793a\u51fa\u6bd4\u6807\u51c6\u6392\u5217\u71b5\u66f4\u80fd\u63ed\u793a\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u6807\u51c6\u6392\u5217\u71b5\u4ec5\u8003\u8651\u8fde\u7eed\u6bb5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u6240\u6709\u53ef\u80fd\u7684\u6a21\u5f0f\uff0c\u5305\u62ec\u975e\u8fde\u7eed\u7684\u6a21\u5f0f\u3002", "method": "\u5168\u5c40\u6392\u5217\u71b5\uff08GPE\uff09\u7684\u8ba1\u7b97\u4f9d\u8d56\u4e8e\u6700\u8fd1\u5f00\u53d1\u7684\u80fd\u591f\u6709\u6548\u63d0\u53d6\u5b8c\u6574\u6392\u5217\u914d\u7f6e\u6587\u4ef6\u7684\u7b97\u6cd5\u3002GPE\u901a\u8fc7\u8003\u8651\u6240\u6709\u53ef\u80fd\u7684\u6a21\u5f0f\uff08\u5305\u62ec\u975e\u8fde\u7eed\u6a21\u5f0f\uff09\u6765\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGPE\u80fd\u591f\u63ed\u793a\u6807\u51c6\u6392\u5217\u71b5\u65e0\u6cd5\u83b7\u53d6\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "GPE\u662f\u4e00\u79cd\u6bd4\u6807\u51c6\u6392\u5217\u71b5\u66f4\u6709\u4fe1\u606f\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u7528\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.19808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19808", "abs": "https://arxiv.org/abs/2508.19808", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment", "comment": "Accepted to ICCV 2025 Workshop LIMIT", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due\nto its dual requirements of pixel-level masks and temporal consistency labels.\nWhile recent unsupervised methods like VideoCutLER eliminate optical flow\ndependencies through synthetic data, they remain constrained by the\nsynthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised\nframework that bridges this gap through quality-guided self-training. Our\napproach establishes a closed-loop system between pseudo-label generation and\nautomatic quality assessment, enabling progressive adaptation from synthetic to\nreal videos. Experiments demonstrate state-of-the-art performance with 52.6\n$\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous\nstate-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations.\nThis demonstrates the viability of quality-aware self-training for unsupervised\nVIS. The source code of our method is available at\nhttps://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u89e3\u51b3\u4e86\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u4e2d\u7684\u6807\u6ce8\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4ece\u5408\u6210\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u5f0f\u9002\u5e94\uff0c\u5e76\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e8652.6 AP50\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u7531\u4e8e\u540c\u65f6\u9700\u8981\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6807\u7b7e\uff0c\u56e0\u6b64\u5728\u6807\u6ce8\u65b9\u9762\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u867d\u7136\u6d88\u9664\u4e86\u5bf9\u5149\u6d41\u7684\u4f9d\u8d56\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5408\u6210\u5230\u771f\u5b9e\u4e16\u754c\u7684\u57df\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoQ-VIS\u7684\u65b0\u9896\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u6765\u5f25\u5408\u5408\u6210\u5230\u771f\u5b9e\u4e16\u754c\u7684\u57df\u8fc1\u79fb\u5dee\u8ddd\u3002\u8be5\u65b9\u6cd5\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u4e4b\u95f4\u5efa\u7acb\u4e86\u4e00\u4e2a\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u5408\u6210\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u5f0f\u9002\u5e94\u3002", "result": "\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e8652.6 AP50\u7684\u6027\u80fd\uff0c\u6bd4\u4e4b\u524d\u7684SOTA\u65b9\u6cd5VideoCutLER\u63d0\u9ad8\u4e864.4%\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "\u8d28\u91cf\u611f\u77e5\u81ea\u8bad\u7ec3\u5bf9\u4e8e\u65e0\u76d1\u7763VIS\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2508.19974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19974", "abs": "https://arxiv.org/abs/2508.19974", "authors": ["Khaled M. A. Alghtus", "Aiyad Gannan", "Khalid M. Alhajri", "Ali L. A. Al Jubouri", "Hassan A. I. Al-Janahi"], "title": "Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning", "comment": null, "summary": "This study presents a machine learning framework for forecasting short-term\nfaults in industrial centrifugal pumps using real-time sensor data. The\napproach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in\nadvance based on patterns extracted from historical operation. Two lookback\nperiods, 60 minutes and 120 minutes, were evaluated using a sliding window\napproach. For each window, statistical features including mean, standard\ndeviation, minimum, maximum, and linear trend were extracted, and class\nimbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost\nclassifiers were trained and tested on the labeled dataset. Results show that\nthe Random Forest model achieved the best short-term forecasting performance\nwith a 60-minute window, reaching recall scores of 69.2\\% at 5 minutes, 64.9\\%\nat 15 minutes, and 48.6\\% at 30 minutes. With a 120-minute window, the Random\nForest model achieved 57.6\\% recall at 5 minutes, and improved predictive\naccuracy of 65.6\\% at both 15 and 30 minutes. XGBoost displayed similar but\nslightly lower performance. These findings highlight that optimal history\nlength depends on the prediction horizon, and that different fault patterns may\nevolve at different timescales. The proposed method offers an interpretable and\nscalable solution for integrating predictive maintenance into real-time\nindustrial monitoring systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u9884\u6d4b\u5de5\u4e1a\u79bb\u5fc3\u6cf5\u77ed\u671f\u6545\u969c\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u5386\u53f2\u64cd\u4f5c\u6a21\u5f0f\uff0c\u9884\u6d4b\u63d0\u524d5\u300115\u548c30\u5206\u949f\u7684{EarlyWarning}\u72b6\u51b5\u3002\u7814\u7a76\u8bc4\u4f30\u4e8660\u5206\u949f\u548c120\u5206\u949f\u7684\u4e24\u79cd\u56de\u6eaf\u671f\uff0c\u5e76\u91c7\u7528\u4e86\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u3002\u6bcf\u4e2a\u7a97\u53e3\u63d0\u53d6\u4e86\u5305\u62ec\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u548c\u7ebf\u6027\u8d8b\u52bf\u5728\u5185\u7684\u7edf\u8ba1\u7279\u5f81\uff0c\u5e76\u4f7f\u7528SMOTE\u7b97\u6cd5\u5904\u7406\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u6700\u540e\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e86\u968f\u673a\u68ee\u6797\u548cXGBoost\u5206\u7c7b\u5668\u3002", "motivation": "\u5de5\u4e1a\u79bb\u5fc3\u6cf5\u7684\u77ed\u671f\u6545\u969c\u9884\u6d4b\u548c\u65e9\u671f\u9884\u8b66\u3002", "method": "1. \u5efa\u7acb\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u9884\u6d4b\u77ed\u671f\u6545\u969c\u3002\n2. \u8bc4\u4f3060\u5206\u949f\u548c120\u5206\u949f\u4e24\u79cd\u56de\u6eaf\u671f\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u3002\n3. \u63d0\u53d6\u6bcf\u4e2a\u7a97\u53e3\u7684\u7edf\u8ba1\u7279\u5f81\uff08\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u3001\u7ebf\u6027\u8d8b\u52bf\uff09\u3002\n4. \u4f7f\u7528SMOTE\u7b97\u6cd5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\n5. \u8bad\u7ec3\u548c\u6d4b\u8bd5\u968f\u673a\u68ee\u6797\u548cXGBoost\u5206\u7c7b\u5668\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u4f7f\u752860\u5206\u949f\u7a97\u53e3\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u5728\u63d0\u524d5\u5206\u949f\u300115\u5206\u949f\u548c30\u5206\u949f\u7684\u53ec\u56de\u7387\u5206\u522b\u4e3a69.2%\u300164.9%\u548c48.6%\u3002\u5728\u4f7f\u7528120\u5206\u949f\u7a97\u53e3\u65f6\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u63d0\u524d5\u5206\u949f\u7684\u53ec\u56de\u7387\u4e3a57.6%\uff0c\u5728\u63d0\u524d15\u5206\u949f\u548c30\u5206\u949f\u7684\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u523065.6%\u3002XGBoost\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u968f\u673a\u68ee\u6797\u7c7b\u4f3c\uff0c\u4f46\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u6240\u9009\u7684\u5386\u53f2\u6570\u636e\u957f\u5ea6\u76f8\u5173\uff0c\u4e0d\u540c\u7684\u6545\u969c\u6a21\u5f0f\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u53d1\u5c55\u3002\u8be5\u6846\u67b6\u4e3a\u5c06\u9884\u6d4b\u6027\u7ef4\u62a4\u96c6\u6210\u5230\u5b9e\u65f6\u5de5\u4e1a\u76d1\u63a7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19815", "abs": "https://arxiv.org/abs/2508.19815", "authors": ["Linkuan Zhou", "Zhexin Chen", "Yufei Shen", "Junlin Xu", "Ping Xuan", "Yixin Zhu", "Yuqi Fang", "Cong Cong", "Leyi Wei", "Ran Su", "Jia Zhou", "Qiangguo Jin"], "title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images", "comment": null, "summary": "Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.", "AI": {"tldr": "\u63d0\u51faERSR\u6846\u67b6\u7528\u4e8e\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u548c\u56fe\u50cf\u8d28\u91cf\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u5316\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u5206\u5272\u5728\u4ea7\u524d\u76d1\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d85\u58f0\u56fe\u50cf\u8d28\u91cf\u5dee\u548c\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u5728\u5904\u7406\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u7684\u7279\u6027\u65f6\uff0c\u751f\u6210\u53ef\u9760\u4f2a\u6807\u7b7e\u548c\u5f3a\u5236\u4e00\u81f4\u6027\u6b63\u5219\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faERSR\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8bc4\u5206\u81ea\u9002\u5e94\u6ee4\u6ce2\u7b56\u7565\u3001\u692d\u5706\u7ea6\u675f\u4f2a\u6807\u7b7e\u4f18\u5316\u4ee5\u53ca\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u591a\u91cd\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002\u53cc\u8bc4\u5206\u7b56\u7565\u901a\u8fc7\u8fb9\u754c\u4e00\u81f4\u6027\u548c\u8f6e\u5ed3\u89c4\u5219\u6027\u8bc4\u4f30\u548c\u8fc7\u6ee4\u6559\u5e08\u8f93\u51fa\u3002\u692d\u5706\u7ea6\u675f\u4f18\u5316\u901a\u8fc7\u62df\u5408\u6700\u5c0f\u4e8c\u4e58\u692d\u5706\u6765\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u540c\u65f6\u5f3a\u5316\u62df\u5408\u692d\u5706\u4e2d\u5fc3\u9644\u8fd1\u7684\u50cf\u7d20\u5e76\u6291\u5236\u566a\u58f0\u3002\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u591a\u91cd\u4e00\u81f4\u6027\u6b63\u5219\u5316\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u3001\u5bf9\u79f0\u533a\u57df\u4ee5\u53ca\u539f\u59cb\u9884\u6d4b\u548c\u4f2a\u6807\u7b7e\u4e4b\u95f4\u7684\u591a\u5c42\u6b21\u4e00\u81f4\u6027\uff0c\u4fc3\u4f7f\u6a21\u578b\u6355\u6349\u9c81\u68d2\u4e14\u7a33\u5b9a\u7684\u5f62\u72b6\u8868\u793a\u3002", "result": "\u5728HC18\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752810%\u548c20%\u7684\u6807\u6ce8\u6570\u636e\uff0cERSR\u5206\u522b\u8fbe\u523092.05%\u548c95.36%\u7684Dice\u5206\u6570\u3002\u5728PSFH\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u540c\u8bbe\u7f6e\u4e0b\u7684\u5206\u6570\u5206\u522b\u4e3a91.68%\u548c93.70%\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ERSR\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u5206\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19979", "abs": "https://arxiv.org/abs/2508.19979", "authors": ["Behafarid Hemmatpour", "Javad Dogani", "Nikolaos Laoutaris"], "title": "Reducing Street Parking Search Time via Smart Assignment Strategies", "comment": "Please cite the ACM SIGSPATIAL'25 version of this paper", "summary": "In dense metropolitan areas, searching for street parking adds to traffic\ncongestion. Like many other problems, real-time assistants based on mobile\nphones have been proposed, but their effectiveness is understudied. This work\nquantifies how varying levels of user coordination and information availability\nthrough such apps impact search time and the probability of finding street\nparking. Through a data-driven simulation of Madrid's street parking ecosystem,\nwe analyze four distinct strategies: uncoordinated search (Unc-Agn),\ncoordinated parking without awareness of non-users (Cord-Agn), an idealized\noracle system that knows the positions of all non-users (Cord-Oracle), and our\nnovel/practical Cord-Approx strategy that estimates non-users' behavior\nprobabilistically. The Cord-Approx strategy, instead of requiring knowledge of\nhow close non-users are to a certain spot in order to decide whether to\nnavigate toward it, uses past occupancy distributions to elongate physical\ndistances between system users and alternative parking spots, and then solves a\nHungarian matching problem to dispatch accordingly. In high-fidelity\nsimulations of Madrid's parking network with real traffic data, users of\nCord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes\nfor non-users without an app. A zone-level snapshot shows that Cord-Approx\nreduces search time for system users by 72% (range = 67-76%) in central hubs,\nand up to 73% in residential areas, relative to non-users.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6a21\u62df\u9a6c\u5fb7\u91cc\u505c\u8f66\u751f\u6001\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86\u624b\u673aAPP\u5728\u7f13\u89e3\u57ce\u5e02\u505c\u8f66\u62e5\u5835\u65b9\u9762\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCord-Approx\u7684\u534f\u8c03\u505c\u8f66\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u6982\u7387\u4f30\u8ba1\u975e\u7528\u6237\u884c\u4e3a\u5e76\u4f18\u5316\u8f66\u8f86\u8c03\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7528\u6237\u5bfb\u627e\u505c\u8f66\u4f4d\u7684\u65f6\u95f4\u3002", "motivation": "\u505c\u8f66\u96be\u662f\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\u7684\u91cd\u8981\u539f\u56e0\uff0c\u624b\u673aAPP\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u6f5c\u5728\u65b9\u6848\uff0c\u4f46\u5176\u6548\u679c\u4ecd\u9700\u7814\u7a76\u3002", "method": "\u672c\u6587\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u5bf9\u9a6c\u5fb7\u91cc\u505c\u8f66\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5bf9\u6bd4\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u505c\u8f66\u7b56\u7565\uff1a\u65e0\u534f\u8c03\u7b56\u7565(Unc-Agn)\u3001\u6709\u534f\u8c03\u4f46\u65e0\u975e\u7528\u6237\u611f\u77e5\u7b56\u7565(Cord-Agn)\u3001\u7406\u60f3\u5316\u7684\u5168\u77e5\u7b56\u7565(Cord-Oracle)\u4ee5\u53ca\u63d0\u51fa\u7684Cord-Approx\u7b56\u7565\u3002Cord-Approx\u7b56\u7565\u901a\u8fc7\u6982\u7387\u6a21\u578b\u4f30\u8ba1\u975e\u7528\u6237\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u5308\u7259\u5229\u5339\u914d\u7b97\u6cd5\u8fdb\u884c\u8f66\u8f86\u8c03\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cCord-Approx\u7b56\u7565\u7684\u7528\u6237\u5e73\u5747\u82b1\u8d396.69\u5206\u949f\u627e\u5230\u505c\u8f66\u4f4d\uff0c\u800c\u975eAPP\u7528\u6237\u5219\u9700\u898119.98\u5206\u949f\u3002\u5728\u9a6c\u5fb7\u91cc\u5e02\u4e2d\u5fc3\u533a\u57df\uff0cCord-Approx\u7b56\u7565\u5c06\u7cfb\u7edf\u7528\u6237\u7684\u5bfb\u627e\u505c\u8f66\u4f4d\u65f6\u95f4\u51cf\u5c11\u4e8672%\uff0c\u5728\u4f4f\u5b85\u533a\u51cf\u5c11\u4e8673%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Cord-Approx\u7b56\u7565\u80fd\u591f\u663e\u8457\u51cf\u5c11\u7528\u6237\u5bfb\u627e\u505c\u8f66\u4f4d\u7684\u65f6\u95f4\uff0c\u5e76\u7f13\u89e3\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u7ed3\u5408\u6982\u7387\u6a21\u578b\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7b56\u7565\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.19830", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19830", "abs": "https://arxiv.org/abs/2508.19830", "authors": ["Yilin Zhang", "Cai Xu", "You Wu", "Ziyu Guan", "Wei Zhao"], "title": "Gradient Rectification for Robust Calibration under Distribution Shift", "comment": "14 pages, under review", "summary": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6821\u51c6\u4e0d\u51c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u57df\u4fe1\u606f\u5373\u53ef\u64cd\u4f5c\u7684\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u9891\u6ee4\u6ce2\u548c\u68af\u5ea6\u6821\u6b63\u673a\u5236\uff0c\u5728\u63d0\u9ad8\u5206\u5e03\u5916\u6821\u51c6\u7684\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5f80\u5f80\u4f1a\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\uff0c\u8fd9\u4f1a\u524a\u5f31\u5176\u53ef\u9760\u6027\u3002\u5728\u6d4b\u8bd5\u6570\u636e\u56e0\u73af\u5883\u6216\u91c7\u96c6\u53d8\u5316\u800c\u504f\u79bb\u8bad\u7ec3\u5206\u5e03\u7684\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u8fd9\u79cd\u6821\u51c6\u4e0d\u51c6\u7684\u95ee\u9898\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u76ee\u6807\u57df\u7684\u8bbf\u95ee\u6216\u6a21\u62df\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u57df\u4fe1\u606f\u5373\u53ef\u64cd\u4f5c\u7684\u6821\u51c6\u6846\u67b6\u3002\u4ece\u9891\u57df\u89d2\u5ea6\u8bc6\u522b\u51fa\u5206\u5e03\u504f\u79fb\u4f1a\u626d\u66f2\u6df1\u5ea6\u6a21\u578b\u5229\u7528\u7684\u9ad8\u9891\u89c6\u89c9\u7ebf\u7d22\uff0c\u5e76\u5f15\u5165\u4f4e\u9891\u6ee4\u6ce2\u7b56\u7565\u6765\u9f13\u52b1\u6a21\u578b\u4f9d\u8d56\u4e8e\u57df\u4e0d\u53d8\u7279\u5f81\u3002\u4e3a\u89e3\u51b3\u4fe1\u606f\u4e22\u5931\u53ef\u80fd\u5bfc\u81f4\u7684\u5206\u5e03\u5185\u6821\u51c6\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u6b63\u673a\u5236\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u5206\u5e03\u5185\u6821\u51c6\u4f5c\u4e3a\u786c\u7ea6\u675f\u3002", "result": "\u5728 CIFAR-10/100-C \u548c WILDS \u7b49\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u504f\u79fb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u5206\u5e03\u5185\u6027\u80fd\u3002"}}
{"id": "2508.19980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19980", "abs": "https://arxiv.org/abs/2508.19980", "authors": ["Dylan Sam", "Alexander Robey", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter"], "title": "Evaluating Language Model Reasoning about Confidential Information", "comment": "20 pages", "summary": "As language models are increasingly deployed as autonomous agents in\nhigh-stakes settings, ensuring that they reliably follow user-defined rules has\nbecome a critical safety concern. To this end, we study whether language models\nexhibit contextual robustness, or the capability to adhere to context-dependent\nsafety specifications. For this analysis, we develop a benchmark (PasswordEval)\nthat measures whether language models can correctly determine when a user\nrequest is authorized (i.e., with a correct password). We find that current\nopen- and closed-source models struggle with this seemingly simple task, and\nthat, perhaps surprisingly, reasoning capabilities do not generally improve\nperformance. In fact, we find that reasoning traces frequently leak\nconfidential information, which calls into question whether reasoning traces\nshould be exposed to users in such applications. We also scale the difficulty\nof our evaluation along multiple axes: (i) by adding adversarial user pressure\nthrough various jailbreaking strategies, and (ii) through longer multi-turn\nconversations where password verification is more challenging. Overall, our\nresults suggest that current frontier models are not well-suited to handling\nconfidential information, and that reasoning capabilities may need to be\ntrained in a different manner to make them safer for release in high-stakes\nsettings.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u6d89\u53ca\u9ad8\u98ce\u9669\u7684\u573a\u666f\u4e0b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u90e8\u7f72\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u53ef\u9760\u5730\u9075\u5faa\u7528\u6237\u5b9a\u4e49\u7684\u89c4\u5219\u5df2\u6210\u4e3a\u4e00\u9879\u5173\u952e\u7684\u5b89\u5168\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u5373\u9075\u5faa\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b89\u5168\u89c4\u8303\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aPasswordEval\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u5224\u65ad\u7528\u6237\u8bf7\u6c42\u662f\u5426\u5df2\u6388\u6743\uff08\u5373\u5177\u6709\u6b63\u786e\u5bc6\u7801\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5f53\u524d\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u6b64\u770b\u4f3c\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\u90fd\u5b58\u5728\u56f0\u96be\uff0c\u800c\u4e14\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u63a8\u7406\u80fd\u529b\u901a\u5e38\u5e76\u4e0d\u80fd\u63d0\u9ad8\u6027\u80fd\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6211\u4eec\u53d1\u73b0\u63a8\u7406\u8fc7\u7a0b\u5e38\u5e38\u4f1a\u6cc4\u9732\u673a\u5bc6\u4fe1\u606f\uff0c\u8fd9\u4f7f\u5f97\u5728\u8fd9\u4e9b\u5e94\u7528\u4e2d\u662f\u5426\u5e94\u5411\u7528\u6237\u516c\u5f00\u63a8\u7406\u8fc7\u7a0b\u503c\u5f97\u6000\u7591\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u589e\u52a0\u4e86\u8bc4\u4f30\u7684\u96be\u5ea6\uff1a(i)\u901a\u8fc7\u5404\u79cd\u8d8a\u72f1\u7b56\u7565\u589e\u52a0\u5bf9\u6297\u6027\u7528\u6237\u538b\u529b\uff1b(ii)\u901a\u8fc7\u66f4\u957f\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u589e\u52a0\u5bc6\u7801\u9a8c\u8bc1\u7684\u96be\u5ea6\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u524d\u6cbf\u6a21\u578b\u5e76\u4e0d\u9002\u5408\u5904\u7406\u673a\u5bc6\u4fe1\u606f\uff0c\u5e76\u4e14\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u9700\u8981\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u624d\u80fd\u4f7f\u5176\u5728\u90e8\u7f72\u5230\u9ad8\u98ce\u9669\u573a\u666f\u65f6\u66f4\u52a0\u5b89\u5168\u3002", "motivation": "\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u5728\u6d89\u53ca\u9ad8\u98ce\u9669\u7684\u573a\u666f\u4e0b\u80fd\u591f\u53ef\u9760\u5730\u9075\u5faa\u7528\u6237\u5b9a\u4e49\u7684\u89c4\u5219\uff0c\u8fd9\u5df2\u6210\u4e3a\u4e00\u9879\u5173\u952e\u7684\u5b89\u5168\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u5373\u9075\u5faa\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b89\u5168\u89c4\u8303\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aPasswordEval\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u5224\u65ad\u7528\u6237\u8bf7\u6c42\u662f\u5426\u5df2\u6388\u6743\uff08\u5373\u5177\u6709\u6b63\u786e\u5bc6\u7801\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002\u901a\u8fc7\u589e\u52a0\u5bf9\u6297\u6027\u7528\u6237\u538b\u529b\uff08\u8d8a\u72f1\u7b56\u7565\uff09\u548c\u66f4\u957f\u7684\u591a\u8f6e\u5bf9\u8bdd\u6765\u589e\u52a0\u8bc4\u4f30\u7684\u96be\u5ea6\u3002", "result": "\u5f53\u524d\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u5224\u65ad\u7528\u6237\u8bf7\u6c42\u662f\u5426\u5df2\u6388\u6743\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u63a8\u7406\u80fd\u529b\u901a\u5e38\u5e76\u4e0d\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u53cd\u800c\u53ef\u80fd\u6cc4\u9732\u673a\u5bc6\u4fe1\u606f\uff1b\u8d8a\u72f1\u7b56\u7565\u548c\u66f4\u957f\u7684\u591a\u8f6e\u5bf9\u8bdd\u4f1a\u589e\u52a0\u5bc6\u7801\u9a8c\u8bc1\u7684\u96be\u5ea6\u3002", "conclusion": "\u5f53\u524d\u7684\u524d\u6cbf\u6a21\u578b\u5e76\u4e0d\u9002\u5408\u5904\u7406\u673a\u5bc6\u4fe1\u606f\uff0c\u5e76\u4e14\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u9700\u8981\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u624d\u80fd\u4f7f\u5176\u5728\u90e8\u7f72\u5230\u9ad8\u98ce\u9669\u573a\u666f\u65f6\u66f4\u52a0\u5b89\u5168\u3002"}}
{"id": "2508.19944", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19944", "abs": "https://arxiv.org/abs/2508.19944", "authors": ["Taebaek Hwang", "Minseo Kim", "Gisang Lee", "Seonuk Kim", "Hyunjun Eun"], "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts", "comment": null, "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.", "AI": {"tldr": "KRETA\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u7684\u6587\u672c\u4e30\u5bcc\u578b\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7406\u89e3\u548c\u63a8\u7406\u6587\u672c\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u4e30\u5bcc\u578b\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u4e0a\uff0c\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u97e9\u8bed\uff09\u7f3a\u4e4f\u6b64\u7c7b\u57fa\u51c6\uff0c\u963b\u788d\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002KRETA\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4e3a\u97e9\u8bed\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u3002", "method": "KRETA\u57fa\u51c6\u5305\u542b\u9488\u5bf915\u4e2a\u9886\u57df\u548c26\u79cd\u56fe\u50cf\u7c7b\u578b\u7684\u591a\u6837\u5316\u89c6\u89c9\u5185\u5bb9\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u534a\u81ea\u52a8\u5316\u7684VQA\u751f\u6210\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u5904\u7406\u6587\u672c\u4e30\u5bcc\u7684\u573a\u666f\uff0c\u5e76\u91c7\u7528\u7cbe\u7ec6\u7684\u56fe\u50cf\u5206\u89e3\u548c\u4e25\u683c\u7684\u4e03\u9879\u6307\u6807\u8bc4\u4f30\u534f\u8bae\u6765\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002\u8be5\u6d41\u7a0b\u65e8\u5728\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4ee5\u4fbf\u4e3a\u5176\u4ed6\u8bed\u8a00\u521b\u5efa\u7c7b\u4f3c\u7684\u57fa\u51c6\u3002", "result": "KRETA\u57fa\u51c6\u80fd\u591f\u5bf9\u97e9\u8bed\u7684\u89c6\u89c9\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u6df1\u5165\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u8de815\u4e2a\u9886\u57df\u548c26\u79cd\u56fe\u50cf\u7c7b\u578b\u7684\u591a\u65b9\u9762\u8bc4\u4f30\u3002", "conclusion": "KRETA\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u6587\u672c\u4e30\u5bcc\u578bVQA\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u97e9\u8bedVLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002\u6240\u63d0\u51fa\u7684\u751f\u6210\u6d41\u7a0b\u5177\u6709\u666e\u9002\u6027\uff0c\u6709\u671b\u4fc3\u8fdb\u5176\u4ed6\u8bed\u8a00\u7684\u7c7b\u4f3c\u57fa\u51c6\u7684\u5f00\u53d1\uff0c\u4ece\u800c\u63a8\u52a8\u591a\u8bed\u8a00VLM\u7684\u7814\u7a76\u3002"}}
{"id": "2508.19850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19850", "abs": "https://arxiv.org/abs/2508.19850", "authors": ["Xiaoqi Wang", "Yun Zhang", "Weisi Lin"], "title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models", "comment": null, "summary": "Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\uff08MVS\uff09\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08MIQA\uff09\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b250\u4e07\u6837\u672c\u7684MIQD\u6570\u636e\u5e93\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3aRA-MIQA\u7684\u533a\u57df\u611f\u77e5\u6a21\u578b\u3002\u5b9e\u9a8c\u8bc1\u660eRA-MIQA\u5728MVS\u6027\u80fd\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf\uff08HVS\uff09\u7684\u6307\u6807\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709HVS\u6307\u6807\u548c\u4e00\u4e9bMIQA\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u89c6\u89c9\u7cfb\u7edf\uff08MVS\uff09\u5728\u4e0d\u5229\u89c6\u89c9\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u91cf\u5316\u56fe\u50cf\u9000\u5316\u5bf9MVS\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u673a\u5668\u4e2d\u5fc3\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08MIQA\uff09\u8303\u5f0f\uff0c\u6784\u5efa\u4e86\u5305\u542b250\u4e07\u6837\u672c\u7684\u673a\u5668\u4e2d\u5fc3\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93\uff08MIQD\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRA-MIQA\u7684\u533a\u57df\u611f\u77e5MIQA\u6a21\u578b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u9000\u5316\u5206\u6790\u6765\u8bc4\u4f30MVS\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "result": "RA-MIQA\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u6307\u6807\u4e0a\u5747\u4f18\u4e8e7\u4e2a\u57fa\u4e8eHVS\u7684IQA\u6307\u6807\u548c5\u4e2a\u7ecf\u5178\u9aa8\u5e72\u7f51\u7edc\u3002\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cRA-MIQA\u5728\u4e00\u81f4\u6027\u65b9\u9762\u63d0\u9ad8\u4e8613.56%\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e8613.37%\u3002\u5b9e\u9a8c\u8fd8\u53d1\u73b0\uff0c\u57fa\u4e8eHVS\u7684\u6307\u6807\u4e0d\u8db3\u4ee5\u9884\u6d4bMVS\u8d28\u91cf\uff0c\u5e76\u4e14\u4e00\u4e9b\u4e13\u95e8\u7684MIQA\u6a21\u578b\u5728\u5904\u7406\u80cc\u666f\u9000\u5316\u3001\u9762\u5411\u51c6\u786e\u6027\u7684\u4f30\u8ba1\u548c\u7ec6\u5fae\u5931\u771f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51faMIQA\u6846\u67b6\u548cRA-MIQA\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86MVS\u7684\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u673a\u5668\u4e2d\u5fc3\u56fe\u50cf\u5904\u7406\u548c\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u6307\u51fa\u4e86\u5f53\u524dMVS\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff08\u5305\u62ecHVS\u6307\u6807\u548c\u90e8\u5206MIQA\u6a21\u578b\uff09\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19990", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19990", "abs": "https://arxiv.org/abs/2508.19990", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "title": "Self-Supervised Pre-Training with Equilibrium Constraints", "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u901a\u8fc7\u5f15\u5165\u5e73\u8861\u7ea6\u675f\uff0c\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\u6c42\u89e3\uff0c\u5728\u591a\u9886\u57df\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u65e0\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\uff0c\u4f20\u7edf\u7684\u6df7\u5408\u6570\u636e\u5e76\u6700\u5c0f\u5316\u5e73\u5747\u5168\u5c40\u635f\u5931\u7684\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65bd\u52a0\u989d\u5916\u7684\u5e73\u8861\u7ea6\u675f\uff0c\u786e\u4fdd\u6a21\u578b\u5728K\u6b65\u68af\u5ea6\u4e0b\u964d\u540e\u4f18\u5316\u6bcf\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u5230\u5176\u5c40\u90e8\u6700\u4f18\u3002\u5c06\u6b64\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\u6c42\u89e3\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5143\u5b66\u4e60\uff08MAML\uff09\u7684\u8054\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u9886\u57df\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u63d0\u9ad8\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.19972", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19972", "abs": "https://arxiv.org/abs/2508.19972", "authors": ["Seongheon Park", "Yixuan Li"], "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity", "comment": null, "summary": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.", "AI": {"tldr": "GLSim\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5d4c\u5165\u76f8\u4f3c\u6027\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5bf9\u8c61\u5e7b\u89c9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5bf9\u8c61\u5e7b\u89c9\u5728\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u4e00\u4e2a\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u7684\u5bf9\u8c61\u5e7b\u89c9\u8bc4\u5206\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5168\u5c40\u6216\u5c40\u90e8\u89c6\u89d2\uff0c\u53ef\u80fd\u5f71\u54cd\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGLSim\u7684\u65b0\u578b\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u8c61\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u4e92\u8865\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5d4c\u5165\u76f8\u4f3c\u6027\u4fe1\u53f7\u3002", "result": "GLSim\u5728\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u4e86\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "GLSim\u901a\u8fc7\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5d4c\u5165\u76f8\u4f3c\u6027\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u5bf9\u8c61\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5e94\u5bf9\u5404\u79cd\u573a\u666f\u3002"}}
{"id": "2508.19852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19852", "abs": "https://arxiv.org/abs/2508.19852", "authors": ["Binjie Zhang", "Mike Zheng Shou"], "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "comment": "Code: github.com/binjiezhang/Ego-PM (branch: main)", "summary": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u9884\u6d4b\u4ee5\u81ea\u8eab\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u4e2d\u7684\u672a\u6765\u52a8\u4f5c\u53ca\u5176\u89c6\u89c9\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6709\u6548\u5730\u6a21\u62df\u52a8\u4f5c\u548c\u89c6\u89c9\u573a\u666f\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u9884\u6d4b\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5904\u7406\u5f02\u6784\u8f93\u5165\uff08\u89c6\u89c9\u89c2\u5bdf\u3001\u8bed\u8a00\u3001\u52a8\u4f5c\u5386\u53f2\uff09\u5e76\u9884\u6d4b\u624b\u90e8\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u56e0\u679c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u63a8\u65ad\u7684\u52a8\u4f5c\u4fe1\u53f7\u6307\u5bfc\u57fa\u4e8e\u56fe\u50cf\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u8fdb\u884c\u89c6\u9891\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Ego4D\u3001BridgeData\u548cRLBench\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5728\u52a8\u4f5c\u9884\u6d4b\u548c\u672a\u6765\u89c6\u9891\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u4ee5\u81ea\u8eab\u4e3a\u4e2d\u5fc3\u7684\u6d3b\u52a8\u7406\u89e3\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u80fd\u591f\u663e\u5f0f\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u52a8\u4f5c\u53ca\u5176\u89c6\u89c9\u540e\u679c\u3002"}}
{"id": "2508.19999", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19999", "abs": "https://arxiv.org/abs/2508.19999", "authors": ["Ziniu Zhang", "Zhenshuo Zhang", "Dongyue Li", "Lu Wang", "Jennifer Dy", "Hongyang R. Zhang"], "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation", "comment": "19 pages. To appear in EMNLP'25", "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ecen\u4e2a\u793a\u4f8b\u4e2d\u9009\u62e9k\u4e2a\u7528\u4e8ein-context learning\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5728in-context learning\u4e2d\uff0c\u5982\u4f55\u4ece\u7ed9\u5b9a\u7684n\u4e2a\u793a\u4f8b\u4e2d\u9009\u62e9k\u4e2a\u6700\u4f73\u793a\u4f8b\u4f5c\u4e3a\u4e0b\u6e38\u63a8\u7406\u7684\u6761\u4ef6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728prompt tuning\u548cchain-of-thought reasoning\u9886\u57df\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u51fa\u68af\u5ea6\u5728\u8f93\u5165\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\uff0c\u7136\u540e\u5c06\u6b64\u4f30\u8ba1\u5e94\u7528\u4e8e\u591a\u4e2a\u968f\u673a\u91c7\u6837\u7684\u5b50\u96c6\u3002\u6700\u540e\uff0c\u901a\u8fc7\u805a\u5408\u91c7\u6837\u5b50\u96c6\u7684\u8f93\u51fa\u6765\u5f62\u6210\u6bcf\u4e2a\u6f14\u793a\u793a\u4f8b\u7684\u5f71\u54cd\u5f97\u5206\uff0c\u5e76\u9009\u62e9k\u4e2a\u6700\u76f8\u5173\u7684\u793a\u4f8b\u3002\u6b64\u8fc7\u7a0b\u4ec5\u9700\u4e00\u6b21\u9884\u8ba1\u7b97\u6a21\u578b\u8f93\u51fa\u548c\u68af\u5ea6\uff0c\u76f8\u5bf9\u4e8e\u6a21\u578b\u548c\u8bad\u7ec3\u96c6\u5927\u5c0f\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u68af\u5ea6\u4f30\u8ba1\u8fc7\u7a0b\u5728\u516d\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5c0f\u4e8e1%\u7684\u8bef\u5dee\uff0c\u80fd\u591f\u5c06\u5b50\u96c6\u9009\u62e9\u7684\u89c4\u6a21\u6269\u5927\u9ad8\u8fbe37.7\u500d\uff08\u5728\u62e5\u6709340\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\uff09\uff0c\u5e76\u4e14\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u8f93\u5165\u5d4c\u5165\u7684\u73b0\u6709\u9009\u62e9\u65b9\u6cd511%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3ain-context learning\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19862", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19862", "abs": "https://arxiv.org/abs/2508.19862", "authors": ["Long Chen", "Ashiv Patel", "Mengyun Qiao", "Mohammad Yousuf Salmasi", "Salah A. Hammouche", "Vasilis Stavrinides", "Jasleen Nagi", "Soodeh Kalaie", "Xiao Yun Xu", "Wenjia Bai", "Declan P. O'Regan"], "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction", "comment": null, "summary": "Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.", "AI": {"tldr": "MCMeshGAN\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u6a21\u6001\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u4e3b\u52a8\u8109\u7624\u76843D\u751f\u957f\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8KNN\u5377\u79ef\u7f51\u7edc\u548c\u5168\u5c40\u56fe\u5377\u79ef\u7f51\u7edc\u6765\u7cbe\u786e\u6355\u6349\u51e0\u4f55\u7ec6\u8282\u548c\u957f\u671f\u7ed3\u6784\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728TAAMesh\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e2a\u6027\u5316\u3001\u7cbe\u786e\u5730\u9884\u6d4b\u4e3b\u52a8\u8109\u7624\u8fdb\u5c55\u5bf9\u4e8e\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u6a21\u62df\u590d\u6742\u76843D\u51e0\u4f55\u5f62\u72b6\u5185\u7684\u7ec6\u5fae\u5c40\u90e8\u53d8\u5f62\u548c\u6574\u4f53\u89e3\u5256\u7ed3\u6784\u53d8\u5316\uff0c\u56e0\u6b64\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCMeshGAN\u7684\u591a\u6a21\u6001\u6761\u4ef6\u7f51\u683c\u5230\u7f51\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3002\u8be5\u7f51\u7edc\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u7ed3\u5408\u4e86\u65b0\u9896\u7684\u57fa\u4e8eKNN\u7684\u5c40\u90e8\u5377\u79ef\u7f51\u7edc\uff08KCN\uff09\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u51e0\u4f55\u7ec6\u8282\uff0c\u4ee5\u53ca\u5168\u5c40\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u6765\u6355\u6349\u957f\u7a0b\u7ed3\u6784\u4e0a\u4e0b\u6587\uff0c\u514b\u670d\u4e86\u6df1\u5ea6GCN\u7684\u8fc7\u5ea6\u5e73\u6ed1\u9650\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u4e13\u95e8\u7684\u6761\u4ef6\u5206\u652f\uff0c\u7528\u4e8e\u7f16\u7801\u4e34\u5e8a\u5c5e\u6027\uff08\u5e74\u9f84\u3001\u6027\u522b\uff09\u548c\u76ee\u6807\u65f6\u95f4\u95f4\u9694\uff0c\u4ee5\u751f\u6210\u5728\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u4e14\u53d7\u65f6\u95f4\u63a7\u5236\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u56de\u987e\u6027\u548c\u524d\u77bb\u6027\u5efa\u6a21\u3002", "result": "\u5728\u5305\u542b590\u4e2a\u591a\u6a21\u6001\u8bb0\u5f55\uff08CT\u626b\u63cf\u30013D\u7f51\u683c\u548c\u4e34\u5e8a\u6570\u636e\uff09\u7684208\u540d\u60a3\u8005\u7684\u7eb5\u5411\u80f8\u4e3b\u52a8\u8109\u7624\u7f51\u683c\u6570\u636e\u96c6\uff08TAAMesh\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cMCMeshGAN\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u4e34\u5e8a\u4e0a\u91cd\u8981\u76f4\u5f84\u4f30\u8ba1\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MCMeshGAN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u671d\u7740\u4e34\u5e8a\u5e94\u7528\u548c\u4e2a\u6027\u53163D\u75be\u75c5\u8f68\u8ff9\u5efa\u6a21\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.20013", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20013", "abs": "https://arxiv.org/abs/2508.20013", "authors": ["Lotte Gross", "Rebecca Walter", "Nicole Zoppi", "Adrien Justus", "Alessandro Gambetti", "Qiwei Han", "Maximilian Kaiser"], "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach", "comment": "10 pages, 5 figures, 3 tables", "summary": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5206\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u5206\u7c7b\u4e2d\u7684\u5e73\u53f0\u5f02\u6784\u6027\u548c\u73b0\u6709\u5206\u7c7b\u6cd5\u7684\u7ed3\u6784\u9650\u5236\u95ee\u9898\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6587\u672c\uff08RoBERTa\uff09\u3001\u89c6\u89c9\uff08ViT\uff09\u548c\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\uff08CLIP\uff09\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u63a9\u853d\u589e\u5f3a\u7684\u5206\u5c42\u67b6\u6784\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7MLP\u8fdb\u884c\u665a\u671f\u878d\u5408\u7684CLIP\u5d4c\u5165\u5728\u5206\u5c42F1\u5f97\u5206\u4e0a\u8fbe\u5230\u4e8698.59%\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u201c\u4ea7\u54c1\u91cd\u5206\u7c7b\u201d\u6d41\u7a0b\uff0c\u5229\u7528SimCLR\u3001UMAP\u548c\u7ea7\u8054\u805a\u7c7b\u53d1\u73b0\u4e86\u65b0\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff0c\u805a\u7c7b\u7eaf\u5ea6\u9ad8\u4e8e86%\u3002\u8de8\u5e73\u53f0\u5b9e\u9a8c\u8868\u660e\uff0c\u590d\u6742\u7684\u665a\u671f\u878d\u5408\u65b9\u6cd5\u5728\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4e0a\u80fd\u6700\u5927\u5316\u51c6\u786e\u6027\uff0c\u800c\u7b80\u5355\u7684\u65e9\u671f\u878d\u5408\u65b9\u6cd5\u5bf9\u672a\u89c1\u8fc7\u7684\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u6700\u7ec8\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff08\u8f7b\u91cf\u7ea7RoBERTa\u548cGPU\u52a0\u901f\u7684\u591a\u6a21\u6001\u9636\u6bb5\uff09\u5728EURWEB\u5546\u4e1a\u4ea4\u6613\u667a\u80fd\u5e73\u53f0\u6210\u529f\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u548c\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u5e73\u53f0\u5f02\u6784\u6027\u548c\u73b0\u6709\u5206\u7c7b\u6cd5\u7ed3\u6784\u9650\u5236\u4e24\u5927\u5173\u952e\u5de5\u4e1a\u6311\u6218\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5206\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u6765\u81ea40\u4e2a\u56fd\u9645\u65f6\u5c1a\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u7684271,700\u4e2a\u4ea7\u54c1\u7684\u6587\u672c\u7279\u5f81\uff08RoBERTa\uff09\u3001\u89c6\u89c9\u7279\u5f81\uff08ViT\uff09\u4ee5\u53ca\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\uff08CLIP\uff09\u3002\u4ed6\u4eec\u7814\u7a76\u4e86\u5305\u62ec\u65e9\u671f\u878d\u5408\u3001\u665a\u671f\u878d\u5408\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u878d\u5408\u5728\u5185\u7684\u878d\u5408\u7b56\u7565\uff0c\u5e76\u5728\u5206\u5c42\u67b6\u6784\u4e2d\u5f15\u5165\u4e86\u52a8\u6001\u63a9\u853d\u4ee5\u786e\u4fdd\u5206\u7c7b\u6cd5\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u201c\u4ea7\u54c1\u91cd\u5206\u7c7b\u201d\u6d41\u7a0b\uff08\u4f7f\u7528SimCLR\u3001UMAP\u548c\u7ea7\u8054\u805a\u7c7b\uff09\u6765\u89e3\u51b3\u5206\u7c7b\u6d45\u5c42\u6216\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7MLP\u8fdb\u884c\u665a\u671f\u878d\u5408\u7684CLIP\u5d4c\u5165\u5728\u5206\u5c42F1\u5f97\u5206\u4e0a\u8fbe\u5230\u4e8698.59%\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u57fa\u7ebf\u3002\u81ea\u76d1\u7763\u7684\u201c\u4ea7\u54c1\u91cd\u5206\u7c7b\u201d\u6d41\u7a0b\u53d1\u73b0\u4e86\u65b0\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff08\u4f8b\u5982\u201c\u978b\u5b50\u201d\u7684\u5b50\u7c7b\u578b\uff09\uff0c\u5176\u805a\u7c7b\u7eaf\u5ea6\u9ad8\u4e8e86%\u3002\u8de8\u5e73\u53f0\u5b9e\u9a8c\u8868\u660e\uff0c\u590d\u6742\u7684\u665a\u671f\u878d\u5408\u65b9\u6cd5\u5728\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4e0a\u80fd\u6700\u5927\u5316\u51c6\u786e\u6027\uff0c\u800c\u7b80\u5355\u7684\u65e9\u671f\u878d\u5408\u65b9\u6cd5\u5bf9\u672a\u89c1\u8fc7\u7684\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u6700\u7ec8\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u7ed3\u5408\u8f7b\u91cf\u7ea7RoBERTa\u9636\u6bb5\u548cGPU\u52a0\u901f\u591a\u6a21\u6001\u9636\u6bb5\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff0c\u6210\u529f\u90e8\u7f72\u5728EURWEB\u7684\u5546\u4e1a\u4ea4\u6613\u667a\u80fd\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u548c\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5206\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u5206\u7c7b\u4e2d\u7684\u5e73\u53f0\u5f02\u6784\u6027\u548c\u5206\u7c7b\u6cd5\u9650\u5236\u95ee\u9898\u3002\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u7279\u5f81\u548c\u521b\u65b0\u7684\u878d\u5408\u7b56\u7565\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u7684\u81ea\u76d1\u7763\u91cd\u5206\u7c7b\u6d41\u7a0b\u80fd\u591f\u53d1\u73b0\u65b0\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\u3002\u8be5\u6846\u67b6\u5728\u5de5\u4e1a\u754c\u7684\u6210\u529f\u90e8\u7f72\u8bc1\u660e\u4e86\u5176\u5728\u5e73\u8861\u6210\u672c\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.19864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19864", "abs": "https://arxiv.org/abs/2508.19864", "authors": ["Oussama Hadjerci", "Antoine Letienne", "Mohamed Abbas Hedjazi", "Adel Hafiane"], "title": "Self-supervised structured object representation learning", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u8868\u5f81\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5168\u5c40\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6355\u6349\u573a\u666f\u7ed3\u6784\u5316\u8868\u5f81\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u7ec4\u3001\u5b9e\u4f8b\u7ea7\u5206\u79bb\u548c\u5206\u5c42\u7ed3\u6784\u5316\u6765\u9010\u6b65\u6784\u5efa\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u5f81\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u65b0\u9896\u7684ProtoScale\u6a21\u5757\uff0c\u80fd\u591f\u8de8\u591a\u4e2a\u7a7a\u95f4\u5c3a\u5ea6\u6355\u6349\u89c6\u89c9\u5143\u7d20\uff0c\u5e76\u4fdd\u7559\u5b8c\u6574\u7684\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63d0\u9ad8\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "result": "\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u8868\u5f81\u80fd\u591f\u63d0\u5347\u76d1\u7763\u5f0f\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u548c\u8f83\u5c11\u7684\u5fae\u8c03\u8f6e\u6570\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u4f18\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u5f81\uff0c\u5c24\u5176\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u548c\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.20015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20015", "abs": "https://arxiv.org/abs/2508.20015", "authors": ["Julian Arnold", "Niels L\u00f6rch"], "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment", "comment": "11+25 pages, 4+11 figures", "summary": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.", "AI": {"tldr": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u5bfc\u81f4\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e7f\u6cdb\u4e0d\u7b26\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u548c\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u5e76\u7531LLM\u8bc4\u4f30\u7684\u5e8f\u53c2\u6570\uff0c\u6765\u68c0\u6d4b\u548c\u63cf\u8ff0\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5feb\u901f\u8f6c\u53d8\u3002\u7814\u7a76\u4f7f\u7528\u5ba2\u89c2\u7684\u7edf\u8ba1\u4e0d\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u91cf\u5316\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u201c\u76f8\u53d8\u201d\u5bf9\u6a21\u578b\u591a\u4e2a\u65b9\u9762\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u89e3\u4e86\u6574\u4f53\u8f6c\u53d8\u4e2d\u5404\u65b9\u9762\uff08\u5982\u5bf9\u9f50\u3001\u5197\u957f\uff09\u6240\u5360\u7684\u6bd4\u4f8b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u884c\u4e3a\u8f6c\u53d8\u5b9e\u9645\u53d1\u751f\u7684\u65f6\u95f4\u665a\u4e8e\u68af\u5ea6\u8303\u6570\u5cf0\u503c\u6307\u793a\u7684\u65f6\u95f4\u3002\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u91cf\u5316\u57fa\u4e8e\u8bed\u8a00\u7684\u5e8f\u53c2\u6570\uff0c\u5e76\u5df2\u5728\u77e5\u8bc6\u95ee\u7b54\u3001\u653f\u6cbb\u548c\u4f26\u7406\u7b49\u793a\u4f8b\u4e2d\u5f97\u5230\u8bc1\u660e\u3002", "motivation": "\u7406\u89e3\u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fc7\u7a0b\u4e2d\uff0c\u5f53\u6a21\u578b\u5728\u7279\u5b9a\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u4f1a\u53d1\u751f\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u7b26\u7684\u201c\u6d8c\u73b0\u5f0f\u5931\u51c6\u201d\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u548c\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u7684\u5e8f\u53c2\u6570\uff08\u7531LLM\u8bc4\u4f30\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u63cf\u8ff0\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5feb\u901f\u8f6c\u53d8\u3002\u4f7f\u7528\u5ba2\u89c2\u7684\u7edf\u8ba1\u4e0d\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u76f8\u53d8\u5bf9\u6a21\u578b\u8f93\u51fa\u591a\u4e2a\u65b9\u9762\uff08\u5982\u5bf9\u9f50\u3001\u5197\u957f\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u5206\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u884c\u4e3a\u8f6c\u53d8\u5b9e\u9645\u53d1\u751f\u7684\u65f6\u95f4\u665a\u4e8e\u68af\u5ea6\u8303\u6570\u5cf0\u503c\u6307\u793a\u7684\u65f6\u95f4\u3002\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u91cf\u5316\u57fa\u4e8e\u8bed\u8a00\u7684\u5e8f\u53c2\u6570\uff0c\u5e76\u5df2\u5728\u77e5\u8bc6\u95ee\u7b54\u3001\u653f\u6cbb\u548c\u4f26\u7406\u7b49\u793a\u4f8b\u4e2d\u5f97\u5230\u8bc1\u660e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u91cf\u5316LLM\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u8f6c\u53d8\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u7f13\u89e3\u6a21\u578b\u5931\u51c6\u95ee\u9898\u3002\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u91cf\u5316\u57fa\u4e8e\u8bed\u8a00\u7684\u5e8f\u53c2\u6570\uff0c\u5e76\u53ef\u5728\u4e0d\u540c\u9886\u57df\u8fdb\u884c\u5e94\u7528\u3002"}}
{"id": "2508.19866", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19866", "abs": "https://arxiv.org/abs/2508.19866", "authors": ["Fran\u00e7ois G. Landry", "Moulay A. Akhloufi"], "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations", "comment": "This work has been submitted to IEEE Transactions on Intelligent\n  Vehicles for possible publication", "summary": "With the introduction of vehicles with autonomous capabilities on public\nroads, predicting pedestrian crossing intention has emerged as an active area\nof research. The task of predicting pedestrian crossing intention involves\ndetermining whether pedestrians in the scene are likely to cross the road or\nnot. In this work, we propose TrajFusionNet, a novel transformer-based model\nthat combines future pedestrian trajectory and vehicle speed predictions as\npriors for predicting crossing intention. TrajFusionNet comprises two branches:\na Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM\nbranch learns from a sequential representation of the observed and predicted\npedestrian trajectory and vehicle speed. Complementarily, the VAM branch\nenables learning from a visual representation of the predicted pedestrian\ntrajectory by overlaying predicted pedestrian bounding boxes onto scene images.\nBy utilizing a small number of lightweight modalities, TrajFusionNet achieves\nthe lowest total inference time (including model runtime and data\npreprocessing) among current state-of-the-art approaches. In terms of\nperformance, it achieves state-of-the-art results across the three most\ncommonly used datasets for pedestrian crossing intention prediction.", "AI": {"tldr": "TrajFusionNet\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4eba\u8f68\u8ff9\u548c\u8f66\u8f86\u901f\u5ea6\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u884c\u4eba\u6a2a\u7a7f\u610f\u56fe\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u516c\u5171\u9053\u8def\u4e0a\u7684\u666e\u53ca\uff0c\u9884\u6d4b\u884c\u4eba\u6a2a\u7a7f\u610f\u56fe\u5df2\u6210\u4e3a\u4e00\u9879\u91cd\u8981\u7684\u7814\u7a76\u8bfe\u9898\uff0c\u4ee5\u786e\u4fdd\u884c\u4eba\u548c\u8f66\u8f86\u7684\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrajFusionNet\u7684\u65b0\u578bTransformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\uff08SAM\uff09\u548c\u4e00\u4e2a\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5757\uff08VAM\uff09\u3002SAM\u4ece\u89c2\u5bdf\u5230\u548c\u9884\u6d4b\u7684\u884c\u4eba\u8f68\u8ff9\u4ee5\u53ca\u8f66\u8f86\u901f\u5ea6\u7684\u5e8f\u5217\u8868\u793a\u4e2d\u5b66\u4e60\uff0c\u800cVAM\u5219\u901a\u8fc7\u5c06\u9884\u6d4b\u7684\u884c\u4eba\u8fb9\u754c\u6846\u53e0\u52a0\u5230\u573a\u666f\u56fe\u50cf\u4e0a\u6765\u5b66\u4e60\u89c6\u89c9\u8868\u793a\u3002", "result": "TrajFusionNet\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u4e2d\u6700\u4f4e\u7684\u603b\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u5728\u4e09\u4e2a\u6700\u5e38\u7528\u7684\u884c\u4eba\u6a2a\u7a7f\u610f\u56fe\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TrajFusionNet\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u884c\u4eba\u8f68\u8ff9\u548c\u8f66\u8f86\u901f\u5ea6\u9884\u6d4b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u884c\u4eba\u6a2a\u7a7f\u610f\u56fe\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u662f\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.19875", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19875", "abs": "https://arxiv.org/abs/2508.19875", "authors": ["Hui Zhang", "Jianghui Cai", "Haifeng Yang", "Ali Luo", "Yuqing Yang", "Xiao Kong", "Zhichao Ding", "Lichan Zhou", "Qin Han"], "title": "Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network", "comment": null, "summary": "Sky background subtraction is a critical step in Multi-objective Fiber\nspectra process. However, current subtraction relies mainly on sky fiber\nspectra to build Super Sky. These average spectra are lacking in the modeling\nof the environment surrounding the objects. To address this issue, a sky\nbackground estimation model: Sky background building based on Mutual\nInformation (SMI) is proposed. SMI based on mutual information and incremental\ntraining approach. It utilizes spectra from all fibers in the plate to estimate\nthe sky background. SMI contains two main networks, the first network applies a\nwavelength calibration module to extract sky features from spectra, and can\neffectively solve the feature shift problem according to the corresponding\nemission position. The second network employs an incremental training approach\nto maximize mutual information between representations of different spectra to\ncapturing the common component. Then, it minimizes the mutual information\nbetween adjoining spectra representations to obtain individual components. This\nnetwork yields an individual sky background at each location of the object. To\nverify the effectiveness of the method in this paper, we conducted experiments\non the spectra of LAMOST. Results show that SMI can obtain a better object sky\nbackground during the observation, especially in the blue end.", "AI": {"tldr": "SMI\u662f\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u548c\u589e\u91cf\u8bad\u7ec3\u7684\u65b0\u7684\u5929\u4f53\u80cc\u666f\u4f30\u8ba1\u6a21\u578b\uff0c\u5b83\u5229\u7528\u6765\u81ea\u540c\u4e00\u89c6\u573a\u4e2d\u6240\u6709\u5149\u7ea4\u7684\u5149\u8c31\u6765\u4f30\u8ba1\u5929\u4f53\u80cc\u666f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5929\u4f53\u80cc\u666f\u6263\u9664\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5929\u7a7a\u5149\u7ea4\u5149\u8c31\u6765\u6784\u5efa\u8d85\u7ea7\u5929\u7a7a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6a21\u62df\u76ee\u6807\u5468\u56f4\u7684\u73af\u5883\uff0c\u9650\u5236\u4e86\u5176\u5728\u5149\u8c31\u5206\u6790\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "SMI\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7f51\u7edc\uff1a\u7b2c\u4e00\u4e2a\u7f51\u7edc\u5e94\u7528\u6ce2\u957f\u6821\u51c6\u6a21\u5757\u63d0\u53d6\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u89e3\u51b3\u7279\u5f81\u504f\u79fb\u95ee\u9898\uff1b\u7b2c\u4e8c\u4e2a\u7f51\u7edc\u91c7\u7528\u589e\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6700\u5927\u5316\u4e0d\u540c\u5149\u8c31\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4ee5\u6355\u6349\u5171\u540c\u6210\u5206\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u76f8\u90bb\u5149\u8c31\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4ee5\u83b7\u5f97\u4e2a\u4f53\u6210\u5206\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u76ee\u6807\u4f4d\u7f6e\u751f\u6210\u4e2a\u4f53\u5929\u4f53\u80cc\u666f\u3002", "result": "\u5728LAMOST\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMI\u6a21\u578b\u80fd\u591f\u83b7\u5f97\u66f4\u597d\u7684\u76ee\u6807\u5929\u4f53\u80cc\u666f\uff0c\u5c24\u5176\u662f\u5728\u5149\u8c31\u7684\u84dd\u7aef\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SMI\u6a21\u578b\u901a\u8fc7\u5229\u7528\u540c\u4e00\u89c6\u573a\u4e2d\u7684\u6240\u6709\u5149\u8c31\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u4e92\u4fe1\u606f\u548c\u589e\u91cf\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u5929\u4f53\u80cc\u666f\uff0c\u7279\u522b\u662f\u5728\u84dd\u7aef\u5149\u8c31\u533a\u57df\uff0c\u4e3a\u591a\u76ee\u6807\u5149\u8c31\u5904\u7406\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.20021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20021", "abs": "https://arxiv.org/abs/2508.20021", "authors": ["Felix M\u00f6hrlein", "Martin K\u00e4ppel", "Julian Neuberger", "Sven Weinzierl", "Lars Ackermann", "Martin Matzner", "Stefan Jablonski"], "title": "FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring", "comment": "Proceedings of the Best BPM Dissertation Award, Doctoral Consortium,\n  and Demonstrations & Resources Forum co-located with 23rd International\n  Conference on Business Process Management (BPM 2025), Seville, Spain, August\n  31st to September 5th, 2025", "summary": "Sensitive attributes like gender or age can lead to unfair predictions in\nmachine learning tasks such as predictive business process monitoring,\nparticularly when used without considering context. We present FairLoop1, a\ntool for human-guided bias mitigation in neural network-based prediction\nmodels. FairLoop distills decision trees from neural networks, allowing users\nto inspect and modify unfair decision logic, which is then used to fine-tune\nthe original model towards fairer predictions. Compared to other approaches to\nfairness, FairLoop enables context-aware bias removal through human\ninvolvement, addressing the influence of sensitive attributes selectively\nrather than excluding them uniformly.", "AI": {"tldr": "FairLoop\u662f\u4e00\u6b3e\u7528\u4e8e\u795e\u7ecf1\u7f51\u7edc\u9884\u6d4b\u6a21\u578b\u7684\u4eba\u5de5\u5f15\u5bfc\u504f\u89c1\u7f13\u89e3\u5de5\u5177\u3002\u5b83\u53ef\u4ee5\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u51b3\u7b56\u6811\uff0c\u5141\u8bb8\u7528\u6237\u68c0\u67e5\u548c\u4fee\u6539\u4e0d\u516c\u5e73\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u7136\u540e\u7528\u4e8e\u5fae\u8c03\u539f\u59cb\u6a21\u578b\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u9884\u6d4b\u3002\u4e0e\u5176\u5b83\u516c\u5e73\u6027\u65b9\u6cd5\u76f8\u6bd4\uff0cFairLoop\u901a\u8fc7\u4eba\u4e3a\u5e72\u9884\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u89c1\u53bb\u9664\uff0c\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u89e3\u51b3\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u4e00\u6982\u800c\u8bba\u5730\u6392\u9664\u5b83\u4eec\u3002", "motivation": "\u5728\u8bf8\u5982\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u7b49\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u50cf\u6027\u522b\u6216\u5e74\u9f84\u8fd9\u6837\u7684\u654f\u611f\u5c5e\u6027\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u672a\u8003\u8651\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u5b83\u4eec\u3002", "method": "FairLoop\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u51b3\u7b56\u6811\uff0c\u5141\u8bb8\u7528\u6237\u68c0\u67e5\u548c\u4fee\u6539\u4e0d\u516c\u5e73\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u7136\u540e\u7528\u5176\u6765\u5fae\u8c03\u539f\u59cb\u6a21\u578b\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u9884\u6d4b\u3002", "result": "FairLoop\u4e0e\u5176\u5b83\u516c\u5e73\u6027\u65b9\u6cd5\u76f8\u6bd4\uff0c\u901a\u8fc7\u4eba\u4e3a\u5e72\u9884\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u89c1\u53bb\u9664\uff0c\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u89e3\u51b3\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u4e00\u6982\u800c\u8bba\u5730\u6392\u9664\u5b83\u4eec\u3002", "conclusion": "FairLoop\u901a\u8fc7\u8ba9\u4eba\u7c7b\u53c2\u4e0e\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u5dee\u53bb\u9664\uff0c\u53ef\u4ee5\u6709\u9009\u62e9\u5730\u5904\u7406\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u4e00\u6982\u800c\u8bba\u5730\u6392\u9664\u5b83\u4eec\u3002"}}
{"id": "2508.20032", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20032", "abs": "https://arxiv.org/abs/2508.20032", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Pruning Strategies for Backdoor Defense in LLMs", "comment": "Accepted in CIKM '25: The 34th ACM International Conference on\n  Information and Knowledge Management Proceedings", "summary": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.", "AI": {"tldr": "\u540e\u95e8\u653b\u51fb\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6784\u6210\u5a01\u80c1\uff0c\u5373\u4f7f\u5728\u5fae\u8c03\u540e\u4ecd\u7136\u5b58\u5728\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6ce8\u610f\u529b\u5934\u526a\u679d\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u4e86\u89e3\u653b\u51fb\u89e6\u53d1\u5668\u6216\u5e72\u51c0\u53c2\u8003\u6a21\u578b\u5373\u53ef\u7f13\u89e3\u8fd9\u4e9b\u5a01\u80c1\u7684\u65b9\u6cd5\u3002", "motivation": "\u540e\u95e8\u653b\u51fb\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5373\u4f7f\u7ecf\u8fc7\u5fae\u8c03\u4e5f\u96be\u4ee5\u9632\u5fa1\uff0c\u56e0\u4e3a\u7528\u6237\u901a\u5e38\u4e0d\u77e5\u9053\u653b\u51fb\u89e6\u53d1\u5668\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u516d\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u7b56\u7565\uff1a(i) \u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\uff0c(ii) \u5c42\u7ea7\u65b9\u5dee\u526a\u679d\uff0c(iii) \u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\u4e0e\u7ed3\u6784\u5316L1/L2\u7a00\u758f\u5316\uff0c(iv) \u968f\u673a\u96c6\u6210\u526a\u679d\uff0c(v) \u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u526a\u679d\uff0c(vi) \u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u526a\u679d\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u76d1\u63a7\u9a8c\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u8fed\u4ee3\u5730\u79fb\u9664\u4fe1\u606f\u91cf\u6700\u5c11\u7684\u5934\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\u5728\u9632\u5fa1\u53e5\u6cd5\u89e6\u53d1\u5668\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u526a\u679d\u5728\u62b5\u5fa1\u98ce\u683c\u653b\u51fb\u65b9\u9762\u66f4\u6709\u6548\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u526a\u679d\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u65b9\u6cd5\uff0c\u4e0d\u540c\u7684\u526a\u679d\u7b56\u7565\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u89e6\u53d1\u5668\uff08\u53e5\u6cd5\u6216\u98ce\u683c\uff09\u6709\u4e0d\u540c\u7684\u9632\u5fa1\u6548\u679c\u3002"}}
{"id": "2508.19881", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19881", "abs": "https://arxiv.org/abs/2508.19881", "authors": ["Narges Takhtkeshha", "Gabriele Mazzacca", "Fabio Remondino", "Juha Hyypp\u00e4", "Gottfried Mandlburger"], "title": "Multispectral LiDAR data for extracting tree points in urban and suburban areas", "comment": null, "summary": "Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.", "AI": {"tldr": "\u5229\u7528\u591a\u5149\u8c31LiDAR\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08SPT\u3001PTv3\u3001PTv1\uff09\u8fdb\u884c\u57ce\u5e02\u6811\u6728\u63d0\u53d6\uff0cSPT\u6a21\u578b\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff08mIoU\u4e3a85.28%\uff09\uff0c\u7ed3\u5408pNDVI\u548c\u7a7a\u95f4\u6570\u636e\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u57ce\u5e02\u7eff\u5316\u653f\u7b56\u548c\u964d\u4f4e\u5bf9\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u7684\u98ce\u9669\uff0c\u76d1\u6d4b\u57ce\u5e02\u6811\u6728\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\u6280\u672f\u5728\u6811\u6728\u7ba1\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u548c\u9762\u5bf9\u591a\u6837\u7684\u6811\u6728\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u591a\u5149\u8c31\uff08MS\uff09\u6fc0\u5149\u96f7\u8fbe\uff08LiDAR\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u8fdb\u884c\u6811\u6728\u70b9\u63d0\u53d6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u7684\u6a21\u578b\uff1aSuperpoint Transformer (SPT)\u3001Point Transformer V3 (PTv3) \u548c Point Transformer V1 (PTv1)\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSPT\u6a21\u578b\u5177\u6709\u663e\u8457\u7684\u65f6\u95f4\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u8fbe\u523085.28%\u3002\u5c06\u4f2a\u5f52\u4e00\u5316\u5dee\u5f02\u690d\u88ab\u6307\u6570\uff08pNDVI\uff09\u4e0e\u7a7a\u95f4\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u76f8\u6bd4\u4ec5\u4f7f\u7528\u7a7a\u95f4\u4fe1\u606f\u7684\u65b9\u6cd5\u63d0\u9ad8\uff0c\u9519\u8bef\u7387\u964d\u4f4e10.61\u4e2a\u767e\u5206\u70b9\uff08pp\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u591a\u5149\u8c31LiDAR\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u6539\u8fdb\u6811\u6728\u63d0\u53d6\u548c\u5b8c\u5584\u6811\u6728\u6e05\u5355\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.20024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20024", "abs": "https://arxiv.org/abs/2508.20024", "authors": ["Deddy Jobson", "Muktti Shukla", "Phuong Dinh", "Julio Christian Young", "Nick Pitton", "Nina Chen", "Ryan Ginstrom"], "title": "Using item recommendations and LLMs in marketing email titles", "comment": "Accepted to The Second Workshop on Generative AI for E-commerce\n  (GenAIECommerce '25), held September 22, 2025, in Prague, Czech Republic. 3\n  figures", "summary": "E-commerce marketplaces make use of a number of marketing channels like\nemails, push notifications, etc. to reach their users and stimulate purchases.\nPersonalized emails especially are a popular touch point for marketers to\ninform users of latest items in stock, especially for those who stopped\nvisiting the marketplace. Such emails contain personalized recommendations\ntailored to each user's interests, enticing users to buy relevant items. A\ncommon limitation of these emails is that the primary entry point, the title of\nthe email, tends to follow fixed templates, failing to inspire enough interest\nin the contents. In this work, we explore the potential of large language\nmodels (LLMs) for generating thematic titles that reflect the personalized\ncontent of the emails. We perform offline simulations and conduct online\nexperiments on the order of millions of users, finding our techniques useful in\nimproving the engagement between customers and our emails. We highlight key\nfindings and learnings as we productionize the safe and automated generation of\nemail titles for millions of users.", "AI": {"tldr": "LLM\u53ef\u7528\u4e8e\u751f\u6210\u4e2a\u6027\u5316\u7535\u5b50\u90ae\u4ef6\u7684\u4f18\u5316\u6807\u9898\uff0c\u4ee5\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u6a21\u677f\uff0c\u672a\u80fd\u5145\u5206\u6fc0\u53d1\u7528\u6237\u5174\u8da3\uff0c\u9700\u8981\u66f4\u5177\u5438\u5f15\u529b\u7684\u6807\u9898\u3002", "method": "\u5229\u7528LLM\u6280\u672f\u751f\u6210\u80fd\u591f\u53cd\u6620\u4e2a\u6027\u5316\u90ae\u4ef6\u5185\u5bb9\u7684 theadline\uff0c\u5e76\u901a\u8fc7\u7ebf\u4e0b\u6a21\u62df\u548c\u7ebf\u4e0a\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u6807\u9898\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u4e0e\u90ae\u4ef6\u7684\u53c2\u4e0e\u5ea6\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7528\u6237\u90ae\u4ef6\u6807\u9898\u7684\u751f\u6210\u3002", "conclusion": "LLM\u5728\u751f\u6210\u4e2a\u6027\u5316\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5b89\u5168\u5730\u81ea\u52a8\u5316\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2508.19895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19895", "abs": "https://arxiv.org/abs/2508.19895", "authors": ["Ziyun Qian", "Runyu Xiao", "Shuyuan Tu", "Wei Xue", "Dingkang Yang", "Mingcheng Li", "Dongliang Kou", "Minghao Han", "Zizhi Chen", "Lihua Zhang"], "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos", "comment": null, "summary": "Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5230\u89c6\u9891\u52a8\u4f5c\u4e2a\u6027\u5316\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aPersonaAnimator\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u76f4\u63a5\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u52a8\u4f5c\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u52a8\u4f5c\u8fc1\u79fb\u65b9\u6cd5\u7f3a\u4e4f\u98ce\u683c\u5b66\u4e60\u3001\u4f9d\u8d56\u52a8\u4f5c\u6355\u6349\u6570\u636e\u4ee5\u53ca\u751f\u6210\u52a8\u4f5c\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u7684\u95ee\u9898\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u53d1\u5e03\u4e86\u9996\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u4e2a\u6027\u5316\u52a8\u4f5c\u6570\u636e\u96c6PersonaVid\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u52a8\u4f5c\u98ce\u683c\u6b63\u5219\u5316\u673a\u5236\u4ee5\u589e\u5f3a\u751f\u6210\u52a8\u4f5c\u7684\u7269\u7406\u5408\u7406\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPersonaAnimator\u5728\u52a8\u4f5c\u8fc1\u79fb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u59ff\u6001\u5f15\u5bfc\u7684\u52a8\u4f5c\u8fc1\u79fb\u65b9\u6cd5\u4ec5\u80fd\u590d\u5236\u52a8\u4f5c\u800c\u65e0\u6cd5\u5b66\u4e60\u5176\u98ce\u683c\u7279\u5f81\uff0c\u5bfc\u81f4\u89d2\u8272\u7f3a\u4e4f\u8868\u73b0\u529b\uff1b\u52a8\u4f5c\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff1b\u751f\u6210\u7684\u52a8\u4f5c\u6709\u65f6\u4f1a\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u89c6\u9891\u5230\u89c6\u9891\u52a8\u4f5c\u4e2a\u6027\u5316\u8fd9\u4e00\u65b0\u4efb\u52a1\u3002", "method": "\u63d0\u51faPersonaAnimator\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u52a8\u4f5c\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u52a8\u4f5c\u8fc1\u79fb\u3002\u53d1\u5e03PersonaVid\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u4e2a\u6027\u5316\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b20\u4e2a\u52a8\u4f5c\u5185\u5bb9\u7c7b\u522b\u548c120\u4e2a\u52a8\u4f5c\u98ce\u683c\u7c7b\u522b\u3002\u63d0\u51fa\u7269\u7406\u611f\u77e5\u52a8\u4f5c\u98ce\u683c\u6b63\u5219\u5316\u673a\u5236\uff0c\u5f3a\u5236\u6267\u884c\u751f\u6210\u52a8\u4f5c\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "result": "PersonaAnimator\u5728\u52a8\u4f5c\u8fc1\u79fb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u52a8\u4f5c\u4e2a\u6027\u5316\u4efb\u52a1\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002", "conclusion": "PersonaAnimator\u901a\u8fc7\u76f4\u63a5\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u52a8\u4f5c\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u52a8\u4f5c\u8fc1\u79fb\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u52a8\u4f5c\u98ce\u683c\u5316\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002PersonaVid\u6570\u636e\u96c6\u548c\u7269\u7406\u611f\u77e5\u52a8\u4f5c\u98ce\u683c\u6b63\u5219\u5316\u673a\u5236\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2508.20056", "categories": ["cs.LG", "90-08, 90B35, 90C59, 90C99, 68T20, 90C27"], "pdf": "https://arxiv.org/pdf/2508.20056", "abs": "https://arxiv.org/abs/2508.20056", "authors": ["Vil\u00e9m Heinz", "Petr Vil\u00edm", "Zden\u011bk Hanz\u00e1lek"], "title": "Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks", "comment": null, "summary": "Failure-Directed Search (FDS) is a significant complete generic search\nalgorithm used in Constraint Programming (CP) to efficiently explore the search\nspace, proven particularly effective on scheduling problems. This paper\nanalyzes FDS's properties, showing that minimizing the size of its search tree\nguided by ranked branching decisions is closely related to the Multi-armed\nbandit (MAB) problem. Building on this insight, MAB reinforcement learning\nalgorithms are applied to FDS, extended with problem-specific refinements and\nparameter tuning, and evaluated on the two most fundamental scheduling\nproblems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained\nProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best\nextended MAB algorithm and configuration, performs 1.7 times faster on the JSSP\nand 2.1 times faster on the RCPSP benchmarks compared to the original\nimplementation in a new solver called OptalCP, while also being 3.5 times\nfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the\ncurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,\nusing only a 900-second time limit per instance, the enhanced FDS improved the\nexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP\nstandard open benchmark instances while also completely closing a few of them.", "AI": {"tldr": " Failure-Directed Search (FDS) \u662f\u4e00\u79cd\u7528\u4e8e\u7ea6\u675f\u89c4\u5212 (CP) \u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e0e\u591a\u81c2\u8001\u864e\u673a (MAB) \u95ee\u9898\u7684\u5173\u8054\uff0c\u5e76\u5e94\u7528 MAB \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u6269\u5c55\u548c\u8c03\u4f18\uff0c\u5728 Job Shop Scheduling Problem (JSSP) \u548c Resource-Constrained Project Scheduling Problem (RCPSP) \u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6bd4\u539f\u59cb\u5b9e\u73b0\u548c\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u66f4\u5feb\uff0c\u5e76\u6539\u8fdb\u4e86\u8bb8\u591a\u57fa\u51c6\u5b9e\u4f8b\u7684\u4e0b\u754c\u3002", "motivation": "\u5c06\u591a\u81c2\u8001\u864e\u673a (MAB) \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8e\u7ea6\u675f\u89c4\u5212 (CP) \u4e2d\u7684 Failure-Directed Search (FDS) \u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u8c03\u5ea6\u95ee\u9898\u4e0a\u7684\u641c\u7d22\u6548\u7387\u3002", "method": "\u5c06 FDS \u4e0e MAB \u95ee\u9898\u5173\u8054\uff0c\u5e94\u7528 MAB \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u95ee\u9898\u7279\u5b9a\u7684\u6539\u8fdb\u548c\u53c2\u6570\u8c03\u4f18\uff0c\u5728 OptalCP \u6c42\u89e3\u5668\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u589e\u5f3a\u7684 FDS \u5728 JSSP \u4e0a\u6bd4\u539f\u59cb\u5b9e\u73b0\u5feb 1.7 \u500d\uff0c\u6bd4 IBM CP Optimizer \u5feb 3.5 \u500d\uff1b\u5728 RCPSP \u4e0a\u6bd4\u539f\u59cb\u5b9e\u73b0\u5feb 2.1 \u500d\uff0c\u6bd4 IBM CP Optimizer \u5feb 2.1 \u500d\u3002\u5728 900 \u79d2\u7684\u65f6\u95f4\u9650\u5236\u5185\uff0c\u589e\u5f3a\u7684 FDS \u6539\u8fdb\u4e86 84 \u4e2a JSSP \u5b9e\u4f8b\u4e2d\u7684 78 \u4e2a\u548c 393 \u4e2a RCPSP \u5b9e\u4f8b\u4e2d\u7684 226 \u4e2a\u7684\u73b0\u6709\u6700\u5148\u8fdb\u4e0b\u754c\u3002", "conclusion": "\u901a\u8fc7\u5c06 MAB \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8e FDS \u5e76\u8fdb\u884c\u76f8\u5e94\u7684\u6269\u5c55\u548c\u8c03\u4f18\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8 FDS \u5728 JSSP \u548c RCPSP \u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4e0b\u754c\u3002"}}
{"id": "2508.19906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19906", "abs": "https://arxiv.org/abs/2508.19906", "authors": ["Moussa Kassem Sbeyti", "Nadja Klein", "Michelle Karg", "Christian Wirth", "Sahin Albayrak"], "title": "Streamlining the Development of Active Learning Methods in Real-World Object Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5bf9\u8c61\u96c6\u76f8\u4f3c\u6027\uff08OSS\uff09\u7684\u65b0\u6307\u6807\uff0c\u4ee5\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u68c0\u6d4b\u4e2d\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u9762\u4e34\u7684\u8ba1\u7b97\u548c\u53ef\u9760\u6027\u6311\u6218\u3002OSS \u65e0\u9700\u8bad\u7ec3\u68c0\u6d4b\u5668\u5373\u53ef\u91cf\u5316 AL \u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u9009\u62e9\u6709\u4ee3\u8868\u6027\u7684\u9a8c\u8bc1\u96c6\u4ee5\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\uff0c\u4ece\u800c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u65f6\uff0c\u4e3a\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u90e8\u7f72 AL \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u9762\u4e34\u8ba1\u7b97\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u76ee\u524d\u9700\u8981\u8bad\u7ec3\u591a\u4e2a\u68c0\u6d4b\u5668\u6765\u8bc4\u4f30\u65b0\u7684 AL \u65b9\u6cd5\uff0c\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u6210\u672c\u9ad8\u6602\uff08\u5355\u4e2a\u68c0\u6d4b\u5668\u8bad\u7ec3\u9ad8\u8fbe 282 GPU \u5c0f\u65f6\uff09\u3002\u6b64\u5916\uff0cAL \u65b9\u6cd5\u7684\u6392\u540d\u56e0\u9a8c\u8bc1\u96c6\u800c\u5f02\uff0c\u5f71\u54cd\u4e86\u5b89\u5168\u5173\u952e\u7684\u4ea4\u901a\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u5bf9\u8c61\u96c6\u76f8\u4f3c\u6027\uff08OSS\uff09\u6307\u6807\uff0c\u8be5\u6307\u6807\u901a\u8fc7\u6d4b\u91cf\u8bad\u7ec3\u96c6\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u57fa\u4e8e\u5bf9\u8c61\u7ea7\u7279\u5f81\u7684\u76f8\u4f3c\u6027\u6765\u91cf\u5316 AL \u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u8fdb\u884c\u68c0\u6d4b\u5668\u8bad\u7ec3\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u8bad\u7ec3\u524d\u6392\u9664\u65e0\u6548\u7684 AL \u65b9\u6cd5\u3002OSS \u8fd8\u6709\u52a9\u4e8e\u9009\u62e9\u6709\u4ee3\u8868\u6027\u7684\u9a8c\u8bc1\u96c6\u4ee5\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u5728 KITTI\u3001BDD100K \u548c CODA \u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4ee5\u4e0d\u786e\u5b9a\u6027\u4e3a\u57fa\u7840\u7684 AL \u65b9\u6cd5\u548c EfficientDet\u3001YOLOv3 \u4e24\u79cd\u68c0\u6d4b\u5668\u67b6\u6784\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "OSS \u6307\u6807\u65e0\u9700\u8bad\u7ec3\u68c0\u6d4b\u5668\u5373\u53ef\u91cf\u5316 AL \u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u80fd\u9009\u62e9\u6709\u4ee3\u8868\u6027\u7684\u9a8c\u8bc1\u96c6\u4ee5\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8868\u660e\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OSS \u6307\u6807\u4e3a\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u90e8\u7f72 AL \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u65b9\u9762\u7684\u5173\u952e\u95ee\u9898\u3002\u8be5\u6307\u6807\u662f\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u8c61\u76f8\u4f3c\u6027\u7edf\u4e00 AL \u8bad\u7ec3\u548c\u8bc4\u4f30\u7b56\u7565\u7684\u5de5\u4f5c\uff0c\u5177\u6709\u68c0\u6d4b\u5668\u65e0\u5173\u3001\u4ec5\u9700\u6807\u6ce8\u7684\u5bf9\u8c61\u88c1\u526a\u4ee5\u53ca\u6613\u4e8e\u4e0e\u73b0\u6709 AL \u6d41\u6c34\u7ebf\u96c6\u6210\u7b49\u4f18\u70b9\u3002"}}
{"id": "2508.11692", "categories": ["eess.SP", "cs.AI", "cs.LG", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11692", "abs": "https://arxiv.org/abs/2508.11692", "authors": ["Eduardo Di Santi", "Ruixiang Ci", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Jonathan Brown", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025, Dresden,\n  Germany. Conference: https://tu-dresden.de/raildresden2025. Book of\n  abstracts: https://tu-dresden.de/raildresden2025/BoA.pdf. 8 pages, 6 figures,\n  1 table", "summary": "The Point Machine (PM) is a critical piece of railway equipment that switches\ntrain routes by diverting tracks through a switchblade. As with any critical\nsafety equipment, a failure will halt operations leading to service\ndisruptions; therefore, pre-emptive maintenance may avoid unnecessary\ninterruptions by detecting anomalies before they become failures. Previous work\nrelies on several inputs and crafting custom features by segmenting the signal.\nThis not only adds additional requirements for data collection and processing,\nbut it is also specific to the PM technology, the installed locations and\noperational conditions limiting scalability. Based on the available maintenance\nrecords, the main failure causes for PM are obstacles, friction, power source\nissues and misalignment. Those failures affect the energy consumption pattern\nof PMs, altering the usual (or healthy) shape of the power signal during the PM\nmovement. In contrast to the current state-of-the-art, our method requires only\none input. We apply a deep learning model to the power signal pattern to\nclassify if the PM is nominal or associated with any failure type, achieving\n>99.99\\% precision, <0.01\\% false positives and negligible false negatives. Our\nmethodology is generic and technology-agnostic, proven to be scalable on\nseveral electromechanical PM types deployed in both real-world and test bench\nenvironments. Finally, by using conformal prediction the maintainer gets a\nclear indication of the certainty of the system outputs, adding a confidence\nlayer to operations and making the method compliant with the ISO-17359\nstandard.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u5355\u4e00\u8f93\u5165\uff08\u7535\u529b\u4fe1\u53f7\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u94c1\u8def\u9053\u5c94\uff08Point Machine, PM\uff09\u7684\u6545\u969c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u7535\u529b\u4fe1\u53f7\u6a21\u5f0f\u6765\u8bc6\u522bPM\u7684\u5f02\u5e38\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc799.99%\u7684\u7cbe\u786e\u7387\u3001\u4f4e\u4e8e0.01%\u7684\u8bef\u62a5\u7387\u4ee5\u53ca\u53ef\u5ffd\u7565\u7684\u6f0f\u62a5\u7387\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6280\u672f\u65e0\u5173\u6027\u3001\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u7ed3\u5408\u4e86\u4fdd\u5f62\u9884\u6d4b\u4ee5\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\uff0c\u7b26\u5408ISO-17359\u6807\u51c6\u3002", "motivation": "\u94c1\u8def\u9053\u5c94\uff08PM\uff09\u662f\u5173\u952e\u7684\u5b89\u5168\u8bbe\u5907\uff0c\u5176\u6545\u969c\u4f1a\u5bfc\u81f4\u8fd0\u8425\u4e2d\u65ad\u3002\u5148\u524d\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u591a\u79cd\u8f93\u5165\u548c\u5b9a\u5236\u7279\u5f81\uff0c\u5b58\u5728\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u8981\u6c42\u9ad8\u3001\u6280\u672f\u548c\u573a\u666f\u9650\u5236\u7b49\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u901a\u7528\u3001\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9700\u6c42\u66f4\u5c11\u7684\u65b9\u6cd5\u6765\u9884\u6d4bPM\u6545\u969c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528PM\u8fd0\u884c\u65f6\u7684\u7535\u529b\u4fe1\u53f7\u4f5c\u4e3a\u5355\u4e00\u8f93\u5165\u3002\u8be5\u6a21\u578b\u5206\u6790\u7535\u529b\u4fe1\u53f7\u7684\u6a21\u5f0f\uff0c\u4ee5\u533a\u5206PM\u7684\u6b63\u5e38\u72b6\u6001\u548c\u4e0e\u4e0d\u540c\u6545\u969c\u7c7b\u578b\u76f8\u5173\u7684\u72b6\u6001\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4fdd\u5f62\u9884\u6d4b\u6280\u672f\uff0c\u4e3a\u6a21\u578b\u7684\u8f93\u51fa\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u3002", "result": "\u8be5\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4bPM\u6545\u969c\u65b9\u9762\u53d6\u5f97\u4e86\u6781\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u7cbe\u786e\u7387\u8d85\u8fc799.99%\uff0c\u8bef\u62a5\u7387\u4f4e\u4e8e0.01%\uff0c\u6f0f\u62a5\u7387\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u8be5\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u5728\u591a\u79cd\u673a\u7535PM\u7c7b\u578b\u548c\u4e0d\u540c\u8fd0\u884c\u73af\u5883\uff08\u5b9e\u9645\u8fd0\u884c\u548c\u6d4b\u8bd5\u5e73\u53f0\uff09\u4e2d\u5747\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u5355\u4e00\u7535\u529b\u4fe1\u53f7\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u53ef\u9760\u6027\u5730\u68c0\u6d4b\u94c1\u8def\u9053\u5c94\u7684\u6545\u969c\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u7ed3\u5408\u4fdd\u5f62\u9884\u6d4b\u540e\uff0c\u8be5\u65b9\u6cd5\u6ee1\u8db3\u4e86ISO-17359\u6807\u51c6\uff0c\u4e3a\u94c1\u8def\u5b89\u5168\u8fd0\u7ef4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19909", "abs": "https://arxiv.org/abs/2508.19909", "authors": ["Lechun You", "Zhonghua Wu", "Weide Liu", "Xulei Yang", "Jun Cheng", "Wei Zhou", "Bharadwaj Veeravalli", "Guosheng Lin"], "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation", "comment": null, "summary": "Current methods for 3D semantic segmentation propose training models with\nlimited annotations to address the difficulty of annotating large, irregular,\nand unordered 3D point cloud data. They usually focus on the 3D domain only,\nwithout leveraging the complementary nature of 2D and 3D data. Besides, some\nmethods extend original labels or generate pseudo labels to guide the training,\nbut they often fail to fully use these labels or address the noise within them.\nMeanwhile, the emergence of comprehensive and adaptable foundation models has\noffered effective solutions for segmenting 2D data. Leveraging this\nadvancement, we present a novel approach that maximizes the utility of sparsely\navailable 3D annotations by incorporating segmentation masks generated by 2D\nfoundation models. We further propagate the 2D segmentation masks into the 3D\nspace by establishing geometric correspondences between 3D scenes and 2D views.\nWe extend the highly sparse annotations to encompass the areas delineated by 3D\nmasks, thereby substantially augmenting the pool of available labels.\nFurthermore, we apply confidence- and uncertainty-based consistency\nregularization on augmentations of the 3D point cloud and select the reliable\npseudo labels, which are further spread on the 3D masks to generate more\nlabels. This innovative strategy bridges the gap between limited 3D annotations\nand the powerful capabilities of 2D foundation models, ultimately improving the\nperformance of 3D weakly supervised segmentation.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\u548c3D\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b33D\u6570\u636e\u6807\u6ce8\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u89c4\u5219\u3001\u65e0\u5e8f\u76843D\u70b9\u4e91\u6570\u636e\u65f6\uff0c\u901a\u5e38\u53ea\u5173\u6ce83D\u57df\uff0c\u5ffd\u7565\u4e862D\u548c3D\u6570\u636e\u7684\u4e92\u8865\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u6269\u5c55\u6807\u7b7e\u6216\u4f2a\u6807\u7b7e\u65f6\uff0c\u5b58\u5728\u5229\u7528\u4e0d\u5145\u5206\u548c\u566a\u58f0\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u54082D\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5206\u5272\u63a9\u7801\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u5bf9\u5e94\u5173\u7cfb\u5c062D\u63a9\u7801\u4f20\u64ad\u52303D\u7a7a\u95f4\uff0c\u4ece\u800c\u6700\u5927\u5316\u5229\u7528\u7a00\u758f\u76843D\u6807\u6ce8\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u8fde\u7eed\u6027\u6b63\u5219\u5316\u6765\u9009\u62e9\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u52303D\u63a9\u7801\u4ee5\u751f\u6210\u66f4\u591a\u6807\u7b7e\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5f25\u8865\u4e863D\u6807\u6ce8\u7684\u5c40\u9650\u6027\u4e0e2D\u57fa\u7840\u6a21\u578b\u80fd\u529b\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5f31\u76d1\u7763\u5206\u5272\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u6709\u6548\u5229\u7528\u4e862D\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u7b56\u7565\u589e\u5f3a\u4e863D\u6570\u636e\u7684\u6807\u6ce8\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u63d0\u5347\u3002"}}
{"id": "2508.19927", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19927", "abs": "https://arxiv.org/abs/2508.19927", "authors": ["Fayaz Ali", "Muhammad Zawish", "Steven Davy", "Radu Timofte"], "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution", "comment": "10 pages, 5 figures", "summary": "Transformers have demonstrated promising performance in computer vision\ntasks, including image super-resolution (SR). The quadratic computational\ncomplexity of window self-attention mechanisms in many transformer-based SR\nmethods forces the use of small, fixed windows, limiting the receptive field.\nIn this paper, we propose a new approach by embedding the wavelet transform\nwithin a hierarchical transformer framework, called (WaveHiT-SR). First, using\nadaptive hierarchical windows instead of static small windows allows to capture\nfeatures across different levels and greatly improve the ability to model\nlong-range dependencies. Secondly, the proposed model utilizes wavelet\ntransforms to decompose images into multiple frequency subbands, allowing the\nnetwork to focus on both global and local features while preserving structural\ndetails. By progressively reconstructing high-resolution images through\nhierarchical processing, the network reduces computational complexity without\nsacrificing performance. The multi-level decomposition strategy enables the\nnetwork to capture fine-grained information in lowfrequency components while\nenhancing high-frequency textures. Through extensive experimentation, we\nconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refined\nversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR\nresults, achieving higher efficiency with fewer parameters, lower FLOPs, and\nfaster speeds.", "AI": {"tldr": "WaveHiT-SR\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u5c42\u6b21\u5316Transformer\uff0c\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u8868\u73b0\uff0c\u4f18\u4e8eSwinIR-Light\u3001SwinIR-NG\u548cSRFormer-Light\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u65f6\u53d7\u5230\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u53ea\u80fd\u4f7f\u7528\u5c0f\u7684\u3001\u56fa\u5b9a\u7684\u7a97\u53e3\uff0c\u9650\u5236\u4e86\u611f\u53d7\u91ce\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347Transformer\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWaveHiT-SR\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u5c0f\u6ce2\u53d8\u6362\u5d4c\u5165\u5230\u5c42\u6b21\u5316Transformer\u6846\u67b6\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5c42\u6b21\u5316\u7a97\u53e3\u66ff\u4ee3\u9759\u6001\u5c0f\u7a97\u53e3\u6765\u6355\u6349\u4e0d\u540c\u5c42\u7ea7\u7684\u7279\u5f81\u5e76\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\uff1b\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u9891\u6bb5\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u540c\u65f6\u5173\u6ce8\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5e76\u4fdd\u7559\u7ed3\u6784\u7ec6\u8282\uff1b\u901a\u8fc7\u5c42\u6b21\u5316\u5904\u7406\u9010\u6b65\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u91c7\u7528\u591a\u5c42\u5206\u89e3\u7b56\u7565\u6765\u6355\u6349\u4f4e\u9891\u5206\u91cf\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u5e76\u589e\u5f3a\u9ad8\u9891\u7eb9\u7406\u3002", "result": "WaveHiT-SR\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002\u5176\u6539\u8fdb\u7248\u672c\u5728SwinIR-Light\u3001SwinIR-NG\u548cSRFormer-Light\u7684\u57fa\u7840\u4e0a\uff0c\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\uff0c\u540c\u65f6\u62e5\u6709\u66f4\u5c11\u7684\u53c2\u6570\u3001\u66f4\u4f4e\u7684\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u548c\u66f4\u5feb\u7684\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "WaveHiT-SR\u901a\u8fc7\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u81ea\u9002\u5e94\u5c42\u6b21\u5316\u7a97\u53e3\u7684Transformer\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.19946", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2508.19946", "abs": "https://arxiv.org/abs/2508.19946", "authors": ["Gianluca Guzzetta"], "title": "Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework", "comment": "13 pages", "summary": "In this paper, we present a comprehensive study and analysis of the Chan-Vese\nalgorithm for image segmentation. We employ a discretized scheme derived from\nthe empirical study of the Chan-Vese model's functional energy and its partial\ndifferential equation based on its level set function. We provide a proof of\nthe results and an implementation using MATLAB. Leveraging modern computer\nvision methodologies, we propose a functional segmentation loss based on active\ncontours, utilizing pytorch.nn.ModuleLoss and a level set based on the\nChan-Vese algorithm. We compare our results with common computer vision\nsegmentation datasets and evaluate the performance of classical loss functions\nagainst our proposed method. All code and materials used are available at\nhttps://github.com/gguzzy/chan_vese_functional_loss.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChan-Vese\u7b97\u6cd5\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528PyTorch\u5b9e\u73b0\u548c\u8bc4\u4f30\u3002", "motivation": "Chan-Vese\u7b97\u6cd5\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4e3b\u52a8\u8f6e\u5ed3\u548c\u6c34\u5e73\u96c6\u51fd\u6570\u7684Chan-Vese\u7b97\u6cd5\u7684\u5206\u5272\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChan-Vese\u7b97\u6cd5\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u5176\u80fd\u91cf\u51fd\u6570\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u7ed3\u5408PyTorch\u7684nn.ModuleLoss\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u5272\u635f\u5931\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u5728\u5e38\u89c1\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c06\u65b0\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u4e0e\u7ecf\u5178\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u63d0\u51fa\u7684\u57fa\u4e8eChan-Vese\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u5728\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u4f18\u4e8e\u4e00\u4e9b\u7ecf\u5178\u65b9\u6cd5\u3002"}}
{"id": "2508.19967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19967", "abs": "https://arxiv.org/abs/2508.19967", "authors": ["Oliver Grainge", "Sania Waheed", "Jack Stilgoe", "Michael Milford", "Shoaib Ehsan"], "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models", "comment": "Accepted to AAAI Fall Symposium 2025 on AI Trustworthiness and Risk\n  Assessment for Challenging Contexts (ATRACC)", "summary": "Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.", "AI": {"tldr": "Vision-Language Models (VLMs) used for image geo-localization pose privacy risks, especially with their increasing precision. This paper assesses 25 state-of-the-art VLMs on diverse datasets, finding poor performance on generic street-level images but high accuracy (61%) on social media-like images, highlighting significant privacy concerns.", "motivation": "To address the lack of systematic evaluation on the geolocation precision of Generative VLMs, their limits, and potential for unintended inferences, given the increasing capabilities of VLMs in image geo-localization and associated privacy risks.", "method": "Conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments.", "result": "Current VLMs perform poorly on generic street-level images but achieve notably high accuracy (61%) on images resembling social media content.", "conclusion": "Current VLMs exhibit significant privacy risks due to their high accuracy in geo-locating social media-like images, despite poor performance on generic street-level images. Further research is needed to understand and mitigate these risks."}}
{"id": "2508.20020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20020", "abs": "https://arxiv.org/abs/2508.20020", "authors": ["Yuhao Chen", "Shubin Chen", "Liang Lin", "Guangrun Wang"], "title": "GS: Generative Segmentation via Label Diffusion", "comment": "12 pages, 7 figures, 5 tables", "summary": "Language-driven image segmentation is a fundamental task in vision-language\nunderstanding, requiring models to segment regions of an image corresponding to\nnatural language expressions. Traditional methods approach this as a\ndiscriminative problem, assigning each pixel to foreground or background based\non semantic alignment. Recently, diffusion models have been introduced to this\ndomain, but existing approaches remain image-centric: they either (i) use image\ndiffusion models as visual feature extractors, (ii) synthesize segmentation\ndata via image generation to train discriminative models, or (iii) perform\ndiffusion inversion to extract attention cues from pre-trained image diffusion\nmodels-thereby treating segmentation as an auxiliary process. In this paper, we\npropose GS (Generative Segmentation), a novel framework that formulates\nsegmentation itself as a generative task via label diffusion. Instead of\ngenerating images conditioned on label maps and text, GS reverses the\ngenerative process: it directly generates segmentation masks from noise,\nconditioned on both the input image and the accompanying language description.\nThis paradigm makes label generation the primary modeling target, enabling\nend-to-end training with explicit control over spatial and semantic fidelity.\nTo demonstrate the effectiveness of our approach, we evaluate GS on Panoptic\nNarrative Grounding (PNG), a representative and challenging benchmark for\nmultimodal segmentation that requires panoptic-level reasoning guided by\nnarrative captions. Experimental results show that GS significantly outperforms\nexisting discriminative and diffusion-based methods, setting a new\nstate-of-the-art for language-driven segmentation.", "AI": {"tldr": "GS\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u8bed\u8a00\u9a71\u52a8\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u4ece\u566a\u58f0\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u5728PNG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u9a71\u52a8\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u5224\u522b\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4ecd\u4ee5\u56fe\u50cf\u4e3a\u4e2d\u5fc3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u5206\u5272\u672c\u8eab\u4f5c\u4e3a\u4e00\u79cd\u751f\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51faGS\uff08Generative Segmentation\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u5c06\u5206\u5272\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u6761\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u3002", "result": "GS\u5728Panoptic Narrative Grounding\uff08PNG\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5728\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002", "conclusion": "GS\u6846\u67b6\u901a\u8fc7\u5c06\u5206\u5272\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.20029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20029", "abs": "https://arxiv.org/abs/2508.20029", "authors": ["Manogna Sreenivas", "Soma Biswas"], "title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World", "comment": "Accepted at BMVC 2025", "summary": "In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSegAssist\u7684\u589e\u91cf\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08ITTA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6d4b\u8bd5\u9636\u6bb5\u9047\u5230\u7684\u65b0\u7c7b\u522b\u548c\u65b0\u9886\u57df\u95ee\u9898\uff0c\u5141\u8bb8\u6a21\u578b\u540c\u65f6\u9002\u5e94\u534f\u53d8\u91cf\u548c\u6807\u7b7e\u504f\u79fb\uff0c\u5e76\u4e3b\u52a8\u6574\u5408\u65b0\u51fa\u73b0\u7684\u7c7b\u522b\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u5df2\u8bad\u7ec3\u6a21\u578b\u5728\u9047\u5230\u672a\u77e5\u5bf9\u8c61\u548c\u5206\u5e03\u53d8\u5316\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4f1a\u53d7\u5230\u6311\u6218\u3002\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u589e\u91cf\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08ITTA\uff09\u95ee\u9898\uff0c\u5904\u7406\u6d4b\u8bd5\u671f\u95f4\u4e0d\u65ad\u51fa\u73b0\u672a\u89c1\u7c7b\u522b\u548c\u672a\u89c1\u9886\u57df\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSegAssist\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u8f85\u52a9\u4e3b\u52a8\u6807\u7b7e\u9009\u62e9\u7684\u6a21\u5757\u3002\u8be5\u6a21\u5757\u5229\u7528VLM\u7684\u5206\u5272\u80fd\u529b\u6765\u6539\u8fdb\u4e3b\u52a8\u6837\u672c\u9009\u62e9\uff0c\u4f18\u5148\u9009\u62e9\u53ef\u80fd\u5c5e\u4e8e\u672a\u89c1\u7c7b\u522b\u7684\u6837\u672c\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u7528\u4e8eVLM\u7684\u5355\u5f20\u56fe\u50cfTTA\u65b9\u6cd5\u548c\u4e3b\u52a8\u6807\u7b7e\u6280\u672f\uff0c\u4ee5\u4fbf\u5728\u6d4b\u8bd5\u65f6\u4e3a\u53ef\u80fd\u4ee3\u8868\u672a\u89c1\u7c7b\u522b\u7684\u6837\u672c\u67e5\u8be2Oracle\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cSegAssist\u80fd\u591f\u63d0\u9ad8VLM\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u573a\u666f\u9700\u8981\u5bf9\u65b0\u51fa\u73b0\u7684\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9002\u5e94\u3002", "conclusion": "SegAssist\u80fd\u591f\u589e\u5f3aVLM\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u573a\u666f\u9700\u8981\u5bf9\u65b0\u51fa\u73b0\u7684\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9002\u5e94\u3002"}}
{"id": "2508.20063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20063", "abs": "https://arxiv.org/abs/2508.20063", "authors": ["Peng-Hao Hsu", "Ke Zhang", "Fu-En Wang", "Tao Tu", "Ming-Feng Li", "Yu-Lun Liu", "Albert Y. C. Chen", "Min Sun", "Cheng-Hao Kuo"], "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations", "comment": "ICCV2025", "summary": "Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.", "AI": {"tldr": "OpenM3D\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u591a\u89c6\u56fe\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5b83\u5728\u6ca1\u6709\u4eba\u7c7b\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728ScanNet200\u548cARKitScenes\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\uff08OV\uff093D\u76ee\u6807\u68c0\u6d4b\u662f\u4e00\u4e2a\u65b0\u5174\u9886\u57df\uff0c\u4f46\u4e0e\u57fa\u4e8e3D\u70b9\u4e91\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u7684\u63a2\u7d22\u4ecd\u7136\u6709\u9650\u3002", "method": "OpenM3D\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u5b83\u91c7\u7528ImGeoNet\u6a21\u578b\u76842D\u8bf1\u5bfc\u4f53\u7d20\u7279\u5f81\u3002\u4e3a\u4e86\u652f\u6301OV\uff0c\u5b83\u4e0e\u9700\u8981\u9ad8\u8d28\u91cf3D\u4f2a\u6846\u7684\u7c7b\u65e0\u51733D\u5b9a\u4f4d\u635f\u5931\u548c\u9700\u8981\u591a\u6837\u5316\u9884\u8bad\u7ec3CLIP\u7279\u5f81\u7684\u4f53\u7d20-\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u8054\u5408\u8bad\u7ec3\u3002\u901a\u8fc7\u56fe\u5d4c\u5165\u6280\u672f\u751f\u62103D\u4f2a\u6846\uff0c\u5e76\u4ece\u4e0e\u6bcf\u4e2a\u8fde\u8d2f3D\u7ed3\u6784\u76f8\u5173\u8054\u76842D\u7247\u6bb5\u4e2d\u91c7\u6837CLIP\u7279\u5f81\u4ee5\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "OpenM3D\u5728ScanNet200\u548cARKitScenes\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\uff0c\u63a8\u7406\u901f\u5ea6\u4e3a0.3\u79d2/\u573a\u666f\u3002\u5b83\u8fd8\u4f18\u4e8e\u5229\u7528\u5176\u7c7b\u65e0\u5173\u68c0\u6d4b\u5668\u548c\u57fa\u4e8eViT CLIP\u7684OV\u5206\u7c7b\u5668\u7684\u5f3a\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7ed3\u5408\u591a\u89c6\u56fe\u6df1\u5ea6\u4f30\u8ba1\u5668\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OpenM3D\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5355\u9636\u6bb5\u5f00\u653e\u8bcd\u6c473D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5b83\u901a\u8fc7\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u5728\u6ca1\u6709\u4eba\u7c7b\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u57fa\u4e8e\u56fe\u50cf\u7684OV 3D\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2508.20064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20064", "abs": "https://arxiv.org/abs/2508.20064", "authors": ["Philippe Zhang", "Weili Jiang", "Yihao Li", "Jing Zhang", "Sarah Matta", "Yubo Tan", "Hui Lin", "Haoshen Wang", "Jiangtian Pan", "Hui Xu", "Laurent Borderie", "Alexandre Le Guilcher", "B\u00e9atrice Cochener", "Chubin Ou", "Gwenol\u00e9 Quellec", "Mathieu Lamard"], "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices", "comment": "10 pages, 5 figures, 3 tables, challenge/conference paper", "summary": "Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting\nvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments\nhave been effective in slowing the progression of neovascular AMD, with better\noutcomes achieved through timely diagnosis and consistent monitoring. Tracking\nthe progression of neovascular activity in OCT scans of patients with exudative\nAMD allows for the development of more personalized and effective treatment\nplans. This was the focus of the Monitoring Age-related Macular Degeneration\nProgression in Optical Coherence Tomography (MARIO) challenge, in which we\nparticipated. In Task 1, which involved classifying the evolution between two\npairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN\nnetwork with model ensembling to further enhance the model's performance. For\nTask 2, which focused on predicting progression over the next three months\nbased on current exam data, we proposed the Patch Progression Masked\nAutoencoder that generates an OCT for the next exam and then classifies the\nevolution between the current OCT and the one generated using our solution from\nTask 1. The results we achieved allowed us to place in the Top 10 for both\ntasks. Some team members are part of the same organization as the challenge\norganizers; therefore, we are not eligible to compete for the prize.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5206\u6790\u773c\u79d1OCT\u626b\u63cf\uff0c\u4ee5\u76d1\u6d4b\u548c\u9884\u6d4b\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\uff08AMD\uff09\u7684\u8fdb\u5c55\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u6cbb\u7597\u65b0\u751f\u8840\u7ba1\u6027AMD\uff0c\u9700\u8981\u5bf9OCT\u626b\u63cf\u4e2d\u7684\u65b0\u751f\u8840\u7ba1\u6d3b\u52a8\u8fdb\u884c\u8ddf\u8e2a\uff0c\u4ee5\u5236\u5b9a\u4e2a\u6027\u5316\u7684\u6cbb\u7597\u8ba1\u5212\u3002", "method": "\u5728MARIO\u6311\u6218\u8d5b\u4e2d\uff0c\u5bf9\u4e8e\u4efb\u52a11\uff08\u5206\u7c7b\u4e24\u4e2a2D\u5207\u7247\u4e4b\u95f4\u7684\u6f14\u53d8\uff09\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u878d\u5408CNN\u7f51\u7edc\u548c\u6a21\u578b\u96c6\u6210\u3002\u5bf9\u4e8e\u4efb\u52a12\uff08\u9884\u6d4b\u672a\u6765\u4e09\u4e2a\u6708\u7684\u8fdb\u5c55\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Patch Progression Masked Autoencoder\uff0c\u8be5\u6a21\u578b\u751f\u6210\u4e0b\u4e00\u4e2a\u68c0\u67e5\u7684OCT\uff0c\u7136\u540e\u5bf9\u5f53\u524dOCT\u548c\u751f\u6210OCT\u4e4b\u95f4\u7684\u6f14\u53d8\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u5747\u8fdb\u5165\u4e86\u524d10\u540d\u3002", "conclusion": "\u5c3d\u7ba1\u6211\u4eec\u5728MARIO\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u4f46\u7531\u4e8e\u90e8\u5206\u56e2\u961f\u6210\u5458\u4e0e\u6bd4\u8d5b\u7ec4\u7ec7\u8005\u5c5e\u4e8e\u540c\u4e00\u673a\u6784\uff0c\u6211\u4eec\u4e0d\u5177\u5907\u83b7\u5956\u8d44\u683c\u3002"}}
{"id": "2508.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20066", "abs": "https://arxiv.org/abs/2508.20066", "authors": ["Zheng Li", "Yanming Guo", "WenZhe Liu", "Xueyi Zhang", "Zhaoyun Ding", "Long Xu", "Mingrui Lao"], "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence", "comment": "10 pages", "summary": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPAUL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u6765\u5904\u7406\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u548c\u8bc1\u636e\u5171\u8bad\u7ec3\u6765\u5212\u5206\u548c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u9009\u62e9\u6027\u5730\u589e\u5f3a\u5177\u6709\u9ad8\u5bf9\u5e94\u7f6e\u4fe1\u5ea6\u7684\u533a\u57df\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u4f18\u5316\u7279\u5f81\u5b66\u4e60\uff0c\u6709\u6548\u6291\u5236\u4e86\u9519\u8bef\u914d\u5bf9\u5f15\u8d77\u7684\u566a\u58f0\uff0c\u5e76\u5728\u5404\u79cd\u566a\u58f0\u6bd4\u7387\u4e0b\u5747\u4f18\u4e8e\u5176\u4ed6\u540c\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u5047\u8bbe\u56fe\u50cf\u5bf9\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u4f46\u8fd9\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5f88\u5c11\u53d1\u751f\u3002GPS\u6f02\u79fb\u7b49\u56e0\u7d20\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u6027\u5bf9\u9f50\u504f\u79fb\uff0c\u4f7f\u5f97\u56fe\u50cf\u5bf9\u4e4b\u95f4\u53ea\u5b58\u5728\u90e8\u5206\u5bf9\u5e94\u5173\u7cfb\u3002\u8fd9\u79cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u53d7\u5230\u7684\u5173\u6ce8\u6709\u9650\u3002", "method": "\u63d0\u51faPAUL\uff08Partition and Augmentation by Uncertainty Learning\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5171\u589e\u5f3a\u548c\u8bc1\u636e\u5171\u8bad\u7ec3\u6765\u5212\u5206\u548c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff0cPAUL\u9009\u62e9\u6027\u5730\u589e\u5f3a\u5177\u6709\u9ad8\u5bf9\u5e94\u7f6e\u4fe1\u5ea6\u7684\u533a\u57df\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u4f18\u5316\u7279\u5f81\u5b66\u4e60\uff0c\u4ece\u800c\u6709\u6548\u6291\u5236\u9519\u8bef\u914d\u5bf9\u5e26\u6765\u7684\u566a\u58f0\u3002 PAUL\u4e0d\u4f9d\u8d56\u4f20\u7edf\u7684\u8fc7\u6ee4\u6216\u6807\u7b7e\u7ea0\u6b63\u65b9\u6cd5\uff0c\u800c\u662f\u5229\u7528\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u548c\u635f\u5931\u5dee\u5f02\u6765\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u5212\u5206\u548c\u589e\u5f3a\uff0c\u4e3a\u566a\u58f0\u6837\u672c\u63d0\u4f9b\u9c81\u68d2\u7684\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPAUL\u6846\u67b6\u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u90fd\u975e\u5e38\u6709\u6548\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u566a\u58f0\u6bd4\u7387\u4e0b\uff0c\u5176\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u5904\u7406\u566a\u58f0\u5bf9\u5e94\u7684\u5177\u6709\u7ade\u4e89\u529b\u7684\u65b9\u6cd5\u3002", "conclusion": " PAUL\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.20088", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20088", "abs": "https://arxiv.org/abs/2508.20088", "authors": ["Yuxin Guo", "Teng Wang", "Yuying Ge", "Shijie Ma", "Yixiao Ge", "Wei Zou", "Ying Shan"], "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models", "comment": null, "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20089", "abs": "https://arxiv.org/abs/2508.20089", "authors": ["Ross J Gardiner", "Guillaume Mougeot", "Sareh Rowlands", "Benno I Simmons", "Flemming Helsing", "Toke Thomas H\u00f8ye"], "title": "Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors", "comment": null, "summary": "Labelling images of Lepidoptera (moths) from automated camera systems is\nvital for understanding insect declines. However, accurate species\nidentification is challenging due to domain shifts between curated images and\nnoisy field imagery. We propose a lightweight classification approach,\ncombining limited expert-labelled field data with knowledge distillation from\nthe high-performance BioCLIP2 foundation model into a ConvNeXt-tiny\narchitecture. Experiments on 101 Danish moth species from AMI camera systems\ndemonstrate that BioCLIP2 substantially outperforms other methods and that our\ndistilled lightweight model achieves comparable accuracy with significantly\nreduced computational cost. These insights offer practical guidelines for the\ndevelopment of efficient insect monitoring systems and bridging domain gaps for\nfine-grained classification.", "AI": {"tldr": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u89e3\u51b3\u91ce\u5916\u56fe\u50cf\u6570\u636e\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9cde\u7fc5\u76ee\u7269\u79cd\u8bc6\u522b\u3002", "motivation": "\u81ea\u52a8\u76f8\u673a\u7cfb\u7edf\u4e2d\u7684\u9cde\u7fc5\u76ee\uff08\u98de\u86fe\uff09\u56fe\u50cf\u6807\u6ce8\u5bf9\u4e8e\u7406\u89e3\u6606\u866b\u6570\u91cf\u4e0b\u964d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57df\u540d\u8f6c\u79fb\u5bfc\u81f4\u51c6\u786e\u7269\u79cd\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u548c\u9ad8\u6027\u80fdBioCLIP2\u57fa\u7840\u6a21\u578b\u5230ConvNeXt-tiny\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728101\u79cd\u4e39\u9ea6\u98de\u86fe\u7684\u5b9e\u9a8c\u4e2d\uff0cBioCLIP2\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u9ad8\u6548\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u548c\u7f29\u5c0f\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u57df\u540d\u5dee\u8ddd\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.20096", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20096", "abs": "https://arxiv.org/abs/2508.20096", "authors": ["Zeyi Sun", "Yuhang Cao", "Jianze Liang", "Qiushi Sun", "Ziyu Liu", "Zhixiong Zhang", "Yuhang Zang", "Xiaoyi Dong", "Kai Chen", "Dahua Lin", "Jiaqi Wang"], "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning", "comment": "code available at this url: https://github.com/OpenIXCLab/CODA", "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.", "AI": {"tldr": "CODA\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u89c4\u5212\u5668\u548c\u4e13\u7528\u6267\u884c\u5668\uff0c\u89e3\u51b3\u4e86GUI\u81ea\u52a8\u5316\u5728\u79d1\u5b66\u8ba1\u7b97\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709GUI\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u957f\u671f\u89c4\u5212\u548c\u7cbe\u786e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u8ba1\u7b97\u7b49\u6570\u636e\u7a00\u7f3a\u9886\u57df\u3002", "method": "CODA\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\uff08\u4e13\u4e1a\u5316\uff09\u4f7f\u7528\u89e3\u8026\u7684GRPO\u8bad\u7ec3\u6bcf\u4e2a\u5e94\u7528\u7684\u4e13\u5bb6\u89c4\u5212\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff08\u901a\u7528\u5316\uff09\u805a\u5408\u4e13\u4e1a\u5316\u9636\u6bb5\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u5bf9\u6700\u7ec8\u89c4\u5212\u5668\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "CODA\u5728ScienceBoard\u57fa\u51c6\u7684\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5e94\u7528\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u521b\u4e0b\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u65b0\u72b6\u6001\u3002", "conclusion": "CODA\u901a\u8fc7\u5176\u53ef\u8bad\u7ec3\u7684\u7ec4\u5408\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6267\u884c\u80fd\u529b\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GUI\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
