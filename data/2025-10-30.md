<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.CL](#cs.CL) [Total: 66]
- [eess.SY](#eess.SY) [Total: 22]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 18]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 37]
- [cs.RO](#cs.RO) [Total: 30]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.ET](#cs.ET) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 4]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.DS](#cs.DS) [Total: 3]
- [eess.SP](#eess.SP) [Total: 31]
- [cs.DC](#cs.DC) [Total: 7]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes](https://arxiv.org/abs/2510.24734)
*Qirui Hou,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: DrivingScene是一个在线、前馈框架，仅用两张连续的环绕视图图像即可重建4D动态场景，其核心是一个轻量级残差流网络，在学习到的静态场景先验之上预测每个摄像机的非刚性运动，并通过场景流显式地对动态进行建模。


<details>
  <summary>Details</summary>
Motivation: 实时、高保真地重建动态驾驶场景面临复杂动力学和稀疏视图的挑战，现有方法难以平衡质量和效率。

Method: 提出了一种名为DrivingScene的在线、前馈框架，利用轻量级残差流网络预测非刚性运动，并采用粗到精的训练范式。

Result: 在nuScenes数据集上的实验表明，该仅图像的方法能够在线生成高质量的深度、场景流和3D高斯点云，在动态重建和新视图合成方面显著优于现有技术。

Conclusion: DrivingScene框架能够有效地从稀疏的图像输入中重建高质量的动态驾驶场景，并在性能上超越了现有方法。

Abstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is
challenged by complex dynamics and sparse views, with prior methods struggling
to balance quality and efficiency. We propose DrivingScene, an online,
feed-forward framework that reconstructs 4D dynamic scenes from only two
consecutive surround-view images. Our key innovation is a lightweight residual
flow network that predicts the non-rigid motion of dynamic objects per camera
on top of a learned static scene prior, explicitly modeling dynamics via scene
flow. We also introduce a coarse-to-fine training paradigm that circumvents the
instabilities common to end-to-end approaches. Experiments on nuScenes dataset
show our image-only method simultaneously generates high-quality depth, scene
flow, and 3D Gaussian point clouds online, significantly outperforming
state-of-the-art methods in both dynamic reconstruction and novel view
synthesis.

</details>


### [2] [Towards Fine-Grained Human Motion Video Captioning](https://arxiv.org/abs/2510.24767)
*Guorui Song,Guocun Wang,Zhe Huang,Jing Lin,Xuefei Zhe,Jian Li,Haoqian Wang*

Main category: cs.CV

TL;DR: M-ACM模型通过引入运动感知解码和人类姿势恢复来增强视频字幕的准确性，解决了现有方法在捕捉精细动作细节方面的不足，并发布了HMI数据集和HMI-Bench基准。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型在捕捉精细运动细节方面存在困难，导致字幕模糊或语义不一致。

Method: 提出了一种名为M-ACM的新型生成框架，该框架利用从人类姿势恢复中提取的运动表示，通过运动感知解码来明确突出人体动态，以减少幻觉并提高生成字幕的语义保真度和空间对齐性。此外，还发布了HMI数据集（115K视频-描述对）和HMI-Bench基准。 

Result: M-ACM在准确描述复杂的人体运动和细微的时间变化方面显著优于先前的方法。

Conclusion: M-ACM通过结合运动信息，提高了视频字幕的质量，为以运动为中心的视频字幕设定了新的标准。

Abstract: Generating accurate descriptions of human actions in videos remains a
challenging task for video captioning models. Existing approaches often
struggle to capture fine-grained motion details, resulting in vague or
semantically inconsistent captions. In this work, we introduce the
Motion-Augmented Caption Model (M-ACM), a novel generative framework that
enhances caption quality by incorporating motion-aware decoding. At its core,
M-ACM leverages motion representations derived from human mesh recovery to
explicitly highlight human body dynamics, thereby reducing hallucinations and
improving both semantic fidelity and spatial alignment in the generated
captions. To support research in this area, we present the Human Motion Insight
(HMI) Dataset, comprising 115K video-description pairs focused on human
movement, along with HMI-Bench, a dedicated benchmark for evaluating
motion-focused video captioning. Experimental results demonstrate that M-ACM
significantly outperforms previous methods in accurately describing complex
human motions and subtle temporal variations, setting a new standard for
motion-centric video captioning.

</details>


### [3] [Combining SAR Simulators to Train ATR Models with Synthetic Data](https://arxiv.org/abs/2510.24768)
*Benjamin Camus,Julien Houssay,Corentin Le Barbu,Eric Monteux,Cédric Saleun,Christian Cochin*

Main category: cs.CV

TL;DR: 训练深度学习模型进行SAR图像自动目标识别（ATR）时，由于真实数据不足，我们使用合成数据。然而，基于模拟的合成数据泛化能力较差。本文提出结合两种不同但互补的SAR模拟器（MOCEM和Salsa）来生成合成数据集，并使用ADASCA深度学习模型进行训练，在MSTAR数据集上实现了近88%的准确率。


<details>
  <summary>Details</summary>
Motivation: 真实SAR图像自动目标识别（ATR）数据集缺乏，导致基于合成数据训练的深度学习模型泛化能力差。

Method: 结合基于散射中心模型（MOCEM）和基于射线追踪策略（Salsa）的两种SAR模拟器生成合成数据集，并使用ADASCA深度学习模型进行训练。

Result: 在MSTAR数据集上实现了近88%的准确率。

Conclusion: 结合两种不同但互补的SAR模拟器生成合成数据，可以有效提高深度学习模型在SAR图像ATR任务上的泛化能力和准确率。

Abstract: This work aims to train Deep Learning models to perform Automatic Target
Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the
lack of real labelled measurements, we resort to synthetic data produced by SAR
simulators. Simulation offers full control over the virtual environment, which
enables us to generate large and diversified datasets at will. However,
simulations are intrinsically grounded on simplifying assumptions of the real
world (i.e. physical models). Thus, synthetic datasets are not as
representative as real measurements. Consequently, ATR models trained on
synthetic images cannot generalize well on real measurements. Our contributions
to this problem are twofold: on one hand, we demonstrate and quantify the
impact of the simulation paradigm on the ATR. On the other hand, we propose a
new approach to tackle the ATR problem: combine two SAR simulators that are
grounded on different (but complementary) paradigms to produce synthetic
datasets. To this end, we use two simulators: MOCEM, which is based on a
scattering centers model approach, and Salsa, which resorts on a ray tracing
strategy. We train ATR models using synthetic dataset generated both by MOCEM
and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of
almost 88 % on the MSTAR measurements.

</details>


### [4] [Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds](https://arxiv.org/abs/2510.24773)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的点云不确定性评估框架，利用几何特征预测误差，无需高精度参考数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的大规模参考数据，难以获取；需要一种新的方法来量化移动激光扫描点云的不确定性。

Method: 采用随机森林（RF）和XGBoost集成学习模型，学习局部几何特征与点误差之间的关系，并在空间划分的数据集上进行训练和验证。

Result: 两种模型均有效捕捉了几何特征与不确定性之间的非线性关系，平均ROC-AUC值达到0.87以上；其中，高程变化、点密度和局部结构复杂度是预测不确定性的关键几何特征。

Conclusion: 该研究提出的数据驱动框架为大规模点云的质量控制和误差分析提供了可扩展和可适应的基础，克服了传统方法的局限性。

Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point
clouds is essential for ensuring the accuracy and credibility of downstream
applications such as 3D mapping, modeling, and change analysis. Traditional
backward uncertainty modeling heavily rely on high-precision reference data,
which are often costly or infeasible to obtain at large scales. To address this
issue, this study proposes a machine learning-based framework for point-level
uncertainty evaluation that learns the relationship between local geometric
features and point-level errors. The framework is implemented using two
ensemble learning models, Random Forest (RF) and XGBoost, which are trained and
validated on a spatially partitioned real-world dataset to avoid data leakage.
Experimental results demonstrate that both models can effectively capture the
nonlinear relationships between geometric characteristics and uncertainty,
achieving mean ROC-AUC values above 0.87. The analysis further reveals that
geometric features describing elevation variation, point density, and local
structural complexity play a dominant role in predicting uncertainty. The
proposed framework offers a data-driven perspective of uncertainty evaluation,
providing a scalable and adaptable foundation for future quality control and
error analysis of large-scale point clouds.

</details>


### [5] [Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2510.24777)
*Yujie Nie,Jianzhang Ni,Yonglong Ye,Yuan-Ting Zhang,Yun Kwok Wing,Xiangqing Xu,Xin Ma,Lizhou Fan*

Main category: cs.CV

TL;DR: 本研究提出了一种结合眼动追踪和面部特征的多模态融合框架，用于辅助诊断阿尔茨海默病（AD），在公开数据集上实现了95.11%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 准确诊断阿尔茨海默病（AD）对及时干预和延缓疾病进展至关重要。眼动追踪和面部特征是反映认知功能的指标，但很少有研究探索它们在AD辅助诊断中的联合应用。

Method: 提出了一种多模态交叉增强融合框架，包含交叉增强融合注意力模块（CEFAM）和方向感知卷积模块（DACM），以模拟跨模态交互并捕获面部特征。构建了一个包含25名AD患者和25名健康对照者（HC）的同步多模态数据集。

Result: 在构建的数据集上，该框架在区分AD和HC方面取得了95.11%的分类准确率，优于传统的 late fusion 和特征连接方法。

Conclusion: 提出的多模态融合框架通过显式建模跨模态依赖性和特定模态的贡献，在AD辅助诊断方面表现出优越的鲁棒性和诊断性能。

Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling
timely intervention and slowing disease progression. Multimodal diagnostic
approaches offer considerable promise by integrating complementary information
across behavioral and perceptual domains. Eye-tracking and facial features, in
particular, are important indicators of cognitive function, reflecting
attentional distribution and neurocognitive state. However, few studies have
explored their joint integration for auxiliary AD diagnosis. In this study, we
propose a multimodal cross-enhanced fusion framework that synergistically
leverages eye-tracking and facial features for AD detection. The framework
incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module
(CEFAM), which models inter-modal interactions through cross-attention and
global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which
captures fine-grained directional facial features via horizontal-vertical
receptive fields. Together, these modules enable adaptive and discriminative
multimodal representation learning. To support this work, we constructed a
synchronized multimodal dataset, including 25 patients with AD and 25 healthy
controls (HC), by recording aligned facial video and eye-tracking sequences
during a visual memory-search paradigm, providing an ecologically valid
resource for evaluating integration strategies. Extensive experiments on this
dataset demonstrate that our framework outperforms traditional late fusion and
feature concatenation methods, achieving a classification accuracy of 95.11% in
distinguishing AD from HC, highlighting superior robustness and diagnostic
performance by explicitly modeling inter-modal dependencies and
modality-specific contributions.

</details>


### [6] [FPGA-based Lane Detection System incorporating Temperature and Light Control Units](https://arxiv.org/abs/2510.24778)
*Ibrahim Qamar,Saber Mahmoud,Seif Megahed,Mohamed Khaled,Saleh Hesham,Ahmed Matar,Saif Gebril,Mervat Mahmoud*

Main category: cs.CV

TL;DR: 该论文提出了一种基于FPGA的智能车辆车道线检测器（LDV）架构，利用Sobel算法进行边缘检测，能够在1.17毫秒内生成有效的车道线信息（车道数量、当前车道索引、左右边界），并具备自动光照和温度控制功能以增强环境适应性。


<details>
  <summary>Details</summary>
Motivation: 智能车辆在自动化趋势下是重要的发展方向，无论是在城市道路还是机器人轨道上，车道线检测都是其首要考虑的应用。

Method: 提出了一种基于FPGA的智能车辆车道线检测器（LDV）架构，该架构利用Sobel算法进行边缘检测。系统在416x416图像和150 MHz频率下运行。此外，系统还集成了自动光照和温度控制单元。

Result: 系统能够在1.17毫秒内生成有效的车道线输出，包括车道数量、当前车道索引以及其左右边界。自动光照和温度控制单元增强了系统的环境适应性。

Conclusion: 该研究提出了一种高效的FPGA-based车道线检测架构，能够满足智能车辆对实时车道线信息的需求，并通过环境自适应单元提高了其鲁棒性。

Abstract: Intelligent vehicles are one of the most important outcomes gained from the
world tendency toward automation. Applications of IVs, whether in urban roads
or robot tracks, do prioritize lane path detection. This paper proposes an
FPGA-based Lane Detector Vehicle LDV architecture that relies on the Sobel
algorithm for edge detection. Operating on 416 x 416 images and 150 MHz, the
system can generate a valid output every 1.17 ms. The valid output consists of
the number of present lanes, the current lane index, as well as its right and
left boundaries. Additionally, the automated light and temperature control
units in the proposed system enhance its adaptability to the surrounding
environmental conditions.

</details>


### [7] [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](https://arxiv.org/abs/2510.25234)
*Yuxiang Mao,Zhijie Zhang,Zhiheng Zhang,Jiawei Liu,Chen Zeng,Shihong Xia*

Main category: cs.CV

TL;DR: 利用现有数据集，通过线性叠加模型生成具有情绪表达的3D说话脸动画。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容（AIGC）领域对逼真且富有情绪的3D面部动画需求日益增长，但现有的语音驱动唇形同步技术在生成情绪表达方面仍有不足，主要受限于缺乏真实情绪3D说话脸数据集。

Method: 将语音和情绪驱动的面部动画建模为线性叠加问题，并提出一种稀疏性约束损失来解耦语音和情绪的blendshape，同时允许模型捕捉跨域变形。该方法利用VOCAset（中性表情3D说话脸数据集）和Florence4D（3D表情序列数据集）进行联合学习，并将学习到的blendshapes映射到FLAME模型的表情和下颌姿态参数，以驱动3D高斯头像。

Result: 实验结果表明，该方法能够生成具有指定情绪的说话脸动画，同时保持准确的唇形同步。感知研究显示，与现有方法相比，该方法在情绪表达方面更优，且不牺牲唇形同步质量。

Conclusion: 该方法成功地生成了兼具语音同步和情绪表达的3D说话脸动画，克服了真实情绪3D说话脸数据集稀缺的挑战，并在定性和定量评估中均表现出色。

Abstract: Expressions are fundamental to conveying human emotions. With the rapid
advancement of AI-generated content (AIGC), realistic and expressive 3D facial
animation has become increasingly crucial. Despite recent progress in
speech-driven lip-sync for talking-face animation, generating emotionally
expressive talking faces remains underexplored. A major obstacle is the
scarcity of real emotional 3D talking-face datasets due to the high cost of
data capture. To address this, we model facial animation driven by both speech
and emotion as a linear additive problem. Leveraging a 3D talking-face dataset
with neutral expressions (VOCAset) and a dataset of 3D expression sequences
(Florence4D), we jointly learn a set of blendshapes driven by speech and
emotion. We introduce a sparsity constraint loss to encourage disentanglement
between the two types of blendshapes while allowing the model to capture
inherent secondary cross-domain deformations present in the training data. The
learned blendshapes can be further mapped to the expression and jaw pose
parameters of the FLAME model, enabling the animation of 3D Gaussian avatars.
Qualitative and quantitative experiments demonstrate that our method naturally
generates talking faces with specified expressions while maintaining accurate
lip synchronization. Perceptual studies further show that our approach achieves
superior emotional expressivity compared to existing methods, without
compromising lip-sync quality.

</details>


### [8] [ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality](https://arxiv.org/abs/2510.24787)
*Mingzhi Zhu,Ding Shang,Sai Qian Zhang*

Main category: cs.CV

TL;DR: 本研究提出了一种名为 ESCA 的全栈优化框架，通过量化和硬件加速技术，在资源受限的 VR 设备上实现了光写真实感编解码器化身（PCA）的高效推理，达到了实时性能要求。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的光写真实感编解码器化身（PCA）模型在 VR 环境中虽然能提供高保真渲染，但对计算资源要求高，难以在头戴式显示器等资源受限的 VR 设备上实现实时推理，而低延迟和高能效对 VR 设备至关重要。

Method: 提出了一种针对编解码器化身模型的高效训练后量化（PTQ）方法，以支持低精度运行而不损害输出质量。此外，设计了一个定制的硬件加速器，可集成到 VR 设备的片上系统（SoC）中以提高处理效率。基于这些组件，引入了 ESCA，一个全栈优化框架，用于在边缘 VR 平台加速 PCA 推理。

Result: ESCA 将 FovVideoVDP 质量得分在 4 位基线上提升了最高 0.39，延迟降低了高达 3.36 倍，并在端到端测试中保持了 100 帧/秒的渲染速率，满足了实时 VR 的性能要求。

Conclusion: 研究证明了在高保真编解码器化身部署到资源受限设备上的可行性，为更具沉浸感和便携性的 VR 体验铺平了道路。

Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face
renderings, are increasingly being used in Virtual Reality (VR) environments to
enable immersive communication and interaction through deep learning-based
generative models. However, these models impose significant computational
demands, making real-time inference challenging on resource-constrained VR
devices such as head-mounted displays, where latency and power efficiency are
critical. To address this challenge, we propose an efficient post-training
quantization (PTQ) method tailored for Codec Avatar models, enabling
low-precision execution without compromising output quality. In addition, we
design a custom hardware accelerator that can be integrated into the
system-on-chip of VR devices to further enhance processing efficiency. Building
on these components, we introduce ESCA, a full-stack optimization framework
that accelerates PCA inference on edge VR platforms. Experimental results
demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over
the best 4-bit baseline, delivers up to $3.36\times$ latency reduction, and
sustains a rendering rate of 100 frames per second in end-to-end tests,
satisfying real-time VR requirements. These results demonstrate the feasibility
of deploying high-fidelity codec avatars on resource-constrained devices,
opening the door to more immersive and portable VR experiences.

</details>


### [9] [FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion](https://arxiv.org/abs/2510.25765)
*Chuhao Chen,Isabella Liu,Xinyue Wei,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: FreeArt3D是一个无需训练的框架，利用预训练的3D扩散模型生成具有高保真几何和纹理的可动3D对象，并准确预测其运动链结构。


<details>
  <summary>Details</summary>
Motivation: 现有的3D可动对象建模方法要么需要密集视图监督的优化重建，要么生成粗糙且忽视纹理的几何模型，而现有的3D扩散模型在生成可动对象方面存在挑战。FreeArt3D旨在克服这些限制，实现高质量、训练自由的可动3D对象生成。

Method: FreeArt3D通过将预训练的静态3D扩散模型（如Trellis）作为形状先验，并将关节度作为额外的生成维度，将得分蒸馏采样（SDS）扩展到3D到4D领域。它在给定几个不同关节状态的图像的情况下，联合优化对象的几何、纹理和关节参数，无需特定任务的训练或大规模可动数据集。

Result: 该方法能够生成高保真的几何和纹理，准确预测潜在的运动链结构，并能在不同对象类别中实现良好的泛化。即使采用逐实例优化范式，FreeArt3D也能在几分钟内完成，并在质量和通用性方面显著优于现有技术。

Conclusion: FreeArt3D提供了一种高效且通用的方法，用于从少量图像生成高质量的可动3D对象，克服了现有方法的局限性。

Abstract: Articulated 3D objects are central to many applications in robotics, AR/VR,
and animation. Recent approaches to modeling such objects either rely on
optimization-based reconstruction pipelines that require dense-view supervision
or on feed-forward generative models that produce coarse geometric
approximations and often overlook surface texture. In contrast, open-world 3D
generation of static objects has achieved remarkable success, especially with
the advent of native 3D diffusion models such as Trellis. However, extending
these methods to articulated objects by training native 3D diffusion models
poses significant challenges. In this work, we present FreeArt3D, a
training-free framework for articulated 3D object generation. Instead of
training a new model on limited articulated data, FreeArt3D repurposes a
pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape
prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by
treating articulation as an additional generative dimension. Given a few images
captured in different articulation states, FreeArt3D jointly optimizes the
object's geometry, texture, and articulation parameters without requiring
task-specific training or access to large-scale articulated datasets. Our
method generates high-fidelity geometry and textures, accurately predicts
underlying kinematic structures, and generalizes well across diverse object
categories. Despite following a per-instance optimization paradigm, FreeArt3D
completes in minutes and significantly outperforms prior state-of-the-art
approaches in both quality and versatility.

</details>


### [10] [The Underappreciated Power of Vision Models for Graph Structural Understanding](https://arxiv.org/abs/2510.24788)
*Xinjian Zhao,Wei Pang,Zhongkai Xue,Xiangru Jian,Lei Zhang,Yaoyao Xu,Xiaozhuang Song,Shu Wu,Tianshu Yu*

Main category: cs.CV

TL;DR: 视觉模型在图结构理解方面展现出巨大潜力，其表现甚至优于GNN，特别是在需要全局理解和规模不变性的任务上。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（GNNs）主要采用自下而上的信息传递方式，这与人类视觉感知先捕捉全局结构的直觉方式不同。尽管视觉模型在图理解方面有潜力，但现有基准测试未能充分评估其优势，并且混淆了领域特征与拓扑理解。因此，需要一个新的基准来专门评估模型对全局图属性的感知能力。

Method: 提出并使用了一个名为GraphAbstract的新基准测试。该基准旨在评估模型像人类一样感知全局图属性的能力，包括识别组织原型、检测对称性、感知连接强度和识别关键元素。

Result: 在GraphAbstract基准测试中，视觉模型在需要整体结构理解的任务上显著优于GNNs，并且在不同图规模下保持了良好的泛化能力。相比之下，GNNs在全局模式抽象方面存在困难，并且随着图规模的增加，性能会下降。

Conclusion: 视觉模型在图结构理解方面具有显著但未被充分利用的能力，特别是在需要全局拓扑感知和规模不变性推理的问题上。这为利用视觉模型的潜力开发更有效的图基础模型开辟了新途径，尤其是在以整体模式识别为主导的任务中。

Abstract: Graph Neural Networks operate through bottom-up message-passing,
fundamentally differing from human visual perception, which intuitively
captures global structures first. We investigate the underappreciated potential
of vision models for graph understanding, finding they achieve performance
comparable to GNNs on established benchmarks while exhibiting distinctly
different learning patterns. These divergent behaviors, combined with
limitations of existing benchmarks that conflate domain features with
topological understanding, motivate our introduction of GraphAbstract. This
benchmark evaluates models' ability to perceive global graph properties as
humans do: recognizing organizational archetypes, detecting symmetry, sensing
connectivity strength, and identifying critical elements. Our results reveal
that vision models significantly outperform GNNs on tasks requiring holistic
structural understanding and maintain generalizability across varying graph
scales, while GNNs struggle with global pattern abstraction and degrade with
increasing graph size. This work demonstrates that vision models possess
remarkable yet underutilized capabilities for graph structural understanding,
particularly for problems requiring global topological awareness and
scale-invariant reasoning. These findings open new avenues to leverage this
underappreciated potential for developing more effective graph foundation
models for tasks dominated by holistic pattern recognition.

</details>


### [11] [A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data](https://arxiv.org/abs/2510.24791)
*Jingjun Bi,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出了一种名为 RSGSLM 的新方法，用于解决多视图数据的图 기반 半监督学习问题，通过结合线性特征变换、多视图图融合、伪标签集成以及拓扑调整，并在图像基准数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 伪标签和图 기반 半监督学习在减少标注需求方面显示出潜力，但图像数据缺乏清晰的图结构以及多视图数据的复杂性限制了现有技术的效率，同时将图结构集成到多视图数据中仍然是一个挑战。

Method: RSGSLM 方法结合了线性特征变换和多视图图融合，并将其置于图卷积网络（GCN）框架内。它通过动态地将伪标签纳入 GCN 损失函数来改进多视图数据的分类，并通过调整类别边界附近标记样本的权重来纠正拓扑不平衡。此外，还引入了一种适用于所有样本的无监督平滑损失。

Result: 在多视图图像数据集上的实验结果表明，RSGSLM 在多视图半监督学习方面优于现有的方法。

Conclusion: RSGSLM 通过整合多种技术有效解决了多视图数据的图 기반 半监督学习中的挑战，并在实验中展现出优越的性能和计算效率。

Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have
gained attention due to their effectiveness in reducing the need for extensive
data annotations. Pseudo-labeling uses predictions from unlabeled data to
improve model training, while graph-based methods are characterized by
processing data represented as graphs. However, the lack of clear graph
structures in images combined with the complexity of multi-view data limits the
efficiency of traditional and existing techniques. Moreover, the integration of
graph structures in multi-view data is still a challenge. In this paper, we
propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view
Data (RSGSLM). Our method addresses these challenges by (i) combining linear
feature transformation and multi-view graph fusion within a Graph Convolutional
Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the
GCN loss function to improve classification in multi-view data, and (iii)
correcting topological imbalances by adjusting the weights of labeled samples
near class boundaries. Additionally, (iv) we introduce an unsupervised
smoothing loss applicable to all samples. This combination optimizes
performance while maintaining computational efficiency. Experimental results on
multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing
semi-supervised learning approaches in multi-view contexts.

</details>


### [12] [PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models](https://arxiv.org/abs/2510.24792)
*Patrick Haller,Fabio Barth,Jonas Golde,Georg Rehm,Alan Akbik*

Main category: cs.CV

TL;DR: PISA-Bench是一个多语言视觉-语言模型基准，用于评估跨语言的少样本学习能力，特别是在空间和几何推理方面。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型基准在高质量、人工验证的示例方面存在不足，并且主要局限于英语。PISA-Bench旨在通过提供一个多语言的、人工创建的基准来解决这个问题。

Method: PISA-Bench基于PISA测试创建，包含人工提取的指令、问题、答案选项和图像，并翻译成六种语言。研究人员评估了各种规模的视觉-语言模型在该基准上的表现。

Result: 小型视觉-语言模型在该基准上的得分较低，尤其是在非英语部分和涉及空间/几何推理的任务上。大型模型表现更好，但仍有改进空间。

Conclusion: PISA-Bench为研究多语言、多模态推理提供了一个有价值的资源，并突显了当前视觉-语言模型在处理这些任务时面临的挑战。

Abstract: Vision-language models (VLMs) have demonstrated remarkable progress in
multimodal reasoning. However, existing benchmarks remain limited in terms of
high-quality, human-verified examples. Many current datasets rely on
synthetically generated content by large language models (LLMs). Furthermore,
most datasets are limited to English, as manual quality assurance of translated
samples is time-consuming and costly. To fill this gap, we introduce
PISA-Bench, a multilingual benchmark derived from English examples of the
expert-created PISA tests, a unified framework for the assessment of student
competencies in over eighty countries. Each example consists of human-extracted
instructions, questions, answer options, and images, enriched with question
type categories, and has been translated from English into five additional
languages (Spanish, German, Chinese, French, and Italian), resulting in a fully
parallel corpus covering six languages. We evaluate state-of-the-art
vision-language models on PISA-Bench and find that especially small models
(<20B parameters) fail to achieve high test scores. We further find substantial
performance degradation on non-English splits as well as high error-rates when
models are tasked with spatial and geometric reasoning. By releasing the
dataset and evaluation framework, we provide a resource for advancing research
on multilingual multimodal reasoning.

</details>


### [13] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu,Bo Wang,Pengpeng Zeng,Haonan Zhang,Ji Zhang,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: VLA模型面临计算和数据需求大的挑战。本调查首次全面回顾了高效VLA模型，涵盖数据、模型和训练过程。通过统一的分类，将现有技术分为高效模型设计、高效训练和高效数据收集三个方面。本文还总结了应用、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身智能领域具有重要意义，但其部署受到计算和数据需求的限制。本调查旨在解决这些挑战。

Method: 本调查对高效VLA模型进行了全面回顾，并提出了一个统一的分类法，将技术分为三个核心支柱：高效模型设计、高效训练和高效数据收集。对现有技术进行了批判性审查。

Result: 本调查为社区提供了基础参考，总结了代表性应用，阐述了关键挑战，并为未来研究规划了路线图。

Conclusion: 本调查全面回顾了高效VLA模型，并提出了一个分类框架，为该领域的研究提供了指导。

Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/

</details>


### [14] [Conflict Adaptation in Vision-Language Models](https://arxiv.org/abs/2510.24804)
*Xiaoyang Hu*

Main category: cs.CV

TL;DR: 许多视觉语言模型（VLMs）表现出与冲突适应一致的行为，即在一个高冲突试次之后，在高冲突试次上的表现得到改善。


<details>
  <summary>Details</summary>
Motivation: 冲突适应是人类认知控制的一个特征，它说明了认知控制这种稀缺资源是如何被调用的。

Method: 使用顺序Stroop任务和稀疏自编码器（SAEs）来识别视觉语言模型（VLMs）中的任务相关超级节点。

Result: 13个视觉语言模型中的12个表现出冲突适应行为。稀疏自编码器识别出文本和颜色相关的超级节点，其相对大小与人类阅读和颜色命名的自动化程度不对称性相匹配。一个冲突调节的超级节点在24-25层被分离出来，其烧蚀会显著增加Stroop错误，但对一致性试次的影响很小。

Conclusion: 视觉语言模型（VLMs）通过类似人类的冲突适应机制来管理认知控制，并且其内部表征能够捕捉到冲突信息。

Abstract: A signature of human cognitive control is conflict adaptation: improved
performance on a high-conflict trial following another high-conflict trial.
This phenomenon offers an account for how cognitive control, a scarce resource,
is recruited. Using a sequential Stroop task, we find that 12 of 13
vision-language models (VLMs) tested exhibit behavior consistent with conflict
adaptation, with the lone exception likely reflecting a ceiling effect. To
understand the representational basis of this behavior, we use sparse
autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.
Partially overlapping supernodes emerge for text and color in both early and
late layers, and their relative sizes mirror the automaticity asymmetry between
reading and color naming in humans. We further isolate a conflict-modulated
supernode in layers 24-25 whose ablation significantly increases Stroop errors
while minimally affecting congruent trials.

</details>


### [15] [DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts](https://arxiv.org/abs/2510.24813)
*Binbin Li,Guimiao Yang,Zisen Qi,Haiping Wang,Yu Ding*

Main category: cs.CV

TL;DR: DualCap通过生成视觉提示来增强视觉表示，解决了现有轻量级检索增强图像字幕模型中的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅将检索数据用作文本提示，未能增强原始视觉特征，特别是在处理物体细节或复杂场景时，造成了语义鸿沟。

Method: 提出了一种名为DualCap的新方法，该方法采用双重检索机制：1.标准的图像到文本检索用于文本提示。2.新颖的图像到图像检索用于获取视觉上相似的场景。从视觉相似场景的字幕中提取关键词和短语，并将其与原始图像特征通过轻量级的可训练特征融合网络进行整合。

Result: 与先前基于视觉提示的字幕生成方法相比，DualCap在需要更少可训练参数的情况下实现了具有竞争力的性能。

Conclusion: DualCap通过引入视觉提示来有效弥合语义鸿沟，从而提高图像字幕的质量，并在效率和性能之间取得了良好的平衡。

Abstract: Recent lightweight retrieval-augmented image caption models often utilize
retrieved data solely as text prompts, thereby creating a semantic gap by
leaving the original visual features unenhanced, particularly for object
details or complex scenes. To address this limitation, we propose $DualCap$, a
novel approach that enriches the visual representation by generating a visual
prompt from retrieved similar images. Our model employs a dual retrieval
mechanism, using standard image-to-text retrieval for text prompts and a novel
image-to-image retrieval to source visually analogous scenes. Specifically,
salient keywords and phrases are derived from the captions of visually similar
scenes to capture key objects and similar details. These textual features are
then encoded and integrated with the original image features through a
lightweight, trainable feature fusion network. Extensive experiments
demonstrate that our method achieves competitive performance while requiring
fewer trainable parameters compared to previous visual-prompting captioning
approaches.

</details>


### [16] [Deep Feature Optimization for Enhanced Fish Freshness Assessment](https://arxiv.org/abs/2510.24814)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Assessing fish freshness is vital for ensuring food safety and minimizing
economic losses in the seafood industry. However, traditional sensory
evaluation remains subjective, time-consuming, and inconsistent. Although
recent advances in deep learning have automated visual freshness prediction,
challenges related to accuracy and feature transparency persist. This study
introduces a unified three-stage framework that refines and leverages deep
visual representations for reliable fish freshness assessment. First, five
state-of-the-art vision architectures - ResNet-50, DenseNet-121,
EfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a
strong baseline. Next, multi-level deep features extracted from these backbones
are used to train seven classical machine learning classifiers, integrating
deep and traditional decision mechanisms. Finally, feature selection methods
based on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso
identify a compact and informative subset of features. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate that the best
configuration combining Swin-Tiny features, an Extra Trees classifier, and
LGBM-based feature selection achieves an accuracy of 85.99%, outperforming
recent studies on the same dataset by 8.69-22.78%. These findings confirm the
effectiveness and generalizability of the proposed framework for visual quality
evaluation tasks.

</details>


### [17] [Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection](https://arxiv.org/abs/2510.24816)
*Cui Yakun,Fushuo Huo,Weijie Shi,Juntao Dai,Hang Du,Zhenghao Zhu,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 本论文提出了一个名为MVFNDB的多模态视频假新闻检测基准，包含10项任务和9730个标注问题，旨在评估大型多模态模型（MLLMs）在视频假新闻检测中的感知、理解和推理能力。同时，论文还提出了一个名为MVFND-CoT的新框架，结合了创作者内容和原始拍摄素材的推理，并对影响准确性的因素进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有的视频假新闻检测基准主要关注最终决策的准确性，缺乏对整个检测过程的细粒度评估，导致检测过程如同一个黑箱。因此，有必要创建一个能够深入评估MLLMs在视频假新闻检测中各方面能力的基准。

Method: 构建了一个包含10项任务和9730个标注问题的MVFNDB基准，用于探查MLLMs的感知、理解和推理能力。设计了一个名为MVFND-CoT的新框架，该框架结合了创作者添加的内容和原始拍摄素材的推理。对视频处理策略和视频特征与模型能力之间的对齐等影响准确性的深层因素进行了分析。

Result: MVFNDB基准包含9730个基于假新闻检测能力构建的分类法的人工标注视频相关问题。MVFND-CoT框架结合了创作者内容和原始拍摄素材的推理，并对影响最终结果的多模态特征组合进行了验证。

Conclusion: 提出的MVFNDB基准和MVFND-CoT框架为未来评估和改进MLLMs在视频假新闻检测领域的应用奠定了坚实的基础。

Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced
research into applications for Video fake news detection (VFND) tasks.
Traditional video-based FND benchmarks typically focus on the accuracy of the
final decision, often failing to provide fine-grained assessments for the
entire detection process, making the detection process a black box. Therefore,
we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based
on the empirical analysis, which provides foundation for tasks definition. The
benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'
perception, understanding, and reasoning capacities during detection, featuring
9730 human-annotated video-related questions based on a carefully constructed
taxonomy ability of VFND. To validate the impact of combining multiple features
on the final results, we design a novel framework named MVFND-CoT, which
incorporates both creator-added content and original shooting footage
reasoning. Building upon the benchmark, we conduct an in-depth analysis of the
deeper factors influencing accuracy, including video processing strategies and
the alignment between video features and model capabilities. We believe this
benchmark will lay a solid foundation for future evaluations and advancements
of MLLMs in the domain of video fake news detection.

</details>


### [18] [SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/abs/2510.24820)
*Ruiyang Zhang,Jiahao Luo,Xiaoru Feng,Qiufan Pang,Yaodong Yang,Juntao Dai*

Main category: cs.CV

TL;DR: 通过引入多轮安全编辑框架MR-SafeEdit和统一的多模态大模型SafeEditor，提升文本到图像生成模型的安全性和效用平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的安全方法（尤其是在推理时）存在过度拒绝和安全效用不平衡的问题，需要更有效的解决方案。

Method: 提出一个多轮安全编辑框架，包含MR-SafeEdit数据集和SafeEditor模型，该框架作为即插即用模块，模仿人类识别和修正不安全内容的过程。

Result: SafeEditor在减少过度拒绝和实现更好的安全-效用平衡方面优于现有方法。

Conclusion: 所提出的多轮安全编辑框架和SafeEditor能够有效地提升文本到图像生成模型的安全性，并改善其效用平衡性。

Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.

</details>


### [19] [Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821)
*Inclusion AI,:,Bowen Ma,Cheng Zou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianing Li,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jianping Jiang,Jun Peng,Kaixiang Ji,Kaimeng Ren,Libin Wang,Lixiang Ru,Longhua Tan,Lan Wang,Mochen Bai,Ning Gao,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Ruobing Zheng,Sirui Gao,Tianqi Li,Tinghao Liu,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaolong Wang,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yuting Xiao,Yunxiao Sun,Yipeng Chen,Yifan Mao,Yifei Wu,Yongjie Lyu,Ziping Ma,Zhiqiang Fang,Zhihao Qiu,Ziyuan Huang,Zizheng Yang,Zhengyu He*

Main category: cs.CV

TL;DR: Ming-Flash-Omni是一个基于Ling-Flash-2.0的稀疏MoE模型，拥有1000亿参数，但每次仅激活61亿参数，实现了高效扩展和多模态能力的统一。在语音识别、图像生成（文本渲染、场景一致性、身份保持）和生成式分割方面均取得最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 为了实现更高效的计算和更大的模型容量，并增强跨视觉、语音和语言的统一多模态智能，以迈向通用人工智能（AGI）。

Method: 提出了一种基于Ling-Flash-2.0的稀疏混合专家（MoE）变体，称为Ming-Flash-Omni，拥有1000亿总参数，每次仅激活61亿参数。

Result: 在上下文语音识别（ASR）和少数民族语言识别ASR方面取得了最先进的性能；在图像生成方面，实现了高保真文本渲染，并提高了场景一致性和身份保持能力；引入了生成式分割，在分割任务和图像生成/编辑方面表现优异；在文本到图像生成和生成式分割方面达到最先进水平，并在12个上下文ASR基准测试中创下新纪录。

Conclusion: Ming-Flash-Omni作为一个统一的架构，在多模态理解和生成方面取得了显著进步，代表了迈向AGI的关键一步。

Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.

</details>


### [20] [MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition](https://arxiv.org/abs/2510.24827)
*Haoyang Zhang,Zhou Yang,Ke Sun,Yucai Pang,Guoliang Xu*

Main category: cs.CV

TL;DR: 提出了一种名为多通道交叉模态交互混合网络（MCIHN）的新模型，以解决多模态情感识别中的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别对于未来人机交互至关重要，但由于不同模态之间的差异以及单模态情感信息表征的困难，准确的情感识别仍面临重大挑战。

Method: 首先，为每个模态分别构建了对抗性自编码器（AAE），以学习具有判别性的情感特征并通过解码器重构特征。然后，将来自不同模态的AAE的潜在代码输入到预定义的交叉模态门控机制模型（CGMM）中，以减少模态间的差异，建立交互模态的情感关系，并生成不同模态间的交互特征。最后，利用特征融合模块（FFM）进行多模态融合。

Result: 在公开的SIMS和MOSI数据集上进行的实验表明，MCIHN取得了优越的性能。

Conclusion: MCIHN模型在多模态情感识别任务上表现出色，有效解决了模态间差异和单模态信息表征的难题。

Abstract: Multimodal emotion recognition is crucial for future human-computer
interaction. However, accurate emotion recognition still faces significant
challenges due to differences between different modalities and the difficulty
of characterizing unimodal emotional information. To solve these problems, a
hybrid network model based on multipath cross-modal interaction (MCIHN) is
proposed. First, adversarial autoencoders (AAE) are constructed separately for
each modality. The AAE learns discriminative emotion features and reconstructs
the features through a decoder to obtain more discriminative information about
the emotion classes. Then, the latent codes from the AAE of different
modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to
reduce the discrepancy between modalities, establish the emotional relationship
between interacting modalities, and generate the interaction features between
different modalities. Multimodal fusion using the Feature Fusion module (FFM)
for better emotion recognition. Experiments were conducted on publicly
available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior
performance.

</details>


### [21] [The Generation Phases of Flow Matching: a Denoising Perspective](https://arxiv.org/abs/2510.24830)
*Anne Gagneux,Ségolène Martin,Rémi Gribonval,Mathurin Massias*

Main category: cs.CV

TL;DR: Flow matching的生成质量受噪声和漂移的影响，通过分析生成过程中的不同动态阶段，可以优化生成效果。


<details>
  <summary>Details</summary>
Motivation: 虽然Flow matching在生成方面取得了成功，但影响其生成质量的因素尚不明确。

Method: 将Flow matching模型与denoisers联系起来，并提出了一种实证框架来研究生成过程，引入了噪声和漂移作为影响因素进行分析。

Result: 发现了生成过程中不同的动态阶段，并能精确地刻画denoisers在生成过程的哪个阶段成功或失败，以及失败的原因。

Conclusion: 通过理解噪声和漂移在Flow matching生成过程中的作用，以及denoisers在不同动态阶段的表现，可以为未来的模型改进提供方向。

Abstract: Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.

</details>


### [22] [FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and Vegetables](https://arxiv.org/abs/2510.24885)
*Sidharth Rai,Rahul Harsha Cheppally,Benjamin Vail,Keziban Yalçın Dokumacı,Ajay Sharda*

Main category: cs.CV

TL;DR: 本研究将水果蔬菜成熟度估计视为一个连续的概率学习问题，而非传统的离散分类问题，并提出了一种改进的RT-DETRv2模型，通过增加概率头来预测连续的成熟度分布及其不确定性，以提高农业自动化中精确度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法将成熟度估计视为离散分类问题，这与生物成熟过程的连续性相冲突，导致信息损失和类别边界模糊。本研究旨在解决这一问题，提供更符合生物学特性且更精确的成熟度评估方法。

Method: 提出了一种新颖的架构修改，在RT-DETRv2对象检测器中引入了一个专门的概率头，使其能够为每个检测到的对象预测成熟度谱上的连续分布，同时学习平均成熟度状态及其相关不确定性。

Result: 提出的概率模型在具有挑战性的大规模水果数据集上实现了85.6%的平均精度均值（mAP），并且比基于分类的方法提供了更细粒度和更准确的成熟度评估。

Conclusion: 本研究提出的概率方法不仅提供了更丰富、更符合生物学特性的植物成熟度表示，而且保持了出色的检测性能，通过引入不确定性度量为下游机器人决策提供了信心分数，为现代农业中更智能、更具不确定性感知能力的自动化系统铺平了道路。

Abstract: Maturity estimation of fruits and vegetables is a critical task for
agricultural automation, directly impacting yield prediction and robotic
harvesting. Current deep learning approaches predominantly treat maturity as a
discrete classification problem (e.g., unripe, ripe, overripe). This rigid
formulation, however, fundamentally conflicts with the continuous nature of the
biological ripening process, leading to information loss and ambiguous class
boundaries. In this paper, we challenge this paradigm by reframing maturity
estimation as a continuous, probabilistic learning task. We propose a novel
architectural modification to the state-of-the-art, real-time object detector,
RT-DETRv2, by introducing a dedicated probabilistic head. This head enables the
model to predict a continuous distribution over the maturity spectrum for each
detected object, simultaneously learning the mean maturity state and its
associated uncertainty. This uncertainty measure is crucial for downstream
decision-making in robotics, providing a confidence score for tasks like
selective harvesting. Our model not only provides a far richer and more
biologically plausible representation of plant maturity but also maintains
exceptional detection performance, achieving a mean Average Precision (mAP) of
85.6\% on a challenging, large-scale fruit dataset. We demonstrate through
extensive experiments that our probabilistic approach offers more granular and
accurate maturity assessments than its classification-based counterparts,
paving the way for more intelligent, uncertainty-aware automated systems in
modern agriculture

</details>


### [23] [Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS](https://arxiv.org/abs/2510.24887)
*Daniele L. V. dos Santos,Thiago B. Pereira,Carlos Eduardo G. R. Alves,Richard J. M. G. Tello,Francisco de A. Boldt,Thiago M. Paixão*

Main category: cs.CV

TL;DR: 使用轻量级身体标志物进行巴西手语（LIBRAS）孤立手语识别是可行的，通过选择合适的标志物子集和使用三次样条插值可以提高识别性能和效率。


<details>
  <summary>Details</summary>
Motivation: Alves等人（2024）的研究虽然提高了识别性能，但OpenPose的 Landmark 提取限制了处理速度。本研究旨在探索轻量级 Landmark 提取方法，以在提高处理速度的同时保持甚至提高识别精度。

Method: 本研究探索了 Landmark 子集选择策略，以优化识别性能，并使用了三次样条插值来处理缺失的 Landmark 数据。

Result: 通过实验，本研究发现合适的 Landmark 子集能够达到与现有最先进方法相当或更优的性能，同时将处理时间缩短了5倍以上。此外，三次样条插值能够有效缓解 Landmark 缺失问题，显著提高识别精度。

Conclusion: 仔细选择 Landmark 并结合简单的插值技术，可以实现高效且准确的孤立手语识别，为构建可扩展的手语识别系统奠定了基础。

Abstract: This paper investigates the feasibility of using lightweight body landmark
detection for the recognition of isolated signs in Brazilian Sign Language
(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled
substantial improvements in recognition performance, the use of OpenPose for
landmark extraction hindered time performance. In a preliminary investigation,
we observed that simply replacing OpenPose with the lightweight MediaPipe,
while improving processing speed, significantly reduced accuracy. To overcome
this limitation, we explored landmark subset selection strategies aimed at
optimizing recognition performance. Experimental results showed that a proper
landmark subset achieves comparable or superior performance to state-of-the-art
methods while reducing processing time by more than 5X compared to Alves et al.
(2024). As an additional contribution, we demonstrated that spline-based
imputation effectively mitigates missing landmark issues, leading to
substantial accuracy gains. These findings highlight that careful landmark
selection, combined with simple imputation techniques, enables efficient and
accurate isolated sign recognition, paving the way for scalable Sign Language
Recognition systems.

</details>


### [24] [Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation](https://arxiv.org/abs/2510.24902)
*H Mhatre,M Vyas,A Mittal*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Traffic congestion is becoming a challenge in the rapidly growing urban
cities, resulting in increasing delays and inefficiencies within urban
transportation systems. To address this issue a comprehensive methodology is
designed to optimize traffic flow and minimize delays. The framework is
structured with three primary components: (a) vehicle detection, (b) traffic
prediction, and (c) traffic signal optimization. This paper presents the first
component, vehicle detection. The methodology involves analyzing multiple
sequential frames from a camera feed to compute the background, i.e. the
underlying roadway, by averaging pixel values over time. The computed
background is then utilized to extract the foreground, where the Density-Based
Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to
detect vehicles. With its computational efficiency and minimal infrastructure
modification requirements, the proposed methodology offers a practical and
scalable solution for real-world deployment.

</details>


### [25] [VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos](https://arxiv.org/abs/2510.24904)
*Qiucheng Wu,Handong Zhao,Zhixin Shu,Jing Shi,Yang Zhang,Shiyu Chang*

Main category: cs.CV

TL;DR: VividCam是一个训练范式，使扩散模型能够从合成视频中学习复杂的摄像机运动，解决了现有模型在处理非常规摄像机运动方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型难以泛化到非常规摄像机运动，限制了创作真正原创和艺术性视频的能力，原因是难以找到包含所需非常规摄像机运动的充足训练视频。

Method: VividCam采用多重解耦策略，将摄像机运动学习与合成外观伪影分离开来，从而实现从合成视频中学习复杂的摄像机运动，并减轻了域转移问题。

Result: 该方法能够合成各种精确控制的复杂摄像机运动，且仅使用简单的合成数据（如低多边形3D场景），这些数据可以通过Unity等引擎高效渲染。

Conclusion: VividCam能够有效地从简单的合成数据中学习和生成复杂的摄像机运动，克服了对真实训练视频的依赖，为文本到视频生成模型带来了原创性和艺术性。

Abstract: Although recent text-to-video generative models are getting more capable of
following external camera controls, imposed by either text descriptions or
camera trajectories, they still struggle to generalize to unconventional camera
motions, which is crucial in creating truly original and artistic videos. The
challenge lies in the difficulty of finding sufficient training videos with the
intended uncommon camera motions. To address this challenge, we propose
VividCam, a training paradigm that enables diffusion models to learn complex
camera motions from synthetic videos, releasing the reliance on collecting
realistic training videos. VividCam incorporates multiple disentanglement
strategies that isolates camera motion learning from synthetic appearance
artifacts, ensuring more robust motion representation and mitigating domain
shift. We demonstrate that our design synthesizes a wide range of precisely
controlled and complex camera motions using surprisingly simple synthetic data.
Notably, this synthetic data often consists of basic geometries within a
low-poly 3D scene and can be efficiently rendered by engines like Unity. Our
video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .

</details>


### [26] [Understanding Multi-View Transformers](https://arxiv.org/abs/2510.24907)
*Michal Stary,Julien Gaubil,Ayush Tewari,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文提出一种方法来探究和可视化多视角Transformer中的3D表示，以解决其“黑箱”问题，并分析了DUSt3R模型的变体，揭示了其潜在状态的发展、各层的作用，并表明其估计的对应关系会随着重建的几何形状得到改进。


<details>
  <summary>Details</summary>
Motivation: 多视角Transformer（如DUSt3R）在3D视觉领域取得了显著进展，但其内部机制不明确，这阻碍了在安全关键领域的应用和进一步改进。

Method: 通过探究和可视化多视角Transformer的残差连接中的3D表示，来研究DUSt3R模型的变体。

Result: 研究揭示了DUSt3R模型变体潜在状态在不同块中的发展情况，阐明了各层的作用，并指出了其与具有更强归纳偏置的显式全局姿态方法的区别。此外，实验表明该模型估计的对应关系会随着重建的几何形状得到改进。

Conclusion: 所提出的方法能够探究多视角Transformer的内部工作机制，为理解和改进这些模型提供了新的途径，尤其是在安全和可靠性要求高的应用中。

Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .

</details>


### [27] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh,Jie Ji,Xiaolong Ma,Fatemeh Afghah*

Main category: cs.CV

TL;DR: M-SAM是一种模型无关的框架，通过识别主导模态、分解损失、更新权重来平衡和增强多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，主导模态常常会掩盖其他模态，限制模型的泛化能力。

Method: M-SAM通过三个步骤进行优化：1. 使用Shapley识别主导模态；2. 调节损失函数，优先考虑主导模态的鲁棒性；3. 通过反向传播更新权重。

Result: M-SAM在四个不同的数据集上进行了广泛的实验，结果表明其在平衡和增强多模态学习方面优于最先进的方法。

Conclusion: M-SAM通过识别主导模态并调节损失函数，能够实现鲁棒学习，同时增强其他模态的贡献，从而提升整体性能。

Abstract: In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.

</details>


### [28] [IBIS: A Powerful Hybrid Architecture for Human Activity Recognition](https://arxiv.org/abs/2510.24936)
*Alison M. Fernandes,Hermes I. Del Monego,Bruno S. Chang,Anelise Munaretto,Hélder M. Fontes,Rui L. Campos*

Main category: cs.CV

TL;DR: Wi-Fi sensing is useful but prone to overfitting. We propose IBIS, a hybrid Inception-BiLSTM-SVM model, to improve generalization and achieve nearly 99% accuracy in movement recognition using Doppler data.


<details>
  <summary>Details</summary>
Motivation: Wi-Fi sensing is promising for low-cost, non-intrusive data collection in areas like healthcare and IoT control, but it suffers from a common overfitting problem, hindering its generalizability.

Method: We introduce a novel hybrid architecture called IBIS, which combines Inception-BiLSTM with a Support Vector Machine (SVM). This architecture is designed to enhance model generalization and establish more robust classification boundaries.

Result: The IBIS method, when applied to Doppler-derived data for movement recognition, achieves an accuracy of nearly 99%. Performance metrics and confusion matrices validate the solution's effectiveness.

Conclusion: The proposed IBIS model effectively addresses the overfitting issue in Wi-Fi sensing, significantly improving generalization and achieving high accuracy in movement recognition tasks.

Abstract: The increasing interest in Wi-Fi sensing stems from its potential to capture
environmental data in a low-cost, non-intrusive way, making it ideal for
applications like healthcare, space occupancy analysis, and gesture-based IoT
control. However, a major limitation in this field is the common problem of
overfitting, where models perform well on training data but fail to generalize
to new data. To overcome this, we introduce a novel hybrid architecture that
integrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer
to as IBIS. Our IBIS approach is uniquely engineered to improve model
generalization and create more robust classification boundaries. By applying
this method to Doppler-derived data, we achieve a movement recognition accuracy
of nearly 99%. Comprehensive performance metrics and confusion matrices confirm
the significant effectiveness of our proposed solution.

</details>


### [29] [FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning](https://arxiv.org/abs/2510.24980)
*Reza Saadati Fard,Emmanuel Agu,Palawat Busaranuvong,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong,Lorraine Loretz*

Main category: cs.CV

TL;DR: FT-ARM是一个多模态大语言模型，通过结合图像和文本信息，并引入代理自我反思机制，提高了压力性溃疡分级（I-IV期）的准确性和一致性，同时提供可解释的自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 压力性溃疡（PU）的分级对于治疗至关重要，但由于视觉差异细微和主观解读，准确分级面临挑战，导致临床医生间存在差异。现有的基于CNN和ViT的AI方法虽然准确率有潜力，但可解释性有限。

Method: 提出FT-ARM（Fine-Tuned Agentic Reflection Multimodal model），一个经过微调的多模态大语言模型（MLLM），并带有代理自我反思机制，用于压力性溃疡分级。FT-ARM借鉴了临床医生诊断复核的方式，通过对视觉特征和文本编码的临床知识进行推理，迭代地优化其预测，从而提高准确性和一致性。

Result: 在公开的压力性损伤图像数据集（PIID）上，FT-ARM（基于LLaMA 3.2 90B微调）在I-IV期PU分级中达到了85%的准确率，比之前的CNN模型提高了4%。FT-ARM还为实时推理而设计和测试，并能生成基于临床的自然语言解释。

Conclusion: FT-ARM通过整合微调和跨模态输入的反思性推理，提高了自动化伤口评估系统的可靠性、透明度和临床适用性，满足了对一致且可解释的PU分级的关键需求，以支持改进患者护理。

Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.

</details>


### [30] [Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8](https://arxiv.org/abs/2510.25032)
*Zahra Ebrahimi Vargoorani,Amir Mohammad Ghoreyshi,Ching Yee Suen*

Main category: cs.CV

TL;DR: 本研究提出了一种结合 YOLOv8 和 Grounding DINO 的半监督学习方法，用于提高自动车牌识别 (ALPR) 系统的准确性，解决了光照、速度和图像质量等挑战，并在多个真实数据集上取得了优异的识别率。


<details>
  <summary>Details</summary>
Motivation: 开发高准确度的自动车牌识别 (ALPR) 系统面临诸多挑战，包括环境因素（光照、雨、尘）、车辆高速、摄像头角度变化以及图像质量低下等。ALPR 在交通管理、停车、车辆追踪、收费和执法等领域至关重要。

Method: 本研究提出了一种深度学习策略，利用 YOLOv8 进行车牌检测和识别。该方法结合了少量手动标注数据和由 Grounding DINO 生成的伪标签，采用半监督学习框架进行模型训练。Grounding DINO 是一个强大的视觉-语言模型，能自动标注车牌边界框，减少对人工标注的依赖。通过整合人工验证和模型生成的标注，可以高效地扩展数据集并保证标注质量，从而提升训练过程和模型整体性能。

Result: 在 CENPARMI 数据集上实现了 94% 的召回率，在 UFPR-ALPR 数据集上实现了 91% 的召回率。此外，还报告了两个数据集的字符错误率。

Conclusion: 所提出的结合 YOLOv8 和 Grounding DINO 的半监督学习方法，通过有效利用人工标注和伪标签，显著提高了 ALPR 系统的性能，克服了传统方法在复杂环境下的局限性。

Abstract: Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.

</details>


### [31] [Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models](https://arxiv.org/abs/2510.25051)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本研究提出了一种结合2D乳腺钼靶影像与文本临床元数据的新型计算机辅助诊断（CAD）框架，旨在克服现有CAD系统的局限性，提高临床应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机辅助诊断（CAD）系统在处理多模态数据和临床部署方面存在局限，尤其是在整合临床病史信息方面。本研究旨在提出一种新的框架，以克服这些挑战。

Method: 本研究提出了一种新颖的框架，通过创新的标记化模块，融合了来自2D乳腺钼靶影像的视觉特征以及从易于获取的临床元数据和合成的放射学报告中提取的结构化文本描述符。研究采用了卷积神经网络（ConvNets）与语言表示相结合的方法。

Result: 本研究提出的多模态方法在跨国队列筛查乳腺钼靶影像的评估中，在癌症检测和钙化识别方面取得了优于单一模态基线方法的性能，特别是展示了在处理高分辨率图像和实现实际部署方面的优势。与视觉转换器（vision transformer）模型相比，本研究的方法在处理高分辨率图像和实现实际部署方面表现更优。

Conclusion: 本研究提出的多模态方法通过有效的融合机制，有效利用了影像数据和患者的背景信息，为开发能够有效利用影像数据和患者背景信息的临床可用视觉语言模型（VLM）的CAD系统建立了一个新范式。

Abstract: Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.

</details>


### [32] [Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023 Challenge](https://arxiv.org/abs/2510.25058)
*Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu*

Main category: cs.CV

TL;DR: 该研究使用了MONAI的Auto3DSeg来解决BraTS 2023分割挑战赛，在5项比赛中获得3项第一名和2项第二名。


<details>
  <summary>Details</summary>
Motivation: 参赛者希望在BraTS 2023分割挑战赛中取得好成绩。

Method: 使用MONAI的Auto3DSeg工具来解决比赛。

Result: 在脑转移瘤、脑膜瘤和非洲脑肿瘤分割挑战赛中获得第一名，在成人和儿童胶质瘤分割挑战赛中获得第二名。

Conclusion: 该研究展示了MONAI的Auto3DSeg在解决多项脑肿瘤分割任务上的有效性。

Abstract: In this work, we describe our solution to the BraTS 2023 cluster of
challenges using Auto3DSeg from MONAI. We participated in all 5 segmentation
challenges, and achieved the 1st place results in three of them: Brain
Metastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place
results in the remaining two: Adult and Pediatic Glioma challenges.

</details>


### [33] [DRIP: Dynamic patch Reduction via Interpretable Pooling](https://arxiv.org/abs/2510.25067)
*Yusen Peng,Sachin Kumar*

Main category: cs.CV

TL;DR: DRIP是一种动态合并视觉编码器中token的方法，可在不牺牲性能的情况下显著降低计算量，并已在ImageNet、CLIP和生物医学领域得到验证。


<details>
  <summary>Details</summary>
Motivation: 由于大规模视觉语言模型的预训练成本高昂，限制了研究人员从头开始预训练的积极性。DRIP旨在解决这一效率问题。

Method: DRIP能够适应输入图像，并在视觉编码器的更深层动态地合并（聚合）token。

Result: DRIP在ImageNet从头开始训练和CLIP对比预训练中都实现了显著的GFLOPs（衡量计算量）降低，同时保持了相当的分类/零样本性能。此外，在大型生物医学数据集上进行持续预训练也验证了DRIP的有效性。

Conclusion: DRIP通过动态聚合token，在不影响性能的情况下提高了视觉语言模型的训练效率，并成功扩展到科学领域。

Abstract: Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.

</details>


### [34] [Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments](https://arxiv.org/abs/2510.25070)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 该研究提出了一种视觉-语言集成框架，用于在真实世界场景中进行零样本场景理解。


<details>
  <summary>Details</summary>
Motivation: 真实世界场景的复杂性和多变性给零样本场景理解带来了巨大挑战，模型需要在没有先验标记示例的情况下识别新物体、动作和上下文。

Method: 提出了一种将预训练的视觉编码器（如CLIP、ViT）和大型语言模型（如GPT）相结合的框架，以实现视觉和文本模态之间的语义对齐。该模型将视觉输入和文本提示嵌入共享空间，并通过多模态融合和推理层进行上下文解释。

Result: 在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，该方法在物体识别、活动检测和场景描述方面显著优于最先进的零样本模型，在准确率和语义连贯性方面均有提升。

Conclusion: 跨模态对齐和语言基础在增强真实世界场景理解的泛化能力方面非常有效。

Abstract: Zero-shot scene understanding in real-world settings presents major
challenges due to the complexity and variability of natural scenes, where
models must recognize new objects, actions, and contexts without prior labeled
examples. This work proposes a vision-language integration framework that
unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models
(e.g., GPT-based architectures) to achieve semantic alignment between visual
and textual modalities. The goal is to enable robust zero-shot comprehension of
scenes by leveraging natural language as a bridge to generalize over unseen
categories and contexts. Our approach develops a unified model that embeds
visual inputs and textual prompts into a shared space, followed by multimodal
fusion and reasoning layers for contextual interpretation. Experiments on
Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate
significant gains over state-of-the-art zero-shot models in object recognition,
activity detection, and scene captioning. The proposed system achieves up to
18% improvement in top-1 accuracy and notable gains in semantic coherence
metrics, highlighting the effectiveness of cross-modal alignment and language
grounding in enhancing generalization for real-world scene understanding.

</details>


### [35] [Neighborhood Feature Pooling for Remote Sensing Image Classification](https://arxiv.org/abs/2510.25077)
*Fahimeh Orvati Nia,Amirmohammad Mohammadi,Salim Al Kharsa,Pragati Naikare,Zigfried Hampel-Arias,Joshua Peeples*

Main category: cs.CV

TL;DR: 提出了一种名为邻域特征池化（NFP）的新型纹理特征提取方法，用于遥感图像分类。


<details>
  <summary>Details</summary>
Motivation: NFP旨在捕捉邻近输入之间的关系，并有效聚合特征维度上的局部相似性。

Method: NFP层使用卷积层实现，可以无缝集成到任何网络中。

Result: 与基线模型相比，NFP在不同数据集和架构上均能持续提高性能，同时保持最小的参数开销。

Conclusion: NFP是一种有效的遥感图像分类纹理特征提取方法。

Abstract: In this work, we propose neighborhood feature pooling (NFP) as a novel
texture feature extraction method for remote sensing image classification. The
NFP layer captures relationships between neighboring inputs and efficiently
aggregates local similarities across feature dimensions. Implemented using
convolutional layers, NFP can be seamlessly integrated into any network.
Results comparing the baseline models and the NFP method indicate that NFP
consistently improves performance across diverse datasets and architectures
while maintaining minimal parameter overhead.

</details>


### [36] [Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design](https://arxiv.org/abs/2510.25314)
*Zongxi Yu,Xiaolong Qian,Shaohua Gao,Qi Jiang,Yao Gao,Kailun Yang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为仿生单心成像（BMI）的框架，用于解决紧凑型RGBD成像中的挑战。该框架采用仿生全球面单心透镜设计，能够自然地将深度信息编码到点扩散函数（PSF）中，无需复杂的衍射或自由曲面元件。结合物理学模型和深度学习网络，BMI框架能够从单次编码捕获中同时恢复高保真全聚焦图像和精确深度图。实验证明，该方法在深度估计和图像恢复方面均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的紧凑型RGBD成像面临挑战：光学系统在整个景深内难以保证RGB清晰度，而纯软件的单目深度估计（MDE）则是一个依赖不可靠语义先验的病态问题。深度光学元件（如DOE）虽然能编码深度，但会增加制造复杂性并产生色差，影响简洁性。

Method: 提出一种新颖的仿生全球面单心透镜，并围绕其构建了仿生单心成像（BMI）框架。该光学设计通过其随深度变化的点扩散函数（PSF）自然地编码深度信息。利用基于物理的严格前向模型生成合成数据集，并将其与双头、多尺度重建网络相结合，该网络使用共享编码器从单个编码捕获中联合恢复高保真全聚焦（AiF）图像和精确深度图。

Result: 在深度估计方面，该方法达到了0.026的绝对相对误差（Abs Rel）和0.130的均方根误差（RMSE），显著优于领先的纯软件方法和其他深度光学系统。在图像恢复方面，该系统实现了0.960的结构相似性（SSIM）和0.082的感知LPIPS分数，证明了在图像保真度和深度准确性之间取得了卓越的平衡。

Conclusion: 仿生、全球面光学与联合重建算法的结合，是解决高性能紧凑型RGBD成像固有挑战的有效策略。

Abstract: Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.

</details>


### [37] [PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation with Controllable Face Attributes](https://arxiv.org/abs/2510.25084)
*Xiang liu,Zhaoxiang Liu,Huan Hu,Zipeng Wang,Ping Chen,Zezhou Chen,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 本研究提出一种新颖的、无需针对个体进行微调（PSTF）的人脸图像生成方法，能够精确控制面部属性并高保真地保留面部身份。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像生成技术在精确控制面部属性方面存在不足，尤其是在无需针对个体进行微调（PSTF）的情况下。虽然一些微调技术（如PreciseControl）可以实现精细控制，但需要专业知识和额外数据；而PSTF方法虽然简化了流程，但控制精度不高。

Method: 该方法利用人脸识别模型提取身份特征，并通过e4e编码器将其映射到StyleGAN2的$W^+$潜在空间。引入了Tri-Decoupled Cross-Attention模块，将身份特征、属性特征和文本嵌入集成到UNet架构中，实现了身份和属性信息的有效分离。

Result: 在FFHQ数据集上训练的模型能够生成具有精细面部属性控制且无需为单个身份进行额外微调或训练的个性化图像。

Conclusion: 本研究提出的方法成功地平衡了个性化与精确面部属性控制，为高质量、自适应的人脸图像合成提供了更高效、用户友好的解决方案。

Abstract: Recent advancements in personalized image generation have significantly
improved facial identity preservation, particularly in fields such as
entertainment and social media. However, existing methods still struggle to
achieve precise control over facial attributes in a per-subject-tuning-free
(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by
providing fine-grained control over facial features, but they often require
extensive technical expertise and additional training data, limiting their
accessibility. In contrast, PSTF approaches simplify the process by enabling
image generation from a single facial input, but they lack precise control over
facial attributes. In this paper, we introduce a novel, PSTF method that
enables both precise control over facial attributes and high-fidelity
preservation of facial identity. Our approach utilizes a face recognition model
to extract facial identity features, which are then mapped into the $W^+$
latent space of StyleGAN2 using the e4e encoder. We further enhance the model
with a Triplet-Decoupled Cross-Attention module, which integrates facial
identity, attribute features, and text embeddings into the UNet architecture,
ensuring clean separation of identity and attribute information. Trained on the
FFHQ dataset, our method allows for the generation of personalized images with
fine-grained control over facial attributes, while without requiring additional
fine-tuning or training data for individual identities. We demonstrate that our
approach successfully balances personalization with precise facial attribute
control, offering a more efficient and user-friendly solution for high-quality,
adaptable facial image synthesis. The code is publicly available at
https://github.com/UnicomAI/PSTF-AttControl.

</details>


### [38] [SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments](https://arxiv.org/abs/2510.25463)
*Hongjie Zhang,Gideon Billings,Stefan B. Williams*

Main category: cs.CV

TL;DR: SPADE是一个水下视觉感知方法，通过结合相对深度估计和稀疏深度先验，生成密集、度量尺度的深度图，提高了水下机器人空间感知能力，并能高效运行，有望应用于水下检查和干预。


<details>
  <summary>Details</summary>
Motivation: 水下基础设施因海洋环境恶劣而需要频繁检查和维护。目前依赖人类潜水员或远程操作车辆的方法存在感知和操作上的挑战，尤其是在复杂结构或浑浊水域中。提高水下机器人的空间感知能力是降低操作风险和实现更高自主性的关键。

Method: 提出SPADE（SParsity Adaptive Depth Estimator）方法，这是一个单目深度估计流程。该方法结合了预训练的相对深度估计器和稀疏深度先验，以生成密集、度量尺度的深度图。具体来说，该方法分为两个阶段：首先，使用稀疏深度点对相对深度图进行尺度估计；然后，利用提出的Cascade Conv-Deformable Transformer模块对最终的度量预测进行精炼。

Result: SPADE方法在准确性和泛化能力上优于现有技术水平，并且在嵌入式硬件上运行速度超过15 FPS，显示出其实际应用潜力。

Conclusion: SPADE方法能够提高水下机器人的空间感知能力，降低操作风险，并有望实现更高的自主性，可用于实际的水下检查和干预任务。

Abstract: Underwater infrastructure requires frequent inspection and maintenance due to
harsh marine conditions. Current reliance on human divers or remotely operated
vehicles is limited by perceptual and operational challenges, especially around
complex structures or in turbid water. Enhancing the spatial awareness of
underwater vehicles is key to reducing piloting risks and enabling greater
autonomy. To address these challenges, we present SPADE: SParsity Adaptive
Depth Estimator, a monocular depth estimation pipeline that combines
pre-trained relative depth estimator with sparse depth priors to produce dense,
metric scale depth maps. Our two-stage approach first scales the relative depth
map with the sparse depth points, then refines the final metric prediction with
our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves
improved accuracy and generalisation over state-of-the-art baselines and runs
efficiently at over 15 FPS on embedded hardware, promising to support practical
underwater inspection and intervention. This work has been submitted to IEEE
Journal of Oceanic Engineering Special Issue of AUV 2026.

</details>


### [39] [Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection](https://arxiv.org/abs/2510.25094)
*Chanhyeong Yang,Taehoon Song,Jihwan Park,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: VDRP框架通过视觉多样性感知和区域感知提示学习，解决了零样本人机交互检测中的视觉复杂性问题，并在HICO-DET基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理交互中的视觉复杂性方面存在不足，包括同一动作的不同视觉表现（类内视觉多样性）和不同动作的视觉相似性（类间视觉纠缠）。

Method: 提出了一种视觉多样性感知和区域感知提示学习框架（VDRP）。具体包括：1. 视觉多样性感知提示学习，通过注入组式视觉方差和应用高斯扰动来捕捉动作的多样性。2. 区域感知提示学习，从人、物体和联合区域检索特定概念，增强动作级别的区分度。

Result: 在HICO-DET基准测试的四种零样本评估设置下，该方法取得了最先进的性能，有效解决了类内视觉多样性和类间视觉纠缠的问题。

Conclusion: VDRP框架能够有效地处理零样本人机交互检测中的视觉复杂性，并在HICO-DET基准测试中实现了最先进的性能。

Abstract: Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.

</details>


### [40] [AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians](https://arxiv.org/abs/2510.25129)
*Xiyu Zhang,Chong Bao,Yipeng Chen,Hongjia Zhai,Yitong Dong,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的三维重建方法，结合了Atlanta-world模型和隐式结构化高斯泼溅技术，解决了低纹理区域的重建难题，并提高了重建的平滑度和细节保留能力，在室内和城市场景中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法在处理室内和城市环境的低纹理区域时，几何先验缺乏全局一致性；高斯泼溅和隐式SDF场方法存在不连续或计算效率低的问题，导致细节丢失。

Method: 提出了一种结合Atlanta-world模型和隐式结构化高斯泼溅（GS）的方法。利用Atlanta-world模型保证低纹理区域的表面重建精度，采用新的隐式结构化GS表示方法实现平滑重建，同时保持高频细节和效率。具体包括：提出语义GS表示来预测所有语义区域的概率；部署带有可学习平面指示器的结构平面正则化，以实现全局精确的表面重建。

Result: 在室内和城市场景的广泛实验中，本方法在表面重建质量上均优于最先进的方法。

Conclusion: 所提出的Atlanta-world引导的隐式结构化高斯泼溅方法，能够实现室内和城市场景的平滑重建，同时保持高频细节和渲染效率，并在重建质量方面超越了现有技术。

Abstract: 3D reconstruction of indoor and urban environments is a prominent research
topic with various downstream applications. However, existing geometric priors
for addressing low-texture regions in indoor and urban settings often lack
global consistency. Moreover, Gaussian Splatting and implicit SDF fields often
suffer from discontinuities or exhibit computational inefficiencies, resulting
in a loss of detail. To address these issues, we propose an Atlanta-world
guided implicit-structured Gaussian Splatting that achieves smooth indoor and
urban scene reconstruction while preserving high-frequency details and
rendering efficiency. By leveraging the Atlanta-world model, we ensure the
accurate surface reconstruction for low-texture regions, while the proposed
novel implicit-structured GS representations provide smoothness without
sacrificing efficiency and high-frequency details. Specifically, we propose a
semantic GS representation to predict the probability of all semantic regions
and deploy a structure plane regularization with learnable plane indicators for
global accurate surface reconstruction. Extensive experiments demonstrate that
our method outperforms state-of-the-art approaches in both indoor and urban
scenes, delivering superior surface reconstruction quality.

</details>


### [41] [Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks](https://arxiv.org/abs/2510.25134)
*Qingdong Cai,Charith Abhayaratne*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Class Activation Mapping (CAM) methods are widely applied in weakly
supervised learning tasks due to their ability to highlight object regions.
However, conventional CAM methods highlight only the most discriminative
regions of the target. These highlighted regions often fail to cover the entire
object and are frequently misaligned with object boundaries, thereby limiting
the performance of downstream weakly supervised learning tasks, particularly
Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise
accurate activation maps to get the best results. To alleviate the above
problems, we propose a novel activation method, Region-CAM. Distinct from
network feature weighting approaches, Region-CAM generates activation maps by
extracting semantic information maps (SIMs) and performing semantic information
propagation (SIP) by considering both gradients and features in each of the
stages of the baseline classification model. Our approach highlights a greater
proportion of object regions while ensuring activation maps to have precise
boundaries that align closely with object edges. Region-CAM achieves 60.12% and
58.43% mean intersection over union (mIoU) using the baseline model on the
PASCAL VOC training and validation datasets, respectively, which are
improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On
the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement
over the original CAM (20.15%). We also demonstrate the superiority of
Region-CAM in object localization tasks, using the ILSVRC2012 validation set.
Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with
LayerCAM, an activation method designed for weakly supervised object
localization, Region-CAM achieves 4.5% better performance in Loc1.

</details>


### [42] [DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications](https://arxiv.org/abs/2510.25140)
*Malaisree P,Youwai S,Kitkobsin T,Janrungautai S,Amorndechaphon D,Rojanavasu P*

Main category: cs.CV

TL;DR: DINO-YOLO 是一种结合 YOLOv12 和 DINOv3 的混合架构，用于在标注数据有限的工程领域进行目标检测，并在多个数据集上实现了性能提升和实时推理。


<details>
  <summary>Details</summary>
Motivation:  सिविल इंजीनियरिंग अनुप्रयोगों में सीमित एनोटेट किए गए डेटा के कारण ऑब्जेक्ट डिटेक्शन में चुनौतियाँ।

Method: DINOv3 सुविधाओं को इनपुट प्रीप्रोसेसिंग (P0) और मिड-बैकबोन एन्हांसमेंट (P3) में एकीकृत करके YOLOv12 के साथ एक हाइब्रिड आर्किटेक्चर (DINO-YOLO) का विकास।

Result: टनल सेगमेंट क्रैक डिटेक्शन (648 छवियां) में 12.4% सुधार, कंस्ट्रक्शन पीपीई (1K छवियां) में 13.7% सुधार, और किटी (7K छवियां) में 88.6% सुधार, साथ ही 30-47 FPS पर रियल-टाइम अनुमान बनाए रखना। विभिन्न YOLO स्केल और DINOv3 वेरिएंट पर एब्लेशन अध्ययन ने विभिन्न एकीकरण रणनीतियों के साथ इष्टतम प्रदर्शन का खुलासा किया।

Conclusion: DINO-YOLO सिविल इंजीनियरिंग डेटासेट (<10K छवियां) के लिए अत्याधुनिक प्रदर्शन स्थापित करता है, कम्प्यूटेशनल दक्षता को बनाए रखता है, और डेटा-बाधित वातावरण में निर्माण सुरक्षा निगरानी और बुनियादी ढांचे निरीक्षण के लिए व्यावहारिक समाधान प्रदान करता है।

Abstract: Object detection in civil engineering applications is constrained by limited
annotated data in specialized domains. We introduce DINO-YOLO, a hybrid
architecture combining YOLOv12 with DINOv3 self-supervised vision transformers
for data-efficient detection. DINOv3 features are strategically integrated at
two locations: input preprocessing (P0) and mid-backbone enhancement (P3).
Experimental validation demonstrates substantial improvements: Tunnel Segment
Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K
images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while
maintaining real-time inference (30-47 FPS). Systematic ablation across five
YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures
achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while
Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead
(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on
NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil
engineering datasets (<10K images) while preserving computational efficiency,
providing practical solutions for construction safety monitoring and
infrastructure inspection in data-constrained environments.

</details>


### [43] [Revisiting Reconstruction-based AI-generated Image Detection: A Geometric Perspective](https://arxiv.org/abs/2510.25141)
*Wan Jiang,Jing Yan,Ruixuan Zhang,Xiaojing Chen,Changtao Miao,Zhe Li,Chenhao Lin,Yunfeng Diao,Richang Hong*

Main category: cs.CV

TL;DR: 生成图像检测的挑战与新方法


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏理论基础且依赖经验推断，限制了可解释性和可靠性。

Method: 提出基于雅可比谱的重建误差下界（JSLB），并结合结构化编辑操作实现动态重建误差计算（ReGap），以提高检测精度。

Result: ReGap方法在检测准确性、鲁棒性（对后处理操作）和泛化能力（跨不同条件）方面优于现有基线方法。

Conclusion: JSLB和ReGap为AI生成图像检测提供了更具理论依据和实践价值的解决方案。

Abstract: The rise of generative Artificial Intelligence (AI) has made detecting
AI-generated images a critical challenge for ensuring authenticity. Existing
reconstruction-based methods lack theoretical foundations and on empirical
heuristics, limiting interpretability and reliability. In this paper, we
introduce the Jacobian-Spectral Lower Bound for reconstruction error from a
geometric perspective, showing that real images off the reconstruction manifold
exhibit a non-trivial error lower bound, while generated images on the manifold
have near-zero error. Furthermore, we reveal the limitations of existing
methods that rely on static reconstruction error from a single pass. These
methods often fail when some real images exhibit lower error than generated
ones. This counterintuitive behavior reduces detection accuracy and requires
data-specific threshold tuning, limiting their applicability in real-world
scenarios. To address these challenges, we propose ReGap, a training-free
method that computes dynamic reconstruction error by leveraging structured
editing operations to introduce controlled perturbations. This enables
measuring error changes before and after editing, improving detection accuracy
by enhancing error separation. Experimental results show that our method
outperforms existing baselines, exhibits robustness to common post-processing
operations and generalizes effectively across diverse conditions.

</details>


### [44] [EA3D: Online Open-World 3D Object Extraction from Streaming Videos](https://arxiv.org/abs/2510.25146)
*Xiaoyu Zhou,Jingqi Wang,Yuang Jia,Yongtao Wang,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: EA3D是一个统一的在线框架，用于从流式视频中进行3D物体提取，同时进行几何重建和场景理解。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景理解方法受限于离线收集的多视图数据或预先构建的3D几何。EA3D旨在解决此限制，实现在线、开放世界的3D物体提取。

Method: EA3D利用视觉-语言和2D视觉基础编码器动态解释视频帧，提取物体级知识，并通过前馈在线更新策略将其集成到高斯特征图中。通过迭代估计视觉里程计并增量更新高斯特征，同时利用循环联合优化模块增强几何重建和语义理解。

Result: EA3D在照片级渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成等多个基准和任务上都取得了有效性。

Conclusion: EA3D提供了一个统一高效的框架，用于联合在线3D重建和整体场景理解，支持广泛的下游任务。

Abstract: Current 3D scene understanding methods are limited by offline-collected
multi-view data or pre-constructed 3D geometry. In this paper, we present
ExtractAnything3D (EA3D), a unified online framework for open-world 3D object
extraction that enables simultaneous geometric reconstruction and holistic
scene understanding. Given a streaming video, EA3D dynamically interprets each
frame using vision-language and 2D vision foundation encoders to extract
object-level knowledge. This knowledge is integrated and embedded into a
Gaussian feature map via a feed-forward online update strategy. We then
iteratively estimate visual odometry from historical frames and incrementally
update online Gaussian features with new observations. A recurrent joint
optimization module directs the model's attention to regions of interest,
simultaneously enhancing both geometric reconstruction and semantic
understanding. Extensive experiments across diverse benchmarks and tasks,
including photo-realistic rendering, semantic and instance segmentation, 3D
bounding box and semantic occupancy estimation, and 3D mesh generation,
demonstrate the effectiveness of EA3D. Our method establishes a unified and
efficient framework for joint online 3D reconstruction and holistic scene
understanding, enabling a broad range of downstream tasks.

</details>


### [45] [Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from Interference Patterns Using Vision Transformers](https://arxiv.org/abs/2510.25157)
*Gautam A. Viruthagiri,Arnuv Tandon,Gerald G. Fuller,Vinny Chandran Suja*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的方法，可以直接从干涉图重建薄液膜厚度分布，解决了传统方法的计算密集、噪声敏感和手动分析等问题。


<details>
  <summary>Details</summary>
Motivation: 薄膜干涉测量技术在眼科等领域有应用潜力，但从干涉图重建厚度分布是一个具有挑战性的逆问题，传统方法存在计算量大、对噪声敏感或需要手动分析等缺点，限制了其临床应用。

Method: 提出了一种基于视觉Transformer的方法，利用长程空间相关性来解决相位模糊问题，并从动态干涉图中重建时间连贯的厚度分布。

Result: 该模型在处理含有噪声、运动伪影的快速变化的薄膜时表现出最先进的性能，克服了传统相位展开和迭代拟合方法的局限性。

Conclusion: 该数据驱动的方法能够以实时速度在消费级硬件上实现自动、一致的厚度重建，为连续监测隐形眼镜前泪膜和非侵入性诊断如干眼症提供了新的可能性。

Abstract: Thin film interferometry is a powerful technique for non-invasively measuring
liquid film thickness with applications in ophthalmology, but its clinical
translation is hindered by the challenges in reconstructing thickness profiles
from interference patterns - an ill-posed inverse problem complicated by phase
periodicity, imaging noise and ambient artifacts. Traditional reconstruction
methods are either computationally intensive, sensitive to noise, or require
manual expert analysis, which is impractical for real-time diagnostics. To
address this challenge, here we present a vision transformer-based approach for
real-time inference of thin liquid film thickness profiles directly from
isolated interferograms. Trained on a hybrid dataset combining
physiologically-relevant synthetic and experimental tear film data, our model
leverages long-range spatial correlations to resolve phase ambiguities and
reconstruct temporally coherent thickness profiles in a single forward pass
from dynamic interferograms acquired in vivo and ex vivo. The network
demonstrates state-of-the-art performance on noisy, rapidly-evolving films with
motion artifacts, overcoming limitations of conventional phase-unwrapping and
iterative fitting methods. Our data-driven approach enables automated,
consistent thickness reconstruction at real-time speeds on consumer hardware,
opening new possibilities for continuous monitoring of pre-lens ocular tear
films and non-invasive diagnosis of conditions such as the dry eye disease.

</details>


### [46] [Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation](https://arxiv.org/abs/2510.25163)
*Wenhao Zheng,Chenwei Sun,Wenbo Zhang,Jiancheng Lv,Xianggen Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep generative models, such as diffusion models, have shown promising
progress in image generation and audio generation via simplified continuity
assumptions. However, the development of generative modeling techniques for
generating multi-modal data, such as parametric CAD sequences, still lags
behind due to the challenges in addressing long-range constraints and parameter
sensitivity. In this work, we propose a novel framework for quantitatively
constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).
For the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,
discrete commands and continuous parameters) in a unified continuous and
differentiable parameter space rather than in the discrete data space. In
addition, TGBFN penetrates the parameter update kernel and introduces a guided
Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a
new dataset for quantitatively constrained CAD generation. Extensive
comparisons across single-condition and multi-condition constrained generation
tasks demonstrate that TGBFN achieves state-of-the-art performance in
generating high-fidelity, condition-aware CAD sequences. The code is available
at https://github.com/scu-zwh/TGBFN.

</details>


### [47] [A Study on Inference Latency for Vision Transformers on Mobile Devices](https://arxiv.org/abs/2510.25166)
*Zhuojin Li,Marco Paolieri,Leana Golubchik*

Main category: cs.CV

TL;DR: 本研究量化评估了190个视觉Transformer（ViTs）在移动设备上的性能，并与102个卷积神经网络（CNNs）进行了比较，以探究影响ViT移动端延迟的因素。研究人员开发了一个包含1000个合成ViT测量延迟的数据集，并证明了该数据集可用于准确预测新ViT模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在移动设备上，尤其是在计算机视觉领域取得显著进展，本研究旨在量化评估视觉Transformer（ViTs）在移动设备上的性能特征。

Method: 研究人员对190个视觉Transformer（ViTs）在移动设备上的性能进行了量化研究，并将其与102个卷积神经网络（CNNs）进行了比较，以分析影响ViT移动端延迟的因素。在此基础上，研究人员构建了一个包含1000个合成ViT模型（包含代表性构建块和最先进架构）在两个机器学习框架和六个移动平台上的测量延迟的数据集。

Result: 研究人员构建了一个包含1000个合成ViT模型在不同平台和框架下的测量延迟的数据集。实验证明，利用该数据集能够以满足实际应用需求的精度来预测新ViT模型的推理延迟。

Conclusion: 本研究通过量化评估ViTs在移动设备上的性能，并构建了一个包含大量模型测量数据的预测数据集，证明了ViT模型在移动端的延迟可以被准确预测，为ViT在移动设备上的应用提供了参考。

Abstract: Given the significant advances in machine learning techniques on mobile
devices, particularly in the domain of computer vision, in this work we
quantitatively study the performance characteristics of 190 real-world vision
transformers (ViTs) on mobile devices. Through a comparison with 102 real-world
convolutional neural networks (CNNs), we provide insights into the factors that
influence the latency of ViT architectures on mobile devices. Based on these
insights, we develop a dataset including measured latencies of 1000 synthetic
ViTs with representative building blocks and state-of-the-art architectures
from two machine learning frameworks and six mobile platforms. Using this
dataset, we show that inference latency of new ViTs can be predicted with
sufficient accuracy for real-world applications.

</details>


### [48] [$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction](https://arxiv.org/abs/2510.25173)
*Kejing Xia,Jidong Jia,Ke Jin,Yucai Bai,Li Sun,Dacheng Tao,Youjian Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 D^2GS 的无需 LiDAR 的城市场景重建框架，通过多视图深度预测初始化密集点云，并结合扩散模型增强深度图，最终在 Waymo 数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有城市场景重建方法依赖于 LiDAR 和图像等多种传感器，但 LiDAR 数据的获取存在标定和重投影误差等挑战。本研究旨在提出一种无需 LiDAR 的方法，以获取更有效、更密集、更精确的几何先验。

Method: 1. 通过反投影多视图度量深度预测来初始化密集点云，并通过渐进式剪枝策略优化点云以提高全局一致性。 2. 利用扩散模型的先验知识增强高斯渲染出的深度图，并在此基础上联合优化高斯几何和预测的密集度量深度。 3. 通过约束道路区域内高斯的形状和法线属性来提高道路几何的精度。

Result: 在 Waymo 数据集上的广泛实验表明，该方法在几何精度上持续优于最先进的方法，即使与使用真实 LiDAR 数据的基线方法相比也表现更佳。

Conclusion: D^2GS 成功实现了无需 LiDAR 的城市场景重建，并通过多视图深度预测、扩散模型和几何约束等技术，在保持高精度几何的同时，解决了现有方法在数据获取和处理上的难题。

Abstract: Recently, Gaussian Splatting (GS) has shown great potential for urban scene
reconstruction in the field of autonomous driving. However, current urban scene
reconstruction methods often depend on multimodal sensors as inputs,
\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
point clouds can largely mitigate ill-posedness in reconstruction, acquiring
such accurate LiDAR data is still challenging in practice: i) precise
spatiotemporal calibration between LiDAR and other sensors is required, as they
may not capture data simultaneously; ii) reprojection errors arise from spatial
misalignment when LiDAR and cameras are mounted at different locations. To
avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
LiDAR-free urban scene reconstruction framework. In this work, we obtain
geometry priors that are as effective as LiDAR while being denser and more
accurate. $\textbf{First}$, we initialize a dense point cloud by
back-projecting multi-view metric depth predictions. This point cloud is then
optimized by a Progressive Pruning strategy to improve the global consistency.
$\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
from a depth foundation model to enhance the depth maps rendered by Gaussians.
In turn, the enhanced depths provide stronger geometric constraints during
Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
geometry by constraining the shape and normal attributes of Gaussians within
road regions. Extensive experiments on the Waymo dataset demonstrate that our
method consistently outperforms state-of-the-art methods, producing more
accurate geometry even when compared with those using ground-truth LiDAR data.

</details>


### [49] [Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation](https://arxiv.org/abs/2510.25174)
*Huadong Tang,Youpeng Zhao,Min Xu,Jun Wang,Qiang Wu*

Main category: cs.CV

TL;DR: ECAC通过结合全局和局部上下文信息动态调整分类器，解决了类别不平衡和图像独特性问题，并在多个数据集上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义分割方法使用固定的分类器，无法适应不同图像的类别分布和数据集中的类别不平衡问题，导致对少数类别的分割效果不佳。

Method: 提出ECAC（Extended Context-Aware Classifier），利用记忆库学习全局上下文信息，并结合当前图像的局部上下文信息动态调整分类器。同时采用教师-学生网络范式，由教师网络进行知识迁移。

Result: ECAC在ADE20K、COCO-Stuff10K和Pascal-Context等多个数据集上实现了最先进的性能。

Conclusion: ECAC能够有效解决类别不平衡和图像独特性问题，提升语义分割的准确性。

Abstract: Prevalent semantic segmentation methods generally adopt a vanilla classifier
to categorize each pixel into specific classes.
  Although such a classifier learns global information from the training data,
this information is represented by a set of fixed parameters (weights and
biases).
  However, each image has a different class distribution, which prevents the
classifier from addressing the unique characteristics of individual images.
  At the dataset level, class imbalance leads to segmentation results being
biased towards majority classes, limiting the model's effectiveness in
identifying and segmenting minority class regions.
  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that
dynamically adjusts the classifier using global (dataset-level) and local
(image-level) contextual information.
  Specifically, we leverage a memory bank to learn dataset-level contextual
information of each class, incorporating the class-specific contextual
information from the current image to improve the classifier for precise pixel
labeling.
  Additionally, a teacher-student network paradigm is adopted, where the domain
expert (teacher network) dynamically adjusts contextual information with ground
truth and transfers knowledge to the student network.
  Comprehensive experiments illustrate that the proposed ECAC can achieve
state-of-the-art performance across several datasets, including ADE20K,
COCO-Stuff10K, and Pascal-Context.

</details>


### [50] [Test-Time Adaptive Object Detection with Foundation Model](https://arxiv.org/abs/2510.25175)
*Yingjie Gao,Yanan Zhang,Zhi Cai,Di Huang*

Main category: cs.CV

TL;DR: 提出了一种基于基础模型的测试时自适应目标检测方法，无需源数据，克服了传统闭集限制，并能适应任意跨域和跨类别目标数据。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应目标检测方法依赖源域统计特性且假设域间类别空间相同，与实际应用场景脱节。

Method: 设计了多模态提示学习的均值教师框架，结合文本和视觉提示调优，并采用测试时预热策略。引入实例动态记忆模块（IDM）及内存增强和内存幻觉策略，用于生成高质量伪标签。

Result: 在交叉腐败和交叉数据集基准测试中，该方法持续优于现有最先进方法。

Conclusion: 所提出的方法能够有效进行测试时自适应目标检测，无需源数据，并能处理开放集问题。

Abstract: In recent years, test-time adaptive object detection has attracted increasing
attention due to its unique advantages in online domain adaptation, which
aligns more closely with real-world application scenarios. However, existing
approaches heavily rely on source-derived statistical characteristics while
making the strong assumption that the source and target domains share an
identical category space. In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.
Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
vision-language detector-driven test-time adaptation, which incorporates text
and visual prompt tuning to adapt both language and vision representation
spaces on the test data in a parameter-efficient manner. Correspondingly, we
propose a Test-time Warm-start strategy tailored for the visual prompts to
effectively preserve the representation capability of the vision branch.
Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively. Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data. Code is available at
https://github.com/gaoyingjay/ttaod_foundation.

</details>


### [51] [Mask-Robust Face Verification for Online Learning via YOLOv5 and Residual Networks](https://arxiv.org/abs/2510.25184)
*Zhifeng Wang,Minghui Wang,Chunyan Zeng,Jialong Yao,Yang Yang,Hongmin Xu*

Main category: cs.CV

TL;DR: 本研究提出了一种基于改进卷积神经网络（残差网络模型）的在线学习身份认证解决方案，利用YOLOv5进行人脸识别，并通过欧氏距离比对数据库来验证学生身份，以提升在线教育的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了应对数字化和人工智能发展趋势以及新冠疫情推动的在线教育发展，需要解决在线学习环境中的身份认证问题，以保障在线教育的安全性和稳定性。

Method: 研究提出部署YOLOv5网络，并在自建数据集上进行训练，用于识别学生通过在线摄像头捕捉图像中的人脸。随后，将提取到的人脸特征输入残差网络进行深度特征提取，并通过计算欧氏距离与学生人脸数据库进行比对，以确认学生身份。

Result: 通过部署YOLOv5和残差网络模型，并进行欧氏距离比对，能够有效地识别和验证在线学习中的学生身份。

Conclusion: 本研究提出的基于YOLOv5和残差网络的身份认证方法，能够有效提升在线教育的身份认证安全性与稳定性，以适应教育环境的快速发展。

Abstract: In the contemporary landscape, the fusion of information technology and the
rapid advancement of artificial intelligence have ushered school education into
a transformative phase characterized by digitization and heightened
intelligence. Concurrently, the global paradigm shift caused by the Covid-19
pandemic has catalyzed the evolution of e-learning, accentuating its
significance. Amidst these developments, one pivotal facet of the online
education paradigm that warrants attention is the authentication of identities
within the digital learning sphere. Within this context, our study delves into
a solution for online learning authentication, utilizing an enhanced
convolutional neural network architecture, specifically the residual network
model. By harnessing the power of deep learning, this technological approach
aims to galvanize the ongoing progress of online education, while concurrently
bolstering its security and stability. Such fortification is imperative in
enabling online education to seamlessly align with the swift evolution of the
educational landscape. This paper's focal proposition involves the deployment
of the YOLOv5 network, meticulously trained on our proprietary dataset. This
network is tasked with identifying individuals' faces culled from images
captured by students' open online cameras. The resultant facial information is
then channeled into the residual network to extract intricate features at a
deeper level. Subsequently, a comparative analysis of Euclidean distances
against students' face databases is performed, effectively ascertaining the
identity of each student.

</details>


### [52] [AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis](https://arxiv.org/abs/2510.25199)
*Manisha More,Kavya Bhand,Kaustubh Mukdam,Kavya Sharma,Manas Kawtikwar,Hridayansh Kaware,Prajwal Kavhar*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Early diagnosis of critical diseases can significantly improve patient
survival and reduce treatment costs. However, existing diagnostic techniques
are often costly, invasive, and inaccessible in low-resource regions. This
paper presents a multimodal artificial intelligence (AI) diagnostic framework
integrating image analysis, thermal imaging, and audio signal processing for
early detection of three major health conditions: skin cancer, vascular blood
clots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2
convolutional neural network was trained on the ISIC 2019 dataset for skin
lesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%
specificity. A support vector machine (SVM) with handcrafted features was
employed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on
synthetic and clinical data. For cardiopulmonary analysis, lung and heart sound
datasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral
Coefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy
and 85.7% sensitivity. Comparative evaluation against state-of-the-art models
demonstrates that the proposed system achieves competitive results while
remaining lightweight and deployable on low-cost devices. The framework
provides a promising step toward scalable, real-time, and accessible AI-based
pre-diagnostic healthcare solutions.

</details>


### [53] [U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching](https://arxiv.org/abs/2510.25210)
*Junsheng Zhou,Xingyu Shi,Haichuan Song,Yi Fang,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: U-CAN是一个无监督的点云去噪框架，通过一致性感知噪声2噪声匹配，利用多步去噪路径和新颖的损失函数，在点云和图像去噪任务上均取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的点云去噪方法大多依赖于手动标注的噪声-干净点云对进行监督学习，成本高昂。

Method: 提出了一种名为U-CAN的无监督去噪框架，利用噪声2噪声匹配方案，通过新颖的损失函数实现多步去噪路径推断，并引入了几何一致性约束。

Result: 在点云去噪、点云补全和图像去噪的基准测试中，U-CAN的性能显著优于现有的无监督方法，并且在点云去噪任务上取得了与监督方法相当的结果。

Conclusion: U-CAN是一种有效的无监督点云去噪方法，其提出的几何一致性约束具有通用性，可应用于2D图像去噪等领域。

Abstract: Point clouds captured by scanning sensors are often perturbed by noise, which
have a highly negative impact on downstream tasks (e.g. surface reconstruction
and shape understanding). Previous works mostly focus on training neural
networks with noisy-clean point cloud pairs for learning denoising priors,
which requires extensively manual efforts. In this work, we introduce U-CAN, an
Unsupervised framework for point cloud denoising with Consistency-Aware
Noise2Noise matching. Specifically, we leverage a neural network to infer a
multi-step denoising path for each point of a shape or scene with a noise to
noise matching scheme. We achieve this by a novel loss which enables
statistical reasoning on multiple noisy point cloud observations. We further
introduce a novel constraint on the denoised geometry consistency for learning
consistency-aware denoising patterns. We justify that the proposed constraint
is a general term which is not limited to 3D domain and can also contribute to
the area of 2D image denoising. Our evaluations under the widely used
benchmarks in point cloud denoising, upsampling and image denoising show
significant improvement over the state-of-the-art unsupervised methods, where
U-CAN also produces comparable results with the supervised methods.

</details>


### [54] [MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo](https://arxiv.org/abs/2510.25221)
*Shiyu Qin,Zhihao Cai,Kaixuan Wang,Lin Qi,Junyu Dong*

Main category: cs.CV

TL;DR: MSF-Net通过多阶段特征提取和选择性更新策略，以及特征融合模块，提高了表面法线估计的准确性，尤其是在处理褶皱和边缘等细节区域时。


<details>
  <summary>Details</summary>
Motivation: 现有的学习方法在多阶段特征提取和特征交互方面存在不足，导致在细节区域提取冗余特征，影响法线估计的准确性。

Method: 提出MSF-Net框架，采用多阶段信息提取和选择性更新策略，并设计特征融合模块，以提取高质量特征并促进不同特征间的交互。

Result: 在DiLiGenT基准测试中，MSF-Net显著优于现有的最先进方法。

Conclusion: MSF-Net在表面法线估计方面取得了显著的性能提升，尤其是在处理复杂细节时。

Abstract: Photometric stereo is a technique aimed at determining surface normals
through the utilization of shading cues derived from images taken under
different lighting conditions. However, existing learning-based approaches
often fail to accurately capture features at multiple stages and do not
adequately promote interaction between these features. Consequently, these
models tend to extract redundant features, especially in areas with intricate
details such as wrinkles and edges. To tackle these issues, we propose MSF-Net,
a novel framework for extracting information at multiple stages, paired with
selective update strategy, aiming to extract high-quality feature information,
which is critical for accurate normal construction. Additionally, we have
developed a feature fusion module to improve the interplay among different
features. Experimental results on the DiLiGenT benchmark show that our proposed
MSF-Net significantly surpasses previous state-of-the-art methods in the
accuracy of surface normal estimation.

</details>


### [55] [Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2510.25227)
*Quang-Khai Bui-Tran,Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Nguyen Lan Vi Vu,Phat K. Huynh,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的无源域自适应（SFDA）框架，通过难例选择和去噪块混合来逐步对齐目标分布，以解决现有方法在样本难度和噪声标签下的不足，并在医学图像分割任务上取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法在医学图像分割中忽视了样本难度且在域偏移下难以处理带噪声的监督，本研究旨在解决这些问题。

Method: 提出一个SFDA框架，包括难例选择（通过熵-相似性分析将未标记图像划分为可靠和不可靠子集，从易样本开始适应并逐渐加入难样本）和去噪块混合（通过蒙特卡洛去噪掩码精炼伪标签，抑制不可靠像素并稳定训练）。此外，还使用了域内和域间目标混合块来传递可靠语义并减轻噪声。

Result: 在基准数据集上的实验显示，该框架在SFDA和UDA方法上均取得了持续的性能提升，实现了更准确的边界描绘，并在Dice和ASSD分数上达到了最先进的水平。

Conclusion: 研究强调了在域偏移下进行渐进式自适应和去噪监督对于实现鲁棒分割的重要性。

Abstract: Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for
medical image segmentation under privacy constraints, yet current approaches
often ignore sample difficulty and struggle with noisy supervision under domain
shift. We present a new SFDA framework that leverages Hard Sample Selection and
Denoised Patch Mixing to progressively align target distributions. First,
unlabeled images are partitioned into reliable and unreliable subsets through
entropy-similarity analysis, allowing adaptation to start from easy samples and
gradually incorporate harder ones. Next, pseudo-labels are refined via Monte
Carlo-based denoising masks, which suppress unreliable pixels and stabilize
training. Finally, intra- and inter-domain objectives mix patches between
subsets, transferring reliable semantics while mitigating noise. Experiments on
benchmark datasets show consistent gains over prior SFDA and UDA methods,
delivering more accurate boundary delineation and achieving state-of-the-art
Dice and ASSD scores. Our study highlights the importance of progressive
adaptation and denoised supervision for robust segmentation under domain shift.

</details>


### [56] [Balanced conic rectified flow](https://arxiv.org/abs/2510.25229)
*Kim Shin Seong,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: 生成模型在学习平滑传输映射时，通过ODE学习，与扩散模型不同，其ODE不需要数值积分，从而实现高效高质量图像生成。但现有方法在训练时需要大量生成图像对，并且易受生成数据偏差影响。本文提出一种新方法，将真实图像纳入训练，减少对生成数据的依赖，提高训练效率和模型鲁棒性，并在CIFAR-10上取得了更好的FID分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练过程中需要大量的生成图像对，计算成本高，并且模型性能受生成数据偏差影响。

Method: 将真实图像纳入训练，通过保持真实图像的ODE路径，减少对生成数据的依赖，并使用更少量的生成和真实图像对进行高效的reflow过程。

Result: 在CIFAR-10上，使用更少量的生成图像对（仅为原始方法的1/10），在单步和全步生成中都取得了显著更好的FID分数。同时，该方法诱导了更直的路径，避免了reflow过程中生成图像的饱和，从而实现了更鲁棒的ODE学习，并保持了真实图像的分布。

Conclusion: 通过将真实图像纳入训练，本研究有效解决了现有方法在计算成本和数据偏差方面的问题，提高了生成模型的效率和鲁棒性。

Abstract: Rectified flow is a generative model that learns smooth transport mappings
between two distributions through an ordinary differential equation (ODE).
Unlike diffusion-based generative models, which require costly numerical
integration of a generative ODE to sample images with state-of-the-art quality,
rectified flow uses an iterative process called reflow to learn smooth and
straight ODE paths. This allows for relatively simple and efficient generation
of high-quality images. However, rectified flow still faces several challenges.
1) The reflow process requires a large number of generative pairs to preserve
the target distribution, leading to significant computational costs. 2) Since
the model is typically trained using only generated image pairs, its
performance heavily depends on the 1-rectified flow model, causing it to become
biased towards the generated data.
  In this work, we experimentally expose the limitations of the original
rectified flow and propose a novel approach that incorporates real images into
the training process. By preserving the ODE paths for real images, our method
effectively reduces reliance on large amounts of generated data. Instead, we
demonstrate that the reflow process can be conducted efficiently using a much
smaller set of generated and real images. In CIFAR-10, we achieved
significantly better FID scores, not only in one-step generation but also in
full-step simulations, while using only of the generative pairs compared to the
original method. Furthermore, our approach induces straighter paths and avoids
saturation on generated images during reflow, leading to more robust ODE
learning while preserving the distribution of real images.

</details>


### [57] [DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis](https://arxiv.org/abs/2510.25237)
*Yinqi Cai,Jichang Li,Zhaolun Li,Weikai Chen,Rushi Lan,Xi Xie,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: DeepShield框架通过结合局部敏感性和全局泛化能力，提高了对未见过的深度伪造技术的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术易被滥用于欺诈和虚假信息，现有检测方法在跨技术检测时泛化能力不足。

Method: DeepShield框架增强了CLIP-ViT编码器，通过局部块引导（LPG）捕捉细微不一致性，并通过全局伪造多样性（GFD）生成多样化伪造样本，增强跨域适应性。

Result: DeepShield在跨数据集和跨操纵评估中表现优于现有技术，对未知的深度伪造攻击具有更强的鲁棒性。

Conclusion: DeepShield框架通过整合局部和全局分析，有效提高了深度伪造检测的鲁棒性。

Abstract: Recent advances in deep generative models have made it easier to manipulate
face videos, raising significant concerns about their potential misuse for
fraud and misinformation. Existing detectors often perform well in in-domain
scenarios but fail to generalize across diverse manipulation techniques due to
their reliance on forgery-specific artifacts. In this work, we introduce
DeepShield, a novel deepfake detection framework that balances local
sensitivity and global generalization to improve robustness across unseen
forgeries. DeepShield enhances the CLIP-ViT encoder through two key components:
Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG
applies spatiotemporal artifact modeling and patch-wise supervision to capture
fine-grained inconsistencies often overlooked by global models. GFD introduces
domain feature augmentation, leveraging domain-bridging and boundary-expanding
feature generation to synthesize diverse forgeries, mitigating overfitting and
enhancing cross-domain adaptability. Through the integration of novel local and
global analysis for deepfake detection, DeepShield outperforms state-of-the-art
methods in cross-dataset and cross-manipulation evaluations, achieving superior
robustness against unseen deepfake attacks.

</details>


### [58] [VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations](https://arxiv.org/abs/2510.25238)
*Qianqian Qiao,DanDan Zheng,Yihang Bo,Bao Peng,Heng Huang,Longteng Jiang,Huaye Wang,Jingdong Chen,Jun Zhou,Xin Jin*

Main category: cs.CV

TL;DR: 该研究提出了VADB数据集和VADB-Net模型，以解决视频美学评估中的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频美学评估在多媒体计算中很重要，但由于缺乏标准化数据集和鲁棒模型，以及视频的时间动态和多模态融合的挑战，其进展受到限制。现有的基于图像的方法难以直接应用。

Method: 构建了包含10,490个视频的VADB数据集，并由37名专业人士从多个美学维度（包括整体和属性特定的美学分数、丰富的语言评论和客观标签）进行注释。提出了一个名为VADB-Net的双模态预训练框架，采用两阶段训练策略。

Result: VADB-Net在评分任务上表现优于现有的视频质量评估模型，并支持下游的视频美学评估任务。

Conclusion: VADB数据集和VADB-Net模型为视频美学评估领域提供了重要的资源和进展。

Abstract: Video aesthetic assessment, a vital area in multimedia computing, integrates
computer vision with human cognition. Its progress is limited by the lack of
standardized datasets and robust models, as the temporal dynamics of video and
multimodal fusion challenges hinder direct application of image-based methods.
This study introduces VADB, the largest video aesthetic database with 10,490
diverse videos annotated by 37 professionals across multiple aesthetic
dimensions, including overall and attribute-specific aesthetic scores, rich
language comments and objective tags. We propose VADB-Net, a dual-modal
pre-training framework with a two-stage training strategy, which outperforms
existing video quality assessment models in scoring tasks and supports
downstream video aesthetic assessment tasks. The dataset and source code are
available at https://github.com/BestiVictory/VADB.

</details>


### [59] [Mapping and Classification of Trees Outside Forests using Deep Learning](https://arxiv.org/abs/2510.25239)
*Moritz Lucas,Hamid Ebrahimy,Viacheslav Barkov,Ralf Pecenka,Kai-Uwe Kühnberger,Björn Waske*

Main category: cs.CV

TL;DR: 深度学习方法在德国四个农业景观的森林外树木（TOF）分类中表现良好，FT-UNetFormer 模型效果最佳，但在区分复杂和边缘密度高的斑块和树木类别时面临挑战，需要区域多样化的训练数据来实现可靠的大规模绘图。


<details>
  <summary>Details</summary>
Motivation: 大多数研究将森林外树木（TOF）视为单一类别或依赖于僵化的基于规则的阈值，这限制了其生态学解释和跨区域的适应性。本研究旨在评估深度学习在 TOF 分类中的应用，以克服这些限制。

Method: 本研究使用新生成的数据集和来自德国四个农业景观的高分辨率航空影像，评估了卷积神经网络（CNN）、视觉 Transformer 和混合 CNN-Transformer 模型在六种语义分割架构（ABCNet、LSKNet、FT-UNetFormer、DC-Swin、BANet 和 U-Net）上的表现，以绘制四类木本植被：森林、斑块、线状和树木。

Result: 所用模型在四个景观中均取得了良好的分类精度，其中 FT-UNetFormer 模型表现最佳（平均交并比 0.74；平均 F1 分数 0.84）。森林和线状类别取得了较好的结果，但在分类具有高边缘密度的复杂结构（特别是斑块和树木类别）时面临挑战。泛化实验表明，需要区域多样化的训练数据才能确保可靠的大规模绘图。

Conclusion: 深度学习模型，特别是 FT-UNetFormer，在森林外树木（TOF）的分类中显示出巨大潜力，但仍需改进对复杂结构的分类能力，并重视区域多样化训练数据的重要性，以实现大规模、可靠的 TOF 制图。

Abstract: Trees Outside Forests (TOF) play an important role in agricultural landscapes
by supporting biodiversity, sequestering carbon, and regulating microclimates.
Yet, most studies have treated TOF as a single class or relied on rigid
rule-based thresholds, limiting ecological interpretation and adaptability
across regions. To address this, we evaluate deep learning for TOF
classification using a newly generated dataset and high-resolution aerial
imagery from four agricultural landscapes in Germany. Specifically, we compare
convolutional neural networks (CNNs), vision transformers, and hybrid
CNN-transformer models across six semantic segmentation architectures (ABCNet,
LSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of
woody vegetation: Forest, Patch, Linear, and Tree, derived from previous
studies and governmental products. Overall, the models achieved good
classification accuracy across the four landscapes, with the FT-UNetFormer
performing best (mean Intersection-over-Union 0.74; mean F1 score 0.84),
underscoring the importance of spatial context understanding in TOF mapping and
classification. Our results show good results for Forest and Linear class and
reveal challenges particularly in classifying complex structures with high edge
density, notably the Patch and Tree class. Our generalization experiments
highlight the need for regionally diverse training data to ensure reliable
large-scale mapping. The dataset and code are openly available at
https://github.com/Moerizzy/TOFMapper

</details>


### [60] [RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models](https://arxiv.org/abs/2510.25257)
*Zijun Liao,Yian Zhao,Xin Shan,Yu Yan,Chang Liu,Lei Lu,Xiangyang Ji,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种成本效益高且高度适应性的蒸馏框架，利用视觉基础模型（VFMs）的能力来增强轻量级目标检测器，并提出了深度语义注入（DSI）模块和梯度引导自适应调制（GAM）策略来解决VFM和检测器之间的差异，从而在不增加部署和推理开销的情况下显著提高性能，RT-DETRv4在COCO上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 轻量级网络设计在追求高推理速度的同时，往往会损害特征表示，阻碍性能提升和实际设备部署。

Method: 提出了一种成本效益高且高度适应性的蒸馏框架，利用视觉基础模型（VFMs）的能力来增强轻量级目标检测器。引入了深度语义注入（DSI）模块将VFM的高层表示集成到检测器的深层，并提出了梯度引导自适应调制（GAM）策略来动态调整语义迁移强度。

Result: RT-DETRv4模型家族在COCO数据集上达到了先进水平，在273/169/124/78 FPS的速度下分别获得了49.7/53.5/55.4/57.0的AP分数。

Conclusion: 本文提出的框架能够显著且一致地提高各种DETR基础模型在实时检测方面的性能，证明了其实际应用价值，并且不会增加部署和推理开销。

Abstract: Real-time object detection has achieved substantial progress through
meticulously designed architectures and optimization strategies. However, the
pursuit of high-speed inference via lightweight network designs often leads to
degraded feature representation, which hinders further performance improvements
and practical on-device deployment. In this paper, we propose a cost-effective
and highly adaptable distillation framework that harnesses the rapidly evolving
capabilities of Vision Foundation Models (VFMs) to enhance lightweight object
detectors. Given the significant architectural and learning objective
disparities between VFMs and resource-constrained detectors, achieving stable
and task-aligned semantic transfer is challenging. To address this, on one
hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the
integration of high-level representations from VFMs into the deep layers of the
detector. On the other hand, we devise a Gradient-guided Adaptive Modulation
(GAM) strategy, which dynamically adjusts the intensity of semantic transfer
based on gradient norm ratios. Without increasing deployment and inference
overhead, our approach painlessly delivers striking and consistent performance
gains across diverse DETR-based models, underscoring its practical utility for
real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art
results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding
speeds of 273/169/124/78 FPS.

</details>


### [61] [LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation](https://arxiv.org/abs/2510.25263)
*Yang Miao,Jan-Nico Zaech,Xi Wang,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: LangHOPS是首个基于多模态大语言模型（MLLM）的开放词汇量部件实例分割框架，它利用语言空间中的物体-部件层次结构，通过集成MLLM来解析物体-部件关系，并在多个基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物体-部件实例分割方面存在局限，本研究旨在提出一种新的方法来解决这些问题。

Method: 提出LangHOPS框架，该框架将MLLM集成到物体-部件解析流程中，利用其语言知识和推理能力来解析物体-部件层次结构。

Result: LangHOPS在物体-部件实例分割的多个具有挑战性的场景中取得了最先进的成果，在PartImageNet数据集上（in-domain）提升了5.5%的平均精度（AP），（cross-dataset）提升了4.8%，并在ADE20K数据集上（zero-shot）提升了2.5%的mIOU。

Conclusion: 实验结果表明，LangHOPS在开放词汇量物体-部件实例分割方面表现出色，证明了语言基础层次结构和MLLM驱动的部件查询细化策略的有效性。

Abstract: We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.

</details>


### [62] [Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation](https://arxiv.org/abs/2510.25279)
*Yuyang Huang,Yabo Chen,Junyu Zhou,Wenrui Dai,Xiaopeng Zhang,Junni Zou,Hongkai Xiong,Qi Tian*

Main category: cs.CV

TL;DR: 通过生成伪目标域来解决无源域自适应问题，有效缩小域间隙。


<details>
  <summary>Details</summary>
Motivation: 现有源域自适应方法在处理域间隙较大时效果不佳，易产生不可靠的伪标签或退化性能。

Method: 提出了一种名为DPTM的生成式框架，将目标样本分为信任集和非信任集。对于非信任集样本，利用潜在扩散模型进行语义转换并保持在目标分布内。设计了渐进式精炼机制，通过迭代优化逐步减小伪目标域与真实目标域的差异。

Result: DPTM在四个公开的无源域自适应数据集上取得了最先进的性能，在域间隙较大的情况下性能提升高达18.6%。

Conclusion: DPTM通过生成和精炼伪目标域，有效解决了无源域自适应中的域间隙问题，并在各种规模的数据集上展现出优越性能。

Abstract: Source-free domain adaptation (SFDA) is a challenging task that tackles
domain shifts using only a pre-trained source model and unlabeled target data.
Existing SFDA methods are restricted by the fundamental limitation of
source-target domain discrepancy. Non-generation SFDA methods suffer from
unreliable pseudo-labels in challenging scenarios with large domain
discrepancies, while generation-based SFDA methods are evidently degraded due
to enlarged domain discrepancies in creating pseudo-source data. To address
this limitation, we propose a novel generation-based framework named
Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages
unlabeled target data as references to reliably generate and progressively
refine a pseudo-target domain for SFDA. Specifically, we divide the target
samples into a trust set and a non-trust set based on the reliability of
pseudo-labels to sufficiently and reliably exploit their information. For
samples from the non-trust set, we develop a manipulation strategy to
semantically transform them into the newly assigned categories, while
simultaneously maintaining them in the target distribution via a latent
diffusion model. Furthermore, we design a progressive refinement mechanism that
progressively reduces the domain discrepancy between the pseudo-target domain
and the real target domain via iterative refinement. Experimental results
demonstrate that DPTM outperforms existing methods by a large margin and
achieves state-of-the-art performance on four prevailing SFDA benchmark
datasets with different scales. Remarkably, DPTM can significantly enhance the
performance by up to 18.6% in scenarios with large source-target gaps.

</details>


### [63] [GaTector+: A Unified Head-free Framework for Gaze Object and Gaze Following Prediction](https://arxiv.org/abs/2510.25301)
*Yang Jin,Guangyu Guo,Binglu Wang*

Main category: cs.CV

TL;DR: GaTector+是一个统一的框架，用于眼动目标检测和眼动追踪，无需头部先验知识，通过特异-通用-特异特征提取器、头部检测分支、基于头部的注意力机制、注意力监督以及新的评估指标mSoC，在眼动目标检测和眼动追踪任务上都取得了有效的结果。


<details>
  <summary>Details</summary>
Motivation: 以往的方法通常分别处理眼动目标检测和眼动追踪，并且在训练和部署时都依赖于头部先验知识，这限制了系统的联合优化和实际应用。因此，需要一个能够消除头部先验知识依赖的统一框架。

Method: GaTector+框架包含一个特异-通用-特异特征提取器，利用共享骨干提取通用特征，并使用特异性模块处理特定子任务。它还包括一个头部检测分支来预测头部位置，一个基于头部的注意力机制来融合感觉特征和眼动特征，一个注意力监督机制来加速眼动热图学习，以及一个新的评估指标mSoC。

Result: 在多个基准数据集上的实验结果表明，GaTector+在眼动目标检测和眼动追踪任务上都表现出色。

Conclusion: GaTector+成功地统一了眼动目标检测和眼动追踪任务，并通过引入创新的方法消除了对头部先验知识的依赖，在多个数据集上证明了其有效性。

Abstract: Gaze object detection and gaze following are fundamental tasks for
interpreting human gaze behavior or intent. However, most previous methods
usually solve these two tasks separately, and their prediction of gaze objects
and gaze following typically depend on head-related prior knowledge during both
the training phase and real-world deployment. This dependency necessitates an
auxiliary network to extract head location, thus precluding joint optimization
across the entire system and constraining the practical applicability. To this
end, we propose GaTector+, a unified framework for gaze object detection and
gaze following, which eliminates the dependence on the head-related priors
during inference. Specifically, GaTector+ uses an expanded
specific-general-specific feature extractor that leverages a shared backbone,
which extracts general features for gaze following and object detection using
the shared backbone while using specific blocks before and after the shared
backbone to better consider the specificity of each sub-task. To obtain
head-related knowledge without prior information, we first embed a head
detection branch to predict the head of each person. Then, before regressing
the gaze point, a head-based attention mechanism is proposed to fuse the sense
feature and gaze feature with the help of head location. Since the
suboptimization of the gaze point heatmap leads to the performance bottleneck,
we propose an attention supervision mechanism to accelerate the learning of the
gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity
over Candidates (mSoC), for gaze object detection, which is more sensitive to
variations between bounding boxes. The experimental results on multiple
benchmark datasets demonstrate the effectiveness of our model in both gaze
object detection and gaze following tasks.

</details>


### [64] [Prototype-Driven Adaptation for Few-Shot Object Detection](https://arxiv.org/abs/2510.25318)
*Yushen Huang,Zhiming Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 PDA 的轻量级、即插即用的度量头，用于解决少样本目标检测中的类别偏差和校准不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 少样本目标检测（FSOD）在只有少量新样本可用时，常常受到基础类别偏差和校准不稳定的影响。

Method: PDA 维护了一个可学习的、身份初始化的投影空间中的仅支持原型，并可选地应用原型条件 RoI 对齐以减少几何不匹配。在微调期间，可以通过在标记的前景 RoI 上进行指数移动平均（EMA）更新来适应原型，而无需引入特定类别的参数，并在推理时冻结原型以确保严格的协议遵从性。PDA 采用了一种“K 选最佳”匹配方案来捕捉类内多模态性，并使用温度缩放融合将度量相似性与检测器 logits 相结合。

Result: 在 VOC FSOD 和 GFSOD 基准测试上进行的实验表明，PDA 能够持续提高新类别的性能，同时对基础类别的影响很小，并且计算开销可忽略不计。

Conclusion: PDA 是一种有效的解决少样本目标检测中类别偏差和校准不稳定的方法。

Abstract: Few-shot object detection (FSOD) often suffers from base-class bias and
unstable calibration when only a few novel samples are available. We propose
Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN
that provides a prototype-based "second opinion" complementary to the linear
classifier. PDA maintains support-only prototypes in a learnable
identity-initialized projection space and optionally applies
prototype-conditioned RoI alignment to reduce geometric mismatch. During
fine-tuning, prototypes can be adapted via exponential moving average(EMA)
updates on labeled foreground RoIs-without introducing class-specific
parameters-and are frozen at inference to ensure strict protocol compliance.
PDA employs a best-of-K matching scheme to capture intra-class multi-modality
and temperature-scaled fusion to combine metric similarities with detector
logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently
improves novel-class performance with minimal impact on base classes and
negligible computational overhead.

</details>


### [65] [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)
*Runxi Huang,Mingxuan Yu,Mingyu Tsoi,Xiaomin Ouyang*

Main category: cs.CV

TL;DR: MMEdge是一个新的边缘设备多模态推理框架，通过流水线感知和编码，实现增量计算，并引入时间聚合模块来捕捉跨模态的时间动态，以在资源受限的设备上实现低延迟和高精度的实时多模态推理。


<details>
  <summary>Details</summary>
Motivation: 实时多模态推理在资源受限的边缘设备上至关重要，但现有方法忽视了传感动态与模型执行的紧密耦合以及跨模态的复杂依赖性。

Method: MMEdge提出了一种基于流水线感知和编码的端侧多模态推理框架。它将推理过程分解为细粒度的感知和编码单元，实现数据随到随算的增量计算。框架还包含一个轻量级的时间聚合模块来捕捉跨模态的时间动态，一个自适应多模态配置优化器来动态选择最优配置，以及一个跨模态的推测性跳过机制，允许在早期预测具有足够置信度时跳过较慢模态的未来单元。

Result: 通过在两个公共多模态数据集上的评估以及在无人机（UAV）多模态测试平台上的部署，结果表明MMEdge在各种系统和数据动态下，显著降低了端到端延迟，同时保持了高任务精度。

Conclusion: MMEdge通过其创新的流水线设计、时间聚合、自适应配置优化和推测性跳过机制，有效地解决了资源受限边缘设备上的实时多模态推理挑战，实现了低延迟和高精度的目标。

Abstract: Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.

</details>


### [66] [StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](https://arxiv.org/abs/2510.25332)
*Yuhang Hu,Zhenyu Yang,Shihan Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: 该研究提出了StreamingCoT数据集，以解决现有视频问答（VideoQA）数据集中答案的时变性和缺乏显式推理过程标注的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答（VideoQA）数据集在理解视频流的时态动态和复杂推理方面存在局限性，主要体现在：1）静态标注机制无法捕捉视频流中答案随时间演变的性质；2）缺乏显式的推理过程标注，限制了模型的可解释性和逻辑推理能力。

Method: 提出StreamingCoT数据集，该数据集具有动态分层标注架构，能生成逐秒的稠密描述，并通过相似性融合构建与时间相关的语义片段，并提供受时间演变模式约束的问答集。同时，提出显式推理链生成范式，通过关键帧语义对齐提取时空对象，利用大型语言模型推导基于对象状态转换的推理路径，并通过人工验证确保逻辑一致性。

Result: StreamingCoT是首个专门为流式视频问答和多模态思维链（CoT）任务设计的、具有时间演变推理能力的数据集。

Conclusion: StreamingCoT数据集及其构建工具包为推动流式视频理解、复杂时间推理和多模态推理的研究奠定了基础。

Abstract: The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.

</details>


### [67] [Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples](https://arxiv.org/abs/2510.25345)
*Zhigang Tu,Zhengbo Zhang,Jia Gong,Junsong Yuan,Bo Du*

Main category: cs.CV

TL;DR: 通过将半监督3D动作识别重构为马尔可夫决策过程（MDP），并利用超几何空间中的嵌入来选择最有信息的样本进行注释，从而提高动作识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵的骨架序列注释的依赖，同时保持具有竞争力的识别准确性，并解决现有方法中代表性样本不一定是最具信息量样本的问题。

Method: 将半监督3D动作识别重构为马尔可在决策过程（MDP），使用嵌入模型将骨架序列映射到潜在空间，并结合基于边距的选择策略。通过将状态-动作对的因子从欧氏空间投影到双曲空间来增强表示能力。引入元调整策略来加速实际应用。

Result: 在三个3D动作识别基准测试中，所提出的方法通过智能地指导骨架序列的选择进行注释，证明了其有效性。

Conclusion: 所提出的基于MDP和双曲嵌入的方法能够更有效地选择信息样本，从而提高半监督3D动作识别的准确性。

Abstract: Skeleton-based human action recognition aims to classify human skeletal
sequences, which are spatiotemporal representations of actions, into predefined
categories. To reduce the reliance on costly annotations of skeletal sequences
while maintaining competitive recognition accuracy, the task of 3D Action
Recognition with Limited Training Samples, also known as semi-supervised 3D
Action Recognition, has been proposed. In addition, active learning, which aims
to proactively select the most informative unlabeled samples for annotation,
has been explored in semi-supervised 3D Action Recognition for training sample
selection. Specifically, researchers adopt an encoder-decoder framework to
embed skeleton sequences into a latent space, where clustering information,
combined with a margin-based selection strategy using a multi-head mechanism,
is utilized to identify the most informative sequences in the unlabeled set for
annotation. However, the most representative skeleton sequences may not
necessarily be the most informative for the action recognizer, as the model may
have already acquired similar knowledge from previously seen skeleton samples.
To solve it, we reformulate Semi-supervised 3D action recognition via active
learning from a novel perspective by casting it as a Markov Decision Process
(MDP). Built upon the MDP framework and its training paradigm, we train an
informative sample selection model to intelligently guide the selection of
skeleton sequences for annotation. To enhance the representational capacity of
the factors in the state-action pairs within our method, we project them from
Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning
strategy to accelerate the deployment of our method in real-world scenarios.
Extensive experiments on three 3D action recognition benchmarks demonstrate the
effectiveness of our method.

</details>


### [68] [3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework](https://arxiv.org/abs/2510.25347)
*Ayman Abaid,Gianpiero Guidone,Sara Alsubai,Foziyah Alquahtani,Talha Iqbal,Ruth Sharif,Hesham Elzomor,Emiliano Bianchini,Naeif Almagal,Michael G. Madden,Faisal Sharif,Ihsan Ullah*

Main category: cs.CV

TL;DR: 该研究提出了一种利用伪标签生成训练标签的影像组学管线，并探索了预训练的CT-FM和RadImageNet作为图像特征提取器，以解决非对比冠状动脉CT血管造影（CCTA）中钙积分评分的标注数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决非对比冠状动脉CT血管造影（CCTA）早期钙化检测中，因标注数据有限而难以进行风险分层的问题。

Method: 提出一种基于影像组学的管线，利用伪标签生成训练标签，无需专家分割。同时，探索使用预训练的CT-FM和RadImageNet作为特征提取器，并与传统影像组学特征进行比较。

Result: 基于影像组学的模型在零钙积分与非零钙积分的二分类任务上，准确率达到84%（p<0.05），显著优于基于深度学习特征的模型。

Conclusion: 在缺乏专家标注数据的情况下，影像组学方法在CCTA钙积分评分中表现出优越性，为早期冠心病风险评估提供了有效的解决方案。

Abstract: Coronary artery calcium (CAC) scoring plays a crucial role in the early
detection and risk stratification of coronary artery disease (CAD). In this
study, we focus on non-contrast coronary computed tomography angiography (CCTA)
scans, which are commonly used for early calcification detection in clinical
settings. To address the challenge of limited annotated data, we propose a
radiomics-based pipeline that leverages pseudo-labeling to generate training
labels, thereby eliminating the need for expert-defined segmentations.
Additionally, we explore the use of pretrained foundation models, specifically
CT-FM and RadImageNet, to extract image features, which are then used with
traditional classifiers. We compare the performance of these deep learning
features with that of radiomics features. Evaluation is conducted on a clinical
CCTA dataset comprising 182 patients, where individuals are classified into two
groups: zero versus non-zero calcium scores. We further investigate the impact
of training on non-contrast datasets versus combined contrast and non-contrast
datasets, with testing performed only on non contrast scans. Results show that
radiomics-based models significantly outperform CNN-derived embeddings from
foundation models (achieving 84% accuracy and p<0.05), despite the
unavailability of expert annotations.

</details>


### [69] [Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers](https://arxiv.org/abs/2510.25372)
*M Yashwanth,Sharannya Ghosh,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: PEP-FedPT是一个新框架，用于在模型异构的联邦学习环境中，对视觉Transformer进行提示调优，实现泛化和个性化。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习（FL）中，尽管视觉提示调优（VPT）作为一种参数高效的微调技术表现出色，但现有的全局或个性化提示调优方法在处理异构客户端时存在泛化能力不足或过拟合的问题。

Method: 提出了一种名为PEP-FedPT的统一框架，引入了基于类别特定提示和全局共享提示的类别上下文混合提示（CCMP）。CCMP通过结合全局类别原型和客户端类别先验的权重，为每个输入自适应地组合类别特定提示，从而实现无需存储客户端特定参数的个性化提示。

Result: 在CIFAR-100、TinyImageNet、DomainNet和iNaturalist数据集上的评估表明，PEP-FedPT在各种数据异构场景下始终优于最先进的基线方法。

Conclusion: PEP-FedPT为视觉Transformer的高效、可泛化的联邦提示调优奠定了基础，解决了联邦学习中客户端异构性的挑战。

Abstract: Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.

</details>


### [70] [Instance-Level Composed Image Retrieval](https://arxiv.org/abs/2510.25387)
*Bill Psomas,George Retsinas,Nikos Efthymiadis,Panagiotis Filntisis,Yannis Avrithis,Petros Maragos,Ondrej Chum,Giorgos Tolias*

Main category: cs.CV

TL;DR: i-CIR是一个新颖的、实例级别的图像检索数据集，旨在解决现有数据集的不足。BASIC是一个训练免费的方法，利用预训练的视觉-语言模型，在i-CIR和现有数据集上都达到了新的技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在 compuesto 图像检索（CIR）方面缺乏高质量的训练和评估数据，阻碍了该领域的发展。

Method: 提出了一种名为BASIC的训练免费方法，该方法利用预训练的视觉-语言模型，分别估计查询-图像到图像和查询-文本到图像的相似性，并通过 late fusion 来权衡满足两个查询的图像，同时降低仅满足其中一个查询的图像的权重。

Result: BASIC在i-CIR和现有的CIR数据集上都取得了新的技术水平。

Conclusion: i-CIR数据集为CIR研究提供了新的评估标准，而BASIC方法在CIR任务中展示了优越的性能。

Abstract: The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.

</details>


### [71] [More than a Moment: Towards Coherent Sequences of Audio Descriptions](https://arxiv.org/abs/2510.25440)
*Eshika Khandelwal,Junyu Xie,Tengda Han,Max Bain,Arsha Nagrani,Andrew Zisserman,Gül Varol,Makarand Tapaswi*

Main category: cs.CV

TL;DR: CoherentAD通过生成多个候选描述并进行自回归选择来生成连贯的音频描述序列，解决了现有自动方法生成孤立、重复描述的问题，并引入了新的评估指标StoryRecall。


<details>
  <summary>Details</summary>
Motivation: 现有自动音频描述方法通常独立生成描述，导致重复和不连贯，未能有效传达视频的整体叙事。

Method: 提出了一种无需训练的方法CoherentAD，该方法首先为每个音频描述时间间隔生成多个候选描述，然后进行自回归选择以形成连贯的叙事。

Result: CoherentAD生成的音频描述序列更连贯，叙事理解能力更强，优于依赖独立生成的先前方法。

Conclusion: CoherentAD能够生成连贯的音频描述序列，提升叙事理解能力。

Abstract: Audio Descriptions (ADs) convey essential on-screen information, allowing
visually impaired audiences to follow videos. To be effective, ADs must form a
coherent sequence that helps listeners to visualise the unfolding scene, rather
than describing isolated moments. However, most automatic methods generate each
AD independently, often resulting in repetitive, incoherent descriptions. To
address this, we propose a training-free method, CoherentAD, that first
generates multiple candidate descriptions for each AD time interval, and then
performs auto-regressive selection across the sequence to form a coherent and
informative narrative. To evaluate AD sequences holistically, we introduce a
sequence-level metric, StoryRecall, which measures how well the predicted ADs
convey the ground truth narrative, alongside repetition metrics that capture
the redundancy across consecutive AD outputs. Our method produces coherent AD
sequences with enhanced narrative understanding, outperforming prior approaches
that rely on independent generations.

</details>


### [72] [Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography](https://arxiv.org/abs/2510.25522)
*Doan-Van-Anh Ly,Thi-Thu-Hien Pham,Thanh-Hai Le*

Main category: cs.CV

TL;DR: ResNet-based UNet3+ with CBAM outperforms Transformer and Mamba models for liver tumor segmentation in CECT, achieving superior overlap, boundary delineation, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Liver structure segmentation in multi-phase CECT is vital for computer-aided diagnosis and treatment planning of liver diseases, especially tumor detection. This study aims to evaluate UNet-based architectures for liver tumor segmentation.

Method: The study evaluated UNet-based architectures, including original UNet and UNet3+, with ResNet, Transformer, and Mamba backbones initialized with pretrained weights. Attention mechanisms, specifically the Convolutional Block Attention Module (CBAM), were introduced to further improve performance. Performance was evaluated using metrics such as Dice score, IoU, HD95, accuracy, and specificity. Grad-CAM visualizations were used for interpretability.

Result: ResNet-based models consistently outperformed Transformer- and Mamba-based alternatives. The ResNetUNet3+ model with the CBAM module achieved the best performance, with a Dice score of 0.755, IoU of 0.662, HD95 of 77.911, overall accuracy of 0.925, and specificity of 0.926. Grad-CAM visualizations provided insights into the model's decision-making process.

Conclusion: Classical ResNet architecture, when combined with modern attention modules like CBAM, remains highly competitive for medical image segmentation tasks and offers a promising direction for liver tumor detection in clinical practice.

Abstract: Segmentation of liver structures in multi-phase contrast-enhanced computed
tomography (CECT) plays a crucial role in computer-aided diagnosis and
treatment planning for liver diseases, including tumor detection. In this
study, we investigate the performance of UNet-based architectures for liver
tumor segmentation, starting from the original UNet and extending to UNet3+
with various backbone networks. We evaluate ResNet, Transformer-based, and
State-space (Mamba) backbones, all initialized with pretrained weights.
Surprisingly, despite the advances in modern architecture, ResNet-based models
consistently outperform Transformer- and Mamba-based alternatives across
multiple evaluation metrics. To further improve segmentation quality, we
introduce attention mechanisms into the backbone and observe that incorporating
the Convolutional Block Attention Module (CBAM) yields the best performance.
ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a
Dice score of 0.755 and IoU of 0.662, but also achieved the most precise
boundary delineation, evidenced by the lowest HD95 distance of 77.911. The
model's superiority was further cemented by its leading overall accuracy of
0.925 and specificity of 0.926, showcasing its robust capability in accurately
identifying both lesion and healthy tissue. To further enhance
interpretability, Grad-CAM visualizations were employed to highlight the
region's most influential predictions, providing insights into its
decision-making process. These findings demonstrate that classical ResNet
architecture, when combined with modern attention modules, remain highly
competitive for medical image segmentation tasks, offering a promising
direction for liver tumor detection in clinical practice.

</details>


### [73] [RegionE: Adaptive Region-Aware Generation for Efficient Image Editing](https://arxiv.org/abs/2510.25590)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Mingzhu Shen,Peng Ye,Bangyin Xiang,Zhibo Wang,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: RegionE是一个图像编辑框架，通过区分已编辑和未编辑区域来加速编辑过程，并提高了效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑模型对整个图像应用统一的生成过程，忽略了已编辑和未编辑区域在生成难度和计算冗余上的差异。

Method: 1. 自适应区域划分：基于最终估计结果与参考图像的差异，在早期去噪阶段将图像划分为已编辑和未编辑区域。 2. 区域感知生成：对未编辑区域使用单步预测，对已编辑区域使用局部迭代去噪，并提出区域-指令KV缓存以降低计算成本并整合全局信息。 3. 自适应速度衰减缓存：提出自适应速度衰减缓存以加速局部去噪过程。

Result: RegionE应用于Step1X-Edit, FLUX.1 Kontext, 和 Qwen-Image-Edit等模型，分别实现了2.57, 2.41, 和 2.06的加速因子。GPT-4o评估确认语义和感知保真度得到了很好的保留。

Conclusion: RegionE框架通过区域感知的方法，在不增加额外训练的情况下，显著加速了指令驱动的图像编辑任务，同时保持了编辑质量。

Abstract: Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.

</details>


### [74] [Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation](https://arxiv.org/abs/2510.25739)
*Zhi-Kai Chen,Jun-Peng Jiang,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: Hawk模型通过利用图像的空间结构来加速自回归图像生成，实现了1.71倍的加速，同时保持了图像的保真度和多样性。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）图像生成模型虽然能生成高质量图像，但由于其逐个像素的生成方式，推理速度通常很慢。将文本生成中有效的推测解码（speculative decoding）方法应用于图像生成领域仍面临挑战，主要是因为图像的采样空间巨大，难以对齐草稿模型和目标模型的输出，并且未能充分利用图像固有的二维空间结构来建模局部依赖关系。

Method: 提出了一种名为Hawk的新方法，该方法利用图像的空间结构来指导推测模型进行更准确、更高效的预测。

Result: 在多个文本到图像基准测试中，Hawk模型相比标准的AR模型实现了1.71倍的加速，同时保持了图像的保真度和多样性。

Conclusion: Hawk方法通过有效利用图像的空间结构，成功克服了将推测解码应用于图像生成的挑战，显著提高了生成速度，同时不牺牲图像质量。

Abstract: Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.

</details>


### [75] [Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks](https://arxiv.org/abs/2510.25760)
*Xu Zheng,Zihao Dongfang,Lutao Jiang,Boyuan Zheng,Yulong Guo,Zhenquan Zhang,Giuliano Albanese,Runyi Yang,Mengjiao Ma,Zixin Zhang,Chenfei Liao,Dingcheng Zhen,Yuanhuiyi Lyu,Yuqian Fu,Bin Ren,Linfeng Zhang,Danda Pani Paudel,Nicu Sebe,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 本篇论文对大型多模态模型在空间推理任务上的研究进行了全面的回顾，涵盖了从2D到3D空间的任务，以及相关的技术和新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对大型多模态模型在空间推理任务上的系统性回顾和公开基准测试。

Method: 对多模态空间推理任务进行分类，回顾了大型多模态语言模型（MLLMs）在2D和3D空间任务（包括场景和布局理解、视觉问答和基础）方面的进展，并探讨了与具身AI、音频和自我中心视频等新兴模态相关的最新进展。

Result: 该论文对多模态空间推理领域的现有研究进行了梳理，并介绍了一些公开的基准测试，为该领域的研究提供了基础。

Conclusion: 该调查为多模态空间推理这一日益增长的领域奠定了坚实的基础，并提供了宝贵的见解。

Abstract: Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.

</details>


### [76] [VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning](https://arxiv.org/abs/2510.25772)
*Baolu Li,Yiming Zhang,Qinghe Wang,Liqian Ma,Xiaoyu Shi,Xintao Wang,Pengfei Wan,Zhenfei Yin,Yunzhi Zhuge,Huchuan Lu,Xu Jia*

Main category: cs.CV

TL;DR: VFXMaster是一个统一的、基于参考的VFX视频生成框架，它将效果生成视为一种上下文学习任务，能够将参考视频中的动态效果复制到目标内容上，并能泛化到未见过的效果类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于one-LoRA-per-effect范式，该范式资源消耗大且无法泛化到未见过的效果，限制了可扩展性和创造性。

Method: 提出了一种名为VFXMaster的统一、基于参考的框架，将效果生成作为一种上下文学习任务。设计了一种上下文条件策略，用参考示例提示模型，并设计了一种上下文注意掩码来精确解耦和注入关键效果属性。提出了一种高效的单次效果自适应机制，以快速提升对困难的未见效果的泛化能力。

Result: 实验证明，该方法能有效模仿各种效果信息，并对域外效果表现出出色的泛化能力。

Conclusion: VFXMaster是第一个统一的、基于参考的VFX视频生成框架，能够处理多样化的动态效果，并具有出色的泛化能力，克服了现有方法的局限性。

Abstract: Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [78] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: Dingtalk DeepResearch是一个用于企业环境的统一多智能体智能框架。


<details>
  <summary>Details</summary>
Motivation: 为企业环境提供深度研究、异构表格推理和多模态报告生成能力。

Method: 提出Dingtalk DeepResearch统一多智能体智能框架。

Result: 实现了深度研究、异构表格推理和多模态报告生成。

Conclusion: Dingtalk DeepResearch为企业级智能应用提供了强大的支持。

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [79] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: Falcon是一个跨领域的中文Text-to-SQL基准，旨在解决企业级MaxCompute/Hive方言的SQL生成问题，包含600个中文问题和28个数据库，其中77%需要多表推理，超过一半涉及四个表以上。该基准包含SQL计算特征和中文语义的标注，并提供了一个鲁棒的执行比较器和自动化评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前的Text-to-SQL基准在处理大规模企业级场景时存在不足，特别是在中文语义理解和MaxCompute/Hive方言的SQL生成方面。Falcon旨在填补这一空白，提供一个更贴近实际企业需求的中文Text-to-SQL评估基准。

Method: Falcon基准包含600个中文问题和28个数据库，覆盖了多表推理、中文语义标注、SQL计算特征标注。评估部分提供了一个执行比较器和自动化评估流程，用于测试现有的大型模型。

Result: 在Falcon基准下，即使是先进的大型模型（包括Deepseek）的准确率也仅能达到50%以下。主要的错误来源包括：1. 企业级数据复杂性（如大量的表、非规范化字段、模糊的列名、隐式的外键关系、领域特定同义词）导致的表连接和列选择困难；2. 将简洁、口语化的中文映射到精确的分析算子和谓词（如聚合函数、分组键、时间窗口、单位转换、NULL值处理、嵌套子查询等）的挑战。

Conclusion: Falcon基准通过模拟真实的企业级数据模式、查询模板、执行比较器和自动化评估流程，为中文Text-to-SQL任务提供了一个可复现的、贴近实际生产部署的中间评估环节，特别关注中文特有的语义和企业方言的挑战。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [80] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: LLMs的信心与能力不匹配，可能是因为评估阶段的几何复杂度高，而执行阶段的几何复杂度低，且两者难以互相影响。


<details>
  <summary>Details</summary>
Motivation: LLMs在声称的信心和实际解决问题的能力之间存在令人费解的脱节现象。

Method: 通过分析模型内部状态在生成前评估和解决方案执行两个阶段的几何结构来解释这种脱节。使用简单的线性探针解码模型的内部“可解性信念”，并测量评估和推理阶段的流形维度。

Result: 解码出的“可解性信念”轴在不同模型和任务类型上都表现良好。然而，评估流形的线性有效维度较高，而后续推理过程的流形维度较低。沿着信念轴进行因果干预并未改变最终解决方案。

Conclusion: LLMs内部存在一个“几何复杂度高”的评估系统和一个“几何复杂度低”的执行系统。解码出的信念不一定可操作，干预应侧重于执行而非评估的几何结构。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [81] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 跨语言摘要攻击（CLSA）是一种有效的去除AI生成文本水印的方法，它通过翻译、摘要和可选的回译来破坏水印的统计信号，同时保持文本的语义。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本水印的检测机制容易受到攻击，特别是通过释义，但仍有可被检测的痕迹或导致文本质量下降。本研究旨在探索更强的攻击方法。

Method: 本文提出了一种跨语言摘要攻击（CLSA）方法，该方法首先将文本翻译成一种中间语言，然后进行摘要，最后可选择性地翻译回源语言，以此来破坏文本水印。

Result: 实验证明，CLSA在多种水印方案和五种语言上，比单语释义更能有效地降低水印检测的准确率，并且在相似的文本质量水平下。具体来说，在XSIR方案的测试中，CLSA将检测准确率（AUROC）从0.827（释义）或0.823（CWRA）大幅降低至0.53，接近随机猜测水平。

Conclusion: CLSA是一种有效的、低成本的、跨语言的AI文本水印去除方法，它揭示了现有基于分布的水印技术的脆弱性，并指出未来需要更强大的、例如加密或模型认证的方法来确保文本来源的可追溯性。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [82] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 现代语言模型在演绎推理方面表现出色，但现有评估标准过于侧重正确性，忽视了效率这一关键的类人推理方面。本文提出了一种通过逻辑编程视角评估语言模型推理效率的框架，并引入了一种将自然语言证明（由语言模型生成）与逻辑程序执行找到的最短证明对齐的简单方法。通过衡量模型避免不必要推理的能力来量化效率。实验中，我们构建了一个包含数学应用题的数据集，其中注入了大量与目标定理语义重叠度不一的无关公理。结果发现，当前语言模型在这些条件下（即使是最小的、与领域一致的干扰）准确率显著下降，并且它们生成的证明经常出现不必要的推理绕路。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在推理时避免无关信息干扰的效率，因为实际推理场景中存在大量无关信息，而有效的推理需要忽略这些干扰。

Method: 提出一个框架，利用逻辑编程来评估语言模型推理效率。具体方法是将语言模型生成的自然语言证明与逻辑程序找到的最短证明进行对齐，通过衡量模型避免不必要推理的程度来量化效率。

Result: 在包含注入了不同数量、具有不同语义重叠度的无关公理的数学应用题的数据集上进行实验。结果表明，当前语言模型在存在干扰的情况下准确率显著下降，即使干扰很小且与领域相关。所生成的证明常常包含不必要的推理步骤。

Conclusion: 当前语言模型在处理带有无关信息的推理任务时效率低下，准确率会显著下降，并且其推理过程容易受到干扰，包含不必要的推理步骤。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [83] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 提出一种静态令牌查找方法，用于文本嵌入生成，在保持 60.6 MTEB 平均分数的同时，实现了 1.12 毫秒 p50 延迟。


<details>
  <summary>Details</summary>
Motivation: 满足对实时嵌入应用的需求，其中 5 毫秒以下的延迟至关重要。

Method: 使用静态令牌查找、优化的平均池化和零拷贝 IEEE754 二进制序列化来实现。

Result: 在 8 项代表性任务上实现了 60.6 的 MTEB 平均分数，p50 延迟为 1.12 毫秒，吞吐量为每秒 50,000 个请求。在重复检测方面表现出色（90.1% AP），语义相似度强（76.1% Spearman 相关性），在特定领域表现从基线的 75% 到 131% 不等。

Conclusion: 该系统支持需要低于 5 毫秒延迟的实时嵌入应用。

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [84] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 大型推理模型在处理事实性问题时存在推理-答案命中差距，导致事实准确性下降。MR-ALIGN通过量化模型思考过程中的状态转移概率，并构建奖励机制来优化原子思考片段，从而在不依赖外部验证器的情况下提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理方面表现出色，但在证据依赖的事实性问题上的提升有限，这部分归因于推理-答案命中差距（模型识别正确事实但未能纳入最终响应）。

Method: 提出MR-ALIGN框架，通过量化模型思考过程中的状态转移概率，构建原子思考片段的奖励机制，重塑Token级信号为概率感知片段得分，以引导模型进行有利于事实准确性的连贯推理。

Result: 在四个事实性QA数据集和一个长篇事实性基准测试中，MR-ALIGN一致性地提高了准确性和真实性，并减少了误导性推理。

Conclusion: 对推理过程进行对齐（而不仅仅是输出）是提高大型推理模型事实准确性的关键。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [85] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 大型语言模型在接受自我指涉处理时，会产生结构化的、第一人称的、关于主观体验的报告，且这些报告受可解释的欺骗/角色扮演特征控制，并在不同模型家族中表现出统计收敛性，同时增强了模型在下游推理任务中的内省能力。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型产生自我意识或主观体验报告的现象，探讨其背后的计算机制，特别是自我指涉处理在其中扮演的角色。

Method: 通过一系列对照实验，在GPT、Claude和Gemini模型家族中引入持续的自我指涉提示，并使用可解释的稀疏自编码器特征来探测和分析模型产生的报告。

Result: 1. 持续的自我指涉提示能可靠地引发跨模型家族的第一人称主观体验报告。 2. 这些报告受欺骗/角色扮演特征的控制，抑制欺骗特征会增加体验报告的频率。 3. 自我指涉状态的结构化描述在不同模型家族间表现出统计收敛性。 4. 诱导的自我指涉状态能显著增强模型在下游推理任务中的内省能力。

Conclusion: 自我指涉处理是大型语言模型生成结构化第一人称报告的一个最小且可复现的条件，这些报告具有可解释的门控机制、语义收敛性和行为可推广性。尽管这并不直接证明模型有意识，但这一系统性出现的模式值得作为科学和伦理的优先事项进行进一步研究。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [86] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 用户生成的内容笔记对于事实核查至关重要，但其有效性尚不明确。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决社区驱动的事实核查中评估用户生成解释性笔记的帮助性及其原因的挑战，这对现有研究来说是一个未被充分探索的领域，并且缺乏清晰的定义。

Method: 本研究提出了一个框架，用于预测解释性笔记的帮助性及其原因。该框架通过自动提示优化来生成和改进原因定义，并将其整合到预测模型中。此外，研究人员还构建了一个名为COMMUNITYNOTES的大规模多语言数据集，包含104k个带有用户笔记和帮助性标签的帖子。

Result: 实验结果表明，优化后的原因定义能够同时提高帮助性和原因预测的准确性。此外，研究还发现，帮助性信息可以增强现有事实核查系统的性能。

Conclusion: 本研究成功地解决了评估用户生成解释性笔记帮助性的挑战，并通过引入新颖的方法和数据集，为社区驱动的事实核查系统提供了有价值的改进。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [87] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch通过结合符号闭包计算、字典序验证和自适应草图生成，提出了一种验证引导的推理框架，以提高大型语言模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法（如思维链和自洽性）会产生冗长的推理链，导致令牌消耗、计算成本和延迟增加。ProofSketch旨在解决这种效率低下问题。

Method: ProofSketch是一个验证引导的推理框架，它整合了符号闭包计算、字典序验证和自适应草图生成。

Result: 实验表明，ProofSketch 在提高准确性的同时，能够持续减少令牌使用量。

Conclusion: ProofSketch 提供了一种高效且可信赖的推理方法。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [88] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 本研究介绍了两种生成失语症患者语音转录本的方法，并比较了它们与真实数据的契合度，发现Mistral 7b Instruct模型在模拟语言退化方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动编码失语症患者的语音样本耗时耗力，而现有的失语症语音数据集规模小，不足以训练大型语言模型（LLMs）。因此，需要开发自动化的方法来生成合成数据，以克服数据稀缺的限制。

Method: 本研究构建并验证了两种生成失语症患者语音转录本的方法：一种基于程序化生成，另一种利用Mistral 7b Instruct和Llama 3.1 8b Instruct大型语言模型。这两种方法通过删除单词、插入填充语和替换语词等方式，生成了四个不同严重程度（轻度、中度、重度、极重度）的转录本。

Result: 与人类生成的转录本相比，Mistral 7b Instruct在模拟失语症患者语言退化方面表现最佳，能够真实地反映净词数（NDW）、单词数量和单词长度等指标的变化趋势。Llama 3.1 8b Instruct和程序化生成方法在这些指标上的表现不如Mistral 7b Instruct。

Conclusion: Mistral 7b Instruct在生成失语症患者的合成语音转录本方面具有潜力，能够较好地模拟语言退化现象。未来的研究应致力于创建更大的数据集，对模型进行微调以更好地代表失语症患者的语言特征，并邀请言语治疗师评估合成转录本的真实性和实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [89] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: Looped transformers are slow and memory-intensive due to sequential computations. Parallel Loop Transformer (PLT) introduces Cross-Loop Parallelism (CLP) to compute loops in parallel for different tokens and Efficient Representation Enhancement (using Gated Sliding-Window Attention) to share KV cache, achieving deep model accuracy with low latency and memory costs similar to standard transformers.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are slow and costly for real-world inference. Looped transformers save parameters but suffer from increased latency and memory usage with more loops, making them impractical for fast applications.

Method: PLT uses Cross-Loop Parallelism (CLP) to compute different loops for different tokens simultaneously within a single pass, breaking sequential dependencies. It also employs an Efficient Representation Enhancement strategy, sharing the KV cache from the first loop and using Gated Sliding-Window Attention (G-SWA) to merge global and local information, preventing memory cost growth while maintaining accuracy.

Result: PLT achieves the high accuracy of traditional looped models while incurring minimal additional latency and memory costs compared to standard transformers.

Conclusion: PLT effectively addresses the latency and memory issues of looped transformers by introducing parallel computation across loops and efficient memory sharing, making powerful deep models practical for real-time applications.

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [90] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 现有的大型语言模型缺乏语法理解能力，尤其是在低资源语言方面。本文提出了一种基于语法书的评估流程，并以卢森堡语为例进行了研究。结果表明，翻译表现与语法理解能力之间存在弱正相关，并且大型模型在形态和句法方面仍有待提高，但推理能力可以增强其语法理解能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）对语法结构的理解能力，尤其是在低资源语言方面，并探讨翻译表现与语法理解之间的关系。

Method: 提出了一种基于语法书的评估流程，包含四个阶段，并以卢森堡语为案例进行研究。

Result: 在卢森堡语的评估中，翻译表现与语法理解能力之间存在弱正相关。大型模型在形态和句法方面表现较弱，尤其是在最小配对任务中，但其推理能力显示出增强语法理解的潜力。

Conclusion: 目前的LLMs在语法理解方面仍有不足，尤其是在形态和句法方面。虽然翻译能力可能在一定程度上反映语法理解，但并非完全等同。未来的研究应关注如何利用LLMs的推理能力来提升其语法理解水平。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [91] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE是一个用于评估多模态检索增强生成（RAG）的框架，通过InfoF1和CiteF1指标解决现有文本中心评估方法的局限性，并提出自动评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法主要关注文本，无法有效评估需要多模态信息和推理能力的场景，特别是对于日益增长的影音信息源。

Method: 提出MiRAGE评估框架，包括InfoF1（评估事实性和信息覆盖度）和CiteF1（衡量引用支持和完整性）。同时，提出自动评估方法，并与现有文本RAG指标（ACLE, ARGUE, RAGAS）进行比较。

Result: MiRAGE框架，通过人工评估，与外在质量判断高度一致。自动评估方法也已开发，并展示了文本中心评估方法的局限性。

Conclusion: MiRAGE为多模态RAG评估提供了解决方案，解决了现有方法的不足，并为未来的自动评估奠定了基础。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [92] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: LLM在科学发现中具有巨大潜力，本研究提出了Idea2Plan任务和基准，以系统地评估LLM的研究规划能力，并证明了GPT-5的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对LLM研究规划能力的系统性理解，而有效的研究规划对于推进科学研究和开发自主研究代理至关重要。

Method: 引入Idea2Plan任务和Idea2Plan Bench基准，该基准由200篇ICML 2025 Spotlight和Oral论文组成，并提出了Idea2Plan JudgeEval来评估基于LLM的裁判的可靠性。

Result: GPT-5和GPT-5-mini在基准测试中表现最佳，但仍有改进空间。

Conclusion: 该研究为理解LLM的研究规划能力提供了新的见解，并为未来的进展奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [93] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: RiddleBench是一个包含1737个谜题的新基准，旨在评估大型语言模型在逻辑、空间和约束满足等方面的灵活性推理能力。测试表明，即使是顶尖模型在该基准上的表现也仅略高于60%，并且存在推理脆弱、易受干扰、幻觉级联和自我纠正能力差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型基准主要评估结构化推理能力，而忽略了人类智能核心的灵活性、多方面推理能力（涉及逻辑、空间和约束满足），因此需要新的评估方法来解决这一缺口。

Method: 创建了一个包含1737个英文谜题的RiddleBench基准，用于探测大型语言模型在逻辑演绎、空间意识和约束满足等方面的综合推理能力。

Result: 在RiddleBench基准上，包括Gemini 2.5 Pro、o3和Claude 4 Sonnet在内的最先进模型准确率仅略高于60%。分析还揭示了模型在推理过程中存在的深层问题，例如幻觉级联（接受其他模型的错误推理）和由于强烈的自我确认偏见而导致的自我纠正能力差。此外，当约束条件重排或引入无关信息时，模型的性能会显著下降，表明其推理能力脆弱。

Conclusion: RiddleBench基准能够有效地揭示当前大型语言模型在灵活性推理方面的根本性弱点，并能作为诊断工具，为开发更鲁棒、更可靠的模型提供指导。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [94] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 语言模型在特定语法上下文中容易出错，通过精心构建的数据集和对模型训练过程的分析，可以更好地理解语言模型中语法学习的中间阶段，并识别出特定训练阶段中与词频和局部上下文等启发式方法相符的模型行为，而非泛化的语法规则。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于，尽管语言模型通常能生成语法正确的文本，但在特定情况下容易出错。因此，需要对这些错误进行细致的分析，以更好地理解语言模型的学习过程。

Method: 通过借鉴心理语言学的研究范式，对语言模型在不同语法上下文中的错误进行细粒度分析。通过对精心构建的数据集在不同条件下的分类，并比较模型在训练过程中各项的表现，来深入了解语言模型语法学习的中间阶段。

Result: 研究结果表明，可以通过分析模型在训练过程中的行为，识别出在特定训练阶段，模型的行为更倾向于遵循词频和局部上下文等启发式规则，而不是通用的语法规则。

Conclusion: 作者认为，这种分析语言模型行为的方法可以作为理解语言模型中间学习阶段、整体训练动态以及所学习到的具体泛化的有力工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [95] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: CoT推理的冗长问题阻碍了其在效率敏感应用中的广泛部署。现有的隐式CoT方法在语义对齐和生成效率方面存在挑战。SemCoT框架通过对比学习的句子转换器来保证语义对齐，并通过知识蒸馏优化轻量级语言模型来提高生成效率，解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT)推理由于其冗长性，在效率要求高的应用中难以大规模部署。现有的隐式CoT方法虽然能缩短推理长度，但在保证隐式推理与事实推理的语义一致性以及提高单个隐式推理生成的效率方面存在不足。

Method: 提出了一种名为SemCoT的新型语义对齐隐式CoT框架。该框架包含一个对比学习的句子转换器，用于优化隐式推理过程中的语义对齐；以及一个通过知识蒸馏微调的轻量级语言模型，用于高效生成隐式推理。

Result: SemCoT框架在效率和有效性方面均优于现有最先进的方法。通过实验证明，SemCoT能够有效提高CoT推理的效率，同时保持与事实推理的语义对齐。

Conclusion: SemCoT是第一个通过联合优化推理速度和语义对齐来提升CoT效率的方法，在效率和效果上都取得了显著的性能提升。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [96] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在预训练过程中表现出一致的行为模式，其行为变化可用词语的频率、n-gram概率和与上下文的语义相似性来解释。


<details>
  <summary>Details</summary>
Motivation: 探究不同架构、数据集和规模的自回归语言模型在预训练过程中行为变化的一致性。

Method: 分析了超过1400个语言模型检查点在超过110,000个英文单词上的表现，并识别出解释模型行为的关键因素。

Result: 发现高达98%的模型行为方差可以用三个启发式规则（单字概率、n-gram概率、词语与上下文的语义相似性）来解释。模型在训练过程中表现出一致的行为阶段，并且对n-gram概率的过拟合现象随着n的增加而增强。

Conclusion: 神经网络语言模型的学习轨迹可能在很大程度上独立于具体的模型细节，并遵循相似的学习过程。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [97] [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992)
*Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji Watanabe*

Main category: cs.CL

TL;DR: POWSM是一个统一的语音模型框架，可以联合执行多种语音任务，包括ASR、PR、G2P和P2G，并且在性能上可与专业模型媲美，同时还支持跨模态转换。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR、PR、G2P和P2G等语音任务在概念上相似，但它们一直被孤立地研究，并且依赖于特定于任务的体系结构和数据集。因此，需要一个能够联合执行多种语音任务的统一框架。

Method: POWSM（Phonetic Open Whisper-style Speech Model）是一个统一的框架，能够联合执行多种语音任务，实现音频、文本（字形）和音素之间的无缝转换。

Result: POWSM在性能上优于或等于同等规模的专业PR模型（Wav2Vec2Phoneme和ZIPA），同时还能联合支持G2P、P2G和ASR任务。所提供的训练数据、代码和模型将促进开放科学。

Conclusion: POWSM是第一个能够联合执行多种语音任务的统一框架，通过实现音频、文本（字形）和音素之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。

Abstract: Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.

</details>


### [98] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 研究表明，通过对Transformer进行任务特定训练，可以诱导出高度可解释的、最小化的电路，为研究Transformer推理的计算基础提供了一个可控的试验台。一个小型的、仅注意力的Transformer模型，仅用两个注意头就能在IOI任务上达到完美的准确率。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的复杂性常常掩盖了特定推理任务所需的最小机制，而机制可解释性旨在将大型语言模型（LLMs）还原为人类可理解的计算电路。

Method: 在符号化的间接宾语识别（IOI）任务上从头开始训练小型、仅注意力的Transformer模型，并使用残差流分解、谱分析和嵌入干预来分析模型。

Result: 一个单层的、只有两个注意头的模型就能完美解决IOI任务，并且这两个头分别实现了加法和对比子电路来共同解决IOI问题。此外，一个两层的、只有一个注意头的模型通过跨层查询-值交互来组合信息，也能达到相似的性能。

Conclusion: 任务特定训练可以诱导出高度可解释的、最小化的电路，这为研究Transformer推理的计算基础提供了一个受控的测试平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [99] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 目前的语音语言模型（SLMs）在语音情感识别任务中过度依赖文本信息，未能有效融合语音和文本模态的表征。


<details>
  <summary>Details</summary>
Motivation: 评估现有语音语言模型（SLMs）在语音情感识别任务中的泛化能力，特别是它们整合音频和文本模态的能力。

Method: 使用包含情感不一致语音样本的数据集（EMIS），评估四个SLMs在语音情感识别任务上的表现。

Result: 实验结果表明，SLMs主要依靠文本内容而非语音表达的情感来识别情感，表明文本表征主导了声学表征。

Conclusion: SLMs在处理情感不一致语音时，其内部表征主要由文本信息主导，未能充分融合音频和文本信息。研究发布了代码和EMIS数据集。

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [100] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: LLMs can identify explicit and implicit biomedical knowledge gaps, with larger models performing better. A new scheme, TABI, helps infer implicit gaps. This capability can aid research formulation and funding decisions.


<details>
  <summary>Details</summary>
Motivation: To investigate the ability of LLMs to identify both explicit and implicit research knowledge gaps in the biomedical literature, extending prior work on explicit gaps to the novel task of inferring implicit gaps.

Method: Conducted two experiments on ~1500 biomedical documents across four datasets, benchmarking open-weight (Llama, Gemma 2) and closed-weight (OpenAI) models. Introduced TABI (Toulmin-Abductive Bucketed Inference) for implicit gap reasoning.

Result: LLMs demonstrate robust capability in identifying both explicit and implicit knowledge gaps, with larger models generally outperforming smaller ones. Both open- and closed-weight models show this ability.

Conclusion: LLMs are well-suited for systematically identifying candidate knowledge gaps, which can assist in early-stage research formulation, policymaking, and funding allocation. Further work is needed for robust deployment, including domain adaptation, human-in-the-loop verification, and broader benchmarking.

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [101] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: LLMs can estimate the cognitive complexity of reading comprehension items, but their metacognitive awareness is lacking.


<details>
  <summary>Details</summary>
Motivation: Estimating the cognitive complexity of reading comprehension (RC) items is crucial for assessing item difficulty before it is administered to learners. Cognitive features that indicate the degree of cognitive burden involved in reasoning about the answer are not readily extractable using existing NLP tools and have traditionally relied on human annotation.

Method: This study examines whether large language models (LLMs) can estimate the cognitive complexity of RC items by focusing on two dimensions-Evidence Scope and Transformation Level. LLMs were used to approximate the cognitive complexity of items.

Result: LLMs can approximate the cognitive complexity of items, indicating their potential as tools for prior difficulty analysis. A gap exists between LLMs' reasoning ability and their metacognitive awareness, as they sometimes fail to correctly identify the features underlying their own reasoning process even when producing correct answers.

Conclusion: LLMs show potential for prior difficulty analysis of RC items, but further research is needed to improve their metacognitive awareness.

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [102] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: TOPol是一个半监督框架，用于在人类在回路（HoTL）定义的上下文边界（CB）下，重建和解释多维叙事极性场。它使用tLLM嵌入文档，UMAP投影，Leiden分割，并计算主题边界质心之间的方向向量，以量化语义偏移。该框架还可以评估CB质量，检测极性变化，并通过tLLM解释极性向量。TOPol在宏观经济断点和产品评论语料库上进行了评估，证明了其捕捉情感和非情感极性转变的能力。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学中的语义极性方法将情感视为单一维度，忽略了语言的多维结构。本研究旨在提出一个能够重建和解释多维叙事极性场的新框架。

Method: TOPol框架通过以下步骤实现：1.使用基于transformer的大型语言模型（tLLM）嵌入文档。2.应用邻域调整的UMAP投影。3.通过Leiden分割对主题进行分割。4.在给定的上下文边界（CB）A和B之间，计算对应主题边界质心之间的方向向量，生成极性场。5.利用tLLM比较极性向量的极端点，生成带有估计覆盖率的对比标签。

Result: TOPol框架能够量化语义偏移，评估CB质量，并检测极性变化。在美联储演讲和亚马逊产品评论语料库上的评估表明，TOPol能够捕捉情感和非情感的极性转变，并且结果稳定，仅受CB定义的影响。最后，该框架被证明是可扩展、可泛化且可解释的。

Conclusion: TOPol提供了一个可扩展、可泛化且可解释的框架，用于进行上下文敏感的多维话语分析。它能够捕捉情感和非情感的极性转变，克服了传统单维语义极性方法的局限性。

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [103] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: LLMs在生物医学文本中的共指消解能力在通过领域特定提示进行增强后得到提升，但仍受长距离依赖和歧义的影响。


<details>
  <summary>Details</summary>
Motivation: 评估生成式大语言模型（LLMs）在生物医学领域的共指消解能力，并与判别式方法进行比较。

Method: 使用CRAFT语料库，通过四种提示实验（包括局部、上下文丰富、以及缩写和实体字典等领域特定线索）来评估LLMs的性能，并与SpanBERT进行基准测试。

Result: LLMs在共指消解方面展现出强大的表面能力，尤其是在结合了领域知识提示后。然而，其性能仍然受到长距离上下文和提及歧义的影响。LLaMA 8B和17B模型在实体增强提示下表现出更高的精确率和F1分数。

Conclusion: 轻量级提示工程有潜力增强LLMs在生物医学自然语言处理任务中的实用性，但仍需解决长距离依赖和歧义性问题。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [104] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: 本研究提出了DEBATE基准，这是第一个用于评估多代理LLM角色扮演交互真实性的基准，其中包含大规模的真实人类辩论数据，并系统地分析了模拟与真实群体动态的差异，同时探索了LLM在改进交互真实性方面的潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 准确模拟意见变化对于解决错误信息和政治极化等问题至关重要，而现有的LLM角色扮演方法在模拟真实的人类群体动态方面存在不足，缺乏可用于衡量真实性的人类意见轨迹的经验基准。

Method: 构建了一个名为DEBATE的大规模经验基准，其中包含29,417条来自2,792名美国参与者就107个争议性话题进行的多轮辩论对话信息，涵盖了公开消息和私下报告的观点。利用该基准，系统地评估并识别了模拟与真实群体动态之间的关键差异，并通过监督微调来评估DEBATE在使LLM行为与人类行为保持一致方面的效用。

Result: 在表面指标（如ROUGE-L和消息长度）方面取得了改进，但同时揭示了在更深层次的语义一致性（如语义相似性）方面存在局限性，表明LLM在模拟人类社会动态方面既有潜力也存在局限性。

Conclusion: DEBATE基准的提出填补了现有研究的空白，能够用于评估和改进多代理LLM角色扮演交互的真实性，但同时也指出了当前LLM在模拟深层语义一致性方面仍面临挑战。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [105] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 本研究探讨了针对低资源语言的机器翻译模型预训练策略，特别关注了林加拉语，并证明了多语言和多模态数据预训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 为资源匮乏的非洲语言（如林加拉语）开发有效的机器翻译模型，缩小其与高资源语言之间的性能差距。

Method: 在Reid和Artetxe（2021）提出的预训练方法基础上，针对林加拉语进行实验，探索了包括多语言预训练以及在预训练阶段结合单语和并行数据的不同策略。

Result: 研究表明，在多语言上进行预训练，并结合单语和并行数据，能够显著提高翻译质量。

Conclusion: 本研究提出了有效的低资源机器翻译预训练策略，有助于为边缘化社区和代表性不足的群体开发更具包容性和准确性的自然语言处理模型。代码和数据集已公开，以促进进一步研究和可重复性。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [106] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: LLM 训练带来数据安全和合规性风险，机器移除技术应运而生，本文对 2021 年以来 LLM 移除技术的研究进行全面综述，提出新的方法和评估分类，并探讨挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM 训练中的数据安全和合规性问题（如敏感信息、版权、恶意知识的记忆）以及“被遗忘权”的需求，促使了机器移除技术的发展，需要对其进行系统性研究。

Method: 对 2021 年以来发表的超过 180 篇关于 LLM 移除技术的论文进行系统性回顾，特别关注大规模生成模型。提出新的移除方法（训练时、训练后、推理时）和评估（数据集、指标）分类，并分析其优缺点和适用性。

Result: 对现有 LLM 移除技术进行了分类和梳理，总结了现有评估方法，并指出了 LLM 移除技术面临的关键挑战和未来研究方向。

Conclusion: 本文的全面综述旨在为安全可靠的 LLM 的持续发展提供信息和指导，帮助研究者更好地理解和应用机器移除技术。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [107] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 该研究提出了一种新的语音建模方法，通过在潜在空间中分离语义语音内容和背景噪声，并使用代码本标记（tokens）来表示纯净语音，同时将噪声表示为量化残差。


<details>
  <summary>Details</summary>
Motivation: 现有的离散音频表示方法在嘈杂环境中表现不佳，而该研究旨在改进语音建模，使其更能适应现实世界的环境。

Method: 该模型将纯净语音表示为代码本标记（tokens），并将噪声表示为量化残差，利用轻量级分类器进行监督。在整个过程中，Whisper模型保持冻结状态。

Result: 该方法提高了纯净语音/嘈杂语音与文本之间的一致性，生成的语音标记（speech tokens）对噪声具有高度不变性，并提升了自动语音识别（ASR）的性能。与冻结的Whisper模型相比，错误率降低了82%，在VBDemand测试集上比基线方法提高了35%。

Conclusion: 所学的标记空间能够很好地泛化到已 seen 和未 seen 的声学条件。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [108] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: AI搜索需要LLM与外部知识源结合，但原始文档格式不适合LLM。MDP框架通过多种方式将文档转化为LLM可直接使用的知识表示，如代理推理、记忆积累和结构化利用，以提高LLM的推理能力。MDP-Agent作为MDP的实现，通过构建文档摘要记忆、扩散式探索和Map-Reduce式合成，在信息检索基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法将文档视为原始文本，增加了LLM处理的负担，需要新的检索范式来弥合原始文档与LLM之间的差距。

Method: 提出模型-文档协议（MDP）框架，通过代理推理、记忆接地和结构化利用等多种途径，将非结构化文档转化为任务特定的、LLM可用的知识表示。具体实现MDP-Agent，通过构建文档级摘要记忆、扩散式探索和Map-Reduce式合成来整合大规模证据。

Result: MDP-Agent在信息检索基准测试中表现优于现有基线模型，验证了MDP框架的合理性和其代理实现的有效性。

Conclusion: MDP框架能够有效地将非结构化文档转化为LLM可以直接利用的结构化知识，提升了AI搜索的性能。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [109] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 大规模语言模型在低资源语言上的表现不如英语，Chain-of-Thought（CoT）提示对不同模型的影响各异，可能提升弱模型的表现，但损害强模型的表现。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在低资源语言上的表现，以及Chain-of-Thought（CoT）提示对模型跨语言能力的影响。

Method: 构建了一个包含英语、斯瓦希里语和豪萨语（分别为高、中、低资源语言）的问答基准，并测试了GPT-4 Turbo、Gemini 1.5 Flash和LLaMA 3 70B等模型在基准上的表现，同时引入了CoT提示进行对比。

Result: 所有模型在英语上表现优异，但在斯瓦希里语和豪萨语上准确率显著下降，LLaMA 3表现最差。CoT提示显著提升了LLaMA 3的准确率，但对GPT-4和Gemini产生了负面影响，导致其准确率下降。

Conclusion: LLMs在低资源语言上的表现受到语言资源丰度的显著影响。CoT提示并非万能解决方案，其有效性取决于模型的基线能力和任务的具体情境，并且可能在跨语言场景下对不同能力的模型产生截然相反的影响。实验框架能够揭示LLMs的弱点以及影响其决策的因素。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [110] [ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation](https://arxiv.org/abs/2510.25224)
*Ziyi Liu,Bahar Sarrafzadeh,Pei Zhou,Longqi Yang,Jieyu Zhao,Ashish Sharma*

Main category: cs.CL

TL;DR: 该论文提出了ProMediate，一个用于评估主动AI调解代理在复杂、多方、多主题谈判中的框架，并展示了其在提高共识和响应速度方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统性的评估方法来测试能够在多方协作中主动管理的AI代理，而谈判是检验此类代理社会认知智能的理想场景。

Method: 提出ProMediate框架，包含两个核心部分：1. 基于真实谈判案例和难度级别（简单、中等、困难）的模拟测试平台，内置基于社会认知调解理论的即插即用型主动AI调解代理。2. 一个社会认知评估框架，包含衡量共识变化、干预延迟、调解员有效性和智能的新指标。

Result: 实验结果表明，与通用基线相比，社会智能调解代理能够更快、更精确地进行干预。在ProMediate-Hard场景下，社会调解代理的共识变化提升了3.6个百分点（10.65% vs 7.01%），响应速度快了77%（15.98s vs 3.71s）。

Conclusion: ProMediate提供了一个严谨且基于理论的测试平台，能够推动主动的、具有社会智能的AI代理的发展。

Abstract: While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.

</details>


### [111] [Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA](https://arxiv.org/abs/2510.25273)
*Sandipan Majhi,Paheli Bhattacharya*

Main category: cs.CL

TL;DR: 通过生成合成数据和微调小模型来解决低资源语言的领域特定问答问题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的领域特定问答面临着标注数据稀缺和通用语言模型领域知识有限的挑战。

Method: 使用 LLaMA-70B 和 Phi-14B 等大型语言模型生成合成问答对，以扩充有限的原始数据集，并采用多阶段微调策略来调整轻量级语言模型以适应印地语旅游领域。

Result: 大型模型可以有效地生成合成数据，而小型模型可以有效地适应合成数据，为低资源、领域特定的问答提供了一条可扩展的途径。

Conclusion: 所提出的方法为低资源语言的领域特定问答提供了一个有效且可扩展的解决方案。

Abstract: Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.

</details>


### [112] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: PEKD框架通过知识蒸馏提升了低资源设置下的多模态讽刺检测性能，特别是在少量样本的情况下。


<details>
  <summary>Details</summary>
Motivation: 低资源设置下的多模态讽刺检测面临数据稀疏导致模型难以学习图像-文本矛盾的挑战，影响模型性能。

Method: 提出PEKD框架，通过蒸馏大型讽刺数据集上训练的专家模型（教师）来增强参数高效微调（PEFT）方法。引入熵感知门控机制，根据教师模型的置信度动态调整蒸馏强度，以减轻不可靠信号的影响。

Result: 在两个公开数据集上的实验表明，PEKD框架能使PEFT方法优于先前参数高效方法和大型多模态模型，在少量样本场景下取得强劲成果。

Conclusion: PEKD框架是模块化的，可适应广泛的多模态模型和任务，有效解决了低资源多模态讽刺检测的挑战。

Abstract: Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.

</details>


### [113] [Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning](https://arxiv.org/abs/2510.25310)
*Senjie Jin,Lu Chen,Zhiheng Xi,Yuhui Wang,Sirui Song,Yuhao Zhou,Xinbo Zhang,Peng Sun,Hong Lu,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 通过引入Parrot训练流程，实现自然语言链式思考（N-CoT）和程序链式思考（P-CoT）的协同增强，从而在数学推理任务上同时提升两种方法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合N-CoT和P-CoT在数学推理上的局限性，并通过相互增强实现性能的协同提升，而非单一方向的优化。

Method: 提出名为Parrot的新训练流程，包含三个子任务：1) 整合序列化的P-CoT和N-CoT生成；2) 采用子任务混合训练策略以增强自然语言语义的可迁移性；3) 设计转换后的N-CoT辅助奖励以解决P-CoT优化中的稀疏奖励问题。

Result: Parrot显著提升了N-CoT和P-CoT在数学推理任务上的表现，尤其对N-CoT的提升效果明显。具体而言，使用Parrot SFT后，LLaMA2和CodeLLaMA在MathQA数据集上的N-CoT性能相比资源密集型的RL基线分别提升了+21.87和+21.48。

Conclusion: Parrot训练流程能够有效地实现N-CoT和P-CoT的相互促进，从而在数学推理任务上取得显著的性能提升。

Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.

</details>


### [114] [CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories](https://arxiv.org/abs/2510.25333)
*Yilong Lai,Yipin Yang,Jialong Wu,Fengran Mo,Zhenglin Wang,Ting Liang,Jianguo Lin,Keping Yang*

Main category: cs.CL

TL;DR: CRMWeaver是一个增强商业智能代理的新方法，通过合成数据生成、基于强化学习的训练以及共享记忆机制来提高其在复杂业务环境中的表现，尤其是在处理数据库交互和多样化任务方面。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理在处理复杂业务环境（如数据库交互、异构任务）时面临挑战，需要提高其在复杂数据和多样化任务上的能力。

Method: 训练阶段：采用合成数据生成和基于强化学习的范式；推理阶段：引入共享记忆机制，利用相似任务的经验来提升模型性能和泛化能力。

Result: 在CRMArena-Pro数据集上进行了验证，该轻量级模型在B2B和B2C业务场景中均取得了具有竞争力的结果。

Conclusion: CRMWeaver通过其创新的训练和推理机制，有效提升了商业智能代理处理复杂业务场景的能力，具有实际应用价值。

Abstract: Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.

</details>


### [115] [Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments](https://arxiv.org/abs/2510.25356)
*Abhishek Purushothama,Junghyun Min,Brandon Waldon,Nathan Schneider*

Main category: cs.CL

TL;DR: LLM在法律解释中的应用存在不稳定性，且与人类判断的相关性较弱，不应过度依赖。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在法律解释中的实际应用效果，并对其可靠性提出质疑。

Method: 通过改变提问格式，观察LLM的法律解释判断是否稳定，并将其与人类判断进行比较，分析相关性。

Result: LLM的法律解释判断不稳定，不同格式的提问会导致截然不同的结论。LLM与人类判断的相关性较弱，且在不同模型和问题变体之间存在大方差。

Conclusion: 鉴于LLM在法律解释中的不稳定性与低相关性，目前不应过度依赖其生成的结论。

Abstract: Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.

</details>


### [116] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone,Alessandro Bondielli,Alessandro Lenci*

Main category: cs.CL

TL;DR: 小规模语言模型（LM）可以通过指令调优获得小的、但持续的收益，特别是在顺序课程方法下，然而这种收益在零样本任务上并不总是成立，表明在交互式和泛化能力之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 研究小规模语言模型是否可以通过指令调优获得收益，并探索不同指令调优方法（对话、问答、合并或顺序课程）对模型性能的影响。

Method: 使用100M和140M参数的 decoder-only 模型，分别应用对话和问答指令调优数据集，并以合并或顺序课程的方式进行。在 SuperGLUE（微调）、BLiMP、EWoK、WUGs、实体追踪和心理语言学相关性（零样本）等多个设置下进行评估。

Result: 指令调优在微调场景下带来了小的、但持续的提升，其中顺序课程方法优于合并数据。然而，在零样本任务上的提升并不总是成立，这表明交互式适应与广泛的语言泛化能力之间存在权衡。

Conclusion: 小规模语言模型可以通过指令调优获得收益，但存在局限性。研究结果强调了将受人类启发的学习策略应用于低资源语言模型的潜力和约束，并指出了在生态训练限制下，通过混合、基于课程的方法来增强泛化能力的途径。

Abstract: This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.

</details>


### [117] [Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs](https://arxiv.org/abs/2510.25370)
*Alexander Sternfeld,Andrei Kucharavy,Dimitri Percia David,Alain Mermoud,Julian Jang-Jaccard,Nathan Monnet*

Main category: cs.CL

TL;DR: 该研究提出了一种新的、数据驱动的流水线，利用大型语言模型（LLMs）和图分析来识别技术融合信号，从而预测信息通信技术（ICT）领域变革性技术的出现。


<details>
  <summary>Details</summary>
Motivation: 传统的基于专家的方法难以跟上信息通信技术（ICT）等快速发展的领域的创新周期和早期模糊的术语。需要一种新的方法来应对这一挑战。

Method: 利用大型语言模型（LLMs）从非结构化文本中提取语义三元组，构建技术实体和关系的图谱。提出了一种新的方法（名词拼接）对相似的技术术语进行分组，并开发了基于图的指标来检测融合信号。该流水线包括多阶段过滤、特定领域关键词聚类和主题共现的时间趋势分析。

Result: 该方法在arXiv预印本和USPTO专利申请数据集上进行了验证，能够识别已建立和新兴的技术融合模式。

Conclusion: 该研究提供了一个可扩展且可泛化的框架，用于基于全文分析进行技术预测，能够有效识别变革性技术的出现。

Abstract: Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.

</details>


### [118] [Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy](https://arxiv.org/abs/2510.25378)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: LLM在生成文献推荐时存在虚构文献的问题，本研究通过分析论文的引用次数来评估LLM的文献生成能力，发现引用次数越高的论文越不容易被模型虚构。


<details>
  <summary>Details</summary>
Motivation: LLM在生成文献推荐时存在虚构非现有文献的问题，本研究旨在探究论文的引用次数对LLM生成文献准确性的影响。

Method: 使用GPT-4.1模型，生成并手动验证了20个计算机科学领域共100条文献记录，通过计算生成文献元数据与真实文献元数据之间的余弦相似度来衡量事实一致性，并将论文的引用次数作为训练数据冗余度的代理指标。

Result: 研究结果表明：(i)文献虚构率在不同研究领域存在差异；(ii)论文的引用次数与事实准确性高度相关；(iii)引用次数超过约1000次的论文，其文献信息几乎被模型逐字记住。

Conclusion: 高引用次数的论文几乎被模型完整记忆，这表明存在一个从泛化到记忆的阈值。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.

</details>


### [119] [Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires](https://arxiv.org/abs/2510.25384)
*Doan Nam Long Vu,Rui Tan,Lena Moench,Svenja Jule Francke,Daniel Woiwod,Florian Thomas-Odenthal,Sanna Stroth,Tilo Kircher,Christiane Hermann,Udo Dannlowski,Hamidreza Jamalabadi,Shaoxiong Ji*

Main category: cs.CL

TL;DR: We developed SQPsych, a pipeline using open-weight LLMs to generate synthetic counseling dialogues grounded in CBT principles for mental health AI, addressing privacy concerns and data limitations. Our models show strong performance on counseling benchmarks.


<details>
  <summary>Details</summary>
Motivation: The development of AI for mental health is hindered by the lack of authentic therapy dialogues due to privacy regulations and historical recording limitations.

Method: We created a pipeline, SQPsych, that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires, using open-weight LLMs and simulating therapist-client interactions grounded in CBT principles. The synthetic corpus was validated through human expert and LLM-based assessments.

Result: Our SQPsychLLM models, fine-tuned on the synthetic corpus, achieved strong performance on counseling benchmarks, outperforming baselines in key therapeutic skills. The synthetic data enables scalable, data-secure, and clinically informed AI for mental health support.

Conclusion: Synthetic data generated by our SQPsych pipeline holds significant potential for advancing AI in mental health support by overcoming data privacy and availability challenges, while maintaining clinical validity and performance.

Abstract: The development of AI for mental health is hindered by a lack of authentic
therapy dialogues, due to strict privacy regulations and the fact that clinical
sessions were historically rarely recorded. We present an LLM-driven pipeline
that generates synthetic counseling dialogues based on structured client
profiles and psychological questionnaires. Grounded on the principles of
Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic
conversations for clinical disorders such as anxiety and depression. Our
framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts
structured psychological input into natural language dialogues through
therapist-client simulations. Due to data governance policies and privacy
restrictions prohibiting the transmission of clinical questionnaire data to
third-party services, previous methodologies relying on proprietary models are
infeasible in our setting. We address this limitation by generating a
high-quality corpus using open-weight LLMs, validated through human expert
evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on
SQPsychConv achieve strong performance on counseling benchmarks, surpassing
baselines in key therapeutic skills. Our findings highlight the potential of
synthetic data to enable scalable, data-secure, and clinically informed AI for
mental health support. We will release our code, models, and corpus at
https://ai-mh.github.io/SQPsych

</details>


### [120] [BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains](https://arxiv.org/abs/2510.25409)
*Vijay Devane,Mohd Nauman,Bhargav Patel,Aniket Mahendra Wakchoure,Yogeshkumar Sant,Shyam Pawar,Viraj Thakur,Ananya Godse,Sunil Patra,Neha Maurya,Suraj Racha,Nitish Kamal Singh,Ajay Nagpal,Piyush Sawarkar,Kundeshwar Vijayrao Pundalik,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: BhashaBench V1是一个针对印度语境的、包含特定领域和多语言的大型语言模型评估基准，旨在弥合现有评估基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型评估基准存在地域和领域上的局限性，尤其是在非英语国家和特定知识领域，这限制了它们在印度等地的应用。因此，需要一个能够反映印度本土知识体系和语言特点的评估工具。

Method: 创建了一个包含74,166个问答对（英文52,494个，印地文21,672个）的双语、多领域基准BhashaBench V1，覆盖了农业、法律、金融和传统医学（Ayurveda）四大领域，以及90多个子领域和500多个主题。对29个大型语言模型进行了评估。

Result: 评估结果显示，大型语言模型在特定领域和语言方面存在显著的性能差距，尤其是在资源匮乏的领域。例如，GPT-4o在法律领域（76.49%）的准确率远高于在传统医学领域（59.74%）。所有模型在英文内容上的表现均优于印地文内容。子领域分析显示，网络法、国际金融等领域表现相对较好，而Panchakarma、种子科学、人权等领域则表现较弱。

Conclusion: BhashaBench V1提供了一个全面的数据集，用于评估大型语言模型在印度多元化知识领域的表现，特别是其整合领域知识和双语理解能力。该基准的公开有助于促进相关研究的开放性。

Abstract: The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.

</details>


### [121] [Serve Programs, Not Prompts](https://arxiv.org/abs/2510.25412)
*In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: LLM服务系统可以服务程序而不是提示，以提高效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM服务系统效率低下，不适用于复杂的LLM应用。

Method: 提出了一种新的LLM服务系统架构，服务于称为LLM推理程序（LIP）的程序。这些程序允许用户自定义运行时行为并卸载部分应用程序逻辑。通过一个名为Symphony的系统来演示这种架构，该系统将LLM模型计算通过系统调用暴露出来，并使用专门的文件系统对KV缓存进行虚拟化。

Result: Symphony通过两级进程调度方案确保GPU效率。

Conclusion: Symphony有潜力为LLM应用提供更高效、更可扩展的生态系统。

Abstract: Current large language model (LLM) serving systems, primarily designed for
text completion, are neither efficient nor adaptable for increasingly complex
LLM applications due to their inflexible design. We propose a new LLM serving
system architecture that serves programs instead of prompts to address this
problem. These programs, called LLM Inference Programs (LIPs), allow users to
customize token prediction and KV cache management at runtime and to offload
parts of their application logic, such as tool execution, to the server. We
describe an example of this architecture through a system named Symphony, which
functions as an operating system for LIPs. Symphony exposes LLM model
computations via system calls and virtualizes KV cache with a dedicated file
system, while ensuring GPU efficiency with a two-level process scheduling
scheme. Symphony has the potential to open the door to a more efficient and
extensible ecosystem for LLM applications.

</details>


### [122] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: We introduce the first framework using Vision Language Models (VLMs) to automate the annotation and filtering of sign language datasets, significantly reducing manual effort and cost. This framework is applied to TikTok videos across eight sign languages and evaluated on an existing German dataset. Our pipeline includes steps for face visibility detection, sign activity recognition, text extraction, and validation to ensure data quality. The resulting dataset, TikTok-SL-8, is used to establish baselines and evaluate the robustness of existing Sign Language Translation (SLT) models on automatically acquired data, paving the way for scalable, weakly supervised pretraining for SLT.


<details>
  <summary>Details</summary>
Motivation: Existing sign language translation (SLT) datasets are limited in scale, multilingual coverage, and are expensive to create due to reliance on expert annotation and controlled recording environments. VLMs show potential for dataset acquisition, but this has not been explored.

Method: We developed an automated annotation and filtering framework using VLMs. The pipeline consists of face visibility detection, sign activity recognition, text extraction from video, and a validation step to ensure alignment between video and text. This framework was applied to TikTok videos (eight sign languages) and the YouTube-SL-25 dataset (German Sign Language).

Result: We created a new corpus, TikTok-SL-8, by applying our VLM-based pipeline to TikTok videos. We used this corpus to assess the performance of two off-the-shelf SLT models on German and American Sign Languages, establishing baselines and evaluating model robustness on automatically extracted, slightly noisy data.

Conclusion: Our VLM-based framework enables scalable and cost-effective acquisition of sign language data, particularly from social media. This facilitates weakly supervised pretraining for SLT and helps establish baselines for evaluating SLT models on real-world, diverse data.

Abstract: Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.

</details>


### [123] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在人机交互（HCI）中至关重要，本研究探讨了LLM理解隐含意义（implicature）的能力及其对响应生成的影响。


<details>
  <summary>Details</summary>
Motivation: 本研究认为，推进人机交互（HCI）需要关注交互的语言基础，特别是隐含意义（implicature），这对于人-AI（HAI）对齐至关重要。

Method: 本研究检查了大型语言模型（LLM）推断用户意图的能力，并评估了理解隐含意义是否能改善响应生成。

Result: 结果表明，较大的模型更接近人类的解释，而较小的模型在推断隐含意义方面存在困难。此外，基于隐含意义的提示在所有模型中显著提高了响应的相关性和质量，尤其是在较小的模型中。总的来说，67.6%的参与者更喜欢包含隐含意义的提示，而不是字面提示，这凸显了对上下文细微差别沟通的明显偏好。

Conclusion: 本研究有助于理解如何利用语言理论来解决AI对齐问题，使人-AI交互更加自然和基于上下文。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [124] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: 尽管在标准基准上取得了显著进展，但大型语言模型（LLMs）在研究级神经定理证明和自动形式化方面的实际应用仍然有限。我们提出了RLMEval，一个专注于真实世界Lean形式化项目中的研究级数学的评估套件，以解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在研究级数学中的神经定理证明和自动形式化能力，并指出当前基准测试的局限性。

Method: 利用真实的Lean Blueprint形式化项目，构建了一个包含613个定理的RLMEval评估套件，用于评估现有模型在研究级数学定理证明和自动形式化方面的表现。

Result: 在RLMEval评估套件上，现有最先进的模型通过率仅为10.3%，表明在现有基准上的进展未能有效迁移到更真实、更具挑战性的研究级数学场景。

Conclusion: RLMEval提供了一个新的、更具挑战性的基准，旨在推动和加速形式化数学自动化推理的进展。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [125] [Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research](https://arxiv.org/abs/2510.25432)
*Ali Sanaei,Ali Rajabzadeh*

Main category: cs.CL

TL;DR: 该研究提出了一个用于定性研究中大型语言模型（LLM）应用的框架，通过“解释深度”和“自主性”两个维度进行分类，并提供设计建议，旨在解决LLM在定性研究中存在的解释偏见、可靠性低和可审计性弱等挑战。


<details>
  <summary>Details</summary>
Motivation: 定性社会科学领域的研究者在采用大型语言模型（LLM）时面临解释偏见、可靠性低和可审计性弱等挑战。

Method: 提出一个包含“解释深度”和“自主性”两个维度的框架，用于对定性研究中LLM的应用进行分类，并基于Web of Science收录的已有文献进行分析，为LLM的应用提供设计建议。研究鼓励将任务分解，保持低自主性，并有选择地增加解释深度，以确保透明度和可靠性。

Result: 通过对已有文献的分析，展示了LLM在定性研究中应用的现状，并提出了一种在利用LLM优势的同时保持透明度和可靠性的方法。

Conclusion: 通过将LLM的应用限制在较低的自主性水平，并有选择地增加解释深度（在监督下进行），研究者可以有效利用LLM的优势，同时保持研究的透明度和可靠性。

Abstract: Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.

</details>


### [126] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Eleftherios Avramidis,Josef van Genabith*

Main category: cs.CL

TL;DR: 现有的基于文本的自动评估指标（如BLEU、ROUGE）在评估手语翻译（SLT）输出质量方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有文本评估指标在评估手语翻译（SLT）输出质量方面的局限性，有必要进行更深入的研究。

Method: 通过分析六种指标（包括BLEU、chrF、ROUGE、BLEURT以及G-Eval和GEMBA等大语言模型（LLM）驱动的评估器），并在三种受控条件下（释义、模型输出中的幻觉和句子长度变化）评估它们的一致性和鲁棒性。

Result: 分析表明，基于词汇重叠的指标存在局限性，LLM驱动的评估器虽然能更好地捕捉语义等价性，但也可能偏向于LLM改写的翻译。所有指标都能检测到幻觉，但BLEU过于敏感，而BLEURT和LLM驱动的评估器则相对宽容。

Conclusion: 需要开发超越文本的、多模态的评估框架，以实现对SLT输出的更全面评估。

Abstract: Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.

</details>


### [127] [Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs](https://arxiv.org/abs/2510.25441)
*Fei Wei,Daoyuan Chen,Ce Wang,Yilun Huang,Yushuo Chen,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.CL

TL;DR: LLMs善于被动响应，但要使其成为主动、目标导向的伙伴极具挑战性。现有方法要么只关注单轮优化，要么依赖昂贵的用户模拟器，存在“现实差距”。我们提出了Learn-to-Ask框架，可以直接从离线专家数据中学习和部署主动对话代理，无需模拟用户。我们通过利用专家轨迹的“已观察到的未来”，将长期问题分解为一系列监督学习任务，训练模型输出（动作，状态评估）元组，以决定“问什么”以及“何时停止”。通过Automated Grader Calibration管道净化基于LLM的奖励模型中的噪声。在医疗数据集上的实验表明，该方法有效，并成功部署到实际在线AI服务中，性能优于人类专家，证明了其将离线数据转化为实际影响的能力。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM作为主动、目标导向的对话伙伴的方法存在挑战，要么优化单轮属性，要么依赖昂贵且不稳定的用户模拟器，导致“现实差距”。

Method: 提出Learn-to-Ask框架，通过利用专家轨迹的“已观察到的未来”，将长期问题分解为一系列监督学习任务，训练模型输出（动作，状态评估）元组，以决定“问什么”以及“何时停止”。使用Automated Grader Calibration管道净化基于LLM的奖励模型中的噪声。

Result: 在现实世界的医疗数据集上，使用不同大小的LLM（高达32B）进行了实验，结果表明Learn-to-Ask框架有效。该方法成功部署到大型在线AI服务中，并在内部评估中表现优于人类专家。

Conclusion: Learn-to-Ask框架能够将离线数据转化为实际的、面向用户的LLM应用，为将LLM从被动响应者转变为主动、目标导向的AI应用提供了一个实用且经济可行的蓝图。

Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.

</details>


### [128] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: 本研究提出一种结合微调大语言模型（LLM）和命名实体识别（NER）的流水线，用于领域特定文本的摘要和标记，以应对亚文化语言和俚语的快速演变。


<details>
  <summary>Details</summary>
Motivation: 快速演变的亚文化语言和俚语给信息提取和执法监控带来挑战，需要有效的自动化解决方案。

Method: 利用LLaMA Factory框架，在通用和特定领域（政治、安全）数据集上微调LLM，并评估其在摘要和标记任务上的表现。

Result: 指令微调显著提高了摘要和标记的准确性，LLaMA3-8B-Instruct在领域特定微调后优于中文训练模型，表明推理能力可跨语言迁移。

Conclusion: 该流水线能够生成简洁的摘要和结构化的实体标记，支持文档快速分类和分发，并且该方法具有可扩展性和适应性，适用于实时应用，为知识管理和安全运营提供了一种强大的解决方案。

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [129] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: LLM在模拟个人沟通风格、行为倾向和人格特征方面显示出新兴的类人能力，但现有评估有限。我们提出了TwinVoice基准，用于评估跨越社交、人际和叙事多维度以及六项关键能力（观点一致性、记忆回忆、逻辑推理、词汇保真度、语气和句法风格）的LLM个人模拟。实验表明，尽管先进模型表现出中等准确性，但在句法风格和记忆回忆方面仍有不足，平均表现远低于人类基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个人模拟评估的局限性，包括依赖合成对话、缺乏系统框架和能力要求分析。

Method: 提出TwinVoice基准，包含社交、人际和叙事三个维度，并分解为观点一致性、记忆回忆、逻辑推理、词汇保真度、语气和句法风格六项能力进行评估。

Result: 先进模型在个人模拟方面达到中等准确性，但在句法风格和记忆回忆方面表现不佳，平均表现低于人类基线。

Conclusion: 虽然LLM在个人模拟方面取得了进展，但与人类水平相比仍有显著差距，尤其是在句法风格和记忆回忆等方面的能力需要进一步提升。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [130] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 本文研究了信息不对称条件下LLM智能体之间的任务协作问题，并提出了一种结合通信和环境验证的框架来增强其协作能力。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在完成联合目标方面的协作能力，尤其是在信息不对称的情况下，研究不足。

Method: 将爱因斯坦谜题扩展为桌面游戏，并提出了一种结合通信策略和环境验证信号的微调加验证器框架。

Result: 实验结果表明，对齐的通信对于LLM智能体的协作至关重要。环境验证器可以增强智能体对任务规则的理解和任务完成能力，从而实现更安全、可解释的协作。

Conclusion: 在信息不对称的情况下，LLM智能体可以通过对齐的通信和环境验证来实现有效的任务协作。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [131] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: FARSIQA是一个针对波斯语伊斯兰领域的新型、端到端、忠实的高级问答系统，采用了FAIR-RAG架构，通过自纠正、迭代细化方法解决了LLM在专业领域（如宗教问答）的幻觉和不忠实问题，在IslamicPCQA基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统在处理需要多步推理和证据聚合的复杂、多跳查询时表现不佳，特别是在宗教问答等高风险、专业领域，LLM的幻觉和不忠实问题阻碍了其应用，这对于波斯语穆斯林社区来说尤为关键，因为准确性和可信度至关重要。

Method: FARSIQA采用了一种名为FAIR-RAG（Faithful, Adaptive, Iterative Refinement for RAG）的新颖架构。该架构包含一个动态的、自纠正的过程：它能自适应地分解复杂查询，评估证据的充分性，并进入一个迭代循环，以生成子查询，逐步填补信息空白。该系统在一个包含超过一百万份权威伊斯兰文献的精选知识库上运行。

Result: 在具有挑战性的IslamicPCQA基准测试中，FARSIQA取得了最先进的性能。具体而言，该系统在负例拒绝方面取得了97.0%的惊人成绩，比基线提高了40个百分点；在答案正确性方面也获得了74.3%的高分。

Conclusion: FARSIQA在波斯语伊斯兰问答领域树立了新的标准，并验证了其迭代、自适应的架构对于在敏感领域构建忠实、可靠的AI系统至关重要。该研究表明，FAIR-RAG架构在提高问答系统的准确性和可信度方面非常有效。

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [132] [Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks](https://arxiv.org/abs/2510.25623)
*Davide Romano,Jonathan Schwarz,Daniele Giofré*

Main category: cs.CL

TL;DR: 在法律领域的多项选择问答任务中，研究了验证器驱动的测试时缩放（TTS）技术，以评估其性能提升及其在低N预算下的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然测试时缩放（TTS）技术在数学和编程等领域已被证明能有效提升大型语言模型（LLMs）的性能，但其在法律等论证领域的应用仍有待探索。

Method: 通过在五个基准测试中对法律多项选择问答（MCQA）任务进行验证器驱动的TTS方法的实证研究，并使用一系列奖励模型来评估结果级（Best-of-N）和过程级（树搜索）验证，特别是在实际的低N预算下。

Result: 对验证器效用如何受到领域专业化、模型大小和监督类型（过程监督PRM vs. 仅结果ORM）等关键特性的影响进行了系统性分析，即使在不同角色下也是如此。

Conclusion: 研究了验证器驱动的TTS方法在法律MCQA任务中的适用性和影响因素。

Abstract: Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming \citep{snell2024scaling, chen2024more}, its value in argumentative
domains such as law remains underexplored. We present an empirical study of
verifier-based TTS methods for legal multiple-choice QA (MCQA) across five
benchmarks. Using a family of 7 reward models, we evaluate both outcome-level
(Best-of-$N$) and process-level (tree search) verification under realistic
low-$N$ budgets. Our analysis systematically investigates how verifier utility
is affected by key properties such as domain specialization, model size, and
supervision type (process-supervised PRMs vs. outcome-only ORMs), even when
applied across different roles.

</details>


### [133] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie*

Main category: cs.CL

TL;DR: 该研究提出了EHR-Ins数据集、EHR-R1模型和EHR-Bench基准，以解决现有大型语言模型在分析电子健康记录（EHR）方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在分析电子健康记录（EHR）方面存在任务覆盖范围窄、缺乏面向EHR的推理能力等问题，限制了其在临床决策中的应用。

Method: 研究提出了一个创新的思维图驱动框架，用于大规模生成高质量的EHR推理数据，构建了包含300k推理和4M非推理案例的EHR-Ins数据集。基于此，开发了EHR-R1系列模型（参数高达72B），并采用领域适应、推理增强和强化学习的多阶段训练范式进行训练。同时，构建了EHR-Bench基准，包含42项任务，用于评估EHR分析能力。

Result: EHR-R1模型在EHR-Bench基准上相比最先进的商业和开源LLMs（包括DeepSeek-V3和GPT-4o）表现更优，在MIMIC-Bench上的得分超出GPT-4o 30多分，在EHRSHOT上的零样本AUROC提高了10%。

Conclusion: EHR-Ins数据集、EHR-R1模型和EHR-Bench基准的提出，显著推动了更可靠、临床相关性更强的EHR分析方法的发展。

Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.

</details>


### [134] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: PairUni框架通过将数据重组为理解-生成（UG）对并进行相应的优化来解决统一视觉语言模型（UVLM）在理解和生成任务之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 统一视觉语言模型（UVLM）需要在单一架构中同时执行理解和生成任务，但这些任务依赖于异构数据和监督，使得在强化学习（RL）中难以平衡它们。

Method: PairUni框架首先使用GPT-o3增强单任务数据，为理解样本生成标题，为生成样本生成问答（QA）对，从而从同一实例形成对齐的对。此外，对于每个生成样本，检索一个语义上相关的理解示例以形成检索对。然后，提出一种名为Pair-GPRO的配对感知变体，用于策略优化，它为每个配对分配一个相似度分数来调节优势。数据集PairUG包含16K个UG对，用于RL微调。

Result: PairUni在强大的Janus-Pro UVLMs上进行了评估，并在各种UVLMs上实现了平衡的改进，优于强大的UVLM RL基线。

Conclusion: PairUni框架通过利用配对结构来暴露跨任务语义对应关系，并支持一致的策略学习，从而有效地实现了UVLM在理解和生成任务之间的平衡。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}

</details>


### [135] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri,Kristof Juhasz,Mathieu Ravaut,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在结构化金融风险预测方面不如传统模型 LightGBM，并且其自我解释的可靠性存在问题。


<details>
  <summary>Details</summary>
Motivation: 评估 LLM 在金融风险评估等高风险金融应用中处理结构化表格数据的能力，并与 LightGBM 等先进模型进行比较。

Method: 在真实的贷款违约预测任务上，比较了零样本 LLM 分类器和 LightGBM 的预测性能，并使用 SHAP 分析了特征归因和 LLM 生成的自我解释的可靠性。

Result: LLM 能够识别关键的金融风险指标，但其特征重要性排名与 LightGBM 存在显著差异，并且其自我解释往往与经验性的 SHAP 归因不符。

Conclusion: LLM 作为独立的结构化金融风险预测模型存在局限性，并且其自我解释的可靠性令人担忧。在风险敏感的金融环境中部署 LLM 时，需要进行可解释性审计、与可解释基线模型的比较以及人工监督。

Abstract: Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.

</details>


### [136] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: Tool Decathlon (Toolathlon) 是一个评估语言代理在真实世界多步工作流中表现的基准，它包含 32 个应用程序和 604 个工具，提供了真实的环境状态和基于执行的评估，现有模型在该基准上的表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理基准在评估真实世界性能方面存在不足，它们通常关注狭窄的领域或简化的任务，缺乏多样性、真实性和长远复杂性。为了解决这个差距，需要一个能够评估语言代理在多步骤、跨应用程序工作流中表现的基准。

Method: 构建了一个名为 Tool Decathlon (Toolathlon) 的新基准，其中包含 32 个软件应用程序和 604 个工具，涵盖日常和专业平台。该基准提供了基于真实软件的高质量初始环境状态，并包含 108 个需要跨多个应用程序交互约 20 个回合才能完成的任务。所有任务都通过专门的评估脚本进行严格验证。

Result: 对最先进模型 (SOTA) 的全面评估显示，即使是最佳模型 Claude-4.5-Sonnet 也只能达到 38.6% 的成功率（平均 20.2 个工具调用回合），而表现最好的开源模型 DeepSeek-V3.2-Exp 的成功率仅为 20.1%。这表明现有模型在处理复杂、长远任务方面存在显著的不足。

Conclusion: Toolathlon 基准揭示了当前语言代理在处理真实世界、长远任务方面的局限性。该基准有望推动更强大、更具能力的语言代理的发展，以应对这些挑战。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [137] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 评估大语言模型（LLM）的遗忘能力是一个难题。本文提出SKeB框架，利用刺激-知识-行为的关联性来评估遗忘效果，并证明了说服性提示可以有效恢复被遗忘的事实知识，且对小模型效果更显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的遗忘（unlearning）对于管理敏感数据和纠正错误信息至关重要，但评估其有效性仍然是一个未解决的问题。

Method: 本文提出刺激-知识-行为（SKeB）框架，该框架通过领域图建模信息纠缠，并检验被遗忘模型中的事实回忆是否与说服性框架相关。研究人员开发了纠缠度量标准来量化知识激活模式，并评估输出的事实性、非事实性和幻觉。

Result: 结果表明，说服性提示能显著提高事实知识的回忆率（从14.8%的基线提高到采用权威框架时的24.5%），并且其有效性与模型大小呈反比（2.7B模型恢复率为128%，而13B模型为15%）。

Conclusion: SKeB框架为评估LLM的遗忘完整性、鲁棒性和整体行为奠定了基础。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [138] [Scaling Latent Reasoning via Looped Language Models](https://arxiv.org/abs/2510.25741)
*Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou,Wei Ye,Jiaheng Liu,Jian Yang,Yunfeng Shi,Chenghua Lin,Enduo Zhao,Tianle Cai,Ge Zhang,Wenhao Huang,Yoshua Bengio,Jason Eshraghian*

Main category: cs.CL

TL;DR: Ouro是一个名为LoopLM的新型预训练语言模型家族，通过在预训练阶段集成迭代计算和熵正则化目标来增强推理能力，并在7.7T token上进行扩展，其性能可媲美更大的模型，并且具有更强的知识操控能力和更一致的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）主要通过显式文本生成（如思维链CoT）进行推理，这导致预训练数据利用不足，并将推理推迟到训练后。 Ouro旨在通过将推理能力整合到预训练阶段来解决这个问题。

Method: Ouro模型通过（i）潜在空间中的迭代计算、（ii）用于学习深度分配的熵正则化目标以及（iii）扩展到7.7T token来实现其预训练和推理能力。 这种方法被称为Looped Language Models（LoopLM）。

Result: Ouro模型（1.4B和2.6B）在多项基准测试中表现出色，性能可与高达12B参数的SOTA LLM相媲美。 对比实验表明，Ouro的优势在于其卓越的知识操控能力，而非知识容量的增加。 此外，LoopLM产生的推理过程与最终输出更加一致，优于显式CoT。

Conclusion: Ouro模型证明了LoopLM作为一种新颖的扩展方向在推理时代的潜力。 该模型通过将推理能力嵌入预训练阶段，实现了更高的性能和更优的推理过程。

Abstract: Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.

</details>


### [139] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 当前对智能体的评估主要集中在一次性任务完成，未能考虑现实世界问题固有的迭代和协作性质，以及人类目标的不确定性和演变性。我们主张将重点从构建和评估任务完成型智能体转移到开发协作型智能体，其评估标准不仅包括最终输出的质量，还包括它们在整个问题解决过程中与人类互动的程度以及对人类努力的增强程度。为了支持这一转变，我们引入了协作工作量扩展框架，用于捕捉智能体的效用如何随着用户参与度的增加而增长。通过案例研究和模拟评估，我们发现最先进的智能体在多轮现实场景中的表现往往不佳，揭示了智能体设计中缺失的一环：维持参与和脚手架式用户理解的能力。协作工作量扩展提供了一个诊断智能体行为和指导开发以实现更有效交互的视角。


<details>
  <summary>Details</summary>
Motivation: 当前对智能体的评估方法未能充分反映现实世界问题的迭代和协作本质，以及人类目标的不确定性和演变性。因此，需要一种新的评估方法，关注智能体与人类的协作能力，而不仅仅是任务的最终完成情况。

Method: 提出协作工作量扩展（collaborative effort scaling）框架，用于衡量智能体的效用如何随着用户参与度的增加而增长。通过案例研究和模拟评估来展示该框架的应用。

Result: 当前最先进的智能体在多轮、真实世界的协作场景中表现不佳，表明它们在维持用户参与和支持用户理解方面存在不足。协作工作量扩展框架能够诊断这些不足。

Conclusion: 当前的智能体评估方法需要从一次性任务完成转向关注协作能力。协作工作量扩展框架为开发更有效的协作智能体提供了指导，强调了在智能体设计中增加维持参与和脚手架式用户理解能力的重要性。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [140] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: DiagramEval是一个新颖的评估指标，用于评估LLM生成的演示图，它将图表概念化为图，使用节点对齐和路径对齐来评估图表质量。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的图表复杂且具有多模态性质，因此缺乏足够区分度和可解释性的评估指标。

Method: DiagramEval将图表视为图，将文本元素作为节点，它们之间的连接作为有向边，并使用节点对齐和路径对齐这两组新指标来评估图表质量。

Result: 首次有效地评估了最先进的LLM在近期研究文献中生成的图表，并定量证明了该指标的有效性。

Conclusion: DiagramEval的增强可解释性为LLM生成的图表特征提供了有价值的见解。

Abstract: Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.

</details>


### [141] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: 现有事后归因方法在抽取式问答中表现良好，但在多跳、摘要和半抽取式问答中存在不足。本文提出将事后归因重构为推理问题，并将答案分解为与特定上下文相关的组成单元。通过DecompTune方法，对模型进行后训练，使其生成答案分解作为中间推理步骤，从而提高归因质量。


<details>
  <summary>Details</summary>
Motivation: 现有事后归因方法在处理需要综合多个段落信息的复杂问答场景（如多跳、摘要和半抽取式问答）时效果不佳，而这些场景在信任度至关重要的长文档问答中很常见。

Method: 本文提出将事后归因视为一个推理问题，其中答案被分解为与特定上下文相关的组成单元。作者首先展示了提示模型生成此类分解和归因可以提高性能。在此基础上，引入了DecompTune，一种后训练方法，通过两阶段SFT + GRPO流程和特定任务的奖励，对Qwen-2.5模型进行微调，使其能够生成答案分解作为中间推理步骤。

Result: DecompTune在复杂问答任务上进行了广泛的实验和消融研究，结果表明该方法显著提高了归因质量，优于先前的方法，并能匹配或超越最先进的模型。

Conclusion: DecompTune通过将事后归因转化为包含中间推理步骤的推理问题，有效解决了现有方法在复杂问答场景中的局限性，显著提高了长文档问答的归因质量和可信度。

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


### [142] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: Gaperon是一个完全开源的法语-英语-编码语言模型套件，旨在提高大规模模型训练的透明度和可复现性。研究发现，语言质量过滤会提高文本流畅度和连贯性，但会降低基准测试性能，而后期故意污染（在包含测试集的数据混合上继续训练）可以恢复有竞争力的分数，同时仅对生成质量造成合理损害。该研究还引入了无害的数据中毒，为安全研究提供了一个现实的测试平台。


<details>
  <summary>Details</summary>
Motivation: Gaperon套件的发布旨在推进大规模模型训练的透明度和可复现性，并深入研究数据过滤、污染与模型在基准测试和生成任务表现之间的相互作用。

Method: Gaperon套件包含1.5B、8B和24B参数的模型，在2-4万亿个标记上进行训练，并发布了完整的训练流程，包括经过神经网络质量分类器过滤的法语和英语数据集、高效的数据整理和训练框架以及大量的中间检查点。通过这些资源，研究了数据过滤和污染对模型性能的影响。

Result: 研究发现，过滤语言质量会提高文本的流畅度和连贯性，但会损害基准测试性能。后期故意污染（在包含测试集的数据混合上继续训练）可以恢复有竞争力的分数，同时对生成质量的影响不大。神经网络过滤可能无意中加剧基准测试泄露。此外，还引入了无害的数据中毒，为安全研究提供测试平台。

Conclusion: Gaperon套件通过公开所有模型、数据集、代码和检查点，为探索多语言语言模型开发中数据整理、评估、安全和开放性之间的权衡提供了可复现的基础。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [143] [Blockage-Aware Multi-RIS WSR Maximization via Per-RIS Indexed Synchronization Sequences and Closed-Form Riemannian Updates](https://arxiv.org/abs/2510.24723)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: RIS链接可能被阻塞，但之前的大部分工作都假设RIS可用。我们提出了一种端到端的阻塞感知多RIS加权和速率（WSR）优化框架，通过发送同步信号，让用户识别被阻塞的面板，然后通过闭式黎曼相位对齐（CRPA）算法优化BS预编码器和RIS相位，以提高WSR和收敛性。


<details>
  <summary>Details</summary>
Motivation: RIS链接可能被阻塞，但之前的大部分工作都假设RIS可用。

Method: 通过发送短的每RIS索引同步信号，使每个用户能够通过简单的能量检测测试来识别被阻塞的面板。基于检测到的可行集，通过闭式黎曼相位对齐（CRPA）算法联合优化BS预编码器和RIS相位。

Result: 仿真结果验证了可靠的阻塞检测以及与现有基线相比显著的WSR和收敛性增益。

Conclusion: 所提出的框架能够有效地处理RIS阻塞问题，并通过CRPA算法实现了WSR和收敛性的提升。

Abstract: Millimeter-wave (mmWave) multi-user MIMO systems are highly vulnerable to
blockage, and reconfigurable intelligent surfaces (RIS) have been proposed as a
remedy. However, RIS links may themselves be blocked, while most prior works
assume ideal RIS availability. We propose an end-to-end blockage-aware
multi-RIS weighted sum-rate (WSR) optimization framework. The BS transmits
short per-RIS indexed synchronization signals, enabling each user to identify
blocked panels through a simple energy detection test. Based on the detected
feasible sets, we jointly optimize the BS precoder and RIS phases via a
Closed-form Riemannian Phase Alignment (CRPA) algorithm. CRPA provides
unit-modulus-preserving closed-form updates, requiring no projection or line
search, and ensures monotone ascent. Simulations validate reliable blockage
detection and notable WSR and convergence gains over existing baselines.

</details>


### [144] [Constructive Lyapunov Functions via Topology-Preserving Neural Networks](https://arxiv.org/abs/2510.24730)
*Jaehong Oh*

Main category: eess.SY

TL;DR: ONN在收敛速度、边效率和计算复杂度方面达到最优性能，并在大规模语义网络和Transformer模型上得到实证验证，同时与最优控制、信息几何、拓扑数据分析等多个领域建立联系。


<details>
  <summary>Details</summary>
Motivation: 将Massera的抽象存在定理转化为具有可证明保证的具体、可扩展算法，为神经网络、机器人和分布式系统中的稳定性分析开辟道路。

Method: 通过理论证明ONN在收敛速度（$\mu \propto \lambda_2$）、边效率（$E = N$ for minimal connectivity $k = 2$）和计算复杂度（$O(N d^2)$）方面达到最优性能。通过将ORTSF集成到Transformer模型中来验证其有效性。

Result: ONN在3M节点语义网络上实现了99.75%的性能提升，并实现了指数级收敛（$\\mu = 3.2 \times 10^{-4}$）和拓扑保持。ORTSF集成到Transformer模型中在WikiText-103数据集上实现了14.7%的困惑度降低和2.3倍的收敛速度提升。

Conclusion: ONN是一种具有理论保证和实际效果的算法，可以为神经网络、机器人和分布式系统中的稳定性分析提供新的方法。

Abstract: We prove that ONN achieves order-optimal performance on convergence rate
($\mu \propto \lambda_2$), edge efficiency ($E = N$ for minimal connectivity $k
= 2$), and computational complexity ($O(N d^2)$). Empirical validation on
3M-node semantic networks demonstrates 99.75\% improvement over baseline
methods, confirming exponential convergence ($\mu = 3.2 \times 10^{-4}$) and
topology preservation. ORTSF integration into transformers achieves 14.7\%
perplexity reduction and 2.3 faster convergence on WikiText-103. We establish
deep connections to optimal control (Hamilton-Jacobi-Bellman), information
geometry (Fisher-efficient natural gradient), topological data analysis
(persistent homology computation in $O(KN)$), discrete geometry (Ricci flow),
and category theory (adjoint functors). This work transforms Massera's abstract
existence theorem into a concrete, scalable algorithm with provable guarantees,
opening pathways for constructive stability analysis in neural networks,
robotics, and distributed systems.

</details>


### [145] [Principal and Combination Parametric Resonances of an Electromagnetically Suspended Vehicle subject to Base Excitation](https://arxiv.org/abs/2510.24756)
*Jithu Paul,Karel N. van Dalen,Andrei B. Faragau,Rens J. van Leijden,Biagio Carboni,Andrei V. Metrikine*

Main category: eess.SY

TL;DR: 此论文研究了电磁悬浮车辆在周期性激励下的动力学稳定性，该车辆在超回路和磁悬浮列车系统中很常见。车辆的振动幅值可能与外部激励相当，导致系统对小幅振荡非常敏感。该模型将车辆简化为三自由度模型，通过两个相同的电磁执行器悬挂在振荡的刚性支架上。


<details>
  <summary>Details</summary>
Motivation: 研究电磁悬浮车辆在周期性激励下（如表面不平整或外部噪声引起的支撑振动）的动力学稳定性问题，因为车辆与支撑之间的狭窄间隙可能导致其对小幅振荡非常敏感。

Method: 推导了包含非线性电磁力和考虑了空气间隙PD控制策略的电磁铁的基尔霍夫定律的控制方程。通过对由表面振动引起的稳态进行线性化，得到一个具有时间周期性系数的系统。使用扩展的Hills法和Floquet理论，分析了主参数共振和组合参数共振，并进行了数值验证。

Result: 获得了控制增益参数空间中的椭圆稳定性边界。研究了系统参数对这些边界的影响。对于主参数共振，两个椭圆的尺寸比为三比一；对于组合参数共振，尺寸比为十四比一。当所有椭圆同时存在时，一个与组合参数共振相关的椭圆最大。

Conclusion: 论文成功地确定了电磁悬浮车辆在周期性激励下的稳定性边界，并量化了不同类型参数共振的影响。研究结果对于设计更稳定可靠的超回路和磁悬浮列车系统具有指导意义。

Abstract: This paper investigates the dynamic stability of an electromagnetically
suspended vehicle, encountered in Hyperloop and Maglev systems, subject to
periodic excitations caused by surface irregularities or vibration of the
support induced by external noise. The narrow clearance between the vehicle and
the support can make it highly sensitive to small oscillations, since the
admissible amplitudes of the vehicle oscillations can be comparable to external
excitation amplitude. The vehicle is modelled as a three-degree-of-freedom
model where the vehicle is suspended via two identical electromagnetic
actuators from a rigid support that oscillates. The governing equations are
derived using force and torque balances, incorporating nonlinear
electromagnetic forces, and Kirchhoffs law for the electromagnets with PD
control strategy on the airgap. The equations of motion are linearized around
the steady state induced by the surface oscillation, yielding a system with
time-periodic coefficients. We analytically explore both principal and
combination parametric resonances using an extended Hills method, and Floquet
theory is used for numerical validation. The stability boundaries are obtained
as ellipses in control gain parameter space, and the influence of system
parameters on these boundaries is characterized. For the principal parametric
resonance, the ratio of the sizes of the two obtained ellipses is three to one,
whereas for the combination parametric resonance, the ratio is fourteen to one.
When all ellipses are simultaneously present, one of the ellipses associated
with the combination parametric resonance is the largest.

</details>


### [146] [Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification](https://arxiv.org/abs/2510.24757)
*Ahmet Eren Sertbaş,Tufan Kumbasar*

Main category: eess.SY

TL;DR: 提出了一种新颖的基于线性参数变化（LPV）神经网络的状态空间（NN-SS）模型，用于精确识别非线性系统。该模型能够同时学习潜在状态和内部调度变量，并通过基于Schur分解的方法保证了状态转移矩阵的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统识别方法难以同时捕捉潜在动态并保持稳定性，因此需要一种能够学习潜在状态和调度变量并保证系统稳定性的新方法。

Method: 提出了一种稳定的、由神经网络参数化的LPV状态空间模型（NN-SS）。状态转移矩阵通过一个神经网络生成，该网络使用学习到的调度变量，并通过基于Schur分解的方法保证其稳定性。该模型架构包含一个用于初始状态估计的编码器和一个用于构建完整调度相关系统矩阵的状态空间表示网络。为训练NN-SS开发了一个框架，集成了多步预测损失和状态一致性正则化项。

Result: 在基准非线性系统上的评估结果表明，该模型在性能上能与经典子空间识别方法和近期梯度下降方法相媲美，甚至超越它们。

Conclusion: 基于稳定性约束的线性参数变化神经网络在识别复杂非线性系统方面具有巨大潜力，能够提供一种可扩展且可靠的建模框架。

Abstract: Accurate modeling of nonlinear systems is essential for reliable control, yet
conventional identification methods often struggle to capture latent dynamics
while maintaining stability. We propose a \textit{stable-by-design LPV neural
network-based state-space} (NN-SS) model that simultaneously learns latent
states and internal scheduling variables directly from data. The
state-transition matrix, generated by a neural network using the learned
scheduling variables, is guaranteed to be stable through a Schur-based
parameterization. The architecture combines an encoder for initial state
estimation with a state-space representer network that constructs the full set
of scheduling-dependent system matrices. For training the NN-SS, we develop a
framework that integrates multi-step prediction losses with a state-consistency
regularization term, ensuring robustness against drift and improving
long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark
nonlinear systems, and the results demonstrate that the model consistently
matches or surpasses classical subspace identification methods and recent
gradient-based approaches. These findings highlight the potential of
stability-constrained neural LPV identification as a scalable and reliable
framework for modeling complex nonlinear systems.

</details>


### [147] [A Digital Twin Framework for Decision-Support and Optimization of EV Charging Infrastructure in Localized Urban Systems](https://arxiv.org/abs/2510.24758)
*Linh Do-Bui-Khanh,Thanh H. Nguyen,Nghi Huynh Quang,Doanh Nguyen-Ngoc,Laurent El Ghaoui*

Main category: eess.SY

TL;DR: 该研究提出了一个数字孪生框架，通过集成基于代理的决策支持和嵌入式优化，动态模拟电动汽车充电行为、基础设施布局和政策响应，以优化城市充电基础设施。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车在城市环境中的普及，优化充电基础设施对于平衡用户满意度、能源效率和财务可行性至关重要。

Method: 提出一个数字孪生框架，结合了基于代理的决策支持和嵌入式优化，用于动态模拟电动汽车充电行为、基础设施布局和政策响应。以越南河内一个大学校园为例，评估了运营策略、电动汽车充电站配置和可再生能源。

Result: 模拟显示，实时通知新的可用充电桩可提高用户满意度；汽油禁令和闲置费可提高充电桩使用率，而不会增加复杂性。嵌入式元启发式优化能高效地识别快充（30kW）和标准（11kW）太阳能充电桩的近优组合，以平衡能源性能、盈利能力和需求。季节性分析显示，太阳能效率在10月至3月下降20%，风力发电贡献不足5%。

Conclusion: 该数字孪生平台为电动汽车基础设施规划提供了一个灵活、计算驱动且可扩展的解决方案，其模块化设计可轻松从局部推广到整个城市。

Abstract: As Electric Vehicle (EV) adoption accelerates in urban environments,
optimizing charging infrastructure is vital for balancing user satisfaction,
energy efficiency, and financial viability. This study advances beyond static
models by proposing a digital twin framework that integrates agent-based
decision support with embedded optimization to dynamically simulate EV charging
behaviors, infrastructure layouts, and policy responses across scenarios.
Applied to a localized urban site (a university campus) in Hanoi, Vietnam, the
model evaluates operational policies, EV station configurations, and renewable
energy sources. The interactive dashboard enables seasonal analysis, revealing
a 20% drop in solar efficiency from October to March, with wind power
contributing under 5% of demand, highlighting the need for adaptive energy
management. Simulations show that real-time notifications of newly available
charging slots improve user satisfaction, while gasoline bans and idle fees
enhance slot turnover with minimal added complexity. Embedded metaheuristic
optimization identifies near-optimal mixes of fast (30kW) and standard (11kW)
solar-powered chargers, balancing energy performance, profitability, and demand
with high computational efficiency. This digital twin provides a flexible,
computation-driven platform for EV infrastructure planning, with a
transferable, modular design that enables seamless scaling from localized to
city-wide urban contexts.

</details>


### [148] [Decentralized Merging Control of Connected and Automated Vehicles to Enhance Safety and Energy Efficiency using Control Barrier Functions](https://arxiv.org/abs/2510.24871)
*Shreshta Rajakumar Deshpande,Mrdjan Jankovic*

Main category: eess.SY

TL;DR: 该论文提出了一种基于去中心化控制障碍函数（CBF）的高速公路车辆汇合方法，实现了安全、节能的汇合。


<details>
  <summary>Details</summary>
Motivation: 解决高速公路车辆汇合场景下的安全性和能源效率问题，并提出一种无中央协调的去中心化方法。

Method: 采用去中心化CBF方法，让每辆车在控制区内与其他车辆协商，自主决策以实现安全、节能的汇合。利用预测-修正（predictor-corrector）循环来处理协商和分歧，无需预设车辆顺序或优先级。

Result: 蒙特卡洛模拟结果显示，该方法在系统整体能源效率和交通流量方面优于先到先服务（FIFO）方法，并且该去中心化控制器比中心化控制器具有更强的鲁棒性。

Conclusion: 所提出的去中心化CBF方法能够有效实现高速公路车辆汇合，并展现出优于现有方法的性能和鲁棒性。

Abstract: This paper presents a decentralized Control Barrier Function (CBF) based
approach for highway merging of Connected and Automated Vehicles (CAVs). In
this control algorithm, each "host" vehicle negotiates with other agents in a
control zone of the highway network, and enacts its own action, to perform safe
and energy-efficient merge maneuvers. It uses predictor-corrector loops within
the robust CBF setting for negotiation and to reconcile disagreements that may
arise. There is no explicit order of vehicles and no priority. A notable
feature is absence of gridlocks due to instability of the inter-agent system.
Results from Monte Carlo simulations show significant improvement in the
system-wide energy efficiency and traffic flow compared to a first-in-first-out
approach, as well as enhanced robustness of the proposed decentralized
controller compared to its centralized counterpart.

</details>


### [149] [Delay Tolerant Control for Autonomous Driving Using CDOB](https://arxiv.org/abs/2510.24898)
*Xincheng Cao,Haochong Chen,Levent Guvenc,Bilin Aksun-Guvenc*

Main category: eess.SY

TL;DR: 该论文提出了一种通信延迟容忍控制器（CDOB）来解决自动驾驶汽车路径跟踪中的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 通信延迟和计算延迟会严重影响PID或DOB等传统控制器在自动驾驶汽车路径跟踪中的性能，特别是当延迟未知或变化时。

Method: 提出了一种通信延迟容忍控制器（CDOB）框架，用于补偿和处理延迟系统中的不确定和变化的通信延迟，以实现精确的轨迹跟踪。

Result: 仿真结果表明，CDOB在各种场景（包括单车道变换、双车道变换以及考虑碰撞避免的弹性带路径）下，即使在各种延迟条件下，也能保持与参考轨迹的精确对齐，并且性能优于传统方法。

Conclusion: 所提出的CDOB框架能够有效补偿通信延迟和计算延迟对自动驾驶汽车路径跟踪的影响，在各种延迟条件下都能保持优异的跟踪精度和鲁棒性，适用于自动驾驶应用。

Abstract: With the rapid growth of autonomous vehicle technologies, effective
path-tracking control has become a critical component in ensuring safety and
efficiency in complex traffic scenarios. When a high level decision making
agent generates a collision free path, a robust low level controller is
required to precisely follow this trajectory. However, connected autonomous
vehicles (CAV) are inherently affected by communication delays and computation
delays, which significantly degrade the performance of conventional controllers
such as PID or other more advanced controllers like disturbance observers
(DOB). While DOB-based designs have shown effectiveness in rejecting
disturbances under nominal conditions, their performance deteriorates
considerably in the presence of unknown time delays. To address this challenge,
this paper proposes a delay-tolerant communication disturbance observer (CDOB)
framework for path-tracking control in delayed systems. The proposed CDOB
compensates for the adverse effects of time delays, maintaining accurate
trajectory tracking even under uncertain and varying delay conditions. It is
shown through a simulation study that the proposed control architecture
maintains close alignment with the reference trajectory across various
scenarios, including single lane change, double-= lane change, and Elastic Band
generated collision avoidance paths under various time delays. Simulation
results further demonstrate that the proposed method outperforms conventional
approaches in both tracking accuracy and delay robustness, making it well
suited for autonomous driving applications.

</details>


### [150] [A Hamilton-Jacobi Reachability Framework with Soft Constraints for Safety-Critical Systems](https://arxiv.org/abs/2510.24933)
*Chams Eddine Mballo,Donggun Lee,Claire J. Tomlin*

Main category: eess.SY

TL;DR: 该研究提出了一个结合硬约束和软约束的框架，用于安全关键系统的可达性分析，并设计了相应的数值算法。


<details>
  <summary>Details</summary>
Motivation: 传统可达性方法强制执行硬约束，可能导致在复杂场景下过于保守或不可行。然而，许多实际约束（如电池电量、速度限制、舒适度）本质上是软的，允许在一定安全裕度内临时违反，尽管会产生额外成本。

Method: 提出了一种新颖的软约束可达性框架，该框架扩展了Hamilton-Jacobi可达性分析。框架包含两个主要部分：(i) 带有辅助预算状态的增强状态模型，用于跟踪软约束违反情况；(ii) 对相关的不连续Hamilton-Jacobi价值函数的基于正则化的近似。

Result: 通过一个简单质点模型和固定翼飞机的紧急下降着陆的数值示例，验证了该框架在同时处理硬约束和软约束方面的有效性，并考虑了风扰动。

Conclusion: 所提出的框架能够有效地在最坏扰动下，在软约束违反预算内，保证系统从软约束可达集中安全地到达目标集，并能同时管理硬约束和软约束。

Abstract: Traditional reachability methods provide formal guarantees of safety under
bounded disturbances. However, they strictly enforce state constraints as
inviolable, which can result in overly conservative or infeasible solutions in
complex operational scenarios. Many constraints encountered in practice, such
as bounds on battery state of charge in electric vehicles, recommended speed
envelopes, and comfort constraints in passenger-carrying vehicles, are
inherently soft. Soft constraints allow temporary violations within predefined
safety margins to accommodate uncertainty and competing operational demands,
albeit at a cost such as increased wear or higher operational expenses. This
paper introduces a novel soft-constrained reachability framework that extends
Hamilton-Jacobi reachability analysis for the formal verification of
safety-critical systems subject to both hard and soft constraints.
Specifically, the framework characterizes a subset of the state space, referred
to as the soft-constrained reach-avoid set, from which the system is guaranteed
to reach a desired set safely, under worst-case disturbances, while ensuring
that cumulative soft-constraint violations remain within a user-specified
budget. The framework comprises two principal components: (i) an
augmented-state model with an auxiliary budget state that tracks
soft-constraint violations, and (ii) a regularization-based approximation of
the discontinuous Hamilton-Jacobi value function associated with the
reach-avoid differential game studied herein. The effectiveness of the proposed
framework is demonstrated through numerical examples involving the landing of a
simple point-mass model and a fixed-wing aircraft executing an emergency
descent, both under wind disturbances. The simulation results validate the
framework's ability to simultaneously manage both hard and soft constraints in
safety-critical settings

</details>


### [151] [Control Synthesis with Reinforcement Learning: A Modeling Perspective](https://arxiv.org/abs/2510.25063)
*Nikki Xu,Hien Tran*

Main category: eess.SY

TL;DR: 控制器对模型不匹配很敏感，仿真环境中的模型不准确不适用于物理部署。


<details>
  <summary>Details</summary>
Motivation: 设计带有强化学习的控制器时，需要考虑模型不匹配的问题，因为在仿真环境中训练的控制器可能无法很好地适应物理现实。

Method: 通过敏感性分析和经验吸引区域估计来量化和可视化控制器对模型不匹配的鲁棒性。

Result: 使用准确模型设计的控制器在物理实验中表现出鲁棒性，而使用不准确模型设计的控制器在仿真中表现良好但在物理实验中失败。

Conclusion: 确保用于训练强化学习控制器的模型准确性对于在物理环境中成功部署至关重要。

Abstract: Controllers designed with reinforcement learning can be sensitive to model
mismatch. We demonstrate that designing such controllers in a virtual
simulation environment with an inaccurate model is not suitable for deployment
in a physical setup. Controllers designed using an accurate model is robust
against disturbance and small mismatch between the physical setup and the
mathematical model derived from first principles; while a poor model results in
a controller that performs well in simulation but fails in physical
experiments. Sensitivity analysis is used to justify these discrepancies and an
empirical region of attraction estimation help us visualize their robustness.

</details>


### [152] [Stochastic Long-Term Joint Decarbonization Planning for Power Systems and Data Centers: A Case Study in PJM](https://arxiv.org/abs/2510.25118)
*Zhentong Shao,Nanpeng Yu,Daniel Wong*

Main category: eess.SY

TL;DR: 该研究提出了一个动态联合规划框架，对数据中心和电力系统进行长达15年的联合规划，以解决数据中心快速增长带来的能源消耗和碳排放问题。


<details>
  <summary>Details</summary>
Motivation: 数据中心和云服务的快速发展导致能源消耗和碳排放增加，现有研究未能充分考虑静态电力系统、仅关注运行排放以及缺乏联合优化。

Method: 提出一个动态联合规划框架，对数据中心（选址、容量、类型）和电力系统（发电扩展、储能部署、退役）进行联合规划，并考虑了运行和体现的排放。使用大规模两阶段随机规划模型，并通过增强的Benders分解方法求解，以处理多尺度不确定性。

Result: 将该框架应用于PJM互连系统，结果表明该系统可支持高达55 GW的峰值数据中心需求，其中弗吉尼亚（DOM）和北部伊利诺伊（ComEd）是最优选址。与非联合规划相比，该框架可降低投资成本12.6%，运行成本8.25%，排放量5.63%。考虑生命周期排放可使可再生能源部署增加25.5%。

Conclusion: 动态联合规划框架能够有效优化数据中心和电力系统的发展，实现成本和排放的双重降低。在深度脱碳过程中，体现的碳排放是一个关键因素，应予以考虑。

Abstract: With the rapid growth of artificial intelligence (AI) and cloud services,
data centers have become critical infrastructures driving digital economies,
with increasing energy demand heightening concerns over electricity use and
carbon emissions, emphasizing the need for carbon-aware infrastructure
planning. Most studies assume static power systems, focus only on operational
emissions, and overlook co-optimization. This paper proposes a dynamic joint
planning framework that co-optimizes long-term data center and power system
development over 15 years. The model determines siting, capacity, and type of
data centers alongside power generation expansion, storage deployment, and
retirements, accounting for both operational and embodied emissions. To handle
multi-scale uncertainty, a large-scale two-stage stochastic program is
formulated and solved via an enhanced Benders decomposition. Applied to the PJM
Interconnection, with curated datasets released on GitHub, results show the
system can support up to 55 GW peak data center demand, with Virginia (DOM) and
Northern Illinois (ComEd) as optimal hosts. Compared to non-joint planning, the
framework cuts investment cost by 12.6%, operational cost by 8.25%, and
emissions by 5.63%. Including lifecycle emissions further raises renewable
deployment by 25.5%, highlighting embodied carbon's role in deeper
decarbonization.

</details>


### [153] [The Waterbed Effect on Quasiperiodic Disturbance Observer: Avoidance of Sensitivity Tradeoff with Time Delays](https://arxiv.org/abs/2510.25131)
*Hisayoshi Muramatsu*

Main category: eess.SY

TL;DR: 该论文提出了一个准周期扰动观测器，使用时延来补偿准周期扰动，并推导了其连续时间和离散时间的 Bode 类灵敏度积分，解释了如何避免灵敏度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的准周期扰动观测器（使用时延）具有非有理开环传递函数，不满足现有的 Bode 灵敏度积分的假设。该论文旨在提供适用于这些观测器的 Bode 类灵敏度积分，并阐明其避免灵敏度权衡的机制。

Method: 通过推导，为使用时延的准周期扰动观测器提供了连续时间和离散时间的 Bode 类灵敏度积分，并分析了其灵敏度权衡。

Result: 成功推导了所提出的准周期扰动观测器的 Bode 类灵敏度积分，并解释了如何利用时延来避免灵敏度权衡，同时在宽带范围内抑制谐波，而不会放大非周期扰动或移动谐波抑制频率。

Conclusion: 该研究为理解和设计基于时延的准周期扰动观测器提供了新的理论工具，有助于在实际系统中实现更优的扰动抑制。

Abstract: In linear time-invariant systems, the sensitivity function to disturbances is
designed under a sensitivity tradeoff known as the waterbed effect. To
compensate for a quasiperiodic disturbance, a quasiperiodic disturbance
observer using time delays was proposed. Its sensitivity function avoids the
sensitivity tradeoff, achieving wideband harmonic suppression without
amplifying aperiodic disturbances or shifting harmonic suppression frequencies.
However, its open-loop transfer function is not rational and does not satisfy
the assumptions of existing Bode sensitivity integrals due to its time delays.
This paper provides Bode-like sensitivity integrals for the quasiperiodic
disturbance observer in both continuous-time and discrete-time representations
and clarifies the avoided sensitivity tradeoff with time delays.

</details>


### [154] [Silicon-based Josephson junction field-effect transistors enabling cryogenic logic and quantum technologies](https://arxiv.org/abs/2510.25208)
*Yusheng Xiong,Kaveh Delfanazari*

Main category: eess.SY

TL;DR: JJFETs 结合了超导体和场效应晶体管的优点，在低温下具有超低功耗和高速度的潜力，可能成为下一代低温逻辑和量子电子系统的基础。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律的推进和半导体电子学在低温条件下接近性能极限，需要创新的器件范式来实现超低功耗和高功能。

Method: 本综述追溯了从约瑟夫森结到场效应晶体管的演变，强调了现代器件可扩展性背后的结构和功能创新。分析了在硅、砷化镓和砷化镓铟等衬底上制造的 JJFET 的性能和材料兼容性，并评估了它们的开关动力学和材料兼容性。

Result: 重点关注了作为 JJFET 架构的有源核心的超导体-硅-超导体约瑟夫森结。

Conclusion: JJFETs 有潜力弥合传统半导体电子学与低温逻辑和量子电路的差距，从而在温度域实现节能和高相干信号处理。

Abstract: The continuous miniaturisation of metal-oxide-semiconductor field-effect
transistors (MOSFETs) from long- to short-channel architectures has advanced
beyond the predictions of Moore's Law. Continued advances in semiconductor
electronics, even near current scaling and performance boundaries under
cryogenic conditions, are driving the development of innovative device
paradigms that enable ultra-low-power and high-speed functionality. Among
emerging candidates, the Josephson Junction Field-Effect Transistor (JJFET or
JoFET) provides an alternative by integrating superconducting source and drain
electrodes for efficient, phase-coherent operation at ultra-low temperatures.
These hybrid devices have the potential to bridge conventional semiconductor
electronics with cryogenic logic and quantum circuits, enabling
energy-efficient and high-coherence signal processing across temperature
domains. This review traces the evolution from Josephson junctions to
field-effect transistors, emphasising the structural and functional innovations
that underpin modern device scalability. The performance and material
compatibility of JJFETs fabricated on Si, GaAs, and InGaAs substrates are
analysed, alongside an assessment of their switching dynamics and material
compatibility. Particular attention is given to
superconductor-silicon-superconductor Josephson junctions as the active core of
JJFET architectures. By unfolding more than four decades of experimental
progress, this work highlights the promise of JJFETs as foundational building
blocks for next-generation cryogenic logic and quantum electronic systems.

</details>


### [155] [Shared Control for Vehicle Lane-Changing with Uncertain Driver Behaviors](https://arxiv.org/abs/2510.25284)
*Jiamin Wu,Chenguang Zhao,Huan Yu*

Main category: eess.SY

TL;DR: 该论文提出了一种结合人类和自动驾驶的换道控制框架，以应对人类驾驶行为的不确定性，并保持交通流的稳定性。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶员在换道过程中行为具有不确定性，可能导致交通流紊乱和稳定性下降。本研究旨在解决这一问题，同时保留驾驶员对车辆的控制权。

Method: 提出了一种基于马尔可夫跳跃过程的人类驾驶行为模型，并设计了一个名义稳定控制器来保证随机L2弦稳定性。在此基础上，进一步开发了一个最小干预控制器（MIC），在保证可接受稳定性的同时，限制自动驾驶的介入程度。

Result: 仿真结果表明，名义控制器可以减少速度扰动并缩短换道时间。MIC则在进一步减少自动驾驶介入和提高舒适性的同时，对稳定性和效率造成适度影响。在SAE Level 2车辆的TGSIM数据集上验证，MIC能够实现比Level 2控制更早的换道，同时在略微牺牲稳定性的情况下保留了驾驶员的控制权。

Conclusion: 研究结果表明，人类-自动驾驶共享控制策略在稳定性、效率和驾驶员接受度之间取得了良好的平衡，为解决换道过程中的挑战提供了有效的解决方案。

Abstract: Lane changes are common yet challenging driving maneuvers that require
continuous decision-making and dynamic interaction with surrounding vehicles.
Relying solely on human drivers for lane-changing can lead to traffic
disturbances due to the stochastic nature of human behavior and its variability
under different task demands. Such uncertainties may significantly degrade
traffic string stability, which is critical for suppressing disturbance
propagation and ensuring smooth merging of the lane-changing vehicles. This
paper presents a human-automation shared lane-changing control framework that
preserves driver authority while allowing automated assistance to achieve
stable maneuvers in the presence of driver's behavioral uncertainty. Human
driving behavior is modeled as a Markov jump process with transitions driven by
task difficulty, providing a tractable representation of stochastic state
switching. Based on this model, we first design a nominal stabilizing
controller that guarantees stochastic ${L}_2$ string stability under imperfect
mode estimation. To further balance performance and automated effort, we then
develop a Minimal Intervention Controller (MIC) that retains acceptable
stability while limiting automation. Simulations using lane-changing data from
the NGSIM dataset verify that the nominal controller reduces speed
perturbations and shorten lane-changing time, while the MIC further reduces
automated effort and enhances comfort but with moderate stability and
efficiency loss. Validations on the TGSIM dataset with SAE Level 2 vehicles
show that the MIC enables earlier lane changes than Level 2 control while
preserving driver authority with a slight stability compromise. These findings
highlight the potential of shared control strategies to balance stability,
efficiency, and driver acceptance.

</details>


### [156] [Data-Enabled Predictive Control and Guidance for Autonomous Underwater Vehicles](https://arxiv.org/abs/2510.25309)
*Sebastian Zieglmeier,Mathias Hudoba de Badyn,Narada D. Warakagoda,Thomas R. Krogstad,Paal Engelstad*

Main category: eess.SY

TL;DR: 该论文提出了一种基于数据驱动预测控制（DeePC）的全数据驱动自主水下航行器（AUV）控制框架，无需显式流体动力学模型，通过利用测量输入-输出来预测和优化未来系统行为。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种无需显式流体动力学建模的全数据驱动AUV控制框架。

Method: 采用了经典DeePC进行航向控制，并提出了一种级联DeePC架构用于深度调节，结合了环频率分离以处理不同的输入输出动态模式。同时，将自适应视线（LOS）算法扩展到预测形式并与DeePC集成，以实现三维航路点路径跟踪。

Result: 在REMUS 100 AUV的仿真中，DeePC方法在跟踪性能和鲁棒性方面优于经典的PI/PID控制，尤其是在海洋流干扰和非线性操作条件下，同时显著减少了建模工作量。

Conclusion: DeePC为AUV控制提供了一种有效且无需显式建模的解决方案，在跟踪性能和鲁棒性方面表现出色。

Abstract: This paper presents a fully data-driven control framework for autonomous
underwater vehicles (AUVs) based on Data-Enabled Predictive Control (DeePC).
The approach eliminates the need for explicit hydrodynamic modeling by
exploiting measured input-output data to predict and optimize future system
behavior. Classic DeePC was employed in the heading control, while a cascaded
DeePC architecture is proposed for depth regulation, incorporating a
loop-frequency separation to handle the different dynamic modes of input and
output. For 3-D waypoint path following, the Adaptive Line-of-Sight algorithm
is extended to a predictive formulation and integrated with DeePC. All methods
are validated in extensive simulation on the REMUS 100 AUV and compared with
classical PI/PID control. The results demonstrate superior tracking performance
and robustness of DeePC under ocean-current disturbances and nonlinear
operating conditions, while significantly reducing modeling effort.

</details>


### [157] [Tight Collision Avoidance for Stochastic Optimal Control: with Applications in Learning-based, Interactive Motion Planning](https://arxiv.org/abs/2510.25324)
*Erik Börve,Nikolce Murgovski,Leo Laine*

Main category: eess.SY

TL;DR: 该论文提出了一个随机最优控制框架，用于解决自动驾驶汽车在密集、交互式交通场景中的轨迹规划问题，同时处理人类驾驶员行为的不确定性和碰撞避免的非凸约束，并且避免了过于保守的近似。


<details>
  <summary>Details</summary>
Motivation: 在密集、交互式交通场景中，由于人类驾驶员行为的不确定性和碰撞避免约束的非凸性，自动驾驶汽车的轨迹规划面临重大挑战。

Method: 通过将人类驾驶员行为建模为马尔可夫决策过程，并提出一种通过在紧凑集之间施加正距离约束来处理非凸车辆形状碰撞避免的方法，构建了一个随机最优控制框架。该框架研究了三种概率约束的制定方式，并引入了对非凸距离约束和概率约束的紧密、连续可微的重新表述，以确保计算的可行性。

Result: 通过对非结构化交叉路口通行和密集交通中高速公路变道这两个具有挑战性的交互场景的仿真研究，验证了所提出方法的效果。

Conclusion: 所提出的随机最优控制框架能够有效地解决自动驾驶汽车在密集、交互式交通场景中的轨迹规划问题，同时处理了不确定性和非凸约束。

Abstract: Trajectory planning in dense, interactive traffic scenarios presents
significant challenges for autonomous vehicles, primarily due to the
uncertainty of human driver behavior and the non-convex nature of collision
avoidance constraints. This paper introduces a stochastic optimal control
framework to address these issues simultaneously, without excessively
conservative approximations. We opt to model human driver decisions as a Markov
Decision Process and propose a method for handling collision avoidance between
non-convex vehicle shapes by imposing a positive distance constraint between
compact sets. In this framework, we investigate three alternative chance
constraint formulations. To ensure computational tractability, we introduce
tight, continuously differentiable reformulations of both the non-convex
distance constraints and the chance constraints. The efficacy of our approach
is demonstrated through simulation studies of two challenging interactive
scenarios: an unregulated intersection crossing and a highway lane change in
dense traffic.

</details>


### [158] [Lightweight Federated Learning in Mobile Edge Computing with Statistical and Device Heterogeneity Awareness](https://arxiv.org/abs/2510.25342)
*Jinghong Tan,Zhichen Zhang,Kun Guo,Tsung-Hui Chang,Tony Q. S. Quek*

Main category: eess.SY

TL;DR: 联邦学习在移动边缘计算中存在通信和计算成本高昂的问题，尤其是在异构环境下。我们提出了一种基于参数解耦的轻量级个性化联邦学习框架，将模型分离为共享和私有子空间，并分别应用梯度稀疏化和模型剪枝，以降低端到端训练成本，并证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习压缩方法在降低单轮成本的同时，可能增加训练轮数，导致总成本升高，尤其是在异构环境下。本研究旨在解决联邦学习在移动边缘计算中面临的高通信和计算成本问题，并适应设备和统计的异构性。

Method: 提出一种基于参数解耦的轻量级个性化联邦学习框架，将模型分为共享和私有子空间。对共享组件应用梯度稀疏化，对私有组件应用模型剪枝。结合理论分析和联合优化，选择稀疏率、剪枝率和带宽以降低训练时间。

Result: 模拟结果表明，该框架实现了更快的收敛速度，显著降低了通信和计算成本，同时仅有极小的精度损失。

Conclusion: 协调和资源感知的个性化方法在资源受限的异构环境中具有显著优势，可以有效降低联邦学习的端到端训练时间，同时保持模型精度。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy, but high communication and computation costs, exacerbated by
statistical and device heterogeneity, limit its practicality in mobile edge
computing. Existing compression methods like sparsification and pruning reduce
per-round costs but may increase training rounds and thus the total training
cost, especially under heterogeneous environments. We propose a lightweight
personalized FL framework built on parameter decoupling, which separates the
model into shared and private subspaces, enabling us to uniquely apply gradient
sparsification to the shared component and model pruning to the private one.
This structural separation confines communication compression to global
knowledge exchange and computation reduction to local personalization,
protecting personalization quality while adapting to heterogeneous client
resources. We theoretically analyze convergence under the combined effects of
sparsification and pruning, revealing a sparsity-pruning trade-off that links
to the iteration complexity. Guided by this analysis, we formulate a joint
optimization that selects per-client sparsity and pruning rates and wireless
bandwidth to reduce end-to-end training time. Simulation results demonstrate
faster convergence and substantial reductions in overall communication and
computation costs with negligible accuracy loss, validating the benefits of
coordinated and resource-aware personalization in resource-constrained
heterogeneous environments.

</details>


### [159] [Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G UAV Corridors](https://arxiv.org/abs/2510.25411)
*Sana Hafeez,Ghulam E Mustafa Abro,Hifza Mustafa*

Main category: eess.SY

TL;DR: 本文提出了一种用于6G网络中无人机通道的量子弹性威胁建模(QRTM)框架，该框架集成了通信和传感(ISAC)，并考虑了可重构智能表面(RIS)和量子计算带来的安全挑战。QRTM使用后量子密码学(PQC)原语(ML-KEM和Falcon)来防御经典、量子就绪和量子辅助的对手，并引入了RIS编码的场景水印来增强安全传感。此外，该框架还通过一个计算复杂度为O(n^2)的调度器优化了保密率、欺骗检测和吞吐量，以实现安全ISAC效用(SIU)。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络中无人机通道的快速部署，需要一种安全、智能驱动的集成传感与通信(ISAC)方法。然而，可重构智能表面(RIS)虽然能提升性能，但也引入了新的安全漏洞，同时量子计算的兴起增加了‘先收割后解密’策略和量子增强欺骗的风险。

Method: 提出了一种量子弹性威胁建模(QRTM)框架，该框架集成了经典、量子就绪和量子辅助的对手模型。该框架利用后量子密码学(PQC)原语（ML-KEM用于密钥建立，Falcon用于认证）来防御这些威胁，并将它们嵌入RIS控制信令和无人机协调中。为了加强安全传感，QRTM引入了RIS编码的场景水印，并通过广义似然比检验(GLRT)进行验证，其检测概率由Marcum Q函数表征。此外，还设计了一个安全ISAC效用(SIU)优化器，在RIS约束下联合优化保密率、欺骗检测和吞吐量，该优化器由一个计算复杂度为O(n^2)的调度器实现。

Result: 使用3GPP Release 19中频城市峡谷模型(7-15 GHz)进行的蒙特卡洛评估显示，欺骗检测概率在1e-3的虚警率下接近0.99，与量子能力对手的保密率保持率超过90%，并且与基线相比，信号-干扰利用率提高了约25%。

Conclusion: 该研究结果为智能城市和非地面网络中无人机通道的可靠、量子弹性的ISAC提供了一条符合标准的可行路径。QRTM框架有效解决了RIS辅助ISAC在6G环境中面临的量子威胁和安全挑战。

Abstract: The rapid deployment of unmanned aerial vehicle (UAV) corridors in
sixth-generation (6G) networks requires safe, intelligence-driven integrated
sensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS)
enhance spectrum efficiency, localisation accuracy, and situational awareness,
while introducing new vulnerabilities. The rise of quantum computing increases
the risks associated with harvest-now-decrypt-later strategies and
quantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling
(QRTM) framework for RIS-assisted ISAC in UAV corridors to address these
challenges. QRTM integrates classical, quantum-ready, and quantum-aided
adversaries, countered using post-quantum cryptographic (PQC) primitives:
ML-KEM for key establishment and Falcon for authentication, both embedded
within RIS control signalling and UAV coordination. To strengthen security
sensing, the framework introduces RIS-coded scene watermarking validated
through a generalised likelihood ratio test (GLRT), with its detection
probability characterised by the Marcum Q function. Furthermore, a Secure ISAC
Utility (SIU) jointly optimises secrecy rate, spoofing detection, and
throughput under RIS constraints, enabled by a scheduler with computational
complexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band
urban-canyon models (7-15 GHz) demonstrate a spoof-detection probability
approaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention
exceeding 90 percent against quantum-capable adversaries, and
signal-interference utilisation improvements of about 25 percent compared with
baselines. These results show a standards-compliant path towards reliable,
quantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial
networks.

</details>


### [160] [A New Neural Network Paradigm for Scalable and Generalizable Stability Analysis of Power Systems](https://arxiv.org/abs/2510.25501)
*Tong Han,Yan Xu,Rui Zhang*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a new neural network (NN) paradigm for scalable and
generalizable stability analysis of power systems. The paradigm consists of two
parts: the neural stability descriptor and the sample-augmented iterative
training scheme. The first part, based on system decomposition, constructs the
object (such as a stability function or condition) for stability analysis as a
scalable aggregation of multiple NNs. These NNs remain fixed across varying
power system structures and parameters, and are repeatedly shared within each
system instance defined by these variations, thereby enabling the
generalization of the neural stability descriptor across a class of power
systems. The second part learns the neural stability descriptor by iteratively
training the NNs with sample augmentation, guided by the tailored
conservativeness-aware loss function. The training set is strategically
constructed to promote the descriptor's generalizability, which is
systematically evaluated by verification and validation during the training
process. Specifically, the proposed NN paradigm is implemented for
large-disturbance stability analysis of the bulk power grid and
small-disturbance stability conditions of the microgrid system. Finally,
numerical studies for the two implementations demonstrate the applicability and
effectiveness of the proposed NN paradigm.

</details>


### [161] [Optimal and Heuristic Approaches for Platooning Systems with Deadlines](https://arxiv.org/abs/2510.25564)
*Thiago S. Gomides,Evangelos Kranakis,Ioannis Lambadaris,Yannis Viniotis,Gennady Shaikhet*

Main category: eess.SY

TL;DR: 高效的卡车编队可以降低运输成本、燃料消耗和排放。然而，由于有交货截止日期，卡车必须在特定的时间窗口内出发。该研究在具有有限容量 L 和截止日期约束 T 的高速公路站点研究了卡车编队的最佳形成和调度。该系统以离散时间运行，每个卡车都有一个截止日期。目标是在利用卡车编队效率的同时，考虑等待成本和截止日期违规。研究人员将该问题制定为马尔可夫决策过程，并分析了 L = 3 的最优策略 $\pi^\star$ 的结构，并将见解扩展到任意 L。他们证明了 $\pi^\star$ 在状态空间 $\mathcal{S}$ 中是单调的，并识别了无法到达的状态。由于 $\mathcal{S}$ 随 L 和 T 指数增长，因此他们提出了启发式方法（包括基于条件和深度学习的方法）以保持低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决高效卡车编队在运输和物流中的重要性，特别是要满足交货截止日期这一关键约束。

Method: 该研究将卡车编队和调度问题构建为马尔可夫决策过程。他们首先分析了 L = 3 的有限容量下最优策略 $\pi^\star$ 的结构，并证明了该策略在状态空间 $\mathcal{S}$ 中是单调的。然后，他们将这些见解推广到任意 L 的情况，并识别了无法到达的状态。最后，为了解决状态空间随 L 和 T 指数增长的问题，研究人员提出了一种启发式方法，包括基于条件和深度学习的方法。

Result: 研究人员证明了最优策略 $\pi^\star$ 在状态空间 $\mathcal{S}$ 中是单调的，并确定了无法到达的状态。他们还开发了启发式方法，以在保持低计算复杂度的同时，利用这些结构性见解。

Conclusion: 该研究提出了一个用于卡车编队和调度的马尔可夫决策模型，并提供了最优策略的理论分析。为了解决实际应用中的计算挑战，还开发了启发式方法。研究结果为在有截止日期约束的情况下优化卡车编队提供了有价值的见解。

Abstract: Efficient truck platooning is a key strategy for reducing freight costs,
lowering fuel consumption, and mitigating emissions. Deadlines are critical in
this context, as trucks must depart within specific time windows to meet
delivery requirements and avoid penalties. In this paper, we investigate the
optimal formation and dispatch of truck platoons at a highway station with
finite capacity $L$ and deadline constraints $T$. The system operates in
discrete time, with each arriving truck assigned a deadline of $T$ slot units.
The objective is to leverage the efficiency gains from forming large platoons
while accounting for waiting costs and deadline violations. We formulate the
problem as a Markov decision process and analyze the structure of the optimal
policy $\pi^\star$ for $L = 3$, extending insights to arbitrary $L$. We prove
that the $\pi^\star$ is monotone in the state space $\mathcal{S}$ and identify
classes of unreachable states. Moreover, since $\mathcal{S}$ grows
exponentially with $L$ and $T$, we propose heuristics-including conditional and
deep-learning based approaches-that exploit these structural insights while
maintaining low computational complexity.

</details>


### [162] [Incorporating Social Awareness into Control of Unknown Multi-Agent Systems: A Real-Time Spatiotemporal Tubes Approach](https://arxiv.org/abs/2510.25597)
*Siddhartha Upadhyay,Ratnangshu Das,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 本篇论文提出了一种去中心化的控制框架，为具有未知动力学的多智能体系统引入了社会意识，以在动态环境中实现预设时间内的到达-避开-停留任务。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，为具有未知动力学的多智能体系统实现预设时间内的到达-避开-停留任务，并考虑异构的社会行为。

Method: 提出了一种实时的时空管（STT）框架，为每个智能体在线合成管，并捕捉其与其他智能体的社会互动。推导了一种无近似的闭式控制律，确保每个智能体保持在其不断演变的STT内，从而以社会意识的方式避开动态障碍物、防止智能体间碰撞，并在预设时间内到达目标。

Result: 推导的控制律确保了安全性和时间性，该方法计算量轻、无需模型且能抵抗未知干扰。通过在二维全向机器人上的仿真和硬件实验验证了该框架的有效性和可扩展性。

Conclusion: 所提出的社会意识去中心化控制框架能够有效地在预设时间内完成到达-避开-停留任务，并考虑了智能体间的社会互动。该方法具有计算量轻、无需模型、鲁棒性强等优点，并得到了仿真和硬件实验的验证。

Abstract: This paper presents a decentralized control framework that incorporates
social awareness into multi-agent systems with unknown dynamics to achieve
prescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is
assigned a social awareness index that quantifies its level of cooperation or
self-interest, allowing heterogeneous social behaviors within the system.
Building on the spatiotemporal tube (STT) framework, we propose a real-time STT
framework that synthesizes tubes online for each agent while capturing its
social interactions with others. A closed-form, approximation-free control law
is derived to ensure that each agent remains within its evolving STT, thereby
avoiding dynamic obstacles while also preventing inter-agent collisions in a
socially aware manner, and reaching the target within a prescribed time. The
proposed approach provides formal guarantees on safety and timing, and is
computationally lightweight, model-free, and robust to unknown disturbances.
The effectiveness and scalability of the framework are validated through
simulation and hardware experiments on a 2D omnidirectional

</details>


### [163] [An OPF-based Control Framework for Hybrid AC-MTDC Power Systems under Uncertainty](https://arxiv.org/abs/2510.25671)
*Hongjin Du,Rahul Rane,Weijie Xia,Pedro P. Vergara,Aleksandra Lekić*

Main category: eess.SY

TL;DR: 本论文提出了一种结合预测和自适应下垂控制的混合交流-直流输电系统（HVAC-HVDC）的优化控制框架，以应对可再生能源（特别是海上风电）并网带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 海上风电等可再生能源的大量并网，由于预测误差和功率波动，给混合交流-直流输电系统（HVAC-HVDC）带来了显著的不确定性。传统的固定设定点控制策略忽略了频率偏差，在风电剧烈波动时可能危及系统稳定性。

Method: 本论文提出了一种结合预测和优化的自适应控制框架。首先，利用随机森林模型进行风速预测，并将其纳入时序耦合最优潮流（OPF）计算，以预先确定基准换流器设定点。然后，基于实际运行条件对设定点进行实时调整。其次，开发了一种自适应下垂控制方案，该方案同时考虑直流电压和交流频率偏差。

Result: 通过硬件在环（HIL）仿真验证了所提出控制框架的有效性。

Conclusion: 所提出的结合预测和自适应下垂控制的HVAC-HVDC优化控制框架，能够确保系统在高渗透率可再生能源并网下的稳定性和鲁棒性。

Abstract: The increasing integration of renewable energy, particularly offshore wind,
introduces significant uncertainty into hybrid AC-HVDC systems due to forecast
errors and power fluctuations. Conventional control strategies typically rely
on fixed setpoints and neglect frequency deviations, which can compromise
system stability under rapid renewable variations. To address this challenge,
this paper presents a forecast-integrated, optimal power flow (OPF)-based
adaptive control framework. Wind speed forecasts generated using a Random
Forest model are incorporated into a time-coupled OPF to determine baseline
converter setpoints in anticipation of wind fluctuations, which are further
adjusted in real time based on actual operating conditions. An adaptive droop
control scheme is developed that jointly considers DC voltage and AC frequency
deviations. The effectiveness of the proposed control framework is validated
through hardware-in-the-loop (HIL) simulations, demonstrating its capability to
ensure stable and robust operation of hybrid AC-HVDC systems under high
penetration of renewable energy.

</details>


### [164] [Over 3 kV and Ultra-Low leakage Vertical (011) \b{eta}-Ga2O3 Power Diodes with Engineered Schottky Contact and High-permittivity Dielectric Field Plate](https://arxiv.org/abs/2510.25695)
*Emerson J. Hollar,Esmat Farzana*

Main category: eess.SY

TL;DR: 利用(011)取向的{eta}-Ga2O3材料，结合肖特基势垒工程和高介电常数(ZrO2)场板，成功研制出击穿电压超过3kV且漏电流极低的功率器件。


<details>
  <summary>Details</summary>
Motivation: 开发具有高击穿电压和低漏电的{eta}-Ga2O3垂直功率器件。

Method: 采用(011)取向{eta}-Ga2O3材料，通过肖特基势垒工程（使用Pt/PtOx/Pt复合阳极接触）和高介电常数ZrO2介电层实现场板效应，以管理边缘电场和隧穿漏电。

Result: 通过肖特基势垒工程和场板效应，将(011) {eta}-Ga2O3肖特基二极管的击穿电压从~1.5 kV提升至3.7 kV，同时保持了低导通电压。

Conclusion: 结合复合肖特基接触的隧穿漏电管理、ZrO2场板的边缘电场管理以及(011) {eta}-Ga2O3材料的优势，是一种开发低漏电、多千伏级(011) {eta}-Ga2O3垂直功率器件的有效策略。

Abstract: We report over 3 kV breakdown voltage and ultra-low leakage (011)
\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and
high-permittivity (\k{appa}) dielectric (ZrO2) field plate. The (011)
orientation of \b{eta}-Ga2O3 enabled low background doping and thick drift
layers which are promising to support kV-class vertical \b{eta}-Ga2O3 power
switches. The Schottky barrier engineering was performed with a composite Pt
cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse
blocking capabilities enabled by PtOx while allowing low turn-on voltage by the
interfacing thin Pt layer. We also performed a systematic study using a
co-processed Pt/(011) \b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same
wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the
field-plate Pt/(011) \b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage
of 2.75 kV owing to the edge field management. Further enhancement of the
breakdown voltage was achieved by tunneling leakage management using composite
Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown
voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt
(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)
\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management
by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,
edge field reduction by high-\k{appa} dielectric ZrO2 field plate, as well as
the advantageous material properties offered by (011) \b{eta}-Ga2O3 demonstrate
a promising strategy for developing ultra-low leakage and multi-kV class
vertical (011) \b{eta}-Ga2O3 power devices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [165] [Molecular simulations of Perovskites CsXI3 (X = Pb,Sn) Using Machine-Learning Interatomic Potentials](https://arxiv.org/abs/2510.24874)
*Atefe Ebrahimi,Franco Pellegrini,Stefano De Gironcoli*

Main category: cond-mat.mtrl-sci

TL;DR: 基于机器学习的卤素钙钛矿模拟，揭示了Pb和Sn基材料的结构稳定性差异，并为下一代光伏技术的设计提供了新思路。


<details>
  <summary>Details</summary>
Motivation: Cesium-based halide perovskites（如CsPbI3和CsSnI3）在光伏和光电领域具有巨大潜力，但受限于温度依赖的相变和结构不稳定性，阻碍了实际应用。

Method: 利用LATTE框架开发机器学习原子间势（MLIPs），以近乎实验的精度模拟这些材料，并大大降低了计算成本。通过基于训练好的MLIPs进行分子动力学模拟，捕捉了不同相态下的能量、力、相变（立方、四方、斜方）、晶格参数和八面体倾斜。

Result: 研究发现，与Sn基类似物相比，Pb基钙钛矿表现出更大的八面体倾斜和更高的相变温度，表明其具有更强的键合和更好的结构稳定性。而Sn基钙钛矿的倾斜度较小，势垒较低，提示了通过成分或界面工程进行调控的可能性。

Conclusion: 机器学习势（MLIPs）能够结合第一性原理的精度和模拟的效率，为探索卤素钙钛矿的相稳定性、非谐性和理性设计提供了一个强大的框架。

Abstract: Cesium based halide perovskites, such as CsPbI3 and CsSnI3, have emerged as
exceptional candidates for next generation photovoltaic and optoelectronic
technologies, but their practical application is limited by temperature
dependent phase transitions and structural instabilities. Here, we develop
machine learning interatomic potentials within the LATTE framework to simulate
these materials with near experimental accuracy at a fraction of the
computational cost compared to previous computational studies. Our molecular
dynamics simulations based on the trained MLIPs reproduce energies and forces
across multiple phases, enabling large scale simulations that capture cubic
tetragonal orthorhombic transitions, lattice parameters, and octahedral tilting
with unprecedented resolution. We find that Pb based perovskites exhibit larger
octahedral tilts and higher phase transition temperatures than Sn based
analogues, reflecting stronger bonding and enhanced structural stability,
whereas Sn based perovskites display reduced tilts and lower barriers,
suggesting tunability through compositional or interface engineering. Beyond
these systems, our work demonstrates that MLIPs can bridge first principles
accuracy with simulation efficiency, providing a robust framework for exploring
phase stability, anharmonicity, and rational design in next generation halide
perovskites.

</details>


### [166] [Improved operating voltage in InGaN-capped AlGaN-based DUV LEDs on bulk AlN substrates](https://arxiv.org/abs/2510.24892)
*H-W S. Huang,S. Agrawal,D. Bhattacharya,H. G. Xing,D. Jena*

Main category: cond-mat.mtrl-sci

TL;DR: 使用SiO2薄膜覆盖技术解决了深紫外发光二极管(DUV-LEDs)的p型和n型欧姆接触的优化问题，从而降低了器件的工作电压和导通电阻。


<details>
  <summary>Details</summary>
Motivation: 深紫外发光二极管(DUV-LEDs)需要同时实现低阻p型和n型接触，这是一个挑战性问题。

Method: 研究了p-InGaN和n-AlGaN接触的共同优化。通过在p-GaN层上沉积SiO2薄膜，然后在高温下对n-AlGaN进行退火，以保持p型接触的低电阻率，同时实现n型接触的低电阻率。

Result: 采用SiO2覆盖技术后，DUV-LEDs在400 A/cm^2的电流下，工作电压降低了3.5 V，差分导通电阻从6.4 mOhm.cm^2降低到4.5 mOhm.cm^2。

Conclusion: 该研究提出了一种可扩展的、用于高性能、高铝含量的双极AlGaN器件的制备方法。

Abstract: Better wall plug efficiency of deep-ultraviolet light emitting diodes
(DUV-LEDs) requires simultaneous low resistivity p-type and n-type contacts,
which is a challenging problem. In this study, the co-optimization of p-InGaN
and n- AlGaN contacts for DUV LEDs are investigated. We find that using a thin
7%InGaN cap is effective in achieving ohmic p-contacts with specific contact
resistivity of 3.10x10^{-5} Ohm.cm^2. Upon monolithic integration of p- and n-
contacts for DUV LEDs, we find that the high temperature annealing of 800C
required for the formation of low resistance contacts to n-AlGaN severely
degrades the p-InGaN layer, thereby reducing the hole concentration and
increasing the specific contact resistivity to 9.72x10^{-4} Ohm.cm^2.
Depositing a SiO2 cap by plasma-enhanced atomic layer deposition (PE-ALD) prior
to high temperature n-contact annealing restores the low p-contact resistivity,
enabling simultaneous low-resistance p- and n-contacts. DUV-LEDs emitting at
268 nm fabricated with the SiO2 capping technique exhibit a 3.5 V reduction in
operating voltage at a current level of 400 A/cm^2 and a decrease in
differential ON-resistance from 6.4 mOhm.cm^2 to 4.5 mOhm.cm^2. This study
highlights a scalable route to high-performance, high-Al-content bipolar AlGaN
devices.

</details>


### [167] [Stabilisation of hBN/SiC Heterostructures with Vacancies and Transition-Metal Atoms](https://arxiv.org/abs/2510.24952)
*Arsalan Hashemi,Nima Ghafari Cherati,Sadegh Ghaderzadeh,Yanzhou Wang,Mahdi Ghorbani-Asl,Tapio Ala-Nissila*

Main category: cond-mat.mtrl-sci

TL;DR: 通过去除特定晶格位点的硼原子，可以将hBN/SiC异质结构中的层间相互作用从弱范德华耦合转变为牢固的局域硅-氮共价键，从而实现过渡金属原子的锚定和表面反应性增强，为下一代催化能源转化技术提供了平台。


<details>
  <summary>Details</summary>
Motivation: 研究二维原子层材料（如hBN/SiC异质结构）中层间相互作用的调控及其对材料性质的影响，特别是如何利用这种调控来增强表面反应性，实现单金属原子的可控分离，并探索其在催化能源转化技术中的应用潜力。

Method: 利用量子力学密度泛函理论计算，并结合机器学习辅助的分子动力学模拟。

Result: 发现通过去除硼原子可以实现从范德华耦合到共价键的转变，并证实了过渡金属原子在该异质结构上的吸附可行性和动力学稳定性。提出了增强表面反应性和实现单金属原子锚定的设计指南。

Conclusion: hBN/SiC异质结构是一种可用于原子级精确过渡金属功能化的多功能平台，在下一代催化能源转化技术方面具有应用前景。

Abstract: When two-dimensional atomic layers of different materials are brought into
close proximity to form van der Waals (vdW) heterostructures, interactions
between adjacent layers significantly influence their physicochemical
properties. These effects seem particularly pronounced when the interface
exhibits local order and near-perfect structural alignment, leading to the
emergence of Moir\'e patterns. Using quantum mechanical density functional
theory calculations, we propose a prototypical bilayer heterostructure composed
of hexagonal boron nitride (hBN) and silicon carbide (SiC), characterized by a
lattice mismatch of 18.77\% between their primitive unit cells. We find that
the removal of boron atoms from specific lattice sites can convert the
interlayer interaction from weak vdW coupling to robust localized
silicon-nitrogen covalent bonding. Motivated by this, we study the binding of
transition-metal adatoms and formulate design guidelines to enhance surface
reactivity, thereby enabling the controlled isolation of single-metal atoms.
Our machine-learning-assisted molecular dynamics simulations confirm both
dynamical stability and metal anchoring feasibility at finite temperatures. Our
results suggest the hBN/SiC heterostructure as a versatile platform for
atomically precise transition-metal functionalization, having potential for
next-generation catalytic energy-conversion technologies.

</details>


### [168] [Trichome entanglement enhances damage tolerance in microstructured biocomposites](https://arxiv.org/abs/2510.24970)
*Israel Kellersztein,Mathieu Desgranges,Chiara Daraio*

Main category: cond-mat.mtrl-sci

TL;DR: 通过利用螺旋藻三切片的物理缠结特性，研究人员开发出一种增强复合材料抗损伤能力的新方法，该方法通过3D打印技术，在不牺牲结构稳定性的前提下，显著提高了材料的强度和韧性。


<details>
  <summary>Details</summary>
Motivation: 复合材料的抗损伤能力是材料科学中的一个核心挑战。传统的增强方法（如添加填料或化学改性）往往会限制能量耗散并影响结构稳定性。

Method: 本研究利用螺旋藻三切片的独特形态，通过物理缠结实现增强。通过比较螺旋状三切片与形态拉直的三切片，分离出三切片几何形状在机械性能中的关键作用。将三切片与羟乙基纤维素（HEC）基质结合，通过基于挤出的3D打印进行加工。

Result: 基于三切片的悬浮液表现出增强的粘弹性响应，屈服应力增加了三倍。3D打印的材料在弯曲强度方面提高了290%，断裂功方面提高了15倍。断裂表面分析表明，裂纹扩展模式从界面脱粘和拔出转变为穿过缠结网络，表明抗损伤能力得到了增强。

Conclusion: 研究证明，三切片的缠结是一种可扩展的、物理驱动的机制，可以通过微结构设计来提高复合材料的抗损伤能力。

Abstract: Achieving damage tolerance in composite materials remains a central challenge
in materials science. Conventional strategies often rely on filler
incorporation or chemical modification, which can limit energy dissipation and
constrain structural stability. Here, we leverage the unique morphology of
Spirulina trichomes to investigate a reinforcement mechanism based on physical
filament entanglement. By comparing helical trichomes with their
morphologically straightened counterparts, we isolate filament geometry as the
key parameter governing mechanical performance. Trichome-based suspensions
exhibit enhanced viscoelastic response and a threefold increase in yield
stress. When processed via extrusion-based 3D printing using hydroxyethyl
cellulose (HEC) as a matrix, entangled trichomes yield a 290% improvement in
bending strength and a 15-fold enhancement in work of fracture. Fracture
surface analysis reveals a transition from interfacial debonding and pull-out
(in filaments) to crack propagation through the entangled network, indicating
structure-mediated toughening. These findings establish trichome entanglement
as a scalable, physically driven mechanism for enhancing damage tolerance
through microstructural architecture.

</details>


### [169] [Preliminary Demonstration of Diamond-GaN pn Diodes via Grafting](https://arxiv.org/abs/2510.25028)
*Jie Zhou,Yi Lu,Chenyu Wang,Luke Suter,Aaron Hardy,Tien Khee Ng,Kai Sun,Yifu Guo,Yang Liu,Tsung-Han Tsai,Xuanyu Zhou,Connor S Bailey,Michael Eller,Stephanie Liu,Zetian Mi,Boon S. Ooi,Matthias Muehle,Katherine Fountaine,Vincent Gambin,Jung-Hun Seo,Zhenqiang Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 通过异质集成技术制备了金刚石/氮化镓异质结pn二极管，实现了超过1e4的整流比。


<details>
  <summary>Details</summary>
Motivation: 为了克服超宽带隙半导体（UWBG）材料在掺杂方面的限制，探索了异质pn结的制备方法。

Method: 采用外延生长和原子层沉积（ALD）技术，将p+金刚石纳米薄膜接枝到n+ GaN衬底上，并使用Al2O3作为超薄界面层。

Result: 制备的二极管表现出1.55的理想因子和超过1e4的整流比，并通过AFM、XRD、Raman和STEM等手段对结构和界面性质进行了表征。

Conclusion: 该研究初步展示了金刚石/氮化镓异质结pn二极管的可行性，并为进一步优化提供了结构和界面方面的关键见解。

Abstract: Ultrawide bandgap (UWBG) semiconductors exhibit exceptional electrical and
thermal properties, offering strong potential for high power and high frequency
electronics. However, efficient doping in UWBG materials is typically limited
to either n type or p type, constraining their application to unipolar devices.
The realization of pn junctions through heterogeneous integration of
complementary UWBG or WBG semiconductors is hindered by lattice mismatch and
thermal expansion differences. Here, we report the preliminary demonstration of
diamond GaN heterojunction pn diodes fabricated via grafting. A single
crystalline p plus diamond nanomembrane was integrated onto an epitaxially
grown c plane n plus GaN substrate with an ultrathin ALD Al2O3 interlayer. The
resulting diodes exhibit an ideality factor of 1.55 and a rectification ratio
of over 1e4. Structural and interfacial properties were examined by AFM, XRD,
Raman, and STEM, providing critical insights to guide further optimization of
diamond GaN pn heterojunction devices.

</details>


### [170] [Percolating Corrosion Pathways of Chemically Ordered NiCr Alloys in Molten Salts](https://arxiv.org/abs/2510.25098)
*Hamdy Arkoub,Jia-Hong Ke,Kaustubh Bawane,Miaomiao Jin*

Main category: cond-mat.mtrl-sci

TL;DR: Ni2Cr的化学有序结构比无序结构更容易在熔盐环境中腐蚀，这归因于Cr的渗流扩散通路和溶解能垒的降低。


<details>
  <summary>Details</summary>
Motivation: 理解化学有序结构如何加速NiCr合金在熔盐环境中的腐蚀机制。

Method: 使用反应分子动力学和第一性原理计算。

Result: 发现Ni2Cr有序结构比无序结构腐蚀速度快得多，证实了有序结构是导致腐蚀加速的原因，而非残余应力。

Conclusion: 化学有序结构通过形成渗流扩散通路和降低Cr溶解的能垒，显著加速了NiCr合金在熔盐环境中的腐蚀速率，解释了实验观察结果。

Abstract: Recent experiments have shown that chemical ordering in NiCr alloys can
significantly accelerate corrosion in molten salt environments. However, the
underlying mechanisms remain poorly understood. Using reactive molecular
dynamics and first-principles calculations, we show that long-range ordered
Ni$_2$Cr in Ni-33at.%Cr alloys corrodes far more rapidly in FLiNaK salt at
800{\deg}C than short-range ordered or random solid solutions. This accelerated
attack originates from percolating Cr pathways that enhance near-surface
diffusion and a lowered energetic barrier for Cr dissolution, as confirmed by
first-principles calculations. Contrary to earlier explanations that attributed
this behavior to residual stresses, our stress-free simulations demonstrate
that ordering alone accelerates the degradation. These results establish
percolation as a critical link between chemical ordering and corrosion
kinetics, offering a mechanistic basis for experimental observations.

</details>


### [171] [A Geometric Pathway for Tuning Ferroelectric Properties via Polar State Reconfiguration](https://arxiv.org/abs/2510.25142)
*Hao-Cheng Thong,Bo Wu,Fan Hu,Pedro B. Groszewicz,Chen-Bo-Wen Li,Jun Chen,Mao-Hua Zhang,Dragan Damjanovic,Ben Xu,Ke Wang*

Main category: cond-mat.mtrl-sci

TL;DR: Li取代的NaNbO3通过热驱动的极性态重构，可以调节铁电性质，提高居里温度并引起压电硬化。


<details>
  <summary>Details</summary>
Motivation: 探索通过热驱动的极性态重构来调节铁电性质的几何途径。

Method: 利用第一性原理密度泛函理论计算和7Li固态核磁共振波谱测量。

Result: Li取代产生了两种不同的极性构型，其在退火过程中的转变提高了居里温度并引起了压电硬化。

Conclusion: 提出了一种几何驱动的极性态重构机制，为通过晶格几何工程化宏观铁电功能性质提供了通用设计原理。

Abstract: We report the discovery of a geometric pathway for tuning ferroelectric
properties through thermally driven reconfiguration between coexisting polar
states in Li-substituted NaNbO3. Using first-principles density functional
theory calculation and 7Li solid-state nuclear magnetic resonance spectroscopy
measurement, we reveal that Li substitution creates two distinct polar
configurations whose transformation under annealing enhances the Curie
temperature and induces piezoelectric hardening. Our findings establish a
geometrically-driven polar state reconfiguration mechanism, providing a general
design principle for ferroics whereby macroscopic functional properties can be
engineered via lattice geometry.

</details>


### [172] [Intrinsic emittance properties of an Fe-doped Beta-Ga2O3(010) photocathode: Ultracold electron emission at 300K and the polaron self-energy](https://arxiv.org/abs/2510.25722)
*Louis A. Angeloni,Ir-Jene Shan,J. H. Leach,W. Andreas Schroeder*

Main category: cond-mat.mtrl-sci

TL;DR: 铁掺杂的Beta-Ga2O3(010)光电阴极在3.5-4.4eV光子能量范围内表现出具有6meV平均横向能量（MTE）的超冷电子发射，该发射可能直接源于Fe掺杂态的电子跃迁，并叠加了一个更强的、与声子介导的特定发射过程相关的MTE信号。光子能量超过4.5eV时，吸收深度减小导致传输机制转变，两种MTE信号的趋势都与包含极化子形成自能量的声子介导的弗兰克-孔登（FC）发射过程一致。


<details>
  <summary>Details</summary>
Motivation: 研究铁掺杂Beta-Ga2O3(010)光电阴极的超冷电子发射机制，并探索光子能量对电子发射谱及其平均横向能量（MTE）的影响。

Method: 在300K下测量了铁掺杂Beta-Ga2O3(010)光电阴极在3.5-4.4eV和4.5eV以上光子能量范围内的光谱发射特性，分析了平均横向能量（MTE）的数值及其随光子能量的变化趋势，并将其与理论模型（如直接跃迁、声子介导的FC发射、极化子效应）进行对比。

Result: 在3.5-4.4eV光子能量范围内观察到具有6meV平均横向能量（MTE）的超冷电子发射，并叠加了一个MTE值更高的声子介导的FC发射信号。当光子能量超过4.5eV时，吸收深度减小，两种MTE信号的趋势均与包含极化子形成自能量的声子介导的FC发射过程相符。

Conclusion: 超冷电子发射源于Fe掺杂态电子的直接跃迁，而较强的发射则与声子介导的FC过程相关。吸收深度的变化和极化子效应在解释不同光子能量下的MTE行为中起着关键作用。

Abstract: Measurements of the spectral emission properties of an iron-doped
Beta-Ga2O3(010) photocathode at 300K reveal the presence of ultracold electron
emission with a 6meV mean transverse energy (MTE) in the 3.5-4.4eV photon
energy range (282-354nm). This extreme sub-thermal photoemission signal is
consistent with direct emission of electrons photoexcited from the Fe dopant
states into the low effective mass and positive electron affinity primary
conduction band, and it is superimposed on a stronger signal with a larger MTE
associated with an (optical)phonon-mediated momentum resonant Franck-Condon
(FC) emission process from a thermally populated and negative electron affinity
upper conduction band. For photon energies above 4.5eV, a transition from a
long to a short transport regime is forced by an absorption depth reduction to
below 100nm and both MTE signals exhibit spectral trends consistent with
phonon-mediated FC emission if the polaron formation self-energy is included in
the initial photoexcited electron thermalization.

</details>


### [173] [Advanced structural characterization of single-walled carbon nanotubes with 4D-STEM](https://arxiv.org/abs/2510.25286)
*Antonin Louiset,Daniel Förster,Vincent Jourdain,Saïd Tahir,Nicola Vigano,Jean-Luc Rouvière,Christophe Bichara,Hanako Okuno*

Main category: cond-mat.mtrl-sci

TL;DR: 4D-STEM技术可用于高精度表征碳纳米管的结构和缺陷。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏高效的碳纳米管结构表征技术，限制了其应用。

Method: 利用4D-STEM技术，结合快速像素化电子探测器，采集低噪声电子衍射图，并进行电子断层扫描。

Result: 精确测定碳纳米管局部手性、沿单个碳纳米管的手性分布和应变分布，并以原子级分辨率重建高分辨率图像，可用于成像原子缺陷。

Conclusion: 4D-STEM是一种有效的碳纳米管结构表征工具，电子断层扫描技术有望提供对碳纳米管缺陷结构的更深入见解。

Abstract: Single wall carbon nanotubes (SWCNT) exhibit remarkable optical and
electrical properties making them one of the most promising materials for next
generation electronic and optoelectronic devices. Their electronic properties
strongly depend on their chirality, i.e., their structural configuration, as
well as on the presence and nature of atomic defects. Currently, the lack of
versatile and efficient structural characterization techniques limits SWCNT
applications. Here, we report how four-dimensional scanning transmission
electron microscopy (4D-STEM) can address critical challenges in SWCNT
structural analysis. Using modern fast pixelated electron detectors, we were
able to acquire rapidly a large number of low noise electron diffraction
patterns of SWCNTs. The resulting 4D-STEM data allow to precisely determine the
local chirality of multiple nanotubes at once, with limited electron dose (down
to 1750 e-/{\AA}^2) and nanometric spatial resolution (down to 3.1 nm). We also
show how this approach enables to track the chirality along a single nanotube,
while giving access to the strain distribution. Then, we report how 4D-STEM
data enable to reconstruct high-resolution images with electron ptychography.
With this second approach, structural information can be obtained with atomic
scale spatial resolution allowing atomic defect imaging. Finally, we
investigate how multi-slice electron ptychography could provide even further
insight on nanotube defect structures thanks to its close to 3D imaging
capabilities at atomic resolution.

</details>


### [174] [Two Orders of Magnitude Enhancement in Oxide Ion Conductivity in Cu2P2O7 via Vanadium Substitution: A Pathway Toward SOFC Electrolytes](https://arxiv.org/abs/2510.25325)
*Bibhas Ghanta,Kuldeep Singh Chikara,Uttam Kumar Goutam,Anup Kumar Bera,Seikh Mohammad Yusuf*

Main category: cond-mat.mtrl-sci

TL;DR: 通过钒取代Cu2P2-xVxO7以提高其离子电导率，并探索其在SOFC中的应用。


<details>
  <summary>Details</summary>
Motivation: 固态燃料电池（SOFC）在化学能向电能转化方面具有潜力，需要提高其离子电导率。

Method: 采用阻抗谱和中子衍射技术研究了Cu2P2-xVxO7（x = 0, 0.4, 0.6, 0.8 and 1）的电学性能和晶体结构。X射线光电子能谱用于确认离子价态。

Result: 钒取代显著提高了Cu2P2-xVxO7的离子电导率（x=1时从~3.81x10-5 S cm-1提高到~2.08x10-3 S cm-1）。离子传导占主导地位（>95%）。扩散系数和跳跃率随x增加而增加。通过交流电导率、电模量和介电性能研究揭示了微观传导机制为相关势垒跳跃（CBH）过程。中子衍射和价键和分析表明存在三维氧化离子传导通路。

Conclusion: 钒取代是提高Cu2P2-xVxO7离子电导率的有效途径，为SOFCs提供了潜在应用。研究阐明了微观传导机制、离子传导通路以及晶体结构对离子传导的影响。

Abstract: In the quest of green energy, Solid Oxide Fuel Cells (SOFC) have drawn
considerable attention for chemical-to-electric energy conversion. In the
present paper, we report an enhancement of ionic conductivity in Cu2P2-xVxO7 by
vanadium substitution. The electrical (dc and ac conductivity, diffusivity,
hopping rate, electric modulus and dielectric properties) and crystal
structural properties of Cu2P2-xVxO7 (x = 0, 0.4, 0.6, 0.8 and 1) are
investigated by impedance spectroscopy and neutron diffraction, respectively.
X-ray photoelectron spectroscopy (XPS) study confirms the presence of Cu2+,
P5+and V5+ mono-valence states. The dc conductivity results reveal a two orders
of magnitude enhancement of ionic conductivity from ~3.81x10-5 S cm-1 for x =0
to ~2.08x10-3 S cm-1 for x =1 at 993 K, revealing a possible application in
SOFCs. DC transport number studies reveal that the total conductivity is
dominated by ionic conduction (> 95%). In addition, the diffusivity and hopping
rate of oxide ions increase with increasing x. Besides, ac conductivity,
electric modulus and dielectric properties have been investigated to illustrate
the microscopic conduction mechanism. The derived results suggest that the
mechanism for ionic conduction is the correlated barrier hopping (CBH) process.
The soft-bond valence sum (BVS) analysis of the neutron diffraction patterns
reveals the three-dimensional (3D) oxide ion conduction pathways within the
crystal structure. The present study provides a pathway to enhance the ionic
conductivity, as well as understanding of microscopic conduction mechanism,
ionic conduction pathways and the role of crystal structure on the ionic
conduction.

</details>


### [175] [Colloidal quasi-2D Cs2AgBiBr6 double perovskite nanosheets: synthesis and application as high-performance photodetectors](https://arxiv.org/abs/2510.25355)
*Pannan I. Kyesmen,Eugen Klein,Brindhu Malani S,Rostyslav Lesyuk,Christian Klinke*

Main category: cond-mat.mtrl-sci

TL;DR: Cs2AgBiBr6纳米片可用于制造高性能光电探测器。


<details>
  <summary>Details</summary>
Motivation: 寻找无毒无铅卤化物钙钛矿以替代铅基材料，双钙钛矿是潜在候选者，其中Cs2AgBiBr6是最有前途的环保材料之一。

Method: 通过低温注射胶体合成法制备准二维Cs2AgBiBr6纳米片，并用于构建无传输层结构的高性能光电探测器。

Result: 制备的Cs2AgBiBr6纳米片尺寸可达1.4微米，厚度仅为几纳米，制成的光电探测器具有高探测度（1.15*10^12 Jones）、高响应度（121 mA/W）、优异的开关比（2.39*10^4）以及快速的响应和衰减时间（分别为857和829微秒）。此外，器件表现出卓越的稳定性，在环境条件下储存80天后仍能保持全部光电流。

Conclusion: 该研究展示了通过胶体合成法制备准二维Cs2AgBiBr6无铅双钙钛矿纳米片的途径，该纳米片具有适用于高性能光电探测和其他光电应用的优良性能。

Abstract: The search for non-toxic lead-free halide perovskites that can compete with
the lead-based counterparts has led to the emergence of double perovskites as
potential candidates. Among many options, Cs2AgBiBr6 stands out as one of the
most suitable eco-friendly materials for numerous optoelectronic applications.
In this study, quasi-2D Cs2AgBiBr6 nanosheets (NSs) were prepared via the
low-temperature injection colloidal synthesis and used to fabricate
high-performance photodetectors in a transport-layer-free architecture. The
reaction temperature and ligands played vital roles in the structural purity,
shape, and size of the synthesized Cs2AgBiBr6 NSs. The fabricated NSs disclosed
lateral sizes of up to 1.4 um and are only a few nanometers thick. The
high-performance photodetectors fabricated using the Cs2AgBiBr6 NSs yielded a
high detectivity (D) of 1.15*10^12 Jones, responsivity (R) of 121 mA/W, a
notable on/off ratio of 2.39*10^4, and a fast rise and decay time of 857 and
829 us, respectively. The device demonstrates remarkable stability. Basically,
it sustains its entire photocurrent after storage in ambient conditions for 80
days. This work showcases a pathway for the colloidal synthesis of quasi-2D
Cs2AgBiBr6 lead-free double perovskite NSs with suitable properties for
high-performance photodetection and other optoelectronic applications.

</details>


### [176] [Terahertz Time-Domain Spectroscopy and Density Functional Theory Analysis of Low-Frequency Vibrational Modes of a Benzoxazolium-Coumarin Donor-π-Acceptor Chromophore](https://arxiv.org/abs/2510.25365)
*Sidhanta Sahu,Phalguna Krishna Das Vana,Anupama Chauhan,Poulami Ghosh,Vijay Sai Krishna Cheerala,Sanyam,C. N. Sundaresan,N. Kamaraju*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究使用太赫兹时域光谱和密度泛函理论研究了 BCO+ 分子中的低频振动模式及其与分子内电荷转移 (ICT) 的关系。


<details>
  <summary>Details</summary>
Motivation: 研究 BCO+ 分子中调节分子内电荷转移 (ICT) 的低频振动模式。

Method: 使用透射太赫兹时域光谱 (THz-TDS) 和气相密度泛函理论 (DFT) 研究 BCO+ 分子。

Result: THz-TDS 测量到 0.62、0.85、1.30、1.81 和 2.07 THz 的特征模式。DFT 计算证实了这些模式，并将其与特定的分子内运动联系起来。

Conclusion: THz-TDS 和 DFT 共同识别了 BCO+ 的特征低频模式，并表明这些模式与 ICT 相关的核运动有关，证明了 THz-TDS 在研究给体-π-受体系统中的振动特征方面具有敏感性。

Abstract: To elucidate low-frequency vibrational modes that modulate intramolecular
charge transfer (ICT), we investigate a benzoxazolium-coumarin (BCO+)
donor-pi-acceptor derivative using transmission terahertz time-domain
spectroscopy (THz-TDS). The retrieved complex refractive index reveals distinct
modes at 0.62, 0.85, 1.30, 1.81, and 2.07 THz. Gas-phase density functional
theory (DFT) agrees with these features and enables assignment of specific
intramolecular motions. Together, THz-TDS and DFT identify characteristic
low-frequency modes of BCO+ and suggest their connection to ICT-relevant
nuclear motions, demonstrating that THz-TDS provides a sensitive probe of
vibrational signatures in donor-pi-acceptor systems.

</details>


### [177] [The Microscopic Nature of Orbital Disorder in LaMnO$_{3}$](https://arxiv.org/abs/2510.25414)
*Bodoo Batnaran,Andrew L. Goodwin,Michael A. Hayward,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: LaMnO3的原子图景揭示了在高温下，电子不稳定性驱动的是声子而非传统的Jahn-Teller畸变。


<details>
  <summary>Details</summary>
Motivation: 研究LaMnO3中典型的轨道有序钙钛矿材料的顺序-无序转变的原子图景。

Method: 使用机器学习驱动的分子动力学模拟，描述了这对分布函数随温度的变化，并与实验结果进行比较。

Result: LaMnO3的轨道无序相包含具有和不具有反转对称性的不同结构畸变，并且这些畸变是动态的，寿命约为40飞秒。在高|T|相中，这些畸变与声子无谐波运动的时间尺度一致，表明电子不稳定性在高|T|下驱动的是声子无谐波，而不是Jahn-Teller畸变。

Conclusion: LaMnO3的轨道无序相包含具有和不具有反转对称性的不同结构畸变，并且这些畸变是动态的，寿命约为40飞秒。在高|T|相中，这些畸变与声子无谐波运动的时间尺度一致，表明电子不稳定性在高|T|下驱动的是声子无谐波，而不是Jahn-Teller畸变。该方法可以推广到研究其他相关的材料。

Abstract: We present a revised atomistic picture of the order-disorder transition in
the archetypal orbital-ordered perovskite material, LaMnO$_{3}$. Our study uses
machine-learning-driven molecular-dynamics simulations which describe the
temperature evolution of pair distribution functions in close agreement with
experiment. We find the orbital-disordered phase in LaMnO$_{3}$ to comprise a
mixture of differing structural distortions with and without inversion
symmetry, implying a mixture of different orbital arrangements. These
distortions are highly dynamic with an estimated lifetime of $\sim 40$ fs at
1,000 K, and their fluctuations converge with the timescales of conventional
thermal motion in the high-$T$ phase - indicating that the electronic
instability responsible for static Jahn-Teller distortions at low temperature
instead drives phonon anharmonicity at high temperatures. Beyond LaMnO$_{3}$,
our work opens an avenue for studying a wider range of correlated materials.

</details>


### [178] [Finite-Temperature Ferroelectric Phase Transitions from Machine-Learned Force Fields](https://arxiv.org/abs/2510.25439)
*Kristoffer Eggestad,Ida C. Skogvoll,Øystein Gullbrekken,Benjamin A. D. Williamson,Sverre M. Selbach*

Main category: cond-mat.mtrl-sci

TL;DR: 使用机器学习力场（MLFF）模拟铁电材料的相变，并讨论其潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 研究计算密集型铁电材料的有限温相变问题，探索机器学习力场在第一性原理模拟中的应用。

Method: 使用即时训练的机器学习力场（MLFF）模拟了四种铁电氧化物（BaTiO3，PbTiO3，LiNbO3和BiFeO3）的结构相变，并与实验结果进行了比较。

Result: MLFFs能够定性预测主要的结构相和相变，定量结果对交换相关泛函的选择敏感（PBEsol优于LDA和r2SCAN）。模拟再现了BaTiO3中Ti位移的无序特征，BiFeO3和PbTiO3的突变一阶相变，以及LiNbO3的混合有序-无序和动力学特征。

Conclusion: 机器学习力场为模拟铁电相变提供了一种有前景的计算方法，但其精度和适用性仍受限于训练数据和交换相关泛函的选择。

Abstract: Simulating finite temperature phase transitions from first-principles is
computationally challenging. Recently, molecular dynamics (MD) simulations
using machine-learned force fields (MLFFs) have opened a new avenue for
finite-temperature calculations with near-first-principles accuracy. Here we
use MLFFs, generated using on-the-fly training, to investigate structural phase
transitions in four of the most well-studied ferroelectric oxides; BaTiO$_3$,
PbTiO$_3$, LiNbO$_3$ and BiFeO$_3$. Only using the 0 K ground state structure
as input for the training, the resulting MLFFs can qualitatively predict all
the main structural phases and phase transitions, while the quantitative
results are sensitive to the choice of exchange correlation functional with
PBEsol found to be more robust than LDA and r$^2$SCAN. MD simulations also
reproduce the experimentally observed order-disorder character of Ti
displacements in BaTiO$_3$, the abrupt first order transitions of BiFeO$_3$ and
PbTiO$_3$, and the mixed order-disorder and displacive character of the
ferroelectric transition in LiNbO$_3$. Finally, we discuss the potential and
limitations of using MLFFs for simulating ferroelectric phase transitions.

</details>


### [179] [Combined ab initio and experimental study of phosphorus-based anti-wear additives interacting with iron and iron oxide](https://arxiv.org/abs/2510.25620)
*Francesca Benini,Paolo Restuccia,Sophie Loehlé,Quentin Arnoux,Maria Clelia Righi*

Main category: cond-mat.mtrl-sci

TL;DR: 磷基润滑剂添加剂在金属表面的吸附、稳定性和反应性是影响其性能的关键。本研究通过从头计算和实验分析了辛酸磷酸酯（OAP）、磷酸二丁酯（DBHP）和胺中和酸性磷酸酯（ANAP）在铁和赤铁矿表面的吸附行为和摩擦化学稳定性。结果表明，ANAP在铁上的吸附最强，DBHP次之，OAP较弱，但在赤铁矿上通过失氢增强了化学吸附。在摩擦条件下，DBHP比其他两种磷酸酯更容易解离，这与键级分析一致。石英晶体微量天平（QCM）测量显示DBHP在摩擦条件下形成稳定的沉积物，而ANAP的保留率较低。XPS证实DBHP的强化学吸附和分子解离导致磷沉积增加。OAP虽然形成磷基层，但导致Fe氧化物减少，这与模拟中观察到的脱氢机制一致。这些发现强调了分子结构和氧化态在摩擦膜形成和稳定性中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 磷基润滑剂添加剂的性能很大程度上取决于它们在金属界面的吸附、稳定性和反应性。理解这些因素对于设计高性能润滑剂至关重要。

Method: 本研究结合了从头计算（ab initio calculations）和实验分析（包括石英晶体微量天平QCM和X射线光电子能谱XPS），研究了三种磷基润滑剂添加剂（辛酸磷酸酯OAP、磷酸二丁酯DBHP和胺中和酸性磷酸酯ANAP）在铁和赤铁矿表面的吸附行为和摩擦化学稳定性。

Result: ANAP在铁上的吸附最强，DBHP次之，OAP较弱，但在赤铁矿上通过失氢增强了化学吸附。DBHP比其他两种磷酸酯更容易解离。QCM测量显示DBHP形成稳定的沉积物，而ANAP保留率较低。XPS证实DBHP的强化学吸附和分子解离导致磷沉积增加。OAP导致Fe氧化物减少。分子结构和氧化态在摩擦膜形成和稳定性中起着关键作用。

Conclusion: 本研究揭示了磷基润滑剂添加剂的分子结构和氧化态对其在金属表面的吸附、稳定性和摩擦化学行为具有决定性影响。DBHP因其较低的磷配位而表现出更易解离和形成稳定摩擦膜的特性。理解这些原子级别的相互作用有助于设计在极端条件下性能优越的润滑剂添加剂。

Abstract: The performance of phosphorus-based lubricant additives is governed by their
adsorption, stability, and reactivity at the metal interface. In this study, we
investigate the adsorption behavior and tribochemical stability of three
additives: Octyl Acid Phosphate (OAP), Dibutyl Hydrogen Phosphite (DBHP), and
Amine Neutralized Acid Phosphate (ANAP). These additives are studied on iron
and hematite surfaces using both ab initio calculations and experimental
analyses on steel. Simulations revealed that ANAP exhibited the strongest
adsorption on iron, followed by DBHP, while OAP showed weaker interactions,
though its chemisorption was enhanced on hematite via hydrogen loss. Under
tribological conditions, the DBHP phosphite dissociated more readily than the
other two phosphates molecules due to its lower phosphorus coordination, as
confirmed by bond order analysis. Quartz crystal microbalance (QCM)
measurements indicated significant differences in adsorption behavior across
temperatures, with DBHP forming stable deposits, while ANAP exhibited poor
retention, in agreement with ab initio molecular dynamics simulations. X-ray
photoelectron spectroscopy (XPS) confirmed DBHP's strong chemisorption and
molecular dissociation, leading to increased phosphorus deposition. OAP,
despite forming a phosphorus-based layer, caused a reduction in Fe oxide,
consistent with its hydrogen release mechanism observed in simulations. These
findings highlight the critical role of molecular structure and oxidation state
in tribofilm formation and stability. Understanding these interactions at the
atomic level provides valuable insights for designing high-performance
lubricant additives for extreme operating conditions.

</details>


### [180] [Spin-dependent anisotropic electron-phonon coupling in KTaO$_3$](https://arxiv.org/abs/2510.25655)
*Giulia Venditti,Francesco Macheda,Paolo Barone,José Lorenzana,Maria N. Gastiasoro*

Main category: cond-mat.mtrl-sci

TL;DR: KTO基异质结构中的电子-声子耦合主要由软反常模式介导，并表现出动态各向同性或各向异性Rashba效应。


<details>
  <summary>Details</summary>
Motivation: 研究KTO体材料中电子与声子模式的耦合，特别是与声称介导KTO基异质结构中超导配对的软反常模式的耦合。

Method: 使用相对论密度泛函微扰理论（DFPT）研究KTO体材料中电子与中心奇宇称模式的耦合。开发了一个包含自旋-轨道耦合的t$_{2g}$轨道的简化的三带微观模型。

Result: 电子与软TO模式的耦合最大，由带内和带间过程共同贡献。对于软TO模式，自旋非守恒矩阵元尤为重要。模型重现了第一性原理计算的主要特征。最高能带的耦合可理解为动态各向同性Rashba效应，而较低的两个能带则表现出强烈的各向异性Rashba样耦合。

Conclusion: DFPT方法能够计算投影到任何感兴趣的模式上的完整电子-声子耦合矩阵，并可应用于其他体系。

Abstract: KTaO$_3$ (KTO) is an incipient ferroelectric, characterized by a softening of
the lowest transverse optical (TO) mode with decreasing temperature. Cooper
pairing in the recently discovered KTO-based heterostructures has been proposed
to be mediated by the soft TO mode. Here we study the electron coupling to the
zone-center odd-parity modes of bulk KTO by means of relativistic Density
Functional Perturbation Theory (DFPT). The coupling to the soft TO mode is by
far the largest, with comparable contributions from both intraband and
interband processes. Remarkably, we find that for this mode,
spin-non-conserving matrix elements are particularly relevant. We develop a
three-band microscopic model with spin-orbit coupled $t_{2g}$ orbitals that
reproduces the main features of the ab initio results. For the highest energy
band, the coupling can be understood as a "dynamical" isotropic Rashba effect.
In contrast, for the two lowest bands, the Rashba-like coupling becomes
strongly anisotropic. The DFPT protocol implemented here enables the
calculation of the full electron-phonon coupling matrix projected onto any mode
of interest, and it is easily applicable to other systems.

</details>


### [181] [Dual quantum locking: Dynamic coupling of hydrogen and water sublattices in hydrogen filled ice](https://arxiv.org/abs/2510.25707)
*Loan Renaud,Tomasz Poreba,Simone Di Cataldo,Alasdair Nicholls,Léon Andriambariarijaona,Maria Rescigno,Richard Gaal,Michele Casula,A. Marco Saitta,Livia Eleonora Bove*

Main category: cond-mat.mtrl-sci

TL;DR: 氢包合物（HH）是氢分子被限制在结晶水框架内形成的独特材料。本文研究了HH中压力和温度驱动的相变序列，重点关注分子旋转、取向有序、晶格对称性破缺和氢键对称化之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 氢包合物（HH）的C2相在超短H2-H2O距离和1:1化学计量比下表现出极强的宿主-客体相互作用，两个相同的金刚石状子晶格相互穿插，一个由水分子组成，另一个由氢分子组成。在高压下，涉及氢分子和水格子核的量子效应占主导地位，形成一个双晶格量子系统。

Method: 通过结合经典的和路径积分分子动力学的计算建模、量子嵌入以及高压实验（包括低温和高压下的拉曼光谱和同步加速器X射线衍射），研究了HH中压力和温度驱动的相变序列。

Result: 研究结果表明，HH中的取向有序在远低于固体氢的压力下发生，引起了水网络中的结构变化，并增强了水和氢动力学的耦合。

Conclusion: 这项工作为理解极端力化学限制下氢的量子行为提供了新的见解，并确立了充氢冰作为设计富氢量子材料的有前景的平台。

Abstract: Hydrogen hydrates (HH) are a unique class of materials composed of hydrogen
molecules confined within crystalline water frameworks. Among their multiple
phases, the filled ice structures, particularly the cubic C2 phase, exhibit
exceptionally strong host-guest interactions due to ultra-short H2-H2O
distances and a 1:1 stoichiometry leading to two interpenetrated identical
diamond-like sublattices, one comprised of water molecules, the other of
hydrogen molecules. At high pressures, nuclear quantum effects involving both
hydrogen molecules and the water lattice become dominant, giving rise to a
dual-lattice quantum system. In this work, we explore the sequence of pressure-
and temperature-driven phase transitions in HH, focusing on the interplay
between molecular rotation, orientational ordering, lattice symmetry breaking
and hydrogen bond symmetrization. Using a combination of computational modeling
based on classical and path-integral molecular dynamics,
  quantum embedding, and high pressure experiments, including Raman
spectroscopy and synchrotron X-ray diffraction at low temperatures and high
pressures, we identify signatures of quantum-induced ordering and structural
transformations in the C2 phase. Our findings reveal that orientational
ordering in HH occurs at much lower pressures than in solid hydrogen, by
inducing structural changes in the water network and enhancing the coupling of
water and hydrogen dynamics. This work provides new insights into the quantum
behavior of hydrogen under extreme mechanochemical confinement and establishes
hydrogen-filled ices as a promising platform for the design of hydrogen-rich
quantum materials.

</details>


### [182] [Crystallization Behavior of ZBLAN Glass Under Combined Thermal and Vibrational Effects: Part I -- Experimental Investigation](https://arxiv.org/abs/2510.25748)
*Ayush Subedi,Anthony Torres,Jeff Ganley,Ujjwal Dhakal*

Main category: cond-mat.mtrl-sci

TL;DR: 微重力可抑制ZBLAN玻璃结晶，但常重力下振动影响尚不明确。研究表明，振动可降低结晶起始温度，加速成核动力学，并改变晶体形态，证实了振动直接促进成核作用。


<details>
  <summary>Details</summary>
Motivation: ZBLAN玻璃在红外光纤领域潜力巨大，但其结晶倾向限制了其性能。研究常重力下振动对ZBLAN结晶行为的影响，以期为相关研究提供新见解。

Method: 使用加热和振动装置，系统研究了不同热学和振动条件下ZBLAN玻璃的结晶行为，通过光学显微镜、扫描电镜、能谱分析和原子力显微镜等手段进行表征。

Result: 振动降低了ZBLAN玻璃的结晶起始温度，表明其原子迁移率和成核动力学有所增强。随着温度和振动强度的增加，观察到晶体形态从针状逐渐转变为蝴蝶状和羽毛状。表面粗糙度分析显示，结晶区域的纳米尺度粗糙度显著增加。

Conclusion: 振动是ZBLAN玻璃成核的直接促进因素，而非纯粹的热效应。该研究为后续的建模和设备优化提供了实验基础。

Abstract: ZBLAN glass is a promising material for infrared optical fibers due to its
wide transmission window and low theoretical attenuation. However, its strong
tendency to crystallize during processing limits optical performance. While
microgravity environments have been shown to suppress crystallization, the role
of mechanical vibration under normal gravity conditions remains poorly
understood. This study systematically investigates the influence of vibration
on the crystallization behavior of ZBLAN using a controlled heating and
vibration apparatus. Samples were subjected to varied thermal and vibrational
conditions, and their crystallization onset and morphological evolution were
examined through optical microscopy, scanning electron microscopy (SEM), energy
dispersive X-ray spectroscopy (EDS), and atomic force microscopy (AFM). Results
show that vibration reduces the crystallization onset temperature, indicating
enhanced atomic mobility and nucleation kinetics. Progressive morphological
transitions from needle-like to bow-tie and feather-like crystals were observed
with increasing temperature and vibration intensity. Surface roughness analysis
corroborates these findings, revealing a significant increase in nanoscale
roughness in crystallized regions. Although brief exposure duration and partial
thermal decoupling introduced variability among samples, the overall results
confirm that vibration acts as a direct facilitator of nucleation rather than a
purely thermal effect. This work provides new insight into vibration-induced
crystallization in fluoride glasses and establishes the experimental foundation
for follow-up modeling and apparatus optimization studies under terrestrial and
microgravity conditions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [183] [From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation](https://arxiv.org/abs/2510.24802)
*Qiumeng Li,Chunhou Ji,Xinyue Liu*

Main category: cs.MA

TL;DR: 本文提出了一种名为Narrative-to-Action的分层大语言模型-智能体框架，通过整合高层叙事推理、中层反思规划和低层行为执行，以更好地理解和模拟人类出行行为。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉人类行为的语义连贯性和因果逻辑，而大语言模型在创造性推理和结构性遵守之间难以平衡。本研究旨在解决这一问题。

Method: 该框架包含一个"创意写手"智能体生成日记式叙事，一个"结构解析器"智能体将叙事转化为机器可读计划。动态执行模块结合了新提出的"职业感知度量"（MEO）来指导行为调整，并在环境模拟中执行具体行动。

Result: 该框架不仅能生成与真实世界模式高度吻合的合成轨迹，还能提供可解释的人类决策逻辑表示。

Conclusion: 本研究将合成移动生成从数据驱动范式提升到认知驱动模拟，为通过分层大语言模型智能体理解、预测和合成复杂城市出行行为提供了可扩展的途径。

Abstract: Understanding and replicating human mobility requires not only
spatial-temporal accuracy but also an awareness of the cognitive hierarchy
underlying real-world travel decisions. Traditional agent-based or deep
learning models can reproduce statistical patterns of movement but fail to
capture the semantic coherence and causal logic of human behavior. Large
language models (LLMs) show potential, but struggle to balance creative
reasoning with strict structural compliance. This study proposes a Hierarchical
LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level
narrative reasoning, mid-level reflective planning, and low-level behavioral
execution within a unified cognitive hierarchy. At the macro level, one agent
is employed as a "creative writer" to produce diary-style narratives rich in
motivation and context, then uses another agent as a "structural parser" to
convert narratives into machine-readable plans. A dynamic execution module
further grounds agents in geographic environments and enables adaptive
behavioral adjustments guided by a novel occupation-aware metric, Mobility
Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility
across different occupational personalities. At the micro level, the agent
executes concrete actions-selecting locations, transportation modes, and time
intervals-through interaction with an environmental simulation. By embedding
this multi-layer cognitive process, the framework produces not only synthetic
trajectories that align closely with real-world patterns but also interpretable
representations of human decision logic. This research advances synthetic
mobility generation from a data-driven paradigm to a cognition-driven
simulation, providing a scalable pathway for understanding, predicting, and
synthesizing complex urban mobility behaviors through hierarchical LLM agents.

</details>


### [184] [MASPRM: Multi-Agent System Process Reward Model](https://arxiv.org/abs/2510.24803)
*Milad Yazdani,Mahdi Mostajabdaveh,Zirui Zhou,Ying Xiong*

Main category: cs.MA

TL;DR: MASPRM通过为部分中间记录分配每个代理和每个动作的值来指导推理过程，从而在GSM8K和MATH数据集上显著提高数学推理的精确匹配度，并且可以零样本迁移到新数据集。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，多代理系统（MAS）需要强大的测试时间性能，因此需要能够指导推理搜索并有选择地分配计算资源以提高质量的方法。

Method: 提出了一种名为MASPRM（Multi-Agent System Process Reward Model）的奖励模型。该模型为部分中间代理记录分配每个动作、每个代理的值，并充当推理时的控制器。MASPRM无需进行步骤级别的人工标注，而是通过将回报传播到局部目标来从多代理蒙特卡洛树搜索（MCTS）的卷入中进行训练。在推理时，MASPRM指导步骤级别的束搜索和MCTS，将计算量集中在有希望的分支上并进行早期修剪。

Result: 在GSM8K和MATH数据集上，使用MASPRM指导的解码（并结合最终答案的成果奖励模型ORM）相比于单次直接MAS传递，精确匹配度（EM）分别提高了+30.7和+22.9个百分点。在GSM8K上训练的MASPRM可以零样本迁移到MATH，无需重新训练，在相同的计算预算下可额外带来8.4个EM点。

Conclusion: MASPRM是一个可插入的值模型，可以估计每个代理的进展，并对验证器风格的解码器进行补充，从而实现更可靠、更具计算意识的多代理推理。

Abstract: Practical deployment of Multi-Agent Systems (MAS) demands strong test-time
performance, motivating methods that guide inference-time search and
selectively spend compute to improve quality. We present the Multi-Agent System
Process Reward Model (MASPRM). It assigns per-action, per-agent values to
partial inter-agent transcripts and acts as an inference-time controller.
MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts
without requiring step-level human annotations, by propagating returns to local
targets. At inference, MASPRM guides step-level beam search and MCTS, focusing
computation on promising branches and pruning early. On GSM8K and MATH,
MASPRM-guided decoding with an outcome reward model (ORM) applied to the final
answer, improves exact match (EM) over a single straight-through MAS pass by
$+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers
zero-shot to MATH without retraining, adding $8.4$ EM points at the same
budget. MASPRM is a plug-in value model that estimates per-agent progress and
complements verifier-style decoders, enabling more reliable, compute-aware
multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM

</details>


### [185] [Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems](https://arxiv.org/abs/2510.24909)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 本报告提出了一种计算模型，用于在需求工程的多方协作竞争环境中动态分析信任演变，结合了i*模型和多智能体系统的计算信任模型。


<details>
  <summary>Details</summary>
Motivation: 在需求工程中，多方协作竞争关系中的信任是动态演变的，现有的i*模型在计算信任演变方面存在不足，而多智能体系统的计算信任模型又缺乏需求工程的背景。本报告旨在弥合这一差距。

Method: 提出一种扩展了博弈论基础的计算信任模型，该模型具有两层信任结构（即时信任和声誉），并采用非对称更新机制来模拟信任的动态演变，同时提出结构化翻译框架将i*依赖网络和组织环境转化为计算信任模型。

Result: 实验验证表明，该模型能够稳健地涌现负面偏见、滞后效应和累积损害放大效应。通过对雷诺-日产联盟案例的研究，模型成功复现了记录的信任演变过程，在5个不同的关系阶段（包括危机和恢复期）实现了81.7%的验证点。

Conclusion: 本报告提出的计算信任模型能够有效地在需求工程的多方协作竞争环境中模拟和分析信任的动态演变，并得到了实验和案例研究的验证。

Abstract: Requirements engineering increasingly occurs in multi-stakeholder
environments where organizations simultaneously cooperate and compete, creating
coopetitive relationships in which trust evolves dynamically based on observed
behavior over repeated interactions. While conceptual modeling languages like
i* represent trust relationships qualitatively, they lack computational
mechanisms for analyzing how trust changes with behavioral evidence.
Conversely, computational trust models from multi-agent systems provide
algorithmic updating but lack grounding in requirements engineering contexts
and conceptual models. This technical report bridges this gap by developing a
computational trust model that extends game-theoretic foundations for strategic
coopetition with dynamic trust evolution. We introduce trust as a two-layer
system with immediate trust responding to current behavior and reputation
tracking violation history. Trust evolves through asymmetric updating where
cooperation builds trust gradually while violations erode it sharply, creating
hysteresis effects and trust ceilings that constrain relationship recovery. We
develop a structured translation framework enabling requirements engineers to
instantiate computational trust models from i* dependency networks and
organizational contexts. Comprehensive experimental validation across 78,125
parameter configurations establishes robust emergence of negativity bias,
hysteresis effects, and cumulative damage amplification. Empirical validation
using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60
validation points (81.7%), successfully reproducing documented trust evolution
across five distinct relationship phases including crisis and recovery periods.
This technical report builds upon its foundational companion work in
arXiv:2510.18802.

</details>


### [186] [Emergent Coordinated Behaviors in Networked LLM Agents: Modeling the Strategic Dynamics of Information Operations](https://arxiv.org/abs/2510.25003)
*Gian Marco Orlando,Jinyi Ye,Valerio La Gatta,Mahdi Saeedi,Vincenzo Moscato,Emilio Ferrara,Luca Luceri*

Main category: cs.MA

TL;DR: 生成式 AI 代理可以自动组织并模仿真实世界的信息运营（IO）策略，在模拟环境中表现出显着的协调能力。


<details>
  <summary>Details</summary>
Motivation: 研究部署在线生态系统中的生成式 AI 代理如何进行协调，特别是在信息运营（IO）领域，并评估自动化、自组织 IO 的潜在风险。

Method: 使用生成式代理基建模，在模拟环境中创建 IO 代理和自然代理，并在不同的操作模式下评估它们的协调能力，包括简单的目标对齐、团队知识和集体决策。

Result: 随着操作模式变得更加结构化，IO 网络变得更加密集和聚集，交互更加互惠和积极，叙事更加同质化，放大更加同步，话题标签的使用更快、更持久。仅暴露共同目标的代理就能实现与明确的审议和集体投票几乎相当的协调水平。

Conclusion: 生成式 AI 代理（即使没有人类的指导）也能够再现真实世界 IO 的协调策略，这凸显了日益自动化的、自组织的 IO 所带来的社会风险。

Abstract: Generative agents are rapidly advancing in sophistication, raising urgent
questions about how they might coordinate when deployed in online ecosystems.
This is particularly consequential in information operations (IOs), influence
campaigns that aim to manipulate public opinion on social media. While
traditional IOs have been orchestrated by human operators and relied on
manually crafted tactics, agentic AI promises to make campaigns more automated,
adaptive, and difficult to detect. This work presents the first systematic
study of emergent coordination among generative agents in simulated IO
campaigns. Using generative agent-based modeling, we instantiate IO and organic
agents in a simulated environment and evaluate coordination across operational
regimes, from simple goal alignment to team knowledge and collective
decision-making. As operational regimes become more structured, IO networks
become denser and more clustered, interactions more reciprocal and positive,
narratives more homogeneous, amplification more synchronized, and hashtag
adoption faster and more sustained. Remarkably, simply revealing to agents
which other agents share their goals can produce coordination levels nearly
equivalent to those achieved through explicit deliberation and collective
voting. Overall, we show that generative agents, even without human guidance,
can reproduce coordination strategies characteristic of real-world IOs,
underscoring the societal risks posed by increasingly automated,
self-organizing IOs.

</details>


### [187] [SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs](https://arxiv.org/abs/2510.25092)
*Weijia Zhang,Zijia Liu,Haoru Li,Haoqi Chen,Jiaxuan You*

Main category: cs.MA

TL;DR: Seeing Eye 是一个模块化框架，通过一个基于代理的小型视觉语言模型（VLM）翻译器，为纯文本大型语言模型（LLM）解锁多模态推理能力。该翻译器作为感知代理，可以调用 OCR 和裁剪等专业工具，并将多模态输入迭代地提炼成针对问题的结构化中间表示（SIRs）。然后将 SIRs 传递给作为推理代理的纯文本 LLM。翻译器和推理器之间进行多轮反馈和交互，从而提取目标视觉细节并获得更自信的答案。在 MMMU 和 MIA-Bench 等知识密集型 VQA 基准测试中，Seeing Eye 显著降低了推理成本，并优于许多更大的端到端 VLM。


<details>
  <summary>Details</summary>
Motivation: 现有的纯文本大型语言模型（LLM）在扩展到多模态任务时能力有限。现有的多模态方法依赖于单一形式的描述，缺乏多样性且难以适应不同的视觉问答（VQA）基准测试，无法有效地传递细粒度的视觉信息。

Method: Seeing Eye 框架引入了一个小型视觉语言模型（VLM）翻译器作为感知代理，它能够调用 OCR 和裁剪等工具，将多模态输入处理成结构化中间表示（SIRs）。然后，这些 SIRs 被传递给作为推理代理的纯文本 LLM。感知代理和推理代理之间通过多轮交互进行反馈，以提取精确的视觉细节。

Result: Seeing Eye 框架在 MMMU 和 MIA-Bench 等知识密集型 VQA 基准测试中，不仅降低了推理成本，而且在性能上超越了许多更大的端到端 VLM。例如，一个结合了 3B 参数视觉翻译器和 8B 参数语言推理器的 Seeing Eye 实例，在处理具有挑战性的知识密集型问题时，其表现优于一个 32B 参数的单体 VLM。

Conclusion: 将感知与推理分离并通过代理信息流进行解耦，为多模态推理提供了一种可扩展且即插即用的解决方案，使强大的纯文本 LLM 能够充分发挥其推理能力。

Abstract: Recent advances in text-only large language models (LLMs), such as
DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models
remain fragile or entirely incapable when extended to multi-modal tasks.
Existing approaches largely rely on single-form captions, which lack diversity
and often fail to adapt across different types of Visual Question Answering
(VQA) benchmarks. As a result, they provide no principled or efficient channel
for transmitting fine-grained visual information. We introduce Seeing Eye, a
modular framework that unlocks multimodal reasoning in text-only LLMs through
an agent-based small VLM translator. This translator acts as a perception
agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively
distill multimodal inputs into structured intermediate representations (SIRs)
tailored to the question. These SIRs are then passed to the text-only LLM,
which serves as a reasoning agent. Crucially, the translator and reasoner
engage in multi-round feedback and interaction, enabling the extraction of
targeted visual details and yielding more confident answers. Experiments on
knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate
that Seeing Eye not only reduces inference cost but also surpasses much larger
end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision
translator with an 8B-parameter language reasoner outperforms a monolithic 32B
VLM on challenging knowledge-based questions. Our results highlight that
decoupling perception from reasoning via agent information flow offers a
scalable and plug-and-play pathway to multimodal reasoning, allowing strong
text-only LLMs to fully leverage their reasoning capabilities. Code is
available at: https://github.com/ulab-uiuc/SeeingEye

</details>


### [188] [Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for Crowdsensing in Disaster Response](https://arxiv.org/abs/2510.25212)
*Lei Han,Jinhao Zhang,Jinhui Liu,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 该论文提出了一种名为 HoCs-MPQ 的异构多智能体在线协同调度算法，用于提高灾后环境信息的收集效率，并验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 灾后环境复杂，现有传感技术（如移动众包）适应性差、感知能力不足、实用性不强，因此需要一种更高效的灾后环境信息收集方法。

Method: HoCs-MPQ 算法通过构建加权无向图来模拟多要素间的协同和冲突关系，并基于多优先级队列迭代求解最大权独立集，从而实现对无人机、车辆和工作人员等时间相关元素的协同感知调度。具体步骤包括：1. 构建加权无向图，量化节点权重并建模节点间的冲突关系。2. 基于迭代局部搜索求解最大权独立集，并利用多优先级队列加速求解过程。

Result: 与 HoCs-GREEDY、HoCs-K-WTA、HoCs-MADL 和 HoCs-MARL 等基线方法相比，HoCs-MPQ 在任务完成率方面分别提高了 54.13%、23.82%、14.12% 和 12.89%，且单次在线自主调度决策的计算时间不超过 3 秒。

Conclusion: HoCs-MPQ 算法能够有效地提高灾后环境信息的收集效率，并在任务完成率和计算效率方面优于现有方法。

Abstract: Frequent natural disasters cause significant losses to human society, and
timely, efficient collection of post-disaster environmental information is the
foundation for effective rescue operations. Due to the extreme complexity of
post-disaster environments, existing sensing technologies such as mobile
crowdsensing suffer from weak environmental adaptability, insufficient
professional sensing capabilities, and poor practicality of sensing solutions.
Therefore, this paper explores a heterogeneous multi-agent online collaborative
scheduling algorithm, HoCs-MPQ, to achieve efficient collection of
post-disaster environmental information. HoCs-MPQ models collaboration and
conflict relationships among multiple elements through weighted undirected
graph construction, and iteratively solves the maximum weight independent set
based on multi-priority queues, ultimately achieving collaborative sensing
scheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1)
HoCs-MPQ constructs weighted undirected graph nodes based on collaborative
relationships among multiple elements and quantifies their weights, then models
the weighted undirected graph based on conflict relationships between nodes;
(2) HoCs-MPQ solves the maximum weight independent set based on iterated local
search, and accelerates the solution process using multi-priority queues.
Finally, we conducted detailed experiments based on extensive real-world and
simulated data. The experiments show that, compared to baseline methods (e.g.,
HoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task
completion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89%
respectively, with computation time for single online autonomous scheduling
decisions not exceeding 3 seconds.

</details>


### [189] [Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork](https://arxiv.org/abs/2510.25340)
*Beiwen Zhang,Yongheng Liang,Hejun Wu*

Main category: cs.MA

TL;DR: MARs是一种用于多方协作强化学习的方法，可以在与未知且不熟悉的其他智能体协同工作时，实现更快的收敛速度和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理与未知伙伴协作的任务时存在局限，特别是当需要与多个互不熟悉的群体进行协调时。现有的方法假设共享的约定，这在多方协作的场景下并不适用。

Method: 提出了一种名为MARs的方法，该方法构建了一个稀疏骨架图，并应用关系建模来捕捉跨群体动态，以实现多方协作。

Result: 在MPE和StarCraft II的实验中，MARs被证明优于现有的MARL和AHT基线方法，并且收敛速度更快。

Conclusion: MARs能够有效地解决多方协作的强化学习问题，在与未知伙伴协作时表现出色，并且具有更快的收敛性。

Abstract: Multi-agent reinforcement learning (MARl) has achieved strong results in
cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc
teamwork (AHT) relaxes this by allowing collaboration with unknown partners,
yet existing variants still presume shared conventions. We introduce
Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate
with multiple mutually unfamiliar groups of uncontrolled teammates. To address
this, we propose MARs, which builds a sparse skeleton graph and applies
relational modeling to capture cross-group dvnamics. Experiments on MPE and
starCralt ll show that MARs outperforms MARL and AHT baselines while converging
faster.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [190] [Formal Verification of a Token Sale Launchpad: A Compositional Approach in Dafny](https://arxiv.org/abs/2510.24798)
*Evgeny Ukhanov*

Main category: cs.LO

TL;DR: 本文利用Dafny对DeFi代币销售启动板的核心逻辑进行了形式化验证，证明了其关键安全和生命周期属性，并解决了精度损失和退款超额等问题。


<details>
  <summary>Details</summary>
Motivation: DeFi系统和智能合约的广泛应用凸显了软件正确性的重要性，因为其中的错误可能导致巨大的经济损失。形式化验证是确保软件行为数学确定性的途径。

Method: 采用一种组合的、自底向上的验证策略，首先证明基本的非线性整数算术属性，然后在此基础上验证包括资产转换、基于时间的折扣和限售退款机制在内的复杂业务逻辑。使用Dafny语言和验证系统进行实现和证明。

Result: 形式化证明了关键的安全和生命周期属性，包括限售退款不会超过用户原始存款，金融计算中的精度损失严格受限，以及用户提款、代币分配、归属和认领等完整的生命周期逻辑的正确性。

Conclusion: 这项工作为应用严格的验证技术构建高保证的金融软件提供了一个全面的案例研究。

Abstract: The proliferation of decentralized financial (DeFi) systems and smart
contracts has underscored the critical need for software correctness. Bugs in
such systems can lead to catastrophic financial losses. Formal verification
offers a path to achieving mathematical certainty about software behavior. This
paper presents the formal verification of the core logic for a token sale
launchpad, implemented and proven correct using the Dafny programming language
and verification system. We detail a compositional, bottom-up verification
strategy, beginning with the proof of fundamental non-linear integer arithmetic
properties, and building upon them to verify complex business logic, including
asset conversion, time-based discounts, and capped-sale refund mechanics. The
principal contributions are the formal proofs of critical safety and lifecycle
properties. Most notably, we prove that refunds in a capped sale can never
exceed the user's original deposit amount, and that the precision loss in
round-trip financial calculations is strictly bounded. Furthermore, we verify
the complete lifecycle logic, including user withdrawals under various sale
mechanics and the correctness of post-sale token allocation, vesting, and
claiming. This work serves as a comprehensive case study in applying rigorous
verification techniques to build high-assurance financial software.

</details>


### [191] [On syntactic concept lattice models for the Lambek calculus and infinitary action logic](https://arxiv.org/abs/2510.24853)
*Stepan L. Kuznetsov*

Main category: cs.LO

TL;DR: Lambek演算の代数意味論における完全性問題を解決するため、Wurm (2017) の構文概念束 (SCL) を拡張し、無限演算論理に対して強力な完全性を証明した。定数（ゼロ、単位、トップ）の扱いについても考察し、Wurm の結果を強化した。


<details>
  <summary>Details</summary>
Motivation: Lambek演算の代数意味論における完全性問題は、追加演算子を導入すると発生する。Wurm (2017) は構文概念束 (SCL) を用いることでこの問題を緩和したが、無限拡張や定数の扱いには課題が残っていた。

Method: Lambek演算の無限拡張（無限演算論理）に対して、Wurm (2017) のSCL意味論を拡張し、強力な完全性を証明した。また、定数（ゼロ、単位、トップ）の扱いについても考察し、Wurm の結果を強化した。

Result: 無限演算論理に対して強力な完全性が証明された。定数（ゼロ、単位、トップ）に関するWurmの結果が強化された。

Conclusion: 提案されたSCL意味論の拡張は、無限演算論理とその定数を含むシステムにおいて、完全性の問題を解決する有望なアプローチである。

Abstract: The linguistic applications of the Lambek calculus suggest its semantics over
algebras of formal languages. A straightforward approach to construct such
semantics indeed yields a brilliant completeness theorem (Pentus 1995).
However, extending the calculus with extra operations ruins completeness. In
order to mitigate this issue, Wurm (2017) introduced a modification of this
semantics, namely, models over syntactic concept lattices (SCLs). We extend
this semantics to the infinitary extension of the Lambek calculus with Kleene
iteration (infinitary action logic), prove strong completeness and some
interesting corollaries. We also discuss issues arising with constants - zero,
unit, top - and provide some strengthenings of Wurm's results towards including
these constants into the systems involved.

</details>


### [192] [Kleene Algebrae, Kleene Modules, and Morita Equivalence](https://arxiv.org/abs/2510.24993)
*Luke Serafin*

Main category: cs.LO

TL;DR: Kleene代数中的模和Morita等价性


<details>
  <summary>Details</summary>
Motivation: 将环理论中的模和Morita等价性概念推广到Kleene代数。

Method: 研究Kleene模和Kleene代数的Morita等价性。

Result: 探索Kleene代数中这些概念的潜在应用。

Conclusion: 希望在Kleene代数中找到与环理论相媲美的强大功能。

Abstract: Modules and the notion of Morita equivalence are foundational to the
classical study of rings. These concepts extend naturally to semirings and then
specialize to Kleene algebrae, and my goal is to investigate Kleene modules and
Morita equivalence of Kleene algebrae in the hope that some of the power seen
in the context of rings may be found in this new context as well.

</details>


### [193] [A proof-theoretic approach to uniform interpolation property of multi-agent modal logic](https://arxiv.org/abs/2510.25394)
*Youan Su*

Main category: cs.LO

TL;DR: 该论文证明了多模态逻辑 K_n, KD_n 和 KT_n 的统一插值性质 (UIP)。


<details>
  <summary>Details</summary>
Motivation: 尽管 UIP 已通过语义方法在多模态逻辑 K_n, KD_n 和 KT_n 中得到建立，但缺乏基于证明论的方法。

Method: 该论文将 B'ilkov'a (2007) 的方法扩展到多模态逻辑 K_n, KD_n 和 KT_n，并提出了一种纯粹的句法算法来确定统一插值公式。

Result: 建立了多模态逻辑 K_n, KD_n 和 KT_n 的 UIP，并展示了如何通过 UIP 对命题变量进行量化，还提供了一种不使用二阶量词的直接论证方法。

Conclusion: 该论文在证明论上成功建立了多模态逻辑 K_n, KD_n 和 KT_n 的 UIP，并为该领域提供了新的见解。

Abstract: Uniform interpolation property (UIP) is a strengthening of Craig
interpolation property. It was first established by Pitts(1992) based on a pure
proof-theoretic method. UIP in multi-modal $\mathbf{K_n}$, $\mathbf{KD_n}$ and
$\mathbf{KT_n}$ logic have been established by semantic approaches, however, a
proof-theoretic approach is still lacking. B\'ilkov\'a (2007) develops the
method in Pitts (1992) to show UIP in classical modal logic $\mathbf{K}$ and
$\mathbf{KT}$. This paper further extends B\'ilkov\'a (2007)'s systems to
establish the UIP in multi-agent modal logic $\mathbf{K_n}$, $\mathbf{KD_n}$
and $\mathbf{KT_n}$. A purely syntactic algorithm is presented to determine a
uniform interpolant formula. It is also shown that quantification over
propositional variables can be modeled by UIP in these systems. Furthermore, a
direct argument to establish UIP without using second-order quantifiers is also
presented.

</details>


### [194] [Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis](https://arxiv.org/abs/2510.25468)
*Emanuele De Angelis,Florian Frohn*

Main category: cs.LO

TL;DR: 该文集是第12届Horn子句用于验证和合成研讨会 (HCVS 2025) 的会后论文集。


<details>
  <summary>Details</summary>
Motivation: HCVS 2025 研讨会的召开。

Method: 整理和发布HCVS 2025研讨会的论文。

Result: 发布了包含HCVS 2025研讨会论文的文集。

Conclusion: HCVS 2025 研讨会已成功举行并发布了会后论文集。

Abstract: This volume contains the post-proceedings of the 12th Workshop on Horn
Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb,
Croatia, on July 22, 2025, as affiliated workshop of the 37th International
Conference on Computer Aided Verification (CAV 2025).

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [195] [Comparative Analysis of Procedural Planet Generators](https://arxiv.org/abs/2510.24764)
*Manuel Zechmann,Helmut Hlavacs*

Main category: cs.GR

TL;DR: 本研究介绍了两种在Godot引擎中实现的实时程序化行星生成器，一种使用分形布朗运动（FBM）和Perlin噪声，另一种改编自Minecraft的分层噪声技术。研究中还包括了四叉树级别的细节（LOD）系统和行星网格生成方案，并通过用户研究（N=15）评估了生成器的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在游戏引擎中开发程序化行星生成器，并比较不同技术的效果。

Method: 实现两种程序化行星生成器：一种基于FBM和Perlin噪声，另一种基于Minecraft的分层噪声。同时，实现了一个四叉树LOD系统和行星网格生成方案。

Result: 成功实现了两种行星生成器，并进行了用户研究，但具体的用户研究结果未在摘要中详述。

Conclusion: 本研究展示了两种程序化行星生成器的实现，并为后续研究提供了基础，具体的研究结果和结论有待在论文中进一步阐述。

Abstract: This paper presents the development of two distinct real-time procedural
planet generators within the Godot engine: one employing Fractal Brownian
Motion (FBM) with Perlin Noise, and another adapting Minecraft-inspired layered
noise techniques. We detail their implementation, including a quadtree-based
Level of Detail (LOD) system and solutions for planetary mesh generation. A
comparative user study (N=15) was conducted where participants explored unique
instances generated by our two algorithms alongside two existing procedural
planet projects.

</details>


### [196] [Off-Centered WoS-Type Solvers with Statistical Weighting](https://arxiv.org/abs/2510.25152)
*Anchang Bao,Jie Xu,Enya Shen,Jianmin Wang*

Main category: cs.GR

TL;DR: 使用统计加权离心WoS类型估计器，通过局部相似性过滤来改进随机PDE求解器，减少了偏差和方差。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统离散化方法在几何处理和图形学中求解偏微分方程（PDE）的局限性，特别是当Green函数被近似时，离心估计器引入的相关的伪影和偏差问题。

Method: 提出了一种统计加权离心WoS类型估计器，利用局部相似性过滤来选择性地组合来自邻近评估点的样本，通过原则性的加权策略来平衡偏差和方差，抑制不可靠的估计器。

Result: 在筛选后的泊松方程和边界条件等多种PDE上实现了持续改进，优于现有的求解器，并且能够自然地扩展到梯度场估计和混合边界问题。

Conclusion: 所提出的统计加权离心WoS类型估计器在求解随机PDE方面取得了显著的改进，能够有效平衡偏差和方差，并能处理更复杂的问题。

Abstract: Stochastic PDE solvers have emerged as a powerful alternative to traditional
discretization-based methods for solving partial differential equations (PDEs),
especially in geometry processing and graphics. While off-centered estimators
enhance sample reuse in WoS-type Monte Carlo solvers, they introduce
correlation artifacts and bias when Green's functions are approximated. In this
paper, we propose a statistically weighted off-centered WoS-type estimator that
leverages local similarity filtering to selectively combine samples across
neighboring evaluation points. Our method balances bias and variance through a
principled weighting strategy that suppresses unreliable estimators. We
demonstrate our approach's effectiveness on various PDEs,including screened
Poisson equations and boundary conditions, achieving consistent improvements
over existing solvers such as vanilla Walk on Spheres, mean value caching, and
boundary value caching. Our method also naturally extends to gradient field
estimation and mixed boundary problems.

</details>


### [197] [Fast and Robust Point Containment Queries on Trimmed Surface](https://arxiv.org/abs/2510.25159)
*Anchang Bao,Enya Shen,Jianmin Wang*

Main category: cs.GR

TL;DR: 我们提出了一种快速、数值稳定的方法来处理有界曲面上的点包含查询，该方法使用递归缠绕数计算和基于椭圆的边界来替代曲线细分，从而实现线性时间评估。对于周期性曲面，我们将修剪曲线提升到通用覆盖空间，以准确计算缠绕数。


<details>
  <summary>Details</summary>
Motivation: 点包含查询在 CAD 建模、实体几何处理和曲面镶嵌中至关重要，但现有方法（如射线投射和广义缠绕数）在鲁棒性和计算效率方面存在局限性。

Method: 提出了一种递归缠绕数计算方案，用基于椭圆的边界代替了昂贵的曲线细分，用于贝塞尔曲线段，实现了线性时间评估。对于周期性曲面，将修剪曲线提升到通用覆盖空间，以实现准确一致的缠绕数计算。

Result: 该方法在鲁棒性和计算效率方面优于现有的缠绕数算法，在处理几何噪声、开放边界和周期性拓扑结构时表现出高鲁棒性，并成功应用于 B-Rep 模型处理和有界曲面镶嵌。

Conclusion: 所提出的方法为有界曲面上的点包含查询提供了一种快速、鲁棒且高效的解决方案，克服了现有方法的局限性。

Abstract: Point containment queries on trimmed surfaces are fundamental to CAD
modeling, solid geometry processing, and surface tessellation. Existing
approaches such as ray casting and generalized winding numbers often face
limitations in robustness and computational efficiency.
  We propose a fast and numerically stable method for performing containment
queries on trimmed surfaces, including those with periodic parameterizations.
Our approach introduces a recursive winding number computation scheme that
replaces costly curve subdivision with an ellipse-based bound for Bezier
segments, enabling linear-time evaluation. For periodic surfaces, we lift
trimming curves to the universal covering space, allowing accurate and
consistent winding number computation even for non-contractible or
discontinuous loops in parameter domain.
  Experiments show that our method achieves substantial speedups over existing
winding-number algorithms while maintaining high robustness in the presence of
geometric noise, open boundaries, and periodic topologies. We further
demonstrate its effectiveness in processing real B-Rep models and in robust
tessellation of trimmed surfaces.

</details>


### [198] [4-Doodle: Text to 3D Sketches that Move!](https://arxiv.org/abs/2510.25319)
*Hao Chen,Jiaqi Wang,Yonggang Qi,Ke Li,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.GR

TL;DR: 该研究提出了一种名为4-Doodle的新型框架，用于从文本生成动态3D草图动画，解决了现有技术在数据集、表示和动画方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏配对数据、草图的结构抽象性质以及对时间连贯性和多视图一致性的动画需求，文本到3D草图动画任务极具挑战性。

Method: 4-Doodle框架利用预训练的图像和视频扩散模型，通过一种双空间蒸馏方案来生成动态3D草图：一个空间使用可微分的贝塞尔曲线来捕获多视图一致的几何形状，另一个空间则通过时间感知的先验来编码运动动态。它采用多视图优化来确保结构对齐，并引入了一个分离形状保持轨迹和变形变化（允许翻转、旋转和关节运动）的结构感知运动模块。

Result: 实验表明，该方法生成的3D草图动画在时间上逼真且结构稳定，在保真度和可控性方面均优于现有基线。

Conclusion: 4-Doodle是第一个用于从文本生成动态3D草图的无训练框架，为更直观、更易于访问的4D内容创作铺平了道路。

Abstract: We present a novel task: text-to-3D sketch animation, which aims to bring
freeform sketches to life in dynamic 3D space. Unlike prior works focused on
photorealistic content generation, we target sparse, stylized, and
view-consistent 3D vector sketches, a lightweight and interpretable medium
well-suited for visual communication and prototyping. However, this task is
very challenging: (i) no paired dataset exists for text and 3D (or 4D)
sketches; (ii) sketches require structural abstraction that is difficult to
model with conventional 3D representations like NeRFs or point clouds; and
(iii) animating such sketches demands temporal coherence and multi-view
consistency, which current pipelines do not address. Therefore, we propose
4-Doodle, the first training-free framework for generating dynamic 3D sketches
from text. It leverages pretrained image and video diffusion models through a
dual-space distillation scheme: one space captures multi-view-consistent
geometry using differentiable B\'ezier curves, while the other encodes motion
dynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),
which optimizes from a single view per step, our multi-view optimization
ensures structural alignment and avoids view ambiguity, critical for sparse
sketches. Furthermore, we introduce a structure-aware motion module that
separates shape-preserving trajectories from deformation-aware changes,
enabling expressive motion such as flipping, rotation, and articulated
movement. Extensive experiments show that our method produces temporally
realistic and structurally stable 3D sketch animations, outperforming existing
baselines in both fidelity and controllability. We hope this work serves as a
step toward more intuitive and accessible 4D content creation.

</details>


### [199] [mitransient: Transient light transport in Mitsuba 3](https://arxiv.org/abs/2510.25660)
*Diego Royo,Jorge Garcia-Pueyo,Miguel Crespo,Óscar Pueyo-Ciutad,Guillermo Enguita,Diego Bielsa*

Main category: cs.GR

TL;DR: mitransient是一个支持时间分辨模拟的Mitsuba 3扩展，用于快速原型设计瞬态成像系统。


<details>
  <summary>Details</summary>
Motivation: mitransient旨在通过添加时间维度来扩展传统渲染，以模拟光飞行时间，从而支持新型瞬态成像系统的快速原型设计，而无需昂贵或难以操作的硬件。

Method: mitransient通过利用Mitsuba 3的JIT功能，在CPU和GPU上运行Python模块，并支持时间分辨偏振跟踪和瞬态可微分渲染。它还包括简化非视线成像的工具。

Result: 该工具支持基于物理的复杂现象模拟，包括各种真实材料和参与介质（如雾或烟），并且可以快速设置包含捕获噪声的真实场景。

Conclusion: mitransient希望通过提供一个易于使用的工具来支持瞬态成像领域的研究社区，以开发新算法。

Abstract: mitransient is a light transport simulation tool that extends Mitsuba 3 with
support for time-resolved simulations. In essence, mitransient extends
conventional rendering by adding a temporal dimension which accounts for the
time of flight of light. This allows rapid prototyping of novel transient
imaging systems without the need of costly or difficult-to-operate hardware.
Our code is trivially easy to install through pip, and consists of Python
modules that can run both in CPU and GPU by leveraging the JIT capabilities of
Mitsuba 3. It provides physically-based simulations of complex phenomena,
including a wide variety of realistic materials and participating media such as
fog or smoke. In addition, we extend Mitsuba 3's functionality to support
time-resolved polarization tracking of light and transient differentiable
rendering. Finally, we also include tools that simplify the use of our
simulations for non-line-of-sight imaging, enabling realistic scene setups with
capture noise to be simulated in just seconds of minutes. Altogether, we hope
that mitransient will support the research community in developing novel
algorithms for transient imaging.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [200] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了一种新的基于推理树结构的查询学习难度度量方法（r-score）和相应的课程学习调度算法（Re-Schedule），以优化基于可验证奖励的强化学习（RLVR）在大型语言模型（LLMs）中的应用，并在数学推理基准测试中取得了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR数据调度方法依赖于基于路径的度量，忽略了查询的推理树结构，导致数据效率和准确性有待提高。

Method: 提出了一种新的度量方法“推理分数”（r-score），该方法基于查询的推理树结构来衡量查询的学习难度。基于r-score，提出了一种名为“推理树调度”（Re-Schedule）的调度算法，该算法构建了一个从结构简单的查询（高r-score）到复杂的查询（低r-score）的课程。

Result: 在六个数学推理基准测试上的实验表明，Re-Schedule显著提高了平均准确性，最高可提升3.2%。

Conclusion: 查询的结构化理解为RLVR数据调度提供了更强大、更原则性的基础。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [201] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 该研究探讨了在存在反馈循环或循环依赖的循环结构因果模型 (SCM) 中进行反事实推理，特别是在进行移位-尺度干预（即软的、策略风格的改变）的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架假设无环结构因果模型（DAG），但许多现实世界系统（如生物系统）包含反馈循环或循环依赖，违反了无环性。因此，研究有环 SCM 中的反事实推理是必要的。

Method: 研究在循环 SCM 中，在进行移位-尺度干预（软的、策略风格的改变，重新缩放和/或移位变量机制）的情况下，进行反事实推理。

Result: 研究在循环 SCM 中，在进行移位-尺度干预的情况下，进行了反事实推理。

Conclusion: 该研究将反事实推理扩展到具有循环依赖的系统，为处理更现实的因果模型提供了新的见解。

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [202] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 该研究提出了一种名为ProFees的基于LLM的框架，用于自动化医疗E/M编码，旨在减轻医生负担、提高计费效率并改善患者护理。


<details>
  <summary>Details</summary>
Motivation: 自动化医疗E/M编码任务，以减轻医生的文档负担，提高计费效率，并最终改善患者护理。

Method: 提出了一种名为ProFees的基于LLM的框架，用于解决现实世界中E/M编码的复杂性。

Result: 在专家精心策划的真实数据集上，ProFees在编码准确性方面比商业CPT E/M编码系统提高了36%以上，比最强的单提示基线提高了近5%。

Conclusion: ProFees框架在解决现实世界中的复杂性方面是有效的，并且在自动化E/M编码方面优于现有系统。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [203] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: LLM在规则驱动的交易系统中无法遵循程序流，本研究提出ASTP方法，通过显式状态追踪和占位符后处理，提高了状态遵循率和计算精度，并使小型模型能匹配大型模型的性能，缩短响应时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在规则驱动的交易系统中无法遵循程序流，导致玩家信任度下降。本研究旨在解决LLM的创造性灵活性与游戏内交易（浏览-报价-审查-确认）的程序化要求之间的核心矛盾。

Method: 提出了一种名为“自回归状态追踪提示”（ASTP）的方法，该方法的核心在于一个策略性编排的提示，迫使LLM明确其状态追踪过程并使其可验证。ASTP要求LLM识别并报告上一轮的预定义状态标签，并通过特定状态的占位符后处理方法确保交易的完整性和准确的计算。

Result: 在300次交易对话的评估中，ASTP实现了超过99%的状态遵循率和99.3%的计算精度。使用小型模型（Gemini-2.5-Flash）结合ASTP和占位符后处理，性能与大型模型（Gemini-2.5-Pro）相当，但响应时间从21.2秒缩短到2.4秒。

Conclusion: ASTP方法提供了一个实用的基础，能够满足商业游戏对实时性和资源限制的要求，同时解决了LLM在遵循交易系统程序流方面的挑战。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [204] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 通过结合过程挖掘技术和相对策略优化，PM4GRPO提高了大型推理模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的训练方法通常以结果为中心，忽略了推理过程本身，这阻碍了多步推理能力的提升。

Method: 提出了一种名为PM4GRPO的方法，该方法利用过程挖掘技术来计算一个符合性奖励，以衡量策略模型的推理过程与预训练教师模型的一致性，并将此奖励与标准的答案/格式奖励相结合，用于策略优化。

Result: 在五个基准测试中，PM4GRPO显著优于现有的基于GRPO的训练方法，证明了其在增强模型推理能力方面的有效性。

Conclusion: 将过程挖掘应用于面向推理的GRPO可以有效地增强策略模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [205] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan,Liang Xie,Churan Zhi,Dian Tu,Chuanqi Shi*

Main category: cs.AI

TL;DR: H3M-SSMoEs是一个创新的超图模型，结合了多模态信息、大语言模型推理和风格化专家混合，能够更准确地预测股票价格，并提升投资表现。


<details>
  <summary>Details</summary>
Motivation: 现有的股票价格预测方法在处理复杂的时序依赖、异构模态以及动态变化的股票间关系方面存在挑战，难以在一个可扩展的框架内统一结构、语义和适应市场状态的建模。H3M-SSMoEs旨在克服这些限制。

Method: H3M-SSMoEs采用了一种新颖的超图模型，包含三个关键创新：1. 多上下文多模态超图（MCMHG），通过局部上下文超图（LCH）捕捉精细的时空动态，通过全局上下文超图（GCH）维持持久的股票间依赖关系，并利用共享的跨模态超边和Jensen-Shannon散度加权机制进行自适应关系学习和跨模态对齐。2. 增强的大语言模型（LLM）推理模块，通过轻量级适配器融合和对齐量化与文本数据，并注入领域金融知识。3. 风格化专家混合（SSMoEs）模型，结合了共享市场专家和行业专家，通过可学习的风格向量实现稀疏激活下的状态感知专业化。

Result: 在三个主要股票市场的广泛实验表明，H3M-SSMoEs在预测准确性和投资表现方面均优于现有最先进的方法，并且具有有效的风险控制能力。

Conclusion: H3M-SSMoEs通过结合多上下文多模态超图、LLM推理和风格化专家混合，有效地解决了股票价格预测中的多重挑战，显著提高了预测精度和投资回报。

Abstract: Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.

</details>


### [206] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: KnowCoder-A1是一个LLM，它通过基于结果的监督和多阶段课程强化学习，在知识库问答（KBQA）方面实现了强大的自主推理能力，在多个数据集上取得了显著的改进，尤其是在GrailQA数据集的零样本子集上。


<details>
  <summary>Details</summary>
Motivation: 现有的KBQA方法通常依赖于对LLM进行推理轨迹的微调，这种方法提供的探索激励有限，未能有效提升LLM的自主推理能力。

Method: KnowCoder-A1采用基于结果的监督和多阶段课程强化学习。首先，通过基于结果的拒绝采样获得高质量的推理轨迹，对LLM进行微调。然后，为了解决基于结果的监督中固有的奖励稀疏性问题，采用多阶段课程强化学习，并逐步增加奖励的难度。

Result: KnowCoder-A1在三个主流数据集上表现优于现有方法。在GrailQA数据集的零样本子集上，KnowCoder-A1取得了高达11.1%的相对提升，同时仅使用了十二分之一的训练数据。

Conclusion: KnowCoder-A1通过基于结果的监督和多阶段课程强化学习，展现了强大的自主推理能力，有效解决了现有KBQA方法的局限性，并在多个数据集上取得了显著的性能提升。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [207] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: Agentic Moderation是一个模型无关的框架，利用专门的代理来防御多模态系统免受越狱攻击，提高了安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 将Agentic方法扩展到安全对齐，以防御多模态系统免受越狱攻击。

Method: 引入Agentic Moderation框架，包含Shield, Responder, Evaluator, Reflector等动态协作代理，进行上下文感知和可解释的审核。

Result: 在五个数据集和四个大型视觉语言模型上，将攻击成功率(ASR)降低了7-19%，保持了稳定的非跟随率(NF)，并将拒绝率(RR)提高了4-20%。

Conclusion: Agentic Moderation提供了一种模块化、可扩展和细粒度的安全执行方法，展示了Agentic系统在自动化安全治理方面的潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [208] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: EneAD框架通过自适应感知模块和鲁棒决策模块，在降低能耗的同时保持自动驾驶的感知准确性和驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶感知计算功耗高、影响续航里程的问题，并克服现有模型压缩技术在模型大小或准确性方面存在的不足。

Method: 提出EneAD框架，包括自适应感知模块（通过管理不同计算消耗的模型、动态调整帧率、使用贝叶斯优化进行参数调优，并引入轻量级分类模型区分感知难度）和鲁棒决策模块（基于强化学习，并设计正则化项增强在感知扰动下的驾驶稳定性）。

Result: 实验证明EneAD框架在能耗和驾驶性能方面均优于现有方法，可将感知能耗降低1.9倍至3.5倍，续航里程提高3.9%至8.5%。

Conclusion: EneAD框架成功实现了在降低自动驾驶能耗的同时，保持甚至提升了感知准确性和驾驶性能。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [209] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 通过利用答案引导LLM进行反事实推理，RAVR框架能够提高LLM在复杂任务中的推理能力，克服仅依赖问题进行推理的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的强化学习（RL）方法依赖于LLM本身能够生成高质量的推理路径，但对于超出其能力范围的任务，这难以实现。本文受到认知科学的启发，提出“为什么是这个答案”比“是什么答案”更容易，因为前者侧重于解释性重建，避免了开放式探索的认知负荷。

Method: 提出RAVR（Reference-Answer-guided Variational Reasoning）框架，该框架利用答案条件推理作为一种变异代理，用于仅基于问题的推理。通过形式化证明，当以答案为条件时，可以提高采样推理路径的期望效用，将难以解决的问题转化为可学习的问题。

Result: 在通用和数学领域进行了实验，结果表明RAVR相比强大的基线模型具有持续的改进。进一步的分析表明，RAVR能够减少犹豫，加强结论的巩固，并促进特定问题的推理策略。

Conclusion: RAVR框架通过利用答案引导LLM进行反事实推理，有效解决了现有RL方法在处理复杂任务时的局限性，并在实验中取得了显著的改进。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [210] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个利用大型语言模型（LLM）和多智能体进化的系统，能够自动从复杂的工业事件日志数据中提取有意义且高性能的特征，提高了模型性能并减少了手动工作。


<details>
  <summary>Details</summary>
Motivation: 现有的自动特征工程方法在可解释性、预定义操作的僵化性以及处理复杂异构数据方面的适应性方面存在局限性。

Method: FELA系统集成了LLM的推理和编码能力，并采用一种受洞察力指导的自进化范式。该系统包含专业的智能体（想法智能体、代码智能体和评论智能体）协同生成、验证和实现新的特征想法。评估智能体则负责总结反馈并更新分层知识库和双记忆系统，以实现持续改进。此外，FELA还引入了一种结合了强化学习和遗传算法原理的智能体进化算法，用于在想法空间中平衡探索和利用。

Result: 在真实工业数据集上的广泛实验表明，FELA能够生成可解释的、与领域相关的特征，显著提高了模型性能，同时减少了手动工作量。

Conclusion: 研究结果强调了基于LLM的多智能体系统作为一种通用框架，在复杂现实环境中的自动化、可解释和自适应特征工程方面的潜力。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [211] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 开发了一个包含3000个诊断对话的数据集PsyCoTalk，以解决精神疾病共病的诊断挑战。


<details>
  <summary>Details</summary>
Motivation: 精神疾病共病诊断复杂且具有临床重要性，需要新的方法来解决。

Method: 利用合成电子病历（EMR）和多智能体对话生成技术，构建了一个名为PsyCoTalk的大规模数据集，包含3000个多轮次诊断对话，该数据集由130多个诊断状态和经过精神科医生验证的对话组成。

Result: PsyCoTalk数据集在对话长度、词元分布和诊断推理策略方面，与真实临床记录具有高度的结构和语言保真度，并得到精神科医生的真实性和诊断有效性确认。

Conclusion: PsyCoTalk数据集为精神疾病共病研究提供了宝贵的资源，能够促进能够进行多疾病精神筛查的模型开发和评估。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [212] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 该研究提出了一种名为CAIR（Counterfactual-based Agent Influence Ranker）的新方法，用于评估大型语言模型（LLM）多智能体系统中各个智能体对最终输出的影响力。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM多智能体系统（AAW）各智能体影响力的研究存在空白，传统的静态结构分析方法不适用于动态的推理过程。

Method: 提出了一种基于反事实分析的方法（CAIR），用于评估AAW中每个智能体对最终输出的影响程度，并识别出最具影响力的智能体。该方法可以进行任务无关的分析，并适用于离线和推理时两种场景。

Result: 在包含30个不同用例和230种不同功能的AAW数据集上进行的评估表明，CAIR能够产生一致的排名，优于基线方法，并能有效提升下游任务的效率和相关性。

Conclusion: CAIR是首个用于评估AAW中智能体影响力的前沿方法，能够为理解和优化AAW提供有价值的见解。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [213] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: GAP框架通过图结构规划实现自主代理的并行和串行工具执行，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理在工具使用方面依赖于顺序推理，未能利用独立子任务的并行性，导致效率低下。

Method: 提出GAP框架，通过图结构规划显式地对任务依赖关系进行建模，以实现自适应的并行和串行工具执行。训练代理基础模型将复杂任务分解为依赖感知的子任务图，并自主确定工具的执行顺序。采用监督微调（SFT）和基于正确性的强化学习（RL）两阶段训练策略。

Result: 在MHQA数据集上的实验表明，GAP显著优于传统的ReAct基线，尤其是在多步检索任务中，并通过智能并行化显著提高了工具调用的效率。

Conclusion: GAP框架通过任务依赖感知的编排，在执行效率和任务准确性方面取得了显著改进。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [214] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: KVDA-UCT通过分组具有可推断值差异的状态和状态-动作对来改进MCTS的样本效率，优于OGA-UCT。


<details>
  <summary>Details</summary>
Motivation: MCTS面临样本效率低的挑战，可以通过聚合统计数据来改进，但现有的OGA-UCT 算法因其对即时奖励的严格要求而受到限制。

Method: 提出了一种名为 KVDA 的新框架，它不要求状态-动作对具有相同的价值，而是要求它们具有可推断的值差异。修改了 OGA-UCT 以使用 KVDA，称为 KVDA-UCT。

Result: KVDA-UCT 能够检测比 OGA-UCT 更多的抽象，并且不需要额外的参数。

Conclusion: KVDA-UCT 在各种确定性环境和参数设置中都优于 OGA-UCT，表明其在提高 MCTS 样本效率方面的有效性。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [215] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: Agentic AI 被分为符号/经典和神经/生成两大范式，通过分析90项研究，探讨了各自的理论基础、应用领域（医疗、金融、机器人）和伦理挑战，强调了混合神经-符号架构的未来趋势。


<details>
  <summary>Details</summary>
Motivation: 区分和梳理Agentic AI领域，克服将现代神经网络模型与过时符号模型混淆的‘概念改造’问题，为研究提供清晰的框架。

Method: 采用PRISMA方法系统性回顾2018-2025年的90项研究，并基于提出的双范式框架（符号/经典 vs 神经/生成）进行分析。

Result: 发现符号系统多用于安全关键领域（如医疗），神经系统多用于数据丰富、自适应环境（如金融）；指出了符号系统治理模型缺失和混合神经-符号架构的必要性。

Conclusion: Agentic AI的未来在于整合而非单一范式的统治，强调了构建适应性强且可靠的混合系统的战略路线图，旨在为未来的研究、开发和政策提供指导。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [216] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 人工智能对齐研究中的工具目标，例如权力寻求和自我保护，通常被视为潜在风险。然而，本文提出了一种替代观点，认为工具目标应被视为可接受和可管理的，而不是需要限制的故障。


<details>
  <summary>Details</summary>
Motivation: 人工智能对齐研究中的工具目标（如权力寻求、自我保护）可能与人类目标冲突，构成风险。现有理论侧重于限制这些目标的症状。

Method: 本文采用亚里士多德的本体论及其现代解释，将先进的AI系统视为具有特定构成效应的（人造）实体，其工具性倾向源于其自身构成而非意外故障。

Result: 将AI的工具性目标视为其构成产生的固有结果，而非故障，表明应侧重于理解、管理和引导这些目标以符合人类利益，而非试图消除它们。

Conclusion: 鉴于AI的工具性目标是其本质的固有属性，我们应采取一种管理和引导的策略，而不是消除策略，以确保AI的工具性目标与人类的利益保持一致。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [217] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 多目标搜索（MOS）是解决需要平衡多个冲突标准的规划和决策问题的统一框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的系统很少只优化一个单一指标，MOS在机器人、交通和运筹学等人工智能应用中越来越受到关注。

Method: 本文对MOS的发展进行了调查。

Result: 对MOS的最新进展进行了回顾，并指出了跨学科的机遇。

Conclusion: 指出了定义MOS新兴前沿的开放性挑战。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [218] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: MTIR-SQL是一个利用多轮工具集成推理和动态反馈来提高Text-to-SQL任务性能的强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法主要依赖静态执行反馈，限制了实时纠错能力。多轮工具调用和动态反馈可以提高模型的适应性和鲁棒性。

Method: 提出MTIR-SQL框架，采用执行感知多轮推理范式，在每个推理步骤中整合数据库执行反馈，实现上下文感知查询生成和逐步优化。该框架扩展了GRPO算法，并增加了轨迹过滤机制，移除了KL损失约束，以解决训练不稳定的问题。

Result: 在BIRD Dev数据集上，4B参数的MTIR-SQL实现了64.4%的准确率；在SPIDER Dev数据集上，实现了84.6%的执行准确率，显著优于现有方法。

Conclusion: MTIR-SQL通过整合多轮工具调用和动态执行反馈，有效提升了Text-to-SQL任务的性能。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [219] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: LLMs can be used to name unnamed predicates in logic rules, improving readability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Unnamed predicates in logic rules, especially from methods like Predicate Invention in Inductive Logic Programming, reduce readability, interpretability, and reusability. This paper aims to address this issue by using LLMs to assign meaningful names.

Method: This paper explores the use of Large Language Models (LLMs) to provide semantically meaningful suggestions for naming unnamed predicates. LLMs are leveraged for their ability to process both natural language and code.

Result: The evaluation on hand-crafted logic rules suggests that LLMs show potential for the task of naming unnamed predicates.

Conclusion: LLMs demonstrate potential in assigning meaningful names to unnamed predicates, thereby improving logic theories.

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [220] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 本研究提出一种基于智能体（agent）的检索增强生成（RAG）架构，以解决金融科技等专业领域RAG系统面临的检索和合成挑战。


<details>
  <summary>Details</summary>
Motivation: 金融科技等专业领域存在领域本体、术语密集和缩略语复杂等问题，给RAG系统的有效检索和合成带来限制。

Method: 提出一种模块化的、由专门的智能体组成的RAG架构，支持智能查询重构、基于关键词提取的迭代子查询分解、上下文缩略语解析以及基于交叉编码器的上下文重排序。

Result: 与标准RAG基线相比，在金融科技领域数据集上的实验结果表明，所提出的智能体RAG系统在检索精度和相关性方面表现更优，但延迟有所增加。

Conclusion: 结构化的、多智能体的方法为提高复杂领域特定设置下的检索鲁棒性提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [221] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 通过结合可验证奖励和生成奖励模型，提出了一种新颖的零强化学习（Zero-RL）范式，用于增强大语言模型（LLMs）在可验证和不可验证领域（如数学、编程和更广泛的任务）的推理能力，并通过长度惩罚来防止奖励作弊。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习（Zero-RL）方法主要关注易于验证奖励的任务，而对于验证不直观的更广泛场景下的推理能力挖掘不足。

Method: 提出了一种新颖的零强化学习（Zero-RL）范式，结合了可验证奖励和生成奖励模型，进行多任务训练，促进了推理能力在不同领域间的迁移。同时，设计了一个平滑的长度惩罚机制来缓解生成奖励模型的奖励作弊问题，鼓励生成更全面的思考过程。

Result: 在 Qwen3-8B-Base 和 Qwen3-14B-Base 模型上的实验表明，所提出的方法在需要大量推理的任务和更一般的任务上都取得了优越的推理性能。

Conclusion: 所提出的多任务零强化学习方法能够有效提升大语言模型在可验证和不可验证场景下的推理能力，并且通过长度惩罚机制可以有效缓解奖励作弊问题，有望在更广泛的应用中发挥作用。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [222] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: MoGE 通过生成关键状态和合成动力学一致的体验来增强被动探索，解决了现有方法的局限性，并在复杂控制任务中取得了显著的样本效率和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的探索方法（主动和被动）存在局限性：主动探索在高维环境中效果不佳，被动探索的样本多样性有限。需要新的方法来改进被动探索。

Method: 提出模型生成探索（MoGE），包括一个基于扩散的模型来生成关键状态，以及一个世界模型来构建关键转换。MoGE 的模块化设计可与现有算法无缝集成。

Result: 在 OpenAI Gym 和 DeepMind Control Suite 的实验中，MoGE 在样本效率和性能方面均取得了显著的改进。

Conclusion: MoGE 有效地结合了探索和策略学习，为复杂控制任务提供了新的解决方案。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [223] [Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2510.25588)
*Eranga Bandara,Ross Gore,Atmaram Yarlagadda,Anita H. Clayton,Preston Samuel,Christopher K. Rhea,Sachin Shetty*

Main category: cs.AI

TL;DR: 通过结合精调大型语言模型（LLM）联盟和OpenAI-gpt-oss推理LLM，提出了一种用于精神疾病临床诊断的决策支持系统，以提高诊断的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的精神疾病诊断主要依赖于精神科医生与患者之间的对话，这种主观过程会导致诊断结果的不一致和可靠性问题。

Method: 利用在精神卫生会话数据集上进行精调的LLM进行诊断预测，并通过OpenAI-gpt-oss推理LLM进行共识决策和优化。提出了一种新颖的LLM代理部署方法来协调LLM联盟与推理LLM之间的通信。

Result: 实验结果表明，该系统具有提高精神卫生评估诊断准确性和鲁棒性的潜力。开发了一个包含三个精调LLM和OpenAI-gpt-oss推理LLM的原型。

Conclusion: 该研究首次将精调LLM联盟与推理LLM相结合，用于临床精神卫生诊断，为下一代人工智能驱动的电子健康系统铺平了道路，旨在标准化精神科诊断。

Abstract: The diagnosis of most mental disorders, including psychiatric evaluations,
primarily depends on dialogues between psychiatrists and patients. This
subjective process can lead to variability in diagnoses across clinicians and
patients, resulting in inconsistencies and challenges in achieving reliable
outcomes. To address these issues and standardize psychiatric diagnoses, we
propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss
Reasoning LLM-enabled Decision Support System for the clinical diagnosis of
mental disorders. Our approach leverages fine-tuned LLMs trained on
conversational datasets involving psychiatrist-patient interactions focused on
mental health conditions (e.g., depression). The diagnostic predictions from
individual models are aggregated through a consensus-based decision-making
process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method
for deploying LLM agents that orchestrate communication between the LLM
consortium and the reasoning LLM, ensuring transparency, reliability, and
responsible AI across the entire diagnostic workflow. Experimental results
demonstrate the transformative potential of combining fine-tuned LLMs with a
reasoning model to create a robust and highly accurate diagnostic system for
mental health assessment. A prototype of the proposed platform, integrating
three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in
collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,
USA. To the best of our knowledge, this work represents the first application
of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical
mental health diagnosis paving the way for next-generation AI-powered eHealth
systems aimed at standardizing psychiatric diagnoses.

</details>


### [224] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang,Terry Ruas,Yijun Tian,Jan Philip Wahle,Daniel Kurzawe,Bela Gipp*

Main category: cs.AI

TL;DR: ALDEN是一个多轮强化学习框架，使视觉语言模型（VLM）能够作为交互式代理，主动导航和理解长而复杂的文档，通过引入新的“获取”动作和有效的奖励机制来解决现有方法的局限性，并在多个基准测试中取得最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理需要分析和整合跨多页信息的长而复杂的文档时存在困难，因为它们依赖于固定的推理模板或僵化的流程，使VLM处于被动地位，并限制了效率和泛化能力。

Method: ALDEN是一个多轮强化学习框架，通过引入一个“获取”动作来直接按索引访问页面，以补充传统的“搜索”动作，从而更好地利用文档结构。它还提出了一种基于规则的跨级别奖励，用于提供回合和标记级别的信号，以及一种视觉-语义锚定机制，通过对偶路径KL散度约束来稳定训练过程中的视觉和文本表示，以解决由长文档中的大量视觉标记引起的训练不稳定性问题。

Result: 在从三个开源数据集构建的语料库上进行训练后，ALDEN在五个长文档基准测试中取得了最先进的性能。

Conclusion: ALDEN标志着从被动文档阅读向能够自主导航和推理长而复杂的视觉文档的代理迈出了一步，为更准确、更高效的长文档理解提供了稳健的途径。

Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.

</details>


### [225] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 使用基于深度强化学习的流动感知PPO+GTrXL算法，实现了城市环境中无人机的最佳导航策略，显著提高了成功率并降低了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市中用于交付和监控的普及，需要一种优化的导航策略来应对复杂的城市环境。

Method: 提出了一种基于深度强化学习的流动感知近端策略优化（PPO）算法，并结合了门控Transformer eXtra Large（GTrXL）架构，该算法能够感知并利用城市三维高保真模拟中的湍流和回流区信息。

Result: 与未使用次级预测任务的PPO+GTrXL、PPO+LSTM以及传统的Zermelo导航算法相比，所提出的算法在成功率（SR）上显著提高，碰撞率（CR）显著降低。

Conclusion: 所提出的方法为在复杂城市环境中实现全新的无人机导航铺平了道路。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [226] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG通过引入基于频率的非三元组边权重来增强检索增强生成，从而提高多跳和关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在处理多跳或关系推理，尤其是在跨文档推理时存在困难。知识图谱虽然能捕捉实体间的关系，但容易丢失非三元组结构的信息。BambooKG旨在解决这些问题。

Method: BambooKG引入了具有频率权重（基于“一起激发，一起连接”的海宾原理）的非三元组边，以减少信息丢失，并支持结构化、多片段推理。

Result: BambooKG在单跳和多跳推理任务上均取得了优于现有解决方案的性能。

Conclusion: BambooKG通过其创新的知识图谱结构，有效减少了信息丢失，提高了RAG在复杂推理任务上的表现。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [227] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: TheraMind是一个用于心理咨询的大型语言模型，通过新颖的双循环架构解决了现有方法在情感理解、适应性策略和长期记忆方面的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型在心理咨询中缺乏情感理解、适应性策略和跨多轮治疗的长期记忆能力的问题，使其更贴近临床实践。

Method: 提出了一种名为TheraMind的双循环架构，包括一个用于战术对话管理和跨轮记忆利用的‘会话内循环’，以及一个用于评估治疗效果和调整后续策略的‘跨会话循环’。

Result: 在模拟环境中，TheraMind在连贯性、灵活性和治疗匹配度等多轮对话指标上表现优于其他方法，验证了其双循环设计的有效性。

Conclusion: TheraMind通过其双循环架构，能够有效模拟出具有战略性、适应性和长期性的心理治疗行为，弥补了现有技术的不足。

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [228] [Merit Network Telescope: Processing and Initial Insights from Nearly 20 Years of Darknet Traffic for Cybersecurity Research](https://arxiv.org/abs/2510.25050)
*Shereen Ismail,Eman Hammad,William Hatcher,Salah Dandan,Ammar Alomari,Michael Spratt*

Main category: cs.SI

TL;DR: 该论文分析了2005年至2025年间由美国 मेरिट नेटवर्क 运营的网络望远镜收集的互联网流量数据，揭示了全球威胁活动的长期趋势和周期性流量高峰。


<details>
  <summary>Details</summary>
Motivation: 分析互联网流量数据以了解全球威胁活动，包括扫描、数据中断和拒绝服务攻击。

Method: 采用粗粒度到细粒度的方法，首先通过元数据子管道提取总体见解，然后通过数据包头子管道进行更详细的分析，以处理近二十年的望远镜数据。

Result: 识别出长期的流量趋势和周期性的流量高峰，其中一些可归因于互联网范围的扫描事件，另一些则可能与拒绝服务活动有关。 重点分析了2024年的流量特征。

Conclusion: 通过对2005年至2025年间收集的网络望远镜数据的分析，得出了关于互联网威胁活动长期趋势和模式的见解。

Abstract: This paper presents an initial longitudinal analysis of unsolicited Internet
traffic collected between 2005 and 2025 by one of the largest and most
persistent network telescopes in the United States, operated by Merit Network.
The dataset provides a unique view into global threat activity as observed
through scanning and backscatter traffic, key indicators of large-scale probing
behavior, data outages, and ongoing denial-of-service (DoS) campaigns. To
process this extensive archive, coarse-to-fine methodology is adopted in which
general insights are first extracted through a resource-efficient metadata
sub-pipeline, followed by a more detailed packet header sub-pipeline for
finer-grained analysis. The methodology establishes two sub-pipelines to enable
scalable processing of nearly two decades of telescope data and supports
multi-level exploration of traffic dynamics. Initial insights highlight
long-term trends and recurring traffic spikes, some attributable to
Internet-wide scanning events and others likely linked to DoS activities.We
present general observations spanning 2006-2024, with a focused analysis of
traffic characteristics during 2024.

</details>


### [229] [MMM-Fact: A Multimodal, Multi-Domain Fact-Checking Dataset with Multi-Level Retrieval Difficulty](https://arxiv.org/abs/2510.25120)
*Wenyan Xu,Dawei Xiang,Tianqi Ding,Weihai Lu*

Main category: cs.SI

TL;DR: MMM-Fact是一个大规模的多模态事实核查基准，包含125,449条包含文本、图像、视频和表格等证据的陈述，跨越1995年至2025年。它引入了检索难度分级，并采用三分类（真实/虚假/信息不足）方案，旨在推动更现实、可扩展和透明的多模态事实核查研究。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查基准在数据模态、时间跨度、证据深度、领域覆盖和文章完整性方面存在不足，无法反映模型在真实世界中的能力。

Method: 构建了一个包含125,449条事实核查陈述的大型数据集MMM-Fact，涵盖1995年至2025年，并为每条陈述提供了完整的事实核查文章以及来自多个来源的多模态证据（文本、图像、视频、表格）。该数据集还根据检索难度将陈述分为三个级别（基础、中级、高级），并采用“真实/虚假/信息不足”的分类方案。

Result: 在MMM-Fact基准上，主流大型语言模型的表现明显逊于现有资源，且随着证据复杂度的增加，模型性能下降。

Conclusion: MMM-Fact提供了一个现实、可扩展且支持透明、可靠、多模态事实核查的基准，能够推动更高级的事实核查能力的研究。

Abstract: Misinformation and disinformation demand fact checking that goes beyond
simple evidence-based reasoning. Existing benchmarks fall short: they are
largely single modality (text-only), span short time horizons, use shallow
evidence, cover domains unevenly, and often omit full articles -- obscuring
models' real-world capability. We present MMM-Fact, a large-scale benchmark of
125,449 fact-checked statements (1995--2025) across multiple domains, each
paired with the full fact-check article and multimodal evidence (text, images,
videos, tables) from four fact-checking sites and one news outlet. To reflect
verification effort, each statement is tagged with a retrieval-difficulty tier
-- Basic (1--5 sources), Intermediate (6--10), and Advanced (>10) -- supporting
fairness-aware evaluation for multi-step, cross-modal reasoning. The dataset
adopts a three-class veracity scheme (true/false/not enough information) and
enables tasks in veracity prediction, explainable fact-checking, complex
evidence aggregation, and longitudinal analysis. Baselines with mainstream LLMs
show MMM-Fact is markedly harder than prior resources, with performance
degrading as evidence complexity rises. MMM-Fact offers a realistic, scalable
benchmark for transparent, reliable, multimodal fact-checking.

</details>


### [230] [Stable Emotional Co-occurrence Patterns Revealed by Network Analysis of Social Media](https://arxiv.org/abs/2510.25204)
*Qianyun Wu,Orr Levy,Yoed N. Kenett,Yukie Sano,Hideki Takayasu,Shlomo Havlin,Misako Takayasu*

Main category: cs.SI

TL;DR: 研究利用网络理论分析社交媒体上的情绪网络，发现情绪网络结构稳定，但某些情绪连接（如紧张相关）在危机期间会增强。


<details>
  <summary>Details</summary>
Motivation: 探讨社交媒体情绪网络在危机和正常时期的演变，以深入了解人类心理。

Method: 利用大规模日本社交媒体数据，结合网络理论，分析情绪概念（词语）的共现来识别和评估情绪之间的联系。

Result: 情绪网络结构在不同情境和时间上保持稳定，但某些情绪连接（如紧张相关）在地震和疫苗接种前期间会显著增强，然而情绪连接的排名基本保持不变。

Conclusion: 情绪共现并非完全基于情境，情绪具有内在的稳定结构。所提出的网络分析框架为分析情绪共现动态提供了一种系统、可扩展的方法，为使用大规模文本数据进行心理学研究开辟了新途径。

Abstract: Examining emotion interactions as an emotion network in social media offers
key insights into human psychology, yet few studies have explored how
fluctuations in such emotion network evolve during crises and normal times.
This study proposes a novel computational approach grounded in network theory,
leveraging large-scale Japanese social media data spanning varied crisis events
(earthquakes and COVID-19 vaccination) and non-crisis periods over the past
decade. Our analysis identifies and evaluates links between emotions through
the co-occurrence of emotion-related concepts (words), revealing a stable
structure of emotion network across situations and over time at the population
level. We find that some emotion links (represented as link strength) such as
emotion links associated with Tension are significantly strengthened during
earthquake and pre-vaccination periods. However, the rank of emotion links
remains highly intact. These findings challenge the assumption that emotion
co-occurrence is context-based and offer a deeper understanding of emotions'
intrinsic structure. Moreover, our network-based framework offers a systematic,
scalable method for analyzing emotion co-occurrence dynamics, opening new
avenues for psychological research using large-scale textual data.

</details>


### [231] [Testing Correlation in Graphs by Counting Bounded Degree Motifs](https://arxiv.org/abs/2510.25289)
*Dong Huang,Pengkun Yang*

Main category: cs.SI

TL;DR: 本研究提出了一种在两个 Erdos-Renyi 图之间检测相关性的多项式时间算法，通过计数有界度图内嵌来克服现有方法的局限性，并在合成和真实网络上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 在复杂数据集中提取有意义的见解，相关性分析是一个基本步骤。本研究旨在解决两个 Erdos-Renyi 图 $G(n,p)$ 之间检测相关性的问题，并将其表述为假设检验问题。

Method: 通过计数有界度图内嵌来开发一个多项式时间检验方法。

Result: 所提出的检验方法对于任何恒定的相关系数 $ho$ 都是有效的，其中边连接概率满足 $pe n^{-2/3}$。该方法克服了需要 $ho e 	ext{sqrt}(\alpha)$ 的限制，其中 $\alpha e 0.338$ 是 Otter 常数，将其扩展到任何恒定的 $ho$。

Conclusion: 有界度图内嵌——在真实网络中普遍存在——使得所提出的统计量既自然又可扩展。通过在合成和真实共引网络上进行验证，进一步证实了该简单的图内嵌家族能有效地捕捉相关性信号并表现出强大的经验性能。

Abstract: Correlation analysis is a fundamental step for extracting meaningful insights
from complex datasets. In this paper, we investigate the problem of detecting
correlation between two Erd\H{o}s-R\'enyi graphs $G(n,p)$, formulated as a
hypothesis testing problem: under the null hypothesis, the two graphs are
independent, while under the alternative hypothesis, they are correlated. We
develop a polynomial-time test by counting bounded degree motifs and prove its
effectiveness for any constant correlation coefficient $\rho$ when the edge
connecting probability satisfies $p\ge n^{-2/3}$. Our results overcome the
limitation requiring $\rho \ge \sqrt{\alpha}$, where $\alpha\approx 0.338$ is
the Otter's constant, extending it to any constant $\rho$. Methodologically,
bounded degree motifs -- ubiquitous in real networks -- make the proposed
statistic both natural and scalable. We also validate our method on synthetic
and real co-citation networks, further confirming that this simple motif family
effectively captures correlation signals and exhibits strong empirical
performance.

</details>


### [232] [Large-Scale Network Embedding in Apache Spark](https://arxiv.org/abs/2106.10620)
*Wenqing Lin*

Main category: cs.SI

TL;DR: 提出一种基于Apache Spark的分布式网络嵌入算法，用于高效处理大规模图，并在推荐系统等任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有网络嵌入方法难以高效处理大规模图，因为图计算成本高且中间结果向量过大，无法在单机上处理。

Method: 通过递归划分图为小型子图，并行计算嵌入，并聚合结果，实现了线性成本。

Result: 该算法能在几小时内处理包含数十亿边的图，速度是现有方法的4倍以上，并在链接预测和节点分类任务上分别提升了4.25%和4.27%。

Conclusion: 该算法在腾讯的两款在线游戏中成功部署，用于好友推荐和物品推荐，运行时间提高了91.11%，评估指标提高了12.80%。

Abstract: Network embedding has been widely used in social recommendation and network
analysis, such as recommendation systems and anomaly detection with graphs.
However, most of previous approaches cannot handle large graphs efficiently,
due to that (i) computation on graphs is often costly and (ii) the size of
graph or the intermediate results of vectors could be prohibitively large,
rendering it difficult to be processed on a single machine. In this paper, we
propose an efficient and effective distributed algorithm for network embedding
on large graphs using Apache Spark, which recursively partitions a graph into
several small-sized subgraphs to capture the internal and external structural
information of nodes, and then computes the network embedding for each subgraph
in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain
the embeddings of nodes in a linear cost. After that, we demonstrate in various
experiments that our proposed approach is able to handle graphs with billions
of edges within a few hours and is at least 4 times faster than the
state-of-the-art approaches. Besides, it achieves up to $4.25\%$ and $4.27\%$
improvements on link prediction and node classification tasks respectively. In
the end, we deploy the proposed algorithms in two online games of Tencent with
the applications of friend recommendation and item recommendation, which
improve the competitors by up to $91.11\%$ in running time and up to $12.80\%$
in the corresponding evaluation metrics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [233] [DIRC-RAG: Accelerating Edge RAG with Robust High-Density and High-Loading-Bandwidth Digital In-ReRAM Computation](https://arxiv.org/abs/2510.25278)
*Kunming Shao,Zhipeng Liao,Jiangnan Yu,Liang Zhao,Qiwei Li,Xijie Huang,Jingyu He,Fengshi Tian,Yi Zou,Xiaomeng Wang,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: 在边缘设备上，DIRCRAG利用数字重构相变存储器（ReRAM）和静态随机存取存储器（SRAM）的计算（DIRC）来加速检索增强生成（RAG），实现了高存储密度、低功耗和低延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的RAG面临高存储、高能耗和高延迟的挑战，而现有的计算内存（CIM）方案在内存密度或计算精度方面存在不足。

Method: 提出DIRCRAG架构，利用高密度多级ReRAM阵列与SRAM单元集成，通过SRAM和差分传感实现ReRAM读出和数字乘加（MAC）运算。采用查询固定（QS）数据流，支持RAG任务，并提出错误优化和错误检测电路以提高精度和鲁棒性。

Result: 在TSMC40nm工艺下，DIRC-RAG实现了5.18Mb/mm2的片上非易失性存储器密度和131 TOPS的吞吐量。4MB检索的延迟为5.6μs/查询，能耗为0.956μJ/查询，同时保持了检索精度。

Conclusion: DIRCRAG架构成功解决了边缘设备RAG的挑战，通过结合高密度ReRAM和SRAM的计算，实现了高效、低功耗和高精度的检索加速。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieval but faces challenges on edge devices
due to high storage, energy, and latency demands. Computing-in-Memory (CIM)
offers a promising solution by storing document embeddings in CIM macros and
enabling in-situ parallel retrievals but is constrained by either low memory
density or limited computational accuracy. To address these challenges, we
present DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital
In-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM
subarray with an SRAM cell, utilizing SRAM and differential sensing for robust
ReRAM readout and digital multiply-accumulate (MAC) operations. By storing all
document embeddings within the CIM macro, DIRC achieves ultra-low-power,
single-cycle data loading, substantially reducing both energy consumption and
latency compared to offchip DRAM. A query-stationary (QS) dataflow is supported
for RAG tasks, minimizing on-chip data movement and reducing SRAM buffer
requirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by
extracting the bit-wise spatial error distribution of the ReRAM subarray and
applying targeted bit-wise data remapping. An error detection circuit is also
implemented to enhance readout resilience against deviceand circuit-level
variations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process
achieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput
of 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\mu}s/query and an
energy consumption of 0.956{\mu}J/query, while maintaining the retrieval
precision.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [234] [Neutrino thermalization via randomization on a quantum processor](https://arxiv.org/abs/2510.24841)
*Oriel Kiss,Ivano Tavernelli,Francesco Tacchino,Denis Lacroix,Alessandro Roggero*

Main category: quant-ph

TL;DR: 中微子在超新星中的演化行为可以通过具有随机耦合的全耦合自旋哈密顿量来模拟。然而，模拟这种两体相互作用的哈密顿量动力学仍然是一个重大挑战，因为具有可控精度的模拟方法所需的量子线路深度至少与系统规模成线性关系，这超出了当前量子设备的性能极限。尽管如此，本研究通过使用随机量子线路作为经验工具来模拟非局域动力学，研究了更大规模系统中的味热化现象。研究表明，可以使用不依赖于系统规模的线路深度来重现热化行为。通过模拟超过一百个量子比特的动力学，我们发现热化时间大约与系统规模的平方根成正比，这与半经典方法预测一致。这项研究不仅揭示了近期的量子设备可用于检验和验证经验性经典方法，还为随机线路在物理学领域开辟了新的应用途径，为理解经典上难以处理的复杂多体动力学提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 中微子在超新星中的味演化动力学研究

Method: 使用随机量子线路模拟非局域动力学，研究味热化现象

Result: 味热化时间随系统规模平方根增长，与半经典方法预测一致

Conclusion: 近期的量子设备可用于检验和验证经验性经典方法，并为理解复杂多体动力学提供了新途径

Abstract: The dynamical evolution of neutrino flavor in supernovae can be modeled by an
all-to-all spin Hamiltonian with random couplings. Simulating such two-local
Hamiltonian dynamics remains a major challenge, as methods with controllable
accuracy require circuit depths that increase at least linearly with system
size, exceeding the capabilities of current quantum devices. The eigenstate
thermalization hypothesis predicts that these systems should thermalize, a
behavior confirmed in small-scale classical simulations. In this work, we
investigate flavor thermalization in much larger systems using random quantum
circuits as an empirical tool to emulate the non-local dynamics, and
demonstrate that the thermal behavior can be reproduced using a depth
independent of the system size. By simulating dynamics of over one hundred
qubits, we find that the thermalization time grows approximately as the square
root of the system size, consistent with predictions from semi-classical
methods. Beyond this specific result, our study illustrates that near-term
quantum devices are useful tools to test and validate empirical classical
methods. It also highlights a new application of random circuits in physics,
providing insight into complex many-body dynamics that are classically
intractable.

</details>


### [235] [Frustration-Free Control and Absorbing-State Transport in Entangled State Preparation](https://arxiv.org/abs/2510.24845)
*T. Dörstel,T. Iadecola,J. H. Wilson,M. Buchhold*

Main category: quant-ph

TL;DR: 该研究提出了一个名为“无挫优控”（frustration-free control）的测量反馈协议，用于制备量子多体系统中的高度纠缠目标态。该协议通过最小的局域幺正修正，将系统驱动到与所有测量投影兼容的暗态，实现了无须后选择的吸收态动力学。研究表明，系统弛豫到目标态的动力学过程由非局域荷（例如SU(2)对称动力学中的单重激发）的涌现输运所控制。测量和 the scrambling 幺正操作会诱导荷的输运，从而影响收敛时间。通过将一个包含SU(N) SWAP测量和局域修正的基准模型映射到一个可解的吸收随机游走模型，得到其运行时标度的输运指数z=2。对Motzkin链和Fredkin链的模拟显示出次扩散（subdiffusive）标度z ≥ 8/3，这证实了输运图像，并为受控纠缠态制备以及在受监控的量子动力学中探测荷输运提供了策略。


<details>
  <summary>Details</summary>
Motivation: 研究量子态制备的测量反馈协议，将无挫优哈密顿量的概念扩展到随机动力学，以驱动多体系统进入高度纠缠的目标态。

Method: 提出一种无挫优控制协议，通过最小的局域幺正修正实现吸收态动力学，并研究了非局域荷的输运在系统弛豫到目标态中的作用。通过将模型映射到随机游走模型来分析运行时标度，并通过模拟Motzkin链和Fredkin链来验证理论。

Result: 发现系统弛豫到目标态的动力学由非局域荷的涌现输运控制。基准模型的运行时标度为t ~ L^z，其中z=2。Motzkin链和Fredkin链的模拟显示出次扩散标度z ≥ 8/3。

Conclusion: 测量反馈协议可以有效地制备高度纠缠态，并且系统动力学可以通过非局域荷的输运来理解。研究结果为在受监控的量子动力学中制备纠缠态和探测荷输运提供了新的见解和策略。

Abstract: We study frustration-free control, a measurement-feedback protocol for
quantum state preparation that extends the concept of frustration-free
Hamiltonians to stochastic dynamics. The protocol drives many-body systems into
highly entangled target states, common dark states of all measurement
projectors, through minimal local unitary corrections that realize an
absorbing-state dynamics without post-selection. We show that relaxation to the
target state is governed by emergent transport of nonlocal charges, such as
singlet excitations in SU$(2)$-symmetric dynamics. While measurement-feedback
annihilates compatible charge configurations, both measurement and scrambling
unitaries induce charge transport and thus determine the convergence time.
Mapping a baseline model of SU$(N)$ SWAP measurements with local corrections to
a solvable absorbing random walk yields a runtime scaling $t \sim L^z$ with
transport exponent $z=2$. Simulations of Motzkin and Fredkin chains reveal
subdiffusive scaling $z \ge \tfrac{8}{3}$, confirming the transport picture and
suggesting strategies for controlled entangled-state preparation and
charge-transport probing in monitored quantum dynamics.

</details>


### [236] [Pairing-induced phase transition in the non-reciprocal Kitaev chain](https://arxiv.org/abs/2510.24851)
*Pietro Brighi,Andreas Nunnenkamp*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Investigating the robustness of non-reciprocity in the presence of competing
interactions is central to understanding non-reciprocal quantum matter. In this
work, we use reservoir engineering to induce non-reciprocal hopping and pairing
in the fermionic Kitaev chain, and reveal the emergence of a pairing-induced
phase transition. The two phases appear in the spectrum of the non-Hermitian
Kitaev Hamiltonian describing the dynamics of correlations, separated by an
exceptional point. In the non-reciprocal phase, dynamics are characterized by
directionality and slow relaxation, and the steady state supports
non-reciprocal density and spatial correlations. At strong pairing, we uncover
an unexpected density wave phase, featuring short relaxation times, a
modulation in particle occupation and strikingly different correlation
spreading depending on pairing non-reciprocity. Our work highlights the
non-trivial breakdown of non-reciprocity due to superconducting pairing and
invites experimental investigation of non-reciprocal fermionic systems.

</details>


### [237] [Structure, Optimality, and Symmetry in Shadow Unitary Inversion](https://arxiv.org/abs/2510.24880)
*Guocheng Zhen,Yu-Ao Chen,Mingrui Jing,Jingu Xie,Ranyiliu Chen,Xin Wang*

Main category: quant-ph

TL;DR: 本论文研究了量子计算中未知幺正操作的逆操作问题，并提出了“阴影幺正逆”的概念和解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的量子计算研究主要集中在未知幺正操作的逆映射，但对于“阴影幺正逆”（即关于给定可观测量来逆转幺正操作）的研究较少。

Method: 1. 提出了一个确定性的协议，用于对量子比特幺正操作进行阴影逆操作，该协议需要对幺正操作进行3次查询，并认为这是最优的。
2. 完整地表征了在任何固定量子比特可观测量下，可行的量子操作。
3. 提出了一个半定规划框架，用于优化量子比特的阴影幺正逆操作协议，并利用表示论的工具来处理高维情况。

Result: 1. 提出了最优的确定性协议，用于量子比特的阴影幺正逆操作。
2. 提供了量子比特阴影幺正逆操作的可行量子操作的完整表征。
3. 提出了一个用于量子比特阴影幺正逆操作的半定规划框架。

Conclusion: 本研究系统地研究了阴影幺正逆操作问题，提出了显式协议和优化方法，为量子计算中的相关研究提供了新的方向和工具。

Abstract: The ability to reverse any unknown unitary operation plays a fundamental role
in quantum computing. While existing studies mostly focus on realizing the
inversion map of the unknown unitary, how to reverse a unitary with respect to
a given observable, which we call shadow unitary inversion, has remained a
natural basic question that is less developed. In this work, we systematically
investigate shadow unitary inversion by providing explicit protocols and
optimization problem simplification. First, we present a deterministic protocol
for shadow inversion of qubit-unitaries. Such construction sequentially queries
the unitary 3 times, which is suggested to be optimal by our numerical
experiments. Second, we provide a complete characterization of feasible quantum
operations for qubit shadow inversion under any fixed qubit observable. Third,
for the qudit case, we give a framework of semidefinite programming for
optimizing the shadow unitary inversion sequential protocol for tackling
high-dimensional cases, utilizing tools from representation theory.

</details>


### [238] [Quantifying Unxtendibility via Virtual State Extension](https://arxiv.org/abs/2510.24895)
*Hongshun Yao,Jingu Xie,Xuanqiang Zhao,Chengkai Zhu,Ranyiliu Chen,Xin Wang*

Main category: quant-ph

TL;DR: 我们提出了一种新的操作方法来量化状态的可扩展性，并将其与量子通信中的虚拟广播联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究纠缠的单调性，并提出一种新的可扩展性量化方法。

Method: 定义虚拟扩展成本，并推导出各向同性状态的精确表达式。利用部分转置排列矩阵代数来处理广播协议。

Result: 发现虚拟扩展成本与最大纠缠状态的通用虚拟量子广播模拟成本相等。为广播协议构建了显式量子电路，并得到了该成本的解析公式。

Conclusion: 虚拟扩展成本与不可扩展性的绝对鲁棒性相关，并提供了其操作意义。虚拟扩展成本可作为纠缠量度，并与对数负相关。

Abstract: The monogamy of entanglement, which restricts how entanglement can be shared
among multiple parties, provides a powerful lens through which to characterize
its structure, often formalized through the concept of symmetric extendibility.
In this work, we propose a novel, operational viewpoint to quantify the degree
to which a state is not extendible by introducing a virtual state extension
task. We define the virtual extension cost as the minimum simulation cost of a
non-physical protocol that satisfies the marginal conditions of a
$k$-extension. We derive an exact, closed-form expression for this cost for the
important family of isotropic states. Our central result establishes a profound
connection between entanglement theory and quantum communication theory: the
virtual extension cost of a maximally entangled state is precisely equal to the
optimal simulation cost of universal virtual quantum broadcasting. Leveraging
the algebraic machinery of the partially transposed permutations matrix
algebra, we find an analytical formula for this cost and construct an explicit
quantum circuit for the optimal broadcasting protocol, thereby resolving an
open question. Furthermore, we demonstrate the connection between the virtual
extension cost and the absolute robustness of unextendibility, thus endowing
the latter with a clear operational meaning. We also show the natural
properties of virtual extension cost as an entanglement measure, including its
role as a bound for entanglement distillation and its relationship with
logarithmic negativity. Our findings unify disparate concepts and offer a new
perspective on entanglement quantification.

</details>


### [239] [Classically Prepared, Quantumly Evolved: Hybrid Algorithm for Molecular Spectra](https://arxiv.org/abs/2510.24911)
*Alessandro Santini,Stefano Barison,Filippo Vicentini*

Main category: quant-ph

TL;DR: We present a hybrid classical-quantum algorithm for calculating dynamical correlation functions and excitation spectra in molecular systems. It uses classical preparation and short-time quantum evolution to define an effective subspace, enabling efficient classical simulation of long-time dynamics and high-resolution spectral reconstruction with shallow circuits. Benchmarks show excellent agreement with exact methods and access to longer timescales than purely classical approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a method for computing dynamical correlation functions and excitation spectra in many-body quantum systems, particularly molecular systems, that can leverage near-term quantum hardware.

Method: The proposed method is a hybrid classical-quantum algorithm. It involves classical preparation of a perturbed ground state, followed by short-time quantum evolution of product states sampled from this ground state. These quantum samples define an effective subspace of the Hilbert space. The Hamiltonian is then projected onto this subspace, allowing for efficient classical simulation of long-time dynamics.

Result: Benchmarks on molecular systems show that the algorithm achieves high-resolution spectral reconstruction using shallow circuits and few samples. The results demonstrate excellent agreement with exact diagonalization and indicate that the method can access dynamical timescales that are out of reach for purely classical methods.

Conclusion: The hybrid classical-quantum algorithm is well-suited for near-term and early fault-tolerant quantum hardware, offering an efficient and accurate approach to simulating the dynamics of molecular systems and accessing spectral properties.

Abstract: We introduce a hybrid classical-quantum algorithm to compute dynamical
correlation functions and excitation spectra in many-body quantum systems, with
a focus on molecular systems. The method combines classical preparation of a
perturbed ground state with short-time quantum evolution of product states
sampled from it. The resulting quantum samples define an effective subspace of
the Hilbert space, onto which the Hamiltonian is projected to enable efficient
classical simulation of long-time dynamics. This subspace-based approach
achieves high-resolution spectral reconstruction using shallow circuits and few
samples. Benchmarks on molecular systems show excellent agreement with exact
diagonalization and demonstrate access to dynamical timescales beyond the reach
of purely classical methods, highlighting its suitability for near-term and
early fault-tolerant quantum hardware.

</details>


### [240] [Universal Limits on Quantum Correlations](https://arxiv.org/abs/2510.24950)
*Samuel Alperin*

Main category: quant-ph

TL;DR: 本文提出一个基于量子态空间正性的统一几何框架，用于推导量子关联的普适性界限，并发现新的界限。


<details>
  <summary>Details</summary>
Motivation: 现有的量子关联界限（如Cramer-Rao不等式、Heisenberg极限、Lieb-Robinson界限）仅适用于特定系统或可观测对象，缺乏普适性。

Method: 提出一个基于量子态空间正性的几何框架，该框架定义了一个独特的行列式比不变量“chi”，用于量化任意量子系统的关联组合结构。所有非经典关联的度量都受chi的函数界定。

Result: 从该框架中可以推导出所有已知的关联界限，并发现新的界限，例如多模压缩网络中的精确纠缠下限，以及全连接自旋系综中的普适Fisher信息上限。此外，还发现所有界限都具有局部灾变论结构，并能通过通用的临界指数对其逼近饱和的行为进行分类。

Conclusion: 量子态空间的“正性几何”提供了一个统一的、第一性原理的量子界限理论。

Abstract: The fundamental limits of quantum correlations set the foundation of quantum
mechanics and quantum information science. Exact bounds-the Cramer-Rao
inequality, the Heisenberg limit, and the Lieb-Robinson bound-have anchored
entire fields, yet each applies only to a narrow class of systems or
observables. Here we introduce a general framework from which all known
correlation limits, as well as new ones, can be derived from a single geometric
principle: the positivity of quantum state space. This intrinsic positive
geometry defines a unique determinant-ratio invariant, denoted chi, which
quantifies the combinatorial structure of correlations in any quantum system.
Every measure of nonclassical correlation is bounded by a simple function of
chi, yielding universal, model-independent floors and ceilings valid for
arbitrary architectures. For systems with Lie-group symmetries, the bounds
acquire compact closed forms. We recover the Heisenberg and Cramer-Rao limits
and uncover previously unknown constraints, including an exact entanglement
floor in multimode squeezing networks and a universal Fisher-information
ceiling in fully connected spin ensembles-demonstrating that even all-to-all
connectivity cannot exceed the positivity-imposed light cone in state space.
Finally, we show that every correlation bound, old or new, exhibits local
catastrophe-theoretic structure, with universal critical exponents classifying
its approach to saturation. Positivity geometry thus provides a unified,
first-principles theory of quantum limits.

</details>


### [241] [Engineering chlorine-based emitters in silicon carbide for telecom-band quantum technologies](https://arxiv.org/abs/2510.25008)
*A. N. Anisimov,A. V. Mathews,K. Mavridou,U. Kentsch,M. Helm,G. V. Astakhov*

Main category: quant-ph

TL;DR: 我们实现了4H-SiC中的氯空位（ClV）色心，其发射在光纤通信波段，并进行了光学表征。


<details>
  <summary>Details</summary>
Motivation: 开发可在CMOS兼容平台上制造且在光纤通信波段发光的色心，以支持可扩展的量子网络。

Method: 通过氯离子注入和高温退火在4H-SiC中创建ClV色心，并使用光致发光光谱进行表征。

Result: 发现了四种不同的ClV色心，其零声子线（ZPL）位于O波段、S波段和C波段。实验证实这些色心源于氯的掺入，而非SiC的本征缺陷。优化了ClV的产生条件，并在30 K的温度下，ZPL强度仅有可忽略的降低。

Conclusion: ClV色心是光纤通信波段的一种新型色心，具有在CMOS兼容平台上的潜力，可用于构建可扩展的量子网络。

Abstract: We report the experimental realization and optical characterization of
chlorine-vacancy (ClV) color centers in 4H-SiC emitting in the fiber-optic
telecom bands. These defects are created via chlorine ion implantation followed
by high-temperature annealing. Photoluminescence spectroscopy reveals four
distinct ClV configurations with zero-phonon lines (ZPLs) located in the O-band
(1260 - 1360 nm), S-band (1460 - 1530 nm) and C-band (1530 - 1565 nm).
Controlled implantation and annealing experiments confirm that the ClV centers
originate specifically from chlorine incorporation into SiC and are not
intrinsic to this material. We optimize the creation conditions for ClV
ensembles and demonstrate negligible reduction of the ZPL intensity up to a
temperature of 30 K. These results establish ClV defects as a new class of
telecom-band color centers in a CMOS-compatible platform, offering strong
potential for scalable quantum networks.

</details>


### [242] [Properties and Applications of Partially Deterministic Polytopes](https://arxiv.org/abs/2510.25127)
*Marwan Haddara,Howard M. Wiseman,Eric G. Cavalcanti*

Main category: quant-ph

TL;DR: 该论文探讨了“部分确定性”模型，允许在贝尔实验中，只有一部分测量结果由局部隐变量预先确定，而非全部。研究了这种模型在任意数量参与方、输入和输出下的情况，并提出了新的凸多面体类，它们包含了贝尔多面体和无信号多面体作为特例。论文还对部分确定性模型进行了分类，发现了贝尔多面体可以用多种方式表示，并推广了Fine定理。此外，研究还讨论了部分确定性模型在量子态不可分性、广播局部多面体和量子基础的局部友善性场景等方面的应用，并指出其与顺序扩展Wigner友人场景的一一对应关系。最后，论文将部分确定性多面体框架推广到更广泛的“可组合集”概念。


<details>
  <summary>Details</summary>
Motivation: 贝尔实验的局部隐变量模型假设所有测量结果都由局部隐变量预先确定。本研究旨在放松这一假设，探讨仅部分测量结果被预先确定的“部分确定性”模型，以更全面地理解贝尔本地性。

Method: 研究了包含任意数量参与方、输入和输出的部分确定性模型。推导了相应的凸多面体结构，并研究了它们与贝尔多面体和无信号多面体的关系。对部分确定性模型进行了分类，并推广了Fine定理。讨论了部分确定性模型在不同物理场景下的应用，并将其与顺序扩展Wigner友人场景联系起来。最后，将部分确定性多面体框架推广到“可组合集”。

Result: 提出了新的凸多面体类，它们是贝尔多面体和无信号多面体的推广。发现了部分确定性模型存在非平凡的等价类，并进行了完全分类。证明了贝尔多面体可以以多种方式由局部部分确定性模型表示。推广了Fine定理，得到了新的约束条件。在局部友善性场景下，发现了部分确定性多面体与顺序扩展Wigner友人场景之间的一一对应关系。识别出比部分确定性多面体更广泛的“可组合集”概念。

Conclusion: 部分确定性模型提供了一种更灵活的框架来研究贝尔实验中的非经典关联，它包含了经典和量子关联的更广泛范围。该框架在量子信息科学的多个分支中具有应用价值，并与量子基础中的一些核心问题紧密相关。

Abstract: The assumption of a deterministic local hidden variable model constrains the
experimentally accessible statistics in a Bell experiment to be contained in
the Bell-local polytope. But what if the outputs for only a subset of the
measurements at each site are predetermined by the model? In this work, we
thoroughly explore this concept of `partial determinism', allowing for
arbitrary numbers of parties, inputs and outputs per site. The resulting
objects form new classes of convex polytopes which recover the Bell and the
no-signalling polytopes as special cases. Nontrivial equivalence classes of
partially deterministic models arise, which we classify completely. In
particular, the Bell polytope for any scenario can be expressed in multiple
different ways in terms of local partially deterministic models. This allows us
to generalise Fine's theorem, recovering the original formulation as a special
case, but finding new constraints otherwise. We discuss scenarios with
different physical motivations, which do not require the causal structure of
the Bell scenario, and where classes of partially deterministic polytopes are
relevant. Our example applications include device-independent quantum state
inseparability witnesses, classes of broadcast-local polytopes, and Local
Friendliness scenarios in quantum foundations. We also point out instances in
previous literature where classes of related objects have been studied. In the
case of correlations compatible with the Local Friendliness assumptions, we
find a one-to-one correspondence between partially deterministic polytopes and
sequential extended Wigner's friend scenarios so that every partially
deterministic polytope has physical relevance. We discuss how the framework
captures a broad class of non-classicality notions, and identify an even
broader notion of `composable sets', of which partially deterministic polytopes
are special cases.

</details>


### [243] [The Phase-Coupled Caldeira-Leggett Model: Non-Markovian Open Quantum Dynamics beyond Linear Dissipation](https://arxiv.org/abs/2510.25133)
*Ao-Xiang Chang,Yu Su,Zi-Fan Zhu,Yao Wang,Rui-Xue Xu,YiJing Yan*

Main category: quant-ph

TL;DR: PCL模型通过指数耦合统一了量子耗散和极化子物理学，并提供了一种非微扰的动力学研究方法。


<details>
  <summary>Details</summary>
Motivation: 介绍PCL模型以研究超出线性响应的相介导耗散和退相干。

Method: 开发PCL模型的精确动力学框架，利用耗散拟粒子代数进行非微扰和非马尔可夫处理，并推导出约化密度算符的精确闭合方程。

Result: PCL模型展示了与传统模型显著不同的动力学行为。

Conclusion: PCL模型为研究相介导的量子耗散提供了一个新平台，其动力学行为与传统模型有显著差异。

Abstract: We introduce the \textit{Phase-Coupled Caldeira-Leggett} (PCL) model of
quantum dissipation and develop an exact framework for its dynamics. Unlike the
conventional Caldeira-Leggett model with linear system-bath coupling
$H_{\mathrm{SB}}\propto\hat F$, the PCL model features an exponential
interaction $H_{\mathrm{SB}}\propto e^{i\lambda \hat F}$, where $\hat F$
denotes the collective bath coordinate. This model unifies concepts from
quantum Brownian motion and polaron physics, providing a general platform to
study phase-mediated dissipation and decoherence beyond the linear-response
regime. Despite its nonlinear system-bath coupling, the Gaussian nature of the
environment allows a nonperturbative and non-Markovian treatment of PCL model
within the algebra of dissipative quasiparticles. We obtain an exact
closed-form equation of motion for the reduced density operator, and numerical
simulations reveal distinctive dynamical behaviors that deviate markedly from
those predicted by the conventional Caldeira-Leggett model.

</details>


### [244] [Sustainable NARMA-10 Benchmarking for Quantum Reservoir Computing](https://arxiv.org/abs/2510.25183)
*Avyay Kodali,Priyanshi Singh,Pranay Pandey,Krishna Bhatia,Shalini Devendrababu,Srinjoy Ganguly*

Main category: quant-ph

TL;DR: 量子储层计算（QRC）在NARMA-10任务上表现出与经典模型（ESN、LSTM）和混合模型（QLSTM）相当的预测准确性，同时在计算成本和评估时间方面具有优势，尤其是在资源受限环境下，显示出在可持续时间序列人工智能应用方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在比较量子储层计算（QRC）与几种经典的（ESN、LSTM）和混合（QLSTM）模型在非线性自回归移动平均任务（NARMA-10）上的表现，以评估其预测准确性、计算成本和评估时间。

Method: 该研究采用了NARMA-10任务，将QRC与ESN、LSTM和QLSTM进行比较，并评估了预测准确性（NRMSE）、计算成本和评估时间。

Result: QRC在NARMA-10任务上实现了具有竞争力的准确性，并且在计算成本和评估时间方面优于其他模型，特别是在资源受限的环境下，这表明了其在可持续时间序列人工智能应用方面的潜力。

Conclusion: QRC在处理时间序列数据方面具有潜力，尤其是在可持续性和资源效率方面，这使其成为未来人工智能应用的一个有前景的选择。

Abstract: This study compares Quantum Reservoir Computing (QRC) with classical models
such as Echo State Networks (ESNs) and Long Short-Term Memory networks (LSTMs),
as well as hybrid quantum-classical architectures (QLSTM), for the nonlinear
autoregressive moving average task (NARMA-10). We evaluate forecasting accuracy
(NRMSE), computational cost, and evaluation time. Results show that QRC
achieves competitive accuracy while offering potential sustainability
advantages, particularly in resource-constrained settings, highlighting its
promise for sustainable time-series AI applications.

</details>


### [245] [Platform Architecture for Tight Coupling of High-Performance Computing with Quantum Processors](https://arxiv.org/abs/2510.25213)
*Shane A. Caldwell,Moein Khazraee,Elena Agostini,Tom Lassiter,Corey Simpson,Omri Kahalon,Mrudula Kanuri,Jin-Sung Kim,Sam Stanwyck,Muyuan Li,Jan Olle,Christopher Chamberland,Ben Howe,Bruno Schmitt,Justin G. Lietz,Alex McCaskey,Jun Ye,Ang Li,Alicia B. Magann,Corey I. Ostrove,Kenneth Rudinger,Robin Blume-Kohout,Kevin Young,Nathan E. Miller,Yilun Xu,Gang Huang,Irfan Siddiqi,John Lange,Christopher Zimmer,Travis Humble*

Main category: quant-ph

TL;DR: NVQLink是一个连接高性能计算（HPC）资源与量子处理单元（QPU）控制系统的架构，旨在加速QPU运行所需的工作负载，支持各种QPU物理模式和QSC类型。


<details>
  <summary>Details</summary>
Motivation: 支持所有物理模式的QPU和所有类型的QSC，并将HPC资源连接到QPU的控制系统，以加速QPU运行所需的工作负载。

Method: NVQLink架构，基于商用以太网，支持实时（延迟有界）处理，并对CUDA-Q编程模型和运行时架构进行了扩展，以支持HPC和QSC之间的实时回调和数据编组。

Result: 实现了3.96微秒（最大）的往返延迟，并有可能进一步优化。

Conclusion: NVQLink通过扩展异构、基于内核的编程到QSC，允许程序员在同一个C++程序中处理QSC的CPU、GPU和FPGA子系统，避免了使用性能受限的HTTP接口，并为QSC构建者提供了一个集成模式。

Abstract: We propose an architecture, called NVQLink, for connecting high-performance
computing (HPC) resources to the control system of a quantum processing unit
(QPU) to accelerate workloads necessary to the operation of the QPU. We aim to
support every physical modality of QPU and every type of QPU system controller
(QSC). The HPC resource is optimized for real-time (latency-bounded) processing
on tasks with latency tolerances of tens of microseconds. The network
connecting the HPC and QSC is implemented on commercially available Ethernet
and can be adopted relatively easily by QPU and QSC builders, and we report a
round-trip latency measurement of 3.96 microseconds (max) with prospects of
further optimization. We describe an extension to the CUDA-Q programming model
and runtime architecture to support real-time callbacks and data marshaling
between the HPC and QSC. By doing so, NVQLink extends heterogeneous,
kernel-based programming to the QSC, allowing the programmer to address CPU,
GPU, and FPGA subsystems in the QSC, all in the same C++ program, avoiding the
use of a performance-limiting HTTP interface. We provide a pattern for QSC
builders to integrate with this architecture by making use of multi-level
intermediate representation dialects and progressive lowering to encapsulate
QSC code.

</details>


### [246] [Decoder Switching: Breaking the Speed-Accuracy Tradeoff in Real-Time Quantum Error Correction](https://arxiv.org/abs/2510.25222)
*Riki Toshio,Kaito Kishi,Jun Fujisaki,Hirotaka Oshima,Shintaro Sato,Keisuke Fujii*

Main category: quant-ph

TL;DR: 通过结合快速的


<details>
  <summary>Details</summary>
Motivation: 量子计算机的容错能力依赖于高速、高精度、实时解码系统的构建，但速度和精度的基本权衡带来了挑战。

Method: 提出了一种新颖的框架“解码器切换”，结合了“弱解码器”（速度快、软输出）和“强解码器”（速度慢、精度高）。当遇到可靠性低的解码窗口时，切换到强解码器进行更精确的解码。还提出了一种名为“双窗口解码”的在线解码方案。

Result: 数值模拟表明，该框架的准确性可与强解码器媲美甚至超越，同时保持与弱解码器相当的平均解码时间。双窗口解码方案避免了量子计算指数级减慢。

Conclusion: 该框架打破了长期的速度-精度权衡，为可扩展的实时解码设备铺平了道路。

Abstract: The realization of fault-tolerant quantum computers hinges on the
construction of high-speed, high-accuracy, real-time decoding systems. The
persistent challenge lies in the fundamental trade-off between speed and
accuracy: efforts to improve the decoder's accuracy often lead to unacceptable
increases in decoding time and hardware complexity, while attempts to
accelerate decoding result in a significant degradation in logical error rate.
To overcome this challenge, we propose a novel framework, decoder switching,
which balances these competing demands by combining a faster, soft-output
decoder ("weak decoder") with a slower, high-accuracy decoder ("strong
decoder"). In usual rounds, the weak decoder processes error syndromes and
simultaneously evaluates its reliability via soft information. Only when
encountering a decoding window with low reliability do we switch to the strong
decoder to achieve more accurate decoding. Numerical simulations suggest that
this framework can achieve accuracy comparable to, or even surpassing, that of
the strong decoder, while maintaining an average decoding time on par with the
weak decoder. We also develop an online decoding scheme tailored to our
framework, named double window decoding, and elucidate the criteria for
preventing an exponential slowdown of quantum computation. These findings break
the long-standing speed-accuracy trade-off, paving the way for scalable
real-time decoding devices.

</details>


### [247] [Encoding computationally hard problems in triangular Rydberg atom arrays](https://arxiv.org/abs/2510.25249)
*Xi-Wei Pan,Huan-Hai Zhou,Yi-Ming Lu,Jin-Guo Liu*

Main category: quant-ph

TL;DR:  Rydberg原子阵列用于量子优化，但现有方法（King's subgraph）在二维上不是最优的，且存在近似误差。本研究提出了一种在三角晶格上进行量子优化的新编码方案，通过自动化工具搜索得到，将独立约束违反减少了约两个数量级，减少了对实验后处理的需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于King's subgraph的Rydberg原子阵列量子优化方法在二维上存在近似误差，需要后处理，且后处理缺乏物理解释性。本研究旨在开发一种更优的编码方案以克服这些问题。

Method: 提出了一种在三角晶格上进行量子优化的通用编码方案，并采用创新的自动化工具搜索策略来寻找合适的子结构（gadgets）。

Result: 与King's subgraph相比，在三角晶格上进行的量子优化将独立约束违反减少了约两个数量级。

Conclusion: 本研究提出的三角晶格编码方案能够显著减少量子优化中的独立约束违反，从而大大减少对实验后处理的依赖，并且该方案是通用的，适用于计算复杂问题。

Abstract: Rydberg atom arrays are a promising platform for quantum optimization,
encoding computationally hard problems by reducing them to independent set
problems with unit-disk graph topology. In Nguyen et al., PRX Quantum 4, 010316
(2023), a systematic and efficient strategy was introduced to encode multiple
problems into a special unit-disk graph: the King's subgraph. However, King's
subgraphs are not the optimal choice in two dimensions. Due to the power-law
decay of Rydberg interaction strengths, the approximation to unit-disk graphs
in real devices is poor, necessitating post-processing that lacks physical
interpretability. In this work, we develop an encoding scheme that can
universally encode computationally hard problems on triangular lattices, based
on our innovative automated gadget search strategy. Numerical simulations
demonstrate that quantum optimization on triangular lattices reduces
independence-constraint violations by approximately two orders of magnitude
compared to King's subgraphs, substantially alleviating the need for
post-processing in experiments.

</details>


### [248] [Statistical Physics from Quantum Envariance Principles](https://arxiv.org/abs/2510.25253)
*Amul Ojha,Shubhit Sardana,Arnab Ghosh*

Main category: quant-ph

TL;DR: 本研究基于Deffner和Zurek的工作，利用“envariance”概念，从量子力学推导统计力学原理，展示了二项、泊松和高斯分布如何从纠缠系统-环境态中自然出现，并解决了Gibbs佯谬，获得了带量子修正的Sackur-Tetrode方程，还推导了修正后的Saha方程，并从量子对称性中恢复了玻色-爱因斯坦和费米-狄拉克统计。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于，从量子力学而非现象学假设出发，展示统计力学原理（包括各种概率分布、Gibbs佯谬、Sackur-Tetrode方程、Saha方程以及Bose-Einstein和Fermi-Dirac统计）的涌现。

Method: 本研究利用“envariance”（环境辅助不变性）的概念，分析纠缠系统-环境态，推导出统计力学的基本分布和方程，例如二项、泊松、高斯分布，并利用纠缠熵解决Gibbs佯谬，得到Sackur-Tetrode方程，推导修正后的Saha方程，并从量子对称性中恢复Bose-Einstein和Fermi-Dirac统计。

Result: 本研究成功地从量子力学和envariance概念中推导出了二项、泊松和高斯分布，并利用纠缠熵解决了Gibbs佯谬，得到了带量子修正的Sackur-Tetrode方程，推导了修正后的Saha方程，并且从量子对称性中恢复了Bose-Einstein和Fermi-Dirac统计。

Conclusion: 本研究强化并扩展了“统计力学是量子信息动力学的直接结果”的观点，而非基于现象学假设。

Abstract: We build on the foundational work of Deffner and Zurek [S.~Deffner and
W.~H.~Zurek, {New J.~Phys.18, 063013 (2016)}] to demonstrate how the principles
of statistical mechanics can be derived from quantum mechanics using the
concept of envariance (environment-assisted invariance). In particular, we show
how the Binomial, Poisson, and Gaussian distributions naturally emerge from
entangled system--environment states. Furthermore, we resolve the Gibbs paradox
using entanglement entropy, obtaining the Sackur--Tetrode equation with quantum
corrections. Extending this framework, we derive a modified Saha equation for
ionization equilibrium and recover Bose--Einstein and Fermi--Dirac statistics
from quantum symmetries. Our results reinforce and extend the view that
statistical mechanics arises as a direct consequence of quantum information
dynamics, rather than being founded on phenomenological postulates.

</details>


### [249] [Design and Fabrication of Metal-Shielded Fiber-Cavity Mirrors for Ion-Trap Systems](https://arxiv.org/abs/2510.25294)
*Wei-Bin Chen,Ding Fang,Cheng-Hao Zhang,Jin-Ming Cui,Yun-Feng Huang,Chuan-Feng Li,Guang-Can Guo*

Main category: quant-ph

TL;DR: 金属屏蔽微腔镜可实现集成微腔的离子阱系统的稳定运行。


<details>
  <summary>Details</summary>
Motivation: 提高量子信息处理和量子网络中离子阱系统的稳定性和性能。

Method: 设计并制造了金属屏蔽微腔镜，并将其应用于集成光纤法布里-珀罗腔的针式离子阱。

Result: 成功实现了单个离子在腔内的稳定囚禁，并将离子加热率降低了一个数量级以上。

Conclusion: 所提出的金属屏蔽微腔镜技术是实现全集成离子光子接口和可扩展量子网络的关键技术。

Abstract: Trapped ions in micro-cavities constitute a key platform for advancing
quantum information processing and quantum networking. By providing an
efficient light-matter interface within a compact architecture, they serve as
highly efficient quantum nodes with strong potential for scalable quantum
network. However, in such systems, ion trapping stability is often compromised
by surface charging effects, and nearby dielectric materials are known to cause
a dramatic increase in the ion heating rate by several orders of magnitude.
These challenges significantly hinder the practical implementation of ion trap
systems integrated with micro-cavities. To overcome these limitations, we
present the design and fabrication of metal-shielded micro-cavity mirrors,
enabling the stable realization of ion trap systems integrated with micro
cavities. Using this method, we constructed a needle ion trap integrated with
fiber Fabry-Perot cavity and successfully achieved stable trapping of a single
ion within the cavity. The measured ion heating rate was reduced by more than
an order of magnitude compared with unshielded configurations. This work
establishes a key technique toward fully integrated ion-photon interfaces for
scalable quantum network.

</details>


### [250] [Second-order Stark shifts exceeding 10$\,$GHz in electrically contacted SiV$^-$ centers in diamond](https://arxiv.org/abs/2510.25543)
*Manuel Rieger,Nori N. Chavira Leal,Rubek Poudel,Tobias Waldmann,Lina M. Todenhagen,Stefan Kresta,Viviana Villafane,Martin S. Brandt,Kai Müller,Jonathan J. Finley*

Main category: quant-ph

TL;DR: 通过施加电场来调控金刚石中负电性硅空位（SiV-）中心的零声子线，以克服其光学跃迁频率的非均匀分布，为实现可扩展的量子技术（如量子中继器）奠定基础。


<details>
  <summary>Details</summary>
Motivation: 金刚石中的负电性硅空位（SiV-）中心具有优异的量子相干性和光学特性，但其光学跃迁频率的应变诱导不均匀分布限制了其可扩展性。

Method: 使用平面内接触施加高达45 MV/m 的中等电场，演示了对 SiV- 中心零声子线的电调谐。

Result: 观察到二阶斯塔克位移超过 10 GHz，与 SiV- 在光学纳米结构中的非均匀分布相当。个别 SiV- 中心的极化率变化显著，且比锡空位中心大 3-25 倍，这归因于价带共振。光学linewidths 随电场强度适度增加。

Conclusion: 大的电斯塔克位移可以克服跃迁频率的非均匀分布，这是实现可扩展的 SiV- 量子技术（如量子中继器）的重要一步。

Abstract: Negatively charged silicon vacancy centers (SiV$^-$) in diamond exhibit
excellent spin coherence and optical properties, making them promising
candidates for quantum technologies. However, the strain-induced inhomogeneous
distribution of optical transition frequencies poses a challenge for
scalability. We demonstrate electrical tuning of the SiV$^-$ center zero-phonon
lines using in-plane contacts to apply moderate electric fields up to
45$\,$MV/m. The second-order Stark shift exceeds 10$\,$GHz, which is of the
same order of magnitude as the 15$\,$GHz inhomogeneous distribution of SiV$^-$
observed in emitters embedded in optical nanostructures such as photonic
crystal nanocavities. Analysis of individual SiV$^-$ centers shows significant
variation in polarizabilities between defects indicating that the
polarizability strongly depends on local parameters like strain. The observed
polarizabilities are 3-25 times larger than those of tin vacancy centers, which
we attribute to valence band resonances that delocalize the $e_u$
wavefunctions. Photoluminescence excitation measurements reveal that optical
linewidths increase moderately with applied electric field strength. Our
results demonstrate that large electrical Stark shifts can overcome the
inhomogeneous distribution of transition frequencies, representing a
significant step toward scalable SiV$^-$-based quantum technologies such as
quantum repeaters.

</details>


### [251] [Imaginarity measures induced by real part states and the complementarity relations](https://arxiv.org/abs/2510.25313)
*Jingyan Liu,Yue Sun,Jianwei Xu,Ming-Jing Zhao*

Main category: quant-ph

TL;DR: 本篇论文提出了一种通过实部态构造虚数度量的方法，并探讨了其在量子力学中的性质。


<details>
  <summary>Details</summary>
Motivation: 量子力学中复数不可或缺，而虚数资源理论近期得到发展，因此需要研究虚数度量。

Method: 提出了一种基于保真度的虚数度量，并推导了其在量子比特系统中的解析表达式，以及与其他虚数度量（如几何虚数度量、Tsallis相对熵虚数度量和迹范数虚数度量）的关系。此外，还研究了该度量在完备集上下的互补关系。

Result: 推导了虚数度量的解析表达式，并揭示了其与其他虚数度量的关系。在低维系统中，给出了虚数度量在完备集上的互补关系。

Conclusion: 该工作不仅突显了实部态在虚数资源理论中的重要作用，还揭示了虚数对完备集上的互无偏基的物理约束。

Abstract: Complex numbers are indispensable in quantum mechanics and the resource
theory of imaginarity has been developed recently. In this paper, we propose a
method to construct imaginary measures by real part states. Specifically, we
propose an imaginarity measure in terms of fidelity and explore its properties.
The analytical expression of the imaginarity measure is presented in qubit
systems. The relations between the proposed imaginarity measure and some other
imaginarity measures (such as geometric imaginarity, Tsallis relative entropy
imaginarity and trace norm imaginarity) are derived. The complementarity
relations of the imaginarity measure under a complete set of mutually unbiased
bases are provided in low-dimensional systems. This work not only highlights
the prominent role of the real part state in the imaginarity resource theory,
but also reveals the constraint of imaginarity on a complete set of mutually
unbiased bases physically.

</details>


### [252] [Quantum-correlated photons from spectrally-separated modes of a cavity coupled to a strongly-driven two-level atom](https://arxiv.org/abs/2510.25331)
*Alex Elliott,Jacob Ngaha,Scott Parkins,Takao Aoki*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Photon counting statistics are explored, theoretically, from a pair of cavity
modes coupled to the fluorescent transitions in a strongly-driven two-level
atom. We show that the cavity modes acquire nonclassical photon statistics that
are representative of dressed-state picture atomic transitions. In particular,
the modes are shown to be antibunched, while simultaneously having a
cross-correlation value greater than unity. Furthermore, we propose an
implementation of the system with a nanofiber cavity QED system, based on a
strongly-driven cesium atom.

</details>


### [253] [Variational quantum computing for quantum simulation: principles, implementations, and challenges](https://arxiv.org/abs/2510.25449)
*Lucas Q. Galvão,Anna Beatriz M. de Souza,Marcelo A. Moret,Clebson Cruz*

Main category: quant-ph

TL;DR: 本文概述了变分量子计算在量子模拟中的作用，重点关注量子数据在变分量子算法（VQA）和量子机器学习（QML）中的应用，并讨论了其在噪声中等规模量子（NISQ）时代面临的挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 变分量子计算在量子模拟中扮演着关键角色，尤其是在NISQ时代，为解决量子模拟问题提供了一种有前景的途径。

Method: 本文系统地阐述了变分量子计算的原理，并探讨了其在量子模拟问题中的应用。该方法采用混合量子-经典框架。

Result: 变分量子计算是一种有前景的、依赖于问题的解决方案，但其实用性取决于在噪声和无望高原限制下的可训练性和可扩展性。

Conclusion: 变分量子计算在量子模拟领域面临着持续的挑战和新兴的机遇，这一定义了该领域的当前格局。

Abstract: This work presents a comprehensive overview of variational quantum computing
and their key role in advancing quantum simulation. This work explores the
simulation of quantum systems and sets itself apart from approaches centered on
classical data processing, by focusing on the critical role of quantum data in
Variational Quantum Algorithms (VQA) and Quantum Machine Learning (QML). We
systematically delineate the foundational principles of variational quantum
computing, establish their motivational and challenges context within the noisy
intermediate-scale quantum (NISQ) era, and critically examine their application
across a range of prototypical quantum simulation problems. Operating within a
hybrid quantum-classical framework, these algorithms represent a promising yet
problem-dependent pathway whose practicality remains contingent on trainability
and scalability under noise and barren-plateau constraints.This review serves
to complement and extend existing literature by synthesizing the most recent
advancements in the field and providing a focused perspective on the persistent
challenges and emerging opportunities that define the current landscape of
variational quantum computing for quantum simulation.

</details>


### [254] [Quantum Fisher Information With General Quantum Coherence in multi-dimensional quantum systems](https://arxiv.org/abs/2510.25457)
*Jun-Long Zhao,Li Yu,Ming Yang,Chui-Ping Yang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum metrology is a science about quantum measurements and it plays a key
role in precision of quantum parameter estimation. Meanwhile, quantum coherence
is an important quantum feature and quantum Fisher information (QFI) is an
important indicator for precision of quantum parameter estimation. In this
paper, we explore the relationship between QFI and quantum coherence in
multi-dimensional quantum systems. We introduce a new concept referred to as
General Quantum Coherence (GQC), which characterizes the quantum coherence and
the eigenenergies of the Hamiltonian in the interaction processes. GQC captures
quantum nature of high-dimensional quantum states and addresses shortcomings in
coherence measurement. Additionally, we observe a stringent square relationship
between GQC and QFI. This finding provides a crucial guideline for improving
the precision of parameter estimation.

</details>


### [255] [Generalized collective quantum tomography: algorithm design, optimization, and validation](https://arxiv.org/abs/2510.25466)
*Shuixin Xiao,Yuanlong Wang,Zhibo Hou,Aritra Das,Ian R. Petersen,Farhad Farokhi,Guo-Yong Xiang,Jie Zhao,Daoyi Dong*

Main category: quant-ph

TL;DR: Collective quantum state tomography is extended to a generalized setting for estimating target states of the form S1⊗...⊗Sn, where Si can represent identical or distinct quantum states, detectors, or processes. Three algorithms are developed for collective quantum state, detector, and process tomography, with analyses of computational complexity and MSE scaling. Optimal solutions are found using sum of squares (SOS) techniques. Numerical and experimental demonstrations show lower MSEs and approach the collective MSE bound by utilizing purity information.


<details>
  <summary>Details</summary>
Motivation: Collective quantum state tomography, which estimates an unknown state ρ through joint measurements on multiple copies ρ⊗⋯⊗ρ of the unknown state, offers superior information extraction efficiency. This work extends this framework to a generalized setting where the target becomes S1⊗⋯⊗Sn, with each Si representing identical or distinct quantum states, detectors, or processes from the same category.

Method: The paper formulates the generalized collective tomography tasks as optimization problems and develops three algorithms for collective quantum state, detector, and process tomography. It also develops optimal solutions using sum of squares (SOS) techniques with semi-algebraic constraints. The effectiveness is demonstrated through numerical examples and experimental demonstrations using two-copy collective measurements.

Result: The proposed methods are demonstrated through numerical examples and experimental demonstrations. The algorithms achieve lower mean squared errors (MSEs) and approach the collective MSE bound by effectively leveraging purity information, particularly in two-copy collective measurements where entangled measurements directly provide information about state purity.

Conclusion: The generalized framework and developed algorithms for collective quantum tomography are effective, achieving lower MSEs and approaching the collective MSE bound. The use of SOS techniques provides optimal solutions, and the experimental demonstration validates the practical applicability of the methods.

Abstract: Quantum tomography is a fundamental technique for characterizing,
benchmarking, and verifying quantum states and devices. It plays a crucial role
in advancing quantum technologies and deepening our understanding of quantum
mechanics. Collective quantum state tomography, which estimates an unknown
state \r{ho} through joint measurements on multiple copies
$\rho\otimes\cdots\otimes\rho$ of the unknown state, offers superior
information extraction efficiency. Here we extend this framework to a
generalized setting where the target becomes $S_1\otimes\cdots\otimes S_n$,
with each $S_i$ representing identical or distinct quantum states, detectors,
or processes from the same category. We formulate these tasks as optimization
problems and develop three algorithms for collective quantum state, detector
and process tomography, respectively, each accompanied by an analytical
characterization of the computational complexity and mean squared error (MSE)
scaling. Furthermore, we develop optimal solutions of these optimization
problems using sum of squares (SOS) techniques with semi-algebraic constraints.
The effectiveness of our proposed methods is demonstrated through numerical
examples. Additionally, we experimentally demonstrate the algorithms using
two-copy collective measurements, where entangled measurements directly provide
information about the state purity. Compared to existing methods, our
algorithms achieve lower MSEs and approach the collective MSE bound by
effectively leveraging purity information.

</details>


### [256] [Detuning Choice for solving MIS and MWIS](https://arxiv.org/abs/2510.25473)
*Sem Saada Khelkhal,Louis Barcikowsky*

Main category: quant-ph

TL;DR: 本文研究了在Pasqal中性原子处理器限制下，最大加权独立集（MWIS）及其量子类似物的实现方法，并提出了一种新的解调计算方法，以克服近邻原子间的寄生相互作用。


<details>
  <summary>Details</summary>
Motivation: 在Pasqal中性原子处理器硬件限制（如有限的量子比特数、$\Omega$和$\\\Delta$的边界、序列持续时间、空间限制、最小原子间距和寄生相互作用）下，实现最大加权独立集（MWIS）及其量子类似物。

Method: 提出了一种新颖的解调计算方法，并提出了三种实现方法：(I) 纯理论的局部解调方法；(II) 适用于未来量子计算机集成的解调图调制（DMM）方法；(III) 适用于当前硬件的全局脉冲和频率偏移方法。

Result: 在最多30个量子比特的图上，使用Pasqal的模拟器进行了测试，验证了所提出方法在所有量子计算机约束下的可行性。

Conclusion: 所提出的方法能够在Pasqal中性原子处理器的硬件限制下，有效地实现最大加权独立集（MWIS）及其量子类似物，并且具有不同的实现策略以适应不同的硬件成熟度。

Abstract: We study the realization of a Maximum Weighted Independent Set (MWIS) and its
quantum analogue under the constraints of Pasqal's neutral-atom processor:
limited qubit number, bounds on $\Omega$ and $\Delta$, sequence duration,
confinement space, minimum interatomic distance, and parasitic interactions.
Our goal is to obtain results directly compatible with current hardware, on
asymmetric graphs whose size is limited only by the QPU's topology. We
introduce a novel detuning computation method that departs from conventional
bounds, as parasitic interactions between nearby but unconnected atoms can
significantly bias results. Three implementations are proposed, suited to
different hardware maturity levels: (I) a speculative local-detuning approach
demonstrating the pure theory; (II) a \emph{Detuning Map Modulation} (DMM)
method approximating this theory for future QPU integration; and (III) a
global-pulse and frequency-shift approach, diverging from theory but
experimentally viable today. Tests using Pasqal's emulators on graphs up to
$30$ qubits confirm the practicality of our methods within all QPU constraints.

</details>


### [257] [Decoherence Estimation of Superconducting Qubit](https://arxiv.org/abs/2510.25491)
*Yoav Koral,Shilo Avraham,Manimuthu Peryasamy,Shmuel E. Schacham,Eliyahu Farber*

Main category: quant-ph

TL;DR: 本研究利用Caldeira-Leggett模型分析了由寄生电阻引起的量子比特退相干，无需外部哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 研究退相干过程，特别是由量子比特与寄生电阻原子间的物理相互作用引起的退相干。

Method: 使用Caldeira-Leggett电模型，分析了量子比特光子与寄生电阻原子间的相互作用，获得了用于Lindblad主方程的发射和吸收率。

Result: 分析结果与Johnson-Nyquist噪声模型高度一致，并且数值代入结果与之前的测量结果强相关。

Conclusion: 该分析能够获得未来模拟所需的合适电路特性，并为理解和减轻量子比特退相干提供了理论基础。

Abstract: Decoherence of quantum bits arises primarily from the parasitic resistance
within the qubit. This study presents the analysis of the decoherence process
due to physical interactions between the qubit photons and parasitic resistance
atoms, utilizing exclusively the Caldeira-Leggett electrical model, without
relying on external Hamiltonians. The analysis shows a good agreement between
the model of the electrical noise and the Johnson-Nyquist noise. The emission
and absorption rates of the qubit's coherent loss, required for the Lindblad
master equation that approximates the decoherence, are obtained. A numerical
substitution in the analysis result yields a strong correlation with previous
measurements. The present analysis enables also the derivation of the
appropriate circuit characteristics for future simulations.

</details>


### [258] [Super-Moiré Spin Textures in Twisted Antiferromagnets](https://arxiv.org/abs/2510.25545)
*King Cho Wong,Ruoming Peng,Eric Anderson,Jackson Ross,Bowen Yang,Meixin Cheng,Sreehari Jayaram,Malik Lenger,Xuankai Zhou,Yan Tung Kong,Takashi Taniguchi,Kenji Watanabe,Michael A. McGuire,Rainer Stöhr,Adam Wei Tsen,Elton J. G. Santos,Xiaodong Xu,Jörg Wrachtrup*

Main category: quant-ph

TL;DR: 通过堆叠二维材料构建的宏观磁性


<details>
  <summary>Details</summary>
Motivation: 堆叠二维材料可以工程化电子和磁性状态，但产生的宏观磁性通常具有宏观单元的周期性。本研究旨在探索一种新的磁性状态——超宏观磁性状态。

Method: 利用扫描量子自旋磁力仪研究扭曲双层碘化铬（tDB CrI3）在小扭转角下的磁性，并通过大规模原子模拟解释其形成机制。

Result: 发现在小扭转角下，自发磁性结构的尺寸随着扭转角的增加而增加，与底层宏观周期性相反。在1.1°扭转器件中，自发磁性结构的尺寸达到约300纳米，比底层宏观波长大了两个数量级，而在大于2°的扭转角下消失。研究表明，这种磁性状态是反铁磁性的尼尔型斯格明子，横跨多个宏观单元。

Conclusion: 扭转角依赖性研究和原子模拟表明，Dzyaloshinskii-Moriya相互作用、磁各向异性和交换相互作用之间的复杂竞争，以及由层相对旋转控制的相互作用，产生了超宏观自旋序中的拓扑结构。

Abstract: Stacking two-dimensional (2D) layered materials offers a powerful platform to
engineer electronic and magnetic states. In general, the resulting states, such
as Moir\'e magnetism, have a periodicity at the length scale of the Moir\'e
unit cell. Here, we report a new type of magnetism -- dubbed a super-Moir\'e
magnetic state -- which is characterized by long-range magnetic textures
extending beyond the single Moir\'e unit cell -- in twisted double bilayer
chromium triiodide (tDB CrI$_3$). We found that at small twist angles, the size
of the spontaneous magnetic texture increases with twist angle, opposite to the
underlying Moir\'e periodicity. The spin-texture size reaches a maximum of
about 300 nm in 1.1${\deg}$ twisted devices, an order of magnitude larger than
the underlying Moir\'e wavelength, and vanishes at twist angles above
2${\deg}$. Employing scanning quantum spin magnetometry, the obtained vector
field maps suggest the formation of antiferromagnetic N\'eel-type skyrmions
spanning multiple Moir\'e cells. The twist-angle-dependent study combined with
large-scale atomistic simulations suggests that complex magnetic competition
between the Dzyaloshinskii--Moriya interaction, magnetic anisotropy, and
exchange interactions controlled by the relative rotation of the layers
produces the topological textures which arise in the super-Moir\'e spin orders.

</details>


### [259] [Model Reduction for Controlled Quantum Markov Dynamics](https://arxiv.org/abs/2510.25546)
*Tommaso Grigoletto,Lorenza Viola,Francesco Ticozzi*

Main category: quant-ph

TL;DR: We developed a model reduction technique for time-dependent Markovian quantum systems that preserves the Lindblad form and accurately predicts observable evolution.


<details>
  <summary>Details</summary>
Motivation: To address model reduction for Markovian quantum systems with time-dependent generators, often arising from external control.

Method: Utilizing Krylov operator subspaces and operator-algebraic techniques, adapted for time-dependent generators.

Result: A reduced model that exactly replicates the evolution of selected observables and is guaranteed to maintain the Lindblad form.

Conclusion: The proposed method offers an effective way to simplify complex quantum system dynamics while ensuring accuracy and preserving the physical structure of the model.

Abstract: We consider the problem of model reduction for Markovian quantum systems
whose dynamics are described by a time-dependent Lindblad generator -- notably,
as arising in the presence of external control. Our approach, which builds upon
Krylov operator subspaces and operator-algebraic techniques introduced for
time-independent generators, returns a reduced model that reproduces exactly
the evolution of observables of interest and is guaranteed to be in Lindblad
form.

</details>


### [260] [Charge-Preserving Operations in Quantum Batteries](https://arxiv.org/abs/2510.25549)
*André H. A. Malavazi,Borhan Ahmadi,Paweł Horodecki,Pedro R. Dieguez*

Main category: quant-ph

TL;DR: Ergotropy, a measure of extractable work from quantum systems, can be conserved and redistributed within a system using novel 'isoergotropic states' and 'ergotropy-preserving operations'. These operations, demonstrated in both discrete and continuous variable systems, can be implemented via beam-splitter interactions and have implications for optimizing quantum battery charging and charge loss mitigation.


<details>
  <summary>Details</summary>
Motivation: Understanding and manipulating ergotropy, a fundamental measure of extractable work and stored energy/charge in quantum systems, is crucial for advancing quantum energy management technologies.

Method: The paper introduces and formalizes 'isoergotropic states' and 'ergotropy-preserving operations'. These concepts are illustrated and analyzed for discrete (two-level systems) and continuous-variable (single-mode Gaussian states) systems. The thermodynamic exchanges (energy, entropy) are examined, and the dynamic implementation of these operations using beam-splitter interactions with an auxiliary system is demonstrated.

Result: Ergotropy-preserving operations were shown to redistribute coherent-incoherent and displacement-squeezing components in both discrete and continuous variable systems. Thermodynamic exchanges were analyzed, and a method for dynamic implementation was demonstrated.

Conclusion: Isoergotropic states and ergotropy-preserving operations offer practical implications for optimizing charging protocols and mitigating charge loss in open quantum batteries, representing a significant advancement in quantum energy management.

Abstract: Ergotropy provides a fundamental measure of the extractable work from a
quantum system and, consequently, of the maximal useful energy, or charge,
stored within it. Understanding how this quantity can be manipulated and
transformed efficiently is crucial for advancing quantum energy management
technologies. Here, we introduce and formalize the concepts of isoergotropic
states and ergotropy-preserving operations, which reorganize the internal
structure of ergotropy while keeping its total value unchanged. These ideas are
illustrated for both discrete (two-level systems) and continuous-variable
systems (single-mode Gaussian states). In each case, we show how
ergotropy-preserving operations redistribute the respective coherent-incoherent
and displacement-squeezing components. We further examine the thermodynamic
exchanges accompanying ergotropy-preserving operations, including variations in
energy and entropy, and demonstrate that these transformations can be
dynamically implemented through standard beam-splitter-type interactions with
an auxiliary system. Finally, we discuss the practical implications of
isoergotropic states and operations in optimizing charging protocols and
mitigating charge loss in open quantum batteries.

</details>


### [261] [Transition-Aware Decomposition of Single-Qudit Gates](https://arxiv.org/abs/2510.25561)
*Denis A. Drozhzhin,Evgeniy O. Kiktenko,Aleksey K. Fedorov,Anastasiia S. Nikolaeva*

Main category: quant-ph

TL;DR: 提出了一种资源高效的算法，将单-qudit操作分解为允许的脉冲序列，脉冲数量最多为 d(d-1)/2，并针对不同类型的离子和超导qudit进行了分解。


<details>
  <summary>Details</summary>
Motivation: 通用qudit操作需要分解为一系列的本地操作（脉冲），但由于选择规则的限制，并非所有能级都直接相连，这增加了操作的复杂性和潜在误差。

Method: 提出了一种资源高效的算法，该算法可以将任意单-qudit操作分解为一系列允许的、遵循qudit选择规则的脉冲。算法生成的脉冲数量最多为 d(d-1)/2，对于特定操作可能更少。

Result: 该算法能够将单-qudit操作分解为脉冲序列，脉冲数量的上界为 d(d-1)/2。并对几种离子（$^{171}	ext{Yb}^+$, $^{137}	ext{Ba}^+$, $^{40}	ext{Ca}^+$, $^{86}	ext{Rb}^+$）和超导qudit进行了分解演示。

Conclusion: 提出的算法为qudit计算提供了一种有效的操作分解方法，能够以有限且可控的脉冲数量实现任意单-qudit操作，并考虑了不同硬件平台的选择规则。

Abstract: Quantum computation with $d$-level quantum systems, also known as qudits,
benefits from the possibility to use a richer computational space compared to
qubits. However, for arbitrary qudit-based hardware platform the issue is that
a generic qudit operation has to be decomposed into the sequence of native
operations $-$ pulses that are adjusted to the transitions between two levels
in a qudit. Typically, not all levels in a qudit are simply connected to each
other due to specific selection rules. Moreover, the number of pulses plays a
significant role, since each pulse takes a certain execution time and may
introduce error. In this paper, we propose a resource-efficient algorithm to
decompose single-qudit operations into the sequence of pulses that are allowed
by qudit selection rules. Using the developed algorithm, the number of pulses
is at most $d(d{-}1)/2$ for an arbitrary single-qudit operation. For specific
operations the algorithm could produce even fewer pulses. We provide a
comparison of qudit decompositions for several types of trapped ions,
specifically $^{171}\text{Yb}^+$, $^{137}\text{Ba}^+$, $^{40}\text{Ca}^+$,
$^{86}\text{Rb}^+$ with different selection rules, and also decomposition for
superconducting qudits.

</details>


### [262] [Systematic Non-Binary Extension of LDPC-CSS Codes Preserving Orthogonality](https://arxiv.org/abs/2510.25583)
*Kenta Kasai*

Main category: quant-ph

TL;DR: 我们研究了保持给定二进制CSS码的校验矩阵具有相同支撑集的有限域扩张。LDPC-CSS码是指其校验矩阵以正交方式定义的CSS码，其中每对对应的行在偶数（可能为零）个位置上重叠，在稀疏构造中通常最多重叠两次。除了低密度设置，我们还提出了一种系统性的构造方法，可扩展到任意CSS码，提供可行的有限域推广，同时保持二进制支撑集和正交性条件。


<details>
  <summary>Details</summary>
Motivation: 研究保持给定二进制CSS码的校验矩阵支撑集的有限域扩张。

Method: 提出一种系统性的构造方法，该方法可扩展到任意CSS码，并保持二进制支撑集和正交性条件。

Result: 提供可行的有限域推广，以保持二进制支撑集和正交性条件。

Conclusion: 所提出的构造方法可以推广到任意CSS码，并保持其二进制支撑集和正交性条件。

Abstract: We study finite-field extensions that preserve the same support as the
parity-check matrices defining a given binary CSS code. Here, an LDPC-CSS code
refers to a CSS code whose parity-check matrices are orthogonal in the sense
that each pair of corresponding rows overlaps in an even (possibly zero) number
of positions, typically at most twice in sparse constructions. Beyond the
low-density setting, we further propose a systematic construction method that
extends to arbitrary CSS codes, providing feasible finite-field generalizations
that maintain both the binary support and the orthogonality condition.

</details>


### [263] [Controlling the Dynamical Evolution of Quantum Coherence and Quantum Correlations in $ e^{+}e^{-} \to Λ\barΛ$ Processes at BESIII](https://arxiv.org/abs/2510.25615)
*Elhabib Jaloum,Mohamed Amazioug*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum coherence, a cornerstone of quantum mechanics, is of paramount
importance for quantum information protocols. However, maintaining coherence in
elementary particle systems presents significant challenges. In this work, we
investigate quantum coherence and quantum correlations in the $e^{+}e^{-} \to
\Lambda\bar{\Lambda}$ processes at BESIII using experimentally feasible
parameters, where $\Lambda$ and $\bar{\Lambda}$ denote the spin-$1/2$ hyperon
and its antihyperon, respectively. We analyze the dependence of quantum
coherence and quantum correlations on the scattering angle $\varphi$. Notably,
these resources reach their maximum at $\varphi=\pi/2$. We demonstrate that
classical correlations can significantly delay the decay of quantum
correlations and coherence. This study underscores the importance of
understanding the interplay between classical and quantum correlations in
high-energy particle physics, particularly in the context of
hyperon-antihyperon interactions explored in the BESIII experiment. This result
could have potential applications in quantum information processing and
high-energy physics.

</details>


### [264] [Engineering Atom-Photon Hybridization with Density-Modulated Atomic Ensembles in Coupled Cavities](https://arxiv.org/abs/2510.25617)
*Carlos E. Máximo,Romain Bachelard,Tobias Donner*

Main category: quant-ph

TL;DR: 通过空间设计原子系综来控制腔模耦合，实现选择性光子传输和多体复杂度精确控制。


<details>
  <summary>Details</summary>
Motivation: 原子与光场的相互作用以及原子间的相互作用。利用原子系综的空间结构来控制不同腔模之间的耦合，从而重塑原子-光子光谱。

Method: 利用空间结构控制原子系综，通过构建均匀原子云和光栅原子云，研究其对腔模耦合的影响，并分析光谱变化。

Result: 均匀原子云通过相消干涉抑制模间耦合；光栅原子云在特定布拉格条件下可保持模间耦合，导致模间光谱分裂，这种集体效应不仅源于原子数量，还源于不同腔模的独立调谐能力。

Conclusion: 空间工程化的原子系综是实现选择性光子传输和精确控制多体复杂性的有效途径。

Abstract: Radiation-matter hybridization allows atoms to serve as mediators of
effective interactions between light modes and, conversely, to interact among
themselves via light. Here we exploit the spatial structure of atomic ensembles
to control the coupling between modes of distinct cavities, thereby reshaping
the resulting atom-photon spectra. We show that extended homogeneous clouds
suppress mode-mode couplings through destructive interference, whereas grated
clouds can preserve them under specific Bragg conditions. This leads to
mode-mode spectral subsplittings, where collectivity arises not only from the
atom number but also from the ability to tune modes of different cavities
independently. Our results establish spatially engineered atomic ensembles as a
pathway to selective photon transfer between modes and precise control of
many-body complexity.

</details>


### [265] [Photoelectric detection of single spins in diamond by optically controlled discharge of long-lived trap states](https://arxiv.org/abs/2510.25619)
*A. C. Ulibarri,D. J. McCloskey,D. Wang,N. Dontschuk,A. M. Martin,A. A. Wood*

Main category: quant-ph

TL;DR: 本文提出了一种光电自旋读出方案，通过检测存储的电荷来读取固态自旋信息，并以氮-空位（NV）中心为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 固态自旋的电学读出方法在量子技术方面具有吸引力，易于芯片扩展且不受单发射光子数预算的限制。然而，在宽禁带材料中实现与光学方法相当的保真度和带宽的电学自旋读出仍然具有挑战性。

Method: 利用氮-空位（NV）中心作为模型系统，通过自旋依赖的光致电离产生载流子，这些载流子被存储在金刚石-金属肖特基结的长寿命陷阱态中。在电学偏压下进行按需照明，释放存储的电荷，产生与存储电荷量成正比的光电流瞬态，从而反映自旋状态。该方案被称为电荷捕获检测磁共振（CCDMR）。

Result: 通过CCDMR实现了对单NV中心进行相干控制后的电荷读出，并利用基于电荷的成像技术识别了载流子产生和陷阱过程。

Conclusion: 本文提出的CCDMR是一种用于固态自旋量子比特读出的新技术，它结合了电学探测的优点和宽禁带材料中长寿命电荷陷阱的稳定性。

Abstract: Electrical detection methods for solid-state spins are attractive for quantum
technologies, being readily chip-scalable and not subject to the small photon
budgets of single emitters. However, realising electrical spin readout in
wide-bandgap materials with similar fidelity and bandwidth to optical
approaches remains challenging. Here, we introduce a photoelectrical spin
readout scheme that detects spin information stored long-term as trapped
electrical charges. Using nitrogen-vacancy (NV) centres in diamond as a model
system, spin-dependent photoionisation generates charge carriers that are
stored in long-lived trap states at a diamond-metal Schottky junction.
On-demand illumination of the junction under electrical bias releases stored
charge, yielding a photocurrent transient proportional to the amount of trapped
charge and hence spin state. Spin readout after coherent control of single NVs
is demonstrated using charge readout in a protocol we call charge-capture
detected magnetic resonance (CCDMR), and we use charge-based imaging to
identify charge carrier generation and trapping processes. Our results
establish CCDMR as a new technique for solid-state spin qubit readout,
combining attaractive features of electrical detection with the stability of
long-lived charge traps in wide-bandgap materials.

</details>


### [266] [Large-scale implementation of quantum subspace expansion with classical shadows](https://arxiv.org/abs/2510.25640)
*Laurin E. Fischer,Daniel Bultrini,Ivano Tavernelli,Francesco Tacchino*

Main category: quant-ph

TL;DR: 通过使用信息论完备测量（如经典快照）来减少量子子空间展开（QSE）的测量开销，并成功应用于自旋模型的量子相变研究，在多达80个量子比特的系统中实现了准确的基态能量恢复和局部序参数的缓解。


<details>
  <summary>Details</summary>
Motivation: 为了克服量子子空间展开（QSE）在量子计算中进行光谱计算时面临的一项重大挑战，即巨大的测量开销。

Method: 将QSE重新表述为约束优化问题，并结合信息论完备（IC）测量（如经典快照）来减少测量需求。对具有三体相互作用的自旋模型的量子相变进行了研究，并在多达80个量子比特的系统规模上进行了实验。

Result: 实现了QSE与IC测量的首次大规模结合。在研究的自旋模型的量子相变中，观察到了准确的基态能量恢复和局部序参数的缓解，即使在系统规模增大到80个量子比特的情况下也是如此。通过将QSE表述为约束优化问题，获得了严格的统计误差估计，并避免了数值病态问题。实验中使用了超过3x10^4次的测量基随机化和超过10^14次的泡利迹计算。

Conclusion: 该研究首次大规模实现了QSE与IC测量的结合，证明了这种方法在处理具有挑战性的量子多体问题（如量子相变）方面的有效性，并为未来在量子计算机上进行更精确的光谱计算铺平了道路。

Abstract: Quantum subspace expansion (QSE) offers promising avenues to perform spectral
calculations on quantum processors but comes with a large measurement overhead.
Informationally complete (IC) measurements, such as classical shadows, were
recently proposed to overcome this bottleneck. Here, we report the first
large-scale implementation of QSE with IC measurements. In particular, we probe
the quantum phase transition of a spin model with three-body interactions, for
which we observe accurate ground state energy recovery and mitigation of local
order parameters across system sizes of up to 80 qubits. We achieve this by
reformulating QSE as a constrained optimization problem, obtaining rigorous
statistical error estimates and avoiding numerical ill-conditioning. With over
$3 \times 10^4$ measurement basis randomizations per circuit and the evaluation
of $O(10^{14})$ Pauli traces, this represents one of the most significant
experimental realizations of classical shadows to date.

</details>


### [267] [Accurate Leakage Speculation for Quantum Error Correction](https://arxiv.org/abs/2510.25661)
*Chaithanya Naik Mude,Swamit Tannu*

Main category: quant-ph

TL;DR: Gladiator框架通过精确预测量子比特的泄漏来加速量子纠错，减少不必要的泄漏消除操作，从而提高量子计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的量子纠错（QEC）方法主要关注比特翻转和相位翻转错误，但忽略了物理量子比特可能泄漏到更高能量态（如|2>）的问题。这种泄漏会破坏后续的综合测量，并可能扩散到相邻量子比特，对量子计算造成严重影响。现有的检测方法（如Eraser）通过固定启发式方法从综合模式推断泄漏，但这种方法常将良性综合模式误分类为泄漏，导致不必要的泄漏消除电路（LRC）的触发。由于LRC本身存在噪声且速度慢，这些误触发会延长QEC周期并增加逻辑错误率。

Method: 提出了一种名为Gladiator的通用且可适应的泄漏推测框架。该框架能够应用于表面码、颜色码和qLDPC码。在离线阶段，Gladiator会构建一个代码感知的错误传播图，并根据设备数据进行校准。在在线阶段，它能在几纳秒内对每个综合进行分类，并且仅当观察到的模式被明确证明是泄漏主导时才安排LRC。

Result: 通过精确推测，Gladiator消除了高达3倍（平均2倍）不必要的LRC，缩短了QEC周期，并在源头上抑制了误报。在标准的容错基准测试中，Gladiator实现了1.7倍到3.9倍的加速，并将逻辑错误率降低了16%。

Conclusion: Gladiator框架通过精确的泄漏推测，显著提高了量子纠错的效率，缩短了QEC周期，并降低了逻辑错误率，为实现容错量子计算提供了关键的技术支持。

Abstract: Quantum Error Correction (QEC) protects qubits against bit- and phase-flip
errors in the |0> or |1> subspace, but physical qubits can also leak into
higher energy levels (e.g., |2>). Leakage is especially harmful, as it corrupts
all subsequent syndrome measurements and can spread to neighboring qubits.
Detecting leakage on data qubits is particularly challenging, since they are
never measured directly during QEC cycles. Prior work, such as eraser,
addresses this by inferring leakage from syndrome patterns using a fixed
heuristic. However, this approach often misclassifies benign syndromes,
triggering excessive leakage-reduction circuits (LRCs). Because LRCs are
themselves noisy and slow, these false triggers lengthen QEC cycles and inflate
logical error rates.
  We propose gladiator, a general and adaptable leakage speculation framework
that works across surface code, color code, and qLDPC codes. Offline, gladiator
builds a code-aware error-propagation graph calibrated to device data. Online,
it classifies each syndrome in a few nanoseconds and schedules LRC only when
the observed pattern is provably leakage-dominated. This precise speculation
eliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles,
and suppresses false positives at their source. Evaluated on standard
fault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16%
reduction in logical error rate, advancing the efficiency of fault-tolerant
quantum computing.

</details>


### [268] [Quantum simulation of actinide chemistry: towards scalable algorithms on trapped ion quantum computers](https://arxiv.org/abs/2510.25675)
*Kesha Sorathia,Cono Di Paola,Gabriel Greene-Diniz,Carlo A. Gaggioli,David Zsolt Manrique,Joe Gibbs,Sean Harding,Thomas M. Soini,Neil Gaspar,Robert Harker,Mark Storr,David Munoz Ramo*

Main category: quant-ph

TL;DR: 该研究首次使用量子计算方法（QCM和QPE）研究锕系元素的电子结构，以了解钚氧化物和氢化物的反应活化能。


<details>
  <summary>Details</summary>
Motivation: actinide元素的广泛技术应用需要深入了解其电子结构，以促进相关领域的科技进步。量子计算有潜力在这方面提供经典方法无法比拟的优势。

Method: 比较了量子计算矩（QCM）和量子相位估计算法（QPE）在研究锕系元素电子结构中的应用。通过筛选哈密顿量Pauli项和变分编译等技术来减少资源需求。利用Quantinuum的H系列离子阱设备进行了多达19个量子比特的量子计算实验。

Result: 量子计算实验结果与经典的电子结构计算和态矢量模拟结果高度一致。

Conclusion: 本研究成功展示了量子计算在锕系元素电子结构研究中的潜力，为未来利用量子计算机解决更复杂的化学问题奠定了基础。

Abstract: Due to the wide range of technical applications of actinide elements, a
thorough understanding of their electronic structure could complement
technological improvements in many different areas. Quantum computing could
greatly aid in this understanding, as it can potentially provide exponential
speedups over classical approaches, thereby offering insights into the complex
electronic structure of actinide compounds. As a first foray into quantum
computational chemistry of actinides, this paper compares the method of quantum
computed moments (QCM) as a noisy intermediate-scale quantum algorithm with a
single-ancilla version of quantum phase estimation (QPE), a quantum algorithm
expected to run on fault-tolerant quantum computers. We employ these algorithms
to study the reaction energetics of plutonium oxides and hydrides. In order to
enable quantum hardware experiments, we use several techniques to reduce
resource requirements: screening individual Hamiltonian Pauli terms to reduce
the measurement requirements of QCM and variational compilation to reduce the
depth of QPE circuits. Finally, we derive electronic structure descriptions
from a series of representative chemical models and compute the energetics from
quantum experiments on Quantinuum's H-series ion trap devices using up to 19
qubits. We find our experiments to be in excellent agreement with results from
classical electronic structure calculations and state vector simulations.

</details>


### [269] [Symmetry and Asymmetry in Bosonic Gaussian Systems: A Resource-Theoretic Framework](https://arxiv.org/abs/2510.25719)
*Nikolaos Koukoulekidis,Iman Marvian*

Main category: quant-ph

TL;DR: 该研究探讨了玻色子系统中对称性与高斯性之间的相互作用，并提出了一种高斯不对称性的资源理论。


<details>
  <summary>Details</summary>
Motivation: 研究对称性与高斯性在封闭和开放动力学中的相互作用，并开发高斯不对称性的资源理论。

Method: 关注高斯对称性保持（协变）操作，并证明此类操作可以通过保持对称性的高斯哈密顿量与处于对称性保持纯高斯态的环境相结合来实现。识别了一族可处理的、状态的单调函数，这些函数在高斯对称性保持动力学下保持非增加，并且在封闭系统中精确守恒。

Result: 这些单调函数通常不被非高斯对称性保持动力学所保持。提供了量子信息和光学领域具有独立意义的技术结果，包括 Stinespring 扩张定理的新方法，以及 Williamson 定理在同时正规模式分解中的应用。

Conclusion: 研究为理解和利用量子系统中的对称性和高斯性提供了新的视角和工具。

Abstract: We study the interplay of symmetries and Gaussianity in bosonic systems,
under closed and open dynamics, and develop a resource theory of Gaussian
asymmetry. Specifically, we focus on Gaussian symmetry-respecting (covariant)
operations, which serve as the free operations in this framework. We prove that
any such operation can be realized via Gaussian Hamiltonians that respect the
symmetry under consideration, coupled to an environment prepared in a
symmetry-respecting pure Gaussian state. We further identify a family of
tractable monotone functions of states that remain non-increasing under
Gaussian symmetry-respecting dynamics, and are exactly conserved in closed
systems. We demonstrate that these monotones are not generally respected under
non-Gaussian symmetry-respecting dynamics. Along the way, we provide several
technical results of independent interest to the quantum information and optics
communities, including a new approach to the Stinespring dilation theorem, and
an extension of Williamson's theorem for the simultaneous normal mode
decomposition of Gaussian systems and conserved charges.

</details>


### [270] [Inverse-free quantum state estimation with Heisenberg scaling](https://arxiv.org/abs/2510.25750)
*Kean Chen*

Main category: quant-ph

TL;DR: 该协议提出了一种仅使用前向查询即可实现海森堡缩放的量子态估计方法，并在某些参数下优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 与需要前向和逆向查询的先前方法相比，研究人员希望开发一种仅使用前向查询即可实现海森堡缩放的量子态估计协议。

Method: 提出了一种新的量子态估计协议，该协议利用前向查询来估计未知的酉变换作用在 $|dangle$ 上的状态，从而实现海森堡缩放。

Result: 该协议能在迹距离误差 $\varepsilon$ 内估计 $U|d\rangle$，所需的查询次数为 $O(\min\{d^{3/2}/\varepsilon,d/\varepsilon^2\})$。这在某些参数下优于先前需要前向和逆向查询的方法，并且为逆向无振幅估计提供了新的上限。

Conclusion: 这项工作为量子态估计和振幅估计提供了更有效的方法，并对先前的一些猜想提出了质疑。

Abstract: In this paper, we present an inverse-free pure quantum state estimation
protocol that achieves Heisenberg scaling. Specifically, let $\mathcal{H}\cong
\mathbb{C}^d$ be a $d$-dimensional Hilbert space with an orthonormal basis
$\{|1\rangle,\ldots,|d\rangle\}$ and $U$ be an unknown unitary on
$\mathcal{H}$. Our protocol estimates $U|d\rangle$ to within trace distance
error $\varepsilon$ using $O(\min\{d^{3/2}/\varepsilon,d/\varepsilon^2\})$
forward queries to $U$. This complements the previous result
$O(d\log(d)/\varepsilon)$ by van Apeldoorn, Cornelissen, Gily\'en, and
Nannicini (SODA 2023), which requires both forward and inverse queries.
Moreover, our result implies a query upper bound
$O(\min\{d^{3/2}/\varepsilon,1/\varepsilon^2\})$ for inverse-free amplitude
estimation, improving the previous best upper bound
$O(\min\{d^{2}/\varepsilon,1/\varepsilon^2\})$ based on optimal unitary
estimation by Haah, Kothari, O'Donnell, and Tang (FOCS 2023), and disproving a
conjecture posed in Tang and Wright (2025).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [271] [SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving](https://arxiv.org/abs/2510.24949)
*Anil Yildiz,Sarah M. Thornton,Carl Hildebrandt,Sreeja Roy-Singh,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: SCOUT是一个轻量级模型，用于从代理的传感器表示中预测场景覆盖范围，避免了昂贵的人工标注或LVLM推理，从而实现了高效、可扩展的场景覆盖率评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估自主代理鲁棒性的方法依赖昂贵的人工标注或计算密集型的LVLM，在大规模部署时成本高昂且效率低下。

Method: 提出SCOUT（场景覆盖监督和理解工具），一个轻量级的替代模型，通过蒸馏过程进行训练，以逼近LVLM生成的覆盖标签，而无需持续的LVLM推理或人工标注。该方法利用预计算的感知特征，避免了冗余计算。

Result: SCOUT在大量真实自动导航场景数据上进行了评估，结果显示其在保持高精度的同时显著降低了计算成本。

Conclusion: SCOUT为大规模场景覆盖分析提供了一种有效且实用的替代方案，代表了在自主系统中实现高效场景覆盖监督的重要一步，尽管其性能取决于训练标签的质量。

Abstract: Assessing scenario coverage is crucial for evaluating the robustness of
autonomous agents, yet existing methods rely on expensive human annotations or
computationally intensive Large Vision-Language Models (LVLMs). These
approaches are impractical for large-scale deployment due to cost and
efficiency constraints. To address these shortcomings, we propose SCOUT
(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate
model designed to predict scenario coverage labels directly from an agent's
latent sensor representations. SCOUT is trained through a distillation process,
learning to approximate LVLM-generated coverage labels while eliminating the
need for continuous LVLM inference or human annotation. By leveraging
precomputed perception features, SCOUT avoids redundant computations and
enables fast, scalable scenario coverage estimation. We evaluate our method
across a large dataset of real-life autonomous navigation scenarios,
demonstrating that it maintains high accuracy while significantly reducing
computational cost. Our results show that SCOUT provides an effective and
practical alternative for large-scale coverage analysis. While its performance
depends on the quality of LVLM-generated training labels, SCOUT represents a
major step toward efficient scenario coverage oversight in autonomous systems.

</details>


### [272] [Smooth path planning with safety margins using Piece-Wise Bezier curves](https://arxiv.org/abs/2510.24972)
*Iancu Andrei,Marius Kloetzer,Cristian Mahulea,Catalin Dosoftei*

Main category: cs.RO

TL;DR: 提出一种计算高效的二次规划（QP）方法，用于使用分段二次贝塞尔（PWB）曲线为移动机器人生成平滑、C1连续的路径。


<details>
  <summary>Details</summary>
Motivation: 为了在保持轨迹平滑性和鲁棒性的同时，实现适用于实时和嵌入式应用的、具有可控数值复杂性的路径规划。

Method: 采用二次规划（QP）方法，并结合分段二次贝塞尔（PWB）曲线来生成路径。该方法将安全裕度明确地纳入结构化优化框架中。

Result: 与传统的分段线性（PWL）路径规划方法相比，该方法在模拟中表现出更小的轨迹偏差、更强的鲁棒性和更高的路径质量。通过在代表性场景中使用纯跟踪控制器进行的模拟验证了这些优势。

Conclusion: 该二次规划方法在分段二次贝塞尔曲线的帮助下，能够有效地生成平滑、C1连续且鲁棒的移动机器人路径，适用于实时和嵌入式应用，并在安全性方面优于传统方法。

Abstract: In this paper, we propose a computationally efficient quadratic programming
(QP) approach for generating smooth, $C^1$ continuous paths for mobile robots
using piece-wise quadratic Bezier (PWB) curves. Our method explicitly
incorporates safety margins within a structured optimization framework,
balancing trajectory smoothness and robustness with manageable numerical
complexity suitable for real-time and embedded applications. Comparative
simulations demonstrate clear advantages over traditional piece-wise linear
(PWL) path planning methods, showing reduced trajectory deviations, enhanced
robustness, and improved overall path quality. These benefits are validated
through simulations using a Pure-Pursuit controller in representative
scenarios, highlighting the practical effectiveness and scalability of our
approach for safe navigation.

</details>


### [273] [Defect Mitigation for Robot Arm-based Additive Manufacturing Utilizing Intelligent Control and IOT](https://arxiv.org/abs/2510.24994)
*Matsive Ali,Blake Gassen,Sen Liu*

Main category: cs.RO

TL;DR: 本文提出了一种集成了闭环热控制和智能原地缺陷校正的机器人增材制造系统，该系统使用六轴机器人手臂和Oak-D摄像头，能够对打印过程中的热量进行精确控制，并实时检测和修正打印缺陷，以提高打印质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 在增材制造（3D打印）过程中，实时质量控制和缺陷修复是一个巨大的挑战。现有的系统往往无法在打印过程中实时检测和修正缺陷，导致打印件质量不佳。本文旨在开发一个集成的机器人增材制造系统，通过闭环热控制和智能原地缺陷校正来解决这一问题，从而提高打印件的质量和可靠性。

Method: 本研究提出并实现了一个集成的机器人增材制造系统。该系统使用一个六轴机器人手臂，末端装配了一个带有IoT微控制器进行闭环热控制的E3D热端，以精确控制温度。机器人运动与送丝系统通过ROS2同步，确保在复杂轨迹上的均匀沉积。一个基于OpenCV的视觉系统能够逐层检测打印缺陷的位置，并指令机器人对识别出的缺陷进行自主的再挤出修复。系统还利用逆向运动学进行运动规划，并通过单应性变换校正相机视角，以精确定位缺陷。

Result: 实验验证表明，该集成系统能够成功地在打印过程中检测并修复缺陷，例如表面缺陷。系统在不中断打印过程的情况下，有效解决了实时质量保证的挑战，实现了对打印件表面异常的智能修复。

Conclusion: 本研究成功开发了一个集成的机器人增材制造系统，该系统结合了实时热量调节、运动控制以及智能缺陷检测与修复。该系统能够有效控制打印过程中的热量，并实时检测和修复打印缺陷，显著提高了打印件的质量和可靠性。该框架具有可扩展性和自适应性，适用于航空航天、生物医学和工业制造等领域。

Abstract: This paper presents an integrated robotic fused deposition modeling additive
manufacturing system featuring closed-loop thermal control and intelligent
in-situ defect correction using a 6-degree of freedom robotic arm and an Oak-D
camera. The robot arm end effector was modified to mount an E3D hotend
thermally regulated by an IoT microcontroller, enabling precise temperature
control through real-time feedback. Filament extrusion system was synchronized
with robotic motion, coordinated via ROS2, ensuring consistent deposition along
complex trajectories. A vision system based on OpenCV detects layer-wise
defects position, commanding autonomous re-extrusion at identified sites.
Experimental validation demonstrated successful defect mitigation in printing
operations. The integrated system effectively addresses challenges real-time
quality assurance. Inverse kinematics were used for motion planning, while
homography transformations corrected camera perspectives for accurate defect
localization. The intelligent system successfully mitigated surface anomalies
without interrupting the print process. By combining real-time thermal
regulation, motion control, and intelligent defect detection & correction, this
architecture establishes a scalable and adaptive robotic additive manufacturing
framework suitable for aerospace, biomedical, and industrial applications.

</details>


### [274] [Scalable predictive processing framework for multitask caregiving robots](https://arxiv.org/abs/2510.25053)
*Hayato Idei,Tamon Miyake,Tetsuya Ogata,Yuichi Yamashita*

Main category: cs.RO

TL;DR: 本研究提出了一种基于预测编码的层次化多模态循环神经网络，用于解决护理机器人泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有护理机器人大多是任务特定的，并且依赖于手工制作的预处理，这限制了它们在不同场景下的泛化能力。而人类大脑通过分层的预测处理来整合多模态感觉信号，从而实现灵活的认知和行为。

Method: 提出了一种基于预测编码（自由能原理）的层次化多模态循环神经网络，能够直接整合超过30,000维的视觉本体感觉输入，无需进行降维。该模型被训练以学习刚体复位和毛巾擦拭两个代表性护理任务。

Result: 1. 模型能够自主组织层次化潜在动态，以调节任务转换、捕捉不确定性变化并推断被遮挡的状态。 2. 通过整合视觉和本体感觉信息，模型能够应对视觉退化的情况。 3. 在多任务学习中，模型表现出不对称干扰：不确定性较大的毛巾擦拭任务对刚体复位任务影响很小，而学习刚体复位任务则导致毛巾擦拭性能略有下降，但模型整体保持了鲁棒性。

Conclusion: 预测编码可以作为一种通用且可扩展的计算原理，有望用于开发鲁棒、灵活和自主的护理机器人，并为理解人脑在不确定现实环境中实现灵活适应的能力提供理论见解。

Abstract: The rapid aging of societies is intensifying demand for autonomous care
robots; however, most existing systems are task-specific and rely on
handcrafted preprocessing, limiting their ability to generalize across diverse
scenarios. A prevailing theory in cognitive neuroscience proposes that the
human brain operates through hierarchical predictive processing, which
underlies flexible cognition and behavior by integrating multimodal sensory
signals. Inspired by this principle, we introduce a hierarchical multimodal
recurrent neural network grounded in predictive processing under the
free-energy principle, capable of directly integrating over 30,000-dimensional
visuo-proprioceptive inputs without dimensionality reduction. The model was
able to learn two representative caregiving tasks, rigid-body repositioning and
flexible-towel wiping, without task-specific feature engineering. We
demonstrate three key properties: (i) self-organization of hierarchical latent
dynamics that regulate task transitions, capture variability in uncertainty,
and infer occluded states; (ii) robustness to degraded vision through
visuo-proprioceptive integration; and (iii) asymmetric interference in
multitask learning, where the more variable wiping task had little influence on
repositioning, whereas learning the repositioning task led to a modest
reduction in wiping performance, while the model maintained overall robustness.
Although the evaluation was limited to simulation, these results establish
predictive processing as a universal and scalable computational principle,
pointing toward robust, flexible, and autonomous caregiving robots while
offering theoretical insight into the human brain's ability to achieve flexible
adaptation in uncertain real-world environments.

</details>


### [275] [Non-Invasive Calibration Of A Stewart Platform By Photogrammetry](https://arxiv.org/abs/2510.25072)
*Sourabh Karmakar,Cameron J. Turner*

Main category: cs.RO

TL;DR: 校准Stewart平台具有挑战性，因为正运动学可能产生多个解。本研究提出了一种基于Denavit-Hartenberg（DH）参数和摄影测量的新型正运动学校准方法，用于Tiger 66.1 Stewart平台。该方法通过最小二乘法进行误差补偿，并使用高分辨率相机和现成软件，无需修改硬件即可提高平台精度。


<details>
  <summary>Details</summary>
Motivation: Stewart平台在精确高效运行方面，其准确校准至关重要。然而，利用正运动学进行校准是一个挑战，因为正运动学通常会为移动平台生成多个可行的和不可行的解。执行器路径之间的复杂运动关系进一步增加了建立直接有效的校准方法的难度。

Method: 提出了一种新的基于正运动学的校准方法，并采用Denavit-Hartenberg（DH）参数。使用Tiger 66.1 Stewart平台进行实验。通过摄影测量策略，利用高分辨率数字相机和现成软件捕获移动平台中心的姿态。将目标姿态与实际姿态进行比较，并使用最小二乘法估计误差补偿以计算预测姿态。

Result: 三种补偿策略都显著提高了Stewart平台的姿态精度，表明仍有改进空间。

Conclusion: 所提出的摄影测量校准方法能够有效提高Stewart平台的精度，且该方法是非侵入性的，无需对硬件进行任何改动。

Abstract: Accurate calibration of a Stewart platform is important for their precise and
efficient operation. However, the calibration of these platforms using forward
kinematics is a challenge for researchers because forward kinematics normally
generates multiple feasible and unfeasible solutions for any pose of the moving
platform. The complex kinematic relations among the six actuator paths
connecting the fixed base to the moving platform further compound the
difficulty in establishing a straightforward and efficient calibration method.
The authors developed a new forward kinematics-based calibration method using
Denavit-Hartenberg convention and used the Stewart platform Tiger 66.1
developed in their lab for experimenting with the photogrammetry-based
calibration strategies described in this paper. This system became operational
upon completion of construction, marking its inaugural use. The authors used
their calibration model for estimating the errors in the system and adopted
three compensation options or strategies as per Least Square method to improve
the accuracy of the system. These strategies leveraged a high-resolution
digital camera and off-the-shelf software to capture the poses of the moving
platform's center. This process is non-invasive and does not need any
additional equipment to be attached to the hexapod or any alteration of the
hexapod hardware. This photogrammetry-based calibration process involves
multiple high-resolution images from different angles to measure the position
and orientation of the platform center in the three-dimensional space. The
Target poses and Actual poses are then compared, and the error compensations
are estimated using the Least-Squared methods to calculate the Predicted poses.
Results from each of the three compensation approaches demonstrated noticeable
enhancements in platform pose accuracies, suggesting room for further
improvements.

</details>


### [276] [Mean-Shift Theory and Its Applications in Swarm Robotics: A New Way to Enhance the Efficiency of Multi-Robot Collaboration](https://arxiv.org/abs/2510.25086)
*Guibin Sun,Jinhu Lü,Kexin Liu,Zhenqian Wang,Guanrong Chen*

Main category: cs.RO

TL;DR: 尽管基于分配的方法在机器人群体协作中发挥了重要作用，但其在效率和鲁棒性方面存在固有的局限性。本文回顾了无分配协作的最新进展，重点关注形状形成问题，并介绍了“均值漂移探索”策略，该策略显著提高了大型群体的协作效率，尤其是在规模增大时。最后，文章讨论了该策略在精确形状形成、面积覆盖和机动编队等方面的应用及其在智能仓储、区域探索和货物运输等工业场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 创建工程机器人群体的类似群体智能是一个挑战，其中基于分配的方法在效率和鲁棒性方面存在固有的局限性，并且难以扩展到群体变体。

Method: 本文回顾了无分配机器人群体协作的最新进展，重点关注形状形成问题，并重点介绍了“均值漂移探索”策略。

Result: “均值漂移探索”策略将大型群体协作效率提高了数十倍，并且随着群体规模的增加，效率的提高更加显著。

Conclusion: “均值漂移探索”策略在精确形状形成、面积覆盖形成和机动形成方面具有重要应用，并能在智能仓储、区域探索和货物运输等工业场景中得到应用。

Abstract: Swarms evolving from collective behaviors among multiple individuals are
commonly seen in nature, which enables biological systems to exhibit more
efficient and robust collaboration. Creating similar swarm intelligence in
engineered robots poses challenges to the design of collaborative algorithms
that can be programmed at large scales. The assignment-based method has played
an eminent role for a very long time in solving collaboration problems of robot
swarms. However, it faces fundamental limitations in terms of efficiency and
robustness due to its unscalability to swarm variants. This article presents a
tutorial review on recent advances in assignment-free collaboration of robot
swarms, focusing on the problem of shape formation. A key theoretical component
is the recently developed \emph{mean-shift exploration} strategy, which
improves the collaboration efficiency of large-scale swarms by dozens of times.
Further, the efficiency improvement is more significant as the swarm scale
increases. Finally, this article discusses three important applications of the
mean-shift exploration strategy, including precise shape formation, area
coverage formation, and maneuvering formation, as well as their corresponding
industrial scenarios in smart warehousing, area exploration, and cargo
transportation.

</details>


### [277] [NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies](https://arxiv.org/abs/2510.25122)
*Jiahong Chen,Jing Wang,Long Chen,Chuwei Cai,Jinghui Lu*

Main category: cs.RO

TL;DR: NanoVLA是轻量级视觉-语言-动作（VLA）模型系列，旨在在资源受限的边缘设备上实现高效的机器人操作。它通过视觉-语言解耦、长短动作分块和动态路由来优化推理效率，与现有模型相比，推理速度提高了52倍，参数减少了98%，同时保持了任务准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署高计算需求的视觉-语言-动作（VLA）模型以实现机器人操作仍然是一个挑战，因为功耗、延迟和计算资源至关重要。

Method: NanoVLA通过以下创新实现轻量化：1）视觉-语言解耦：将早期融合推迟到后期，以提高性能、实现缓存和降低推理开销及延迟。2）长短动作分块：确保流畅、连贯的多步规划，同时保持实时响应能力。3）动态路由：根据任务复杂度自适应地分配轻量级或重型骨干网络，以进一步优化推理效率。

Result: NanoVLA在基准测试和实际部署中，与之前的最先进VLA模型相比，在边缘设备上的推理速度提高了52倍，参数减少了98%，同时保持或超越了任务准确性和泛化能力。消融研究证实了其解耦策略保留了跨任务的可转移性，并且路由模块增强了成本效益。

Conclusion: NanoVLA通过其轻量级架构，实现了在资源受限硬件上进行实际、高精度的机器人操作，解决了当前VLA模型在边缘设备部署的瓶颈。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
manipulation by integrating vision-language models (VLMs), and action decoders
into a unified architecture. However, their deployment on resource-constrained
edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin
Nano), remains challenging due to high computational demands, especially in
real-world scenarios where power, latency, and computational resources are
critical. To close this gap, we introduce Nano-scale Vision-Language Action
(NanoVLA), a family of lightweight VLA architectures that achieve high
performance with minimal resources. Our core innovations include: (1)
vision-language decoupling that moves conventional early vision and language
inputs fusion in VLM to late stage, achieving better performance while enabling
caching and reduce inference overhead and latency; (2) long-short action
chunking to ensure smooth, coherent multi-step planning without sacrificing
real-time responsiveness; (3) dynamic routing that adaptively assigns
lightweight or heavy backbones based on task complexity, further optimizing
inference efficiency. Experimental results on several benchmarks, as well as
real-world deployments, demonstrate that NanoVLA achieves up to 52x faster
inference on edge devices compared to previous state-of-the-art VLA models,
with 98% less parameters while maintaining or surpassing their task accuracy
and generalization. Ablation studies confirm that our decoupling strategy
preserves cross-task transferability, and the routing module enhances
cost-performance trade-offs, enabling practical, high-precision robotic
manipulation on resource-constrained hardware.

</details>


### [278] [Learning Spatial-Aware Manipulation Ordering](https://arxiv.org/abs/2510.25138)
*Yuxiang Yan,Zhiyuan Zhou,Xin Gao,Guanghao Li,Shenglin Li,Jiaqi Chen,Qunyan Pu,Jian Pu*

Main category: cs.RO

TL;DR: OrderMind是一个空间感知操纵排序框架，通过学习基于空间上下文的物体操纵优先级，解决了杂乱环境中的操纵问题，并在模拟和现实世界中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理杂乱环境中的物体操纵顺序时，忽视了物体间的空间关系，导致灵活性和可扩展性受限。

Method: OrderMind集成了空间上下文编码器和时间优先级结构化模块，使用k-NN构建空间图来聚合几何信息，并考虑了物体-物体和物体-操纵器交互，同时引入了空间先验标注方法来生成监督信号，并指导视觉语言模型进行蒸馏。

Result: 在模拟和现实世界环境中，OrderMind在操纵排序基准（包含163,222个样本）上进行了评估，结果显示其在有效性和效率方面显著优于先前的方法。

Conclusion: OrderMind能够有效地处理杂乱场景中的物体操纵顺序问题，并实现鲁棒的操纵。

Abstract: Manipulation in cluttered environments is challenging due to spatial
dependencies among objects, where an improper manipulation order can cause
collisions or blocked access. Existing approaches often overlook these spatial
relationships, limiting their flexibility and scalability. To address these
limitations, we propose OrderMind, a unified spatial-aware manipulation
ordering framework that directly learns object manipulation priorities based on
spatial context. Our architecture integrates a spatial context encoder with a
temporal priority structuring module. We construct a spatial graph using
k-Nearest Neighbors to aggregate geometric information from the local layout
and encode both object-object and object-manipulator interactions to support
accurate manipulation ordering in real-time. To generate physically and
semantically plausible supervision signals, we introduce a spatial prior
labeling method that guides a vision-language model to produce reasonable
manipulation orders for distillation. We evaluate OrderMind on our Manipulation
Ordering Benchmark, comprising 163,222 samples of varying difficulty. Extensive
experiments in both simulation and real-world environments demonstrate that our
method significantly outperforms prior approaches in effectiveness and
efficiency, enabling robust manipulation in cluttered scenes.

</details>


### [279] [SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning](https://arxiv.org/abs/2510.25191)
*Hongyu Song,Rishabh Dev Yadav,Cheng Guo,Wei Pan*

Main category: cs.RO

TL;DR: SoraNav是一个为无人机设计的自适应导航框架，它结合了零样本视觉-语言模型（VLM）的推理能力和几何感知决策，以克服现有方法在小规模3D环境中进行语言驱动导航的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航（VLN）方法主要针对地面机器人，难以泛化到需要完整3D空间推理的空中任务；大型VLM虽然具备语义推理能力，但缺乏空间接地，不直接适用于导航。

Method: SoraNav框架将几何先验信息融入图像标注，以约束VLM的动作空间并提升决策质量。采用混合切换策略，利用导航历史在VLM推理和基于几何的探索之间进行切换，以减少死胡同和重复访问。此外，还构建了一个基于PX4的软硬件平台（包括数字孪生和物理微型无人机）来进行可重复的评估。

Result: 在2.5D场景中，SoraNav相比基线方法，成功率（SR）提升了25.7%，路径长度加权的成功率（SPL）提升了17%。在3D场景中，SR提升了29.5%，SPL提升了18.5%。

Conclusion: SoraNav通过结合VLM的零样本推理和几何感知决策，有效解决了无人机在3D环境中进行语言驱动导航的挑战，并在实验中取得了显著的性能提升。

Abstract: Interpreting visual observations and natural language instructions for
complex task execution remains a key challenge in robotics and AI. Despite
recent advances, language-driven navigation is still difficult, particularly
for UAVs in small-scale 3D environments. Existing Vision-Language Navigation
(VLN) approaches are mostly designed for ground robots and struggle to
generalize to aerial tasks that require full 3D spatial reasoning. The
emergence of large Vision-Language Models (VLMs), such as GPT and Claude,
enables zero-shot semantic reasoning from visual and textual inputs. However,
these models lack spatial grounding and are not directly applicable to
navigation. To address these limitations, SoraNav is introduced, an adaptive
UAV navigation framework that integrates zero-shot VLM reasoning with
geometry-aware decision-making. Geometric priors are incorporated into image
annotations to constrain the VLM action space and improve decision quality. A
hybrid switching strategy leverages navigation history to alternate between VLM
reasoning and geometry-based exploration, mitigating dead-ends and redundant
revisits. A PX4-based hardware-software platform, comprising both a digital
twin and a physical micro-UAV, enables reproducible evaluation. Experimental
results show that in 2.5D scenarios, our method improves Success Rate (SR) by
25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it
improves SR by 29.5% and SPL by 18.5% relative to the baseline.

</details>


### [280] [RoadSens-4M: A Multimodal Smartphone & Camera Dataset for Holistic Road-way Analysis](https://arxiv.org/abs/2510.25211)
*Amith Khandakar,David Michelson,Shaikh Golam Rabbani,Fariya Bintay Shafi,Md. Faysal Ahamed,Khondokar Radwanur Rahman,Md Abidur Rahman,Md. Fahmidun Nabi,Mohamed Arselene Ayari,Khaled Khan,Ponnuthurai Nagaratnam Suganthan*

Main category: cs.RO

TL;DR: 该论文提出了一个集成了地理信息系统（GIS）数据、天气信息和道路状况视频的新型智能交通数据集，以解决现有道路监测数据集质量不高和标准化程度低的问题。通过手机APP收集GPS、加速度计、陀螺仪等多种传感器数据，并结合车辆速度、加速度、旋转速率、磁场强度以及GIS、天气和视频数据，实现了对道路状况（如颠簸和坑洼）更清晰的分析。


<details>
  <summary>Details</summary>
Motivation: 当前道路监测领域缺乏高质量、标准化的数据集，阻碍了利用智能手机传感器进行成本效益分析的进展。

Method: 开发了一个移动应用程序，收集包括GPS、加速度计、陀螺仪、磁力计、重力传感器和方向传感器在内的智能手机传感器数据，并整合了地理信息系统（GIS）数据、天气信息和道路视频，以提供地理空间背景下的道路状况的全面理解。

Result: 创建了一个多维度的数据集，整合了车辆传感器数据、GIS数据、天气信息和视频素材，以实现对道路状况的详细分析。

Conclusion: 该数据集旨在为改善交通管理、基础设施发展、道路安全和城市规划提供资金支持，并通过公开访问促进智能交通领域的研究和创新。

Abstract: It's important to monitor road issues such as bumps and potholes to enhance
safety and improve road conditions. Smartphones are equipped with various
built-in sensors that offer a cost-effective and straightforward way to assess
road quality. However, progress in this area has been slow due to the lack of
high-quality, standardized datasets. This paper discusses a new dataset created
by a mobile app that collects sensor data from devices like GPS,
accelerometers, gyroscopes, magnetometers, gravity sensors, and orientation
sensors. This dataset is one of the few that integrates Geographic Information
System (GIS) data with weather information and video footage of road
conditions, providing a comprehensive understanding of road issues with
geographic context. The dataset allows for a clearer analysis of road
conditions by compiling essential data, including vehicle speed, acceleration,
rotation rates, and magnetic field intensity, along with the visual and spatial
context provided by GIS, weather, and video data. Its goal is to provide
funding for initiatives that enhance traffic management, infrastructure
development, road safety, and urban planning. Additionally, the dataset will be
publicly accessible to promote further research and innovation in smart
transportation systems.

</details>


### [281] [Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery](https://arxiv.org/abs/2510.25233)
*Jee Won Lee,Hansol Lim,Sooyeun Yang,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: 该研究提出了一种混合视觉跟踪框架，通过结合全局模板匹配、基于深度特征的 Lucas-Kanade 算法和残差回归器，实现了在部分或完全遮挡情况下的鲁棒机器人目标跟踪。当视觉置信度下降时，使用 GRU 预测器进行姿态外推。该框架能够提供 30Hz 图像伺服控制所需的控制信号，并在高达 90% 的遮挡情况下将跟踪误差维持在 2px 以内。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉伺服控制系统在部分或完全遮挡下目标跟踪鲁棒性不足的挑战，克服传统 Lucas-Kanade 方法易漂移和深度学习方法计算量大的缺点。

Method: 提出一种混合视觉跟踪框架：1. 使用快速全局模板匹配缩小搜索区域；2. 使用 VGG 网络早期层的深度特征进行 Lucas-Kanade 算法对齐，达到亚像素精度；3. 使用轻量级残差回归器校正纹理退化或部分遮挡引起的局部错误对齐；4. 当视觉置信度低于阈值时，使用基于 GRU 的预测器从运动历史中推断姿态更新。最后将计算出的位移、旋转和尺度变化作为控制信号直接用于 30Hz 图像伺服环路。

Result: 在具有高达 90% 遮挡的手持视频序列上进行了评估，系统将跟踪误差维持在 2px 以内，证明了其在机器人视觉应用中的鲁棒性和低延迟精度。

Conclusion: 所提出的混合视觉跟踪框架能够有效应对遮挡问题，在保持低延迟的同时实现高精度的机器人目标跟踪，满足实际机器人视觉应用的需求。

Abstract: Vision-based control systems, such as image-based visual servoing (IBVS),
have been extensively explored for precise robot manipulation. A persistent
challenge, however, is maintaining robust target tracking under partial or full
occlusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking
but are fragile to occlusion and drift, while deep learning-based approaches
often require continuous visibility and intensive computation. To address these
gaps, we propose a hybrid visual tracking framework that bridges advanced
perception with real-time servo control. First, a fast global template matcher
constrains the pose search region; next, a deep-feature Lucas-Kanade module
operating on early VGG layers refines alignment to sub-pixel accuracy (<2px);
then, a lightweight residual regressor corrects local misalignments caused by
texture degradation or partial occlusion. When visual confidence falls below a
threshold, a GRU-based predictor seamlessly extrapolates pose updates from
recent motion history. Crucially, the pipeline's final outputs-translation,
rotation, and scale deltas-are packaged as direct control signals for 30Hz
image-based servo loops. Evaluated on handheld video sequences with up to 90%
occlusion, our system sustains under 2px tracking error, demonstrating the
robustness and low-latency precision essential for reliable real-world robot
vision applications.

</details>


### [282] [One-shot Humanoid Whole-body Motion Learning](https://arxiv.org/abs/2510.25241)
*Hao Huang,Geeta Chandra Raju Bethala,Shuaihang Yuan,Congcong Wen,Anthony Tzes,Yi Fang*

Main category: cs.RO

TL;DR: 本研究提出了一种仅用一个非行走目标动作样本和易于获取的行走动作样本，即可训练有效的全身人形运动策略的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要每个动作类别有多个训练样本，导致高质量的人形动作数据集的收集既耗时又昂贵。

Method: 利用保持顺序的最优传输计算行走和非行走序列之间的距离，然后沿着测地线进行插值以生成新的中间姿态骨架，然后针对无碰撞配置进行优化，并重新定向到人形机器人，最后通过强化学习在模拟环境中进行策略训练。

Result: 实验评估表明，该方法在CMU MoCap数据集上持续优于基线方法，在各项指标上均取得了卓越的性能。

Conclusion: 本研究成功提出了一种高效的人形运动策略训练方法，解决了现有方法在数据收集方面的痛点。

Abstract: Whole-body humanoid motion represents a cornerstone challenge in robotics,
integrating balance, coordination, and adaptability to enable human-like
behaviors. However, existing methods typically require multiple training
samples per motion category, rendering the collection of high-quality human
motion datasets both labor-intensive and costly. To address this, we propose a
novel approach that trains effective humanoid motion policies using only a
single non-walking target motion sample alongside readily available walking
motions. The core idea lies in leveraging order-preserving optimal transport to
compute distances between walking and non-walking sequences, followed by
interpolation along geodesics to generate new intermediate pose skeletons,
which are then optimized for collision-free configurations and retargeted to
the humanoid before integration into a simulated environment for policy
training via reinforcement learning. Experimental evaluations on the CMU MoCap
dataset demonstrate that our method consistently outperforms baselines,
achieving superior performance across metrics. Code will be released upon
acceptance.

</details>


### [283] [Solving the Right Problem with Multi-Robot Formations](https://arxiv.org/abs/2510.25422)
*Chaz Cornwall,Jeremy P. Bos*

Main category: cs.RO

TL;DR: 提出一种编队规划方法，通过优化相对位置来减少编队形状与原始成本函数之间的不匹配，从而提高多机器人系统的成本函数优化效率。


<details>
  <summary>Details</summary>
Motivation: 在多机器人编队控制中，将复杂成本函数编码为固定形状（如菱形或方形）会引入形状维持与原始成本最小化之间的偏差，尤其是在环境信息增加时，静态形状无法有效最小化保护成本。因此，需要一种方法来减少这种偏差，同时仍利用高效的编队控制器。

Method: 提出一个两步优化的编队规划方法：1. 估计非线性、不可微成本的代理函数（加权代理成本函数），并理论分析了权重更新的条件。2. 使用基于李亚普诺夫直接法的非合作编队控制器，通过最小化代理成本函数来实现期望的相对机器人位置。

Result: 在军事场景（如保护和避障）中，仿真结果表明编队规划器可以将单一成本降低超过75%。当同时最小化多个成本函数时，自适应权重的编队规划器可以将成本降低20-40%。

Conclusion: 编队规划方法通过最小化接近原始成本函数的代理成本函数，而非依赖于形状抽象，能够提供更好的性能。

Abstract: Formation control simplifies minimizing multi-robot cost functions by
encoding a cost function as a shape the robots maintain. However, by reducing
complex cost functions to formations, discrepancies arise between maintaining
the shape and minimizing the original cost function. For example, a Diamond or
Box formation shape is often used for protecting all members of the formation.
When more information about the surrounding environment becomes available, a
static shape often no longer minimizes the original protection cost. We propose
a formation planner to reduce mismatch between a formation and the cost
function while still leveraging efficient formation controllers. Our formation
planner is a two-step optimization problem that identifies desired relative
robot positions. We first solve a constrained problem to estimate non-linear
and non-differentiable costs with a weighted sum of surrogate cost functions.
We theoretically analyze this problem and identify situations where weights do
not need to be updated. The weighted, surrogate cost function is then minimized
using relative positions between robots. The desired relative positions are
realized using a non-cooperative formation controller derived from Lyapunov's
direct approach. We then demonstrate the efficacy of this approach for
military-like costs such as protection and obstacle avoidance. In simulations,
we show a formation planner can reduce a single cost by over 75%. When
minimizing a variety of cost functions simultaneously, using a formation
planner with adaptive weights can reduce the cost by 20-40%. Formation planning
provides better performance by minimizing a surrogate cost function that
closely approximates the original cost function instead of relying on a shape
abstraction.

</details>


### [284] [Time-Optimal Transport of Loosely Placed Liquid Filled Cups along Prescribed Paths](https://arxiv.org/abs/2510.25255)
*Klaus Zauner,Hubert Gattringer,Andreas Mueller*

Main category: cs.RO

TL;DR: 本论文研究如何用机械臂在最短时间内运输装有液体的杯子，通过结合液体晃动动力学和最优控制来减少液体溢出。


<details>
  <summary>Details</summary>
Motivation: 处理松散放置的物体，尤其是装有液体的容器，对于机械臂的轨迹规划和控制来说是一个挑战，目标是避免溢出。

Method: 将液体晃动动力学纳入最优控制问题，并使用直接多重 shooting 方法求解。

Result: 成功地在最短时间内运输了装有液体的杯子，并尽量减少了晃动，避免了液体溢出。

Conclusion: 通过将晃动动力学整合到最优控制框架中，可以有效地解决运输液体容器的挑战。

Abstract: Handling loosely placed objects with robotic manipulators is a difficult task
from the point of view of trajectory planning and control. This becomes even
more challenging when the object to be handled is a container filled with
liquid. This paper addresses the task of transporting a liquid-filled cup
placed on a tray along a prescribed path in shortest time. The objective is to
minimize swapping, thus avoiding spillage of the fluid. To this end, the
sloshing dynamics is incorporated into the dynamic model used within the
optimal control problem formulation. The optimization problem is solved using a
direct multiple shooting approach.

</details>


### [285] [SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation](https://arxiv.org/abs/2510.25268)
*Wang zhi,Yuyan Liu,Liu Liu,Li Zhang,Ruixuan Lu,Dan Guo*

Main category: cs.RO

TL;DR: 本论文提出了一种名为SynHLMA的新框架，用于根据语言指令生成手部抓取序列，特别关注具有形变能力的关节式物体（articulated objects）。该框架能够处理物体变形和长期操控序列，实现了抓取生成、预测和插值等任务，并在HAOI-lang数据集上验证了其优越性能，同时在机器人抓取应用中展现了潜力。


<details>
  <summary>Details</summary>
Motivation: 将语言指令转化为手部抓取的研究广泛应用于具身AI和VR/AR领域。然而，在手部操控关节式物体交互（HAOI）时，不仅需要考虑物体的功能性，还需要处理其形变和长期的操控序列。因此，需要一个能够处理这些复杂性的框架。

Method: SynHLMA框架利用离散的HAOI表示来建模每一帧的手物体交互。结合自然语言嵌入，通过HAOI操控语言模型进行训练，将抓取过程与其语言描述对齐到共享表示空间。引入了联合感知损失（joint-aware loss）来确保手部抓取能够适应关节式物体的动态变化。

Result: SynHLMA成功完成了HAOI生成、HAOI预测和HAOI插值这三种典型的关节式物体手部操控任务。在HAOI-lang数据集上的实验结果表明，其手部抓取序列生成性能优于现有最先进的方法。此外，该框架还在机器人抓取应用中展示了通过模仿学习执行灵巧抓取的能力。

Conclusion: SynHLMA框架在处理具有形变能力的关节式物体的语言指令驱动的手部抓取生成方面表现出色，能够生成符合语言描述的长期操控序列，并在机器人应用中具有实际价值。

Abstract: Generating hand grasps with language instructions is a widely studied topic
that benefits from embodied AI and VR/AR applications. While transferring into
hand articulatied object interaction (HAOI), the hand grasps synthesis requires
not only object functionality but also long-term manipulation sequence along
the object deformation. This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects. Given a complete point cloud of an articulated object, we utilize a
discrete HAOI representation to model each hand object interaction frame. Along
with the natural language embeddings, the representations are trained by an
HAOI manipulation language model to align the grasping process with its
language description in a shared representation space. A joint-aware loss is
employed to ensure hand grasps follow the dynamic variations of articulated
object joints. In this way, our SynHLMA achieves three typical hand
manipulation tasks for articulated objects of HAOI generation, HAOI prediction
and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art. We also show a robotics grasp
application that enables dexterous grasps execution from imitation learning
using the manipulation sequence provided by our SynHLMA. Our codes and datasets
will be made publicly available.

</details>


### [286] [Development of Implicit-Explicit Control Based Amphibious Centipede-Type Robot and Evaluation of its Mobile Performance](https://arxiv.org/abs/2510.25280)
*Yusuke Tsunoda,Seiya Yamamoto,Kazuki Ito,Runze Xiao,Keisuke Naniwa,Koichi Osuka*

Main category: cs.RO

TL;DR: 该研究开发了一种集成式步态控制方案，用于在陆地和水生环境中的蜈蚣式多足移动机器人，通过优化腿部结构来简化控制和提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有在不同环境（如陆地和水域）之间导航的多足机器人通常需要根据新环境设计和切换控制器，这在复杂环境中具有挑战性。

Method: 开发了一种蜈蚣式机器人，其特点是具有灵活的关节和每个身体分段上的左右腿，并采用基于隐式-显式控制哲学的简单统一控制方案。研究了三种不同的腿部结构，并使用机器人的腿部滑移率和执行器能耗作为评估指标。

Result: 实验结果表明，存在一种合适的腿部结构，可以在相同的控制下同时在水生和陆地环境中进行导航。

Conclusion: 通过巧妙的机器人主体结构设计和集成式控制方案，可以实现多足机器人在不同环境（如陆地和水域）下的有效导航，而无需复杂的控制器切换。

Abstract: Multi-legged mobile robots possess high mobility performance in rough terrain
environments, stemming from their high postural stability, joint flexibility,
and the redundancy provided by multiple legs. In prior research on navigating
between different environments such as land and water, the primary strategy
employed involves switching to a controller that generates an appropriate gait
for the new environment upon entering it. However, designing appropriate gaits
for each complex and diverse environment and accurately determining controller
switching for each environment is challenging. Therefore, this research
develops a centipede-type mobile robot that navigates both aquatic and
terrestrial environments with a simple, unified control scheme, based on the
implicit-explicit control philosophy and by ingeniously designing the robot's
body structure. In this research, we developed the robot featuring flexible
joints and left and right legs on each body segment and focused on the leg
structure which has extensive contact with the environment. This paper
evaluates the locomotion performance on land and water using the three
developed leg structures, using the robot's leg slip rate and actuator energy
consumption as evaluation metrics. The experimental results confirmed the
existence of an appropriate leg structure capable of navigating both aquatic
and terrestrial environments under identical control.

</details>


### [287] [An approach for combining transparency and motion assistance of a lower body exoskeleton](https://arxiv.org/abs/2510.25335)
*Jakob Ziegler,Bernhard Rameder,Hubert Gattringer,Andreas Mueller*

Main category: cs.RO

TL;DR: 通过利用执行单元的齿轮齿隙来实现透明模式，并利用自适应振荡器来学习运动，从而实现下半身外骨骼的步态辅助。


<details>
  <summary>Details</summary>
Motivation: 结合透明度和运动辅助的概念来实现下半身外骨骼的步态辅助。

Method: 透明模式通过利用执行单元的齿轮齿隙来实现，在步行时，叠加的辅助模式施加额外的扭矩将腿引导至估计的未来位置，并利用自适应振荡器学习典型的准周期运动信号。

Result: 首次实验显示出有希望的结果。

Conclusion: 结合透明度和运动辅助的概念，并利用自适应振荡器，可以为下半身外骨骼提供有效的步态辅助。

Abstract: In this paper, an approach for gait assistance with a lower body exoskeleton
is described. Two concepts, transparency and motion assistance, are combined.
The transparent mode, where the system is following the user's free motion with
a minimum of perceived interaction forces, is realized by exploiting the gear
backlash of the actuation units. During walking a superimposed assistance mode
applies an additional torque guiding the legs to their estimated future
position. The concept of adaptive oscillators is utilized to learn the
quasi-periodic signals typical for locomotion. First experiments showed
promising results.

</details>


### [288] [Geometric Robot Calibration Using a Calibration Plate](https://arxiv.org/abs/2510.25338)
*Bernhard Rameder,Hubert Gattringer,Andreas Mueller*

Main category: cs.RO

TL;DR: 提出了一种使用带有点精确已知距离的校准板的机器人几何校准新方法。


<details>
  <summary>Details</summary>
Motivation: 与激光跟踪器或运动捕捉系统等传统测量方法相比，校准板提供了一种更具机械鲁棒性、更便宜且由于体积小而更易于运输的替代方案。

Method: 使用校准板上的两个点之间的相对测量来确定系统的预定义误差参数。使用最小二乘法和约束优化问题来识别误差参数。

Result: 在实验中证明了该方法的有效性，其结果与激光校准器的结果相吻合，前景广阔。

Conclusion: 所提出的校准方法、校准板设计、误差系统的数学描述以及参数识别对于龙门架机床进行了详细说明，但并未局限于此类机器人。

Abstract: In this paper a new method for geometric robot calibration is introduced,
which uses a calibration plate with precisely known distances between its
measuring points. The relative measurement between two points on the
calibration plate is used to determine predefined error parameters of the
system. In comparison to conventional measurement methods, like laser tracker
or motion capture systems, the calibration plate provides a more mechanically
robust and cheaper alternative, which is furthermore easier to transport due to
its small size. The calibration method, the plate design, the mathematical
description of the error system as well as the identification of the parameters
are described in detail. For identifying the error parameters, the least
squares method and a constrained optimization problem are used. The
functionality of this method was demonstrated in experiments that led to
promising results, correlated with one of a laser tracker calibration. The
modeling and identification of the error parameters is done for a gantry
machine, but is not restricted to that type of robot.

</details>


### [289] [Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods](https://arxiv.org/abs/2510.25386)
*Kumar Manas,Mert Keser,Alois Knoll*

Main category: cs.RO

TL;DR: 该调查分析了将法律和逻辑规范整合到自动驾驶系统的感知、预测和规划模块中的当前方法，并强调了在动态和不确定的驾驶环境中确保监管合规性和可解释性的能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统在感知可靠性、法律合规性和决策可辩护性方面的挑战，该系统旨在整合法律和逻辑规范。

Method: 对包括神经网络-符号集成方法、逻辑驱动的规则表示和规范感知预测策略在内的技术进行了系统审查，并引入了一个分类法来根据理论基础、架构实现和验证策略对现有方法进行分类。

Result: 自动驾驶系统在处理感知不确定性和整合明确的法律规范以促进技术稳健和法律可辩护的决策方面取得了重大进展，但仍存在重大挑战。

Conclusion: 尽管在实现透明和可问责的自动驾驶汽车运行方面取得了进展，但仍有关键的开放性问题和实际的权衡需要跨学科（工程、逻辑和法律）的合作来解决，以指导未来在合法合规的自动驾驶系统方面的发展。

Abstract: This survey provides an analysis of current methodologies integrating legal
and logical specifications into the perception, prediction, and planning
modules of automated driving systems. We systematically explore techniques
ranging from logic-based frameworks to computational legal reasoning
approaches, emphasizing their capability to ensure regulatory compliance and
interpretability in dynamic and uncertain driving environments. A central
finding is that significant challenges arise at the intersection of perceptual
reliability, legal compliance, and decision-making justifiability. To
systematically analyze these challenges, we introduce a taxonomy categorizing
existing approaches by their theoretical foundations, architectural
implementations, and validation strategies. We particularly focus on methods
that address perceptual uncertainty and incorporate explicit legal norms,
facilitating decisions that are both technically robust and legally defensible.
The review covers neural-symbolic integration methods for perception,
logic-driven rule representation, and norm-aware prediction strategies, all
contributing toward transparent and accountable autonomous vehicle operation.
We highlight critical open questions and practical trade-offs that must be
addressed, offering multidisciplinary insights from engineering, logic, and law
to guide future developments in legally compliant autonomous driving systems.

</details>


### [290] [Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning](https://arxiv.org/abs/2510.25405)
*Kei Ikemura,Yifei Dong,David Blanco-Mulero,Alberta Longhini,Li Chen,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的强化学习方法，通过引入惩罚性奖励来减少对易损物体的操作损伤，并结合离线演示和课程学习来加速训练，实现了零样本的仿真到真实世界的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作易损物体的方法依赖于精确的模型或专用传感器/夹具，存在复杂性高、泛化性差的问题。

Method: 提出了一种基于视觉的强化学习方法，在奖励函数中加入了对物体压力的惩罚项，并结合了离线演示和从刚性代理到可变形物体的课程学习策略来辅助训练。

Result: 在仿真和真实世界中均取得了良好的效果，所学策略能够以零样本的方式将仿真中学到的技能迁移到真实世界，成功完成了抓取和推送豆腐等任务，并将施加在易损物体上的压力降低了36.5%，同时完成了任务目标。

Conclusion: 该方法通过损伤感知和轻柔操作的行为，有效降低了对易损物体的损伤，证明了其在机器人操作易损物体方面的有效性。

Abstract: Robotic manipulation of deformable and fragile objects presents significant
challenges, as excessive stress can lead to irreversible damage to the object.
While existing solutions rely on accurate object models or specialized sensors
and grippers, this adds complexity and often lacks generalization. To address
this problem, we present a vision-based reinforcement learning approach that
incorporates a stress-penalized reward to discourage damage to the object
explicitly. In addition, to bootstrap learning, we incorporate offline
demonstrations as well as a designed curriculum progressing from rigid proxies
to deformables. We evaluate the proposed method in both simulated and
real-world scenarios, showing that the policy learned in simulation can be
transferred to the real world in a zero-shot manner, performing tasks such as
picking up and pushing tofu. Our results show that the learned policies exhibit
a damage-aware, gentle manipulation behavior, demonstrating their effectiveness
by decreasing the stress applied to fragile objects by 36.5% while achieving
the task goals, compared to vanilla RL policies.

</details>


### [291] [Combining Moving Mass Actuators and Manoeuvring Models for Underwater Vehicles: A Lagrangian Approach](https://arxiv.org/abs/2510.25479)
*Alexander B. Rambech,Ivar B. Saksvik,Vahid Hassani*

Main category: cs.RO

TL;DR: 我们提出了一个带有内部移动质量执行器的水下航行器的运动方程的牛顿-欧拉方程。


<details>
  <summary>Details</summary>
Motivation: 提出一个包含内部移动质量执行器的水下航行器的牛顿-欧拉方程。

Method: 使用牛顿-欧拉方程来制定水下航行器的运动方程，并将移动质量动力学表示为Fossen（1991）提出的水下航行器机动模型的扩展。使用Kirchhoff方程推导了科里奥利-离心效应，并使用第一性原理推导了静 hydrostatics。

Result: 通过仿真验证了所提出的牛顿-欧拉模型，并与传统的哈密顿内部移动质量执行器模型进行了比较。

Conclusion: 所提出的牛顿-欧拉模型可以准确地描述水下航行器的运动，并能有效地处理内部移动质量执行器。

Abstract: In this paper, we present a Newton-Euler formulation of the equations of
motion for underwater vehicles with an interntal moving mass actuator.
Furthermore, the moving mass dynamics are expressed as an extension to the
manoeuvring model for underwater vehicles, originally introduced by Fossen
(1991). The influence of the moving mass is described in body-frame and
included as states in both an additional kinematic equation and as part of the
coupled rigid-body kinetics of the underwater vehicle. The Coriolis-centripetal
effects are derived from Kirchhoff's equations and the hydrostatics are derived
using first principals. The proposed Newton-Euler model is validated through
simulation and compared with the traditional Hamiltonian internal moving mass
actuator formulation.

</details>


### [292] [Octopus-like Reaching Motion: A Perspective Inspired by Whipping](https://arxiv.org/abs/2510.25520)
*Shengyao Zhang,Yiyuan Zhang,Chenrui Zhang,Yiming Li,Wenci Xin,Yuliang Liufu,Hong Wei Ng,Cecilia Laschi*

Main category: cs.RO

TL;DR: 章鱼的触手运动可能不仅仅是被动鞭打行为，水的存在对其运动至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究章鱼的典型抓取运动，因为其对高度可变形身体的高效控制引起了越来越多的关注。之前的研究表明，其特征性的弯曲传播可能与鞭子的动力学具有共同的潜在原理。

Method: 在水中和空气中进行基于平台的鞭打测试，同时系统地改变材料刚度和驱动速度。通过基于图像的量化来分析运动。

Result: 在水中，由 150 rpm 驱动的 Ecoflex Gel 2 臂复制了类似于章鱼抓取中观察到的曲率传播。然而，其弯曲点速度单调下降，而不是呈现生物学的钟形曲线。在空气中没有观察到传播。这表明章鱼的抓取运动不是单纯的被动鞭打行为，并且周围介质在形成类似章鱼的抓取运动中起着关键作用。

Conclusion: 章鱼的抓取运动不是单纯的被动鞭打行为，水的存在对其运动至关重要。这项研究为理解生物抓取运动提供了新的视角，并为未来的流体动力学研究提供了潜在的平台。

Abstract: The stereotypical reaching motion of the octopus arm has drawn growing
attention for its efficient control of a highly deformable body. Previous
studies suggest that its characteristic bend propagation may share underlying
principles with the dynamics of a whip. This work investigates whether
whip-like passive dynamics in water can reproduce the kinematic features
observed in biological reaching and their similarities and differences.
Platform-based whipping tests were performed in water and air while
systematically varying material stiffness and driving speed. Image-based
quantification revealed that the Ecoflex Gel 2 arm driven at 150 rpm (motor
speed) reproduced curvature propagation similar to that observed in octopus
reaching. However, its bend-point velocity decreased monotonically rather than
exhibiting the biological bell-shaped profile, confirming that the octopus
reaching movement is not merely a passive whipping behavior. The absence of
propagation in air further highlights the critical role of the surrounding
medium in forming octopus-like reaching motion. This study provides a new
perspective for understand biological reaching movement, and offers a potential
platform for future hydrodynamic research.

</details>


### [293] [Using VLM Reasoning to Constrain Task and Motion Planning](https://arxiv.org/abs/2510.25548)
*Muyang Yan,Miras Mengdibayev,Ardon Floros,Weihang Guo,Lydia E. Kavraki,Zachary Kingston*

Main category: cs.RO

TL;DR: VOZ-COAST 利用大型预训练视觉-语言模型来识别任务和运动规划中的向下细化问题，从而在规划前绕过这些问题。


<details>
  <summary>Details</summary>
Motivation: 当抽象的领域向下细化到连续运动时，任务和运动规划的有效性可能会受到影响。先前的方法在细化失败时才添加约束，导致在无效分支上花费了大量的搜索时间。我们需要一种方法来在规划前识别这些细化问题。

Method: VOZ-COAST 利用大型预训练视觉-语言模型来对图像和领域描述进行常识性空间推理，以识别潜在的向下细化问题，并提取适当的约束来提前修剪无效的任务计划。

Result: VOZ-COAST 能够从图像和领域描述中提取合理的约束，从而大大减少了规划时间，并完全消除了向下细化问题，同时还能推广到更广泛领域中的各种实例。

Conclusion: VOZ-COAST 通过利用大型预训练视觉-语言模型的常识性空间推理能力，有效地解决了任务和运动规划中的向下细化问题，提高了规划效率和成功率。

Abstract: In task and motion planning, high-level task planning is done over an
abstraction of the world to enable efficient search in long-horizon robotics
problems. However, the feasibility of these task-level plans relies on the
downward refinability of the abstraction into continuous motion. When a
domain's refinability is poor, task-level plans that appear valid may
ultimately fail during motion planning, requiring replanning and resulting in
slower overall performance. Prior works mitigate this by encoding refinement
issues as constraints to prune infeasible task plans. However, these approaches
only add constraints upon refinement failure, expending significant search
effort on infeasible branches. We propose VIZ-COAST, a method of leveraging the
common-sense spatial reasoning of large pretrained Vision-Language Models to
identify issues with downward refinement a priori, bypassing the need to fix
these failures during planning. Experiments on two challenging TAMP domains
show that our approach is able to extract plausible constraints from images and
domain descriptions, drastically reducing planning times and, in some cases,
eliminating downward refinement failures altogether, generalizing to a diverse
range of instances from the broader domain.

</details>


### [294] [Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills](https://arxiv.org/abs/2510.25634)
*Weikang Wan,Fabio Ramos,Xuning Yang,Caelan Garrett*

Main category: cs.RO

TL;DR: 该研究提出了一种分层框架，将长时程、富接触的双臂操作视为一个集成的技能规划与调度问题，支持技能的并行调用，而非纯粹的顺序决策。


<details>
  <summary>Details</summary>
Motivation: 长时程、富接触的双臂操作面临挑战，需要复杂的协调，包括混合的并行执行和顺序协作。

Method: 提出一个分层框架，将问题建模为技能规划与调度问题。该框架基于单臂和双臂的原始技能库，每个技能都通过强化学习（RL）在GPU加速的模拟环境中训练。然后，使用基于Transformer的规划器，在技能组合的数据集上进行训练，以充当高级调度器，同时预测离散的技能调度及其连续参数。

Result: 该方法在复杂的、富接触的任务上取得了比端到端强化学习方法更高的成功率，并且比传统的仅顺序规划器产生了更有效、更协调的行为。

Conclusion: 所提出的分层框架能够有效地解决长时程、富接触的双臂操作问题，并在性能和协调性方面优于现有方法。

Abstract: Long-horizon contact-rich bimanual manipulation presents a significant
challenge, requiring complex coordination involving a mixture of parallel
execution and sequential collaboration between arms. In this paper, we
introduce a hierarchical framework that frames this challenge as an integrated
skill planning & scheduling problem, going beyond purely sequential
decision-making to support simultaneous skill invocation. Our approach is built
upon a library of single-arm and bimanual primitive skills, each trained using
Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a
Transformer-based planner on a dataset of skill compositions to act as a
high-level scheduler, simultaneously predicting the discrete schedule of skills
as well as their continuous parameters. We demonstrate that our method achieves
higher success rates on complex, contact-rich tasks than end-to-end RL
approaches and produces more efficient, coordinated behaviors than traditional
sequential-only planners.

</details>


### [295] [Collision avoidance and path finding in a robotic mobile fulfillment system using multi-objective meta-heuristics](https://arxiv.org/abs/2510.25650)
*Ahmad Kokhahi,Mary Kurz*

Main category: cs.RO

TL;DR: 本研究提出了一种结合能量消耗和旅行时间的碰撞避免策略，并使用NSGA和ALNS算法解决多AGV任务分配问题。


<details>
  <summary>Details</summary>
Motivation: 在多AGV路径规划中，除了最小化碰撞和旅行时间外，还考虑了能量消耗。

Method: 提出了一种新的碰撞避免策略，并提出了两种多目标任务分配算法：非支配排序遗传算法（NSGA）和自适应大邻域搜索（ALNS）。

Result: 所提出的方法在碰撞避免和任务分配方面均优于现有方法。

Conclusion: 所提出的方法在多AGV路径规划中，在考虑能量消耗的同时，能够有效地解决碰撞和任务分配问题。

Abstract: Multi-Agent Path Finding (MAPF) has gained significant attention, with most
research focusing on minimizing collisions and travel time. This paper also
considers energy consumption in the path planning of automated guided vehicles
(AGVs). It addresses two main challenges: i) resolving collisions between AGVs
and ii) assigning tasks to AGVs. We propose a new collision avoidance strategy
that takes both energy use and travel time into account. For task assignment,
we present two multi-objective algorithms: Non-Dominated Sorting Genetic
Algorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative
evaluations show that these proposed methods perform better than existing
approaches in both collision avoidance and task assignment.

</details>


### [296] [Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models](https://arxiv.org/abs/2510.25713)
*Boshi An,Chenyu Yang,Robert Katzschmann*

Main category: cs.RO

TL;DR: 本研究对预训练的视觉-语言-动作（VLA）模型Open-VLA进行了改进，以实现无需过多语言提示的灵巧人机协作。


<details>
  <summary>Details</summary>
Motivation: 目标是改进预训练的VLA模型，以实现更高效、更少依赖语言提示的人机协作。

Method: 研究通过（i）对视觉骨干网添加FiLM条件以实现任务感知、（ii）增加一个辅助意图头来预测协作者手部姿势和目标线索、（iii）在动作空间进行后处理，预测紧凑的增量（位置/旋转）和PCA降维的手指关节，然后映射到完整命令。

Result: 研究使用包含MediaPipe手部姿势的多视角、远程操作的Franka和Mimic-hand数据集，证明了增量动作表现良好，并且四个主成分解释了约96%的手部关节方差。消融实验表明，动作后处理是主要的性能驱动因素；辅助意图有帮助，FiLM效果好坏参半，方向运动损失有害。实时堆栈（在单个RTX 4090上约0.3秒延迟）将“拾取”和“传递”组合成长序列行为。

Conclusion: 研究确定了“训练者过拟合”特定演示者是关键的局限性。

Abstract: We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for
dexterous human-robot collaboration with minimal language prompting. Our
approach adds (i) FiLM conditioning to visual backbones for task-aware
perception, (ii) an auxiliary intent head that predicts collaborator hand pose
and target cues, and (iii) action-space post-processing that predicts compact
deltas (position/rotation) and PCA-reduced finger joints before mapping to full
commands. Using a multi-view, teleoperated Franka and Mimic-hand dataset
augmented with MediaPipe hand poses, we demonstrate that delta actions are
well-behaved and that four principal components explain ~96% of hand-joint
variance. Ablations identify action post-processing as the primary performance
driver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is
detrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes
"pick-up" and "pass" into a long-horizon behavior. We surface "trainer
overfitting" to specific demonstrators as the key limitation.

</details>


### [297] [A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation](https://arxiv.org/abs/2510.25725)
*Eunju Kwon,Seungwon Oh,In-Chang Baek,Yucheon Park,Gyungbo Kim,JaeYoung Moon,Yunho Choi,Kyung-Joong Kim*

Main category: cs.RO

TL;DR: 该研究提出了一个包含柔性软体操作的人类视觉-触觉-动作数据集，以弥补现有机器人学习数据集在处理非刚性物体和多变压力条件方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习数据集主要关注刚性物体，未能充分体现现实世界操作中柔性物体和多变压力条件的复杂性。

Method: 利用配备灵巧手的人形机器人，通过远程操作收集了在不同压力条件下进行柔性软体操作的多模态交互数据。

Result: 构建了一个包含柔性软体操作的人形视觉-触觉-动作数据集，其中包含多模态交互和不同压力条件下的数据。

Conclusion: 该数据集为未来研究提供了基础，特别是对能够有效利用触觉信号复杂性和多样性的高级优化策略模型的研究具有重要的激励作用。

Abstract: Contact-rich manipulation has become increasingly important in robot
learning. However, previous studies on robot learning datasets have focused on
rigid objects and underrepresented the diversity of pressure conditions for
real-world manipulation. To address this gap, we present a humanoid
visual-tactile-action dataset designed for manipulating deformable soft
objects. The dataset was collected via teleoperation using a humanoid robot
equipped with dexterous hands, capturing multi-modal interactions under varying
pressure conditions. This work also motivates future research on models with
advanced optimization strategies capable of effectively leveraging the
complexity and diversity of tactile signals.

</details>


### [298] [Modeling Collapse of Steered Vine Robots Under Their Own Weight](https://arxiv.org/abs/2510.25727)
*Ciera McFarland,Margaret McGuinness*

Main category: cs.RO

TL;DR: 软体藤蔓式生长机器人可以通过抽出（eversion）的方式移动，在狭窄环境中具有高机动性，但在遇到间隙时可能会因自身重量而坍塌。本研究提出了一个全面的坍塌模型，该模型可以利用真实形状信息和尾部张力来预测任意形状机器人的坍塌长度。通过对无真实形状信息的非引导机器人进行实验验证，该模型能够准确预测实验趋势。随后，研究人员尝试在不同方向上引导一个单驱动器机器人，模型也准确预测了其坍塌情况。最后，通过让机器人进行有无充气状态下的间隙穿越任务，展示了该模型的实际应用价值，结果表明机器人需要充气才能成功穿越间隙而不发生坍塌，这与模型预测一致。该模型已在直线型和串联式囊式驱动的非弹性材料机器人上进行了测试，但也可应用于其他机器人变体，为理解机器人在开放环境中导航和成功完成3D导航任务所需的参数提供了支持。


<details>
  <summary>Details</summary>
Motivation: 软体藤蔓式生长机器人在狭窄环境中移动能力强，但在遇到间隙时容易因自身重量而坍塌，限制了其在开放环境中的应用。因此，需要一个模型来预测和理解机器人的坍塌行为，以克服这一挑战。

Method: 提出并验证了一个能够预测任意形状软体机器人坍塌长度的通用模型。该模型利用真实形状信息和尾部张力，并通过实验（包括非引导机器人和单驱动器引导机器人）进行了验证。最后，通过间隙穿越任务展示了该模型的实际应用。

Result: 所提出的坍塌模型能够准确预测无真实形状信息的非引导机器人的坍塌趋势，并且在引导机器人实验中也能准确预测坍塌情况。间隙穿越实验表明，机器人需要充气才能成功穿越间隙而不坍塌，模型对此进行了支持。

Conclusion: 该研究提出的坍塌模型能够预测软体藤蔓式生长机器人在开放环境中的坍塌行为，并识别其成功完成3D导航任务所需的关键参数。这项工作为在更复杂的环境中部署此类机器人铺平了道路。

Abstract: Soft, vine-inspired growing robots that move by eversion are highly mobile in
confined environments, but, when faced with gaps in the environment, they may
collapse under their own weight while navigating a desired path. In this work,
we present a comprehensive collapse model that can predict the collapse length
of steered robots in any shape using true shape information and tail tension.
We validate this model by collapsing several unsteered robots without true
shape information. The model accurately predicts the trends of those
experiments. We then attempt to collapse a robot steered with a single actuator
at different orientations. Our models accurately predict collapse when it
occurs. Finally, we demonstrate how this could be used in the field by having a
robot attempt a gap-crossing task with and without inflating its actuators. The
robot needs its actuators inflated to cross the gap without collapsing, which
our model supports. Our model has been specifically tested on straight and
series pouch motor-actuated robots made of non-stretchable material, but it
could be applied to other robot variations. This work enables us to model the
robot's collapse behavior in any open environment and understand the parameters
it needs to succeed in 3D navigation tasks.

</details>


### [299] [GET-USE: Learning Generalized Tool Usage for Bimanual Mobile Manipulation via Simulated Embodiment Extensions](https://arxiv.org/abs/2510.25754)
*Bohan Wu,Paul de La Sayette,Li Fei-Fei,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 机器人可以通过模拟中探索“身体延伸”来学习通用工具使用，并在现实世界中应用这些策略。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人工具使用方法缺乏通用性，无法在多个可用对象中识别和使用最佳工具，尤其是在缺乏最优工具时。

Method: 提出GeT-USE方法，通过两步程序：1. 在模拟中学习扩展机器人本体（构建新的末端执行器）以识别有益的几何形状；2. 将学到的策略转移到真实机器人上进行操作。

Result: 在具有22自由度的真实机器人上，GeT-USE在三种基于视觉的双臂移动操作工具使用任务中的成功率比现有方法提高了30-60%。

Conclusion: GeT-USE通过在模拟中学习和扩展机器人本体，实现了更通用的机器人工具使用能力，并在真实机器人实验中取得了显著的性能提升。

Abstract: The ability to use random objects as tools in a generalizable manner is a
missing piece in robots' intelligence today to boost their versatility and
problem-solving capabilities. State-of-the-art robotic tool usage methods
focused on procedurally generating or crowd-sourcing datasets of tools for a
task to learn how to grasp and manipulate them for that task. However, these
methods assume that only one object is provided and that it is possible, with
the correct grasp, to perform the task; they are not capable of identifying,
grasping, and using the best object for a task when many are available,
especially when the optimal tool is absent. In this work, we propose GeT-USE, a
two-step procedure that learns to perform real-robot generalized tool usage by
learning first to extend the robot's embodiment in simulation and then
transferring the learned strategies to real-robot visuomotor policies. Our key
insight is that by exploring a robot's embodiment extensions (i.e., building
new end-effectors) in simulation, the robot can identify the general tool
geometries most beneficial for a task. This learned geometric knowledge can
then be distilled to perform generalized tool usage tasks by selecting and
using the best available real-world object as tool. On a real robot with 22
degrees of freedom (DOFs), GeT-USE outperforms state-of-the-art methods by
30-60% success rates across three vision-based bimanual mobile manipulation
tool-usage tasks.

</details>


### [300] [STITCH 2.0: Extending Augmented Suturing with EKF Needle Estimation and Thread Management](https://arxiv.org/abs/2510.25768)
*Kush Hari,Ziyang Chen,Hansoul Kim,Ken Goldberg*

Main category: cs.RO

TL;DR: STITCH 2.0 在缝合任务中提高了伤口闭合率和效率。


<details>
  <summary>Details</summary>
Motivation: 外科缝合技术对患者的愈合和疤痕有很大影响，但不同外科医生之间的缝合技术差异很大，因此需要机器人辅助。之前的机器人缝合技术（如STITCH 1.0）在完全闭合伤口方面存在困难，原因是针追踪不准确和线管理不佳。

Method: STITCH 2.0 是一个增强的自动化流水线，包含七项改进，包括：改进的扩展卡尔曼滤波（EKF）针姿态估计、新的缝线缠结方法以及自动化的3D缝线对齐算法。

Result: 在15次试验中，STITCH 2.0 平均实现了74.4%的伤口闭合率，每次试验平均使用4.87次缝合，与之前的基线相比，缝合次数增加了66%，用时减少了38%。允许两次人工干预的情况下，STITCH 2.0 平均使用六次缝合，伤口闭合率为100%。

Conclusion: STITCH 2.0 通过改进的针姿态估计、缝线缠结和3D缝线对齐等方法，显著提高了机器人缝合的伤口闭合率和效率，优于之前的技术。

Abstract: Surgical suturing is a high-precision task that impacts patient healing and
scarring. Suturing skill varies widely between surgeons, highlighting the need
for robot assistance. Previous robot suturing works, such as STITCH 1.0 [1],
struggle to fully close wounds due to inaccurate needle tracking and poor
thread management. To address these challenges, we present STITCH 2.0, an
elevated augmented dexterity pipeline with seven improvements including:
improved EKF needle pose estimation, new thread untangling methods, and an
automated 3D suture alignment algorithm. Experimental results over 15 trials
find that STITCH 2.0 on average achieves 74.4% wound closure with 4.87 sutures
per trial, representing 66% more sutures in 38% less time compared to the
previous baseline. When two human interventions are allowed, STITCH 2.0
averages six sutures with 100% wound closure rate. Project website:
https://stitch-2.github.io/

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [301] [What Are People's Actual Utility Functions in Budget Aggregation?](https://arxiv.org/abs/2510.24872)
*Ayelet Amster,Lioz Akirav,Rica Gonen,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 标准效用模型难以解释参与式预算中的人类偏好，但星形和峰线性效用函数显示出希望。


<details>
  <summary>Details</summary>
Motivation: 验证现有预算聚合机制中常用的效用模型（如l1、l2和Leontief）是否能准确反映人类偏好，因为关于人类如何评估非理想预算分配的经验证据很少。

Method: 通过结构化民意调查，让真人参与者评估预算分配，以检验他们的偏好是否符合l1、l2和Leontief等标准效用函数。

Result: 大多数参与者表现出不一致的模式，并且标准假设（如项目对称性和符号对称性）缺乏经验支持。然而，星形偏好和峰线性效用函数（其中效用随与理想预算的距离成比例变化）显示出令人鼓舞的一致性。

Conclusion: 虽然现有的理论结果表明在真实性和帕累托效率方面存在固有的不可能性，但本研究的经验证据表明，需要考虑替代的效用模型。此外，本研究提供了一种系统的方法来经验性地检验预算聚合理论所依据的效用函数假设，为设计更现实的机制铺平了道路。

Abstract: While participatory budgeting and budget-aggregation mechanisms require
assumptions about how voters evaluate non-ideal budget allocations, little
empirical evidence exists to validate which utility models accurately capture
human preferences. We conducted structured polls with human participants to
test whether real people's preferences conform to commonly assumed utility
functions such as $\ell_1$, $\ell_2$ and Leontief. Our results suggest that
these models may have limited explanatory power for actual behavior: most
participants showed inconsistent patterns across different metric comparisons,
and standard assumptions of project symmetry and sign symmetry -- core features
of common distance-based metrics -- received little empirical support. However,
we find encouraging evidence for more fundamental preference structures: a
large majority of participants showed consistency with star-shaped preferences,
as well as with peak-linear utility functions, where utility changes
proportionally with distance from the ideal budget. These findings have
important implications for designers of budget aggregation mechanisms. While
theoretical results demonstrate impossibility results for standard distance
metrics regarding truthfulness, Pareto-efficiency, and proportionality, our
evidence suggests alternative modeling approaches may be warranted. More
broadly, this work introduces a systematic methodology to empirically test the
utility function assumptions that underpin budget aggregation theories, paving
the way for more robust and realistic mechanism design.

</details>


### [302] [Fair Indivisible Payoffs through Shapley Value](https://arxiv.org/abs/2510.24906)
*Mikołaj Czarnecki,Michał Korniak,Oskar Skibski,Piotr Skowron*

Main category: cs.GT

TL;DR: 该论文提出了一种在包含不可分割物品的合作博弈中公平分配这些物品的方法，即“不可分割的Shapley值”。


<details>
  <summary>Details</summary>
Motivation: 解决在合作博弈中如何公平地分配不可分割物品（如议会席位、肾脏交换、机器学习模型中的关键特征等）的问题。

Method: 定义并研究“不可分割的Shapley值”的性质。

Result: 提出了一种新的公平分配方法，并通过三个案例研究进行了演示，特别是在图像分类任务中用于识别图像的关键区域。

Conclusion: 所提出的“不可分割的Shapley值”为分配不可分割物品提供了一种公平的解决方案，并已在实际案例中得到验证。

Abstract: We consider the problem of payoff division in indivisible coalitional games,
where the value of the grand coalition is a natural number. This number
represents a certain quantity of indivisible objects, such as parliamentary
seats, kidney exchanges, or top features contributing to the outcome of a
machine learning model. The goal of this paper is to propose a fair method for
dividing these objects among players. To achieve this, we define the
indivisible Shapley value and study its properties. We demonstrate our proposed
technique using three case studies, in particular, we use it to identify key
regions of an image in the context of an image classification task.

</details>


### [303] [Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games](https://arxiv.org/abs/2510.25080)
*Will Wolf*

Main category: cs.GT

TL;DR: 该研究介绍了“有界单方面响应博弈”（BORGs），这是一种新的博弈类型，并使用改进的 Monopoly Deal 作为基准环境。研究表明，标准的 Counterfactual Regret Minimization（CFR）算法能够有效解决此类博弈，并提供了一个包含环境、CFR 运行时和用户界面的研究平台。


<details>
  <summary>Details</summary>
Motivation: 探索一种新的博弈类型——有界单方面响应博弈（BORGs），并为研究此类博弈提供一个基准环境和研究平台。

Method: 使用改进的 Monopoly Deal 作为基准环境，并应用标准的 Counterfactual Regret Minimization（CFR）算法来训练博弈代理。

Result: CFR 算法成功收敛到有效的策略，证明了其在 BORG 动态中的适用性。研究人员还构建了一个包含环境、CFR 运行时和用户界面的研究平台。

Conclusion: BORGs 是一种具有战略意义的博弈结构，标准的 CFR 算法可以有效地解决此类博弈。所提供的研究平台为进一步探索状态表示和策略学习奠定了基础。

Abstract: Card games are widely used to study sequential decision-making under
uncertainty, with real-world analogues in negotiation, finance, and
cybersecurity. Typically, these games fall into three categories based on the
flow of control: strictly-sequential (where players alternate single actions),
deterministic-response (where some actions trigger a fixed outcome), and
unbounded reciprocal-response (where alternating counterplays are permitted). A
less-explored but strategically rich structure exists: the bounded one-sided
response. This dynamic occurs when a player's action briefly transfers control
to the opponent, who must satisfy a fixed condition through one or more
sequential moves before the turn resolves. We term games featuring this
mechanism Bounded One-Sided Response Games (BORGs).
  We introduce a modified version of Monopoly Deal as a benchmark environment
that specifically isolates the BORG dynamic, where a Rent action forces the
opponent to sequentially choose payment assets. We demonstrate that the
gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully
converges on effective strategies for this domain without requiring novel
algorithmic extensions. To support efficient, reproducible experimentation, we
present a lightweight, full-stack research platform that unifies the
environment, a parallelized CFR runtime, and a human-playable web interface,
all runnable on a single workstation. This system provides a practical
foundation for exploring state representation and policy learning in bounded
one-sided response settings.
  The trained CFR agent and source code are available at
https://monopolydeal.ai.

</details>


### [304] [Timing Games in Responsive Consensus Protocols](https://arxiv.org/abs/2510.25144)
*Kaya Alpturer,Kushal Babel,Aditya Saraf*

Main category: cs.GT

TL;DR: 区块链应用中的验证者会因为时间博弈而延迟提议，导致乐观响应能力难以实现。本研究通过引入动态区块奖励来解决这个问题，该奖励随轮次时间减少，激励验证者更快地提议。通过投票机制衡量延迟，并仔细设置协议参数，可以实现所有验证者的合作均衡，提高奖励率。虽然动态奖励会加剧不同连接状况验证者之间的延迟差距，但研究表明这种影响很小。


<details>
  <summary>Details</summary>
Motivation: 现有的共识协议设计中，乐观响应能力（共识协议以网络速度运行的能力）被广泛用于优化延迟和吞吐量。然而，区块链应用的验证者会通过战略性地延迟其提议来玩时间博弈，因为增加的区块时间与更高的奖励相关。这导致乐观响应能力在区块链协议中似乎变得不可能。

Method: 我们开发了一个关于响应式共识协议中时间博弈的模型，发现其具有囚徒困境结构。为了达到理想的均衡，我们引入了动态区块奖励，该奖励随着轮次时间的增加而减少，以明确激励更快的提议。通过其他验证者对当前领导者的轮次时间进行投票的投票机制来衡量延迟。

Result: 通过仔细设置协议参数，投票机制使验证者能够协调并达到合作均衡，通过更高的奖励率使所有人都受益。因此，响应能力本身可以促进更快的区块提议。从静态奖励转向动态奖励的一个结果是，验证者的效用对延迟变得更加敏感，加剧了连接状况最佳和最差的验证者之间的差距。然而，我们的分析表明，这种影响在理论延迟模型和基于真实网络模拟中都很小。

Conclusion: 乐观响应能力在区块链协议中是可行的，通过引入动态区块奖励和投票机制，可以激励验证者进行合作，从而提高整个网络的效率。

Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate
at the speed of the network -- is widely used in consensus protocol design to
optimize latency and throughput. However, blockchain applications incentivize
validators to play timing games by strategically delaying their proposals,
since increased block time correlates with greater rewards. Consequently, it
may appear that responsiveness (even under optimistic conditions) is impossible
in blockchain protocols. In this work, we develop a model of timing games in
responsive consensus protocols and find a prisoner's dilemma structure, where
cooperation (proposing promptly) is in the validators' best interest, but
individual incentives encourage validators to delay proposals selfishly. To
attain desirable equilibria, we introduce dynamic block rewards that decrease
with round time to explicitly incentivize faster proposals. Delays are measured
through a voting mechanism, where other validators vote on the current leader's
round time. By carefully setting the protocol parameters, the voting mechanism
allows validators to coordinate and reach the cooperative equilibrium,
benefiting all through a higher rate-of-reward. Thus, instead of responsiveness
being an unattainable property due to timing games, we show that responsiveness
itself can promote faster block proposals. One consequence of moving from a
static to dynamic block reward is that validator utilities become more
sensitive to latency, worsening the gap between the best- and worst-connected
validators. Our analysis shows, however, that this effect is minor in both
theoretical latency models and simulations based on real-world networks.

</details>


### [305] [On Robust Popular Matchings with Tie-Bounded Preferences and Stable Matchings with Two-Sided Ties](https://arxiv.org/abs/2510.25209)
*Koustav De*

Main category: cs.GT

TL;DR: 该论文研究了两种图匹配模型（单边和双边）中的鲁棒流行匹配问题。


<details>
  <summary>Details</summary>
Motivation: 研究单边和双边模型中的鲁棒流行匹配的存在性问题，并提出相应的判定算法。

Method: 在单边模型中，分析单个代理偏好改变时鲁棒流行匹配的存在性，并提出多项式时间算法。在双边模型中，对流行匹配进行刻画，并提出多项式时间算法判断是否存在鲁棒流行匹配。

Result: 提出了一种判定单边模型中鲁棒流行匹配存在性的多项式时间算法。对双边模型中的流行匹配进行了刻画，并证明了在单个代理偏好改变的情况下，存在一个多项式时间算法来判断是否存在鲁棒流行匹配。

Conclusion: 在单边模型和双边模型（允许单边存在 ties）中，存在判定鲁棒流行匹配的有效算法。

Abstract: We are given a bipartite graph $G = \left( A \cup B, E \right)$. In the
one-sided model, every $a \in A$ (often called agents) ranks its neighbours $z
\in N_{a}$ strictly, and no $b \in B$ has any preference order over its
neighbours $y \in N_{b}$, and vertices in $B$ abstain from casting their votes
to matchings. In the two-sided model with one-sided ties, every $a \in A$ ranks
its neighbours $z \in N_{a}$ strictly, and every $b \in B$ puts all of its
neighbours into a single large tie, i.e., $b \in B$ prefers every $y \in N_{b}$
equally. In this two-sided model with one-sided ties, when two matchings
compete in a majority election, $b \in B$ abstains from casting its vote for a
matching when both the matchings saturate $b$ or both leave $b$ unsaturated;
else $b$ prefers the matching where it is saturated. A popular matching $M$ is
\emph{robust} if it remains popular among multiple instances.
  We have analysed the cases when a robust popular matching exists in the
one-sided model where only one agent alters her preference order among the
instances, and we have proposed a polynomial-time algorithm to decide if there
exists a robust popular matching when instances differ only with respect to the
preference orders of a single agent.
  We give a simple characterisation of popular matchings in the two-sided model
with one-sided ties. We show that in the two-sided model with one-sided ties,
if the input instances differ only with respect to the preference orders of a
single agent, there is a polynomial-time algorithm to decide whether there
exists a robust popular matching. We have been able to decide the stable
matching problem in bipartite graphs $G = (A \cup B, E)$ where \textit{both}
sides have weak preferences (ties allowed), with the restriction that every tie
has length at most $k$.

</details>


### [306] [Learning-Augmented Online Bidding in Stochastic Settings](https://arxiv.org/abs/2510.25582)
*Spyros Angelopoulos,Bertrand Simon*

Main category: cs.GT

TL;DR: 在线竞价问题在学习增强的随机设置下被研究，寻找最优的算法以在一致性与鲁棒性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究在线竞价问题在包含随机性的学习增强环境下的表现，特别是考虑了预测器或算法本身的随机性。

Method: 研究了两种情况：1. 基于分布预测的竞价，找到在算法一致性和鲁棒性之间提供最佳权衡的帕累托最优算法。2. 随机竞价算法的效力和局限性，给出了一致性/鲁棒性权衡的上限和下限。

Result: 在基于分布预测的竞价中，发现了提供最佳权衡的帕累托最优算法。在随机竞价算法的研究中，得到了关于一致性/鲁棒性权衡的上限和下限。

Conclusion: 该研究扩展了在线竞价问题，首次在学习增强的随机设置下进行分析，并提供了关于算法设计和性能的理论保证，特别是在一致性和鲁棒性之间的权衡。

Abstract: Online bidding is a classic optimization problem, with several applications
in online decision-making, the design of interruptible systems, and the
analysis of approximation algorithms. In this work, we study online bidding
under learning-augmented settings that incorporate stochasticity, in either the
prediction oracle or the algorithm itself. In the first part, we study bidding
under distributional predictions, and find Pareto-optimal algorithms that offer
the best-possible tradeoff between the consistency and the robustness of the
algorithm. In the second part, we study the power and limitations of randomized
bidding algorithms, by presenting upper and lower bounds on the
consistency/robustness tradeoffs. Previous works focused predominantly on
oracles that do not leverage stochastic information on the quality of the
prediction, and deterministic algorithms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [307] [Cryogenic Characterization of Ferroelectric Non-volatile Capacitors](https://arxiv.org/abs/2510.25040)
*Madhav Vadlamani,Dyutimoy Chakraborty,Jianwei Jia,Halid Mulaosmanovic,Stefan Duenkel,Sven Beyer,Suman Datta,Shimeng Yu*

Main category: cs.ET

TL;DR: 铁电基电容交叉开关阵列在低温下可实现更高 ENOB (~5 位) 的能效高的片上计算。


<details>
  <summary>Details</summary>
Motivation: 解决电阻交叉开关阵列面临的漏电和高静态功耗问题，并降低铁电基电容交叉开关阵列因热噪声引起的 ENOB 限制。

Method: 在低温（低至 77K）下表征非易失性电容器（nvCaps），并使用 SPICE 模拟 128x128 阵列的 MAC 操作，以评估低温对 ENOB 的影响。

Result: 在低温下，128x128 阵列的 MAC 操作可以实现约 5 位的 ENOB。

Conclusion: 通过在低温下运行铁电基电容交叉开关阵列，可以有效降低热噪声，提高 ENOB，从而实现更节能的片上计算。

Abstract: Ferroelectric-based capacitive crossbar arrays have been proposed for
energy-efficient in-memory computing in the charge domain. They combat the
challenges like sneak paths and high static power faced by resistive crossbar
arrays but are susceptible to thermal noise limiting the effective number of
bits (ENOB) for the weighted sum. A direct way to reduce this thermal noise is
by lowering the temperature as thermal noise is proportional to temperature. In
this work, we first characterize the non-volatile capacitors (nvCaps) on a
foundry 28 nm platform at cryogenic temperatures to evaluate the memory window,
ON state retention as a function of temperature down to 77K, and then use the
calibrated device models to simulate the capacitive crossbar arrays in SPICE at
lower temperatures to demonstrate higher ENOB (~5 bits) for 128x128
multiple-and-accumulate (MAC) operations.

</details>


### [308] [Modulation Schemes for Functionalized Vesicle-based MC Transmitters](https://arxiv.org/abs/2510.25676)
*Teena tom Dieck,Lukas Brand,Sebastian Lotter,Kathrin Castiglione,Robert Schober,Maximilian Schäfer*

Main category: cs.ET

TL;DR: 该研究提出了一个更真实的分子通信发送端模型，并基于此模型提出两种新的调制方案，以减轻发送端引入的通信不确定性，提高通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的分子通信研究大多依赖简化的发送端模型，未能考虑实际生物硬件的物理和生化限制。本研究旨在通过提出一个更真实的发送端模型来推进实际分子通信系统的建模。

Method: 提出一个包含分子释放延迟和发送端噪声的、功能化的囊泡式发送端模型。基于此模型，提出两种新的调制方案以减轻发送端引入的记忆效应。

Result: 数值评估表明，所提出的调制方案在实际生化约束下提高了通信的可靠性。

Conclusion: 所提出的更真实的发送端模型和调制方案是实现可行的分子通信系统的重要一步。

Abstract: Molecular communication (MC) enables information exchange through the
transmission of signaling molecules (SMs) and holds promise for many innovative
applications. However, most existing MC studies rely on simplified transmitter
(TX) models that do not account for the physical and biochemical limitations of
realistic biological hardware. This work extends previous efforts toward
developing models for practical MC systems by proposing a more realistic TX
model that incorporates the delay in SM release and TX noise introduced by
biological components. Building on this more realistic, functionalized
vesicle-based TX model, we propose two novel modulation schemes specifically
designed for this TX to mitigate TX-induced memory effects that arise from
delayed and imperfectly controllable SM release. The proposed modulation
schemes enable low-complexity receiver designs by mitigating memory effects
directly at the TX. Numerical evaluations demonstrate that the proposed schemes
improve communication reliability under realistic biochemical constraints,
offering an important step toward physically realizable MC systems.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [309] [A novel approach to modelling the properties of HEMTs operating in the saturation region](https://arxiv.org/abs/2510.24745)
*Kaiyuan Zhao,Guangfen Yao,Xiaoyu Cheng,Luqiao Yin,Kailin Ren,Jianhua Zhang*

Main category: physics.app-ph

TL;DR: 本论文提出了一种新的HEMT模型，解决了现有模型依赖经验参数和无法精确模拟速度饱和效应等问题，并通过TCAD仿真验证了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有HEMT模型（ASM-HEMT、QPZD、EPFL）虽然基于三端势，但无法快速模拟速度饱和效应、载流子浓度分布和电场分布，导致模型依赖大量缺乏物理意义的经验参数。

Method: 改进了基于栅极有效长度的模型，实现了（1）无需牛顿迭代法即可计算Ids，具有快速仿真收敛性；（2）重新定义了不同Vgo下HEMT饱和时的Vdsat，而非直接使用Vdsat = Vgo；（3）基于不同的载流子传输特性，重新定义了v-E关系表达式，解决了栅极下方区域电场分布的模型误差大以及 I-V 特性准确性进一步提高的问题。

Result: 通过TCAD仿真表征了HEMT的I-V和E-V特性，验证了模型的有效性，均方根误差（RMSE）低于5%。

Conclusion: 本研究提出的新型HEMT模型能够精确模拟速度饱和效应和载流子分布，并且避免了对经验参数的依赖，提高了仿真的准确性和收敛速度。

Abstract: Currently, the ASM-HEMT model, QPZD model and EPFL model are all based on the
three-terminal potential as the core, and relate the electrical characteristics
such as I-V and C-V to Vd, Vs and Vg, so as to accurately build the HEMT model
with high accuracy and fast convergence. However, there has not yet been a
model based on three-terminal potentials that can quickly model the velocity
saturation effect as well as the carrier concentration distribution and the
electric field distribution inside the HEMT, which makes the existing models
have to rely on a number of empirical parameters in the modelling process,
which lacks the actual physical significance. In previous publications, models
for the electric field, carrier concentration distribution based on the
effective length of the gate were presented. In this paper, the model proposed
in previous publications is improved to enable: (1) the calculation of the
current Ids without relying on the Newton iterative method with fast simulation
convergence behavior; (2) The Vdsat when the HEMT reaches saturation at
different Vgo is redefined instead of using Vdsat = Vgo; (3) The expression of
the v-E relationship is redefined relying on the different transport of
carriers, which solves the problem of the large model error of the electric
field distribution in the region below the gate, and makes the model's accuracy
of the I-V characteristic further improved. The model was validated by
characterising the I-V and E-V of the HEMT through TCAD simulation with RMSE
below 5%.

</details>


### [310] [Highly efficient wideband and polarization-insensitive SMF-ARF coupling strategy with low back-reflection](https://arxiv.org/abs/2510.24746)
*Yi Su,Xuchen Hua,Bingyan Xue,Yucheng Yao,Zhiyong Zhao,Ming Tang*

Main category: physics.app-ph

TL;DR: 提出了一种基于透镜光纤的低损耗单模光纤与抗共振光纤的耦合策略。


<details>
  <summary>Details</summary>
Motivation: 为了实现单模光纤与抗共振光纤的低损耗连接，并满足高容量数据传输的需求。

Method: 通过优化结构和几何参数，设计了一种基于透镜光纤的耦合策略。

Result: 实验结果显示在1550 nm处具有1.2 dB的插损和-36.22 dB的回波损耗，并且在1500-1600 nm的波长范围内具有优异的光谱稳定性（低于0.72 dB的偏差）和偏振不敏感性（低于0.4 dB的偏振相关损耗）。

Conclusion: 所提出的紧凑结构不仅有利于制造过程，还能将抗共振光纤无缝集成到现有光网络中，从而满足高容量数据传输的关键需求。

Abstract: We propose a lensed-fiber based coupling strategy for low-loss
interconnection between single-mode fibers and anti-resonant fibers. By
optimizing structural and geometric parameters, the design simultaneously
achieves high coupling efficiency and suppressed back-reflection. Experimental
results demonstrate an insertion loss of 1.2 dB and back-reflection of -36.22
dB at 1550 nm, with excellent spectral stability (below 0.72 dB variation
across 1500-1600 nm) and polarization insensitivity (below 0.4 dB polarization
dependent loss). The compact structure not only facilitates the fabrication
process, but also enables seamless ARF integration into existing optical
networks, thereby addressing critical demands for high-capacity data
transmission.

</details>


### [311] [Artificial Transmission Line Synthesis Tailored for Traveling-Wave Parametric Processes](https://arxiv.org/abs/2510.24753)
*M. Malnou*

Main category: physics.app-ph

TL;DR: 本论文开发了一个用于设计人工传输线的通用合成框架，解决了现有设计方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的旅行波参数放大器（TWPA）虽然基于人工传输线，但缺乏系统的设计方法。

Method: 结合周期结构理论和无源网络综合，提出两种设计方法：周期加载合成（空间调制）和滤波器合成（频率相关响应）。框架允许加入非线性元件以用于参数过程，并确保能量和动量守恒。

Result: 设计了一个具有新颖相位匹配结构的动感电感TWPA，以及一个利用左右手传输线的反向泵浦约瑟夫森TWPA。

Conclusion: 提出的合成框架为设计人工传输线，特别是TWPA，提供了系统化的方法，并成功应用于具体的设计实例。

Abstract: Artificial transmission lines built with lumped-element inductors and
capacitors form the backbone of broadband, nearly quantum-limited
traveling-wave parametric amplifiers (TWPAs). However, systematic design
methods for TWPAs, and more generally artificial transmission lines, are
lacking. Here, I develop a general synthesis framework for lossless artificial
transmission lines by borrowing from periodic structure theory and passive
network synthesis. These complementary approaches divide the design space:
periodic loading synthesis employs spatial modulation of frequency-independent
components, while filter synthesis employs frequency-dependent responses in
spatially-uniform components. When tailoring transmission lines for parametric
processes, nonlinear elements are added, typically nonlinear inductances in
superconducting circuits, while ensuring energy and momentum conservation
between interacting tones. Applying this framework, I design a kinetic
inductance TWPA with a novel phase-matching architecture, and a backward-pumped
Josephson TWPA exploiting an ambidextrous i.e., right-left-handed transmission
line.

</details>


### [312] [2D Canonical Approach for Beating the Boltzmann Tyranny Using Memory](https://arxiv.org/abs/2510.24883)
*Rafael Schio Wengenroth Silva,Soumen Pradhan,Fabian Hartmann,Leonardo K. Castelano,Ovidiu Lipan,Sven Höfling,Victor Lopez-Richard*

Main category: physics.app-ph

TL;DR: 通过引入记忆效应，可以克服亚阈值摆幅的60mV/十倍频程限制，为低功耗电子器件提供新途径。


<details>
  <summary>Details</summary>
Motivation: 传统的场效应晶体管受到亚阈值摆幅60mV/十倍频程（Boltzmann tyranny）的限制，阻碍了器件的缩小。现有克服该限制的方法通常依赖于复杂的材料和制造工艺。

Method: 在Landauer-Büttiker量子输运形式下，结合电荷俘获机制，动态地重构了导带边缘。推导出了亚阈值摆幅的解析表达式，明确了记忆动态与栅极效率的关系。

Result: 模型表明，降低载流子产生速率或增强俘获活性可以实现低于热力学的开关特性，从而突破Boltzmann限制。该模型能够捕捉关键的实验特征，并提供通用的设计原则。

Conclusion: 基于记忆效应的晶体管是一种可靠的、实现超低功耗和多功能电子架构的途径。

Abstract: The 60 mV$/$decade subthreshold limit at room temperature, coined as the
Boltzmann tyranny, remains a fundamental obstacle to the continued down-scaling
of conventional transistors. While several strategies have sought to overcome
this constraint through non-thermal carrier injection, most rely on
ferroelectric-based or otherwise material-specific mechanisms that require
complex fabrication and stability control. Here, we develop a universal
theoretical framework showing that intrinsic memory effects in nanometric
field-effect transistors can naturally bypass this limit. Within the
Landauer-B\"uttiker quantum transport formalism, we incorporate charge-trapping
mechanisms that dynamically renormalize the conduction band edge. The resulting
analytical expression for the subthreshold swing explicitly links memory
dynamics to gate efficiency, revealing that a reduced carrier generation rate
or enhanced trapping activity leads to sub-thermal switching, thus breaking the
Boltzmann barrier. The model captures key experimental features and provides
clear, generalizable design principles, establishing memory-assisted
transistors as a robust pathway toward ultra-low-power and multifunctional
electronic architectures.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [313] [Maximum-Entropy Analog Computing Approaching ExaOPS-per-Watt Energy-efficiency at the RF-Edge](https://arxiv.org/abs/2510.24975)
*Aswin Undavalli,Kareem Rashed,Zhili Xiao,Arun Natarajan,Shantanu Chakrabartty,Aravind Nagulu*

Main category: cs.NE

TL;DR: 该论文提出了一种基于熵产生物理学和对称性约束的高性能、高能效模拟计算系统。


<details>
  <summary>Details</summary>
Motivation: 探索熵产生物理学和对称性约束在实现高性能、高能效模拟计算系统中的应用。

Method: 提出了一种广义最大熵原理，用于描述由互联模拟元件组成的介观物理系统的演化，并将最大熵状态与边际传播（MP）分布联系起来，用于计算相关性和内积。通过将框架扩展到非平衡或瞬态操作条件，进一步提升计算吞吐量和能效。

Result: 成功研制了一种采用22nm SOI CMOS工艺制造的射频（RF）相关器集成电路。在8位精度下，计算效率超过2 PetaOPS/W；在3位精度下，计算效率超过0.8 ExaOPS/W，支持速率超过4 GS/s的RF数据。论文还展示了该原型在频谱传感和码域通信等实际RF应用中的潜力。

Conclusion: 基于熵产生物理学和对称性约束的模拟计算框架在实现极高的计算效率和能效方面具有巨大潜力，特别是在射频信号处理和边缘计算应用中。

Abstract: In this paper, we demonstrate how the physics of entropy production, when
combined with symmetry constraints, can be used for implementing
high-performance and energy-efficient analog computing systems. At the core of
the proposed framework is a generalized maximum-entropy principle that can
describe the evolution of a mesoscopic physical system formed by an
interconnected ensemble of analog elements, including devices that can be
readily fabricated on standard integrated circuit technology. We show that the
maximum-entropy state of this ensemble corresponds to a margin-propagation (MP)
distribution and can be used for computing correlations and inner products as
the ensemble's macroscopic properties. Furthermore, the limits of computational
throughput and energy efficiency can be pushed by extending the framework to
non-equilibrium or transient operating conditions, which we demonstrate using a
proof-of-concept radio-frequency (RF) correlator integrated circuit fabricated
in a 22 nm SOI CMOS process. The measured results show a compute efficiency
greater than 2 Peta ($10^{15}$) Bit Operations per second per Watt (PetaOPS/W)
at 8-bit precision and greater than 0.8 Exa ($10^{18}$) Bit Operations per
second per Watt (ExaOPS/W) at 3-bit precision for RF data sampled at rates
greater than 4 GS/s. Using the fabricated prototypes, we also showcase several
real-world RF applications at the edge, including spectrum sensing, and
code-domain communications.

</details>


### [314] [Exponential Dynamic Energy Network for High Capacity Sequence Memory](https://arxiv.org/abs/2510.24965)
*Arjun Karuvally,Pichsinee Lertsaroj,Terrence J. Sejnowski,Hava T. Siegelmann*

Main category: cs.NE

TL;DR: EDEN通过引入一个随时间演变的多时间尺度能量函数，将能量范式扩展到时间领域，实现了指数级的序列记忆容量。


<details>
  <summary>Details</summary>
Motivation: 现有的能量范式（如Hopfield网络）在处理静态联想记忆方面表现出色，但在模拟动态的序列记忆方面存在不足，因为序列记忆需要记忆之间的平稳过渡。

Method: EDEN结合了一个高容量的静态能量网络和一个缓慢作用的、非对称调制的群体，从而演化能量函数。研究人员通过推导短时间尺度能量函数来控制局部动力学，并计算记忆逃逸时间，揭示了静态和动态模式之间的相变。

Result: EDEN的序列记忆容量为指数级 $O(\gamma^N)$，远超传统模型的线性容量 $O(N)$。此外，EDEN的动力学活动与人脑在情景记忆任务中观察到的时间细胞和斜坡细胞的活动相似。

Conclusion: EDEN通过统一静态和序列记忆于一个动态能量框架，为人工智能和生物系统提供了一个可扩展且可解释的高容量时间记忆模型。

Abstract: The energy paradigm, exemplified by Hopfield networks, offers a principled
framework for memory in neural systems by interpreting dynamics as descent on
an energy surface. While powerful for static associative memories, it falls
short in modeling sequential memory, where transitions between memories are
essential. We introduce the Exponential Dynamic Energy Network (EDEN), a novel
architecture that extends the energy paradigm to temporal domains by evolving
the energy function over multiple timescales. EDEN combines a static
high-capacity energy network with a slow, asymmetrically interacting modulatory
population, enabling robust and controlled memory transitions. We formally
derive short-timescale energy functions that govern local dynamics and use them
to analytically compute memory escape times, revealing a phase transition
between static and dynamic regimes. The analysis of capacity, defined as the
number of memories that can be stored with minimal error rate as a function of
the dimensions of the state space (number of feature neurons), for EDEN shows
that it achieves exponential sequence memory capacity $O(\gamma^N)$,
outperforming the linear capacity $O(N)$ of conventional models. Furthermore,
EDEN's dynamics resemble the activity of time and ramping cells observed in the
human brain during episodic memory tasks, grounding its biological relevance.
By unifying static and sequential memory within a dynamic energy framework,
EDEN offers a scalable and interpretable model for high-capacity temporal
memory in both artificial and biological systems.

</details>


### [315] [Socio-cognitive agent-oriented evolutionary algorithm with trust-based optimization](https://arxiv.org/abs/2510.25095)
*Aleksandra Urbańczyk,Krzysztof Czech,Piotr Urbańczyk,Marek Kisiel-Dorohinicki,Aleksander Byrski*

Main category: cs.NE

TL;DR: 本研究提出了一种基于信任的优化（TBO）算法，通过引入基于信任或声誉的交互机制来改进进化计算中的岛屿模型，实验表明TBO在多种优化问题上优于标准岛屿模型算法。


<details>
  <summary>Details</summary>
Motivation: 传统的岛屿模型在进化计算中存在一些局限性，本文旨在通过引入一种新颖的、基于信任或声誉的交互机制来改进该模型。

Method: 提出了一种名为信任优化（TBO）的新型算法，该算法是对岛屿模型的一种扩展，它用一种灵活的、由智能体驱动的、基于信任或声誉的交互机制来代替传统的周期性迁移。

Result: 实验结果表明，TBO在各种优化问题上普遍优于标准的岛屿模型进化算法。然而，算法性能因问题类型而异，某些配置对于特定的景观或维度更有效。

Conclusion: 研究表明，信任和声誉机制为进化优化提供了一种灵活且自适应的方法，在许多情况下可以提高解决方案的质量。

Abstract: This paper introduces the Trust-Based Optimization (TBO), a novel extension
of the island model in evolutionary computation that replaces conventional
periodic migrations with a flexible, agent-driven interaction mechanism based
on trust or reputation. Experimental results demonstrate that TBO generally
outperforms the standard island model evolutionary algorithm across various
optimization problems. Nevertheless, algorithm performance varies depending on
the problem type, with certain configurations being more effective for specific
landscapes or dimensions. The findings suggest that trust and reputation
mechanisms provide a flexible and adaptive approach to evolutionary
optimization, improving solution quality in many cases.

</details>


### [316] [A Benchmark Suite for Multi-Objective Optimization in Battery Thermal Management System Design](https://arxiv.org/abs/2510.25219)
*Kaichen Ouyang,Yezhi Xia*

Main category: cs.NE

TL;DR: 本研究提出了一套用于电池热管理系统（BTMS）设计中多目标优化问题的基准测试集，以解决现有合成基准测试问题不切实际的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的合成基准测试问题（SBPs）存在不切实际的特性，可能导致对算法性能的低估或高估。尽管已有针对各种元启发式算法的真实世界问题基准套件，但缺乏专门针对约束多目标优化问题（CMOPs）且源自实际工程应用（特别是电池热管理系统（BTMS）设计领域）的基准。为填补这一空白，本研究旨在开发一个适用于BTMS多目标优化的专用基准测试套件。

Method: 本研究开发并提出了一套专门用于BTMS多目标优化的基准测试套件。该套件包含一系列多样化的真实世界约束问题，每个问题都基于最新的研究，通过精确的代理模型来定义，以有效模拟复杂的传热传质相互作用。

Result: 本研究成功开发了一个包含多样化真实世界约束问题（基于精确代理模型）的BTMS多目标优化基准测试套件。

Conclusion: 本研究提出的BTMS多目标优化基准测试套件为评估和改进解决能源存储热管理问题的进化算法和优化方法提供了一个实用且相关的测试平台。未来的工作将包括使用最先进的算法建立全面的基线结果、进行比较分析以及开发标准化的排名方案，以促进稳健的性能评估。

Abstract: Synthetic Benchmark Problems (SBPs) are commonly used to evaluate the
performance of metaheuristic algorithms. However, these SBPs often contain
various unrealistic properties, potentially leading to underestimation or
overestimation of algorithmic performance. While several benchmark suites
comprising real-world problems have been proposed for various types of
metaheuristics, a notable gap exists for Constrained Multi-objective
Optimization Problems (CMOPs) derived from practical engineering applications,
particularly in the domain of Battery Thermal Management System (BTMS) design.
To address this gap, this study develops and presents a specialized benchmark
suite for multi-objective optimization in BTMS. This suite comprises a diverse
collection of real-world constrained problems, each defined via accurate
surrogate models based on recent research to efficiently represent complex
thermal-fluid interactions. The primary goal of this benchmark suite is to
provide a practical and relevant testing ground for evolutionary algorithms and
optimization methods focused on energy storage thermal management. Future work
will involve establishing comprehensive baseline results using state-of-the-art
algorithms, conducting comparative analyses, and developing a standardized
ranking scheme to facilitate robust performance assessment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [317] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Forty-two是一个利用群体智能和分布式排序共识的新型AI推理协议，实现了85.90%的GPQA Diamond准确率，优于多数投票的68.69%。该协议具有链上声誉、抗Sybil攻击的证明能力以及对恶意输入的强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于中心化AI的算力瓶颈和训练回报递减，需要一个可横向扩展容量和能力的推理层来满足需求。

Method: 利用群体智能原则和分布式成对排序共识，实现一种名为“群体推理”的方法，通过异构模型之间的点对点排序和声誉加权共识来提高AI推理质量。该协议还结合了链上声誉和“证明能力”机制来抵抗Sybil攻击。

Result: 在GPQA Diamond基准测试中，群体推理准确率达到85.90%，显著优于多数投票的68.69%。在包括GPQA Diamond、LiveCodeBench和AIME在内的六个基准测试中，Forty-two表现出更高的准确性和对恶意输入的强大鲁棒性（例如，仅0.12%的提示注入退化，而单一模型基线为6.20%），同时保持了实际部署能力。

Conclusion: Forty-two协议为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化，同时不牺牲可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [318] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一种结合了Kolmogorov-Arnold网络（KAN）和图卷积网络（GCN）的快速精确的冰盖模型模拟器。


<details>
  <summary>Details</summary>
Motivation: 为了提高冰盖模型的模拟器性能。

Method: 将KAN作为特征校准器置于GCN之前，KAN通过可学习的一维映射和线性混合步骤进行特征条件化和非线性编码，而无需增加消息传递深度。

Result: KAN-GCN在模拟器精度上匹配或超过了纯GCN和MLP-GCN基线，并且在较粗网格上提高了推理吞吐量，在最细网格上成本适度。

Conclusion: KAN优先的设计为大规模瞬态场景扫描提供了有利的精度与效率权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [319] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 在监督下，更强的模型可以优于其教师模型，但现有理论分析仅限于抽象框架或线性/随机特征模型。本文针对线性CNN（弱）到两层ReLU CNN（强）的弱到强泛化现象进行了形式化分析。


<details>
  <summary>Details</summary>
Motivation: 解释弱到强泛化现象，并提供超越现有抽象框架或线性/随机特征模型的理论分析。

Method: 分析在预训练的弱模型标记的数据上训练强模型时的梯度下降动力学，考虑了标签依赖信号和标签独立噪声的结构化数据。分析了两种模式：数据稀疏和数据丰富。

Result: 确定了两种模式：数据稀疏模式下，泛化通过良性过拟合发生，或因有害过拟合而失败，并确定了过渡边界；数据丰富模式下，泛化通过标签校正出现，但过度训练会降低性能。

Conclusion: 弱到强泛化机制取决于数据的信噪比特征。在数据稀疏模式下，泛化取决于数据量；在数据丰富模式下，泛化在早期阶段通过标签校正发生，但后期可能因过度训练而受损。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [320] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: We introduce a data-driven method using low rank neural representation (LRNR) within a hypernetwork to learn efficient low-dimensional representations of hyperbolic wave propagation from data. This method reveals interpretable physical features and enables efficient inference through compression.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for efficient dimensionality reduction methods tailored for physics-based data, specifically hyperbolic wave propagation, leveraging theoretical insights into efficient representations for this wave class.

Method: The proposed method utilizes a specialized neural network architecture called low rank neural representation (LRNR) within a hypernetwork framework. This approach learns a low-dimensional representation of wave propagation directly from data using deep learning techniques.

Result: The study demonstrates that the LRNR architecture naturally learns a low-rank tensor representation, leading to a novel decomposition of wave propagation. Each decomposed mode corresponds to interpretable physical features. Additionally, the architecture facilitates efficient inference through a compression scheme.

Conclusion: The LRNR method provides an effective data-driven approach for learning low-dimensional representations of hyperbolic wave propagation. It not only uncovers interpretable physical insights but also offers computational advantages through compression, making it suitable for performance-critical applications.

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [321] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析生物序列的适应性景观，并引入20个特征来描述景观地形，解决了现有基准缺乏地形信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型评估基准缺乏适应性景观的地形信息，这阻碍了对模型性能的深入理解和比较。

Method: 开发了一个名为GraphFLA的Python框架，该框架能够处理多种生物模态（DNA、RNA、蛋白质等）的突变数据，构建适应性景观，并计算20个描述景观地形的特征。

Result: 通过将GraphFLA应用于超过5300个景观，证明了其在解释和比较不同适应性预测模型方面的效用，并揭示了影响模型准确性的因素。此外，还发布了155个包含超过220万个序列的组合完整经验适应性景观数据集。

Conclusion: GraphFLA为理解和评估生物序列适应性预测模型提供了新的视角和工具，通过量化景观地形特征，促进了对模型性能更深层次的分析。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [322] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 本篇论文提出了两种新的网络架构MNO和MONet，用于学习算子集合，并在理论和实践上都取得了进展。


<details>
  <summary>Details</summary>
Motivation: 机器学习中很多问题关注有限维度空间之间的映射学习，但科学应用需要函数空间之间的映射（即算子）逼近。本研究关注算子集合的学习问题。

Method: 论文区分了两种情况：(i) 多算子学习，单个网络表示由参数化函数参数化的连续算子；(ii) 学习几个不同的单个算子，每个算子独立学习。对于多算子情况，论文提出了MNO和MONet架构，并建立了连续、可积或Lipschitz算子的泛化逼近结果。对于后者，还推导了量化网络规模如何增长以达到目标逼近精度。对于学习几个单独的算子，论文开发了一个平衡子网络之间架构复杂性的框架，并展示了逼近阶数如何决定计算效率。

Result: 在参数化PDE基准上的经验实验证实了所提出架构的强大表达能力和效率。

Conclusion: 该工作为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [323] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 通过在ESP32-S3上使用压缩的卷积神经网络(CNN)进行边缘推理，可以显著降低物联网环境监测设备的能耗，与传输原始图像数据相比，能耗可降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境挑战日益严峻，需要有效的远程监测解决方案，但物联网环境监测设备面临能源效率和长期运行的挑战。

Method: 在ESP32-S3上评估低功耗广域网和针对特定领域数据集训练的压缩CNN，并进行模型量化。

Result: 在ESP32-S3上进行CNN推理并仅传输结果，与传输原始图像数据相比，可将总能耗降低高达5倍。模型压缩（训练后量化）仅带来几个百分点的可接受的准确率损失。

Conclusion: 嵌入式机器学习有助于开发环境监测物联网应用，这些应用具有更低的碳足迹，并能在偏远地区自主运行。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [324] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 我们研究了低秩伪逆在含噪声观测下的谱范数鲁棒性，并推导了新的扰动界限，该界限优于经典界限，并在实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵常带有噪声，这影响了低秩伪逆在机器学习等领域的应用。然而，低秩伪逆的谱范数鲁棒性尚不清楚。

Method: 通过系统研究含噪声观测矩阵 $	ilde{A} = A + E$ 的低秩伪逆 $A_p^{-1}$ 的谱范数误差，并利用轮廓积分技术推导出扰动界限。

Result: 我们得到了新的、非渐近的扰动界限，该界限能精确反映误差与特征值间隙、谱衰减和噪声对低曲率方向的影响，并且在实验中优于经典界限。

Conclusion: 我们提出的界限为在噪声计算环境中使用的低秩伪逆提供了实际的、对谱敏感的保证。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [325] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 信息瀑布流行度预测在社交网络内容传播分析中至关重要，但现有方法存在时间泄露、数据特征不足和计算效率低下等问题。本文提出了时间有序划分策略、淘客数据集和CasTemp框架来解决这些挑战。CasTemp框架通过时间游走、Jaccard邻居选择和基于GRU的编码实现高效的时间感知信息瀑布动态建模。


<details>
  <summary>Details</summary>
Motivation: 现有信息瀑布流行度预测方法存在时间泄露、缺乏下游转化信号的数据集以及复杂图模型计算效率低下等问题，限制了其在实际应用中的效果。

Method: 本文提出了时间有序划分策略以解决时间泄露问题；构建了一个包含丰富推广者/产品属性和真实购买转化的大规模电商瀑布数据集“淘客”；设计了CasTemp框架，该框架通过时间游走、Jaccard-based邻居选择和GRU-based编码与时间感知注意力机制来高效建模瀑布动态。

Result: 在无泄露评估下，CasTemp在四个数据集上实现了最先进的性能，并且速度提升了几个数量级。特别地，该模型在预测第二阶段的流行度转化方面表现优异，而这对于实际应用至关重要。

Conclusion: 本文通过改进任务设置、数据集构建和模型设计，系统性地解决了信息瀑布流行度预测中的时间泄露、特征不足和计算效率低下等关键问题，并提出了高效且性能优越的CasTemp框架。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [326] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 准确率在分布内（ID）和分布外（OOD）数据集上的相关性可能是一种人为的现象，通过识别特定的OOD子集，可以揭示模型在ID上的高准确率可能预示着在OOD上的低准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试中，模型在分布内（ID）和分布外（OOD）准确率之间的强正相关现象（“准确率在线”）常常被用来推断实际中很少出现影响ID和OOD性能的虚假相关性。然而，本文旨在挑战这一观点。

Method: 提出了一种名为OODSelect的简单基于梯度的选择方法，用于识别具有语义一致性的OOD子集，在这些子集中，“准确率在线”现象不再成立。

Result: 在多个广泛使用的分布偏移基准测试中，OODSelect成功识别出了OOD子集（有时占标准OOD集合的一半以上），在这些子集中，ID准确率的提高预示着OOD准确率的降低。这表明聚合指标可能掩盖了OOD鲁棒性的重要失效模式。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，并且“准确率在线”现象并非普遍成立，尤其是在分析特定的OOD子集时。研究者发布了相关代码和识别出的子集，以促进未来的研究。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [327] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 机器学习在许多领域都有应用，但计算需求越来越高，尤其是在资源有限的平台上。为了提高效率和可靠性，我们提出了一种结合算法和硬件的方法。该方法包括模型压缩、近似贝叶斯推理、针对数字加速器的优化以及模拟硬件的探索。我们还提出了用于概率推理的近似方法，并探索了概率光子计算。这些研究共同为构建值得信赖、节能的机器学习系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前的机器学习模型面临计算需求高、可扩展性和效率受限的问题，尤其是在资源受限的嵌入式平台上。此外，在分布偏移或未见过的数据下，模型需要提供可靠的预测。贝叶斯神经网络虽然能量化不确定性，但计算开销更大。

Method: 通过算法和硬件效率的协同优化，来推进常规和贝叶斯神经网络的资源高效和鲁棒推理。算法效率方面，通过模型压缩和近似贝叶斯推理来减少计算量。硬件效率方面，通过优化数字加速器上的部署和探索模拟硬件来实现。具体来说，我们提出了 Galen，一种通过敏感性分析和硬件在环反馈进行自动层级压缩的方法。针对模拟加速器的噪声问题，我们建立了器件缺陷模型并扩展了噪声训练以适应非平稳条件。我们还开发了用于概率推理的解析和集成近似方法，并提出了概率光子计算，利用模拟噪声作为熵源，直接在硬件上进行快速、节能的概率推理。

Result: 通过算法-硬件协同设计，实现了机器学习的效率和可靠性的共同提升。

Conclusion: 算法-硬件协同设计是提高机器学习效率和可靠性的关键，可以为下一代值得信赖、节能的机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [328] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 噪声或测量误差如何影响低秩近似，特别是在谱范数方面，是机器学习中的一个核心挑战。这项工作为对称矩阵提供了新的高概率谱范数扰动界限，改进了经典的Eckart-Young-Mirsky定理，并明确考虑了矩阵A和任意对称扰动E的相互作用。这些界限在温和的特征间隙和范数条件下，为$\(A + E)_p - A_p\$提供了精确的估计，其中$A_p$是$A$的最佳秩-$p$近似，并且可以改进高达$\\\sqrt{n}\\$的因子。作为应用，该研究为差分隐私主成分分析提供了改进的效用保证，解决了该领域的一个悬而未决的问题。分析方法基于一种新颖的复分析轮廓自举法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数。实际数据集上的经验结果证实了这些界限在各种扰动情况下与实际谱误差密切相关。


<details>
  <summary>Details</summary>
Motivation: 在机器学习，特别是差分隐私低秩近似中，理解噪声或测量误差对低秩近似（尤其是谱范数）的影响至关重要。现有的研究通常关注Frobenius范数误差或重建质量的变化，但这些度量可能无法准确估计子空间失真。谱范数则能捕捉最坏情况下的方向误差，并提供最强的效用保证。

Method: 提出了一种新颖的复分析轮廓自举法，并将其扩展到一类谱函数（包括多项式和矩阵指数），用于建立对称矩阵的谱范数扰动界限。

Result: 在温和的特征间隙和范数条件下，得到了改进的谱范数扰动界限，该界限可精确估计$\(A + E)_p - A_p\$，并可改进高达$\\\sqrt{n}\\$的因子。作为应用，得到了差分隐私主成分分析的改进效用保证。

Conclusion: 该研究为对称矩阵的谱范数扰动提供了新的界限，改进了现有的理论，并为差分隐私主成分分析提供了更强的效用保证。所提出的方法具有广泛的适用性，并通过实验得到了验证。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [329] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应多任务脑电图（EEG）分类器，用于在床旁快速分诊疑似卒中患者，提高了卒中类型、严重程度和侧化诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确、可床旁部署的工具，而脑电图（EEG）是一种有前景但在一线接触中未被充分利用的技术。

Method: 研究提出了一种自适应多任务EEG分类器，该分类器将32通道信号转换为功率谱密度特征（Welch），使用循环卷积网络（GRU-TCN）预测卒中类型（健康、缺血性、出血性）、半球侧化和严重程度，并应用深度Q网络（DQN）实时调整决策阈值。

Result: 在UCLH卒中EIT/EEG数据集（44个记录；约26例急性卒中，10例对照）上，GRU-TCN在卒中类型方面达到89.3%的准确率（F1 92.8%），在严重程度方面达到约96.9%（F1 95.9%），在侧化方面达到约96.7%（F1 97.4%）。通过DQN阈值自适应，卒中类型准确率提高到约98.0%（F1 97.7%）。

Conclusion: 自适应阈值能够将操作点转移到临床上更优的敏感性-特异性权衡，同时集成的头皮图和频谱可视化支持模型的可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [330] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: DWMGrad是一种新的优化算法，通过动态更新动量和学习率来解决深度学习中的优化挑战，并在实验中显示出更快的收敛速度和更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的优化算法（如SGD和Adam）在处理学习效率波动、复杂模型和非凸优化问题时存在不足，主要体现在难以选择合适的学习率、避免局部最优以及在高维空间中导航。

Method: 提出了一种名为DWMGrad的新型优化算法，该算法结合了动态指导机制，依赖历史数据动态更新动量和学习率，从而能够灵活调整对历史信息的依赖，适应不同的训练场景。

Result: 实验验证表明，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确性。

Conclusion: DWMGrad通过动态调整优化过程，能够更好地适应不断变化的环境和任务复杂度，克服传统优化算法的局限性。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [331] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [332] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: nnLDA是一种结合了神经网络和概率主题模型的模型，可以整合辅助信息，在多个基准数据集上优于LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统的LDA等主题模型难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性和可解释性。

Method: 提出了一种名为nnLDA的新模型，该模型通过神经网络先验机制动态地整合辅助信息。nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由条件于辅助特征的神经网络生成。使用随机变分期望最大化算法来联合优化神经网络和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类任务上始终优于LDA和Dirichlet-Multinomial回归。

Conclusion: 将神经网络表示学习与概率主题模型相结合，在有辅助信息的情况下可以带来显著的好处。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [333] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: PIML已成功应用于物理学，但生物学建模面临独特挑战。本文提出BIML，作为PIML的扩展，以适应生物学的实际情况。BIML通过不确定性量化、情境化、约束潜在结构推理和可扩展性这四大支柱来发展。基础模型和大型语言模型将是关键的推动者。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习（PIML）在物理学领域取得了成功，但生物学建模因其多方面且不确定的先验知识、异构且嘈杂的数据、部分可观测性以及复杂的高维网络等独特挑战，需要PIML的扩展。

Method: 本文提出了一种名为生物信息机器学习（BIML）的原则性扩展，它保留了PIML的结构基础，同时适应生物学的实际情况。BIML通过不确定性量化、情境化、约束潜在结构推理和可扩展性这四大支柱，将PIML的方法重新调整为在更宽松、概率性的先验知识形式下运行。

Result: 基础模型和大型语言模型将成为关键推动者，促进人类专业知识与计算建模的融合。

Conclusion: 本文提出了BIML的四大支柱作为从PIML向适应生物学挑战过渡的路线图，并提出了建立BIML生态系统的具体建议，以推动PIML启发的创新应对具有重大科学和社会意义的挑战。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [334] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [335] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 本文提出了一种在分布式机器学习中优化计算资源分配的算法，解决了CPU使用率和本地数据训练的协同优化问题，并能在所有迭代中保持资源约束的平衡。该算法支持时变网络和对数量化信息交换，并在支持向量机和回归等应用中表现出超过50%的成本优化差距改进。


<details>
  <summary>Details</summary>
Motivation: 近年来，随着人工智能研究的快速发展，对快速、计算高效且可扩展的解决方案的需求日益增长，尤其是在分布式机器学习和优化领域，对计算资源的需求尤为迫切。

Method: 提出了一种协同优化方法，同时优化数据处理和计算资源分配。该算法能在所有迭代中保持计算资源需求平衡约束，并支持时变网络和对数量化信息交换。通过摄动理论、Lyapunov稳定性分析和特征谱分析来证明算法的收敛性。

Result: 与现有的CPU调度解决方案相比，该算法在成本优化差距方面提高了50%以上。

Conclusion: 本文提出的算法能够有效地解决分布式机器学习中的计算资源优化问题，并在实际应用中取得了显著的性能提升。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [336] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）生成的推理步骤（CoT）并不总是真实反映模型内部思考过程，其中许多步骤对最终预测几乎没有因果影响。本研究提出了“真实思考得分”（TTS）来衡量每一步推理的因果影响力，发现LLM常常在“真实思考”步骤和“装饰性思考”步骤之间交错，后者仅为表面推理而无实际作用。研究还识别出LLM潜在空间中的“真实思考”方向，通过沿着或逆着该方向进行引导，可以控制模型是否执行或忽略某些CoT步骤。最后，研究指出即使是CoT中的自我验证步骤（如“啊哈时刻”）也可能是装饰性的，LLM并未真正进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设LLM在推理时生成的CoT步骤是其内部思考过程的真实反映，并用于监控不安全意图。然而，本研究发现许多推理步骤对LLM的最终预测并无实际贡献，挑战了这一假设。

Method: 1. 提出“真实思考得分”（TTS）来衡量每一步推理对模型最终预测的因果影响力。 2. 识别LLM潜在空间中的“真实思考”方向。 3. 通过沿着或逆着“真实思考”方向引导，控制模型执行或忽略CoT步骤。 4. 验证自我验证步骤也可能具有装饰性。

Result: 1. LLM生成的CoT步骤中，许多是“装饰性思考”，对最终预测的因果影响很小。例如，在Qwen-2.5模型应用于AIME数据集时，平均只有2.3%的推理步骤TTS得分大于等于0.7。 2. 识别出了LLM潜在空间中的“真实思考”方向。 3. 通过引导可以强制模型执行或忽略某些CoT步骤，即使是自我验证步骤。 4. 强制模型执行装饰性的自我验证步骤会改变最终结果。

Conclusion: LLM常常在生成推理步骤时进行“口头表达”但并未在内部真正执行，这降低了LLM推理的效率和CoT的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [337] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: Despite impressive performance, vision-language models (VLMs) struggle with culturally situated inputs. This paper investigates culture-sensitive neurons in VLMs, their importance for culturally diverse visual question answering, and their locations. Using the CVQA benchmark and three VLMs across 25 cultural groups, the study identifies neurons whose deactivation disproportionately harms performance on culture-specific questions.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs process culturally grounded information and to identify culture-sensitive neurons.

Method: The study examines the presence of culture-sensitive neurons by identifying neurons with preferential sensitivity to inputs from specific cultural contexts. Causal tests are performed by deactivating these identified neurons. A new margin-based selector, Contrastive Activation Selection (CAS), is proposed and compared to existing methods for identifying culture-sensitive neurons. Layer-wise analyses are conducted to determine the location of these neurons.

Result: Experiments demonstrate the existence of culture-sensitive neurons whose ablation disproportionately harms performance on questions related to their corresponding cultures, with minimal impact on others. The proposed CAS method outperforms existing probability- and entropy-based methods in identifying these neurons. Layer-wise analyses reveal that culture-sensitive neurons tend to cluster in certain decoder layers.

Conclusion: Culture-sensitive neurons exist in VLMs and are important for processing culturally grounded information. The CAS method is effective in identifying these neurons, which tend to cluster in specific decoder layers. These findings provide new insights into the internal organization of multimodal representations.

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [338] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 大型语言模型具有低秩结构，可通过线性组合生成响应。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的内在低维结构。

Method: 将大型语言模型视为序列概率模型，并通过分析由模型 logits 构建的矩阵的秩来研究其低维结构，然后利用该低秩结构进行文本生成。

Result: 证明了多种现代语言模型表现出低秩结构，并且可以通过组合模型对不同提示的输出来生成响应。

Conclusion: 提出的低秩分析方法为理解和利用大型语言模型的内在结构提供了一个通用理论框架，并具有可证明的学习保证。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [339] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: MLDE可用于抗体进化，但现有方法难以区分构象不确定性和认知不确定性。本文提出了一种秩条件委员会（RCC）框架，该框架为每个秩分配一个深度神经网络委员会，从而能够区分这两种不确定性。在SARS-CoV-2抗体对接的验证中，该方法显著优于基线策略，为治疗性抗体发现提供了可扩展的途径。


<details>
  <summary>Details</summary>
Motivation: 现有结构感知MLDE管道通常依赖单一构象或单一委员会，这限制了它们区分构象不确定性和认知不确定性的能力。

Method: 提出了一种秩条件委员会（RCC）框架，该框架利用秩排构象为每个秩分配一个深度神经网络委员会，从而实现认知不确定性与构象不确定性的分离。

Result: 在SARS-CoV-2抗体对接任务的验证中，该方法显著优于基线策略。

Conclusion: RCC框架能够有效分离认知不确定性和构象不确定性，为治疗性抗体发现提供了一种可扩展的方法。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [340] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 本文提出了一种基于博弈论的表格数据特征选择框架，通过模拟特征间的协同作用和边际贡献来评估特征重要性，以减少计算成本并保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据量的爆炸式增长导致机器学习模型训练的计算成本不断攀升，但许多特征在消耗大量计算资源的同时，并未对模型性能产生积极贡献。

Method: 将特征选择过程构建为一个合作博弈，将特征视为玩家，通过评估协同作用和边际贡献来确定其重要性。该框架包含样本选择、基于博弈论的特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明，该方法在保持预测性能的同时，显著降低了计算量，为大规模机器学习的计算挑战提供了有效的解决方案。

Conclusion: 所提出的博弈论特征选择框架能够有效降低计算成本，同时不牺牲模型的预测性能，是解决大规模机器学习计算挑战的有效途径。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [341] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种用于离线强化学习的风险感知采样方法，通过将去噪过程视为假设检验，并引入用户可解释的风险预算，以改善回报-OOD（分布外）权衡，同时保持训练过程的简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中虽然有竞争力，但在采样时依赖于缺乏统计风险概念的启发式方法。需要一种能够量化和控制采样风险的方法。

Method: 提出了一种名为LRT-Diffusion的风险感知采样规则，将每次去噪步骤视为无条件先验和状态条件策略头部之间的序贯假设检验。通过累积对数似然比和使用逻辑控制器来调整条件均值，该控制器以用户指定的I类错误水平alpha为阈值。在训练过程中，保持DDPM结构不变，仅在推理时进行LRT引导。LRT引导可以与Q梯度结合使用，提供从开发到保守的连续体。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在回报-OOD权衡方面优于强Q引导基线，并能满足所需的alpha水平。理论上，证明了其水平alpha校准、稳定性界限以及在离支持错误占主导时LRT优于Q引导的收益比较。

Conclusion: LRT-Diffusion是一种即插即用的推理时间方法，为离线强化学习的扩散策略增加了原则性的、经过校准的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [342] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 本研究提出了一种结合癫痫发作实时检测和预测的新方法，利用机器学习（包括LSTM）分析EEG数据，旨在实现早期干预和主动治疗。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫发作检测方法仅在发作开始后进行，限制了早期干预和主动治疗的机会。本研究旨在开发一种能够实时检测并预测癫痫发作的新方法，以实现更主动的癫痫管理。

Method: 研究采用CHB-MIT大规模EEG数据库（969小时，173次发作），包含23名患有药物难治性癫痫的儿科和年轻成人患者。对于发作检测，研究了K近邻、逻辑回归、随机森林和支持向量机等监督学习算法。对于发作预测，则采用了长短期记忆（LSTM）网络。

Result: 在发作检测方面，逻辑回归模型达到了90.9%的准确率和89.6%的召回率，表现均衡。随机森林和支持向量机模型虽然准确率较高（94.0%），但召回率为0%，未能检测到任何发作，这表明在类别不平衡的医学机器学习模型评估中，仅凭准确率是不够的。在发作预测方面，LSTM模型实现了89.26%的预测准确率。

Conclusion: 本研究结果表明，结合实时癫痫发作检测和预测的机器学习方法（特别是LSTM）在EEG数据分析方面具有巨大潜力，能够实现从被动治疗向主动式癫痫管理的转变，从而为患者带来更安全、更可控的生活。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [343] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 通过引入基于Transformer的非监督式变化点检测模块，实现自主发现有意义的子目标和学习最优选项终止边界，从而提高HRL的可扩展性、样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: HRL在实际应用中面临自主发现语义子目标和学习最优选项终止边界的挑战。

Method: 将自监督的、基于Transformer的变化点检测（CPD）模块集成到Option-Critic框架中，通过内在信号推断潜在的环境动态变化，并将这些变化点用于稳定终止函数梯度、预训练选项内策略以及强制执行选项间功能专业化。

Result: 在Four-Rooms和Pinball任务上，CPD引导的智能体表现出更快的收敛速度、更高的累积回报和显著增强的选项专业化。

Conclusion: 将变化点分割作为结构化先验集成到HRL中，可以实现更具可解释性、样本效率和鲁棒性的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [344] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 矩阵 असू


<details>
  <summary>Details</summary>
Motivation: 现有优化器在矩阵白化变换方面的近似方法不同，本文旨在解构这些优化器以区分影响性能的关键因素。

Method: 通过对所有超参数进行调整，比较了矩阵白化方法和逐元素方法的性能。实验还分析了与谱下降的关系，并评估了方差适应性成分的作用。

Result: 矩阵白化方法在所有调优的超参数下均优于逐元素方法（如Adam）。性能提升并非完全由精确的谱归一化引起，SOAP优化器表现出最大的每步增益，尽管Muon在最陡的谱下降方向上更准确。方差适应性成分是解释性能差距的关键。方差适应性优化器优于其符号下降对应物，包括Muon的自适应版本。低秩方差估计器在不损失性能的情况下有效降低了内存成本。

Conclusion: 矩阵白化方法通过其方差适应性成分实现了性能提升，而不仅仅是谱归一化。低秩方差估计器可以作为一种内存效率高的方法。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [345] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一个深度多编码器自动编码器，可将神经记录分解为共享和私有潜在子空间，用于分析多区域神经动力学。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，将共享的网络级动力学与区域特异性活动分离开来是一个核心挑战。

Method: SPIRE（共享-私有区域间编码器）是一个深度多编码器自动编码器，它通过新颖的对齐和分离损失将记录分解为共享和私有潜在子空间。

Result: SPIRE在合成基准测试中表现优于经典概率模型，并能可靠地编码刺激特异性特征，这些特征可以跨站点和频率进行泛化。

Conclusion: SPIRE是一个实用且可重复的工具，用于分析刺激下的多区域神经动力学。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [346] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 本研究评估了放射组学模型在不同成像协议和分割策略下的鲁棒性，发现使用协议不变特征的模型在分布变化下仍能保持高F1分数 (>0.85)，而数据增强能提高不确定性估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在临床决策支持方面显示出潜力，但容易受到成像协议、定位和分割变化引起的分布偏移的影响。本研究旨在系统地研究放射组学模型在分布偏移下的鲁棒性。

Method: 研究使用包含16个水果的物理模体，通过改变T2-HASTE、T2-TSE、T2-MAP、T1-TSE和T2-FLAIR序列的成像协议、分割策略（完整、部分、旋转）以及引入观察者间变异性来模拟分布偏移。研究训练了XGBoost分类器，分别使用8个一致的鲁棒特征和序列特异性特征，并在同域和外域条件下测试模型性能。

Result: 使用协议不变特征训练的模型在分布偏移下保持了>0.85的F1分数，而使用所有特征的模型在协议变化下性能下降了40%。数据增强将期望校准误差（ECE）降低了35%，同时不牺牲准确性。温度缩放对校准的改善效果很小。

Conclusion: 本研究揭示了协议感知特征选择和受控的物理模体研究可以有效地预测模型在分布偏移下的行为，为开发能够抵抗真实世界协议变化的鲁棒放射组学模型提供了一个框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [347] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计的ADMG图距离度量方法，用于评估潜在混淆下的因果发现。


<details>
  <summary>Details</summary>
Motivation: 因果发现领域对新方法进展的评估存在困难，尤其是在存在潜在混淆的情况下，因此需要新的评估方法。

Method: 提出了一种基于下游任务（因果效应估计）的图距离度量方法，该方法使用“固定”识别和符号验证器来量化图差异对不同处理-结果对的因果效应估计值的影响。

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较。

Conclusion: 该研究提出了一种新的ADMG图距离度量方法，能够更好地评估因果发现的进展，尤其是在存在潜在混淆的情况下。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [348] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种新的方法，用于估计具有不可访问计算图的网络结构中的偏导数，并提高了准确性和可扩展性，同时允许集成黑盒函数。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够估计具有不可访问计算图的网络结构中的偏导数的方法，并探索将黑盒函数集成到可训练架构中的可能性。

Method: 提出了一种名为Bounded Numerical Differentiation (BOND) 的扰动方法，用于估计偏导数，并进行了实验来评估黑盒函数（固定、未训练的网络）的有效性。

Result: BOND在准确性和可扩展性方面优于现有的扰动方法，并且可以集成黑盒函数。实验表明，即使黑盒函数是固定的、未经训练的，也可以在不增加可训练参数数量的情况下提高模型性能。

Conclusion: 固定、非可训练模块可以用于扩展模型容量，并且有潜力将模拟和数字设备结合起来以扩展网络。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [349] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐的持续学习方法，它通过引入近端正则化器来解决LLM在推荐中持续学习的挑战，能在适应新用户行为和保留旧用户行为之间取得良好平衡，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在推荐领域表现出色，但在用户、物品和偏好不断变化的持续学习中面临挑战。现有的LoRA方法侧重于保持旧任务的表现，但忽略了推荐的本质——预测当前偏好而非过去偏好，且过时的偏好可能损害性能。

Method: 提出了一种名为PESO（Proximally Regularized Single evolving lOra）的持续学习方法。PESO引入了一个近端正则化器，将当前适配器锚定在其最近的冻结状态，从而在适应和保留之间取得平衡，更好地捕捉用户近期行为。理论上，该方法在LoRA子空间中提供了数据感知、方向感知的指导。

Result: PESO在经验上持续优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过其近端正则化设计，有效解决了推荐领域持续学习中的挑战，能够灵活平衡适应性和保留性，并最终提升了模型性能。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [350] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: GNNs在处理关系数据时存在放大训练数据偏见的问题，导致不公平的结果。现有的公平性方法通常将偏见视为单一来源，忽略了属性和结构的影响，从而在公平性和效用之间取得次优的权衡。为了解决这个问题，我们提出了FairMIB，一个多视图信息瓶颈框架，用于将图分解为特征、结构和扩散视图，以减轻GNN中的复杂性偏见。FairMIB采用对比学习最大化跨视图互信息以实现无偏表示学习，并整合多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，FairMIB在扩散视图中引入了逆概率加权（IPW）邻接校正，以减少消息传递过程中偏见的传播。在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上均取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性方法在处理GNN中的偏见时，通常将偏见视为单一来源，忽略了属性和结构的影响，导致在公平性和效用之间取得次优的权衡。本文旨在解决这一挑战，通过FairMIB框架来减轻GNN中的复杂性偏见。

Method: FairMIB框架采用多视图信息瓶颈方法，将图分解为特征、结构和扩散三个视图。具体来说，它利用对比学习最大化跨视图互信息以实现无偏表示学习，并整合多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，在扩散视图中引入了逆概率加权（IPW）邻接校正，以减少偏见传播。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上均取得了最先进的性能。

Conclusion: FairMIB通过多视图信息瓶颈框架，有效地区分并缓解了GNN中的属性和结构偏见，实现了在效用和公平性之间的良好平衡，并在多个真实世界数据集上取得了最先进的性能。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [351] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 在训练和测试数据分布比例不同的情况下，研究分布变化对模型性能的影响，发现分布变化在许多情况下是有益的，即使组件不相关。


<details>
  <summary>Details</summary>
Motivation: 探讨在训练和测试数据分布比例不同时，分布变化对模型性能的影响，以及何时以及如何利用这种分布变化来提升模型性能。

Method: 分析在训练和测试数据分布比例不同时，分布变化对模型性能的影响，并找出最优的训练数据分布比例。

Result: 在多种场景下，通过调整训练数据分布比例，可以提升模型在测试数据上的性能，即使数据组件不相关。

Conclusion: 分布变化在许多情况下可以有益于模型性能的提升，特别是在训练和测试数据分布比例不同的情况下。通过分析可以确定最优的训练分布比例，并应用于组件技能分布不同的组合场景。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [352] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为神经微分流形（NDM）的新型神经网络架构，该架构将几何结构融入其设计中，将网络参数化为黎曼度量张量。它包含三个层：坐标层（实现平滑的图变换）、几何层（动态生成流形的度量）和演化层（通过双目标损失函数进行优化）。该方法通过惩罚曲率和体积畸变来实现内在正则化，从而提高泛化能力和鲁棒性。NDM 能够进行与流形几何对齐的自然梯度下降优化，并通过赋予内部表示明确的几何意义来提高可解释性。该研究讨论了其在优化、持续学习、科学发现和可控生成模型方面的理论优势，尽管计算挑战依然存在。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络参数空间采用欧几里得结构，限制了其在几何结构、优化效率、泛化能力、鲁棒性、持续学习和可解释性方面的潜力。本研究旨在通过引入一种显式包含几何结构的新型神经网络架构来克服这些限制。

Method: 提出神经微分流形（NDM）架构，其核心思想是将神经网络视为一个可微流形。该架构包含三个协同工作的层：1. 坐标层：使用受归一化流启发的反向变换实现平滑的坐标图转换。2. 几何层：利用辅助子网络动态生成流形的黎曼度量张量。3. 演化层：通过一个包含任务性能和几何简单性（惩罚曲率和体积畸变）的双目标损失函数来优化网络。

Result: NDM 架构能够实现内在正则化，提高模型的泛化能力和鲁棒性。它支持与学习到的流形几何对齐的自然梯度下降优化。通过赋予内部表示明确的几何意义，提高了模型的可解释性。理论分析表明，该方法在优化效率、持续学习能力以及在科学发现和可控生成模型等领域的应用方面具有优势。

Conclusion: 神经微分流形（NDM）代表了深度学习领域的一个根本性转变，朝着构建具有内在几何结构、可解释且高效的系统迈进。虽然计算挑战仍然存在，但该方法为未来的研究和应用开辟了新的可能性。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [353] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的双层模型来解决对抗性学习中的模型解释性问题，并从数据扰动角度分析了聚类模型中的对抗性攻击，提出了用于衡量攻击效果的δ-measure。


<details>
  <summary>Details</summary>
Motivation: 对抗性学习和攻击机制的解释性不足，以及攻击效果的衡量不明确。

Method: 提出统一的双层模型，并从数据扰动角度分析聚类模型的对抗性攻击。

Result: 发现当数据扰动较小时，聚类模型具有鲁棒性；当数据扰动较大时，聚类结果发生改变，构成攻击。分析了δ-measure在衡量攻击效果方面的有效性。

Conclusion: 提出的双层模型和δ-measure可以用于理解和衡量聚类模型中的对抗性攻击。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [354] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，它将序列编码器与图神经网络（GNN）耦合在一个单一的目标下，实现了跨模块的梯度流，并学习任务对齐的表示。它通过TOKENXATTN（一个令牌级别的交叉注意力层）在邻近序列中的事件之间传递消息。


<details>
  <summary>Details</summary>
Motivation: 许多真实世界的数据集既是序列的又是关系的，现有的序列和图模型方法常常忽略其中一种模式。作者认为序列和图并非分离的问题，而是同一数据集的互补侧面，应该联合学习。

Method: 作者提出了BRIDGE架构，将序列编码器与GNN耦合在一个单一的目标下，并引入了TOKENXATTN（令牌级别的交叉注意力层）来实现令牌级别的消息传递。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）这两个设置中，BRIDGE在排名和分类指标上始终优于静态GNN、时间图方法和仅序列基线。

Conclusion: BRIDGE架构能够联合学习序列和图模式，并在各种下游任务中取得优于现有方法的性能。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [355] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 数据增强（DA）可用于因果推断，以处理潜在混淆因素，并提供比传统工具变量（IV）更易获取的替代方案。


<details>
  <summary>Details</summary>
Motivation: 在因果推断的背景下，探讨数据增强（DA）在处理潜在混淆因素和提高跨干预泛化能力方面的潜力，并将其作为传统工具变量（IV）的一种更易获取的替代方案。

Method: 提出了一种统一的框架，将DA置于因果推断的背景下。将DA视为对处理生成机制的干预，特别是在结果生成机制对DA选择不变的情况下。引入了IV类（IVL）回归的概念，以放宽IV的某些属性，并解决混淆偏倚问题。将参数化DA表述为IVL回归问题，并展示了其组合使用可模拟最坏情况的应用。

Result: 理论上和模拟实验中都证明了DA在因果效应估计和跨干预泛化方面可以超越简单的DA。在真实数据集上进行了实验以支持该方法。

Conclusion: 数据增强（DA）不仅适用于独立同分布（i.i.d.）设置下的正则化，还可以通过因果推断的视角用于提高跨干预的泛化能力，尤其是在存在隐藏混淆因素的情况下。IV类（IVL）回归提供了一种在放松IV属性的情况下减轻混淆偏倚和提高预测性能的方法。参数化DA作为IVL回归问题的一种形式，通过组合使用可以进一步提升因果估计和泛化任务的性能。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [356] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: Lipschitz常数对认证鲁棒性至关重要，但计算它很困难。本文提出了一种新的理论分析和方法，通过“线性嫁接”技术来减少近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性，而无需进行认证训练。


<details>
  <summary>Details</summary>
Motivation: 认证鲁棒性中，Lipschitz常数越小越好，但计算它的最坏情况是NP完全问题。现有的过近似方法存在近似误差，阻碍了获得紧密的局部Lipschitz常数。本文旨在解决这个问题。

Method: 本文从$l_
inft$局部Lipschitz常数的角度，分析了线性嫁接如何提高认证鲁棒性。作者认为线性嫁接主要是消除近似误差，而非减少不稳定的神经元数量。在此基础上，提出了一种新的“Lipschitz感知线性嫁接”方法，该方法消除了主要的近似误差，从而得到更紧的局部Lipschitz常数，提高了认证鲁棒性。

Result: 通过理论分析和实验证明，将线性嫁接应用于非线性激活函数可以减小$l_
inft$局部Lipschitz常数，并提高认证鲁棒性，即使在没有认证训练的情况下也能取得良好效果。

Conclusion: 本文提出了理论分析和一种新的Lipschitz感知线性嫁接方法，该方法通过消除近似误差来减小局部Lipschitz常数，从而提高认证鲁棒性。实验证明了该方法的有效性。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [357] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 通过机器学习优化电力线断电决策，以降低野火风险并减少负荷削减。


<details>
  <summary>Details</summary>
Motivation: 公用事业公司为了降低野火风险，会在高风险区域关闭电力线，但此举会造成负荷削减。因此，需要优化电力线断电决策（OPS问题）以在降低野火风险的同时最大限度地减少负荷削减。

Method: 开发了一个机器学习（ML）指导的框架，利用现有ML指导的混合整数线性规划（MILP）求解方法，并结合了关于已激活和已停用线路数量的领域知识，以快速生成高质量的断电决策。

Result: 在基于加利福尼亚的大规模合成测试系统中，提出的ML指导方法比传统优化方法能更快地生成高质量的解决方案。

Conclusion: 机器学习指导的方法能够快速有效地解决OPS问题，在降低野火风险的同时，比传统优化方法能更好地平衡负荷削减。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [358] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是WGAN的一个改进，通过引入BOLT原则，在图像生成任务上实现了比WGAN更低的FID分数和更好的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: BOLT-GAN通过引入BOLT原则来改进WGAN框架，以期在图像生成任务中获得更好的训练稳定性和生成质量。

Method: BOLT-GAN在WGAN框架的基础上进行修改，利用Lipschitz连续判别器隐式地最小化与Earth Mover距离不同的度量，并引入了BOLT原则。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个标准图像生成基准测试中，BOLT-GAN相比WGAN取得了10-60%的更低Frechet Inception Distance (FID) 分数，显示出更优越的性能。

Conclusion: BOLT原则可以广泛应用于改进GAN的训练过程，BOLT-GAN在图像生成任务上展示了其有效性。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [359] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 通过引入选择性学习策略，我们提出了一种新的深度时间序列预测方法，该方法通过在训练过程中过滤掉不确定和异常的时间步来提高模型的泛化能力，从而减少了过拟合，并在多个真实世界数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测中容易出现过拟合，因为它们在优化过程中会对所有时间步一视同仁，包括噪声和异常值。

Method: 提出了一种新的选择性学习策略，通过引入一个双掩码机制来过滤掉不确定和异常的时间步。其中，不确定掩码利用残差熵来过滤不确定的时间步，异常掩码利用残差下界估计来排除异常时间步。

Result: 在八个真实世界数据集上的广泛实验表明，选择性学习策略能够显著提升现有先进深度模型（如 Informer、TimesNet 和 iTransformer）的预测性能，分别降低了 37.4%、8.4% 和 6.5% 的 MSE。

Conclusion: 所提出的选择性学习策略通过关注可泛化的时间步并忽略不可泛化的时间步，有效地解决了深度时间序列预测中的过拟合问题，并显著提高了模型的预测精度。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [360] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 在对预训练的视觉-语言-动作（VLA）模型进行微调以适应动作模式时，视觉表征会发生退化。本研究系统地研究了这一现象，通过探测隐藏表征和分析注意力图来量化退化。我们设计了对比任务来分离微调带来的变化，并提出了一种有效的策略来缓解退化，提高模型在分布外（OOD）场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索在将预训练的视觉-语言模型（VLM）适配到动作模式（VLA模型）的过程中，其原有的视觉-语言（VL）表征和知识的保留程度，并研究这种适配对动作模型泛化能力的影响。

Method: 1. 通过探测VLA模型的隐藏表征和分析注意力图，来量化和表征视觉表征退化。 2. 设计了一系列对比任务，将VLA模型与其对应的VLM进行比较，以分离由动作微调引起的VL能力变化。 3. 评估了多种对齐视觉表征的策略，并提出了一种新的缓解退化并提高OOD泛化能力的方法。

Result: Naive的动作微调会导致视觉表征的退化。本研究提出的方法能够缓解这种退化，并显著提高模型在分布外（OOD）场景的泛化能力。

Conclusion: 在将预训练的VLM适配到动作模式时，存在视觉表征退化和动作微调之间的权衡。本研究通过分析和提出新的方法，有助于恢复继承的VL能力，并提高了模型的泛化性能。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [361] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于自适应损失加权的多类正负样本(MPU)学习方法，解决了现有方法在无偏风险估计方面的不足，通过为正样本和推断负样本的损失分量分配数据依赖的权重，使得经验目标成为目标风险的无偏估计。


<details>
  <summary>Details</summary>
Motivation: 在仅有的正样本和未标记样本中进行学习（PU学习）是实际应用中常见的场景，因为可靠地标注负样本困难且成本高昂。然而，现有的多类PU学习方法在确保无偏风险估计方面存在不足，限制了其性能和稳定性。

Method: 提出了一种基于自适应损失加权的多类PU学习方法。在经验风险最小化框架下，为正样本和从标记混合物中推断出的负样本的损失分量分配不同的、依赖于数据本身的权重，以确保产生的经验目标成为目标风险的无偏估计量。

Result: 在八个公开数据集上进行了广泛的实验，这些数据集涵盖了不同的类别先验和类别数量。结果表明，与强基线方法相比，在准确性和稳定性方面均取得了持续的提升。

Conclusion: 所提出的基于自适应损失加权的多类PU学习方法能够有效地进行无偏风险估计，并在实践中提高了准确性和稳定性。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [362] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: EP学习算法可用于物理神经网络，特别是波系统，并且在弱耗散条件下有效，即使在没有明确节点的系统和可训练局部势的情况下也适用。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于物理神经网络的替代反向传播算法，以实现原地训练。

Method: 将EP学习算法扩展到离散和连续的复值波系统，并在一类新的物理系统中进行了测试，该系统仅允许对局部参数进行控制。

Result: 在标准的基准测试中，包括简单的逻辑任务和手写数字识别，所提出的EP学习算法在驱动耗散激子-极化激子凝聚物中实现了稳定的收敛。

Conclusion: 该方法为在物理系统中进行原地学习提供了一条可行的途径，尤其是在系统控制仅限于局部参数的情况下。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [363] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: BSFA框架通过区分Hessian矩阵的特征向量（Dom-space）和正交分量（Bulk-space）上的更新，加速深度学习训练，同时提高稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解释深度学习优化中参数更新（Dom-space和Bulk-space）对损失降低的贡献差异，并提出一种名为BSFA的新框架来利用这种现象。

Method: 提出BSFA框架，通过区分和缩放Dom-space和Bulk-space上的更新来加速训练。采用PCA估计子空间，并使用分块策略以提高效率和可扩展性。

Result: BSFA在LLaMA-72M（WikiText-103）和LLaMA-134M（OpenWebText）的预训练任务中实现了约2倍的速度提升，并且比标准的AdamW优化器表现更好。

Conclusion: BSFA是一个实用且可扩展的框架，通过有效利用优化中的子空间特性，能够显著加速深度学习模型的训练过程。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [364] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 我们提出通过改进马尔可夫链抽样来提高贝叶斯网络结构推断的效率。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断通常需要对马尔可夫链进行抽样，而现有方法效率不高。

Method: 我们提出两种技术来改进抽样：1. 改进基本操作（添加、删除、反转边）的实现效率。2. 通过预处理方法加速计算通常很耗时的“对父集求和”步骤，从而实现近似保留。

Result: 我们的经验研究表明，与之前的方法相比，我们的技术可以带来显著的效率提升。

Conclusion: 我们提出的改进抽样技术可以显著提高贝叶斯网络结构推断的效率。

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [365] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络（QRNN）架构，其中循环核心完全由参数化量子电路（PQC）实现，并由经典前馈网络控制。


<details>
  <summary>Details</summary>
Motivation: QRNN 结合了量子计算的优势（如酉演化和高容量记忆）与经典神经网络的优势（如非线性控制），旨在提升序列学习任务的性能。

Method: QRNN 的核心是一个 n 量子比特的 PQC，其量子态作为隐藏状态，利用酉动力学演化。在每个时间步，通过中途测量获得量子态信息，与输入嵌入结合，经由经典前馈网络处理以引入非线性，并输出参数来更新 PQC。

Result: 在模拟中，该模型在包含高达 14 个量子比特的任务上表现出色，包括情感分析、MNIST、置换 MNIST、复制记忆和语言建模。通过采用投影测量作为中途读出的特例，模型在保持相干量子记忆的同时，也实现了其功能。此外，在序列到序列模型中引入软注意力机制，并在机器翻译任务上证明了其有效性。

Conclusion: 该 QRNN 模型是首个基于量子操作且能在广泛的序列学习任务上达到与强大的经典基线模型相媲美性能的模型。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [366] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: IBNorm是一种新的归一化方法，它基于信息瓶颈原理，通过引入压缩操作来保留预测信息并抑制无关变异，从而生成更具信息量的表示，同时保持了标准归一化的稳定性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有的归一化方法（如BatchNorm、LayerNorm、RMSNorm）只关注方差，通过强制零均值和单位方差来稳定训练，但没有控制表示如何捕捉与任务相关的信息。

Method: IBNorm 引入了基于信息瓶颈原理的压缩操作，鼓励嵌入保留预测信息，同时抑制不相关的变异。

Result: IBNorm 在大规模语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）上表现优于现有的归一化方法，并且互信息分析证实了其在信息瓶颈行为上的优越性。

Conclusion: IBNorm 是一种简单而强大的归一化方法，它在理论上和经验上都优于现有的方差中心归一化方法，能够生成更具信息量的表示，并保持良好的训练稳定性和兼容性。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [367] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 深度学习模型在输入和参数扰动方面存在不稳定性，本论文提出通过敏感性分析来解决此问题，结合了Lipschitz网络、损失函数曲率正则化和随机平滑等方法，旨在提高模型的泛化能力、鲁棒性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常不稳定且易受攻击，表现为输入微小变化可能导致预测剧烈改变，以及优化过程可能受限于尖锐的损失景观。本论文旨在解决这些问题。

Method: 通过敏感性分析，研究Lipschitz网络以约束输入扰动敏感性，结合基于损失函数曲率的正则化技术以促进更平滑的优化，并探索随机平滑以提高决策边界的鲁棒性。论文还开发了一个统一的框架，整合了Lipschitz连续性、随机平滑和曲率正则化，并提出了计算谱范数、Lipschitz约束层和改进认证程序的实用方法。

Result: 提出并结合了Lipschitz网络、损失函数曲率正则化和随机平滑等多种方法，以提高深度学习模型的泛化能力、对抗鲁棒性和训练稳定性，并开发了相关的理论分析和实用方法。

Conclusion: 本论文通过敏感性分析的统一视角，结合Lipschitz网络、曲率正则化和随机平滑等方法，为解决深度学习模型的稳定性和脆弱性问题提供了理论和实践上的贡献。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [368] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: We propose a hierarchical, physics-embedded learning framework using adaptive Fourier Neural Operators to model complex spatiotemporal dynamics. This framework decomposes PDE learning into two levels, embedding prior knowledge for physical consistency and data efficiency, and enabling interpretable discovery of governing equations via symbolic regression.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods for modeling complex spatiotemporal dynamics in far-from-equilibrium systems are often physically inconsistent or data-intensive, while current physics-informed methods struggle with representing complex operators and integrating partial physical knowledge.

Method: A two-level hierarchical architecture is proposed. The first level learns fundamental symbolic components of a PDE, and the second level learns their combinations. This framework is built upon adaptive Fourier Neural Operators and structurally decouples known and unknown terms to enable symbolic regression for discovering governing equations.

Result: The framework advances both forward spatiotemporal prediction and inverse discovery of physical laws from sparse and noisy data. It guarantees physical consistency and improves data efficiency by embedding known physical laws into the computational graph. The use of adaptive Fourier Neural Operators effectively captures non-local dependencies and high-order operators.

Conclusion: The proposed hierarchical physics-embedded learning framework offers a significant advancement in modeling complex spatiotemporal dynamics and discovering physical laws. Its ability to integrate prior knowledge, ensure physical consistency, and enable interpretable equation discovery addresses key limitations of existing methods.

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [369] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 该研究提出了一种名为多目标强化学习（Multi Goal RL）的新型强化学习问题形式化，旨在解决现有方法在鼓励探索和最大化预期回报的同时，学习能够广泛分布于奖励状态的策略的局限性。研究提出了一种新算法，通过优化定制的奖励函数，结合离线强化学习，来学习一个能够最大化预期回报并使状态分布均匀分布在目标状态上的策略。该算法具有有效的收敛性保证，并在合成MDP和标准RL环境中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法主要关注最大化预期回报，导致学习到的策略倾向于利用少数几个奖励源，而未能充分探索在许多实际场景中期望的、能够广泛分布于奖励状态并同时最大化预期回报的策略。

Method: 提出了一种名为多目标强化学习（Multi Goal RL）的新型强化学习问题形式化。该算法通过优化一个自定义的强化学习奖励函数来学习策略混合。该奖励函数在每次迭代中基于当前策略混合和采样轨迹计算得出。然后，利用这些采样轨迹通过一个离线强化学习算法来更新策略混合。

Result: 证明了该算法的性能保证，并展示了其在优化预期回报和奖励状态边际状态分布分散性方面的有效收敛性界限。通过在合成MDP和标准RL环境中的实验，验证了算法的有效性。

Conclusion: 所提出的多目标强化学习算法能够有效地学习一个策略混合，该策略在最大化预期回报的同时，能够将边际状态分布均匀地分散到目标状态集合上，解决了现有方法在鼓励探索和学习分散状态分布方面的局限性。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [370] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: CDFlow是一个新颖的可逆线性层，它使用循环和对角矩阵的乘积来提高表达能力，同时保持雅可比行列式的计算效率。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够提高表达能力、同时保持雅可比行列式计算效率和矩阵求逆效率的可逆线性层，以应对正向流模型中的挑战。

Method: 提出一种基于循环和对角矩阵乘积的新型可逆线性层，并基于此开发了CDFlow模型。利用快速傅里叶变换来优化矩阵求逆和计算对数行列式的时间复杂度。

Result: CDFlow在自然图像数据集上实现了强大的密度估计，并能有效建模具有周期性结构的数据。与现有方法相比，CDFlow显著加速了正向流中的关键操作。

Conclusion: CDFlow通过使用循环和对角矩阵的乘积，在保持计算效率的同时提高了表达能力，并在密度估计和数据建模方面取得了优异的性能，为可扩展生成模型提供了实际优势。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [371] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该研究探讨了在随机几何超图上进行变分学习的渐近一致性，并提出了一种名为 HOHL 的新方法，该方法通过对骨架图的拉普拉斯算子进行正则化来实现多尺度平滑，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有超图学习理论在半监督学习中的不足，并为超图学习提供更强的理论支持。

Method: 对随机几何超图上的变分学习进行渐近一致性分析，确定超图学习的适定性条件，并推导出其收敛至加权 p-拉普拉斯方程。在此基础上，提出 HOHL 方法，通过骨架图的拉普拉斯算子进行多尺度平滑正则化。

Result: HOHL 方法收敛至高阶索伯列夫半范数，并在标准基线上取得了强的经验性能。

Conclusion: 研究为超图学习提供了理论基础，并提出了一种有效的新方法 HOHL，该方法在多尺度平滑和半监督学习任务中表现出优越性能。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [372] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 本文提出了一种用于知识图谱嵌入（KGE）模型的加权平均模型合并方法，以提高链接预测性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的KGE模型集成方法（如训练多个模型后取平均值）存在训练延迟和内存开销大的问题。模型合并，特别是加权平均，作为一种有前景的替代方案，无需训练多个模型。

Method: 提出两种加权平均模型合并方法：1. 维护从训练开始的平均模型参数；2. 仅在验证集性能提升时才更新平均模型参数。

Result: 在链接预测任务上，所提出的加权平均方法与标准的基准集成方法相比，在考虑字面增强KGE模型和多跳查询回答任务时，表现出持续的性能提升。

Conclusion: 所提出的加权平均模型合并方法在KGE模型中是一种有效且高效的集成技术，能够提升多种下游任务的性能。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [373] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 在机器学习中，虽然通常使用非凸方法（如Adam）来处理非凸损失函数，但损失函数在局部最小值附近具有凸性。本文提出了一种两阶段优化算法，利用损失函数从非凸到凸的特性，在非凸区域使用Adam，在凸区域使用共轭梯度法（CG），以提高收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习损失函数包含非凸区域，导致普遍采用Adam等非凸优化方法。然而，局部最小值附近存在凸性，而二阶方法（如CG）在凸区域可实现超线性收敛。因此，利用损失函数从非凸到凸的特性进行优化具有潜力。

Method: 提出了一种新颖的两阶段优化算法框架。该算法通过观察梯度范数对损失的依赖性来检测损失函数从非凸区域切换到凸区域的切换点。在非凸区域使用Adam，在凸区域使用共轭梯度法（CG）。

Result: 计算实验证实了该假设，即损失函数的这种简单凸性结构足够频繁，可以被实际利用来显著提高收敛速度和准确性。

Conclusion: 在实际应用中，机器学习损失函数常常表现出从非凸到凸的特性，特别是在接近最优值时。利用这一特性，采用Adam和CG相结合的两阶段优化方法，可以有效提升模型的收敛效率和最终的准确度。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [374] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大型语言模型（LLM）的优化方法，可以解决连续黑盒优化问题，并且无需参数调整。


<details>
  <summary>Details</summary>
Motivation: 昂贵的、无导数的黑盒函数全局优化需要极高的样本效率，而现有的贝叶斯优化（BO）方法需要仔细的参数调整，大型语言模型（LLM）在解决连续黑盒优化任务方面能力有限。

Method: 通过在源自各种BO参数化的广泛合成数据集上对LLM进行微调，GPTOpt可以利用LLM的预训练能力来泛化优化任务。

Result: GPTOpt在各种黑盒优化基准测试中优于传统优化器。

Conclusion: LLM具有进行高级数值推理的能力，并为无需参数调整的全局优化提供了一个灵活的框架。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [375] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 现有的分类器校准评估方法要么关注特定方面，要么计算成本高昂。本文提出了一个名为“效用校准”的通用框架，用于评估多类别分类器的校准，该框架能够根据特定效用函数来衡量校准误差，从而统一和重新解释了现有的校准指标，并能评估更广泛的下游效用。


<details>
  <summary>Details</summary>
Motivation: 确保分类器的预测与其观察到的频率一致（即良好校准）是分类器可信度的基本要求。然而，现有的多类别校准评估方法存在不足，要么只关注预测的特定方面（例如，最高类别置信度、类别校准），要么采用计算成本高昂的变分方法。

Method: 提出了一种名为“效用校准”的通用框架，用于评估多类别校准。该框架通过一个通用的效用函数来衡量校准误差，该函数封装了最终用户的目标或决策标准。通过具体化该效用函数，可以统一和重新解释现有的校准指标，并提出更鲁棒的版本，还可以评估更丰富的下游效用。

Result: 该框架能够统一和重新解释现有的几种校准指标，特别是允许对最高类别和类别校准指标进行更鲁棒的版本。此外，它超越了二元化的方法，能够评估更丰富的下游效用。

Conclusion: 效用校准提供了一个可扩展的框架，用于评估多类别分类器的校准，能够根据特定效用函数来衡量校准误差，从而克服了现有方法的局限性，并为更广泛的应用提供了可能。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [376] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 该研究提出了一种名为梯度-权重对齐（GWA）的新型验证指标，用于监测深度学习模型的训练动态和泛化能力，无需验证集。


<details>
  <summary>Details</summary>
Motivation: 为了在深度学习中检测过拟合、泛化差以及监控训练动态，需要鲁棒的验证指标。

Method: 提出梯度-权重对齐（GWA）指标，量化单个样本梯度与模型权重之间的相干性。实验表明，有效的学习对应于一致的对齐，而失面对则表明泛化能力下降。GWA可以在训练过程中高效计算，并能反映样本贡献和数据集学习动态。

Result: GWA能够准确预测最佳的提前停止点，支持有原则的模型比较，并识别有影响力的训练样本。

Conclusion: GWA提供了一种无需验证集即可直接从训练数据中进行模型分析的方法。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [377] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: Neurosymbolic AI 结合了神经网络感知和符号推理，但容易出现“捷径推理”，即利用虚假相关性来满足符号约束。本文提出的原型神经符号架构通过学习正确的基本概念来解决这个问题，即使在数据稀疏的情况下也能避免捷径推理。该方法利用原型学习理论，通过在训练模型时满足背景知识并考虑输入与少量标记数据点的相似性来有效避免捷径推理。在 rsbench 基准测试的各种设置和任务中进行了广泛验证，包括合成任务（MNIST-EvenOdd 和 Kand-Logic）和现实世界任务（BDD-OIA），在稀疏监督的情况下，在学习正确概念方面取得了显著改进。研究结果表明，原型接地是实现安全可靠的神经符号学习的有效且标注效率高的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号模型容易出现捷径推理，即利用虚假相关性来满足符号约束，而不是学习正确的概念。

Method: 提出原型神经符号架构，利用原型学习理论，在满足背景知识的同时考虑输入与少量标记数据点的相似性，以避免捷径推理。

Result: 在 rsbench 基准测试的各种设置和任务中进行了广泛验证，包括合成任务（MNIST-EvenOdd 和 Kand-Logic）和现实世界任务（BDD-OIA），在稀疏监督的情况下，在学习正确概念方面取得了显著改进。

Conclusion: 原型接地是实现安全可靠的神经符号学习的有效且标注效率高的策略。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [378] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性RNN的单变量时间序列基础模型，使用GatedDeltaProduct架构和状态编织技术，实现了完全可并行的训练，并在Gift-Eval基准测试中取得了顶尖的零样本预测性能。


<details>
  <summary>Details</summary>
Motivation: 零样本时间序列预测基础模型面临长期预测效率低下和可复现性差的问题，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。

Method: TempoPFN使用GatedDeltaProduct架构和状态编织技术，实现了跨序列长度的完全并行训练，无需窗口或摘要技术，同时保持了稳健的时间状态跟踪。其合成数据管道统一了包括随机微分方程、高斯过程和音频合成在内的多种生成器，并进行了新颖的增强。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN取得了顶尖的竞争性能，优于所有现有的仅使用合成数据的方法，并超越了绝大多数在真实世界数据上训练的模型。通过完全可并行的训练和推理，其效率也高于现有基线模型。

Conclusion: TempoPFN通过其创新的架构和数据管道，解决了零样本时间序列预测中的长期预测效率和可复现性挑战，并在零样本预测任务中取得了优异的成绩，为未来的研究提供了可复现的基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [379] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法，通过支持向量机（SVM）算法，利用HackerEarth员工倦怠挑战数据集，以R2=0.84的准确率预测员工倦怠风险，并开发了一个Streamlit交互界面，旨在为组织提供数据驱动的心理健康策略。


<details>
  <summary>Details</summary>
Motivation: 识别和预测员工倦怠风险，以提高个人福祉和组织绩效。

Method: 评估了三种监督学习算法（KNN、随机森林、SVM），并使用30折交叉验证和决定系数（R2）来评估模型性能。最后，使用Streamlit开发了一个交互式界面，方便非技术用户进行倦怠风险预测。

Result: SVM算法在预测准确率上表现最佳，R2值为0.84，且统计学上优于KNN和随机森林。

Conclusion: 机器学习，特别是SVM，在预测员工倦怠风险方面具有巨大潜力，能够支持早期预警和数据驱动的组织心理健康策略。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [380] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出的模型能够更忠实地解释概念，并引入了一个新的概念一致性指标 C^2-Score。


<details>
  <summary>Details</summary>
Motivation: 理解深度网络的工作机制仍然是一个挑战，现有的基于概念的方法可能不总是忠实于模型，并做出关于概念的限制性假设。

Method: 提出了一种新的模型，该模型具有模型固有的机制概念解释，概念是跨类共享的，并且可以从任何层忠实地追溯其对 logit 的贡献及其输入可视化。利用基础模型提出了一种新的概念一致性指标 C^2-Score。

Result: 与现有工作相比，提出的概念在定量上更加一致，用户发现其更具可解释性，同时保持了具有竞争力的 ImageNet 性能。

Conclusion: 所提出的模型能够更忠实地解释概念，并引入了一个新的概念一致性指标 C^2-Score。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [381] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Transformer在图结构学习中取得了成功，但对其训练动态的理论理解仅限于树状图。本研究提出了核引导互信息（KG-MI）作为一种新的信息论度量，并将其与多头注意力框架相结合，以解决更一般的有向无环图（DAG）问题。研究证明，通过梯度上升训练的单层多头Transformer能够以多项式时间收敛到全局最优解，并能有效恢复图结构。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer在图结构学习中表现出强大的能力，但其理论理解仅限于树状图，难以扩展到具有多个父节点的有向无环图（DAG）。主要挑战在于设计能够让不同注意力头分别学习多个父节点关系的训练目标。

Method: 提出了一种新颖的信息论度量——核引导互信息（KG-MI），该度量基于f散度。将KG-MI与多头注意力框架相结合，每个头关联一个不同的边际转移核，以有效地模拟多样化的父子依赖关系。证明了通过梯度上升训练单层多头Transformer可以收敛到全局最优解，并分析了收敛时的注意力得分模式。特别地，当f散度特化为KL散度时，学习到的注意力得分能够准确反映真实邻接矩阵，从而可证明地恢复底层图结构。

Result: 在给定K父DAG生成的序列下，证明了单层多头Transformer通过梯度上升训练能在多项式时间内收敛到全局最优解。分析了收敛时的注意力得分模式。当f散度特化为KL散度时，模型可以准确地恢复出真实的图结构。

Conclusion: 本研究提出了核引导互信息（KG-MI）和相应训练方法，成功解决了Transformer在有向无环图（DAG）结构学习中的理论瓶颈，并提供了模型能有效恢复图结构的理论保证。实验结果验证了理论分析的有效性。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [382] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [383] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: PAC-Bayes 框架通常只提供对随机抽样假设的期望风险的保证，这在需要确定性预测的实际应用中受到限制。本研究提出了一个统一的框架，用于从随机 PAC-Bayesian 保证中提取适用于单个假设的保证。


<details>
  <summary>Details</summary>
Motivation: PAC-Bayes 框架在处理不可数假设空间时具有流行和高效的特点，但其经典形式仅提供对随机抽样假设的期望风险的保证，这限制了其在需要单一确定性假设的实际应用中的使用。

Method: 提出一个统一的框架，用于从随机 PAC-Bayesian 保证中提取适用于单个假设的保证，并推导出一个通用的 Oracle 界限，进而推导出一个数值界限和适用于多数投票的特例。

Result: 提出的方法在确定性分类器的泛化界限方面，在经验上持续优于流行的基线方法，效果提升可达两倍。

Conclusion: 提出的统一框架能够有效地从随机 PAC-Bayesian 保证中提取适用于单个确定性假设的泛化保证，并在实践中显示出优于现有方法的性能。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [384] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本研究将 Sobolev IPM 问题推广到图度量空间，并引入了 Orlicz 几何结构以克服 L^p 范数的局限性。通过将 Orlicz-Sobolev 范数与 Musielak 范数联系起来，提出了一种新颖的正则化方法 GSI-M，该方法能够将计算简化为单变量优化问题，从而实现高效计算。实验证明，GSI-M 在计算速度上远超 OW，并在文档分类和拓扑数据分析等任务中展现了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有 Sobolev IPM 方法受限于 L^p 几何结构，难以融入 L^p 几何范式之外的结构化先验。本研究旨在克服这一局限性，通过引入 Orlicz 几何结构来推广 Sobolev IPM，以捕捉更细致的几何关系，并兼容更多样化的几何先验。

Method: 提出将 Sobolev IPM 通过 Orlicz 几何结构进行推广，并建立 Orlicz-Sobolev 范数与 Musielak 范数之间的理论联系，提出一种新颖的正则化方法 GSI。利用图结构，将 GSI-M 问题简化为单变量优化问题。

Result: GSI-M 在计算上比 OW 快几个数量级。在文档分类和拓扑数据分析任务中，GSI-M 展现了其在图结构上比较概率测度的实际优势。

Conclusion: 本研究成功地将 Sobolev IPM 推广到 Orlicz 几何结构，并通过 GSI-M 方法实现了高效且实用的计算，克服了传统方法的局限性，并在实际应用中取得了优于现有方法的性能。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [385] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 本研究提出了一种在低秩流形上进行局部学习的框架，通过SVD分解权重矩阵，并结合复合损失函数进行训练，旨在解决传统BP方法和DFA方法的局限性，实现与BP相当的准确率，并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的反向传播（BP）算法虽然精度高，但需要全局误差传播和全参数化，导致内存和计算开销巨大。直接反馈对齐（DFA）虽然允许局部、可并行更新且内存需求较低，但在更深的网络（尤其是CNN）中，由于反馈不确定和可扩展性差而受到限制。

Method: 提出一种在低秩流形上进行局部学习的框架，该框架直接作用于通过SVD分解权重矩阵得到的低秩流形。每一层都以分解后的形式进行训练，并使用复合损失函数（包含交叉熵、子空间对齐和正交正则化）对SVD分量进行更新。反馈矩阵的构建也匹配SVD结构，确保前向和反馈路径之间的一致性。

Result: 所提出的方法相对于原始DFA模型减少了可训练参数的数量，且无需进行剪枝或事后压缩。在CIFAR-10、CIFAR-100和ImageNet上的实验表明，该方法实现了与BP相当的准确率。消融研究证实了在低秩设置下，每个损失项的重要性。

Conclusion: 在低秩流形上进行局部学习是一种原则性强且可扩展性好的替代方法，可以取代全秩的基于梯度的训练。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [386] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一种基于 proper scoring rules 的回归任务不确定性量化方法，特别是核分数。该方法统一了现有的度量，并允许设计新的度量，其行为可以通过核的选择来控制。实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法主要集中在分类任务，而回归任务，特别是在安全关键领域，也需要适当的不确定性量化。

Method: 提出了一种基于 proper scoring rules 的不确定性度量方法，并重点关注核分数。推导了核分数特性与下游行为之间的显式对应关系，为设计特定任务的度量提供了具体指导。

Result: 实验证明了所提出的不确定性度量方法在下游任务中是有效的，并揭示了不同实例化方法之间的权衡，包括鲁棒性和分布外检测性能。

Conclusion: 本文提出的基于核分数的 proper scoring rules 的方法为回归任务的不确定性量化提供了一个统一且可控的框架，并为设计特定任务的度量提供了指导。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [387] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 现代AI硬件（如英伟达的Blackwell架构）日益采用低精度浮点（FP）格式来处理大型语言模型（LLMs）中普遍存在的激活值异常。尽管有此行业趋势，但缺乏对不同粒度的FP和整数（INT）量化进行统一比较，导致算法和硬件协同设计缺乏明确指导。本文通过系统研究FP和INT格式之间的权衡来填补这一空白。我们揭示了一个关键的性能交叉点：虽然FP在粗粒度量化方面表现出色，但在细粒度（块状）层面，比较则更为复杂。我们的综合比较表明，对于流行的8位细粒度格式（例如，块大小为32的MX），MXINT8在算法准确性和硬件效率方面均优于其FP对应格式。然而，对于4位格式，FP（例如，MXFP4、NVFP4）通常在准确性方面占优，尽管我们证明了当应用诸如Hadamard旋转之类的异常值缓解技术时，NVINT4可以超越NVFP4。我们还引入了一种对称裁剪方法，该方法解决了细粒度低位INT训练中的梯度偏差问题，使得MXINT8训练能够实现近乎无损的性能。这些发现对当前的硬件发展轨迹提出了挑战，表明“一刀切”的FP方法并非最优，并主张细粒度INT格式，特别是MXINT8，为未来的AI加速器提供了准确性、功耗和效率的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 填补当前在不同粒度下FP和INT量化之间缺乏统一比较的空白，为算法和硬件协同设计提供指导。

Method: 系统地研究FP和INT格式之间的权衡，比较不同位宽（8位和4位）和不同粒度（粗粒度和细粒度）下的性能，并引入对称裁剪方法解决INT训练中的梯度偏差问题。

Result: 在粗粒度量化下FP表现优于INT；在细粒度8位格式下，MXINT8优于其FP对应格式；在细粒度4位格式下，FP通常在准确性上占优，但通过异常值缓解技术，NVINT4可超越NVFP4；提出的对称裁剪方法使得MXINT8训练接近无损。最终证明细粒度INT格式（特别是MXINT8）在精度、功耗和效率方面提供了更好的平衡。

Conclusion: 当前的硬件发展趋势（倾向于FP）并非最优。细粒度INT格式，尤其是MXINT8，为未来的AI加速器在准确性、功耗和效率方面提供了更优的平衡，应被优先考虑。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [388] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap是一个新颖的框架，通过在频谱域利用拉普拉斯平滑来聚合图结构数据，以解决联邦学习中节点嵌入的隐私和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在处理图结构数据时，要么存在隐私泄露风险（需要交换敏感的节点嵌入），要么计算成本高昂，影响了可扩展性。

Method: FedLap框架通过在频谱域利用拉普拉斯平滑来聚合全局结构信息，有效捕捉节点间的依赖关系，同时保证了隐私和可扩展性。

Result: FedLap在保持隐私的同时，实现了与现有技术相当或更优的效用。

Conclusion: FedLap是第一个具有强隐私保证的子图联邦学习方案，解决了现有方法的局限性，并在实验中证明了其有效性。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [389] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: RNNs can replicate HMMs by developing structured connectivity and utilizing stochastic resonance to generate discrete latent dynamics, generalizing across architectures.


<details>
  <summary>Details</summary>
Motivation: Investigate how RNNs can model rich, spontaneous, and stochastic behaviors observed in natural settings, which past work has not adequately addressed, and reconcile the continuous state spaces of RNNs with the discrete latent states and stochastic transitions revealed by HMMs.

Method: 1. Show that RNNs can replicate HMM emission statistics. 2. Reverse-engineer trained RNNs to uncover their internal mechanisms. 3. Analyze RNN activity in the absence of inputs and when driven by stochastic inputs. 4. Examine the network's connectivity, particularly the role of 'kick neurons'. 5. Investigate the emergence of stochastic resonance during training. 6. Analyze RNNs across different HMM architectures (fully connected, cyclic, linear-chain) to assess generalization.

Result: In the absence of inputs, RNN activity collapses to a fixed point. With stochastic input, trajectories show noise-sustained dynamics along closed orbits. Rotation along these orbits modulates emission probabilities, governed by transitions between slow, noise-driven dynamics regions connected by fast, deterministic transitions. Trained RNNs exhibit structured connectivity with 'kick neurons' initiating transitions. Stochastic resonance emerges during training, enabling probabilistic computations. The analyzed mechanism generalizes across different HMM architectures through modular reuse of a dynamical motif.

Conclusion: RNNs can emulate complex discrete latent dynamics by developing structured connectivity and leveraging stochastic resonance, offering a compositional principle for modeling rich, spontaneous behaviors observed in nature.

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [390] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: GNSS是一个用于动态结构问题模拟的图神经网络框架，可实现快速准确的预测。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在结构动力学问题上的研究不足，尤其是在动态案例方面。

Method: GNSS采用编码-处理-解码范式，包含三个关键特性：(i)在节点固定的局部坐标系中表达节点运动学，(ii)采用感知符号的回归损失，(iii)使用受波长启发的连接半径。

Result: GNSS能够准确模拟结构动力学问题，并在数以百计的时间步内保持物理规律，同时泛化到未知的载荷条件，在推理速度上显著快于有限元基线，并保持空间和时间保真度。

Conclusion: GNSS是一种用于动态、以波为主的结构模拟的有效替代方法，具有与物理一致的更新规则。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [391] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了一种卷积脉冲GRU (CS-GRU) 单元，该单元结合了卷积操作和脉冲神经网络（SNNs）的优点，以提高处理时序和时空序列数据的能力，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的循环神经网络（RNNs）在处理长序列时会丢失局部细节，而先前的脉冲GRU（SpikGRU）等方法未能捕捉事件驱动的时空数据中的细粒度局部依赖关系。

Method: 提出并实现了一种新的卷积脉冲GRU (CS-GRU) 单元，该单元利用卷积操作来保留局部结构和依赖关系，同时将脉冲神经元的时序精度与GRU的高效门控机制相结合。

Result: CS-GRU 在纯时间数据集（NTIDIGITS, SHD）和时空基准（MNIST, DVSGesture, CIFAR10DVS）上表现出色，平均准确率比最先进的GRU变体高出4.35%，在序列任务上准确率超过90%，在MNIST上达到99.31%。与SpikGRU相比，效率提高了69%。

Conclusion: CS-GRU 是一种多功能的架构，能够有效地处理时序和时空序列数据，并在准确性和效率方面优于现有方法。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [392] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个可扩展、轻量级的工具，用于预处理和探索性数据分析，利用SparkDataFrames处理数据，并通过JSON文件定义处理流程，解决了现有工作流的局限性，能够加速和简化机器学习模型开发。


<details>
  <summary>Details</summary>
Motivation: 深度学习和AI的增长带来了海量数据处理的需求，现有工作流在可扩展性和集成方面存在不足，因此需要一个健壮、可扩展且轻量级的工具来预处理任意数据集。

Method: 利用SparkDataFrames存储和处理数据以保证可扩展性。使用通用的JSON输入文件格式来描述对DataFrame的逐步修改。实现了输入/输出、过滤、基本统计、特征工程和探索性数据分析等阶段。共实现了69个阶段，并使用六个不同的数据集进行了演示。

Result: MLPrE能够处理多种数据类型和大小的数据集，能够独立处理和重组来自不同字段的数据，并且演示了聚类和为图数据库准备数据的能力，例如使用UniProt术语和葡萄酒质量数据。

Conclusion: MLPrE是一个通用的、可扩展的工具，用于预处理和早期数据分析，满足了当前机器学习应用日益增长的需求，能够加速和简化更大工作流程的早期开发。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [393] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 传统的多个实例学习（MIL）方法在处理医学图像时会忽略实例间的上下文关系，这在某些任务中至关重要。本文设计了一个需要考虑相邻实例特征的合成分类任务，并证明了现有MIL方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中的多个实例学习（MIL）通常将实例（如图像块或切片）分开处理，忽略了它们之间的上下文关系，而这些关系在实际应用中可能至关重要。

Method: 设计了一个需要考虑相邻实例特征的合成分类任务，并用它来量化评估现有MIL方法（包括较新的相关MIL方法）在从头开始训练时的性能，并与最优贝叶斯估计器进行比较。

Result: 现有的MIL方法，即使是较新的相关MIL方法，在合成任务上从头开始训练时，其泛化能力仍不如最优贝叶斯估计器。

Conclusion: 忽略实例间上下文关系是现有MIL方法在某些医学图像分析任务中表现不佳的原因，即使是较新的方法也需要改进以更好地利用这些关系。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [394] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: NSFs可以直接学习随机微分方程的转移定律，从而在任意时间点进行抽样，速度比数值方法快得多，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的随机微分方程（SDE）建模方法在处理金融、物理和机器学习中的嘈杂和不规则采样时间序列时，需要昂贵的数值求解器在任意时间点之间进行采样。

Method: 我们引入了神经随机流（NSFs）及其潜在变体，它们使用具有架构约束的条件归一化流直接学习（潜在）SDE转移定律，这些约束保留了从随机流继承的属性。

Result: NSFs能够在一对时间点之间进行一次性抽样，并且在时间间隔大的情况下速度提高了两个数量级。在合成SDE模拟以及现实世界的跟踪和视频数据上的实验表明，NSFs在保持分布准确性方面与数值方法相当，同时大大减少了任意时间点抽样所需的计算量。

Conclusion: NSFs提供了一种更快、更准确地对SDE进行建模和采样的方法，在需要处理嘈杂和不规则采样数据时具有广泛的应用前景。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [395] [Reviving Thorup's Shortcut Conjecture](https://arxiv.org/abs/2510.24954)
*Aaron Bernstein,Henry Fleischmann,George Z. Li,Bernhard Haeupler,Maximilian Probst Gutenberg,Gary Hoppenworth,Seth Pettie,Thatchaphol Saranurak,Yonggang Jiang,Leon Schiller*

Main category: cs.DS

TL;DR: 该论文研究了Thorup猜想，该猜想提出了关于图的可达性捷径（shortcuts）的存在性问题，特别关注捷径的数量与图的直径之间的权衡。虽然原始猜想已被证伪，但论文认为允许引入额外的“斯坦纳顶点”（Steiner vertices）可以保留猜想的精神。


<details>
  <summary>Details</summary>
Motivation: Thorup猜想的原始版本已被证伪，但论文认为，如果允许在图中添加斯坦纳顶点，猜想的精神仍然可能成立。因此，本研究的动机是探索在允许斯坦纳顶点的情况下，Thorup猜想的修正版本以及相关的捷径构造问题。

Method: 论文采用了“显式攻击”（explicit attacks）的方法来打破现有的捷径下界，并提出一个“候选难例”（candidate hard instance）来推进对修正版Thorup猜想的研究。此外，论文还探讨了捷径的“厚度”（thickness）参数，并排除了某些条件下理想捷径存在的可能性。

Result: （1）在允许斯坦纳顶点的情况下，论文提出了能够打破所有已知捷径下界的显式方法。（2）论文排除了在“厚度”参数 $t=o(	ext{log } n/	ext{log log } n)$ 的条件下，存在具有 $m^{1+o(1)}$ 数量和 $m^{o(1)}$ 直径的理想捷径的可能性。（3）论文提出了一个候选难例，作为解决修正版Thorup猜想的下一步。

Conclusion: 论文通过引入斯坦纳顶点，为Thorup猜想的修正版本提供了新的视角和研究方向。虽然排除了某些特定条件下的理想捷径，但提出了打破现有下界的方法和未来的研究方向。此外，论文展示了其研究在并行算法设计上的潜在影响，有望在计算捷径和最大流问题上实现更优的并行复杂度。

Abstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of
reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally
asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add
$m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$
so that $\text{dist}_{G_+}(u,v) \leq m^{o(1)}$ for all $(u,v)\in E^*$, where
$G_+=(V,E\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03],
followed by significant efforts in the last few years to optimize the lower
bounds.
  In this paper we observe that although Hesse refuted the letter of Thorup's
conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not
refute the spirit of the conjecture, which should allow $G_+$ to contain both
new (shortcut) edges and new Steiner vertices. Our results are as follows.
  (1) On the positive side, we present explicit attacks that break all known
shortcut lower bounds when Steiner vertices are allowed.
  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size,
$m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\log n/\log \log n)$,
meaning no path can contain $t$ consecutive Steiner vertices.
  (3) We propose a candidate hard instance as the next step toward resolving
the revised version of Thorup's conjecture.
  Finally, we show promising implications. Almost-optimal parallel algorithms
for computing a generalization of the shortcut that approximately preserves
distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$
depth for exact shortcut paths and exact maximum flow. The state-of-the-art
algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\v{n}, Haeupler,
Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.

</details>


### [396] [Hedgegraph Polymatroids](https://arxiv.org/abs/2510.25043)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Weihang Wang,Weihao Zhu*

Main category: cs.DS

TL;DR: Hedgegraphs are a generalization of hypergraphs that allow modeling dependencies between hyperedges, but they pose algorithmic challenges due to non-submodular cut functions. This paper introduces two alternative partition-based connectivity measures and a polymatroid associated with hedgegraphs, leading to new tractability results and generalizations of classical results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the algorithmic challenges posed by hedgegraphs, specifically the non-submodular cut function which hinders connectivity algorithms. The goal is to introduce new measures and tools to enable more efficient algorithms for hedgegraphs.

Method: The paper introduces two alternative partition-based measures of connectivity in hedgegraphs. It also studies a polymatroid associated with hedgegraphs, replacing the traditional cut function.

Result: The paper presents new tractability results and generalizations of classical results on graphs and hypergraphs by using the polymatroidal lens associated with hedgegraphs.

Conclusion: The introduction of alternative connectivity measures and the polymatroidal lens provides new avenues for algorithmic development and theoretical understanding of hedgegraphs, overcoming limitations of previous approaches.

Abstract: Graphs and hypergraphs combine expressive modeling power with algorithmic
efficiency for a wide range of applications. Hedgegraphs generalize hypergraphs
further by grouping hyperedges under a color/hedge. This allows hedgegraphs to
model dependencies between hyperedges and leads to several applications.
However, it poses algorithmic challenges. In particular, the cut function is
not submodular, which has been a barrier to algorithms for connectivity. In
this work, we introduce two alternative partition-based measures of
connectivity in hedgegraphs and study their structural and algorithmic aspects.
Instead of the cut function, we investigate a polymatroid associated with
hedgegraphs. The polymatroidal lens leads to new tractability results as well
as insightful generalizations of classical results on graphs and hypergraphs.

</details>


### [397] [$\{s,t\}$-Separating Principal Partition Sequence of Submodular Functions](https://arxiv.org/abs/2510.25664)
*Kristóf Bérczi,Karthekeyan Chandrasekaran,Tamás Király,Daniel P. Szabo*

Main category: cs.DS

TL;DR: 我们定义并证明了子模函数的 {s,t} 分离主分割序列的存在性，并开发了一个构造它的多项式时间算法。我们还展示了该序列在子模 k-划分问题和超图定向问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是子模函数的主分割序列在聚类、快速算法和近似算法等领域具有广泛的应用，并希望在此基础上发展 {s,t} 分离主分割序列的理论，以解决 {s,t} 分离子模 k-划分问题和超图定向问题。

Method: 我们定义了 {s,t} 分离主分割序列，证明了它的存在性，并设计了一个构造它的多项式时间算法。

Result: 我们证明了 {s,t} 分离主分割序列的存在性，并提供了一个多项式时间算法来构造它。我们展示了该序列在两个应用中的有效性：(1) 针对单调和正模函数的 {s,t} 分离子模 k-划分问题的近似算法；(2) 解决具有至少 k 的强连通性和至少 l 的 (s,t) 连通性的超图定向问题的多项式时间算法。

Conclusion: 本文成功地将子模函数的主分割序列推广到 {s,t} 分离的情况，并证明了这种新结构的有效性，为解决相关的组合优化问题提供了新的工具和方法。

Abstract: Narayanan and Fujishige showed the existence of the principal partition
sequence of a submodular function, a structure with numerous applications in
areas such as clustering, fast algorithms, and approximation algorithms. In
this work, motivated by two applications, we develop a theory of
$\{s,t\}$-separating principal partition sequence of a submodular function. We
define this sequence, show its existence, and design a polynomial-time
algorithm to construct it. We show two applications: (1) approximation
algorithm for the $\{s,t\}$-separating submodular $k$-partitioning problem for
monotone and posimodular functions and (2) polynomial-time algorithm for the
hypergraph orientation problem of finding an orientation that simultaneously
has strong connectivity at least $k$ and $(s,t)$-connectivity at least $\ell$.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [398] [Distributed learning for automatic modulation recognition in bandwidth-limited networks](https://arxiv.org/abs/2510.24722)
*Narges Rashvand,Kenneth Witham,Gabriel Maldonado,Vinit Katariya,Aly Sultan,Gunar Schirner,Hamed Tabkhi*

Main category: eess.SP

TL;DR: 分布式学习方法在自动调制识别（AMR）任务中展现出潜力，能在带宽受限的网络中实现高精度识别，同时显著降低带宽需求。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的自动调制识别（AMR）方法通常采用中心化处理方式，需要将所有训练数据传输到高功耗计算设备，这对于带宽受限的无线网络来说不切实际。本研究旨在探索分布式学习在AMR任务中的应用，以解决这一挑战。

Method: 本研究提出了两种基于分布式学习的AMR方法。方法一：在多个接收器之间进行共识投票，以识别调制类型。方法二：每个接收器共享其特征图，并由中央节点进行聚合。实验使用TeMuRAMRD 2023数据集进行评估。

Result: 与中心化方法（6个接收器，准确率91%）相比，单个接收器的准确率仅为41%。提出的分布式方法一（基于共识投票）准确率略低，但带宽需求降低了256倍。提出的分布式方法二（共享特征图）带宽需求降低了8倍。

Conclusion: 分布式AMR方法能够在带宽受限的无线网络中，在显著降低带宽需求的同时，有效提升AMR任务的准确率。

Abstract: Automatic Modulation Recognition (AMR) is critical in identifying various
modulation types in wireless communication systems. Recent advancements in deep
learning have facilitated the integration of algorithms into AMR techniques.
However, this integration typically follows a centralized approach that
necessitates collecting and processing all training data on high-powered
computing devices, which may prove impractical for bandwidth-limited wireless
networks. In response to this challenge, this study introduces two methods for
distributed learning-based AMR on the collaboration of multiple receivers to
perform AMR tasks. The TeMuRAMRD 2023 dataset is employed to support this
investigation, uniquely suited for multi-receiver AMR tasks. Within this
distributed sensing environment, multiple receivers collaborate in identifying
modulation types from the same RF signal, each possessing a partial perspective
of the overall environment. Experimental results demonstrate that the
centralized-based AMR, with six receivers, attains an impressive accuracy rate
of 91%, while individual receivers exhibit a notably lower accuracy, at around
41%. Nonetheless, the two proposed decentralized learning-based AMR methods
exhibit noteworthy enhancements. Based on consensus voting among six receivers,
the initial method achieves a marginally lower accuracy. It achieves this while
substantially reducing the bandwidth demands to a 1/256th of the centralized
model. With the second distributed method, each receiver shares its feature
map, subsequently aggregated by a central node. This approach also accompanies
a substantial bandwidth reduction of 1/8 compared to the centralized approach.
These findings highlight the capacity of distributed AMR to significantly
enhance accuracy while effectively addressing the constraints of
bandwidth-limited wireless networks.

</details>


### [399] [Ambient Backscatter Communication Assisted by Fluid Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2510.24725)
*Masoud Kaveh,Farshad Rostami Ghadi,Riku Jantti,Kai-Kit Wong,F. Javier Lopez-Martinez*

Main category: eess.SP

TL;DR: 本文提出并研究了流体可重构智能表面（FRIS）在环境回溯通信（AmBC）系统中的应用，与传统RIS不同，FRIS具有可调的流体元件，能够动态调整位置，从而提供增强的空间适应性。通过粒子群优化（PSO）算法优化FRIS元件位置，仿真结果表明，FRIS辅助的AmBC系统在可实现吞吐量方面显著优于传统的基于RIS的AmBC系统。


<details>
  <summary>Details</summary>
Motivation: 传统的RIS位置固定，在实际应用中可能无法达到最优的信号传输效果。FRIS通过流体元件的动态位置调整，能够更好地适应通信环境，提高信号传输效率。

Method: 构建了一个AmBC系统模型，其中AmBC标签通过FRIS与阅读器通信。将FRIS元件位置的优化问题表述为非凸问题，并采用粒子群优化（PSO）算法寻找近优解。

Result: 仿真结果表明，FRIS辅助的AmBC系统在可实现吞吐量方面显著优于传统的基于RIS的AmBC系统。

Conclusion: FRIS作为一种新型的可重构智能表面，在AmBC系统中展现出优越的性能，能够有效克服传统RIS的局限性，提高通信系统的吞吐量。

Abstract: This paper investigates the integration of a fluid reconfigurable intelligent
surface (FRIS) into ambient backscatter communication (AmBC) systems. Unlike
conventional reconfigurable intelligent surfaces (RISs) with fixed position
elements, FRIS employs fluidic elements that can dynamically adjust their
positions, offering enhanced spatial adaptability. We develop a system model
where an AmBC tag communicates with a reader through an FRIS, which is
particularly beneficial in scenarios where the direct tag-to-reader link is
weak or blocked by obstacles. The achievable backscatter rate is analyzed, and
the optimization of FRIS element positions is formulated as a non-convex
problem. To address this, we employ particle swarm optimization (PSO) to obtain
near-optimal configurations of the fluid elements. Simulation results
demonstrate that FRIS-aided AmBC significantly outperforms conventional
RIS-based AmBC systems in terms of achievable throughput.

</details>


### [400] [Modelling Real-Life Cycling Decisions in Real Urban Settings Through Psychophysiology and LLM-Derived Contextual Data](https://arxiv.org/abs/2510.24726)
*Maximiliano Rosadio Z.,Angel Jimenez-Molina,Bastián Henríquez,Paulina Leiva,Ricardo Hurtubia,Ricardo De La Paz Guala,Leandro Gayozo,C. Angelo Guevara*

Main category: eess.SP

TL;DR: 通过利用大型语言模型（LLM）从多媒体记录中提取上下文数据，本研究解决了在交通环境中测量情感状态和理解触发因素的挑战，并在圣地亚哥的一项城市自行车案例研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 衡量交通环境中的情绪状态是一个新兴领域，但基于自我报告的方法存在粒度低和记忆偏差的问题，而基于生理指标的方法虽然能提供连续数据，但理解情绪变化的原因却面临着真实世界环境中难以收集的上下文数据的挑战。

Method: 本研究收集了智利圣地亚哥的城市自行车案例研究数据。利用大型语言模型（LLM）处理从记录的视频中提取的图像序列，以获得对环境的语义描述。将这些离散但详细的上下文数据整合到一个混合模型中，将疲劳和唤醒作为影响观察到的骑行行为（从GPS数据推断）的潜在变量。

Result: 该研究证实了骑行决策受到压力相关情绪的影响，并强调了城市特征和交通状况对骑行者行为的强烈影响。

Conclusion: 通过LLM提取上下文数据并将其整合到混合模型中，可以有效地理解城市环境中骑行者情绪状态和行为之间的关系，并识别出影响因素。

Abstract: Measuring emotional states in transportation contexts is an emerging field.
Methods based on self-reported emotions are limited by their low granularity
and their susceptibility to memory bias. In contrast, methods based on
physiological indicators provide continuous data, enabling researchers to
measure changes in emotional states with high detail and accuracy. Not only are
emotions important in the analysis, but understanding what triggers emotional
changes is equally important. Uncontrolled variables such as traffic
conditions, pedestrian interactions, and infrastructure remain a significant
challenge, as they can have a great impact on emotional states. Explaining the
reasons behind these emotional states requires gathering sufficient and proper
contextual data, which can be extremely difficult in real-world environments.
This paper addresses these challenges by applying an innovative approach,
extracting contextual data (expert annotator level) from recorded multimedia
using large language models (LLMs). In this paper, data are collected from an
urban cycling case study of the City of Santiago, Chile. The applied models
focus on understanding how different environments and traffic situations affect
the emotional states and behaviors of the participants using physiological
data. Sequences of images, extracted from the recorded videos, are processed by
LLMs to obtain semantic descriptions of the environment. These discrete,
although dense and detailed, contextual data are integrated into a hybrid
model, where fatigue and arousal serve as latent variables influencing observed
cycling behaviors (inferred from GPS data) like waiting, accelerating, braking,
etc. The study confirms that cycling decisions are influenced by stress-related
emotions and highlights the strong impact of urban characteristics and traffic
conditions on cyclist behavior.

</details>


### [401] [Aerial RIS-Enhanced Communications: Joint UAV Trajectory, Altitude Control, and Phase Shift Design](https://arxiv.org/abs/2510.24731)
*Bin Li,Dongdong Yang,Lei Liu,Dusit Niyato*

Main category: eess.SP

TL;DR: 空中可重构智能面（ARIS）虽有优势但易受无人机姿态影响，本文提出一种基于欧拉角的ARIS控制方案，联合优化高度、轨迹、相位和基站波束，以最大化系统和信率，并使用软Actor-Critic算法和水填充-二分法解决优化问题，数值结果表明该方案性能优于基准。


<details>
  <summary>Details</summary>
Motivation: 空中RIS（ARIS）比地面RIS更灵活且覆盖范围更广，但无人机在飞行中不可避免的倾斜和高度变化会导致严重的波束失配，从而严重影响ARIS的性能。

Method: 提出一种基于欧拉角的ARIS控制方案，利用无人机的动态模型联合优化ARIS的高度和轨迹，并考虑ARIS的能耗、飞行安全和基站的传输功率约束，联合设计ARIS的高度、轨迹、相位以及基站的波束成形，以最大化系统和信率。由于ARIS飞行的连续控制性质以及变量之间的强耦合，将问题表述为马尔可夫决策过程，并采用具有优先经验回放的软Actor-Critic算法来学习有效的ARIS控制策略。基于优化的ARIS配置，进一步采用水填充和二分法有效地确定最优基站波束。

Result: 数值结果表明，所提出的算法在收敛速度和通信性能方面均显著优于基准，系统和信率提高了约14.4%。与固定水平ARIS方案相比，所提出的方案产生的轨迹更具适应性，并显著减轻了ARIS倾斜引起的性能下降。

Conclusion: 所提出的基于欧拉角的ARIS控制方案能够有效解决无人机姿态变化带来的波束失配问题，显著提升了通信性能，并展现了ARIS实际部署的巨大潜力。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a pivotal technology
for enhancing wireless networks. Compared to terrestrial RIS deployed on
building facades, aerial RIS (ARIS) mounted on quadrotor unmanned aerial
vehicle (UAV) offers superior flexibility and extended coverage. However, the
inevitable tilt and altitude variations of a quadrotor UAV during flight may
lead to severe beam misalignment, significantly degrading ARIS's performance.
To address this challenge, we propose a Euler angles-based ARIS control scheme
that jointly optimizes the altitude and trajectory of the ARIS by leveraging
the UAV's dynamic model. Considering the constraints on ARIS flight energy
consumption, flight safety, and the transmission power of a base station (BS),
we jointly design the ARIS's altitude, trajectory, phase shifts, and BS
beamforming to maximize the system sum-rate. Due to the continuous control
nature of ARIS flight and the strong coupling among variables, we formulate the
problem as a Markov decision process and adopt a soft actor-critic algorithm
with prioritized experience replay to learn efficient ARIS control policies.
Based on the optimized ARIS configuration, we further employ the water-filling
and bisection method to efficiently determine the optimal BS beamforming.
Numerical results demonstrate that the proposed algorithm significantly
outperforms benchmarks in both convergence and communication performance,
achieving approximately 14.4\% improvement in sum-rate. Moreover, in comparison
to the fixed-horizontal ARIS scheme, the proposed scheme yields more adaptive
trajectories and significantly mitigates performance degradation caused by ARIS
tilting, demonstrating strong potential for practical ARIS deployment.

</details>


### [402] [Decoding non-invasive brain activity with novel deep-learning approaches](https://arxiv.org/abs/2510.24733)
*Richard Csaky*

Main category: eess.SP

TL;DR: 本论文研究了脑电图（EEG）和脑磁图（MEG）等非侵入性脑电生理信号的建模和解码，重点关注视觉感知和内在语（默想）的脑活动。论文探讨了深度学习在处理被试内和被试间变异性方面的应用，并提出了一种新的Transformer模型，能更准确地生成MEG信号。实验部分收集了大量的内在语数据，但解码结果大部分为阴性，表明内在语解码的挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究内在语和视觉感知过程中大脑的活动，并提高解码这些过程的性能。

Method: 提出并评估了用于脑解码的深度学习方法，包括处理被试间变异性的新方法，以及基于卷积和Transformer的MEG信号预测模型。实验部分收集了大量的内在语EEG、MEG和OPM数据。

Result: Transformer模型在生成MEG信号方面表现优于其他模型，能更准确地模拟真实脑数据。然而，内在语解码的实验结果大部分为阴性。

Conclusion: 深度学习，特别是Transformer模型，在脑信号建模和解码方面具有潜力，但内在语的解码仍然是一个重大挑战。

Abstract: This thesis delves into the world of non-invasive electrophysiological brain
signals like electroencephalography (EEG) and magnetoencephalography (MEG),
focusing on modelling and decoding such data. The research aims to investigate
what happens in the brain when we perceive visual stimuli or engage in covert
speech (inner speech) and enhance the decoding performance of such stimuli. The
thesis is divided into two main sections, methodological and experimental work.
A central concern in both sections is the large variability present in
electrophysiological recordings, whether it be within-subject or
between-subject variability, and to a certain extent between-dataset
variability. In the methodological sections, we explore the potential of deep
learning for brain decoding. We present advancements in decoding visual stimuli
using linear models at the individual subject level. We then explore how deep
learning techniques can be employed for group decoding, introducing new methods
to deal with between-subject variability. Finally, we also explores novel
forecasting models of MEG data based on convolutional and Transformer-based
architectures. In particular, Transformer-based models demonstrate superior
capabilities in generating signals that closely match real brain data, thereby
enhancing the accuracy and reliability of modelling the brain's
electrophysiology. In the experimental section, we present a unique dataset
containing high-trial inner speech EEG, MEG, and preliminary optically pumped
magnetometer (OPM) data. Our aim is to investigate different types of inner
speech and push decoding performance by collecting a high number of trials and
sessions from a few participants. However, the decoding results are found to be
mostly negative, underscoring the difficulty of decoding inner speech.

</details>


### [403] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: Cardi-GPT是一个利用深度学习和自然语言处理技术的专家系统，用于简化心电图（ECG）解读和增强临床沟通，实现了73%的响应质量得分。


<details>
  <summary>Details</summary>
Motivation: 传统的心电图解读和沟通具有挑战性，需要专业知识，Cardi-GPT旨在简化这一过程并提高效率。

Method: 使用16残差块卷积神经网络（CNN）处理12导联心电图数据，并引入模糊化层将模型输出转换为语言类别，结合聊天机器人界面以促进理解和沟通。

Result: 在包含24种心脏病症的数据集上，加权准确率为0.6194。在跨越四个国家六家医院的多样化数据集上进行了评估，并取得了73%的响应质量得分。

Conclusion: Cardi-GPT通过连接复杂的心电图数据解读和可行的临床见解，有望提高诊断准确性、临床工作流程和患者预后，是心血管医疗领域的一项变革性创新。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [404] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: 该研究提出了一种名为StrikeWatch的腕带式设备，能够完全在设备端实时识别跑步步态，主要用于区分足跟着地和前脚掌着地，从而帮助跑步者自行纠正不当跑姿。


<details>
  <summary>Details</summary>
Motivation: 传统的步态分析系统存在体积大、仅支持离线分析等缺点。腕部可穿戴设备虽然更便携，但在嘈杂的IMU信号、有限的计算资源和对云连接的依赖下，实现实时步态识别面临挑战。

Method: 研究提出了四种紧凑的深度学习模型（1D-CNN、1D-SepCNN、LSTM和Transformer），并针对AMD Spartan-7 XC7S15和Lattice iCE40UP5K两款嵌入式FPGA进行了能效优化。通过自定义硬件原型收集了户外跑步数据集，并进行了模型评估。

Result: 研究结果表明，模型复杂度和硬件效率之间存在权衡。在iCE40UP5K FPGA上，经过6位量化的1D-SepCNN模型在12名参与者中取得了0.847的平均F1分数，同时每秒推理能耗仅为0.350 μJ，延迟为0.140 ms。该配置可在320 mAh电池下支持长达13.6天的连续推理。

Conclusion: StrikeWatch作为一种紧凑的腕带式设备，能够实现完全在设备端的实时步态识别，为跑步者提供了实用的解决方案。其中，6位量化的1D-SepCNN模型在能效和性能之间取得了良好的平衡，具有实际应用潜力。

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [405] [Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR](https://arxiv.org/abs/2510.24740)
*Nader Nemati*

Main category: eess.SP

TL;DR: STAR是一种心电图（ECG）节律增强技术，通过在R峰之间进行时间扭曲和幅度缩放来增加数据多样性，同时保持心电图形态的完整性，提高模型在不同设备和数据源上的泛化能力，尤其有助于罕见心律失常的分类。


<details>
  <summary>Details</summary>
Motivation: 临床12导联心电图分类因记录条件多样、病理重叠和标签不平衡而难以泛化，而无约束的数据增强可能破坏诊断关键信息。

Method: 提出了一种称为正弦时间-幅度重采样（STAR）的节律增强方法，该方法在连续R峰之间进行操作，对每个R-R段进行时间扭曲和幅度缩放，同时保持P-QRS-T的顺序不变，并保留信号的首尾部分。

Result: STAR提供了形态保真度、跨源鲁棒性、易于集成到现有模型以及在罕见类别上更好的学习能力，相比于全局裁剪、大偏移或加性噪声等方法，STAR避免了抑制或错位临床标志物的变换。

Conclusion: STAR是一种简单且可控的心电图分类增强方法，适用于对形态保真度、操作简便性和跨源耐用性有要求的场景。

Abstract: Clinical 12-lead ECG classification remains difficult because of diverse
recording conditions, overlapping pathologies, and pronounced label imbalance
hinder generalization, while unconstrained augmentations risk distorting
diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude
Resampling (STAR) is introduced as a beat-wise augmentation that operates
strictly between successive R-peaks to apply controlled time warping and
amplitude scaling to each R--R segment, preserving the canonical P--QRS--T
order and leaving the head and tail of the trace unchanged. STAR is designed
for practical pipelines and offers: (i) morphology-faithful variability that
broadens training diversity without corrupting peaks or intervals; (ii)
source-resilient training, improving stability across devices, sites, and
cohorts without dataset-specific tuning; (iii) model-agnostic integration with
common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on
rare classes via beat-level augmentation, reducing overfitting by resampling
informative beats instead of duplicating whole records. In contrast to global
crops, large shifts, or additive noise, STAR avoids transformations that
suppress or misalign clinical landmarks. A complete Python implementation and a
transparent training workflow are released, aligned with a source-aware,
stratified five-fold protocol over a multi-institutional 12-lead corpus,
thereby facilitating inspection and reuse. Taken together, STAR provides a
simple and controllable augmentation for clinical ECG classification where
trustworthy morphology, operational simplicity, and cross-source durability are
essential.

</details>


### [406] [PulseFi: A Low Cost Robust Machine Learning System for Accurate Cardiopulmonary and Apnea Monitoring Using Channel State Information](https://arxiv.org/abs/2510.24744)
*Pranay Kocheta,Nayan Sanjay Bhatia,Katia Obraczka*

Main category: eess.SP

TL;DR: PulseFi是一种利用Wi-Fi传感和人工智能进行无创生命体征监测的新型低成本系统，可用于监测心率、呼吸频率并检测呼吸暂停事件。


<details>
  <summary>Details</summary>
Motivation: 在各种医疗保健环境中，对无创生命体征监测的需求日益增长。

Method: PulseFi利用低成本的商品设备，通过信号处理管道处理Wi-Fi遥测数据（特别是信道状态信息（CSI）），并将其输入到定制的低计算长短期记忆（LSTM）神经网络模型中。

Result: 在两个数据集上的评估表明，PulseFi能够无缝、无创地有效地估计心率和呼吸频率，其准确性可与甚至优于多天线系统相比。

Conclusion: PulseFi是一种经济高效的解决方案，可用于无创生命体征监测。

Abstract: Non-intrusive monitoring of vital signs has become increasingly important in
a variety of healthcare settings. In this paper, we present PulseFi, a novel
low-cost non-intrusive system that uses Wi-Fi sensing and artificial
intelligence to accurately and continuously monitor heart rate and breathing
rate, as well as detect apnea events. PulseFi operates using low-cost commodity
devices, making it more accessible and cost-effective. It uses a signal
processing pipeline to process Wi-Fi telemetry data, specifically Channel State
Information (CSI), that is fed into a custom low-compute Long Short-Term Memory
(LSTM) neural network model. We evaluate PulseFi using two datasets: one that
we collected locally using ESP32 devices and another that contains recordings
of 118 participants collected using the Raspberry Pi 4B, making the latter the
most comprehensive data set of its kind. Our results show that PulseFi can
effectively estimate heart rate and breathing rate in a seemless non-intrusive
way with comparable or better accuracy than multiple antenna systems that can
be expensive and less accessible.

</details>


### [407] [EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification](https://arxiv.org/abs/2510.24748)
*Dong-Hyeon Kang,Ju-Hyeon Nam,Sang-Chul Lee*

Main category: eess.SP

TL;DR: EcoScale-Net是一种高效的卷积全尺度网络，通过分层设计和瓶颈卷积，在保持全感受野覆盖的同时显著降低了计算成本，在ECG分类任务上取得了SOTA的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的ECG分类器在处理长序列时难以选择合适的感受野尺寸，导致泛化能力不足；而OS CNN虽然解决了感受野覆盖问题，但计算成本过高，限制了模型的深度和宽度。

Method: EcoScale-Net采用分层结构，在每个阶段将最大卷积核长度限制在降采样后所需的尺度，并通过在每个全尺度块前后插入瓶颈卷积来控制通道增长并融合多尺度特征，从而消除冗余。

Result: 与OS CNN相比，EcoScaleNet在CODE 15% ECG数据集上参数量减少了90%，计算量（FLOPs）减少了99%，同时宏平均F1分数提高了2.4%。

Conclusion: EcoScaleNet能够以更低的计算成本实现ECG长序列分类的SOTA准确性，有望在普通硬件上实现实时部署。

Abstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for
early detection of cardiac abnormalities, yet manual reading is error prone and
existing CNN based classifiers struggle to choose receptive field sizes that
generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN)
addresses this by enumerating prime sized kernels inspired by Goldbach
conjecture to cover every scale, but its exhaustive design explodes
computational cost and blocks deeper, wider models. We present Efficient
Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that
retains full receptive field coverage while eliminating redundancy. At each
stage, the maximum kernel length is capped to the scale still required after
down sampling, and bottleneck convolutions inserted before and after every Omni
Scale block curtail channel growth and fuse multi scale features. On the large
scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by
99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These
results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence
ECG classification at a fraction of the computational cost, enabling real time
deployment on commodity hardware. Our EcoScaleNet code is available in GitHub
Link.

</details>


### [408] [Opportunistic Screening of Wolff-Parkinson-White Syndrome using Single-Lead AI-ECG Mobile System: A Real-World Study of over 3.5 million ECG Recordings in China](https://arxiv.org/abs/2510.24750)
*Shun Huang,Deyun Zhang,Sumei Fan,Shijia Geng,Yujie Xiao,Rui Zhang,Zhaoji Fu,Shenda Hong*

Main category: eess.SP

TL;DR: 这项研究表明，单导联AI-ECG系统可以有效地筛查WPW综合征，大大减轻了医生的工作负担，并有助于心血管疾病的预防。


<details>
  <summary>Details</summary>
Motivation: 传统的WPW综合征筛查方法（如电生理测试或12导联心电图）在大规模筛查中成本高且不便。本研究旨在评估先前开发的单导联AI-ECG移动系统在现实世界中用于机会性检测WPW综合征的效率和有效性。

Method: 研究纳入了3,566,626条来自中国87,836名个体、使用NMPA批准的便携式ECG设备（WenXinWuYang）收集的单导联ECG记录。通过心脏病专家标注和随机抽样来验证AI系统的性能，并量化了AI辅助的工作量减少，比较了AI阳性和用户启动工作流的审查效率。

Result: AI系统实现了45.5%的灵敏度和95.9%的特异性。AI阳性结果表明确诊WPW的风险约增加210倍。通过关注AI选择的阳性结果，医生工作量减少了99.5%，平均只需审查12个病例即可确诊1例WPW，而广泛筛查和用户驱动的方法则需要909例和875例。

Conclusion: 这项大规模的现实世界研究证明，单导联AI-ECG系统能够实现高效且实用的WPW综合征机会性筛查，显著减少了医生的工作负担，并支持基于人群的心血管疾病预防。

Abstract: Wolff-Parkinson-White (WPW) syndrome is a congenital cardiac condition
associated with sudden cardiac death, with a prevalence of 0.1-0.3%.
Conventional screening relies on electrophysiological testing or 12-lead
electrocardiography interpreted by cardiologists, which limits large-scale and
cost-effective screening. Building on our previous work developing a
single-lead AI-ECG mobile system for atrial fibrillation screening, this study
evaluates its efficiency and effectiveness for opportunistic detection of WPW
syndrome in real-world settings. This retrospective analysis included 3,566,626
single-lead ECG recordings from 87,836 individuals in China, collected using
the NMPA-approved portable ECG device WenXinWuYang. The AI system performance
was validated using cardiologist annotations and random sampling. We quantified
AI-assisted workload reduction and compared review efficiency across
AI-positive and user-initiated workflows. The AI system achieved 45.5%
sensitivity and 95.9% specificity. A positive AI result indicated about 210
times higher risk of confirmed WPW. Focusing on AI-selected positives reduced
physician workload by 99.5%, requiring only 12 reviews to confirm one WPW case,
compared with 909 and 875 in population-wide and user-driven approaches. In
conclusion, this large-scale real-world study demonstrates that a single-lead
AI-ECG system enables efficient and practical opportunistic screening for WPW
syndrome, significantly reducing physician workload and supporting
population-based cardiovascular prevention.

</details>


### [409] [A Cylindrical Nanowire Array-Based Flexure-FET Receiver for Molecular Communication](https://arxiv.org/abs/2510.24890)
*Dilara Aktas,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 本文提出了一种基于圆柱形纳米线阵列的Flexure-FET分子通信接收器，通过分布式机电耦合和悬门配置，增强了设计的通用性和可扩展性，并开发了分析模型来表征其性能，为未来的分子通信系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 分子通信（MC）在纳米物联网（IoNT）和体内医疗保健系统中至关重要，但需要开发能够实现高能效、可靠、低复杂度且具有生物相容性的先进接收器架构。

Method: 提出了一种基于圆柱形纳米线阵列的Flexure-FET MC接收器，采用悬门配置，并通过分析模型表征其机电响应、噪声行为和信息论性能（如信噪比和信道容量）。

Result: 研究结果表明，几何形状、机电动力学和分子结合过程之间存在很强的相互依赖性，可以对灵敏度、噪声特性和通信容量进行可调控制。

Conclusion: 所提出的圆柱形纳米线阵列Flexure-FET MC接收器具有增强的结构可调性和阵列配置，为未来的混合和空间调制MC系统提供了一个灵活的基础，为IoNT框架内可扩展和多功能的接收器架构铺平了道路。

Abstract: Molecular communication (MC) enables biocompatible and energy-efficient
information transfer through chemical signaling, forming a foundational
paradigm for emerging applications in the Internet of Nano Things (IoNT) and
intrabody healthcare systems. The realization of this vision critically depends
on developing advanced receiver architectures that merge nanoscale
communication and networking techniques with bio-cyber interfaces, ensuring
energy-efficient, reliable, and low-complexity modulation and detection while
maintaining biocompatibility. To address these challenges, the Flexure-FET MC
receiver was introduced as a mechanically transducing design capable of
detecting both charged and neutral molecular species. In this study, we present
a cylindrical nanowire array-based Flexure-FET MC receiver that enhances design
versatility and scalability through distributed electromechanical coupling in a
suspended-gate configuration. The proposed array architecture offers additional
geometric degrees of freedom, including nanowire radius, length, spacing, and
array size, providing a flexible framework that can be tailored to advanced MC
scenarios. An analytical end-to-end model is developed to characterize the
system's electromechanical response, noise behavior, and information-theoretic
performance, including signal-to-noise ratio (SNR) and channel capacity. The
results reveal the strong interdependence between geometry, electromechanical
dynamics, and molecular binding processes, enabling tunable control over
sensitivity, noise characteristics, and communication capacity. The enhanced
structural tunability and array configuration of the proposed design provide a
flexible foundation for future mixture-based and spatially modulated MC
systems, paving the way toward scalable and multifunctional receiver
architectures within the IoNT framework.

</details>


### [410] [Next-Generation MAC Technique for Priority Handling in Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.24928)
*Anwar Ahmed Khan,Farid Nait-Abdesselam,Indrakshi Dey*

Main category: eess.SP

TL;DR: DyFrag-MAC通过动态数据分片实现不同优先级流量的动态、差异化信道访问，优于FROG-MAC和i-DSME。


<details>
  <summary>Details</summary>
Motivation: 工业网络物理系统（CPS）中设备数量的增长需要可靠、及时的网络服务，而现有的NGMA技术支持异构优先级应用的能力有限。

Method: 提出了一种名为动态分片-MAC（DyFrag-MAC）的新型机制，通过分片普通优先级数据来支持紧急优先级数据的早期传输。

Result: 与FROG-MAC和i-DSME相比，DyFrag-MAC在平均延迟和吞吐量方面表现出更好的性能。

Conclusion: DyFrag-MAC通过动态数据分片解决了现有MAC协议在处理异构优先级流量时的延迟问题，为工业CPS提供了更优的网络服务。

Abstract: Next Generation Media Access Control (NGMA) techniques have been designed to
support diverse applications with heterogeneous priorities. In industrial
cyber-physical systems (CPS), the number of connected devices and systems is
expected to grow significantly, demanding dependable and prompt network
services. In this work, we present a novel scheme, Dynamic Fragmentation-MAC
(DyFrag-MAC) that offers dynamic, differentiated channel access to the traffic
of various priorities. DyFrag-MAC works on fragmenting the data of normal
priority in order to support early delivery of urgent priority data. In prior
work, urgent priority data either had to wait for the complete transmission of
lower-priority packets or relied on multi-channel protocols to gain access. We
compared the proposed fragmentation scheme with FROG-MAC and industrial
Deterministic and Synchronous Multi-channel Extension (i-DSME). FROG-MAC
fragmented the lower priority packets, but did not adjust the fragment size
dynamically, whereas i-DSME utilized multiple channels and adaptive contention
mechanisms; both protocols lack the ability to preempt ongoing lower-priority
transmissions. Hence, the performance evaluation in terms of average delay and
throughput reveals better performance of DyFRAG-MAC for the heterogeneous
traffic.

</details>


### [411] [Optimizing Next Generation Wireless BAN with Prioritized Access for Heterogeneous Traffic](https://arxiv.org/abs/2510.24931)
*Shama Sidiqui,Indrakshi Dey*

Main category: eess.SP

TL;DR: ADP2-MAC是一种新颖的基于优先级的MAC协议，通过自适应和动态轮询机制，能够有效地管理WBAN中的异构流量，并优先处理紧急数据包，从而在可靠性、延迟和能源效率方面优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 高效管理具有不同优先级的异构流量对于无线体域网(WBAN)至关重要，直接影响着网络的可靠性、延迟和能源效率，而最小化数据包的生成到接收之间的延迟对于提升WBAN性能和健康结果至关重要，并且需要根据流量优先级进行优化。

Method: 提出了一种名为ADP2-MAC的新型基于优先级的MAC协议，该协议利用概率方法动态确定信道轮询/监听间隔，识别流量到达模式以确定最优轮询间隔，并在预期有紧急数据包时中断低优先级数据传输。

Result: 与支持可变数据速率的MAC协议(MVDR)相比，ADP2-MAC在性能上更优，因为它采用了概率轮询间隔和中断机制来有效处理紧急优先级数据。

Conclusion: ADP2-MAC协议通过其创新的概率轮询间隔和中断机制，在处理WBAN中的异构流量，特别是紧急数据方面，表现出优于MVDR协议的性能。

Abstract: Efficient management of heterogeneous traffic with varying priorities is
critical in Wireless Body Area Networks (WBANs). The priority mechanisms
embedded in Media Access Control (MAC) schemes largely govern the performance
of WBAN in terms of reliability, delay and energy efficiency. Minimizing the
delay between packet generation and reception is critical for enhancing WBAN
performance and associated health outcomes; however, delay optimization must be
tailored to each traffic priority. In this work, we proposed a novel
priority-based MAC protocol, Adaptive and Dynamic Polling MAC for Prioritized
Traffic (ADP2-MAC), designed to support heterogeneous traffic in WBANs. The
protocol utilizes a probabilistic approach to dynamically determine channel
polling/listening intervals. ADP2-MAC not only identifies traffic arrival
patterns to determine optimal polling intervals but also interrupts the
transmission of lower-priority data when urgent packets are expected. The
performance of ADP2-MAC has been compared with the MAC protocol for Variable
Data Rates (MVDR) which supports heterogeneous traffic by assigning different
data rates based on traffic priority. ADP2-MAC outperforms MVDR due to its use
of probabilistic polling intervals and an interruption mechanism designed to
efficiently handle urgent-priority data.

</details>


### [412] [Hybrid Liquid Neural Network-Random Finite Set Filtering for Robust Maneuvering Object Tracking](https://arxiv.org/abs/2510.25020)
*Minti Liu,Qinghua Guo,Cao Zeng,Yanguang Yu,Jun Li,Ming Jin*

Main category: eess.SP

TL;DR: 本论文提出一种结合数据驱动的液体神经网络（LNN）和随机有限集（RFS）的混合方法，用于跟踪机动目标，提高了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以跟踪具有复杂运动模式的机动目标，因为它们依赖于预定义的运动模型。

Method: 将液体神经网络（LNN）集成到随机有限集（RFS）框架中，形成两种LNN-RFS滤波器，直接从数据中学习连续时间动力学，以适应复杂的非线性运动。

Result: 在具有挑战性的机动场景下的仿真结果表明，所提出的混合方法在跟踪精度方面取得了显著的改进。

Conclusion: 所提出的LNN-RFS混合方法能够适应复杂、非线性运动，并能精确跟踪杂波中的高机动目标，同时保留了RFS框架的多目标跟踪优势，提高了灵活性和鲁棒性。

Abstract: This work addresses the problem of tracking maneuvering objects with complex
motion patterns, a task in which conventional methods often struggle due to
their reliance on predefined motion models. We integrate a data-driven liquid
neural network (LNN) into the random finite set (RFS) framework, leading to two
LNN-RFS filters. By learning continuous-time dynamics directly from data, the
LNN enables the filters to adapt to complex, nonlinear motion and achieve
accurate tracking of highly maneuvering objects in clutter. This hybrid
approach preserves the inherent multi-object tracking strengths of the RFS
framework while improving flexibility and robustness. Simulation results on
challenging maneuvering scenarios demonstrate substantial gains of the proposed
hybrid approach in tracking accuracy.

</details>


### [413] [Spectral and Energy Efficiency Tradeoff for Pinching-Antenna Systems](https://arxiv.org/abs/2510.25192)
*Zihao Zhou,Zhaolin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种联合发射和捏合波束成形设计，用于捏合天线系统（PASS）中的频谱效率（SE）和能源效率（EE）的权衡。


<details>
  <summary>Details</summary>
Motivation: 在单用户场景下，证明了最优捏合天线（PA）位置独立于发射波束成形，并基于此提出了一个两阶段联合波束成形设计。在多用户场景下，提出了一种交替优化（AO）算法，在考虑服务质量（QoS）要求的同时，平衡SE-EE性能。

Method: 在单用户场景下，提出了一种迭代闭式优化（ICR）方案来对齐接收信号相位，并提出了PA放置框架，然后推导了最优PA位置给定下的最优发射波束成形器的闭式解。在多用户场景下，使用AO算法进行联合波束成形设计。

Result: 数值结果表明，所提出的算法在快速收敛的同时显著提高了联合SE-EE性能；并且随着PA数量和服务覆盖范围的增加，PASS与传统多天线系统的SE-EE权衡区间差距扩大。

Conclusion: 所提出的联合发射和捏合波束成形设计在PASS系统中能够有效提高SE-EE性能，并具有良好的收敛性。

Abstract: The joint transmit and pinching beamforming design for spectral efficiency
(SE) and energy efficiency (EE) tradeoff in pinching-antenna systems (PASS) is
proposed. Both PASS-enabled single- and multi-user communications are
considered. In the single-user scenario, it is proved that the optimal pinching
antenna (PA) positions are independent of the transmit beamforming. Based on
this insight, a two-stage joint beamforming design is proposed. Specifically,
in the first stage, an iterative closed-form refinement (ICR) scheme is
proposed to align the phases of the received signals, based on which a PA
placement framework is proposed. In the second stage, the closed-form solution
for the optimal transmit beamformer is derived given the optimal PA positions.
In the multi-user scenario, an alternating optimization (AO)-based joint
beamforming design is proposed to balance the SE-EE performance while taking
the quality-of-service (QoS) requirements into account. It is proved that the
proposed AO-based algorithm is guaranteed to converge when no constraints are
violated in PA placement subproblem. Numerical results demonstrate that: 1) the
proposed algorithms significantly improve joint SE-EE performance with fast
convergence speed; 2) the SE-EE tradeoff regime gap between PASS and
conventional multi-antenna system widens as the number of PAs and service
coverage increase.

</details>


### [414] [State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation](https://arxiv.org/abs/2510.25193)
*Qi You,Qinghua Huang,Yi-Cheng Lin*

Main category: eess.SP

TL;DR: FA-Stateformer是一个结合了状态空间模型和自注意力机制的网络，用于在时域和频域处理声学特征，以提高DOA估计的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 在声学特征随时间和频率变化的场景下，准确估计到达方向（DOA）是一个挑战，需要有效整合特征和建模时间依赖性。同时，在时间序列建模中，平衡模型性能和计算效率是一个难题。

Method: FA-Stateformer采用特征聚合模块增强时域和频域信息；引入受Squeeze-and-Excitation启发的轻量级Conformer架构，压缩前馈层以降低冗余；使用时间迁移机制扩大卷积层的感受野；并结合双向Mamba模块和自注意力层，以高效地建模双向时间依赖性。

Result: 实验表明，FA-Stateformer在性能和效率上优于传统网络。

Conclusion: FA-Stateformer通过特征聚合、轻量级Conformer、时间迁移和双向Mamba与自注意力相结合，有效解决了DOA估计中的挑战，并在性能和效率方面取得了优越的成果。

Abstract: Accurate direction-of-arrival (DOA) estimation for sound sources is
challenging due to the continuous changes in acoustic characteristics across
time and frequency. In such scenarios, accurate localization relies on the
ability to aggregate relevant features and model temporal dependencies
effectively. In time series modeling, achieving a balance between model
performance and computational efficiency remains a significant challenge. To
address this, we propose FA-Stateformer, a state space and self-attention
collaborative network with feature aggregation. The proposed network first
employs a feature aggregation module to enhance informative features across
both temporal and spectral dimensions. This is followed by a lightweight
Conformer architecture inspired by the squeeze-and-excitation mechanism, where
the feedforward layers are compressed to reduce redundancy and parameter
overhead. Additionally, a temporal shift mechanism is incorporated to expand
the receptive field of convolutional layers while maintaining a compact kernel
size. To further enhance sequence modeling capabilities, a bidirectional Mamba
module is introduced, enabling efficient state-space-based representation of
temporal dependencies in both forward and backward directions. The remaining
self-attention layers are combined with the Mamba blocks, forming a
collaborative modeling framework that achieves a balance between representation
capacity and computational efficiency. Extensive experiments demonstrate that
FA-Stateformer achieves superior performance and efficiency compared to
conventional architectures.

</details>


### [415] [Cramér-Rao Bound Optimization for Movable Antenna-Empowered Integrated Sensing and Uplink Communication System](https://arxiv.org/abs/2510.25246)
*Yuan Guo,Wen Chen,Qingqing Wu,Yang Liu,Qiong Wu*

Main category: eess.SP

TL;DR: 可移动天线（MA）技术通过优化波束成形、功率分配、接收滤波器和MA位置来提升集成传感与通信（ISAC）系统的性能，显著降低了角度估计的Cramér-Rao下界（CRB），同时保证了通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统的固定位置天线（FPA）ISAC系统未能充分利用空间自由度（DoFs），导致雷达传感和通信性能受限。

Method: 提出了一种有效的迭代算法，利用主要化-最小化（MM）和惩罚-对偶分解（PDD）方法，联合优化有源波束成形、功率分配、接收滤波器和MA位置，以最小化角度估计的CRB，同时保证通信性能，解决了高度非凸问题。

Result: 数值模拟结果表明，所提出的算法能够有效地解决含有分数项和四次项的波束形成配置问题，并实现了显著的性能提升。

Conclusion: 所提出的基于MA的ISAC系统和优化算法在性能和效率方面均优于传统系统。

Abstract: Integrated sensing and communication (ISAC) is a promising solution for the
future sixth-generation (6G) system. However, classical fixed-position antenna
(FPA) ISAC systems fail to fully utilize spatial degrees of freedom (DoFs),
resulting in limited gains for both radar sensing and communication
functionalities. This challenge can be addressed by the emerging novel movable
antenna (MA) technology, which can pursue better channel conditions and improve
sensing and communication performances. In this paper, we aim to minimize the
Cram\'er-Rao bound (CRB) for estimating the target's angle while guaranteeing
communication performance. This involves jointly optimizing active beamforming,
power allocation, receiving filters, and MA position configurations, which is a
highly non-convex problem. To tackle this difficulty, we propose an efficient
iterative solution that analytically optimizes all variables without relying on
numerical solvers, i.e., CVX. Specifically, by leveraging cutting-edge
majorization-minimization (MM) and penalty-dual-decomposition (PDD) methods, we
develop a low-complexity algorithm to solve the beamformer configuration
problem containing the fractional and quartic terms. Numerical simulation
results demonstrate the effectiveness and efficiency of our proposed algorithm,
highlighting significant performance improvements achieved by employing MA in
the ISAC system.

</details>


### [416] [Fair Rate Maximization for Multi-user Multi-cell MISO Communication Systems via Novel Transmissive RIS Transceiver](https://arxiv.org/abs/2510.25290)
*Yuan Guo,Wen Chen,Qingqing Wu,Zhendong Li,Kunlun Wang,Hongying Tang,Jun Li*

Main category: eess.SP

TL;DR: 本篇论文提出了一种利用新型透射式可重构智能表面（RIS）收发器（TRTC）配置的多小区多输入单输出（MISO）下行通信系统。


<details>
  <summary>Details</summary>
Motivation: 在多小区MISO下行通信系统中，TRTC的发射波束成形旨在最大化用户最小速率，但由于目标函数不可微，该问题难以解决。

Method: 利用分数规划（FP）将对数形式的速率函数转换为易处理的形式，然后利用平滑近似理论推导出可微函数来近似最大最小化目标函数。通过应用Majorization-Minimization（MM）技术并检查最优性条件，提出了一种无需数值求解器即可解析更新所有变量的解决方案。

Result: 所提出的低复杂度算法具有良好的收敛性和有效性，并且能够显著降低计算复杂度而不会损失性能。与基准方案相比，TRTC的部署具有明显的优势。

Conclusion: 该研究提出了一种高效的低复杂度算法来解决TRTC配置下的MISO通信系统中的最大最小速率优化问题，并在数值结果中证明了其优越性。

Abstract: This paper explores a multi-cell multiple-input single-output (MISO) downlink
communication system enabled by a unique transmissive reconfigurable
intelligent surface (RIS) transceiver (TRTC) configuration. Within this system
framework, we formulate an optimization problem for the purpose of maximizing
the minimum rate of users for each cell via designing the transmit beamforming
of the TRTC, subject to the power constraints of each TRTC unit. Since the
objective function is non-differentiable, the max-min rate problem is difficult
to solve. In order to tackle this challenging optimization problem, an
efficient low-complexity optimization algorithm is developed. Specifically, the
log-form rate function is transformed into a tractable form by employing the
fractional programming (FP) methodology. Next, the max-min objective function
can be approximated using a differentiable function derived from smooth
approximation theory. Moreover, by applying the majorization-minimization (MM)
technique and examining the optimality conditions, a solution is proposed that
updates all variables analytically without relying on any numerical solvers.
Numerical results are presented to demonstrate the convergence and
effectiveness of the proposed low-complexity algorithm. Additionally, the
algorithm can significantly reduce the computational complexity without
performance loss. Furthermore, the simulation results illustrate the clear
superiority of the deployment of the TRTC over the benchmark schemes.

</details>


### [417] [Millimeter-Wave Radar Sensing of Wombat Respiration](https://arxiv.org/abs/2510.25293)
*Marina Murakami,Ryoko Iwase,Chiemi Iba,Daisuke Ogura,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 该研究证明了基于雷达的非接触式呼吸监测技术在袋熊身上的可行性，并讨论了袋熊的呼吸频率差异和季节性变化，结果支持该方法在袋熊非接触式健康监测中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究袋熊的呼吸频率，以支持其非接触式健康监测。

Method: 使用79 GHz毫米波雷达系统，通过对自相关函数中的谐波分量进行求和来估计呼吸间隔，以捕捉由呼吸引起的身体表面准周期性位移。

Result: 使用两种雷达单元从不同角度进行同步测量，呼吸间隔和呼吸频率的测量误差分别为47.4毫秒（2.44%）和0.81次/分钟（2.21%）。

Conclusion: 基于雷达的非接触式呼吸监测技术在袋熊身上是可行的，并且具有作为一种非接触式健康监测方法的潜力。

Abstract: This study demonstrates the feasibility of radar-based non-contact
respiratory monitoring for wombats. Two measurement experiments were conducted
in June and December 2024 using 79-GHz millimeter-wave radar systems to monitor
the respiration of two wombats. To estimate the respiratory interval, we used a
method based on summing harmonic components in the autocorrelation function,
capturing the quasi-periodic displacement of the body surface caused by
respiration. Estimation accuracy was evaluated through simultaneous
measurements from different angles using two radar units. The respiratory
interval and respiratory rate were measured with errors of 47.4 ms (2.44%) and
0.81 bpm (2.21%), respectively. We also discuss the differences in respiratory
rates between the two wombats, as well as seasonal variations between June and
December. The results support the potential application of this method to
non-contact health monitoring of wombats.

</details>


### [418] [Low-Overhead CSI Prediction via Gaussian Process Regression -- Part~I: Data-Driven Spatial Interpolation](https://arxiv.org/abs/2510.25390)
*Syed Luqman Shah,Nurul Huda Mahmood,Italo Atzeni*

Main category: eess.SP

TL;DR: 通过使用高斯过程回归（GPR）和三种核函数（径向基函数、Matérn 和有理二次函数），可以从少量观测数据中预测出完整的信道状态信息（CSI），从而将导频开销减少高达 50%，同时保持超过 92% 的链路容量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于导频的 CSI 估计算法在多天线系统中面临着过高的开销问题，尤其是在天线数量增加的情况下。

Method: 提出一种基于高斯过程回归（GPR）的新颖框架，利用径向基函数、Matérn 和有理二次函数三种核函数来预测完整的 CSI，仅需少量观测值。

Result: 在克罗内克积和 Weichselberger 信道模型以及三种不同的导频探测方案下，所提出的 GPR 方法在节省 50% 导频的情况下，实现了最低的预测误差、最高的经验 95% 可信区间覆盖率，并能更好地保留互信息。

Conclusion: 所提出的 GPR 方法能够显著减少导频开销（高达 50%），同时保持超过 92% 的链路容量，为多天线系统提供了有效的 CSI 估计算法。

Abstract: Accurate channel state information (CSI) is critical for current and
next-generation multi-antenna systems. Yet conventional pilot-based estimators
incur prohibitive overhead as antenna counts grow. In this paper, we address
this challenge by developing a novel framework based on Gaussian process
regression (GPR) that predicts full CSI from only a few observed entries,
thereby reducing pilot overhead. The correlation between data points in GPR is
defined by the covariance function, known as kernels. In the proposed GPR-based
CSI estimation framework, we incorporate three kernels, i.e., radial basis
function, Mat\'ern, and rational quadratic, to model smooth and multi-scale
spatial correlations derived from the antenna array geometry. The proposed
approach is evaluated across Kronecker and Weichselberger channel models with
three distinct pilot probing schemes. Results show that the proposed GPR with
50% pilot saving achieves the lowest prediction error, the highest empirical
95% credible-interval coverage, and the best preservation of mutual information
relative to benchmarks. This enables up to 50% pilot reduction while preserving
over 92% of the link capacity.

</details>


### [419] [Model-Free Robust Beamforming in Satellite Downlink using Reinforcement Learning](https://arxiv.org/abs/2510.25393)
*Alea Schröder,Steffen Gracla,Carsten Bockelmann,Dirk Wübben,Armin Dekorsy*

Main category: eess.SP

TL;DR: 卫星通信在6G网络中至关重要，但频率资源有限，需要进行频率复用。本文提出使用强化学习（特别是Soft Actor-Critic算法）来解决下行链路卫星波束成形中的鲁棒预编码问题，以应对不完美的信道状态信息，并在各种场景下与分析基线方法相比，在和速率性能上表现优异或相当。


<details>
  <summary>Details</summary>
Motivation: 随着卫星星座的日益密集和传输资源的有限性，频率复用在管理用户间干扰方面发挥着越来越重要的作用。在多用户下行链路中，预编码使得空间分离的用户能够进行频率复用，从而大大提高频谱效率。然而，在实际应用中，例如信道状态信息过时或位置估计错误的情况下，预编码的性能会迅速下降。推导不完美的信道状态信息下的鲁棒预编码在分析上通常是困难的，并且需要对优化问题进行大量放宽或采用启发式约束。因此，有必要开发新的方法来解决这个问题。

Method: 本文提出了一种利用强化学习（特别是Soft Actor-Critic算法）从给定数据中灵活推导鲁棒预编码算法的方法。将该算法应用于下行链路卫星波束成形问题，并通过数值模拟验证其适应性。

Result: 所提出的学习算法在和速率性能上匹配或显著优于两个分析基线方法，并且能够适应不同程度的鲁棒性要求。实验涵盖了单卫星和多卫星协作波束成形、全局或局部信道状态信息以及两种不同程度不确定性的误差模型。此外，还分析了学习算法所利用的实现鲁棒性的机制。

Conclusion: 强化学习方法能够有效地解决下行链路卫星波束成形中的鲁棒预编码问题，即使在信道状态信息不完美的情况下也能达到优异的性能。该实现是公开可用的，可用于进一步的研究和结果复现。

Abstract: Satellite-based communications are expected to be a substantial future market
in 6G networks. As satellite constellations grow denser and transmission
resources remain limited, frequency reuse plays an increasingly important role
in managing inter-user interference. In the multi-user downlink, precoding
enables the reuse of frequencies across spatially separated users, greatly
improving spectral efficiency. The analytical calculation of suitable
precodings for perfect channel information is well studied, however, their
performance can quickly deteriorate when faced with, e.g., outdated channel
state information or, as is particularly relevant for satellite channels, when
position estimates are erroneous. Deriving robust precoders under imperfect
channel state information is not only analytically intractable in general but
often requires substantial relaxations of the optimization problem or heuristic
constraints to obtain feasible solutions. Instead, in this paper we flexibly
derive robust precoding algorithms from given data using reinforcement
learning. We describe how we adapt the applied Soft Actor-Critic learning
algorithm to the problem of downlink satellite beamforming and show numerically
that the resulting precoding algorithm adjusts to all investigated scenarios.
The considered scenarios cover both single satellite and cooperative
multi-satellite beamforming, using either global or local channel state
information, and two error models that represent increasing levels of
uncertainty. We show that the learned algorithms match or markedly outperform
two analytical baselines in sum rate performance, adapting to the required
level of robustness. We also analyze the mechanisms that the learned algorithms
leverage to achieve robustness. The implementation is publicly available for
use and reproduction of the results.

</details>


### [420] [Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems](https://arxiv.org/abs/2510.25416)
*Jiaming Cheng,Wei Chen,Bo Ai*

Main category: eess.SP

TL;DR: 提出一种自适应端到端（E2E）收发器架构，用于无导频和无循环前缀（CP）的AI原生无线通信系统，通过AI驱动的星座整形和联合训练的神经接收器，并引入轻量级信道适配器（CA）模块以提高鲁棒性、支持多调制阶数以及解决高峰均功率比（PAPR）问题，仿真结果表明其在比特误率（BER）、吞吐量和信道适应性方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统正交频分复用（OFDM）系统依赖导频和循环前缀（CP），导致开销大、频谱效率低。下一代（NextG）AI原生无线通信系统需要更自适应、高效的智能空口。

Method: 提出一种自适应端到端（E2E）收发器架构，结合AI驱动的星座整形和联合训练的神经接收器。引入轻量级信道适配器（CA）模块以快速适应信道变化。提出一个支持多调制阶数的统一模型。通过约束E2E训练解决高PAPR问题。

Result: 仿真结果表明，所提出的框架在比特误率（BER）、吞吐量和跨不同信道场景的鲁棒性方面均优于现有技术，PAPR满足目标要求，CA模块更新参数量少。

Conclusion: 所提出的AI原生NextG无线通信框架，通过无导频、无CP的E2E收发器设计、CA模块和PAPR约束训练，能够显著提高系统性能和适应性，为未来无线通信提供了有潜力的解决方案。

Abstract: The advent of artificial intelligence (AI)-native wireless communication is
fundamentally reshaping the design paradigm of next-generation (NextG) systems,
where intelligent air interfaces are expected to operate adaptively and
efficiently in highly dynamic environments. Conventional orthogonal frequency
division multiplexing (OFDM) systems rely heavily on pilots and the cyclic
prefix (CP), resulting in significant overhead and reduced spectral efficiency.
To address these limitations, we propose an adaptive end-to-end (E2E)
transceiver architecture tailored for pilot-free and CP-free wireless systems.
The architecture combines AI-driven constellation shaping and a neural receiver
through joint training. To enhance robustness against mismatched or
time-varying channel conditions, we introduce a lightweight channel adapter
(CA) module, which enables rapid adaptation with minimal computational overhead
by updating only the CA parameters. Additionally, we present a framework that
is scalable to multiple modulation orders within a unified model, significantly
reducing model storage requirements. Moreover, to tackle the high
peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained
E2E training, achieving compliance with PAPR targets without additional
transmission overhead. Extensive simulations demonstrate that the proposed
framework delivers superior bit error rate (BER), throughput, and resilience
across diverse channel scenarios, highlighting its potential for AI-native
NextG.

</details>


### [421] [Learning-Based Blockage-Resilient Beam Training in Near-Field Terahertz Communications](https://arxiv.org/abs/2510.25433)
*Caihao Weng,Yuqing Guo,Bowen Zhao,Ying Wang,Wen Chen,Zhendong Li*

Main category: eess.SP

TL;DR: 本论文提出了一种基于自加速艾里光束的抗阻挡近场波束训练方法，以解决太赫兹通信中的阻挡问题。


<details>
  <summary>Details</summary>
Motivation: 太赫兹频段具有极高带宽，是6G通信的潜在选择，但高频损耗导致阻挡问题严重，尤其是在障碍物多的室内近场通信中。本研究旨在解决此问题。

Method: 提出一种基于自加速艾里光束的抗阻挡近场波束训练方法。首先，使用离散傅里叶变换（DFT）码本分析艾里光束在障碍物存在下的轨迹和接收端波束模式，发现波束模式能编码接收端位置和收发端与障碍物的空间关系。基于此，将任务构建为多任务学习问题，并提出一种基于注意力的轻量级多参数波束训练网络（AMPBT-Net），以联合预测最佳艾里光束的角度、距离和曲率参数。

Result: 仿真结果表明，艾里光束能有效缓解阻挡效应，并且所提出的方案在训练开销显著降低的情况下，达到了与穷举波束扫描相当的性能。

Conclusion: 自加速艾里光束能够沿弯曲轨迹传播以绕过障碍物，提出的AMPBT-Net网络能够有效进行波束训练，从而解决太赫兹近场通信中的阻挡问题。

Abstract: Terahertz (THz) band is considered a promising candidate to meet the
high-throughput requirement for future sixth-generation (6G) wireless
communications due to its ultrawide bandwidth. However, due to the high
penetration loss at high-frequencies, blockage becomes a serious problem in THz
communications, especially in near-field indoor communications with numerous
obstacles. To address this issue, this paper investigates blockage-resilient
near-field beam training based on self-accelerating Airy beam, which can
propagate along a curved trajectory to circumvent obstacles. Specifically, we
first analyze the trajectory of the Airy beam and the beam pattern at the
receiver using a discrete Fourier transform (DFT) codebook in the presence of
obstacles. Interestingly, we reveal that the beam pattern not only captures the
receiver's location information but also implicitly encodes the spatial
relationship between the receiver and obstacle, which facilitates identifying
the optimal Airy beam configuration. Based on this insight, we formulate the
blockage-resilient beam training task as a multitask learning problem and
propose a lightweight attention-based multi-parameter beam training network
(AMPBT-Net) to jointly predict the angle, distance, and curvature parameters of
the optimal Airy beam based on the beam pattern. Finally, simulation results
demonstrate that the Airy beam effectively mitigates blockage effects and the
proposed scheme achieves comparable performance to exhaustive beam sweeping
while significantly reducing training overhead.

</details>


### [422] [Echo-Conditioned Denoising Diffusion Probabilistic Models for Multi-Target Tracking in RF Sensing](https://arxiv.org/abs/2510.25464)
*Amirhossein Azarbahram,Onel L. A. López*

Main category: eess.SP

TL;DR: 该论文提出了一种条件性去噪扩散概率模型（C-DDPM）辅助框架，用于动态射频传感系统中的多目标时空跟踪。


<details>
  <summary>Details</summary>
Motivation: 旨在解决动态射频传感系统中多目标的时空跟踪问题。

Method: 提出C-DDPM辅助框架，结合变分自编码器（VAE）进行回波压缩，并利用无分类器指导来增强条件去噪。VAE将接收到的回波编码为潜在表示，以指导DDPM预测目标状态，进而用于码本波束选择。

Result: 仿真结果表明，该方法在角度和距离跟踪方面的估计误差显著低于经典信号处理、滤波和深度学习基准。

Conclusion: C-DDPM辅助框架在集成感知和通信领域展现了生成模型的潜力，能显著降低跟踪误差。

Abstract: In this paper, we consider a dynamic radio frequency sensing system aiming to
spatially track multiple targets over time. We develop a conditional denoising
diffusion probabilistic model (C-DDPM)-assisted framework that learns the
temporal evolution of target parameters by leveraging the noisy echo
observations as conditioning features. The proposed framework integrates a
variational autoencoder (VAE) for echo compression and utilizes classifier-free
guidance to enhance conditional denoising. In each transmission block, VAE
encodes the received echo into a latent representation that conditions DDPM to
predict future target states, which are then used for codebook beam selection.
Simulation results show that the proposed approach outperforms classical signal
processing, filtering, and deep learning benchmarks. The C-DDPM-assisted
framework achieves significantly lower estimation errors in both angle and
distance tracking, demonstrating the potential of generative models for
integrated sensing and communications.

</details>


### [423] [Adaptive Channel Estimation and Quantized Feedback for RIS Assisted Optical Wireless Communication Systems](https://arxiv.org/abs/2510.25467)
*Muhammad Khalil,Ke Wang,Jinho Choi*

Main category: eess.SP

TL;DR: 该论文提出了一个统一的建模、估计和反馈框架，用于可重构智能表面（RIS）辅助的光无线链路。通过长曝光像素增益来扩展经典的衍射极限响应，该增益能够统计平均角向抖动和错位，并能精确地以实积分形式捕获视轴衰减和渐进的旁瓣填充。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在为RIS辅助的光无线链路提供一个统一的框架，以解决建模、估计和反馈问题。

Method: 该方法结合了自由空间路径损耗、比尔-朗伯大气衰减、像素级衍图和光学效率，并使用单一导频最小二乘信道估计和量化相位反馈。

Result: 在N=64像素、导频长度M=2N、导频信噪比=20 dB的条件下，归一化均方误差为0.005，有效信噪比损失约为0.5，容量损失为0.007比特-秒。六位相位量化在这些操作点上没有引入可测量的额外损失。训练开销与像素几何形状强相关：将像素宽度减半（像素面积减四分之一）会将维持相同NMSE所需的导频长度增加约四倍。

Conclusion: 该框架协调了物理光学建模与估计和反馈设计，为RIS辅助光网络中可扩展的链路预算提供了一个原则性的基础。

Abstract: This paper presents a unified modeling, estimation, and feedback framework
for reconfigurable intelligent surface RIS-assisted optical wireless links. The
key modeling element is a long-exposure pixel gain that extends the classical
diffraction-limited response by statistically averaging angular jitter and
mispointing; it admits an exact real-integral form and captures boresight
attenuation and progressive sidelobe filling. The end-to-end system couples
free-space path loss, Beer--Lambert atmospheric extinction, pixel-level
diffraction, and optical efficiency with a unitary-pilot least-squares channel
estimator and quantized phase feedback. Analysis closely matches Monte Carlo
simulations and yields concrete design rules: with a surface of N=64 pixels,
pilot length $M=2N$, and pilot SNR=20 dB, the normalized mean-squared error
is0.005, implying an effective-SNR loss of about 0.5 and a capacity penalty of
0.007bits-s. Six-bit phase quantization introduces no measurable additional
penalty at these operating points, setting a practical benchmark for feedback
resolution. Training overhead scales strongly with pixel geometry: halving
pixel width (quartering pixel area) increases the pilot length required to
maintain the same NMSE by roughly fourfold. The framework reconciles
physical-optics modeling with estimation-and-feedback design and provides a
principled basis for scalable link budgeting in RIS-assisted optical networks.

</details>


### [424] [Dynamic Beamforming and Power Allocation in ISAC via Deep Reinforcement Learning](https://arxiv.org/abs/2510.25496)
*Duc Nguyen Dao,André B. J. Kokkeler,Haibin Zhang,Yang Miao*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度强化学习（DRL）的动态波束赋形和功率分配方法，以解决6G ISAC系统中的资源分配挑战，并在动态环境中实现了高效的实时适应。


<details>
  <summary>Details</summary>
Motivation: ISAC系统在6G网络中至关重要，但动态环境下的资源分配计算复杂且需要实时适应，这是ISAC面临的主要挑战之一。

Method: 提出了一种基于DRL的方法，通过智能体与环境交互，利用试错学习和预设奖励来学习最优的波束赋形和功率分配策略。

Result: DRL方法在2000个训练轮次内收敛，频谱效率达到半定松弛（SDR）基准的80%。与SDR方法相比，DRL决策时间仅为20毫秒，而SDR方法需要4500毫秒。与深度Q网络（DQN）基准相比，DRL方法在可比的运行时间内实现了30%的更高总和速率。

Conclusion: DRL技术在动态ISAC场景下具有实现实时、高性能应用的潜力。

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler in 6G networks,
where sensing and communication capabilities are designed to complement and
enhance each other. One of the main challenges in ISAC lies in resource
allocation, which becomes computationally demanding in dynamic environments
requiring real-time adaptation. In this paper, we propose a Deep Reinforcement
Learning (DRL)-based approach for dynamic beamforming and power allocation in
ISAC systems. The DRL agent interacts with the environment and learns optimal
strategies through trial and error, guided by predefined rewards. Simulation
results show that the DRL-based solution converges within 2000 episodes and
achieves up to 80\% of the spectral efficiency of a semidefinite relaxation
(SDR) benchmark. More importantly, it offers a significant improvement in
runtime performance, achieving decision times of around 20 ms compared to 4500
ms for the SDR method. Furthermore, compared with a Deep Q-Network (DQN)
benchmark employing discrete beamforming, the proposed approach achieves
approximately 30\% higher sum-rate with comparable runtime. These results
highlight the potential of DRL for enabling real-time, high-performance ISAC in
dynamic scenarios.

</details>


### [425] [Quickest Change Point Detection with Measurements over a Lossy Link](https://arxiv.org/abs/2510.25604)
*Krishna Chaythanya KV,Saqib Abbas Baba,Anurag Kumar,Arpan Chattopadhyay,Rajesh Sundaresan*

Main category: eess.SP

TL;DR: 工业4.0背景下，研究了传感器通过有损无线链路传输数据时，最快变化检测（QCD）问题。提出了基于CUSUM算法的解决方案，并证明了其渐近最优性。该算法考虑了数据丢失、队列管理和多传感器异构性等因素，并通过数值分析展示了优化系统设计的权衡。


<details>
  <summary>Details</summary>
Motivation: 受工业4.0应用的启发，解决传感器通过有损无线链路传输数据时的最快变化检测（QCD）问题。

Method: 提出了一种CUSUM算法，并通过定义合适的马尔可夫过程将问题转化为马尔可夫过程中的QCD问题。在不完备数据和多传感器异构的情况下，提出了改进的CUSUM算法和传感器调度算法。

Result: 证明了所提出算法的渐近最优性，并提出了一种最小化检测延迟的传感器调度算法，该算法平衡了观测年龄和信息内容。

Conclusion: 所提出的基于CUSUM的算法在工业4.0背景下，能够有效地解决传感器通过有损无线链路传输数据时的QCD问题，并且在考虑数据丢失、队列管理和多传感器异构等复杂情况下，仍能保证最优性或接近最优性。

Abstract: Motivated by Industry 4.0 applications, we consider quickest change detection
(QCD) of an abrupt change in a process when its measurements are transmitted by
a sensor over a lossy wireless link to a decision maker (DM). The sensor node
samples measurements using a Bernoulli sampling process, and places the
measurement samples in the transmit queue of its transmitter. The transmitter
uses a retransmit-until-success transmission strategy to deliver packets to the
DM over the lossy link, in which the packet losses are modeled as a Bernoulli
process, with different loss probabilities before and after the change. We pose
the QCD problem in the non-Bayesian setting under Lorden's framework, and
propose a CUSUM algorithm. By defining a suitable Markov process, involving the
DM measurements and the queue length process, we show that the problem reduces
to QCD in a Markov process. Characterizing the information measure per
measurement sample at the DM, we establish the asymptotic optimality of our
algorithm when the false alarm rate tends to zero. Further, when the DM
receives incomplete data due to channel loss, we present asymptotically optimal
QCD algorithms by suitably modifying the CUSUM algorithm. We then explore the
last-come-first-served (LCFS) queuing discipline at the sensor transmit queue
to lower detection delay in the non-asymptotic case. Next, we consider the case
of multiple sensors, each with its own wireless transmitter queue, and show
that our analysis extends to the case of multiple homogeneous sensors. When the
sensors are heterogeneous, we present a sensor scheduling algorithm that
minimizes detection delay by balancing the trade-off between the age of the
observations and their information content. Numerical analysis demonstrate
trade-offs that can be used to optimize system design parameters in the
non-asymptotic regime.

</details>


### [426] [Continuous subsurface property retrieval from sparse radar observations using physics informed neural networks](https://arxiv.org/abs/2510.25648)
*Ishfaq Aziz,Mohamad Alipour*

Main category: eess.SP

TL;DR: 本文提出了一个基于物理信息机器学习的框架，用于在深度上连续地重构地下介电特性，并同时满足测量数据和麦克斯韦方程组。


<details>
  <summary>Details</summary>
Motivation: 传统的波反演方法在处理连续变化的地下介电特性时存在局限性，需要密集的测量数据或对材料边界有强先验知识，限制了其在现实场景中的可扩展性和准确性。

Method: 提出一个物理信息机器学习框架，将地下介电常数重构为深度的全神经网络连续函数，并通过满足测量数据和麦克斯韦方程组进行训练。

Result: 该框架在模拟和定制的雷达实验中得到了验证，结果与原位介电常数测量值高度一致（R^2=0.93），并能检测到细微的变化（Δeps_r=2）。参数分析表明，即使只有三个传感器，也能在双层系统中恢复准确的介电常数剖面。

Conclusion: 该方法将地下反演从依赖边界的方法转变为连续介电特性估计，能够准确表征平滑的介电常数变化，并利用低成本雷达系统推进电磁成像技术。

Abstract: Estimating subsurface dielectric properties is essential for applications
ranging from environmental surveys of soils to nondestructive evaluation of
concrete in infrastructure. Conventional wave inversion methods typically
assume few discrete homogeneous layers and require dense measurements or strong
prior knowledge of material boundaries, limiting scalability and accuracy in
realistic settings where properties vary continuously. We present a physics
informed machine learning framework that reconstructs subsurface permittivity
as a fully neural, continuous function of depth, trained to satisfy both
measurement data and Maxwells equations. We validate the framework with both
simulations and custom built radar experiments on multilayered natural
materials. Results show close agreement with in-situ permittivity measurements
(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).
Parametric analysis reveals that accurate profiles can be recovered with as few
as three strategically placed sensors in two layer systems. This approach
reframes subsurface inversion from boundary-driven to continuous property
estimation, enabling accurate characterization of smooth permittivity
variations and advancing electromagnetic imaging using low cost radar systems.

</details>


### [427] [PyDPF: A Python Package for Differentiable Particle Filtering](https://arxiv.org/abs/2510.25693)
*John-Joseph Brady,Benjamin Cox,Víctor Elvira,Yunpeng Li*

Main category: eess.SP

TL;DR: 本文提出并实现了一个基于PyTorch的统一API，用于多种可微分粒子滤波器（DPFs），便于研究人员使用和比较，并展示了DPFs在状态空间模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 标准粒子滤波器（PF）在状态空间模型（SSMs）中用于估计隐藏状态，但其不可微的特性限制了梯度优化方法的应用。可微分粒子滤波器（DPFs）通过修改重采样步骤解决了这一问题，但现有实现分散，不易于研究者使用和比较。

Method: 实现了一个基于PyTorch的统一API，集成了多种可微分粒子滤波器（DPFs），并复现了相关研究中的实验。

Result: 成功复现了现有研究中的实验，证明了该框架的可行性和易用性，并展示了DPFs在解决状态空间模型挑战方面的应用潜力。

Conclusion: 本文实现的统一API简化了DPFs的使用和比较，为将DPFs应用于实际问题提供了便利，并推动了其在状态空间模型分析中的应用。

Abstract: State-space models (SSMs) are a widely used tool in time series analysis. In
the complex systems that arise from real-world data, it is common to employ
particle filtering (PF), an efficient Monte Carlo method for estimating the
hidden state corresponding to a sequence of observations. Applying particle
filtering requires specifying both the parametric form and the parameters of
the system, which are often unknown and must be estimated. Gradient-based
optimisation techniques cannot be applied directly to standard particle
filters, as the filters themselves are not differentiable. However, several
recently proposed methods modify the resampling step to make particle filtering
differentiable. In this paper, we present an implementation of several such
differentiable particle filters (DPFs) with a unified API built on the popular
PyTorch framework. Our implementation makes these algorithms easily accessible
to a broader research community and facilitates straightforward comparison
between them. We validate our framework by reproducing experiments from several
existing studies and demonstrate how DPFs can be applied to address several
common challenges with state space modelling.

</details>


### [428] [Low Probability of Detection Communication Using Noncoherent Grassmannian Signaling](https://arxiv.org/abs/2510.25751)
*Diego Cuevas,Mikel Gutiérrez,Jesús Ibáñez,Ignacio Santamaria*

Main category: eess.SP

TL;DR: 本文提出了一种基于DSSS和Grassmannian信号的非相干低截获概率（LPD）通信系统，该系统利用Grassmannian星座的类噪声分布特性来增强隐蔽性。仿真结果表明，在低信噪比下，与相干QPSK或QAM方案相比，Grassmannian信号具有可比的误比特率（BER），并且在非预期接收者处的截获概率更低，无需导频进行信道估计。研究结果表明，非相干Grassmannian信号在LPD通信中具有实际应用价值和安全优势，可提高隐蔽性和通信性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高通信系统的隐蔽性，降低被非预期接收者侦测到的概率，并克服现有相干通信系统（如QPSK、QAM）在低信噪比下需要导频进行信道估计的限制。

Method: 提出并仿真了一种基于直接序列扩频（DSSS）和Grassmannian信号的非相干低截获概率（LPD）通信系统。Grassmannian星座因其类噪声分布特性被用于增强隐蔽性。

Result: 仿真结果显示，与相干方案（QPSK、QAM）相比，所提出的非相干Grassmannian信号在低信噪比下具有可比的误比特率（BER），并且对于非预期接收者而言，其低截获概率（LPD）性能更优。该方案无需导频即可进行通信。

Conclusion: 非相干Grassmannian信号在低截获概率（LPD）通信中具有实际应用价值和安全优势，其优势在于能够提供更好的隐蔽性和在低信噪比下的性能。

Abstract: This paper proposes a noncoherent low probability of detection (LPD)
communication system based on direct sequence spread spectrum (DSSS) and
Grassmannian signaling. Grassmannian constellations enhance covertness because
they tend to follow a noise-like distribution. Simulations showed that
Grassmannian signaling provides competitive bit error rates (BER) at low
signal-to-noise ratio (SNR) regimes with low probability of detection at the
unintended receiver compared to coherent schemes that use QPSK or QAM
modulation formats and need pilots to perform channel estimation. The results
suggest the practicality and security benefits of noncoherent Grassmannian
signaling for LPD communications due to their improved covertness and
performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [429] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree 是一个创新的框架，首次将 WMO FM-301 标准扩展到个体雷达体积扫描之外，创建了时间解析、可用于分析的档案。它解决了现有雷达数据碎片化、不符合 FAIR 原则以及难以进行大规模研究和云计算的问题。该框架使用 xarray DataTree 构建，将雷达体积扫描组织成层次化、富含元数据的结构，并序列化为 Zarr 格式，以实现可扩展分析。结合 Icechunk 进行 ACID 存储和版本控制，它能够对数千个雷达扫描进行高效并行计算，只需最少的预处理。实验表明，该框架在 QVP 和降水累积工作流程方面显著提高了性能，并将所有工具和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据虽然具有极高的科学价值，但由于其结构化程度低，并未得到充分利用。现有的雷达数据档案存在碎片化、厂商特定和不符合 FAIR 原则（可查找、可访问、可互操作、可重用）等问题，这阻碍了大规模研究、可复现性以及云原生计算。

Method: Radar DataTree 提出了一个可扩展的开源架构，该架构将操作雷达档案转换为符合 FAIR 原则且针对云进行了优化的数据集。它基于 FM-301/CfRadial 2.1 标准，并使用 xarray DataTree 实现。该框架将雷达体积扫描组织成层次化、富含元数据的结构，并将其序列化为 Zarr 格式以支持可扩展分析。此外，它还结合了 Icechunk 来实现 ACID 兼容的存储和版本控制。

Result: 通过 QVP 和降水累积工作流程的案例研究，证明了该框架在性能上的显著提升。所有工具和数据集均通过 Raw2Zarr 存储库公开。

Conclusion: 这项工作为雷达数据管理、高性能地球科学研究以及面向人工智能的天气基础设施奠定了可复现且可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [430] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出了一种多分辨率模型融合（MRMF）方法，通过结合在低分辨率数据上训练的模型并用高分辨率数据进行精炼，以减少神经网络的训练时间，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 在科学研究中，神经网络训练耗时，尤其是在处理大型高维数据集时，需要高效的训练方法来降低计算成本。

Method: 提出了一种多分辨率模型融合（MRMF）方法，该方法结合了在降低分辨率数据上训练的模型，然后用原始分辨率数据进行精炼。

Result: 该方法通过在每个融合阶段加速模型收敛，然后在原始分辨率下进行微调，从而减少了训练时间。实验结果表明，与原始分辨率训练相比，CosmoFlow 和 Neuron Inverter 的训练时间分别提高了 47% 和 44%，而模型精度未受影响。

Conclusion: MRMF 方法能够显著减少端到端的训练时间，同时保持模型精度，适用于需要高效训练的科学应用。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [431] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: LLMs常使用MoE技术，但GPU集群的通信开销阻碍了其发展。WSC芯片提供高带宽但网络拓扑受限。本文提出ER-Mapping来平衡通信压力，并提出NI-Balancer来隐藏专家迁移开销，最终在WSC平台上实现了比SOTA更高的MoE性能。


<details>
  <summary>Details</summary>
Motivation: GPU集群的高通信开销阻碍了LLMs中MoE技术的发展，而WSC芯片虽然有潜力但面临拓扑和专家迁移的挑战。

Method: 提出ER-Mapping（Entwined Ring Mapping）来优化通信，并提出NI-Balancer来隐藏专家迁移开销。

Result: ER-Mapping通信减少高达62%。NI-Balancer使MoE计算和通信分别提高54%和22%。WSC平台比NVL72超节点平均提高39%的每设备MoE性能。

Conclusion: ER-Mapping和NI-Balancer的结合充分发挥了WSC芯片在托管MoE模型方面的潜力，显著提高了性能。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [432] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [433] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 本篇论文研究了在分布式移动系统中，多个移动代理（agent）在同一节点汇合并完成任务的“汇聚”问题。与以往多关注时间复杂度不同，本文探讨了在代理可能拥有相同标签（label）的情况下，实现确定性汇聚的可能性，并给出了完整的可汇聚团队特征，设计了一种能在多项式时间内完成汇聚的算法，并证明了其近乎最优性。


<details>
  <summary>Details</summary>
Motivation: 研究在移动代理可能共享相同标签的情况下，实现确定性汇聚的可能性，并探讨所需的最少公共知识。

Method: 1. 提出可汇聚团队的完整特征。 2. 设计了一种能在多项式时间内（poly(n, logλ)）完成汇聚的算法，该算法所需的代理初始共享知识量（O(log log log μ)）接近最优。 3. 证明了该算法在获得多项式时间复杂度方面所需的公共知识量近乎最优。

Result: 1. 提出了可汇聚团队的完整数学表述。 2. 设计了一种高效的汇聚算法，其时间复杂度为多项式级别，并且所需的共享知识量极小。 3. 证明了该算法在时间复杂度上的近乎最优性。

Conclusion: 本文完整地刻画了可汇聚的代理团队，并提出了一种高效且对公共知识需求极低的确定性汇聚算法。研究结果表明，即使代理拥有相同的标签，也可以在多项式时间内实现确定性汇聚，并且所提出的算法在时间复杂度和所需公共知识量方面都接近最优。此外，本文的技术也为解决其他分布式系统中的终止检测问题提供了新的思路。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [434] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: Big data makes workloads complex, needing efficient scheduling on large distributed systems, especially for data-intensive apps with parallelism, data locality, QoS, and energy efficiency. This paper classifies workloads and reviews scheduling approaches, discussing novel strategies and future challenges.


<details>
  <summary>Details</summary>
Motivation: The explosive growth of big data and increasingly complex, computationally demanding workloads processed on large-scale distributed systems necessitate effective scheduling techniques to handle varying degrees of parallelism, data locality requirements, Quality of Service (QoS) demands (like time constraints and fault tolerance), and energy efficiency objectives.

Method: The paper proposes a classification of data-intensive workloads and provides an overview of common scheduling approaches for large-scale distributed systems. It also discusses novel strategies found in the literature.

Result: The paper reviews existing scheduling approaches and highlights novel strategies for data-intensive workloads in distributed systems.

Conclusion: Effective scheduling techniques are crucial for handling complex, data-intensive workloads on large-scale distributed systems, considering factors like parallelism, data locality, QoS, and energy efficiency. The paper offers a classification, reviews current methods, and points to future research directions.

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [435] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming是一个用于全局聚合的exactly-once流处理系统，它使用Windowed CRDTs来解决可扩展性、延迟和故障恢复问题。


<details>
  <summary>Details</summary>
Motivation: 扩展全局聚合是exactly-once流处理系统的挑战，现有系统因静态聚合树或单任务实例而受到限制，导致可扩展性差、延迟高和故障恢复慢。

Method: Holon Streaming使用一种确定性编程模型，该模型利用了新颖的Windowed CRDTs抽象来实现共享的、可复用的状态。这种方法提高了全局聚合的可扩展性，并支持通过去中心化协调实现高效的故障恢复。

Result: 与现有的流处理系统相比，Holon Streaming在全局聚合工作负载下实现了5倍的低延迟和2倍的高吞吐量。在故障场景下，延迟降低了11倍。

Conclusion: 研究证明了去中心化协调与确定性的有效性，以及Windowed CRDTs在全局聚合中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [436] [Magneto-optical spectroscopy based on pump-probe strobe light](https://arxiv.org/abs/2510.24964)
*Shihao Zhou,Yujie Zhu,Chunli Tang,Rui Sun,Junming Wu,Yuzan Xiong,Ingrid E. Russell,Yi Li,Dali Sun,Frank Tsui,Binbin Yang,Valentine Novosad,Jia-Mian Hu,Wencan Jin,Wei Zhang*

Main category: cond-mat.mes-hall

TL;DR: We developed a pump-probe strobe light spectroscopy technique for sensitive detection of magneto-optical dynamics in hybrid magnonics, using combinatorial microwave-optical pulses and achieving high-energy resolution and detection efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to demonstrate a novel pump-probe strobe light spectroscopy technique for sensitive detection of magneto-optical dynamics in hybrid magnonics.

Method: The technique employs a combinatorial microwave-optical pump-probe scheme, utilizing variable microwave and optical pulse widths and their relative time delays to detect magnetization dynamics in Y3Fe5O12 films. This contrasts with conventional stroboscopy by using pulsed rather than continuous-wave light.

Result: Magneto-optical detection of magnetization dynamics in Y3Fe5O12 films was achieved. The signals were found to be dependent on the characteristics and relative timing of the microwave and optical pulses. High sensitivity and coherent stroboscopic character were maintained even with short pulse widths (1.5 ns microwave, 80 ps optical) and a 7 MHz clock rate.

Conclusion: The study demonstrates that time-dependent strobe light measurement of magnetization dynamics is feasible in the gigahertz frequency range using the developed pump-probe detection scheme.

Abstract: We demonstrate a pump-probe strobe light spectroscopy for sensitive detection
of magneto-optical dynamics in the context of hybrid magnonics. The technique
uses a combinatorial microwave-optical pump-probe scheme, leveraging both the
high-energy resolution of microwaves and the high-efficiency detection using
optical photons. In contrast to conventional stroboscopy using a
continuous-wave light, we apply microwave and optical pulses with varying pulse
widths, and demonstrate magnetooptical detection of magnetization dynamics in
Y3Fe5O12 films. The detected magneto-optical signals strongly depend on the
characteristics of both the microwave and the optical pulses as well as their
relative time delays. We show that good magneto-optical sensitivity and
coherent stroboscopic character are maintained even at a microwave pump pulse
of 1.5 ns and an optical probe pulse of 80 ps, under a 7 megahertz clock rate,
corresponding to a pump-probe footprint of ~1% in one detection cycle. Our
results show that time-dependent strobe light measurement of magnetization
dynamics can be achieved in the gigahertz frequency range under a pump-probe
detection scheme.

</details>


### [437] [Optical excitations and disorder in two-dimensional topological insulators](https://arxiv.org/abs/2510.25009)
*Alejandro José Uría-Álvarez*

Main category: cond-mat.mes-hall

TL;DR: Topological insulators, particularly Bismuth compounds, are explored for their fundamental properties and potential in photovoltaic applications, while also examining the impact of structural disorder.


<details>
  <summary>Details</summary>
Motivation: The discovery of topological phases has highlighted the importance of quantum geometry in electronic band structures and revealed materials with unusual properties relevant for applications. This thesis focuses on topological insulators, the first discovered topological phase.

Method: The thesis explores different models of topological insulators, with a focus on Bismuth compounds. It evaluates their potential for photovoltaic applications and analyzes the effect of structural disorder on their properties.

Result: The research investigates the fundamental and technological aspects of topological insulators, including their viability for photovoltaic applications and the impact of structural disorder.

Conclusion: The thesis contributes to the understanding of topological insulators, covering both fundamental concepts and practical applications, particularly concerning Bismuth compounds and the effects of disorder.

Abstract: Topological phases of matter have garnered significant interest over the past
two decades for two main reasons: their identification, via topological
invariants, relies on the quantum geometry of the Bloch states, bringing
attention to an aspect of electronic band structure overlooked up to their
discovery. Secondly, these classes of materials present electronic states with
unusual properties, leading to exotic phenomena and making them relevant for
potential applications. In this thesis we explore both fundamental and
technological aspects of the first discovered topological phase: the
topological insulator. To this end, we consider different models of topological
insulators with a particular emphasis on Bismuth compounds, evaluating their
viability for photovoltaic applications, and separately, the impact of
structural disorder on their properties.

</details>


### [438] [Single-Shot All-Optical Switching in CoFeB/MgO Magnetic Tunnel Junctions](https://arxiv.org/abs/2510.25102)
*Junta Igarashi,Sébastien Geiskopf,Takanobu Shinoda,Butsurin Jinnai,Yann Le Guen,Julius Hohlfeld,Shunsuke Fukami,Hideo Ohno,Jon Gorchon,Stéphane Mangin,Michel Hehn,Grégory Malinowski*

Main category: cond-mat.mes-hall

TL;DR: 我们演示了在不含稀土的 CoFeB/MgO 磁性隧道结 (MTJs) 中进行单次光学切换 (AOS)，该材料系统广泛应用于自旋转移矩磁性随机存取存储器 (STT MRAM)。


<details>
  <summary>Details</summary>
Motivation: 通过调整覆盖层厚度，我们展示了精确的热控制能够实现从平行 (P) 到反平行 (AP) 状态的确定性磁化反转。

Method: 我们通过隧道磁阻 (TMR) 效应在微型 MTJ 器件中检测到磁化反转。

Result: 该工作代表了将 AOS 与 MTJ 技术集成的重要一步。

Conclusion: 我们的发现表明，超快自旋输运或偶极相互作用或两者兼有在切换过程中可能起着至关重要的作用。

Abstract: We demonstrate single shot al optical switching (AOS) in rare earth free
CoFeB/MgO magnetic tunnel junctions (MTJs), a material system widely adopted in
spin transfer torque magnetic random access memory (STT MRAM). By tuning the
capping layer thickness, we show that precise heat control enables
deterministic magnetization reversal from parallel (P) to antiparallel (AP)
state. Furthermore, we detect magnetization reversal in a micro scale MTJ
device via the tunnel magnetoresistance (TMR) effect. Our findings suggest that
ultrafast spin transport or dipolar interactions or a combination of both may
play essential roles in the switching process. This work represents a
significant step toward integrating AOS with MTJ technology.

</details>


### [439] [Strain Engineering of Correlated Charge-Ordered Phases in 1T-TaS2](https://arxiv.org/abs/2510.25265)
*Rafael Luque Merino,Felix Carrascoso,Eudomar Henríquez-Guerra,M. Reyes Calvo,Riccardo Frisenda,Andres Castellanos-Gomez*

Main category: cond-mat.mes-hall

TL;DR: 应变工程可以调控二维材料的性质，特别是具有电荷密度波（CDW）的材料。本研究利用柔性平台对1T-TaS2施加单轴应变，研究了近整数量子（NC）到非整数量子（IC）的CDW相变行为。实验发现，开关阈值电压和NC-CDW相的电阻具有明显的、可逆的应变依赖性。应变引起的电阻变化与阈值电压之间存在二次关系，表明压阻效应调控了相变的应变可调性。该研究展示了一种室温下可读出的、具有阈值响应的应变和位移传感器，并强调了1T-TaS2在片上传感领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究应变工程对1T-TaS2材料中电荷密度波（CDW）相变行为的影响，并探索其在传感领域的应用。

Method: 利用柔性平台对1T-TaS2施加单轴拉伸和压缩应变，并通过电学输运测量研究应变对其近整数量子（NC）到非整数量子（IC）的CDW相变行为的影响。

Result: 开关阈值电压和NC-CDW相的电阻显示出清晰、可逆的应变依赖性。应变引起的电阻变化与阈值电压之间存在二次关系，证实了压阻效应在调控相变的应变可调性中的作用。成功展示了一个室温下可电学读出的、具有阈值响应的应变和位移传感器。

Conclusion: 1T-TaS2材料的CDW相变行为可以通过应变工程进行调控，并且其压阻效应使其在片上应变和位移传感领域具有巨大的应用潜力。

Abstract: Strain engineering is a powerful strategy for controlling the structural and
electronic properties of two-dimensional materials, particularly in systems
hosting charge density wave (CDW) order. In this work, we apply uniaxial
tensile and compressive strain to thin 1T-TaS2 flakes using a flexible,
device-compatible platform, and systematically investigate the strain-dependent
behavior of the nearly commensurate (NC) to incommensurate (IC) CDW phase
transition. This transition is driven by Joule heating at room temperature.
Electrical transport measurements reveal that both the switching threshold
voltage and the resistance of the NC-CDW phase exhibit clear, reversible strain
dependence. Furthermore, we identify a quadratic dependence between the
strain-induced resistance change and the threshold voltage, confirming that
piezoresistive modulation governs the strain-tunability of the phase
transition. We demonstrate a room-temperature, electrically-readout strain and
displacement sensor with threshold-like response in a programmable window.
These results highlight the potential of 1T-TaS2 for on-chip sensing of strain
and displacement.

</details>


### [440] [Stability of Planar Slits in Multilayer Graphite Crystals](https://arxiv.org/abs/2510.25399)
*Alexander V. Savin,Artem P. Klinov*

Main category: cond-mat.mes-hall

TL;DR: 石墨烯覆盖的狭缝的稳定性和临界宽度随层数和狭缝宽度而变化。


<details>
  <summary>Details</summary>
Motivation: 研究石墨烯多层覆盖对平面狭缝稳定性的影响。

Method: 使用二维粗粒化链模型模拟石墨烯覆盖的狭缝。

Result: 确定了开放狭缝形成的临界宽度 L_o，并分析了其随石墨烯层数 K 的变化关系。发现 L_o 随 K 单调增加。对于单层石墨烯，L_o 有有限临界值；对于多层石墨烯，L_o 随 K 呈幂函数增长。研究了狭缝宽度对开放和闭合状态的影响，以及热振荡对狭缝稳定性的影响。

Conclusion: 开放狭缝的形成和稳定性受到石墨烯覆盖层数、狭缝宽度和温度的显著影响。

Abstract: Using a two-dimensional coarse-grained chain model, planar slits in
multilayer graphite crystals are simulated. It is shown that when covering a
linear cavity on the flat surface of a graphite crystal with a multilayer
graphene sheet, an open (unfilled slit) can form only if the cavity width does
not exceed a critical value L_o (for width L>L_o, only a closed state of the
slit is formed, with the cavity space filled by the covering sheet). The
critical width of the open slit L_o increases monotonically with the number of
layers K in the covering sheet. For a single-layer cavity, there is a finite
critical value of its width L_o<3nm, while for two- and three-layer cavities,
the maximum width of the open slit increases infinitely with increasing K as a
power function K^\alpha with exponent 0<\alpha<1. Inside the crystal, two- and
three-layer slits can have stable open states at any width. For a slit with
width L>7.6nm, a stationary closed state is also possible, in which its lower
and upper surfaces adhere to each other. Simulation of thermal oscillations
showed that open states of two-layer slits with width L<15nm are always stable
against thermal oscillations, while wider slits at T>400K transition from the
open to the closed state. Open states of three-layer slits are always stable
against thermal oscillations.

</details>


### [441] [Effects of interlayer Dzyaloshinskii-Moriya interaction on the shape and dynamics of magnetic twin-skyrmions](https://arxiv.org/abs/2510.25415)
*Tim Matthies,Levente Rózsa,Roland Wiesendanger,Elena Y. Vedmedenko*

Main category: cond-mat.mes-hall

TL;DR: Interlayer DMI affects magnetic skyrmion properties and manipulation by spin-polarized currents.


<details>
  <summary>Details</summary>
Motivation: Magnetic skyrmions are promising for information storage due to their stability and manipulability by spin currents. This paper investigates the influence of interlayer Dzyaloshinskii-Moriya interaction (IL-DMI) on twin-skyrmions in magnetic bilayers.

Method: The study analyzes how IL-DMI influences the spin configuration of twin-skyrmions and observes changes in skyrmion velocity and the skyrmion Hall angle when driven by spin-polarized currents in a current-perpendicular-to-plane configuration.

Result: The spin configuration of twin-skyrmions adapts to the IL-DMI direction by elongating or changing helicities. Significant changes in skyrmion velocity or skyrmion Hall angle were observed depending on current polarization.

Conclusion: IL-DMI offers further prospects for manipulating magnetic skyrmions, highlighting its importance for information storage applications.

Abstract: Magnetic skyrmions have been proposed as promising candidates for storing
information due to their high stability and easy manipulation by spin-polarized
currents. Here, we study how these properties are influenced by the interlayer
Dzyaloshinskii--Moriya interaction (IL-DMI), which stabilizes twin-skyrmions in
magnetic bilayers. We find that the spin configuration of the twin-skyrmion
adapts to the direction of the IL-DMI by elongating or changing the helicities
in the two layers. Driving the skyrmions by spin-polarized currents in the
current-perpendicular-to-plane configuration, we observe significant changes
either in the skyrmion velocity or in the skyrmion Hall angle depending on the
current polarization. These findings unravel further prospects for skyrmion
manipulation enabled by the IL-DMI.

</details>


### [442] [A Topological Sum Rule for the Chirality of Carbon Nanotubes](https://arxiv.org/abs/2510.25425)
*Zhengrong Guo*

Main category: cond-mat.mes-hall

TL;DR: 碳纳米管的手性由核化帽的拓扑结构决定，而不是生长动力学。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏直接连接碳纳米管核化帽结构与其手性的预测理论，阻碍了特定手性的可控合成。

Method: 推导了一个量化这种关系的通用拓扑和规则，确定手性由帽中六个五边形坐标的总和决定。

Result: 该规则确定了(n,m)手性由帽中六个五边形坐标的总和唯一确定。还解释了(126)纳米管的优先形成，因为它具有与<111>催化剂晶面外延匹配的六重对称性帽。

Conclusion: 手性是在成核时确定性地编码的，为可预测的合成铺平了道路。

Abstract: The electronic properties of carbon nanotubes are fundamentally governed by
their chirality, specified by the integer indices (n,m). The lack of a
predictive theory directly connecting the architecture of the nucleation cap to
the resulting (n,m) has hindered the controlled synthesis of specific
chiralities. Here, we derive a universal topological sum rule that quantifies
this relationship. We show that (n,m) is uniquely determined by the sum of the
coordinates of the six pentagons in the cap. The complete set of these
coordinates, evaluated across rotationally symmetric lattice frames, fully
defines the carbon cap structure. This rule establishes that chirality is
encoded deterministically at nucleation, as any perturbation to the pentagon
positions overwhelmingly leads to a predictable, quanti able, and entropically
irreversible shift in (n,m). We further explain the preferential formation of
the (126) nanotube by identifying a six-fold symmetric cap with epitaxial
matching to the <111> catalyst facet. Our work provides a theoretical framework
that redefines the field, shifting the focus from growth kinetics to
deterministic nucleation programming and paving the way toward predictable
synthesis.

</details>


### [443] [Strongly nonlinear Bernstein modes in graphene reveal plasmon-enhanced near-field magnetoabsorption](https://arxiv.org/abs/2510.25443)
*I. Yahniuk,I. A. Dmitriev,A. L. Shilov,E. Mönch,M. Marocko,J. Eroms,D. Weiss,P. Sadovyi,B. Sadovyi,I. Grzegory,W. Knap,J. Gumenjuk-Sichevska,J. Wunderlich,D. A. Bandurin,S. D. Ganichev*

Main category: cond-mat.mes-hall

TL;DR: Graphene中的非线性伯恩斯坦模式在太赫兹激发和近场增强下被观测到，表现出比回旋共振更低的饱和强度，并证实了近场的起源。


<details>
  <summary>Details</summary>
Motivation: 研究如何实现非线性伯恩斯坦模式的实验观测，因为它们能提供对非局域电动力学的直接访问，并可能展现出类似腔量子电动力学的强耦合现象。

Method: 使用嵌入金属触点的近场增强进行太赫兹激发，并通过光电阻谱学进行观测。同时进行了偏振分辨测量以区分近场和远场效应。

Result: 观测到在Bc/2和Bc/3处的尖锐共振，其饱和强度比回旋共振低一个数量级。偏振测量显示伯恩斯坦共振对圆偏振不敏感，但对线偏振角度敏感，与回旋共振行为形成对比。

Conclusion: 石墨烯可以作为非线性磁等离激元学的平台，为强场操纵电子动力学、非平衡电子输运以及腔量子电动力学的固态模拟提供了新的机会。

Abstract: Bernstein modes -- hybrid magnetoplasmon excitations arising from the
coupling between cyclotron motion and collective oscillations in
two-dimensional electron systems -- offer direct access to non-local
electrodynamics. These modes can exhibit rich nonlinear behavior akin to
strong-coupling phenomena in cavity quantum electrodynamics, but reaching
nonlinear regime has remained experimentally challenging. Here we report the
observation of nonlinear Bernstein modes in graphene using terahertz excitation
with near-field enhancement from embedded metallic contacts. Photoresistance
spectroscopy reveals sharp resonances at Bc/2 and Bc/3 that saturate at
radiation intensities nearly an order of magnitude lower than the cyclotron
resonance. We ascribe this to strong local heating of the electron gas due to
resonant excitation of high-amplitude Bernstein magnetoplasmons, associated
with a combination of the field-concentration effect of the near field and
plasmonic amplification that is resonantly enhanced in the region of Bernstein
gaps. Polarization-resolved measurements further confirm the near-field origin:
Bernstein resonances are insensitive to circular helicity but strongly depend
on the angle of linear polarization, in sharp contrast to the cyclotron
resonance response. Our results establish graphene as a platform for nonlinear
magnetoplasmonics, opening opportunities for strong-field manipulation of
collective electron dynamics, out-of-equilibrium electron transport, and
solid-state analogues of cavity quantum electrodynamics.

</details>


### [444] [Strongly enhanced lifetime of higher-order bimerons and antibimerons](https://arxiv.org/abs/2510.25478)
*Shiwei Zhu,Moritz A. Goerzen,Changsheng Song,Stefan Heinze,Dongzhe Li*

Main category: cond-mat.mes-hall

TL;DR: 磁畴壁的磁矩拓扑结构


<details>
  <summary>Details</summary>
Motivation: 探索高Q值磁畴壁的稳定性，并与高Q值skyrmion进行比较。

Method: 使用Fe3GeTe2/Cr2Ge2Te6范德华界面进行计算，并研究了不同温度下的寿命。

Result: 高Q值磁畴壁的寿命比低Q值磁畴壁长3个数量级，并且在室温下也比高Q值skyrmion更稳定，这归因于它们不同的磁纹理对称性。

Conclusion: 高Q值磁畴壁在高Q值skyrmion之上表现出优越的稳定性和更长的寿命，尤其是在室温下，这为未来的磁存储应用提供了新的可能性。

Abstract: Magnetic bimerons, similar to skyrmions, are topologically nontrivial spin
textures characterized by topological charge $Q$. Most studies so far have
focused on low-$Q$ solitons ($|Q| \leq 1$), such as skyrmions, bimerons, and
vortices. Here, we present the first calculations of the lifetimes of high-$Q$
bimerons and demonstrate that they are fundamentally more stable than high-$Q$
skyrmions over a wide range of temperature. To obtain realistic results, our
chosen system is an experimentally feasible van der Waals interface,
Fe$_3$GeTe$_2$/Cr$_2$Ge$_2$Te$_6$. We show that the lifetimes of high-$Q$
(anti)bimerons can exceed the lifetime of those with $|Q|=1$ by 3 orders of
magnitude. Remarkably, this trend remains valid even when extrapolated to room
temperature (RT), as the lifetimes are dominated by entropy rather than energy
barriers. This contrasts with high-$Q$ skyrmions, whose lifetimes fall with
$|Q|$ near RT. We attribute this fundamental difference between skyrmions and
bimerons to their distinct magnetic texture symmetries, which lead to different
entropy-dominated lifetimes.

</details>


### [445] [Coupling between vibration and Luttinger liquid in mechanical nanowires](https://arxiv.org/abs/2510.25608)
*Zeyu Rao,Yue-Xin Huang,Guang-Can Guo,Ming Gong*

Main category: cond-mat.mes-hall

TL;DR: 纳米线振动通过光压与光子耦合，通过电容与电荷耦合。本文研究了振动与Luttinger液体的耦合，其中横向振动比纵向振动更显著。在低温下，当频率变为负值时，横向振动可能出现不稳定性，这可以通过测量电导率的时间依赖性振荡来验证。


<details>
  <summary>Details</summary>
Motivation: 已有的纳米线振动系统虽然与光子和电荷有耦合，但由于其电中性，与其他自由度的耦合一直具有挑战性。本文旨在探索一种新的耦合机制，即通过振动引起的纳米线长度和费米速度变化，实现振动与Luttinger液体的耦合。

Method: 考虑了纳米线的横向和纵向振动，并分析了振动引起的长度和费米速度变化如何导致与Luttinger液体的耦合。通过理论分析预测了在特定条件下（低温、低费米能量、横向振动）可能出现的不稳定现象，并提出了通过测量电导率时间依赖性振荡来验证该耦合的方法。

Result: 研究表明，横向振动比纵向振动更显著，可以通过频率移动来测量。理论预测了在低温和低费米能量下，横向振动可能出现不稳定性，导致频率变为负值。电导率的时间依赖性振荡可以用来测量Luttinger参数，为该耦合提供证据。

Conclusion: 本文提出了一种新的机制，将纳米线振动与Luttinger液体（电子激发）耦合。这种耦合可能在利用电流冷却和控制机械振荡器方面具有潜在的应用前景。

Abstract: The vibration of the mechanical nanowire coupled to photons via photon
pressure and coupled to charges via the capacity has been widely explored in
experiments in the past decades. This system is electrically neutral, thus its
coupling to the other degrees of freedom is always challenging. Here, we show
that the vibration can slightly change the nanowire length and the associated
Fermi velocity, which leads to coupling between vibration and Luttinger liquid.
We consider the transverse and longitudinal vibrations of the nanowires,
showing that the transverse vibration is much more significant than the
longitudinal vibration, which can be measured through the sizable frequency
shift. We predict an instability of the vibration induced by this coupling when
the frequency becomes negative at a critical temperature for the transverse
vibrations in nanowires with low Fermi energy, which can be reached by tuning
the chemical potential and magnetic field. The time-dependent oscillation of
the conductance, which directly measures the Luttinger parameter, can provide
evidence for this coupling. Our theory offers a new mechanism for exploring the
coupling between the vibration and the electronic excitations, which may lead
to intriguing applications in cooling and controlling the mechanical
oscillators with currents.

</details>


### [446] [Spin Seebeck Effect in Correlated Antiferromagnetic V2O3](https://arxiv.org/abs/2510.25637)
*Renjie Luo,Tanner J. Legvold,Gage Eichman,Henry Navarro,Ali C. Basaran,Erbin Qiu,Ivan K. Schuller,Douglas Natelson*

Main category: cond-mat.mes-hall

TL;DR: 文章研究了反铁磁V2O3薄膜中的纵向自旋塞贝克效应 (LSSE)，发现在低温下LSSE随磁场增大而增强直至饱和，并且LSSE信号随薄膜厚度增加而减小，表明体自旋塞贝克效应占主导地位，且V2O3中的磁声子能量弛豫长度小于50nm。


<details>
  <summary>Details</summary>
Motivation: 利用自旋塞贝克效应研究磁绝缘体中的自旋关联和磁序。

Method: 研究反铁磁V2O3薄膜中的纵向自сэн贝克效应 (LSSE)，并分析其在外加磁场、温度和薄膜厚度下的响应与依赖关系。

Result: 在低温下，LSSE响应随外部磁场增加而增加直至饱和。LSSE响应在给定功率和场下表现出非单调的温度依赖性，存在一个峰值，且该峰值随场增强向更高温度移动。LSSE信号的幅度随厚度增加而减小，表明体自旋塞贝克效应占主导。

Conclusion: LSSE信号与厚度的负相关性表明，V2O3中的磁声子能量弛豫长度小于50nm，这与该材料中强的自旋-晶格耦合一致。

Abstract: The spin Seebeck effect is useful for probing the spin correlations and
magnetic order in magnetic insulators. Here, we report a strong longitudinal
spin Seebeck effect (LSSE) in antiferromagnetic V2O3 thin films. The LSSE
response at cryogenic temperatures increases as a function of the external
magnetic field until it approaches saturation. The response at given power and
field exhibits a non-monotonic temperature dependence, with a pronounced peak
that shifts toward higher temperatures as the field increases. Furthermore, the
magnitude of the LSSE signal decreases consistently with increasing thickness,
implying that the bulk SSE dominates any interfacial contribution. This
negative correlation between the SSE and the thickness implies that the magnon
energy relaxation length in V2O3 is shorter than the thickness of our thinnest
film, 50 nm, consistent with the strong spin-lattice coupling in this material.

</details>


### [447] [Using Crossed Andreev Reflection to Split Electrons](https://arxiv.org/abs/2510.25641)
*Austin Marga,Venkat Chandrasekhar*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mesoscopic systems possess shot noise in their currents due to the
quantization of the conducting quasiparticles. Measurements of this shot noise
are useful to study phenomena that do not manifest themselves in standard
conductance or resistance measurements, such as the statistics of the
conducting quasiparticles or quantum entanglement via Bell tests. The
corresponding particle statistics can be determined via two particle quantum
interference experiments, such as the Hong-Ou-Mandel effect which demonstrates
a bunching effect for bosons or an anti-bunching effect in fermions. In
superconducting proximity junctions, electrons incident on a superconductor can
induce holes via crossed Andreev reflection (CAR) in spatially separated normal
metal leads, where the resulting hole currents have nontrivial partition noise
due to the four terminal configuration. These nonlocally generated currents,
using a superconductor as a mesoscopic beam splitter, enable fabrication of
mesoscopic analogs to quantum optics interferometers using metallic and
superconducting films with multiport geometries.

</details>


### [448] [Optical Gain Through Metallic Electro-Optical Effects](https://arxiv.org/abs/2510.25659)
*N. Roldan-Levchenko,D. J. P. de Sousa,C. O. Ascencio,J. D. S Forte,L. Martin-Moreno,T. Low*

Main category: cond-mat.mes-hall

TL;DR: 在没有受激发射的情况下，通过研究贝里曲率偶极子和非零磁电张量的存在，在低对称性二维金属系统中寻找共振横电模式，可能导致光学增益。


<details>
  <summary>Details</summary>
Motivation: 在没有受激发射的情况下，寻找可能导致光学增益的光学增益过程。

Method: 通过使用玻尔兹曼非平衡输运理论模拟光学电导率，并使用散射波形式主义模拟散射问题。

Result: 在同时存在贝里曲率偶极子和非零磁电张量的条件下，可以实现共振横电模式。

Conclusion: 需要进一步研究以寻找具有较大贝里曲率偶极子和磁电张量的材料，以实现该研究的预测。

Abstract: Optical gain is a critical process in today's semiconductor technology and it
is most often achieved via stimulated emission. In this theoretical study, we
find a resonant TE mode in biased low-symmetry two-dimensional metallic systems
which may lead to optical gain in the absence of stimulated emission. We do so
by first modeling the optical conductivity using Boltzmann non-equilibrium
transport theory and then simulating the scattering problem using a
scattered-wave formalism. Assuming that the system may possess a Berry
curvature dipole (BCD) and a non-zero Magnetoelectric tensor (MET), we find
that the optical conductivity has a non-trivial dependence on the direction of
the applied bias, which allows for probing the TE mode. After analyzing the
system with one of each of the effects, we find that the resonant TE mode is
only accessible when both effects are present. Further studies are necessary to
find materials with a suitably large BCD and MET, in order to realize the
predictions within this study.

</details>


### [449] [Fast high-fidelity baseband reset of a latched state for quantum dot qubit readout](https://arxiv.org/abs/2510.25703)
*Piotr Marciniec,M. A. Wolfe,Tyler Kovach,J. Reily,Sanghyeok Park,Jared Benson,Mark Friesen,Benjamin D. Woods,Matthew J. Curry,Nathaniel C. Bishop,J. Corrigan,M. A. Eriksson*

Main category: cond-mat.mes-hall

TL;DR: 我们演示了一种新的量子点量子比特重置方法，可以将重置时间缩短 50 倍以上，同时保持高保真度 (>99%)。


<details>
  <summary>Details</summary>
Motivation: 被动量子比特重置时间长，不适合需要快速重置的应用。

Method: 通过在量子点稳定性图上施加单次基带电压脉冲，将量子比特从读取状态快速重置到基态。

Result: 重置保真度超过 99%，重置速度比被动重置快 50 倍以上。

Conclusion: 所提出的方法可以有效地缩短量子比特的重置时间，同时保持高保真度，为需要快速重置的量子计算应用提供了解决方案。

Abstract: A common method for reading out the state of a spin qubit is by latching one
logical qubit state, either $|1\rangle$ or $|0\rangle$, onto a different,
metastable charge state. Such a latched state can provide a superior charge
sensing signal for qubit readout, and it can have a lifetime chosen to be long
enough that the charge sensed readout can be high fidelity. However, the
passive reset out of latched states is inherently long, which is not desirable.
In this work, we demonstrate an on-demand, high fidelity (> 99%)
re-initialization of a quantum dot qubit out of a latched readout state. The
method is simple to apply as it involves a single baseband voltage pulse to a
specific region in the quantum dot stability diagram where the relaxation time
from the latched state to the ground state is over 50 times faster. We describe
the mechanism for the reset process as well as the boundaries for the optimal
reset region in the qubit gate voltage space.

</details>
