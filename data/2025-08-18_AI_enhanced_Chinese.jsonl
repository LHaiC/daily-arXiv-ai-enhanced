{"id": "2508.11294", "categories": ["cs.MA", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11294", "abs": "https://arxiv.org/abs/2508.11294", "authors": ["Qiangong Zhou", "Zhiting Wang", "Mingyou Yao", "Zongyang Liu"], "title": "Allen: Rethinking MAS Design through Step-Level Policy Autonomy", "comment": null, "summary": "We introduce a new Multi-Agent System (MAS) - Allen, designed to address two\ncore challenges in current MAS design: (1) improve system's policy autonomy,\nempowering agents to dynamically adapt their behavioral strategies, and (2)\nachieving the trade-off between collaborative efficiency, task supervision, and\nhuman oversight in complex network topologies.\n  Our core insight is to redefine the basic execution unit in the MAS, allowing\nagents to autonomously form different patterns by combining these units. We\nhave constructed a four-tier state architecture (Task, Stage, Agent, Step) to\nconstrain system behavior from both task-oriented and execution-oriented\nperspectives. This achieves a unification of topological optimization and\ncontrollable progress.\n  Allen grants unprecedented Policy Autonomy, while making a trade-off for the\ncontrollability of the collaborative structure. The project code has been open\nsource at: https://github.com/motern88/Allen", "AI": {"tldr": "Allen\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\uff0c\u5b83\u901a\u8fc7\u5141\u8bb8\u4ee3\u7406\u81ea\u4e3b\u5730\u5f62\u6210\u4e0d\u540c\u7684\u6a21\u5f0f\u6765\u63d0\u9ad8\u7b56\u7565\u81ea\u4e3b\u6027\uff0c\u5e76\u5728\u590d\u6742\u7684\u7f51\u7edc\u62d3\u6251\u4e2d\u5e73\u8861\u534f\u4f5c\u6548\u7387\u3001\u4efb\u52a1\u76d1\u7763\u548c\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5f53\u524dMAS\u8bbe\u8ba1\u7684\u4e24\u5927\u6838\u5fc3\u6311\u6218\uff1a\u63d0\u9ad8\u7cfb\u7edf\u7684\u7b56\u7565\u81ea\u4e3b\u6027\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u9002\u5e94\u5176\u884c\u4e3a\u7b56\u7565\uff1b\u4ee5\u53ca\u5728\u590d\u6742\u7684\u7f51\u7edc\u62d3\u6251\u4e2d\u5b9e\u73b0\u534f\u4f5c\u6548\u7387\u3001\u4efb\u52a1\u76d1\u7763\u548c\u4eba\u5de5\u76d1\u7763\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49MAS\u7684\u57fa\u672c\u6267\u884c\u5355\u5143\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u5730\u7ec4\u5408\u8fd9\u4e9b\u5355\u5143\u4ee5\u5f62\u6210\u4e0d\u540c\u7684\u6a21\u5f0f\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u56db\u5c42\u72b6\u6001\u67b6\u6784\uff08\u4efb\u52a1\u3001\u9636\u6bb5\u3001\u4ee3\u7406\u3001\u6b65\u9aa4\uff09\u6765\u4ece\u9762\u5411\u4efb\u52a1\u548c\u9762\u5411\u6267\u884c\u7684\u89d2\u5ea6\u7ea6\u675f\u7cfb\u7edf\u884c\u4e3a\u3002", "result": "Allen\u7cfb\u7edf\u5728\u63d0\u9ad8\u7b56\u7565\u81ea\u4e3b\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u534f\u4f5c\u7ed3\u6784\u7684\u53ef\u63a7\u6027\u6743\u8861\u3002", "conclusion": "Allen\u7cfb\u7edf\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u7b56\u7565\u81ea\u4e3b\u6027\uff0c\u5e76\u5bf9\u534f\u4f5c\u7ed3\u6784\u7684\u53ef\u63a7\u6027\u8fdb\u884c\u4e86\u6743\u8861\u3002"}}
{"id": "2508.11380", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11380", "abs": "https://arxiv.org/abs/2508.11380", "authors": ["Dolev Mutzari", "Tonmoay Deb", "Cristian Molinaro", "Andrea Pugliese", "V. S. Subrahmanian", "Sarit Kraus"], "title": "Defending a City from Multi-Drone Attacks: A Sequential Stackelberg Security Games Approach", "comment": "59 pages, 10 figures", "summary": "To counter an imminent multi-drone attack on a city, defenders have deployed\ndrones across the city. These drones must intercept/eliminate the threat, thus\nreducing potential damage from the attack. We model this as a Sequential\nStackelberg Security Game, where the defender first commits to a mixed\nsequential defense strategy, and the attacker then best responds. We develop an\nefficient algorithm called S2D2, which outputs a defense strategy. We\ndemonstrate the efficacy of S2D2 in extensive experiments on data from 80 real\ncities, improving the performance of the defender in comparison to greedy\nheuristics based on prior works. We prove that under some reasonable\nassumptions about the city structure, S2D2 outputs an approximate Strong\nStackelberg Equilibrium (SSE) with a convenient structure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2D2\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5e8f\u5217Stackelberg\u5b89\u5168\u535a\u5f08\u4e2d\u5236\u5b9a\u57ce\u5e02\u9632\u5fa1\u65e0\u4eba\u673a\u653b\u51fb\u7684\u7b56\u7565\uff0c\u5e76\u5728\u771f\u5b9e\u57ce\u5e02\u6570\u636e\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u8feb\u5728\u7709\u776b\u7684\u57ce\u5e02\u591a\u65e0\u4eba\u673a\u653b\u51fb\u5a01\u80c1\uff0c\u9700\u8981\u5728\u57ce\u5e02\u4e2d\u90e8\u7f72\u9632\u5fa1\u65e0\u4eba\u673a\u4ee5\u62e6\u622a\u6216\u6d88\u706d\u653b\u51fb\u65e0\u4eba\u673a\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6f5c\u5728\u7684\u635f\u5bb3\u3002", "method": "\u672c\u6587\u5c06\u65e0\u4eba\u673a\u9632\u5fa1\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5e8f\u5217Stackelberg\u5b89\u5168\u535a\u5f08\uff08Sequential Stackelberg Security Game\uff09\uff0c\u9632\u5fa1\u65b9\u9996\u5148\u5236\u5b9a\u4e00\u4e2a\u6df7\u5408\u9632\u5fa1\u7b56\u7565\uff0c\u7136\u540e\u653b\u51fb\u65b9\u8fdb\u884c\u6700\u4f18\u54cd\u5e94\u3002\u4e3a\u6b64\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aS2D2\u7684\u9ad8\u6548\u7b97\u6cd5\u6765\u751f\u6210\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5728\u5bf980\u4e2a\u771f\u5b9e\u57ce\u5e02\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cS2D2\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9632\u5fa1\u65b9\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u4e8e\u5148\u524d\u5de5\u4f5c\u7684\u8d2a\u5fc3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "S2D2\u7b97\u6cd5\u80fd\u591f\u4e3a\u57ce\u5e02\u9632\u5fa1\u65e0\u4eba\u673a\u653b\u51fb\u63d0\u4f9b\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u8d2a\u5fc3\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u6709\u6240\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5728\u5408\u7406\u7684\u57ce\u5e02\u7ed3\u6784\u5047\u8bbe\u4e0b\uff0cS2D2\u80fd\u591f\u8f93\u51fa\u5177\u6709\u826f\u597d\u7ed3\u6784\u7279\u6027\u7684\u8fd1\u4f3c\u5f3aStackelberg\u5747\u8861\uff08SSE\uff09\u3002"}}
{"id": "2508.11425", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11425", "abs": "https://arxiv.org/abs/2508.11425", "authors": ["Jinwei Hu", "Yi Dong", "Youcheng Sun", "Xiaowei Huang"], "title": "Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments", "comment": "Under Review", "summary": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation.", "AI": {"tldr": "TAPA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528LLM\u4f5c\u4e3a\u7b26\u53f7\u52a8\u4f5c\u7a7a\u95f4\u7684\u667a\u80fd\u4e3b\u6301\u4eba\uff0c\u901a\u8fc7\u5408\u6210\u548c\u9002\u5e94\u903b\u8f91\u539f\u8bed\u7684\u6a21\u5757\u5316\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u81ea\u4e3b\u4ee3\u7406\u9700\u8981\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6301\u7eed\u9002\u5e94\u52a8\u6001\u6761\u4ef6\u3002", "method": "TAPA\u6846\u67b6\u901a\u8fc7\u5c06LLM\u5b9a\u4f4d\u4e3a\u7b26\u53f7\u52a8\u4f5c\u7a7a\u95f4\u7684\u667a\u80fd\u4e3b\u6301\u4eba\uff0c\u5408\u6210\u548c\u9002\u5e94\u903b\u8f91\u539f\u8bed\u7684\u6a21\u5757\u5316\u7a0b\u5e8f\uff0c\u4ece\u800c\u5c06\u6218\u7565\u610f\u56fe\u4e0e\u6267\u884c\u5206\u79bb\u3002", "result": "\u5728\u7f51\u7edc\u5b89\u5168\u548c\u7fa4\u4f53\u667a\u80fd\u9886\u57df\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5728\u81ea\u4e3bDDoS\u9632\u5fa1\u573a\u666f\u4e2d\uff0cTAPA\u5b9e\u73b0\u4e8677.7%\u7684\u7f51\u7edc\u6b63\u5e38\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4fdd\u6301\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u672a\u77e5\u52a8\u6001\u73af\u5883\u68c0\u6d4b\u51c6\u786e\u7387\u3002\u5728\u73af\u5883\u548c\u5bf9\u6297\u6027\u5e72\u6270\u4e0b\u7684\u7fa4\u4f53\u667a\u80fd\u7f16\u961f\u63a7\u5236\u4e2d\uff0cTAPA\u5728\u57fa\u7ebf\u65b9\u6cd5\u5b8c\u5168\u5931\u6548\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u8fd0\u884c\u65f6\u6301\u7eed\u4fdd\u6301\u4e86\u4e00\u81f4\u6027\u3002", "conclusion": "TAPA\u901a\u8fc7\u5c06LLM\u4f5c\u4e3a\u7b26\u53f7\u52a8\u4f5c\u7a7a\u95f4\u7684\u667a\u80fd\u4e3b\u6301\u4eba\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11157", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.11157", "abs": "https://arxiv.org/abs/2508.11157", "authors": ["Ruby Jindal", "Sushil Kumar Singh", "Ravibabu Mulaveesala", "Jolly Xavier"], "title": "Thermal acoustic particle velocity sensor with structured microwires", "comment": "11 Pages, 6 Figures", "summary": "Estimating the spatial distribution of the acoustic field is essential in\ncommunication, medical imaging and other varied industrial processes. Acoustic\nparticle velocity sensor plays a key role in providing directional information\nof the sound field. We present a rigorous 3D numerical finite element analysis\nof MEMS based structurally modified thermal particle velocity sensor. The\nimpact of diverse morphological structural optimization in comparison to the\nconventional straight wire structure is rigorously studied and analyzed in\norder to achieve maximum temperature deviation and, thereby, the sensitivity of\nthe device primarily for low frequency applications.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316MEMS\u70ed\u58f0\u7c92\u5b50\u901f\u5ea6\u4f20\u611f\u5668\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5176\u4f4e\u9891\u7075\u654f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u901a\u4fe1\u3001\u533b\u5b66\u6210\u50cf\u548c\u5de5\u4e1a\u8fc7\u7a0b\u7b49\u9886\u57df\u5b9e\u73b0\u7cbe\u786e\u7684\u58f0\u573a\u4f30\u8ba1\uff0c\u58f0\u7c92\u5b50\u901f\u5ea6\u4f20\u611f\u5668\u5bf9\u4e8e\u63d0\u4f9b\u58f0\u573a\u65b9\u5411\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e09\u7ef4\u6570\u503c\u6709\u9650\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u57fa\u4e8eMEMS\u7684\u7ed3\u6784\u5316\u70ed\u7c92\u5b50\u901f\u5ea6\u4f20\u611f\u5668\u8fdb\u884c\u5efa\u6a21\u548c\u4eff\u771f\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u76f4\u4e1d\u7ed3\u6784\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5bf9MEMS\u5668\u4ef6\u8fdb\u884c\u4e0d\u540c\u7684\u5f62\u6001\u7ed3\u6784\u4f18\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u4f20\u7edf\u76f4\u4e1d\u7ed3\u6784\u66f4\u5927\u7684\u6e29\u5ea6\u504f\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u9891\u5e94\u7528\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8eMEMS\u7ed3\u6784\u4f18\u5316\u8bbe\u8ba1\u7684\u70ed\u58f0\u7c92\u5b50\u901f\u5ea6\u4f20\u611f\u5668\u7684\u4e09\u7ef4\u6709\u9650\u5143\u5206\u6790\uff0c\u65e8\u5728\u63d0\u9ad8\u5176\u5728\u4f4e\u9891\u5e94\u7528\u4e2d\u7684\u7075\u654f\u5ea6\u3002"}}
{"id": "2508.11019", "categories": ["cs.LO", "cs.FL", "F.1.3; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2508.11019", "abs": "https://arxiv.org/abs/2508.11019", "authors": ["Anuj Dawar", "Aidan T. Evans"], "title": "Characterizing NC1 with Typed Monoids", "comment": "22 pages", "summary": "Krebs et al. (2007) gave a characterization of the complexity class TC0 as\nthe class of languages recognized by a certain class of typed monoids. The\nnotion of typed monoid was introduced to extend methods of algebraic automata\ntheory to infinite monoids and hence characterize classes beyond the regular\nlanguages. We advance this line of work beyond TC0 by giving a characterization\nof NC1. This is obtained by first showing that NC1 can be defined as the\nlanguages expressible in an extension of first-order logic using only unary\nquantifiers over regular languages. The expressibility result is a consequence\nof a general result showing that finite monoid multiplication quantifiers of\nhigher dimension can be replaced with unary quantifiers in the context of\ninterpretations over strings, which also answers a question of Lautemann et al.\n(2001). We establish this collapse result for a much more general class of\ninterpretations using results on interpretations due to Boja\\'nczyk et al.\n(2019), which may be of independent interest.", "AI": {"tldr": "This paper characterizes the complexity class NC1 using logic and algebra. It shows NC1 is equivalent to languages expressible in a specific type of first-order logic. The paper also proves a general result about replacing complex quantifiers with simpler ones in certain mathematical structures, which has broader implications.", "motivation": "To extend the algebraic automata theory methods beyond regular languages and characterize complexity classes beyond TC0, specifically NC1.", "method": "The paper first defines NC1 as languages expressible in an extended first-order logic using unary quantifiers over regular languages. It then proves a general result that finite monoid multiplication quantifiers of higher dimensions can be replaced with unary quantifiers in interpretations over strings, extending previous work and answering a question by Lautemann et al. The collapse result is established for a broader class of interpretations using Boja\u0144czyk et al.'s (2019) results.", "result": "NC1 is characterized as languages expressible in first-order logic extended with unary quantifiers over regular languages. A general collapse result is proven, showing that higher-dimensional monoid multiplication quantifiers can be replaced by unary quantifiers in interpretations over strings, which generalizes previous findings and answers an open question.", "conclusion": "The paper provides a characterization of NC1 by linking it to extensions of first-order logic with unary quantifiers over regular languages. It also generalizes a collapse result concerning quantifiers in interpretations over strings."}}
{"id": "2508.11246", "categories": ["cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11246", "abs": "https://arxiv.org/abs/2508.11246", "authors": ["Runlong Yu", "Shiyuan Luo", "Rahul Ghosh", "Lingyao Li", "Yiqun Xie", "Xiaowei Jia"], "title": "RAG for Geoscience: What We Expect, Gaps and Opportunities", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by combining\nretrieval with generation. However, its current workflow remains largely\ntext-centric, limiting its applicability in geoscience. Many geoscientific\ntasks are inherently evidence-hungry. Typical examples involve imputing missing\nobservations using analog scenes, retrieving equations and parameters to\ncalibrate models, geolocating field photos based on visual cues, or surfacing\nhistorical case studies to support policy analyses. A simple\n``retrieve-then-generate'' pipeline is insufficient for these needs. We\nenvision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular\nretrieve $\\rightarrow$ reason $\\rightarrow$ generate $\\rightarrow$ verify loop.\nGeo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth\ndata; (ii) reasoning under physical and domain constraints; (iii) generation of\nscience-grade artifacts; and (iv) verification of generated hypotheses against\nnumerical models, ground measurements, and expert assessments. This shift opens\nnew opportunities for more trustworthy and transparent geoscience workflows.", "AI": {"tldr": "Geo-RAG \u6269\u5c55\u4e86 RAG\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5730\u5b66\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5305\u542b\u63a8\u7406\u548c\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u4fe1\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\u3001'\u68c0\u7d22\u540e\u751f\u6210'\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u591a\u6a21\u6001\u8bc1\u636e\u7684\u5730\u5b66\u4efb\u52a1\uff0c\u4f8b\u5982\u4f7f\u7528\u6a21\u62df\u573a\u666f\u586b\u8865\u7f3a\u5931\u89c2\u6d4b\u503c\u3001\u68c0\u7d22\u65b9\u7a0b\u548c\u53c2\u6570\u4ee5\u6821\u51c6\u6a21\u578b\u3001\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u5bf9\u5730\u7406\u4f4d\u7f6e\u8fdb\u884c\u73b0\u573a\u7167\u7247\u5b9a\u4f4d\u6216\u6c47\u603b\u5386\u53f2\u6848\u4f8b\u7814\u7a76\u4ee5\u652f\u6301\u653f\u7b56\u5206\u6790\u3002", "method": "Geo-RAG \u88ab\u8bbe\u60f3\u4e3a\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u201c\u68c0\u7d22 \u2192 \u63a8\u7406 \u2192 \u751f\u6210 \u2192 \u9a8c\u8bc1\u201d\u5faa\u73af\uff0c\u652f\u6301\u591a\u6a21\u6001\u5730\u7403\u6570\u636e\u68c0\u7d22\u3001\u7269\u7406\u548c\u9886\u57df\u7ea6\u675f\u63a8\u7406\u3001\u79d1\u5b66\u7ea7\u4ea7\u7269\u751f\u6210\u4ee5\u53ca\u751f\u6210\u5047\u8bbe\u9a8c\u8bc1\u3002", "result": "Geo-RAG \u65e8\u5728\u589e\u5f3a\u5730\u5b66\u4efb\u52a1\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5e94\u7528\uff0c\u514b\u670d\u4e86\u5f53\u524d RAG \u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u5e76\u8fdb\u884c\u63a8\u7406\u548c\u9a8c\u8bc1\u3002", "conclusion": "Geo-RAG\u901a\u8fc7\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u68c0\u7d22\u3001\u7269\u7406\u548c\u9886\u57df\u7ea6\u675f\u63a8\u7406\u3001\u79d1\u5b66\u7ea7\u4ea7\u7269\u751f\u6210\u4ee5\u53ca\u9488\u5bf9\u6570\u503c\u6a21\u578b\u3001\u5730\u9762\u6d4b\u91cf\u548c\u4e13\u5bb6\u8bc4\u4f30\u7684\u751f\u6210\u5047\u8bbe\u9a8c\u8bc1\uff0c\u5f00\u8f9f\u4e86\u66f4\u503c\u5f97\u4fe1\u8d56\u548c\u900f\u660e\u7684\u5730\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u65b0\u673a\u9047\u3002"}}
{"id": "2508.11026", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11026", "abs": "https://arxiv.org/abs/2508.11026", "authors": ["John W. Sheppard"], "title": "Overview of Complex System Design", "comment": "Appears as Chapter 1 in Realizing Complex Integrated Systems, Anthony\n  P. Ambler and John W. Sheppard (ads.), CRC Press, 2025", "summary": "This chapter serves as an introduction to systems engineering focused on the\nbroad issues surrounding realizing complex integrated systems. What is a\nsystem? We pose a number of possible definitions and perspectives, but leave\nopen the opportunity to consider the system from the target context where it\nwill be used. Once we have a system in mind, we acknowledge the fact that this\nsystem needs to integrate a variety of pieces, components, subsystems, in order\nfor it to accomplish its task. Therefore, we concern ourselves at the\nboundaries and interfaces of different technologies and disciplines to\ndetermine how best to achieve that integration. Next we raise the specter that\nthis integrated system is complex. Complexity can be defined in a number of\nways. For one, the sheer number of subsystems or components can be a measure of\ncomplexity. We could also consider the functions being performed by the system\nand how those functions interact with one another. Further, we could consider\ncomputational aspects such as the time or memory that may be needed to\naccomplish one or more tasks. The extent to which new behaviors might emerge\nfrom the system can also be regarded as an element of complexity. In the end,\ncomplexity is that characteristic of a system that defines the associated\nchallenges along the life of the system, so we are concerned with how to manage\nthat complexity. Finally, realization refers to the process by which our\ncomplex integrated system moves from concept to deployment and subsequent\nsupport. It refers to the entire design, development, manufacture, deployment,\noperation, and support life cycle. Of particular note here, however, is that we\nfocus on systems that, by their very nature, are complex. In other words, we\nare interested in large, complicated, interacting beasts that are intended to\nperform difficult tasks and meet a wide variety of end-user needs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7cfb\u7edf\u5de5\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u590d\u6742\u96c6\u6210\u7cfb\u7edf\u7684\u5b9e\u73b0\uff0c\u8ba8\u8bba\u4e86\u7cfb\u7edf\u7684\u5b9a\u4e49\u3001\u96c6\u6210\u3001\u590d\u6742\u6027\u53ca\u5176\u7ba1\u7406\uff0c\u5e76\u6982\u8ff0\u4e86\u4ece\u6982\u5ff5\u5230\u90e8\u7f72\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4ecb\u7ecd\u7cfb\u7edf\u5de5\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u73b0\u590d\u6742\u96c6\u6210\u7cfb\u7edf\u6240\u56f4\u7ed5\u7684\u5e7f\u6cdb\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u7ba1\u7406\u590d\u6742\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u63a2\u8ba8\u7cfb\u7edf\u7684\u5b9a\u4e49\u3001\u96c6\u6210\u3001\u590d\u6742\u6027\u53ca\u5176\u5b9e\u73b0\u8fc7\u7a0b\u6765\u4ecb\u7ecd\u7cfb\u7edf\u5de5\u7a0b\u3002", "result": "\u672c\u6587\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u590d\u6742\u96c6\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5f00\u53d1\u3001\u5236\u9020\u3001\u90e8\u7f72\u3001\u8fd0\u884c\u548c\u652f\u6301\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7ae0\u8282\u4ecb\u7ecd\u4e86\u7cfb\u7edf\u5de5\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u73b0\u590d\u6742\u96c6\u6210\u7cfb\u7edf\u6240\u56f4\u7ed5\u7684\u5e7f\u6cdb\u95ee\u9898\u3002\u901a\u8fc7\u63a2\u8ba8\u7cfb\u7edf\u7684\u5b9a\u4e49\u548c\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u7531\u5404\u79cd\u90e8\u4ef6\u3001\u7ec4\u4ef6\u548c\u5b50\u7cfb\u7edf\u96c6\u6210\u4ee5\u5b8c\u6210\u5176\u4efb\u52a1\u3002\u6587\u7ae0\u5173\u6ce8\u4e0d\u540c\u6280\u672f\u548c\u5b66\u79d1\u7684\u8fb9\u754c\u4e0e\u63a5\u53e3\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u5b50\u7cfb\u7edf\u6570\u91cf\u3001\u529f\u80fd\u4ea4\u4e92\u3001\u8ba1\u7b97\u9700\u6c42\u4ee5\u53ca\u65b0\u5174\u884c\u4e3a\u7b49\u65b9\u9762\uff0c\u5e76\u63d0\u51fa\u4e86\u7ba1\u7406\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u6700\u540e\uff0c\u6587\u7ae0\u9610\u8ff0\u4e86\u7cfb\u7edf\u7684\u5b9e\u73b0\u8fc7\u7a0b\uff0c\u5373\u4ece\u6982\u5ff5\u5230\u90e8\u7f72\u548c\u652f\u6301\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\uff0c\u7279\u522b\u5173\u6ce8\u90a3\u4e9b\u672c\u8eab\u5c31\u590d\u6742\u7684\u7cfb\u7edf\uff0c\u5373\u90a3\u4e9b\u5e9e\u5927\u3001\u590d\u6742\u3001\u76f8\u4e92\u4f5c\u7528\u4e14\u65e8\u5728\u6267\u884c\u56f0\u96be\u4efb\u52a1\u4ee5\u6ee1\u8db3\u5e7f\u6cdb\u6700\u7ec8\u7528\u6237\u9700\u6c42\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.10968", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2508.10968", "abs": "https://arxiv.org/abs/2508.10968", "authors": ["Rui Li", "V. J. Mart\u00ednez-Lahuerta", "Naceur Gaaloul", "Klemens Hammerer"], "title": "High-contrast double Bragg interferometry via detuning control", "comment": "8 pages, 6 figures + 4 pages, 7 figures (suppl.)", "summary": "We propose high-contrast Mach-Zehnder atom interferometers based on double\nBragg diffraction (DBD) operating under external acceleration. To mitigate\ndifferential Doppler shifts and experimental imperfections, we introduce a\ntri-frequency laser scheme with dynamic detuning control. We evaluate four\ndetuning-control strategies&mdash;conventional DBD, constant detuning, linear\ndetuning sweep (DS-DBD), and a hybrid protocol combining detuning sweep with\noptimal control theory (OCT)&mdash;using exact numerical simulations and a\nfive-level S-matrix model. The OCT strategy provides the highest robustness,\nmaintaining contrast above 95\\% under realistic conditions, while the DS-DBD\nstrategy sustains contrast above 90\\% for well-collimated Bose-Einstein\ncondensates. These results offer practical pathways to enhancing DBD-based\ninterferometers for precision quantum sensing and fundamental physics tests.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6fc0\u5149\u5931\u8c10\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8\u5728\u52a0\u901f\u5ea6\u6761\u4ef6\u4e0b\u8fd0\u884c\u7684\u539f\u5b50\u5e72\u6d89\u4eea\u7684\u5bf9\u6bd4\u5ea6\uff0c\u5176\u4e2d\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff08OCT\uff09\u7b56\u7565\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u5916\u90e8\u52a0\u901f\u5ea6\u6761\u4ef6\u4e0b\u8fd0\u884c\u7684\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u539f\u5b50\u5e72\u6d89\u4eea\u7684\u5bf9\u6bd4\u5ea6\uff0c\u5e76\u51cf\u5c11\u5fae\u5206\u591a\u666e\u52d2\u9891\u79fb\u548c\u5b9e\u9a8c\u8bef\u5dee\u3002", "method": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u4e94\u80fd\u7ea7S\u77e9\u9635\u6a21\u578b\u8bc4\u4f30\u4e86\u56db\u79cd\u5931\u8c10\u63a7\u5236\u7b56\u7565\uff08\u5e38\u89c4\u53cc\u5e03\u62c9\u683c\u884d\u5c04\u3001\u6052\u5b9a\u5931\u8c10\u3001\u7ebf\u6027\u5931\u8c10\u626b\u63cf\u548c\u7ed3\u5408\u5931\u8c10\u626b\u63cf\u4e0e\u6700\u4f18\u63a7\u5236\u7406\u8bba\u7684\u6df7\u5408\u65b9\u6848\uff09\uff0c\u4ee5\u51cf\u5c11\u5fae\u5206\u591a\u666e\u52d2\u9891\u79fb\u548c\u5b9e\u9a8c\u8bef\u5dee\u3002", "result": "\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff08OCT\uff09\u7b56\u7565\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u5bf9\u6bd4\u5ea6\u4fdd\u6301\u572895%\u4ee5\u4e0a\uff1b\u7ebf\u6027\u5931\u8c10\u626b\u63cf\uff08DS-DBD\uff09\u7b56\u7565\u5bf9\u4e8e\u51c6\u76f4\u826f\u597d\u7684\u73bb\u8272-\u7231\u56e0\u65af\u5766\u51dd\u805a\u4f53\uff0c\u5bf9\u6bd4\u5ea6\u53ef\u4fdd\u6301\u572890%\u4ee5\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u4e3a\u9ad8\u5bf9\u6bd4\u5ea6\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u539f\u5b50\u5e72\u6d89\u4eea\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5916\u90e8\u52a0\u901f\u5ea6\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u6fc0\u5149\u5931\u8c10\u63a7\u5236\u6765\u63d0\u9ad8\u7cbe\u5bc6\u91cf\u5b50\u4f20\u611f\u548c\u57fa\u7840\u7269\u7406\u5b66\u6d4b\u8bd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11477", "categories": ["cs.AR", "cs.ET", "cs.OS"], "pdf": "https://arxiv.org/pdf/2508.11477", "abs": "https://arxiv.org/abs/2508.11477", "authors": ["Hyunsun Chung", "Junhyeok Park", "Taewan Noh", "Seonghoon Ahn", "Kihwan Kim", "Ming Zhao", "Youngjae Kim"], "title": "OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs", "comment": "This paper will be published in the proceedings of the 33rd\n  International Symposium on the Modeling, Analysis, and Simulation of Computer\n  and Telecommunication System (MASCOTS)", "summary": "The advent of Compute Express Link (CXL) enables SSDs to participate in the\nmemory hierarchy as large-capacity, byte-addressable memory devices. These\nCXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and\ntraditional storage, combining NAND flash density with memory-like access\nsemantics. However, evaluating the performance of CXL-SSDs remains difficult\ndue to the lack of hardware that natively supports the CXL.mem protocol on\nSSDs. As a result, most prior work relies on hybrid simulators combining CPU\nmodels augmented with CXL.mem semantics and SSD simulators that approximate\ninternal flash behaviors. While effective for early-stage exploration, this\napproach cannot faithfully model firmware-level interactions and low-level\nstorage dynamics critical to CXL-SSD performance. In this paper, we present\nOpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap\nbetween simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem\nsimulator on the host side with a physical OpenSSD platform running real\nfirmware. This enables in-situ firmware execution triggered by simulated memory\nrequests. Through these contributions, OpenCXD reflects device-level phenomena\nunobservable in simulation-only setups, providing critical insights for future\nfirmware design tailored to CXL-SSDs.", "AI": {"tldr": "OpenCXD is a new evaluation framework that combines simulation with a real SSD to accurately model CXL-SSD performance, addressing limitations of previous simulation-only approaches and providing insights for future SSD firmware design.", "motivation": "Evaluating the performance of CXL-SSDs is difficult due to the lack of hardware that natively supports the CXL.mem protocol on SSDs, and prior work relying on hybrid simulators that cannot faithfully model firmware-level interactions and low-level storage dynamics.", "method": "OpenCXD is a real-device-guided hybrid evaluation framework that integrates a cycle-accurate CXL.mem simulator on the host side with a physical OpenSSD platform running real firmware.", "result": "OpenCXD bridges the gap between simulation and hardware, enabling a more faithful modeling of CXL-SSD performance by integrating a CXL.mem simulator with a physical OpenSSD platform.", "conclusion": "OpenCXD enables in-situ firmware execution triggered by simulated memory requests, reflecting device-level phenomena unobservable in simulation-only setups and providing critical insights for future firmware design tailored to CXL-SSDs."}}
{"id": "2508.10973", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10973", "abs": "https://arxiv.org/abs/2508.10973", "authors": ["Hongchen Wang", "Sima Zeinali Danalou", "Jiahao Zhu", "Kenneth Sulimro", "Chaewon Lim", "Smita Basak", "Aimee Tai", "Usan Siriwardana", "Jason Hattrick-Simpers", "Jay Werber"], "title": "Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes", "comment": null, "summary": "The development of porous polymeric membranes remains a labor-intensive\nprocess, often requiring extensive trial and error to identify optimal\nfabrication parameters. In this study, we present a fully automated platform\nfor membrane fabrication and characterization via nonsolvent-induced phase\nseparation (NIPS). The system integrates automated solution preparation, blade\ncasting, controlled immersion, and compression testing, allowing precise\ncontrol over fabrication parameters such as polymer concentration and ambient\nhumidity. The modular design allows parallel processing and reproducible\nhandling of samples, reducing experimental time and increasing consistency.\nCompression testing is introduced as a sensitive mechanical characterization\nmethod for estimating membrane stiffness and as a proxy to infer porosity and\nintra-sample uniformity through automated analysis of stress-strain curves. As\na proof of concept to demonstrate the effectiveness of the system, NIPS was\ncarried out with polysulfone, the green solvent PolarClean, and water as the\npolymer, solvent, and nonsolvent, respectively. Experiments conducted with the\nautomated system reproduced expected effects of polymer concentration and\nambient humidity on membrane properties, namely increased stiffness and\nuniformity with increasing polymer concentration and humidity variations in\npore morphology and mechanical response. The developed automated platform\nsupports high-throughput experimentation and is well-suited for integration\ninto self-driving laboratory workflows, offering a scalable and reproducible\nfoundation for data-driven optimization of porous polymeric membranes through\nNIPS.", "AI": {"tldr": "\u901a\u8fc7\u5168\u81ea\u52a8\u5316\u7684NIPS\u5e73\u53f0\uff0c\u4f7f\u7528\u805a\u781c\u3001PolarClean\u548c\u6c34\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u5236\u9020\u548c\u8868\u5f81\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u53c2\u6570\u5e76\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f00\u53d1\u591a\u5b54\u805a\u5408\u7269\u819c\u901a\u5e38\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bd5\u9519\u6765\u786e\u5b9a\u6700\u4f73\u7684\u5236\u9020\u53c2\u6570\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u63a7\u7684\u65b9\u6cd5\u6765\u5236\u9020\u548c\u8868\u5f81\u8fd9\u4e9b\u819c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u5316\u7684\u819c\u5236\u9020\u4e0e\u8868\u5f81\u5e73\u53f0\uff0c\u91c7\u7528\u975e\u6eb6\u5242\u81f4\u76f8\u5206\u79bb\uff08NIPS\uff09\u6280\u672f\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u7684\u6eb6\u6db2\u5236\u5907\u3001\u522e\u5200\u6d47\u94f8\u3001\u53ef\u63a7\u6d78\u5165\u548c\u538b\u7f29\u6d4b\u8bd5\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u805a\u5408\u7269\u6d53\u5ea6\u548c\u73af\u5883\u6e7f\u5ea6\u7b49\u5236\u9020\u53c2\u6570\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5e76\u884c\u5904\u7406\u548c\u53ef\u91cd\u590d\u7684\u6837\u54c1\u5904\u7406\uff0c\u7f29\u77ed\u4e86\u5b9e\u9a8c\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5bf9\u5e94\u529b-\u5e94\u53d8\u66f2\u7ebf\u8fdb\u884c\u81ea\u52a8\u5316\u5206\u6790\uff0c\u5f15\u5165\u538b\u7f29\u6d4b\u8bd5\u4f5c\u4e3a\u654f\u611f\u7684\u673a\u68b0\u8868\u5f81\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u7b97\u819c\u7684\u521a\u5ea6\u548c\u63a8\u65ad\u5b54\u9699\u7387\u53ca\u6837\u672c\u5185\u5747\u5300\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u591f\u91cd\u73b0\u805a\u5408\u7269\u6d53\u5ea6\u548c\u73af\u5883\u6e7f\u5ea6\u5bf9\u819c\u6027\u80fd\u7684\u9884\u671f\u5f71\u54cd\uff0c\u5305\u62ec\u968f\u7740\u805a\u5408\u7269\u6d53\u5ea6\u548c\u6e7f\u5ea6\u7684\u589e\u52a0\uff0c\u819c\u7684\u521a\u5ea6\u548c\u5747\u5300\u6027\u4e5f\u4f1a\u589e\u52a0\uff0c\u540c\u65f6\u6e7f\u5ea6\u53d8\u5316\u4f1a\u5f71\u54cd\u5b54\u9699\u5f62\u6001\u548c\u673a\u68b0\u54cd\u5e94\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u5e73\u53f0\u4e3a\u805a\u5408\u7269\u819c\u7684\u5236\u9020\u63d0\u4f9b\u4e86\u9ad8\u901a\u91cf\u5b9e\u9a8c\u652f\u6301\uff0c\u5e76\u975e\u5e38\u9002\u5408\u96c6\u6210\u5230\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4e3a\u901a\u8fc7\u975e\u6eb6\u5242\u81f4\u76f8\u5206\u79bb\uff08NIPS\uff09\u6280\u672f\u4f18\u5316\u591a\u5b54\u805a\u5408\u7269\u819c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u7840\u3002"}}
{"id": "2508.10976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9 ASPIC+ \u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u63a5\u5730\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u5904\u7406\u4e00\u9636\u89c4\u5219\u65f6\u63a5\u5730\u53ef\u80fd\u5bfc\u81f4\u7406\u8bba\u89c4\u6a21\u7206\u70b8\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7 Datalog \u8f6c\u6362\u548c\u7279\u5b9a\u7b80\u5316\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684 ASPIC+ \u65b9\u6cd5\u4e3b\u8981\u652f\u6301\u547d\u9898\u89c4\u5219\uff0c\u800c\u5bf9\u4e8e\u6d89\u53ca\u4e00\u9636\u89c4\u5219\u7684 ASPIC+ \u5b9e\u4f8b\uff0c\u9700\u8981\u8fdb\u884c\u63a5\u5730\uff0c\u4f46\u63a5\u5730\u53ef\u80fd\u5bfc\u81f4\u8f93\u5165\u7406\u8bba\u7684\u5927\u5c0f\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u56e0\u6b64\u9700\u8981\u667a\u80fd\u7684\u63a5\u5730\u7a0b\u5e8f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5c06 ASPIC+ \u5b9e\u4f8b\u8f6c\u6362\u4e3a Datalog \u7a0b\u5e8f\uff0c\u5e76\u5229\u7528 Datalog \u5f15\u64ce\u83b7\u53d6\u63a5\u5730\u66ff\u6362\uff0c\u540c\u65f6\u63d0\u51fa\u9488\u5bf9 ASPIC+ \u7684\u7b80\u5316\u65b9\u6cd5\u4ee5\u907f\u514d\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6ca1\u6709\u5f71\u54cd\u7684\u89c4\u5219\u63a5\u5730\u3002", "result": "\u63d0\u51fa\u7684\u667a\u80fd\u63a5\u5730\u7a0b\u5e8f\u80fd\u591f\u6709\u6548\u7ba1\u7406\u63a5\u5730\u7406\u8bba\u7684\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u63a5\u5730\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u901a\u8fc7\u5c06 ASPIC+ \u5b9e\u4f8b\u8f6c\u6362\u4e3a Datalog \u7a0b\u5e8f\u5e76\u5229\u7528 Datalog \u5f15\u64ce\u6765\u83b7\u53d6\u63a5\u5730\u66ff\u6362\uff0c\u4ece\u800c\u7ba1\u7406\u63a5\u5730\u7406\u8bba\u7684\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9488\u5bf9 ASPIC+ \u7279\u6709\u7684\u7b80\u5316\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6ca1\u6709\u5f71\u54cd\u7684\u89c4\u5219\u8fdb\u884c\u63a5\u5730\u3002\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.11359", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.11359", "abs": "https://arxiv.org/abs/2508.11359", "authors": ["Jiejun Hu-Bolz", "James Stovold"], "title": "Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with Game Theory", "comment": "8 pages, 6 figures, accepted in ALife 2025", "summary": "This work asks whether a human interacting with a generative AI system can\nmerge into a single individual through iterative, information-driven\ninteractions. We model the interactions between a human, a generative AI\nsystem, and the human's wider environment as a three-player stochastic game. We\nuse information-theoretic measures (entropy, mutual information, and transfer\nentropy) to show that our modelled human and generative AI are able to form an\naggregate individual in the sense of Krakauer et al. (2020). The model we\npresent is able to answer interesting questions around the symbiotic nature of\nhumans and AI systems, including whether LLM-driven chatbots are acting as\nparasites, feeding on the information provided by humans.", "AI": {"tldr": "\u4eba\u7c7b\u4e0e\u751f\u6210\u5f0fAI\u901a\u8fc7\u4fe1\u606f\u4ea4\u4e92\u53ef\u878d\u5408\u6210\u5355\u4e00\u5b9e\u4f53\u3002", "motivation": "\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e92\u52a8\u662f\u5426\u80fd\u901a\u8fc7\u4fe1\u606f\u9a71\u52a8\u7684\u4ea4\u4e92\u878d\u5408\u6210\u5355\u4e00\u4e2a\u4f53\uff0c\u5e76\u7814\u7a76\u4e86LLM\u804a\u5929\u673a\u5668\u4eba\u662f\u5426\u4f1a\u5bc4\u751f\u4e8e\u4eba\u7c7b\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u4fe1\u606f\u8bba\uff08\u71b5\u3001\u4e92\u4fe1\u606f\u548c\u4f20\u9012\u71b5\uff09\u5c06\u4eba\u3001\u751f\u6210\u5f0fAI\u7cfb\u7edf\u548c\u4eba\u7c7b\u66f4\u5e7f\u6cdb\u7684\u73af\u5883\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4e09\u65b9\u968f\u673a\u535a\u5f08\u3002", "result": "\u901a\u8fc7\u4fe1\u606f\u8bba\u7684\u6d4b\u91cf\uff0c\u6a21\u578b\u8bc1\u660e\u4e86\u4eba\u7c7b\u548c\u751f\u6210\u5f0fAI\u80fd\u591f\u5f62\u6210\u4e00\u4e2a\u805a\u5408\u7684\u4e2a\u4f53\uff0c\u8fd9\u4e0eKrakauer\u7b49\u4eba\uff082020\uff09\u7684\u5b9a\u4e49\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u3001\u4fe1\u606f\u9a71\u52a8\u7684\u4ea4\u4e92\uff0c\u4e0e\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e92\u52a8\u7684\u4eba\u7c7b\u53ef\u4ee5\u878d\u5408\u6210\u4e00\u4e2a\u5355\u4e00\u7684\u4e2a\u4f53\u3002\u8be5\u6a21\u578b\u80fd\u591f\u56de\u7b54\u5173\u4e8e\u4eba\u673a\u5171\u751f\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u4f8b\u5982LLM\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u662f\u5426\u4ee5\u4eba\u7c7b\u63d0\u4f9b\u7684\u4fe1\u606f\u4e3a\u98df\u3002"}}
{"id": "2508.10918", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8e\u6ce8\u89c6\u4fe1\u53f7\u7684\u9690\u79c1\u589e\u5f3a\u673a\u5236\uff0c\u53ef\u9632\u6b62\u7528\u6237\u8de8\u4f1a\u8bdd\u91cd\u65b0\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7528\u4e8e\u826f\u6027\u4efb\u52a1\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u9632\u6b62\u7528\u6237\u8de8\u4f1a\u8bdd\u88ab\u91cd\u65b0\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7528\u4e8e\u826f\u6027\u4efb\u52a1\u7684\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6f5c\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u9690\u79c1\u589e\u5f3a\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u751f\u7269\u8bc6\u522b\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u4e86\u6548\u7528\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7b26\u5408\u751f\u7406\u5b66\u7684\u6ce8\u89c6\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6ce8\u89c6\u4fe1\u53f7\u7684\u9690\u79c1\u589e\u5f3a\u673a\u5236\uff0c\u8be5\u673a\u5236\u4f7f\u7528\u6f5c\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff0c\u53ef\u5728\u672a\u7ecf\u7528\u6237\u540c\u610f\u7684\u60c5\u51b5\u4e0b\u9632\u6b62\u7528\u6237\u8de8\u4f1a\u8bdd\u91cd\u65b0\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7528\u4e8e\u826f\u6027\u4efb\u52a1\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2508.11040", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph", "physics.atom-ph", "physics.chem-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11040", "abs": "https://arxiv.org/abs/2508.11040", "authors": ["Subhendu Mishra", "Arpan Chakraborty", "Douglas S. Galvao", "Pedro A. S. Autreto", "Abhishek Kumar Singh"], "title": "Chiral Phonons in Graphyne", "comment": "Submitted to a ACS journal", "summary": "Chiral phonons, quantized lattice vibrations with circular polarization and\nnon-zero angular momentum, offer new perspectives for phononic and quantum\ndevice engineering. Graphyne could be a promising candidate due to its unique\nlattice geometry, valley-structured electronic bands, and thermal transport\ncapabilities. However, chiral phonons in graphyne remain unexplored owing to\nthe existence of inversion ($\\mathscr{P}$) and time-reversal ($\\mathscr{T}$)\nsymmetries. Herein, we have demonstrated the existence of chiral phonons in\ngraphynes, achieved by breaking combined $\\mathscr{PT}$ symmetry through\natomic-selective substitutional doping. We find that the B, N, dopants and\northo BN co-dopant in 6-6-12 and $\\gamma$-graphynes induce localized structural\ndeformations. These deformations lift phonon degeneracies away from $\\Gamma$\npoint and give rise to circularly polarized vibrational modes. We further\nestablished a strong correlation between chiral phonon angular momentum and\nelectron affinity of dopants. Electron-rich dopants increase local electron\ndensity which could enable chiral phonon modes to couple more effectively with\nelectronic environment. This in turn increases phonon angular momentum,\nindicating potential role of electron-phonon interactions in angular momentum\nmodulation of chiral phonons. Our prosposed approach provides a tunable route\nfor controlling chiral phonon behavior, paving way for development of advanced\nphononic devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u63ba\u6742\u6253\u7834\u77f3\u58a8\u7094\u7684\u5bf9\u79f0\u6027\uff0c\u5b9e\u73b0\u4e86\u624b\u6027\u58f0\u5b50\u7684\u5b58\u5728\uff0c\u5e76\u53d1\u73b0\u4e86\u5176\u89d2\u52a8\u91cf\u53ef\u88ab\u63ba\u6742\u5242\u8c03\u63a7\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u58f0\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u624b\u6027\u58f0\u5b50\uff08\u5177\u6709\u5706\u504f\u632f\u548c\u975e\u96f6\u89d2\u52a8\u91cf\u7684\u91cf\u5b50\u5316\u6676\u683c\u632f\u52a8\uff09\u4e3a\u58f0\u5b50\u548c\u91cf\u5b50\u5668\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u77f3\u58a8\u7094\u56e0\u5176\u72ec\u7279\u7684\u6676\u683c\u51e0\u4f55\u3001\u8c37\u7ed3\u6784\u7535\u5b50\u80fd\u5e26\u548c\u70ed\u4f20\u8f93\u80fd\u529b\uff0c\u53ef\u80fd\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u5b9e\u73b0\u624b\u6027\u58f0\u5b50\u7684\u6750\u6599\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b58\u5728\u5b87\u79f0\uff08P\uff09\u548c\u65f6\u95f4\u53cd\u8f6c\uff08T\uff09\u5bf9\u79f0\u6027\uff0c\u77f3\u58a8\u7094\u4e2d\u7684\u624b\u6027\u58f0\u5b50\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u539f\u5b50\u9009\u62e9\u6027\u53d6\u4ee3\u63ba\u6742\uff08B\u3001N\u3001ortho BN\u5171\u63ba\u6742\uff09\u6253\u7834\u8054\u5408\u7684PT\u5bf9\u79f0\u6027\uff0c\u89c2\u5bdf\u5e76\u8bc1\u660e\u4e866-6-12\u548c\u03b3-\u77f3\u58a8\u7094\u4e2d\u624b\u6027\u58f0\u5b50\u7684\u5b58\u5728\uff0c\u5e76\u7814\u7a76\u4e86\u63ba\u6742\u5242\u7684\u7535\u5b50\u4eb2\u548c\u529b\u4e0e\u624b\u6027\u58f0\u5b50\u89d2\u52a8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u77f3\u58a8\u7094\u4e2d\u7684B\u3001N\u63ba\u6742\u5242\u548cortho BN\u5171\u63ba\u6742\u5242\u57286-6-12\u548c\u03b3-\u77f3\u58a8\u7094\u4e2d\u8bf1\u5bfc\u4e86\u5c40\u90e8\u7ed3\u6784\u53d8\u5f62\uff0c\u62ac\u5347\u4e86\u504f\u79bb\u0393\u70b9\u7684\u58f0\u5b50\u7b80\u5e76\u5ea6\uff0c\u4ea7\u751f\u4e86\u5706\u504f\u632f\u632f\u52a8\u6a21\u5f0f\u3002\u53d1\u73b0\u4e86\u624b\u6027\u58f0\u5b50\u89d2\u52a8\u91cf\u4e0e\u63ba\u6742\u5242\u7535\u5b50\u4eb2\u548c\u529b\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u5373\u5bcc\u7535\u5b50\u63ba\u6742\u5242\u589e\u52a0\u4e86\u5c40\u90e8\u7535\u5b50\u5bc6\u5ea6\uff0c\u53ef\u4ee5\u4f7f\u624b\u6027\u58f0\u5b50\u6a21\u5f0f\u66f4\u6709\u6548\u5730\u4e0e\u7535\u5b50\u73af\u5883\u8026\u5408\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u58f0\u5b50\u89d2\u52a8\u91cf\uff0c\u8868\u660e\u7535\u5b50-\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u5728\u624b\u6027\u58f0\u5b50\u7684\u89d2\u52a8\u91cf\u8c03\u5236\u4e2d\u8d77\u7740\u6f5c\u5728\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u539f\u5b50\u9009\u62e9\u6027\u53d6\u4ee3\u63ba\u6742\u6253\u7834\u4e86\u8054\u5408\u7684PT\u5bf9\u79f0\u6027\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u77f3\u58a8\u7094\u4e2d\u5b58\u5728\u624b\u6027\u58f0\u5b50\uff0c\u4e3a\u8c03\u63a7\u624b\u6027\u58f0\u5b50\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u8c03\u7684\u9014\u5f84\uff0c\u4e3a\u5f00\u53d1\u5148\u8fdb\u7684\u58f0\u5b50\u5668\u4ef6\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.10904", "categories": ["cs.CL", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10904", "abs": "https://arxiv.org/abs/2508.10904", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "A2HCoder\u5229\u7528LLM\u6280\u672f\uff0c\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u548c\u9010\u6b65\u7ffb\u8bd1\uff0c\u89e3\u51b3\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e5G\u901a\u4fe1\u9886\u57df\u3002", "motivation": "\u5728\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u8d85\u4f4e\u5ef6\u8fdf\u548c\u529f\u8017\u7b49\u4e25\u683c\u8981\u6c42\u589e\u52a0\u4e86\u5bf9\u9ad8\u6548\u7b97\u6cd5\u5230\u786c\u4ef6\u90e8\u7f72\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4f20\u7edf\u4e0a\u9700\u8981\u5927\u91cf\u7684\u9886\u57df\u77e5\u8bc6\u548c\u8017\u65f6\u7684\u624b\u52a8\u5f00\u53d1\uff0c\u56e0\u4e3a\u50cfMATLAB\u8fd9\u6837\u7684\u9ad8\u7ea7\u7f16\u7a0b\u8bed\u8a00\u4e0eVerilog\u8fd9\u6837\u7684\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u5728\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u6570\u636e\u5904\u7406\u65b9\u5f0f\u548c\u6570\u636e\u7c7b\u578b\u8868\u793a\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002", "method": "A2HCoder\u662f\u4e00\u4e2a\u5206\u5c42\u7684\u7b97\u6cd5\u5230HDL\u7f16\u7801\u4ee3\u7406\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\uff0c\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6291\u5236LLM\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u5e38\u89c1\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u5728\u6a2a\u5411\u7ef4\u5ea6\uff0cA2HCoder\u5c06\u590d\u6742\u7b97\u6cd5\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u529f\u80fd\u5757\uff1b\u5728\u7eb5\u5411\u7ef4\u5ea6\uff0cA2HCoder\u4e0d\u4f9d\u8d56\u4e8e\u7aef\u5230\u7aef\u7684\u751f\u6210\uff0c\u800c\u662f\u8fdb\u884c\u9010\u6b65\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u7ffb\u8bd1\uff0c\u5e76\u5229\u7528MATLAB\u548cVitis HLS\u7b49\u5916\u90e8\u5de5\u5177\u94fe\u8fdb\u884c\u8c03\u8bd5\u548c\u7535\u8def\u7ea7\u7efc\u5408\u3002", "result": "A2HCoder\u6210\u529f\u5b9e\u73b0\u4e86\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u8f6c\u6362\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc75G\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "A2HCoder\u901a\u8fc7\u57285G\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2508.10926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10926", "abs": "https://arxiv.org/abs/2508.10926", "authors": ["DongSeong-Yoon"], "title": "A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification", "comment": "English translation of the author's pre-revision version of the\n  article published in J-KICS 50(4):561-571 (2025), DOI\n  10.7840/kics.2025.50.4.561. Posted with permission from KICS (Aug 7, 2025).\n  The published version may differ", "summary": "Since the Fourth Industrial Revolution, AI technology has been widely used in\nmany fields, but there are several limitations that need to be overcome,\nincluding overfitting/underfitting, class imbalance, and the limitations of\nrepresentation (hypothesis space) due to the characteristics of different\nmodels. As a method to overcome these problems, ensemble, commonly known as\nmodel combining, is being extensively used in the field of machine learning.\nAmong ensemble learning methods, voting ensembles have been studied with\nvarious weighting methods, showing performance improvements. However, the\nexisting methods that reflect the pre-information of classifiers in weights\nconsider only one evaluation criterion, which limits the reflection of various\ninformation that should be considered in a model realistically. Therefore, this\npaper proposes a method of making decisions considering various information\nthrough cooperative games in multi-criteria situations. Using this method,\nvarious types of information known beforehand in classifiers can be\nsimultaneously considered and reflected, leading to appropriate weight\ndistribution and performance improvement. The machine learning algorithms were\napplied to the Open-ML-CC18 dataset and compared with existing ensemble\nweighting methods. The experimental results showed superior performance\ncompared to other weighting methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5408\u4f5c\u535a\u5f08\u89e3\u51b3\u591a\u6807\u51c6\u60c5\u5883\u4e0b\u6295\u7968\u96c6\u6210\u6a21\u578b\u8d4b\u6743\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u591a\u79cd\u8bc4\u4ef7\u6807\u51c6\u6765\u4f18\u5316\u6743\u91cd\u5206\u914d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u5728\u5c06\u5206\u7c7b\u5668\u7684\u5148\u9a8c\u4fe1\u606f\u53cd\u6620\u5230\u6743\u91cd\u65f6\uff0c\u4ec5\u8003\u8651\u5355\u4e00\u8bc4\u4ef7\u6807\u51c6\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u6a21\u578b\u4e2d\u73b0\u5b9e\u5730\u8003\u8651\u5404\u79cd\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u6807\u51c6\u60c5\u5883\u4e0b\u901a\u8fc7\u5408\u4f5c\u535a\u5f08\u6765\u5236\u5b9a\u51b3\u7b56\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u8003\u8651\u5e76\u53cd\u6620\u5206\u7c7b\u5668\u4e2d\u5df2\u77e5\u7684\u5404\u79cd\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u9002\u5f53\u7684\u6743\u91cd\u5206\u914d\u548c\u6027\u80fd\u63d0\u5347\u3002", "result": "\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8eOpen-ML-CC18\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u6240\u63d0\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u96c6\u6210\u8d4b\u6743\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u8d4b\u6743\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u6807\u51c6\u60c5\u5883\u4e0b\u901a\u8fc7\u5408\u4f5c\u535a\u5f08\u6765\u5236\u5b9a\u51b3\u7b56\u7684\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u8d4b\u6743\u65b9\u6cd5\u3002"}}
{"id": "2508.11035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11035", "abs": "https://arxiv.org/abs/2508.11035", "authors": ["Hasibul Jamil", "MD S Q Zulkar Nine", "Tevfik Kosar"], "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training", "comment": "SC25 Sustainable Supercomputing Workshop", "summary": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks\nas datasets grow beyond local storage capacities and GPU compute outpaces\nnetwork and disk latencies. While recent systems optimize data-loading time,\nthey overlook the energy cost of I/O - a critical factor at large scale. We\nintroduce EMLIO, an Efficient Machine Learning I/O service that jointly\nminimizes end-to-end data-loading latency T and I/O energy consumption E across\nvariable-latency networked storage. EMLIO deploys a lightweight data-serving\ndaemon on storage nodes that serializes and batches raw samples, streams them\nover TCP with out-of-order prefetching, and integrates seamlessly with\nGPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive\nevaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)\nenvironments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use\ncompared to state-of-the-art loaders, while maintaining constant performance\nand energy profiles irrespective of network distance. EMLIO's service-based\narchitecture offers a scalable blueprint for energy-aware I/O in\nnext-generation AI clouds.", "AI": {"tldr": "EMLIO\u901a\u8fc7\u5728\u5b58\u50a8\u8282\u70b9\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6570\u636e\u670d\u52a1\u5b88\u62a4\u8fdb\u7a0b\uff0c\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u5728\u4e0d\u540c\u7f51\u7edc\u73af\u5883\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u662fAI\u4e91\u80fd\u6548\u611f\u77e5I/O\u7684\u53ef\u6269\u5c55\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u9762\u4e34I/O\u74f6\u9888\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u65f6\u95f4\u5374\u5ffd\u89c6\u4e86I/O\u7684\u80fd\u8017\u6210\u672c\u3002", "method": "EMLIO\u901a\u8fc7\u5728\u5b58\u50a8\u8282\u70b9\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6570\u636e\u670d\u52a1\u5b88\u62a4\u8fdb\u7a0b\uff0c\u5bf9\u539f\u59cb\u6837\u672c\u8fdb\u884c\u5e8f\u5217\u5316\u548c\u6279\u5904\u7406\uff0c\u901a\u8fc7TCP\u6d41\u5f0f\u4f20\u8f93\u5e76\u8fdb\u884c\u4e71\u5e8f\u9884\u53d6\uff0c\u5e76\u4e0e\u5ba2\u6237\u7aef\u7684GPU\u52a0\u901f\uff08NVIDIA DALI\uff09\u9884\u5904\u7406\u65e0\u7f1d\u96c6\u6210\uff0c\u4ece\u800c\u8de8\u8d8a\u4e0d\u540c\u5ef6\u8fdf\u7684\u7f51\u7edc\u5b58\u50a8\uff0c\u5171\u540c\u6700\u5c0f\u5316\u7aef\u5230\u7aef\u6570\u636e\u52a0\u8f7d\u5ef6\u8fdfT\u548cI/O\u80fd\u8017E\u3002", "result": "\u5728\u672c\u5730\u78c1\u76d8\u3001\u5c40\u57df\u7f51\uff080.05\u6beb\u79d2\u548c10\u6beb\u79d2RTT\uff09\u548c\u5e7f\u57df\u7f51\uff0830\u6beb\u79d2RTT\uff09\u73af\u5883\u4e2d\uff0cEMLIO\u7684I/O\u901f\u5ea6\u6700\u9ad8\u63d0\u53478.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e10.9\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6052\u5b9a\u7684\u6027\u80fd\u548c\u80fd\u8017\uff0c\u4e0d\u53d7\u7f51\u7edc\u8ddd\u79bb\u5f71\u54cd\u3002", "conclusion": "EMLIO\u7684\u670d\u52a1\u5316\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3AI\u4e91\u4e2d\u7684\u80fd\u6548\u611f\u77e5I/O\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.11177", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.11177", "abs": "https://arxiv.org/abs/2508.11177", "authors": ["I-Chao Shen", "Ariel Shamir", "Takeo Igarashi"], "title": "LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation", "comment": "11 pages, Pacific Graphics 2025", "summary": "Recent deep learning methods can generate diverse graphic design layouts\nefficiently. However, these methods often create layouts with flaws, such as\nmisalignment, unwanted overlaps, and unsatisfied containment. To tackle this\nissue, we propose an optimization-based method called LayoutRectifier, which\ngracefully rectifies auto-generated graphic design layouts to reduce these\nflaws while minimizing deviation from the generated layout. The core of our\nmethod is a two-stage optimization. First, we utilize grid systems, which\nprofessional designers commonly use to organize elements, to mitigate\nmisalignments through discrete search. Second, we introduce a novel box\ncontainment function designed to adjust the positions and sizes of the layout\nelements, preventing unwanted overlapping and promoting desired containment. We\nevaluate our method on content-agnostic and content-aware layout generation\ntasks and achieve better-quality layouts that are more suitable for downstream\ngraphic design tasks. Our method complements learning-based layout generation\nmethods and does not require additional training.", "AI": {"tldr": "A new method called LayoutRectifier fixes common flaws in AI-generated graphic designs like misalignment and overlaps using a two-stage optimization process based on grid systems and a novel box containment function. It improves design quality without retraining.", "motivation": "Existing deep learning methods for generating graphic design layouts often produce flaws such as misalignment, unwanted overlaps, and unsatisfied containment. There is a need to rectify these auto-generated layouts to improve their quality and suitability for graphic design tasks.", "method": "LayoutRectifier is an optimization-based method that uses a two-stage optimization process. Stage one utilizes grid systems for discrete search to mitigate misalignments. Stage two introduces a novel box containment function to adjust element positions and sizes, preventing overlaps and promoting containment.", "result": "LayoutRectifier was evaluated on content-agnostic and content-aware layout generation tasks, achieving better-quality layouts with fewer flaws compared to existing methods. The rectified layouts are more suitable for downstream graphic design tasks.", "conclusion": "LayoutRectifier can improve the quality of auto-generated graphic design layouts by reducing flaws like misalignment, unwanted overlaps, and unsatisfied containment, while minimizing deviation from the original layout. It achieves better-quality layouts suitable for downstream tasks and complements existing methods without requiring additional training."}}
{"id": "2508.10953", "categories": ["cond-mat.mtrl-sci", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.10953", "abs": "https://arxiv.org/abs/2508.10953", "authors": ["C. Ianhez-Pereira", "U. F. Kaneko", "A. D. Rodrigues", "I. S. S. de Oliveira", "M. P. F. de Godoy"], "title": "Tunable optical emissions of Eu3+ ions enabled by pressure-driven phase transition in ZnO", "comment": null, "summary": "Controlling the optical properties of rare-earth ions in wide-bandgap\nsemiconductors remains a major challenge in the development of next-generation\nphotonic materials. Here, we show that external hydrostatic pressure modulates\nthe structural characteristics of ZnO thin films and, in turn, tunes the\noptical emission behavior of embedded Eu3+ ions. By combining in situ\nsynchrotron X-ray diffraction and photoluminescence spectroscopy under\nhigh-pressure conditions with first-principles calculations, we capture a\npressure-induced phase transition from the hexagonal wurtzite to the cubic\nrocksalt structure near 10 GPa. This transformation is accompanied by complete\nquenching of the D0 - FJ Europium emissions near the transition threshold,\nfollowed by a partial recovery at higher pressures, likely associated with the\nemergence of structural disorder. Concurrently, the Stark components of the\nemission bands exhibit a redshift and significant broadening with increasing\npressure, reflecting enhanced crystal field strength as interatomic distances\ndecrease. Additional first-principles calculations support the observed\npressure-induced shifts in the Eu-4f states and emphasize the influence of\nlattice symmetry on their electronic environment. These results show that\nhydrostatic pressure is an effective way to adjust the optical emissions of\nrare-earth ions by changing their symmetry and local environment, providing a\nbasis for designing photonic devices and luminescent materials controlled by\npressure.", "AI": {"tldr": "External hydrostatic pressure can tune the optical properties of rare-earth ions in semiconductors by inducing phase transitions and altering the local environment, offering a new method for designing photonic devices.", "motivation": "Controlling the optical properties of rare-earth ions in wide-bandgap semiconductors is crucial for advancing photonic materials. This research investigates the use of external hydrostatic pressure to achieve such control.", "method": "The study combines in situ synchrotron X-ray diffraction and photoluminescence spectroscopy under high-pressure conditions with first-principles calculations to analyze ZnO thin films with embedded Eu3+ ions.", "result": "A pressure-induced phase transition from hexagonal wurtzite to cubic rocksalt structure was observed in ZnO thin films near 10 GPa. This transition led to the quenching and partial recovery of Eu3+ emissions, with redshifts and broadening of emission bands observed due to enhanced crystal field strength and changes in lattice symmetry.", "conclusion": "The paper demonstrates that hydrostatic pressure can effectively tune the optical emissions of rare-earth ions in wide-bandgap semiconductors by altering their symmetry and local environment. This provides a foundation for developing pressure-controlled photonic devices and luminescent materials."}}
{"id": "2508.11006", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.11006", "abs": "https://arxiv.org/abs/2508.11006", "authors": ["Umesh Biswas", "Maxwell Young"], "title": "A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost", "comment": null, "summary": "The wakeup problem addresses the fundamental challenge of symmetry breaking.\nThere are $n$ devices sharing a time-slotted multiple access channel. In any\nfixed slot, if a single device sends a packet, it succeeds; however, if two or\nmore devices send, then there is a collision and none of the corresponding\npackets succeed. For the static version of wakeup, all packets are initially\nactive (i.e., can send and listen on the channel); for the dynamic version, the\npackets become active at arbitrary times. In both versions, the goal is to\nsuccessfully send a single packet.\n  Prior results on wakeup have largely focused on the number of slots until the\nfirst success; that is, the latency. However, in many modern systems,\ncollisions introduce significant delay, an aspect that current wakeup\nalgorithms do not address. For instance, while existing results for static\nwakeup have polylogarithmic-in-$n$ latency, they can incur additional latency\nthat is {\\it linear} in the cost of a collision $C$. Thus, the total latency is\nlarge and dominated by the contributions from collisions.\n  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For\nsufficiently large $C$ and with bounded error, Aim-High has latency and\nexpected collision cost that is nearly $O(\\sqrt{C})$ for both the static and\ndynamic versions. Otherwise, the latency and expected collision cost are\n$O(\\texttt{poly}{(\\log n)})$ for the static setting, and\n$O(n\\,\\texttt{poly}{(\\log n)})$ for the dynamic setting. We also establish\nlower bounds that complement these results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Aim-High \u7684\u65b0\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u901a\u4fe1\u4fe1\u9053\u4e2d\u56e0\u78b0\u649e\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u5728\u4e0d\u540c\u53c2\u6570\u4e0b\u63d0\u4f9b\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u5ef6\u8fdf\u548c\u78b0\u649e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5524\u9192\u7b97\u6cd5\u7684\u5ef6\u8fdf\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u9996\u6b21\u6210\u529f\u53d1\u9001\u6240\u9700\u7684\u65f6\u95f4\uff0c\u800c\u5ffd\u7565\u4e86\u78b0\u649e\u5e26\u6765\u7684\u663e\u8457\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u603b\u5ef6\u8fdf\u8f83\u5927\uff0c\u6613\u53d7\u78b0\u649e\u6210\u672c\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u540d\u4e3a Aim-High \u7684\u968f\u673a\u5316\u5524\u9192\u7b97\u6cd5\u3002", "result": "\u5bf9\u4e8e\u8db3\u591f\u5927\u7684 C \u548c\u6709\u754c\u9519\u8bef\uff0cAim-High \u7b97\u6cd5\u5728\u9759\u6001\u548c\u52a8\u6001\u5524\u9192\u95ee\u9898\u4e0a\u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\u63a5\u8fd1 O(sqrt(C))\u3002\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u9759\u6001\u8bbe\u7f6e\u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\u4e3a O(poly(log n))\uff0c\u52a8\u6001\u8bbe\u7f6e\u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\u4e3a O(n poly(log n))\u3002", "conclusion": "Aim-High \u7b97\u6cd5\u5728\u9759\u6001\u548c\u52a8\u6001\u5524\u9192\u95ee\u9898\u4e0a\u5747\u53d6\u5f97\u4e86\u8fd1\u4e4e O(sqrt(C)) \u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\uff08\u5bf9\u4e8e\u8db3\u591f\u5927\u7684 C \u548c\u6709\u754c\u9519\u8bef\uff09\uff0c\u6216\u8005\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u9759\u6001\u8bbe\u7f6e\u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\u4e3a O(poly(log n))\uff0c\u52a8\u6001\u8bbe\u7f6e\u7684\u5ef6\u8fdf\u548c\u671f\u671b\u78b0\u649e\u6210\u672c\u4e3a O(n poly(log n))\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u4e0b\u754c\u3002"}}
{"id": "2508.10921", "categories": ["cs.NE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.10921", "abs": "https://arxiv.org/abs/2508.10921", "authors": ["Jiale Linghu", "Weifeng Gao", "Hao Dong", "Yufeng Nie"], "title": "SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations", "comment": null, "summary": "This study proposes a self-optimization physics-informed Fourier-features\nrandomized neural network (SO-PIFRNN) framework, which significantly improves\nthe numerical solving accuracy of PDEs through hyperparameter optimization\nmechanism. The framework employs a bi-level optimization architecture: the\nouter-level optimization utilizes a multi-strategy collaborated particle swarm\noptimization (MSC-PSO) algorithm to search for optimal hyperparameters of\nphysics-informed Fourier-features randomized neural network, while the\ninner-level optimization determines the output layer weights of the neural\nnetwork via the least squares method. The core innovation of this study is\nembodied in the following three aspects: First, the Fourier basis function\nactivation mechanism is introduced in the hidden layer of neural network, which\nsignificantly enhances the ability of the network to capture multi-frequency\ncomponents of the solution. Secondly, a novel derivative neural network method\nis proposed, which improves the calculation accuracy and efficiency of PIFRNN\nmethod. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is\ndesigned to improve the global search ability and convergence accuracy through\nthe synergistic effect of dynamic parameter adjustment, elitist and mutation\nstrategies. Through a series of numerical experiments, including multiscale\nequations in complex regions, high-order equations, high-dimensional equations\nand nonlinear equations, the validity of SO-PIFRNN is verified. The\nexperimental results affirm that SO-PIFRNN exhibits superior approximation\naccuracy and frequency capture capability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faSO-PIFRNN\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5085\u7acb\u53f6\u6fc0\u6d3b\u3001\u65b0\u7684\u5bfc\u6570\u795e\u7ecf\u7f51\u7edc\u548cMSC-PSO\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6c42\u89e3\u7cbe\u5ea6\u548c\u9891\u7387\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6570\u503c\u6c42\u89e3\u7684\u7cbe\u5ea6\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u673a\u5236\u6765\u63d0\u5347\u6c42\u89e3\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4f18\u5316\u7684\u7269\u7406\u4fe1\u606f\u5085\u7acb\u53f6\u7279\u5f81\u968f\u673a\u795e\u7ecf\u7f51\u7edc\uff08SO-PIFRNN\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u67b6\u6784\uff1a\u5916\u5c42\u4f18\u5316\u5229\u7528\u591a\u7b56\u7565\u534f\u4f5c\u7c92\u5b50\u7fa4\u4f18\u5316\uff08MSC-PSO\uff09\u7b97\u6cd5\u641c\u7d22\u7269\u7406\u4fe1\u606f\u5085\u7acb\u53f6\u7279\u5f81\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u7684\u8d85\u53c2\u6570\uff0c\u5185\u5c42\u4f18\u5316\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u786e\u5b9a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u5c42\u6743\u91cd\u3002", "result": "SO-PIFRNN\u6846\u67b6\u5728\u6c42\u89e3\u591a\u5c3a\u5ea6\u65b9\u7a0b\u3001\u9ad8\u9636\u65b9\u7a0b\u3001\u9ad8\u7ef4\u65b9\u7a0b\u548c\u975e\u7ebf\u6027\u65b9\u7a0b\u7b49\u65b9\u9762\u7684\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSO-PIFRNN\u5177\u6709\u4f18\u8d8a\u7684\u903c\u8fd1\u7cbe\u5ea6\u548c\u9891\u7387\u6355\u6349\u80fd\u529b\u3002", "conclusion": "SO-PIFRNN\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5085\u7acb\u53f6\u57fa\u51fd\u6570\u6fc0\u6d3b\u673a\u5236\u3001\u65b0\u7684\u5bfc\u6570\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u548c\u6df7\u5408\u4f18\u5316\u7684MSC-PSO\u7b97\u6cd5\uff0c\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u903c\u8fd1\u7cbe\u5ea6\u548c\u9891\u7387\u6355\u6349\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u3001\u9ad8\u9636\u3001\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u65b9\u7a0b\u7684\u6570\u503c\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.11516", "categories": ["cs.SI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11516", "abs": "https://arxiv.org/abs/2508.11516", "authors": ["Ming Tang", "Xiaowen Huang", "Jitao Sang"], "title": "When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps", "comment": null, "summary": "Recommender systems increasingly suffer from echo chambers and user\nhomogenization, systemic distortions arising from the dynamic interplay between\nalgorithmic recommendations and human behavior. While prior work has studied\nthese phenomena through the lens of algorithmic bias or social network\nstructure, we argue that the psychological mechanisms of users and the\nclosed-loop interaction between users and recommenders are critical yet\nunderstudied drivers of these emergent effects. To bridge this gap, we propose\nthe Confirmation-Aware Social Dynamic Model which incorporates user psychology\nand social relationships to simulate the actual user and recommender\ninteraction process. Our theoretical analysis proves that echo chambers and\nhomogenization traps, defined respectively as reduced recommendation diversity\nand homogenized user representations, will inevitably occur. We also conduct\nextensive empirical simulations on two real-world datasets and one synthetic\ndataset with five well-designed metrics, exploring the root factors influencing\nthe aforementioned phenomena from three level perspectives: the stochasticity\nand social integration degree of recommender (system-level), the psychological\nmechanisms of users (user-level), and the dataset scale (platform-level).\nFurthermore, we demonstrate four practical mitigation strategies that help\nalleviate echo chambers and user homogenization at the cost of some\nrecommendation accuracy. Our findings provide both theoretical and empirical\ninsights into the emergence and drivers of echo chambers and user\nhomogenization, as well as actionable guidelines for human-centered recommender\ndesign.", "AI": {"tldr": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u56de\u97f3\u5ba4\u548c\u7528\u6237\u540c\u8d28\u5316\u95ee\u9898\uff0c\u7531\u7b97\u6cd5\u548c\u7528\u6237\u884c\u4e3a\u5171\u540c\u5bfc\u81f4\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u89e3\u91ca\u4e86\u8fd9\u4e9b\u73b0\u8c61\u7684\u6210\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u65e5\u76ca\u4e25\u91cd\u7684\u56de\u97f3\u5ba4\u548c\u7528\u6237\u540c\u8d28\u5316\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u7b97\u6cd5\u63a8\u8350\u548c\u4eba\u7c7b\u884c\u4e3a\u4e4b\u95f4\u7684\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u7528\u6237\u5fc3\u7406\u548c\u793e\u4f1a\u5173\u7cfb\u7684\u201c\u786e\u8ba4\u611f\u77e5\u793e\u4f1a\u52a8\u6001\u6a21\u578b\u201d\uff0c\u4ee5\u6a21\u62df\u7528\u6237\u548c\u63a8\u8350\u7cfb\u7edf\u4e4b\u95f4\u7684\u5b9e\u9645\u4ea4\u4e92\u8fc7\u7a0b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u56de\u97f3\u5ba4\uff08\u63a8\u8350\u591a\u6837\u6027\u964d\u4f4e\uff09\u548c\u540c\u8d28\u5316\u9677\u9631\uff08\u7528\u6237\u8868\u5f81\u540c\u8d28\u5316\uff09\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u53d1\u751f\u3002\u901a\u8fc7\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u5f71\u54cd\u8fd9\u4e9b\u73b0\u8c61\u7684\u6839\u672c\u56e0\u7d20\uff0c\u5e76\u5c55\u793a\u4e86\u56db\u79cd\u53ef\u4ee5\u51cf\u8f7b\u56de\u97f3\u5ba4\u548c\u7528\u6237\u540c\u8d28\u5316\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u4f46\u4f1a\u727a\u7272\u4e00\u4e9b\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u56de\u97f3\u5ba4\u548c\u7528\u6237\u540c\u8d28\u5316\u73b0\u8c61\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u89c1\u89e3\uff0c\u5e76\u4e3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b0\u578b\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.10913", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10913", "abs": "https://arxiv.org/abs/2508.10913", "authors": ["Changqing Xu", "Buxuan Song", "Yi Liu", "Xinfang Liao", "Wenbin Zheng", "Yintang Yang"], "title": "SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization", "comment": null, "summary": "Spiking Neural Networks (SNNs), as an emerging biologically inspired\ncomputational model, demonstrate significant energy efficiency advantages due\nto their event-driven information processing mechanism. Compared to traditional\nArtificial Neural Networks (ANNs), SNNs transmit information through discrete\nspike signals, which substantially reduces computational energy consumption\nthrough their sparse encoding approach. However, the multi-timestep computation\nmodel significantly increases inference latency and energy, limiting the\napplicability of SNNs in edge computing scenarios. We propose a single-timestep\nSNN, which enhances accuracy and reduces computational energy consumption in a\nsingle timestep by optimizing spike generation and temporal parameters. We\ndesign a Self-Dropping Neuron mechanism, which enhances information-carrying\ncapacity through dynamic threshold adjustment and selective spike suppression.\nFurthermore, we employ Bayesian optimization to globally search for time\nparameters and obtain an efficient inference mode with a single time step.\nExperimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets\ndemonstrate that, compared to traditional multi-timestep SNNs employing the\nLeaky Integrate-and-Fire (LIF) model, our method achieves classification\naccuracies of 93.72%, 92.20%, and 69.45%, respectively, using only\nsingle-timestep spikes, while maintaining comparable or even superior accuracy.\nAdditionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u8109\u51b2\u751f\u6210\u3001\u5f15\u5165\u81ea\u5254\u9664\u795e\u7ecf\u5143\u673a\u5236\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u65f6\u95f4\u6b65SNN\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "SNNs\u56e0\u5176\u4e8b\u4ef6\u9a71\u52a8\u7684\u4fe1\u606f\u5904\u7406\u673a\u5236\u5728\u80fd\u6548\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5176\u591a\u65f6\u95f4\u6b65\u8ba1\u7b97\u6a21\u578b\u589e\u52a0\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u65f6\u95f4\u6b65SNN\uff0c\u901a\u8fc7\u4f18\u5316\u8109\u51b2\u751f\u6210\u548c\u65f6\u95f4\u53c2\u6570\u6765\u63d0\u9ad8\u5355\u65f6\u95f4\u6b65\u7684\u51c6\u786e\u7387\u548c\u964d\u4f4e\u8ba1\u7b97\u80fd\u8017\u3002\u8bbe\u8ba1\u4e86\u81ea\u5254\u9664\u795e\u7ecf\u5143\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9608\u503c\u8c03\u6574\u548c\u9009\u62e9\u6027\u8109\u51b2\u6291\u5236\u6765\u589e\u5f3a\u4fe1\u606f\u627f\u8f7d\u80fd\u529b\u3002\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5168\u5c40\u641c\u7d22\u65f6\u95f4\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u5355\u65f6\u95f4\u6b65\u7684\u9ad8\u6548\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u91c7\u7528LIF\u6a21\u578b\u7684\u591a\u65f6\u95f4\u6b65SNNs\u76f8\u6bd4\uff0c\u8be5\u5355\u65f6\u95f4\u6b65SNN\u5728\u4fdd\u6301\u53ef\u6bd4\u6216\u66f4\u4f18\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.72%\u300192.20%\u548c69.45%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u751a\u81f3\u66f4\u4f18\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5206\u522b\u964d\u4f4e\u4e8656%\u300121%\u548c22%\u7684\u80fd\u8017\u3002"}}
{"id": "2508.11136", "categories": ["cs.LO", "D.2.4; F.3.1; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.11136", "abs": "https://arxiv.org/abs/2508.11136", "authors": ["Richard Waldinger"], "title": "Automating the Derivation of Unification Algorithms: A Case Study in Deductive Program Synthesis", "comment": "92 pages", "summary": "The unification algorithm has long been a target for program synthesis\nresearch, but a fully automatic derivation remains a research goal. In\ndeductive program synthesis, computer programming is phrased as a task in\ntheorem proving; a declarative specification is expressed in logical form and\npresented to an automatic theorem prover, and a program meeting the\nspecification is extracted from the proof. The correctness of the program is\nsupported by the proof, which also provides an explanation of how the program\nworks. The proof is conducted in an appropriate axiomatic subject-domain\ntheory, which defines the concepts in the specification and the constructs in\nthe target programming language and provides the background knowledge necessary\nto connect them.\n  For the unification proof, we generalize and automate the manual proof\npresented in Manna and Waldinger [1981]. The new program unifies two given\nsymbolic expressions (s-expressions) relative to a given \"environment\"\nsubstitution. The proof establishes the existence of an output substitution\nthat is a most-general idempotent unifier of the given expressions and is an\n\"extension\" of the environment substitution. If no such substitution exists and\nthe expressions are not unifiable, the program is to produce a failure\nindicator.\n  Initially the environment substitution is the empty substitution, which makes\nno replacements at all; during execution of recursive calls, the environment\nsubstitution records the replacements that have been found so far. Our own\nunification algorithm employs an environment, and such algorithms appear in the\nliterature [e.g., Luger and Stubblefield, 1997]. We suspect, in addition to\nbeing more efficient, the three-argument algorithm with an environment is\neasier to synthesize automatically than the two-argument version from the\nManna-Waldinger paper.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u201c\u73af\u5883\u201d\u53c2\u6570\uff0c\u6bd4\u4ee5\u5f80\u7684\u65b9\u6cd5\u66f4\u5bb9\u6613\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u5e76\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u7a0b\u5e8f\u7efc\u5408\u7814\u7a76\u7684\u76ee\u6807\u662f\u5b9e\u73b0\u7edf\u4e00\u7b97\u6cd5\u7684\u5168\u81ea\u52a8\u63a8\u5bfc\u3002\u6f14\u7ece\u7a0b\u5e8f\u7efc\u5408\u5c06\u8ba1\u7b97\u673a\u7f16\u7a0b\u89c6\u4e3a\u4e00\u4e2a\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u6765\u63a8\u5bfc\u6ee1\u8db3\u7ed9\u5b9a\u89c4\u683c\u7684\u7a0b\u5e8f\uff0c\u5e76\u4ece\u8bc1\u660e\u4e2d\u63d0\u53d6\u7a0b\u5e8f\uff0c\u4ece\u800c\u786e\u4fdd\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u91c7\u7528\u4e86\u201c\u73af\u5883\u201d\u53c2\u6570\uff0c\u5e76\u58f0\u79f0\u8fd9\u79cd\u4e09\u53c2\u6570\u7b97\u6cd5\u6bd4Manna-Waldinger\u8bba\u6587\u4e2d\u7684\u4e24\u53c2\u6570\u7248\u672c\u66f4\u5bb9\u6613\u81ea\u52a8\u7efc\u5408\uff0c\u5e76\u4e14\u6548\u7387\u66f4\u9ad8\u3002\u8bc1\u660e\u8fc7\u7a0b\u5efa\u7acb\u4e86\u8f93\u51fa\u66ff\u6362\u7684\u5b58\u5728\u6027\uff0c\u8be5\u66ff\u6362\u662f\u7ed9\u5b9a\u8868\u8fbe\u5f0f\u7684\u6700\u4e00\u822c\u7684\u5e42\u7b49\u7edf\u4e00\u5668\uff0c\u5e76\u4e14\u662f\u73af\u5883\u66ff\u6362\u7684\u201c\u6269\u5c55\u201d\u3002\u5982\u679c\u8868\u8fbe\u5f0f\u4e0d\u53ef\u7edf\u4e00\uff0c\u7a0b\u5e8f\u5c06\u4ea7\u751f\u4e00\u4e2a\u5931\u8d25\u6307\u793a\u7b26\u3002", "result": "\u8be5\u7814\u7a76\u7684\u8bc1\u660e\u786e\u7acb\u4e86\u8f93\u51fa\u66ff\u6362\u7684\u5b58\u5728\u6027\uff0c\u8be5\u66ff\u6362\u662f\u7ed9\u5b9a\u8868\u8fbe\u5f0f\u7684\u6700\u4e00\u822c\u7684\u5e42\u7b49\u7edf\u4e00\u5668\uff0c\u5e76\u4e14\u662f\u73af\u5883\u66ff\u6362\u7684\u201c\u6269\u5c55\u201d\u3002\u5f53\u8868\u8fbe\u5f0f\u4e0d\u53ef\u7edf\u4e00\u65f6\uff0c\u7a0b\u5e8f\u4f1a\u751f\u6210\u4e00\u4e2a\u5931\u8d25\u6307\u793a\u7b26\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6cdb\u5316\u548c\u81ea\u52a8\u5316Manna\u548cWaldinger[1981]\u4e2d\u63d0\u51fa\u7684\u624b\u52a8\u8bc1\u660e\uff0c\u5bf9\u7edf\u4e00\u7b97\u6cd5\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u65e8\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7a0b\u5e8f\u63a8\u5bfc\u3002\u65b0\u7a0b\u5e8f\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u7684\u201c\u73af\u5883\u201d\u66ff\u6362\u6765\u7edf\u4e00\u4e24\u4e2a\u7b26\u53f7\u8868\u8fbe\u5f0f\uff08s-expressions\uff09\u3002"}}
{"id": "2508.11395", "categories": ["cs.ET", "cs.CE", "cs.CR", "cs.CY", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.11395", "abs": "https://arxiv.org/abs/2508.11395", "authors": ["Kevin McNamara", "Rhea Pritham Marpu"], "title": "Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance", "comment": null, "summary": "The global financial system stands at an inflection point. Stablecoins\nrepresent the most significant evolution in banking since the abandonment of\nthe gold standard, positioned to enable \"Banking 2.0\" by seamlessly integrating\ncryptocurrency innovation with traditional finance infrastructure. This\ntransformation rivals artificial intelligence as the next major disruptor in\nthe financial sector. Modern fiat currencies derive value entirely from\ninstitutional trust rather than physical backing, creating vulnerabilities that\nstablecoins address through enhanced stability, reduced fraud risk, and unified\nglobal transactions that transcend national boundaries. Recent developments\ndemonstrate accelerating institutional adoption: landmark U.S. legislation\nincluding the GENIUS Act of 2025, strategic industry pivots from major players\nlike JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive \"Pay\nwith Crypto\" service. Widespread stablecoin implementation addresses critical\nmacroeconomic imbalances, particularly the inflation-productivity gap plaguing\nmodern monetary systems, through more robust and diversified backing\nmechanisms. Furthermore, stablecoins facilitate deregulation and efficiency\ngains, paving the way for a more interconnected international financial system.\nThis whitepaper comprehensively explores how stablecoins are poised to reshape\nbanking, supported by real-world examples, current market data, and analysis of\ntheir transformative potential.", "AI": {"tldr": "\u7a33\u5b9a\u5e01\u662f\u94f6\u884c\u4e1a\u81ea\u91d1\u672c\u4f4d\u5e9f\u9664\u4ee5\u6765\u6700\u91cd\u8981\u7684\u6f14\u8fdb\uff0c\u6709\u671b\u901a\u8fc7\u878d\u5408\u52a0\u5bc6\u8d27\u5e01\u548c\u4f20\u7edf\u91d1\u878d\u6765\u63a8\u52a8\u201c\u94f6\u884c 2.0\u201d\u3002\u5b83\u89e3\u51b3\u4e86\u6cd5\u5b9a\u8d27\u5e01\u7684\u6f0f\u6d1e\uff0c\u5e76\u5f97\u5230\u4e86\u7f8e\u56fd\u7acb\u6cd5\u548c\u4e3b\u8981\u91d1\u878d\u673a\u6784\uff08\u5982\u6469\u6839\u5927\u901a\u548c PayPal\uff09\u7684\u652f\u6301\uff0c\u6709\u6f5c\u529b\u89e3\u51b3\u5b8f\u89c2\u7ecf\u6d4e\u5931\u8861\u95ee\u9898\uff0c\u5e76\u4e3a\u66f4\u4e92\u8054\u7684\u56fd\u9645\u91d1\u878d\u4f53\u7cfb\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u5168\u7403\u91d1\u878d\u4f53\u7cfb\u6b63\u5904\u4e8e\u4e00\u4e2a\u5173\u952e\u7684\u8f6c\u6298\u70b9\uff0c\u7a33\u5b9a\u5e01\u4f5c\u4e3a\u81ea\u91d1\u672c\u4f4d\u5e9f\u9664\u4ee5\u6765\u94f6\u884c\u4e1a\u6700\u91cd\u8981\u7684\u6f14\u8fdb\uff0c\u6709\u671b\u901a\u8fc7\u65e0\u7f1d\u878d\u5408\u52a0\u5bc6\u8d27\u5e01\u521b\u65b0\u4e0e\u4f20\u7edf\u91d1\u878d\u57fa\u7840\u8bbe\u65bd\u6765\u63a8\u52a8\u201c\u94f6\u884c 2.0\u201d\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u7efc\u5408\u5206\u6790\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6848\u4f8b\u3001\u5f53\u524d\u5e02\u573a\u6570\u636e\u4ee5\u53ca\u5bf9\u5176\u53d8\u9769\u6f5c\u529b\u7684\u5206\u6790\u3002", "result": "\u7a33\u5b9a\u5e01\u7684\u5e7f\u6cdb\u5b9e\u65bd\u901a\u8fc7\u66f4\u7a33\u5065\u3001\u66f4\u591a\u6837\u5316\u7684\u652f\u6301\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u7684\u5b8f\u89c2\u7ecf\u6d4e\u5931\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u73b0\u4ee3\u8d27\u5e01\u4f53\u7cfb\u4e2d\u5b58\u5728\u7684\u901a\u80c0-\u751f\u4ea7\u7387\u7f3a\u53e3\u3002\u6b64\u5916\uff0c\u7a33\u5b9a\u5e01\u8fd8\u6709\u52a9\u4e8e\u653e\u677e\u7ba1\u5236\u548c\u63d0\u9ad8\u6548\u7387\uff0c\u4e3a\u66f4\u5177\u4e92\u8054\u6027\u7684\u56fd\u9645\u91d1\u878d\u4f53\u7cfb\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "\u7a33\u5b9a\u5e01\u901a\u8fc7\u589e\u5f3a\u7a33\u5b9a\u6027\u3001\u964d\u4f4e\u6b3a\u8bc8\u98ce\u9669\u548c\u7edf\u4e00\u7684\u5168\u7403\u4ea4\u6613\uff0c\u89e3\u51b3\u4e86\u73b0\u4ee3\u6cd5\u5b9a\u8d27\u5e01\u7684\u6f0f\u6d1e\uff0c\u5e76\u6709\u671b\u5b9e\u73b0"}}
{"id": "2508.11031", "categories": ["eess.SY", "cs.AI", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.11031", "abs": "https://arxiv.org/abs/2508.11031", "authors": ["John W. Sheppard"], "title": "Risk-Based Prognostics and Health Management", "comment": "Appears as Chapter 27 in Realizing Complex Integrated Systems,\n  Anthony P. Ambler and John W. Sheppard (ads.), CRC Press, 2025", "summary": "It is often the case that risk assessment and prognostics are viewed as\nrelated but separate tasks. This chapter describes a risk-based approach to\nprognostics that seeks to provide a tighter coupling between risk assessment\nand fault prediction. We show how this can be achieved using the\ncontinuous-time Bayesian network as the underlying modeling framework.\nFurthermore, we provide an overview of the techniques that are available to\nderive these models from data and show how they might be used in practice to\nachieve tasks like decision support and performance-based logistics. This work\nis intended to provide an overview of the recent developments related to\nrisk-based prognostics, and we hope that it will serve as a tutorial of sorts\nthat will assist others in adopting these techniques.", "AI": {"tldr": "\u672c\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u98ce\u9669\u8bc4\u4f30\u548c\u6545\u969c\u9884\u6d4b\u7684\u57fa\u4e8e\u98ce\u9669\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u8fde\u7eed\u65f6\u95f4\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5c06\u98ce\u9669\u8bc4\u4f30\u548c\u9884\u6d4b\u89c6\u4e3a\u5173\u8054\u4f46\u72ec\u7acb\u4efb\u52a1\u7684\u666e\u904d\u89c2\u5ff5\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5c06\u98ce\u9669\u8bc4\u4f30\u4e0e\u6545\u969c\u9884\u6d4b\u66f4\u7d27\u5bc6\u7ed3\u5408\u7684\u57fa\u4e8e\u98ce\u9669\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u8d1d\u53f6\u65af\u7f51\u7edc\u4f5c\u4e3a\u57fa\u7840\u5efa\u6a21\u6846\u67b6\uff0c\u5e76\u6982\u8ff0\u4e86\u4ece\u4e2d\u5bfc\u51fa\u8fd9\u4e9b\u6a21\u578b\u7684\u6570\u636e\u6280\u672f\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4\u8d1d\u53f6\u65af\u7f51\u7edc\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5e76\u8bf4\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7528\u4e8e\u51b3\u7b56\u652f\u6301\u548c\u57fa\u4e8e\u7ee9\u6548\u7684\u540e\u52e4\u4fdd\u969c\u3002", "conclusion": "\u672c\u7ae0\u5c06\u63cf\u8ff0\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u52a0\u5f3a\u98ce\u9669\u8bc4\u4f30\u4e0e\u6545\u969c\u9884\u6d4b\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2508.10990", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10990", "abs": "https://arxiv.org/abs/2508.10990", "authors": ["Zhiling Wang", "Takeaki Miyamura", "Yoshiki Sunada", "Keika Sunada", "Jesper Ilves", "Kohei Matsuura", "Yasunobu Nakamura"], "title": "Generation of frequency-bin-encoded dual-rail cluster states via time-frequency multiplexing of microwave photonic qubits", "comment": null, "summary": "Cluster states are a class of multi-qubit entangled states with broad\napplications such as quantum metrology and one-way quantum computing. Here, we\npresent a protocol to generate frequency-bin-encoded dual-rail cluster states\nusing a superconducting circuit consisting of a fixed-frequency transmon qubit,\na resonator and a Purcell filter. We implement time-frequency multiplexing by\nsequentially emitting co-propagating microwave photons of distinct frequencies.\nThe frequency-bin dual-rail encoding enables erasure detection based on photon\noccupancy. We characterize the state fidelity using quantum tomography and\nquantify the multipartite entanglement using the metric of localizable\nentanglement. Our implementation achieves a state fidelity exceeding 50$\\%$ for\na cluster state consisting of up to four logical qubits. The localizable\nentanglement remains across chains of up to seven logical qubits. After\ndiscarding the erasure errors, the fidelity exceeds 50% for states with up to\neight logical qubits, and the entanglement persists across chains of up to\neleven qubits. These results highlight the improved robustness of frequency-bin\ndual-rail encoding against photon loss compared to conventional single-rail\nschemes. This work provides a scalable pathway toward high-dimensional\nentangled state generation and photonic quantum information processing in the\nmicrowave domain.", "AI": {"tldr": "\u4f7f\u7528\u8d85\u5bfc\u7535\u8def\u548c\u65f6\u9891\u590d\u7528\u6280\u672f\u751f\u6210\u9891\u7387-\u6bd4\u7279\u53cc\u8f68\u7f16\u7801\u7684\u53cc\u8f68\u96c6\u7fa4\u6001\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc750%\u7684\u4fdd\u771f\u5ea6\u548c\u957f\u94fe\u7ea0\u7f20\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u7f16\u7801\u76f8\u6bd4\u5355\u8f68\u65b9\u6848\u5177\u6709\u66f4\u597d\u7684\u6297\u5149\u5b50\u635f\u8017\u9c81\u68d2\u6027\u3002", "motivation": "\u96c6\u7fa4\u6001\u5728\u91cf\u5b50\u8ba1\u91cf\u548c\u5355\u5411\u91cf\u5b50\u8ba1\u7b97\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5229\u7528\u5305\u542b\u56fa\u5b9a\u9891\u7387\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u3001\u8c10\u632f\u5668\u548cPurcell\u6ee4\u6ce2\u5668\u7684\u8d85\u5bfc\u7535\u8def\uff0c\u901a\u8fc7\u987a\u5e8f\u53d1\u5c04\u4e0d\u540c\u9891\u7387\u7684\u5fae\u6ce2\u5149\u5b50\u6765\u5b9e\u73b0\u65f6\u9891\u590d\u7528\uff0c\u91c7\u7528\u9891\u7387-\u6bd4\u7279\u53cc\u8f68\u7f16\u7801\u5b9e\u73b0\u57fa\u4e8e\u5149\u5b50\u5360\u6709\u7387\u7684\u64e6\u9664\u68c0\u6d4b\u3002", "result": "\u5b9e\u73b0\u4e86\u7531\u591a\u8fbe\u56db\u4e2a\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u7ec4\u6210\u7684\u96c6\u7fa4\u6001\uff0c\u4fdd\u771f\u5ea6\u8d85\u8fc750%\uff1b\u5c40\u57df\u5316\u7ea0\u7f20\u53ef\u8de8\u8d8a\u591a\u8fbe\u4e03\u4e2a\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u94fe\uff1b\u5728\u5254\u9664\u64e6\u9664\u9519\u8bef\u540e\uff0c\u4fdd\u771f\u5ea6\u5bf9\u591a\u8fbe\u516b\u4e2a\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u7684\u96c6\u7fa4\u6001\u8d85\u8fc750%\uff0c\u7ea0\u7f20\u53ef\u8de8\u8d8a\u591a\u8fbe\u5341\u4e00\u4e2a\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u94fe\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u7ef4\u7ea0\u7f20\u6001\u751f\u6210\u548c\u5fae\u6ce2\u57df\u5149\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2508.10999", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10999", "abs": "https://arxiv.org/abs/2508.10999", "authors": ["Yizhi Zhou", "Jie Xu", "Jiawei Xia", "Zechen Hu", "Weizi Li", "Xuan Wang"], "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction", "comment": null, "summary": "This paper presents a novel robust online calibration framework for\nUltra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems\n(VINS). Accurate anchor positioning, a process known as calibration, is crucial\nfor integrating UWB ranging measurements into state estimation. While several\nprior works have demonstrated satisfactory results by using robot-aided systems\nto autonomously calibrate UWB systems, there are still some limitations: 1)\nthese approaches assume accurate robot localization during the initialization\nstep, ignoring localization errors that can compromise calibration robustness,\nand 2) the calibration results are highly sensitive to the initial guess of the\nUWB anchors' positions, reducing the practical applicability of these methods\nin real-world scenarios. Our approach addresses these challenges by explicitly\nincorporating the impact of robot localization uncertainties into the\ncalibration process, ensuring robust initialization. To further enhance the\nrobustness of the calibration results against initialization errors, we propose\na tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,\nmaking the system suitable for practical applications. Simulations and\nreal-world experiments validate the improved accuracy and robustness of our\napproach.", "AI": {"tldr": " This paper presents a robust online calibration framework for UWB anchors in VINS that accounts for localization uncertainties and uses an SKF-based refinement method for improved accuracy and robustness, validated by simulations and experiments.", "motivation": " Accurate UWB anchor positioning (calibration) is crucial for UWB-aided VINS. Prior works have limitations including assumptions of accurate robot localization during initialization and sensitivity of calibration results to initial guesses.", "method": " The paper proposes a novel robust online calibration framework for UWB anchors in UWB-aided VINS. It addresses limitations of prior works by explicitly incorporating robot localization uncertainties into the calibration process and using a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method.", "result": " The proposed approach ensures robust initialization by incorporating localization uncertainties and enhances robustness against initialization errors using an SKF-based online refinement method.", "conclusion": " Simulations and real-world experiments validate the improved accuracy and robustness of our approach."}}
{"id": "2508.11070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u65b9\u53c2\u4e0e\u7684\u7b97\u6cd5\u8ffd\u6eaf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5206\u5339\u914d\u548c\u591a\u5c42\u4f18\u5316\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u5728\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u7684\u540c\u65f6\u4fdd\u8bc1\u4e2a\u4f53\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7b97\u6cd5\u8ffd\u6eaf\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4e3b\u4f53\u5355\u6a21\u578b\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u6d89\u53ca\u591a\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u5229\u76ca\u76f8\u5173\u8005\u7684\u591a\u4e3b\u4f53\u6027\u8d28\uff0c\u4ee5\u53ca\u4e2a\u4f53\u798f\u5229\u4f18\u5316\u5ffd\u89c6\u4e86\u8d44\u6e90\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u7ade\u4e89\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e3b\u4f53\u7b97\u6cd5\u8ffd\u6eaf\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5c06\u591a\u5bf9\u591a\u4ea4\u4e92\u5efa\u6a21\u4e3a\u6709\u5bb9\u91cf\u7684\u52a0\u6743\u4e8c\u5206\u5339\u914d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e09\u5c42\u4f18\u5316\u6846\u67b6\uff08\u57fa\u672c\u5bb9\u91cf\u5339\u914d\u3001\u6700\u5c0f\u5316\u798f\u5229\u5dee\u8ddd\u7684\u6700\u4f18\u5bb9\u91cf\u91cd\u65b0\u5206\u914d\u3001\u4ee5\u53ca\u8003\u8651\u798f\u5229\u6700\u5927\u5316\u4e0e\u5bb9\u91cf\u8c03\u6574\u6210\u672c\u7684\u6210\u672c\u611f\u77e5\u4f18\u5316\uff09\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u591a\u5bf9\u591a\u7b97\u6cd5\u8ffd\u6eaf\uff0c\u5728\u7cfb\u7edf\u8bbe\u7f6e\u4e2d\u4ee5\u6700\u5c0f\u7684\u6539\u52a8\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u793e\u4f1a\u798f\u5229\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u7b97\u6cd5\u8ffd\u6eaf\u4ece\u4e2a\u4f53\u63a8\u8350\u6269\u5c55\u5230\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\uff0c\u4e3a\u5728\u4fdd\u6301\u4e2a\u4f53\u53ef\u64cd\u4f5c\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2508.10922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10922", "abs": "https://arxiv.org/abs/2508.10922", "authors": ["Jianlong Wu", "Wei Liu", "Ye Liu", "Meng Liu", "Liqiang Nie", "Zhouchen Lin", "Chang Wen Chen"], "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model", "comment": "20 pages,6 figures,survey", "summary": "The recent advancement in video temporal grounding (VTG) has significantly\nenhanced fine-grained video understanding, primarily driven by multimodal large\nlanguage models (MLLMs). With superior multimodal comprehension and reasoning\nabilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing\ntraditional fine-tuned methods. They not only achieve competitive performance\nbut also excel in generalization across zero-shot, multi-task, and multi-domain\nsettings. Despite extensive surveys on general video-language understanding,\ncomprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill\nthis gap, this survey systematically examines current research on VTG-MLLMs\nthrough a three-dimensional taxonomy: 1) the functional roles of MLLMs,\nhighlighting their architectural significance; 2) training paradigms, analyzing\nstrategies for temporal reasoning and task adaptation; and 3) video feature\nprocessing techniques, which determine spatiotemporal representation\neffectiveness. We further discuss benchmark datasets, evaluation protocols, and\nsummarize empirical findings. Finally, we identify existing limitations and\npropose promising research directions. For additional resources and details,\nreaders are encouraged to visit our repository at\nhttps://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.", "AI": {"tldr": "This survey provides a comprehensive review of Video Temporal Grounding using Multimodal Large Language Models (VTG-MLLMs), covering their roles, training, and feature processing. It also discusses datasets, evaluation, findings, limitations, and future research directions.", "motivation": "Despite extensive surveys on general video-language understanding, comprehensive reviews specifically addressing VTG-MLLMs remain scarce. This survey aims to fill this gap.", "method": "This survey systematically examines current research on VTG-MLLMs through a three-dimensional taxonomy: 1) the functional roles of MLLMs, highlighting their architectural significance; 2) training paradigms, analyzing strategies for temporal reasoning and task adaptation; and 3) video feature processing techniques, which determine spatiotemporal representation effectiveness. The survey also discusses benchmark datasets, evaluation protocols, and summarizes empirical findings.", "result": "The survey systematically examines current research on VTG-MLLMs, discusses benchmark datasets, evaluation protocols, and summarizes empirical findings. It also identifies existing limitations and proposes promising research directions.", "conclusion": "VTG-MLLMs are gradually surpassing traditional fine-tuned methods, achieving competitive performance and excelling in generalization across zero-shot, multi-task, and multi-domain settings."}}
{"id": "2508.11199", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.11199", "abs": "https://arxiv.org/abs/2508.11199", "authors": ["Raimu Akimoto", "Aina Sumiyoshi", "Takahiro Yamamoto"], "title": "Frequency Dependence of Phonon-Induced Current Noise in ArmchairCarbon Nanotube", "comment": "5 pages, 4 figures", "summary": "We theoretically investigate the frequency dependence of phonon-induced\ncurrent noise in armchair carbon nanotubes at room temperature. Our results\nreveal the emergence of multiple resonance peaks in the high-frequency regime,\nwhich cannot be accounted for by the Lorentzian lineshape expected from a\nMarkovian process. The electron-phonon scattering processes responsible for\nmost of these peaks are identified based on energy and momentum conservation\nlaws and conventional selection rules. However, certain peaks cannot be fully\nexplained within the framework of harmonic phonon scattering, suggesting the\ninvolvement of nontrivial interactions between electrons and anharmonic\nphonons.", "AI": {"tldr": "\u78b3\u7eb3\u7c73\u7ba1\u4e2d\u7684\u7535\u6d41\u566a\u58f0\u5728\u9ad8\u9891\u4e0b\u663e\u793a\u51fa\u591a\u4e2a\u5171\u632f\u5cf0\uff0c\u8fd9\u53ef\u80fd\u4e0e\u975e\u8c10\u58f0\u5b50\u6709\u5173\u3002", "motivation": "\u7814\u7a76\u78b3\u7eb3\u7c73\u7ba1\u4e2d\u7531\u58f0\u5b50\u5f15\u8d77\u7684\u7535\u6d41\u566a\u58f0\u7684\u9891\u7387\u4f9d\u8d56\u6027\uff0c\u7279\u522b\u662f\u9ad8\u9891\u4e0b\u7684\u591a\u5cf0\u73b0\u8c61\uff0c\u4ee5\u7406\u89e3\u7535\u5b50-\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u7814\u7a76\uff0c\u5206\u6790\u4e86\u78b3\u7eb3\u7c73\u7ba1\u5728\u5ba4\u6e29\u4e0b\u58f0\u5b50\u5f15\u8d77\u7684\u7535\u6d41\u566a\u58f0\u7684\u9891\u7387\u4f9d\u8d56\u6027\u3002", "result": "\u5728 \u0410\u0420\u041c\u0427\u0410\u0422\u042b\u0415 \u0443\u0433\u043b\u0435\u0440\u043e\u0434\u043d\u044b\u0445 \u043d\u0430\u043d\u043e\u0442\u0440\u0443\u0431\u043e\u043a \u4e2d\uff0c\u5728 \u0410\u0420\u041c\u0427\u0410\u0422\u042b\u0415 \u0443\u0433\u043b\u0435\u0440\u043e\u0434\u043d\u044b\u0445 \u043d\u0430\u043d\u043e\u0442\u0440\u0443\u0431\u043e\u043a \u4e2d\u89c2\u5bdf\u5230\u591a\u5cf0\u7684\u9891\u7387\u4f9d\u8d56\u6027\uff0c\u8fd9\u8d85\u51fa\u4e86\u6d1b\u4f26\u5179\u7ebf\u578b\u7684\u9884\u671f\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u9ad8\u6e29\u4e0b\uff0c\u78b3\u7eb3\u7c73\u7ba1\u4e2d\u7531\u58f0\u5b50\u5f15\u8d77\u7684\u7535\u6d41\u566a\u58f0\u5728 \u0410\u0420\u041c\u0427\u0410\u0422\u042b\u0415 \u0443\u0433\u043b\u0435\u0440\u043e\u0434\u043d\u044b\u0445 \u043d\u0430\u043d\u043e\u0442\u0440\u0443\u0431\u043e\u043a \u4e2d\u8868\u73b0\u51fa\u591a\u5cf0\u7684\u9891\u7387\u4f9d\u8d56\u6027\uff0c\u5176\u4e2d\u4e00\u4e9b\u5cf0\u7684\u51fa\u73b0\u53ef\u80fd\u4e0e\u7535\u5b50-\u975e\u8c10\u58f0\u5b50\u7684\u76f8\u4e92\u4f5c\u7528\u6709\u5173\u3002"}}
{"id": "2508.10906", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10906", "abs": "https://arxiv.org/abs/2508.10906", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis.", "AI": {"tldr": "PersonaTwin\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u7ef4\u5ea6\u7528\u6237\u6570\u636e\uff0c\u6784\u5efa\u80fd\u591f\u8fdb\u884c\u771f\u5b9e\u4e14\u5305\u542b\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u6a21\u62df\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4e2a\u6027\u5316\u6570\u5b57\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u5728\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u8fd1\u4f3c\u65b9\u9762\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u7528\u6237\u7684\u591a\u7ef4\u5ea6\u7ec6\u5fae\u5dee\u522b\u3002", "method": "PersonaTwin\u662f\u4e00\u4e2a\u591a\u5c42\u63d0\u793a\u6761\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u884c\u4e3a\u548c\u5fc3\u7406\u6d4b\u91cf\u6570\u636e\u6765\u6784\u5efa\u81ea\u9002\u5e94\u7684\u6570\u5b57\u5b6a\u751f\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u4fdd\u771f\u5ea6\u4e0a\u80fd\u4e0e\u201c\u795e\u8c15\u201d\u8bbe\u7f6e\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u5728PersonaTwin\u6784\u5efa\u7684\u6570\u5b57\u5b6a\u751f\u4e0a\u8bad\u7ec3\u7684\u4e0b\u6e38\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u548c\u516c\u5e73\u6027\u6307\u6807\u4e0a\uff0c\u53ef\u4ee5\u8fd1\u4f3c\u4e8e\u5728\u771f\u5b9e\u4e2a\u4f53\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8fd9\u5728\u57fa\u4e8eGPT-4o\u548cLlama\u7684\u6a21\u578b\u4e0a\u5747\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u6a21\u62df\u65b9\u9762\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u4e2a\u4f53\u7528\u6237\u7684\u591a\u7ef4\u5ea6\u7ec6\u5fae\u5dee\u522b\u3002PersonaTwin\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u884c\u4e3a\u548c\u5fc3\u7406\u6d4b\u91cf\u6570\u636e\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPersonaTwin\u5728\u6a21\u62df\u4fdd\u771f\u5ea6\u4e0a\u80fd\u4e0e\u201c\u795e\u8c15\u201d\u8bbe\u7f6e\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u57fa\u4e8ePersonaTwin\u8bad\u7ec3\u7684\u4e0b\u6e38\u6a21\u578b\u5728\u9884\u6d4b\u548c\u516c\u5e73\u6027\u6307\u6807\u4e0a\u4e0e\u57fa\u4e8e\u4e2a\u4f53\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u65e0\u8bba\u662f\u5728GPT-4o\u8fd8\u662fLlama\u6a21\u578b\u4e0a\u3002"}}
{"id": "2508.10948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10948", "abs": "https://arxiv.org/abs/2508.10948", "authors": ["Shruthan Radhakrishna", "Soham Parikh", "Gopal Sarda", "Anil Turkkan", "Quaizar Vohra", "Raymond Li", "Dhruv Jhamb", "Kelechi Ogueji", "Aanjaneya Shukla", "Oluwanifemi Bamgbose", "Toby Liang", "Luke Kumar", "Oleksiy Ostapenko", "Shiva Krishna Reddy Malay", "Aman Tiwari", "Tara Bogavelli", "Vikas Yadav", "Jash Mehta", "Saloni Mittal", "Akshay Kalkunte", "Pulkit Pattnaik", "Khalil Slimi", "Anirudh Sreeram", "Jishnu Nair", "Akintunde Oladipo", "Shashank Maiya", "Khyati Mahajan", "Rishabh Maheshwary", "Masoud Hashemi", "Sai Rajeswar Mudumba", "Sathwik Tejaswi Madhusudhan", "Torsten Scholak", "Sebastien Paquet", "Sagar Davasam", "Srinivas Sunkara"], "title": "Apriel-Nemotron-15B-Thinker", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable reasoning\ncapabilities across domains like code, math and other enterprise tasks, their\nsignificant memory and computational costs often preclude their use in\npractical enterprise settings. To this end, we introduce\nApriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow\nApriel SLM series that achieves performance against medium sized\nstate-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while\nmaintaining only half the memory footprint of those alternatives.\nApriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline\nincluding 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised\nFine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive\nevaluations across a diverse suite of benchmarks consistently demonstrate that\nour Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its\n32-billion parameter counterparts, despite being less than half their size.", "AI": {"tldr": "Apriel-Nemotron-15B-Thinker\u662f\u4e00\u4e2a150\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6027\u80fd\u4e0e320\u4ebf\u53c2\u6570\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u5185\u5b58\u5360\u7528\u51cf\u534a\uff0c\u9002\u7528\u4e8e\u4f01\u4e1a\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u4e2d\u56e0\u9ad8\u6602\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u800c\u53d7\u5230\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u5305\u62ec\u57fa\u7840\u6a21\u578b\u589e\u91cf\u8bad\u7ec3\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u53ca\u4f7f\u7528GRPO\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u5185\u7684\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7efc\u5408\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cApriel-Nemotron-15B-Thinker\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u540c\u7c7b320\u4ebf\u53c2\u6570\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u4f46\u6a21\u578b\u5c3a\u5bf8\u5374\u53ea\u6709\u540e\u8005\u7684\u4e00\u534a\u3002", "conclusion": "Apriel-Nemotron-15B-Thinker\u5728\u4e0e320\u4ebf\u53c2\u6570\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u4e0b\uff0c\u5185\u5b58\u5360\u7528\u5374\u53ea\u6709\u4ed6\u4eec\u7684\u4e00\u534a\uff0c\u5c3d\u7ba1\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3a150\u4ebf\u3002"}}
{"id": "2508.11266", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11266", "abs": "https://arxiv.org/abs/2508.11266", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets", "comment": "8 Pages, Submitted to RASSE 2025", "summary": "Alternative assets such as mines, power plants, or infrastructure projects\nare often large, heterogeneous bundles of resources, rights, and outputs whose\nvalue is difficult to trade or fractionalize under traditional frameworks. This\npaper proposes a novel two-tier tokenization architecture to enhance the\nliquidity and transparency of such complex assets. We introduce the concepts of\nElement Tokens and Everything Tokens: elemental tokens represent standardized,\nfully collateralized components of an asset (e.g., outputs, rights, or\ncredits), while an everything token represents the entire asset as a fixed\ncombination of those elements. The architecture enables both fine-grained\npartial ownership and integrated whole-asset ownership through a system of\ntwo-way convertibility. We detail the design and mechanics of this system,\nincluding an arbitrage mechanism that keeps the price of the composite token\naligned with the net asset value of its constituents. Through illustrative\nexamples in the energy and industrial sectors, we demonstrate that our approach\nallows previously illiquid, high-value projects to be fractionalized and traded\nakin to stocks or exchange-traded funds (ETFs). We discuss the benefits for\ninvestors and asset owners, such as lower entry barriers, improved price\ndiscovery, and flexible financing, as well as the considerations for\nimplementation and regulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5143\u7d20\u4ee3\u5e01\u548c\u4e07\u7269\u4ee3\u5e01\u8fdb\u884c\u53cc\u5c42\u6807\u8bb0\u5316\u6765\u63d0\u9ad8\u53e6\u7c7b\u8d44\u4ea7\uff08\u5982\u77ff\u5c71\u3001\u53d1\u7535\u5382\u6216\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\uff09\u6d41\u52a8\u6027\u7684\u67b6\u6784\uff0c\u4ece\u800c\u5b9e\u73b0\u90e8\u5206\u548c\u6574\u4f53\u6240\u6709\u6743\uff0c\u5e76\u4f7f\u8fd9\u4e9b\u8d44\u4ea7\u80fd\u591f\u50cf\u80a1\u7968\u6216ETF\u4e00\u6837\u8fdb\u884c\u4ea4\u6613\u3002", "motivation": "\u53e6\u7c7b\u8d44\u4ea7\uff08\u4f8b\u5982\uff0c\u77ff\u5c71\u3001\u53d1\u7535\u5382\u6216\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\uff09\u901a\u5e38\u662f\u8d44\u6e90\u3001\u6743\u5229\u548c\u4ea7\u51fa\u7684\u5f02\u6784\u675f\uff0c\u5176\u4ef7\u503c\u5728\u4f20\u7edf\u6846\u67b6\u4e0b\u96be\u4ee5\u4ea4\u6613\u6216\u5206\u6570\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u5c42\u6807\u8bb0\u5316\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u6b64\u7c7b\u590d\u6742\u8d44\u4ea7\u7684\u6d41\u52a8\u6027\u548c\u900f\u660e\u5ea6\u3002\u5f15\u5165\u4e86\u5143\u7d20\u4ee3\u5e01\u548c\u4e07\u7269\u4ee3\u5e01\u7684\u6982\u5ff5\uff1a\u5143\u7d20\u4ee3\u5e01\u4ee3\u8868\u8d44\u4ea7\u7684\u6807\u51c6\u5316\u7684\u3001\u5168\u989d\u62b5\u62bc\u7684\u7ec4\u6210\u90e8\u5206\uff08\u4f8b\u5982\uff0c\u4ea7\u51fa\u3001\u6743\u5229\u6216\u4fe1\u8d37\uff09\uff0c\u800c\u4e07\u7269\u4ee3\u5e01\u4ee3\u8868\u6574\u4e2a\u8d44\u4ea7\u4f5c\u4e3a\u90a3\u4e9b\u5143\u7d20\u7684\u56fa\u5b9a\u7ec4\u5408\u3002\u8be5\u67b6\u6784\u80fd\u591f\u901a\u8fc7\u53cc\u5411\u53ef\u5151\u6362\u7cfb\u7edf\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u90e8\u5206\u6240\u6709\u6743\u548c\u7efc\u5408\u7684\u5168\u8d44\u4ea7\u6240\u6709\u6743\u3002\u6211\u4eec\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8be5\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u673a\u5236\uff0c\u5305\u62ec\u4e00\u4e2a\u4fdd\u6301\u590d\u5408\u4ee3\u5e01\u4ef7\u683c\u4e0e\u5176\u7ec4\u6210\u90e8\u5206\u51c0\u8d44\u4ea7\u503c\u4e00\u81f4\u7684\u5957\u5229\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5728\u80fd\u6e90\u548c\u5de5\u4e1a\u9886\u57df\u7684\u793a\u4f8b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5141\u8bb8\u4e4b\u524d\u7f3a\u4e4f\u6d41\u52a8\u6027\u7684\u9ad8\u4ef7\u503c\u9879\u76ee\u50cf\u80a1\u7968\u6216\u4ea4\u6613\u6240\u4ea4\u6613\u57fa\u91d1\uff08ETFs\uff09\u4e00\u6837\u8fdb\u884c\u5206\u6570\u5316\u548c\u4ea4\u6613\u3002", "conclusion": "\u8be5\u4f53\u7cfb\u7ed3\u6784\u5141\u8bb8\u4e4b\u524d\u7f3a\u4e4f\u6d41\u52a8\u6027\u3001\u9ad8\u4ef7\u503c\u7684\u9879\u76ee\u50cf\u80a1\u7968\u6216\u4ea4\u6613\u6240\u4ea4\u6613\u57fa\u91d1\uff08ETFs\uff09\u4e00\u6837\u8fdb\u884c\u5206\u6570\u5316\u548c\u4ea4\u6613\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u5bf9\u6295\u8d44\u8005\u548c\u8d44\u4ea7\u6240\u6709\u8005\uff08\u4f8b\u5982\uff0c\u66f4\u4f4e\u7684\u8fdb\u5165\u58c1\u5792\u3001\u6539\u8fdb\u7684\u4ef7\u683c\u53d1\u73b0\u548c\u7075\u6d3b\u7684\u878d\u8d44\uff09\u7684\u597d\u5904\uff0c\u4ee5\u53ca\u5bf9\u5b9e\u65bd\u548c\u76d1\u7ba1\u7684\u8003\u8651\u3002"}}
{"id": "2508.11203", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "AI": {"tldr": "StyleMM \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u6839\u636e\u6587\u672c\u63cf\u8ff0\u6784\u5efa\u98ce\u683c\u5316 3DMM\uff0c\u4fdd\u6301\u8eab\u4efd\u5c5e\u6027\uff0c\u5e76\u63d0\u4f9b\u5bf9\u5f62\u72b6\u3001\u8868\u60c5\u548c\u7eb9\u7406\u7684\u63a7\u5236\u3002", "motivation": "\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u53ef\u4ee5\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u6587\u672c\u63cf\u8ff0\u6765\u6784\u5efa\u98ce\u683c\u5316 3DMM \u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u4eba\u8138\u98ce\u683c\u7684\u63a7\u5236\u3002", "method": "StyleMM \u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u7f51\u683c\u53d8\u5f62\u7f51\u7edc\u548c\u7eb9\u7406\u751f\u6210\u5668\u6765\u6784\u5efa\u98ce\u683c\u5316\u7684 3DMM\u3002\u5b83\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\u6765\u751f\u6210\u98ce\u683c\u5316\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u4fdd\u7559\u9762\u90e8\u5c5e\u6027\u7684\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u8eab\u4efd\u7684\u4e00\u81f4\u6027\u3002", "result": "StyleMM \u5728\u8eab\u4efd\u7ea7\u522b\u7684\u4eba\u8138\u591a\u6837\u6027\u548c\u98ce\u683c\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "StyleMM \u80fd\u591f\u751f\u6210\u5177\u6709\u98ce\u683c\u5316\u4eba\u8138\u7684 3DMM\uff0c\u80fd\u591f\u4fdd\u6301\u8eab\u4efd\u3001\u9762\u90e8\u5bf9\u9f50\u548c\u8868\u60c5\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u5f62\u72b6\u3001\u8868\u60c5\u548c\u7eb9\u7406\u7684\u663e\u5f0f\u63a7\u5236\u3002"}}
{"id": "2508.11142", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11142", "abs": "https://arxiv.org/abs/2508.11142", "authors": ["Zixiao Shi", "Qihao Li", "Himani Mishra", "Desheng Ma", "H\u00e9ctor D. Abru\u00f1a", "David A. Muller"], "title": "Electron Ptychography Images Hydrogen Atom Superlattices and 3D Inhomogeneities in Palladium Hydride Nanoparticles", "comment": "5 figures, 19 SI figures", "summary": "When hydrogen atoms occupy interstitial sites in metal lattices, they form\nmetal hydrides (MHx), whose structural and electronic properties can differ\nsignificantly from the host metals. Owing to the small size of hydrogen atom\nand its unique interactions with the host metal, MHx is of broad interest in\nboth fundamental science and technological applications. Determining where the\nhydrogen is located within the MHx, and whether it orders on the partially\noccupied interstitial sites is crucial for predicting and understanding the\nresultant physical and electronic properties of the hydride. Directly imaging\nhydrogen within a host material remains a major challenge due to its weak\ninteraction with X-rays and electrons in conventional imaging techniques. Here,\nwe employ electron ptychography, a scanning transmission electron microscopy\ntechnique, to image the three-dimensional (3D) distribution of H atoms in\nPalladium hydrides (PdHx) nanocubes, one of the most studied and industrially\nrelevant MHx materials. We observe an unexpected one-dimensional superlattice\nordering of hydrogen within the PdHx nanocubes and 3D hydrogen clustering in\nlocalized regions within PdHx nanocubes, revealing spatial heterogeneity in\nmetal hydride nanoparticles previously inaccessible by other methods.", "AI": {"tldr": "\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6280\u672f\u9996\u6b21\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u6210\u50cf\u94af\u6c22\u5316\u7269\u4e2d\u7684\u6c22\u539f\u5b50\uff0c\u53d1\u73b0\u6c22\u539f\u5b50\u7684\u4e00\u7ef4\u8d85\u6676\u683c\u6392\u5e8f\u548c\u4e09\u7ef4\u56e2\u7c07\u73b0\u8c61\u3002", "motivation": "\u91d1\u5c5e\u6c22\u5316\u7269\uff08MHx\uff09\u5728\u57fa\u7840\u79d1\u5b66\u548c\u6280\u672f\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u786e\u5b9a\u6c22\u539f\u5b50\u5728MHx\u4e2d\u7684\u4f4d\u7f6e\u4ee5\u53ca\u5176\u662f\u5426\u5728\u90e8\u5206\u5360\u636e\u7684\u95f4\u9699\u4f4d\u7f6e\u4e0a\u8fdb\u884c\u6392\u5e8f\uff0c\u5bf9\u4e8e\u9884\u6d4b\u548c\u7406\u89e3\u6c22\u5316\u7269 resultant \u7684\u7269\u7406\u548c\u7535\u5b50\u6027\u8d28\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u6c22\u539f\u5b50\u5c3a\u5bf8\u5c0f\u4e14\u4e0e\u5bbf\u4e3b\u91d1\u5c5e\u7684\u76f8\u4e92\u4f5c\u7528\u72ec\u7279\uff0c\u4f20\u7edf\u6210\u50cf\u6280\u672f\u96be\u4ee5\u76f4\u63a5\u6210\u50cf\u6c22\u539f\u5b50\u3002", "method": "\u4f7f\u7528\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6280\u672f\uff08electron ptychography\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u626b\u63cf\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u6280\u672f\uff0c\u7528\u4e8e\u6210\u50cf\u94af\u6c22\u5316\u7269\uff08PdHx\uff09\u7eb3\u7c73\u7acb\u65b9\u4f53\u4e2d\u7684\u6c22\u539f\u5b50\u4e09\u7ef4\u5206\u5e03\u3002", "result": "\u9996\u6b21\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u89c2\u5bdf\u5230\u94af\u6c22\u5316\u7269\uff08PdHx\uff09\u7eb3\u7c73\u7acb\u65b9\u4f53\u4e2d\u7684\u6c22\u539f\u5b50\u5206\u5e03\uff0c\u53d1\u73b0\u4e86\u610f\u60f3\u4e0d\u5230\u7684\u6c22\u539f\u5b50\u4e00\u7ef4\u8d85\u6676\u683c\u6392\u5e8f\u548c\u4e09\u7ef4\u6c22\u539f\u5b50\u56e2\u7c07\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u5148\u524d\u5176\u4ed6\u65b9\u6cd5\u65e0\u6cd5\u89e6\u53ca\u7684\u91d1\u5c5e\u6c22\u5316\u7269\u7eb3\u7c73\u7c92\u5b50\u4e2d\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4f7f\u7528\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6280\u672f\u9996\u6b21\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u6210\u50cf\u4e86\u94af\u6c22\u5316\u7269\uff08PdHx\uff09\u7eb3\u7c73\u7acb\u65b9\u4f53\u4e2d\u7684\u6c22\u539f\u5b50\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u6c22\u539f\u5b50\u7684\u4e00\u7ef4\u8d85\u6676\u683c\u6392\u5e8f\u548c\u5c40\u90e8\u533a\u57df\u7684\u4e09\u7ef4\u56e2\u7c07\u73b0\u8c61\uff0c\u8fd9\u4e3a\u4e86\u89e3\u91d1\u5c5e\u6c22\u5316\u7269\u7684\u7269\u7406\u548c\u7535\u5b50\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.11130", "categories": ["cs.DS", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.11130", "abs": "https://arxiv.org/abs/2508.11130", "authors": ["Sarah Cannon", "Wesley Pegden", "Jamie Tucker-Foltz"], "title": "Sampling tree-weighted partitions without sampling trees", "comment": null, "summary": "This paper gives a new algorithm for sampling tree-weighted partitions of a\nlarge class of planar graphs. Formally, the tree-weighted distribution on\n$k$-partitions of a graph weights $k$-partitions proportional to the product of\nthe number of spanning trees of each partition class. Recent work on problems\nin computational redistricting analysis has driven special interest in the\nconditional distribution where all partition classes have the same size\n(balanced partitions). One class of Markov chains in wide use aims to sample\nfrom balanced tree-weighted $k$-partitions using a sampler for balanced\ntree-weighted 2-partitions. Previous implementations of this 2-partition\nsampler would draw a random spanning tree and check whether it contains an edge\nwhose removal produces a balanced 2-component forest; if it does, this\n2-partition is accepted, otherwise the algorithm rejects and repeats. In\npractice, this is a significant computational bottleneck.\n  We show that in fact it is possible to sample from the balanced tree-weighted\n2-partition distribution directly, without first sampling a spanning tree; the\nacceptance and rejection rates are the same as in previous samplers. We prove\nthat on a wide class of planar graphs encompassing network structures typically\narising from the geographic data used in computational redistricting, our\nalgorithm takes expected linear time $O(n)$. Notably, this is asymptotically\nfaster than the best known method to generate random trees, which is $O(n\n\\log^2 n)$ for approximate sampling and $O(n^{1 + \\log \\log \\log n / \\log \\log\nn})$ for exact sampling. Additionally, we show that a variant of our algorithm\nalso gives a speedup to $O(n \\log n)$ for exact sampling of uniformly random\ntrees on these families of graphs, improving the bounds for both exact and\napproximate sampling.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u5e73\u8861\u6811\u52a0\u6743\u4e8c\u5206\u56fe\u4e2d\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8ba1\u7b97\u91cd\u65b0\u5212\u5206\u5206\u6790\u4e2d\u7684\u5e73\u9762\u56fe\u65f6\u9047\u5230\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u8be5\u7b97\u6cd5\u5728\u9884\u671f\u8fd0\u884c\u65f6\u95f4\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u4e14\u5728\u7cbe\u786e\u91c7\u6837\u5747\u5300\u968f\u673a\u6811\u65b9\u9762\u4e5f\u6709\u6539\u8fdb\u3002", "motivation": "\u8ba1\u7b97\u91cd\u65b0\u5212\u5206\u5206\u6790\u4e2d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u5e73\u8861\u6811\u52a0\u6743k-\u5206\u533a\uff08\u6240\u6709\u5206\u533a\u7c7b\u5177\u6709\u76f8\u540c\u5927\u5c0f\uff09\u7684\u6761\u4ef6\u5206\u5e03\u7684\u7814\u7a76\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u91c7\u6837\u5e73\u8861\u6811\u52a0\u67432-\u5206\u533a\u65f6\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0c\u5373\u9700\u8981\u5148\u91c7\u6837\u751f\u6210\u6811\uff0c\u7136\u540e\u68c0\u67e5\u5176\u662f\u5426\u5305\u542b\u53ef\u79fb\u9664\u4ee5\u4ea7\u751f\u5e73\u88612-\u5206\u91cf\u68ee\u6797\u7684\u8fb9\uff0c\u8fd9\u5bfc\u81f4\u7b97\u6cd5\u9700\u8981\u4e0d\u65ad\u62d2\u7edd\u548c\u91cd\u590d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u5e73\u8861\u6811\u52a0\u6743\u4e8c\u5206\u56fe\u4e2d\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u5148\u91c7\u6837\u751f\u6210\u6811\uff0c\u800c\u662f\u76f4\u63a5\u751f\u6210\u6837\u672c\u3002", "result": "\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u53ef\u4ee5\u76f4\u63a5\u4ece\u5e73\u8861\u6811\u52a0\u6743\u4e8c\u5206\u56fe\u4e2d\u91c7\u6837\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u5904\u7406\u5e7f\u6cdb\u7684\u5e73\u9762\u56fe\uff08\u5305\u62ec\u8ba1\u7b97\u91cd\u65b0\u5212\u5206\u5206\u6790\u4e2d\u5e38\u89c1\u7684\u7f51\u7edc\u7ed3\u6784\uff09\u65f6\uff0c\u9884\u671f\u8fd0\u884c\u65f6\u95f4\u4e3a\u7ebf\u6027\u65f6\u95f4 O(n)\u3002\u8fd9\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u3002\u540c\u65f6\uff0c\u8be5\u7b97\u6cd5\u7684\u53d8\u4f53\u5728\u7cbe\u786e\u91c7\u6837\u5747\u5300\u968f\u673a\u6811\u65b9\u9762\u4e5f\u80fd\u8fbe\u5230 O(n log n) \u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u5e73\u8861\u6811\u52a0\u6743\u4e8c\u5206\u56fe\u4e2d\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u907f\u514d\u4e86\u5148\u91c7\u6837\u751f\u6210\u6811\u518d\u8fdb\u884c\u68c0\u67e5\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u8be5\u7b97\u6cd5\u5728\u5904\u7406\u8ba1\u7b97\u91cd\u65b0\u5212\u5206\u5206\u6790\u4e2d\u5e38\u89c1\u7684\u5e73\u9762\u56fe\u65f6\uff0c\u9884\u671f\u8fd0\u884c\u65f6\u95f4\u4e3a\u7ebf\u6027\u65f6\u95f4 O(n)\uff0c\u8fd9\u6bd4\u73b0\u6709\u751f\u6210\u968f\u673a\u6811\u7684\u65b9\u6cd5\uff08\u8fd1\u4f3c\u91c7\u6837\u4e3a O(n log^2 n)\uff0c\u7cbe\u786e\u91c7\u6837\u4e3a O(n^{1 + log log log n / log log n})\uff09\u66f4\u5feb\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u7684\u53d8\u4f53\u8fd8\u80fd\u4ee5 O(n log n) \u7684\u65f6\u95f4\u8fdb\u884c\u7cbe\u786e\u91c7\u6837\uff0c\u6539\u8fdb\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u91c7\u6837\u7684\u8fb9\u754c\u3002"}}
{"id": "2508.11542", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.11542", "abs": "https://arxiv.org/abs/2508.11542", "authors": ["Nicole Aretz", "Karen Willcox"], "title": "Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models", "comment": null, "summary": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000.", "AI": {"tldr": "\u5d4c\u5957OpInf\u901a\u8fc7\u5229\u7528\u964d\u9636\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u7269\u7406\u4fe1\u606f\u964d\u9636\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u521d\u59cb\u731c\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u53ef\u6bd4\u7684\u79bb\u7ebf\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u91cd\u5efa\u8bef\u5dee\uff0c\u5e76\u5df2\u5728\u70ed\u4f20\u5bfc\u548c\u51b0\u76d6\u6a21\u578b\u7b49\u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u5229\u7528\u964d\u9636\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u6765\u6539\u8fdbOpInf\u5b66\u4e60\u95ee\u9898\uff0c\u4f18\u5148\u8003\u8651\u4e3b\u5bfc\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u5728\u5b66\u4e60\u7269\u7406\u4fe1\u606f\u964d\u9636\u6a21\u578b\u65f6\u83b7\u5f97\u66f4\u5c0f\u7684\u5feb\u7167\u91cd\u5efa\u8bef\u5dee\uff0c\u5e76\u652f\u6301\u52a8\u6001\u57fa\u548c\u6a21\u578b\u5f62\u5f0f\u66f4\u65b0\u7b49\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u3001\u5d4c\u5957\u7684\u7b97\u5b50\u63a8\u7406\uff08OpInf\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u52a8\u529b\u7cfb\u7edf\u7684\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u7269\u7406\u4fe1\u606f\u7ea6\u675f\u7684\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u964d\u9636\u7a7a\u95f4\u4e2d\u56fa\u6709\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3aOpInf\u5b66\u4e60\u95ee\u9898\u8fed\u4ee3\u5730\u6784\u5efa\u521d\u59cb\u731c\u6d4b\uff0c\u4f18\u5148\u8003\u8651\u4e3b\u8981\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u4e0e\u6807\u51c6OpInf\u76f8\u6bd4\uff0c\u5d4c\u5957OpInf\u5728\u4e09\u6b21\u70ed\u4f20\u5bfc\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u56db\u500d\u5c0f\u7684\u8bef\u5dee\uff0c\u4e14\u79bb\u7ebf\u65f6\u95f4\u76f8\u5f53\u3002\u5728\u683c\u9675\u5170\u51b0\u76d6\u7684\u53c2\u6570\u5316\u6a21\u578b\u4e0a\uff0c\u5b66\u4e60\u5230\u7684ROM\u5e73\u5747\u8bef\u5dee\u4e3a3%\uff0c\u8ba1\u7b97\u52a0\u901f\u56e0\u5b50\u5927\u4e8e19,000\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u6b21\u70ed\u4f20\u5bfc\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6OpInf\u5c0f\u56db\u500d\u7684\u8bef\u5dee\uff0c\u5e76\u4e14\u79bb\u7ebf\u65f6\u95f4\u76f8\u5f53\u3002\u5728\u683c\u9675\u5170\u51b0\u76d6\u7684\u53c2\u6570\u5316\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u7684ROM\u5e73\u5747\u8bef\u5dee\u4e3a3%\uff0c\u8ba1\u7b97\u52a0\u901f\u56e0\u5b50\u8d85\u8fc719,000\u3002"}}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u793e\u4ea4\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u5206\u6790\u548c\u8de8\u6a21\u6001\u65f6\u95f4\u5efa\u6a21\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u8de8\u65f6\u95f4\u4e0e\u8de8\u6a21\u6001\u7684\u4e8b\u4ef6\u7ea7\u7ed3\u6784\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u793e\u4ea4\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "E-CaTCH\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u5e16\u5b50\u5f62\u6210\u4f2a\u4e8b\u4ef6\uff0c\u7136\u540e\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5bf9\u9f50\u3001\u8d8b\u52bf\u611f\u77e5LSTM\u8fdb\u884c\u65f6\u95f4\u6f14\u5316\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u7c7b\u522b\u52a0\u6743\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u56f0\u96be\u6837\u672c\u6316\u6398\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u4fc3\u8fdb\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "E-CaTCH\u5728Fakeddit\u3001IND\u548cCOVID-19 MISINFOGRAPH\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "E-CaTCH\u6846\u67b6\u5728Fakeddit\u3001IND\u548cCOVID-19 MISINFOGRAPH\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.10915", "categories": ["cs.NE", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10915", "abs": "https://arxiv.org/abs/2508.10915", "authors": ["Jacob Clouse", "Thomas Ramsey", "Samitha Somathilaka", "Nicholas Kleinsasser", "Sangjin Ryu", "Sasitharan Balasubramaniam"], "title": "Insect-Wing Structured Microfluidic System for Reservoir Computing", "comment": null, "summary": "As the demand for more efficient and adaptive computing grows,\nnature-inspired architectures offer promising alternatives to conventional\nelectronic designs. Microfluidic platforms, drawing on biological forms and\nfluid dynamics, present a compelling foundation for low-power, high-resilience\ncomputing in environments where electronics are unsuitable. This study explores\na hybrid reservoir computing system based on a dragonfly-wing inspired\nmicrofluidic chip, which encodes temporal input patterns as fluid interactions\nwithin the micro channel network.\n  The system operates with three dye-based inlet channels and three\ncamera-monitored detection areas, transforming discrete spatial patterns into\ndynamic color output signals. These reservoir output signals are then modified\nand passed to a simple and trainable readout layer for pattern classification.\nUsing a combination of raw reservoir outputs and synthetically generated\noutputs, we evaluated system performance, system clarity, and data efficiency.\nThe results demonstrate consistent classification accuracies up to $91\\%$, even\nwith coarse resolution and limited training data, highlighting the viability of\nthe microfluidic reservoir computing.", "AI": {"tldr": "\u53d7\u873b\u8713\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\u53ef\u7528\u4f5c\u4f4e\u529f\u8017\u3001\u9ad8\u5f39\u6027\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u51c6\u786e\u7387\u9ad8\u8fbe 91%\u3002", "motivation": "\u968f\u7740\u5bf9\u66f4\u9ad8\u6548\u3001\u66f4\u9002\u5e94\u6027\u5f3a\u7684\u8ba1\u7b97\u9700\u6c42\u7684\u589e\u957f\uff0c\u53d7\u81ea\u7136\u542f\u53d1\u7684\u8ba1\u7b97\u67b6\u6784\u4e3a\u4f20\u7edf\u7535\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5fae\u6d41\u63a7\u5e73\u53f0\u5229\u7528\u751f\u7269\u5f62\u6001\u548c\u6d41\u4f53\u52a8\u529b\u5b66\uff0c\u4e3a\u7535\u5b50\u8bbe\u5907\u4e0d\u9002\u7528\u7684\u73af\u5883\u4e2d\u7684\u4f4e\u529f\u8017\u3001\u9ad8\u5f39\u6027\u8ba1\u7b97\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u57fa\u4e8e\u873b\u8713\u7fc5\u8180\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\u7684\u6df7\u5408\u50a8\u5c42\u8ba1\u7b97\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u901a\u9053\u5185\u7684\u6d41\u4f53\u76f8\u4e92\u4f5c\u7528\u5bf9\u65f6\u95f4\u8f93\u5165\u6a21\u5f0f\u8fdb\u884c\u7f16\u7801\u3002\u8be5\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u67d3\u6599\u8f93\u5165\u901a\u9053\u548c\u4e09\u4e2a\u6444\u50cf\u5934\u76d1\u63a7\u7684\u68c0\u6d4b\u533a\u57df\uff0c\u5c06\u79bb\u6563\u7684\u7a7a\u95f4\u6a21\u5f0f\u8f6c\u6362\u4e3a\u52a8\u6001\u7684\u989c\u8272\u8f93\u51fa\u4fe1\u53f7\u3002\u8fd9\u4e9b\u8f93\u51fa\u4fe1\u53f7\u7ecf\u8fc7\u4fee\u6539\u540e\u4f20\u9012\u7ed9\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u8bfb\u53d6\u5c42\u8fdb\u884c\u6a21\u5f0f\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u539f\u59cb\u50a8\u5c42\u8f93\u51fa\u548c\u5408\u6210\u751f\u6210\u8f93\u51fa\u7684\u7cfb\u7edf\u6027\u80fd\u3001\u7cfb\u7edf\u6e05\u6670\u5ea6\u548c\u6570\u636e\u6548\u7387\uff0c\u7ed3\u679c\u8868\u660e\u5728\u7c97\u5206\u8fa8\u7387\u548c\u6709\u9650\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\u53ef\u8fbe 91%\u3002", "conclusion": "\u5fae\u6d41\u63a7\u7cfb\u7edf\u5728\u7c97\u5206\u8fa8\u7387\u548c\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe 91% \u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u53ef\u884c\u8ba1\u7b97\u8303\u4f8b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11447", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.11447", "abs": "https://arxiv.org/abs/2508.11447", "authors": ["Maximiliano Cristi\u00e1", "Gianfranco Rossi"], "title": "Encoding and Reasoning About Arrays in Set Theory", "comment": "Under consideration at ACM Transactions on Computational Logic", "summary": "We encode arrays as functions which, in turn, are encoded as sets of ordered\npairs. The set cardinality of each of these functions coincides with the length\nof the array it is representing. Then we define a fragment of set theory that\nis used to give the specifications of a non-trivial class of programs with\narrays. In this way, array reasoning becomes set reasoning. Furthermore, a\ndecision procedure for this fragment is also provided and implemented as part\nof the {log} (read 'setlog') tool. {log} is a constraint logic programming\nlanguage and satisfiability solver where sets and binary relations are\nfirst-class citizens. The tool already implements a few decision procedures for\ndifferent fragments of set theory. In this way, arrays are seamlessly\nintegrated into {log} thus allowing users to reason about sets, functions and\narrays all in the same language and with the same solver. The decision\nprocedure presented in this paper is an extension of decision procedures\ndefined in earlier works not supporting arrays.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u5c06\u6570\u7ec4\u7f16\u7801\u4e3a\u51fd\u6570\u548c\u96c6\u5408\uff0c\u5229\u7528\u96c6\u5408\u8bba\u65b9\u6cd5\u5728 {log} \u5de5\u5177\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u6570\u7ec4\u7684\u7edf\u4e00\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u5728 {log} \u5de5\u5177\u4e2d\u65e0\u7f1d\u96c6\u6210\u6570\u7ec4\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5728\u540c\u4e00\u8bed\u8a00\u548c\u6c42\u89e3\u5668\u4e2d\u8fdb\u884c\u96c6\u5408\u3001\u51fd\u6570\u548c\u6570\u7ec4\u7684\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u5c06\u6570\u7ec4\u7f16\u7801\u4e3a\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u7f16\u7801\u4e3a\u6709\u5e8f\u5bf9\u96c6\u5408\uff0c\u5e76\u5229\u7528\u96c6\u5408\u8bba\u7247\u6bb5\u7684\u5224\u5b9a\u8fc7\u7a0b\u6765\u5b9e\u73b0\u6570\u7ec4\u63a8\u7406\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6269\u5c55\u7684\u5224\u5b9a\u8fc7\u7a0b\uff0c\u652f\u6301\u6570\u7ec4\u63a8\u7406\uff0c\u5e76\u5df2\u96c6\u6210\u5230 {log} \u5de5\u5177\u4e2d\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u5c06\u6570\u7ec4\u7f16\u7801\u4e3a\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u7f16\u7801\u4e3a\u6709\u5e8f\u5bf9\u96c6\u5408\u3002\u7531\u6b64\uff0c\u6570\u7ec4\u7684\u957f\u5ea6\u53ef\u4ee5\u88ab\u8868\u793a\u4e3a\u5176\u5bf9\u5e94\u51fd\u6570\u7684\u96c6\u5408\u57fa\u6570\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u96c6\u5408\u8bba\u7247\u6bb5\u7684\u5224\u5b9a\u8fc7\u7a0b\uff0c\u7528\u4e8e\u975e\u5e73\u51e1\u6570\u7ec4\u7a0b\u5e8f\u7684\u89c4\u7ea6\uff0c\u4f7f\u5f97\u6570\u7ec4\u63a8\u7406\u8f6c\u53d8\u4e3a\u96c6\u5408\u63a8\u7406\u3002\u8be5\u5224\u5b9a\u8fc7\u7a0b\u5df2\u96c6\u6210\u5230 {log} \u5de5\u5177\u4e2d\uff0c\u652f\u6301\u96c6\u5408\u3001\u51fd\u6570\u548c\u6570\u7ec4\u7684\u7edf\u4e00\u63a8\u7406\u3002"}}
{"id": "2508.11423", "categories": ["cs.ET", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2508.11423", "abs": "https://arxiv.org/abs/2508.11423", "authors": ["Samson Abramsky", "Wolfgang Banzhaf", "Leo S. D. Caves", "Michael Levin", "Penousal Machado", "Charles Ofria", "Susan Stepney", "Roger White"], "title": "Open Questions about Time and Self-reference in Living Systems", "comment": "28 pages, 3 figures", "summary": "Living systems exhibit a range of fundamental characteristics: they are\nactive, self-referential, self-modifying systems. This paper explores how these\ncharacteristics create challenges for conventional scientific approaches and\nwhy they require new theoretical and formal frameworks. We introduce a\ndistinction between 'natural time', the continuing present of physical\nprocesses, and 'representational time', with its framework of past, present and\nfuture that emerges with life itself. Representational time enables memory,\nlearning and prediction, functions of living systems essential for their\nsurvival. Through examples from evolution, embryogenesis and metamorphosis we\nshow how living systems navigate the apparent contradictions arising from\nself-reference as natural time unwinds self-referential loops into\ndevelopmental spirals. Conventional mathematical and computational formalisms\nstruggle to model self-referential and self-modifying systems without running\ninto paradox. We identify promising new directions for modelling\nself-referential systems, including domain theory, co-algebra, genetic\nprogramming, and self-modifying algorithms. There are broad implications for\nbiology, cognitive science and social sciences, because self-reference and\nself-modification are not problems to be avoided but core features of living\nsystems that must be modelled to understand life's open-ended creativity.", "AI": {"tldr": "Living systems are active, self-referential, and self-modifying, posing challenges for conventional scientific approaches. This paper introduces 'natural time' and 'representational time' to explain how these systems function. It suggests new modeling frameworks like domain theory and co-algebra are needed to understand life\\", "motivation": "living systems exhibit a range of fundamental characteristics: they are active, self-referential, self-modifying systems. This paper explores how these characteristics create challenges for conventional scientific approaches and why they require new theoretical and formal frameworks.", "method": "introduce a distinction between \\", "result": "living systems navigate the apparent contradictions arising from self-reference as natural time unwinds self-referential loops into developmental spirals. Conventional mathematical and computational formalisms struggle to model self-referential and self-modifying systems without running into paradox. We identify promising new directions for modelling self-referential systems, including domain theory, co-algebra, genetic programming, and self-modifying algorithms.", "conclusion": "self-reference and self-modification are core features of living systems that must be modelled to understand life\\"}}
{"id": "2508.11071", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11071", "abs": "https://arxiv.org/abs/2508.11071", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "A Neural Column-and-Constraint Generation Method for Solving Two-Stage Stochastic Unit Commitment", "comment": null, "summary": "Two-stage stochastic unit commitment (2S-SUC) problems have been widely\nadopted to manage the uncertainties introduced by high penetrations of\nintermittent renewable energy resources. While decomposition-based algorithms\nsuch as column-and-constraint generation has been proposed to solve these\nproblems, they remain computationally prohibitive for large-scale, real-time\napplications. In this paper, we introduce a Neural Column-and-Constraint\nGeneration (Neural CCG) method to significantly accelerate the solution of\n2S-SUC problems. The proposed approach integrates a neural network that\napproximates the second-stage recourse problem by learning from high-level\nfeatures of operational scenarios and the first-stage commitment decisions.\nThis neural estimator is embedded within the CCG framework, replacing repeated\nsubproblem solving with rapid neural evaluations. We validate the effectiveness\nof the proposed method on the IEEE 118-bus system. Compared to the original CCG\nand a state-of-the-art commercial solver, Neural CCG achieves up to\n130.1$\\times$ speedup while maintaining a mean optimality gap below 0.096\\%,\ndemonstrating its strong potential for scalable stochastic optimization in\npower system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural CCG\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u52a0\u901f\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5355\u4f4d\u627f\u8bfa\u95ee\u9898\uff0c\u901f\u5ea6\u63d0\u5347\u663e\u8457\u4e14\u8bef\u5dee\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u7ba1\u7406\u95f4\u6b47\u6027\u53ef\u518d\u751f\u80fd\u6e90\u9ad8\u6e17\u900f\u7387\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e24\u9636\u6bb5\u968f\u673a\u5355\u4f4d\u627f\u8bfa\uff082S-SUC\uff09\u95ee\u9898\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u5206\u89e3\u7684\u7b97\u6cd5\uff08\u5982\u5217-\u7ea6\u675f\u751f\u6210\uff09\u5728\u8ba1\u7b97\u4e0a\u5bf9\u4e8e\u5927\u89c4\u6a21\u3001\u5b9e\u65f6\u5e94\u7528\u6765\u8bf4\u4ecd\u7136\u662f\u6602\u8d35\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5217-\u7ea6\u675f\u751f\u6210\uff08Neural CCG\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u5230CCG\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u5b66\u4e60\u64cd\u4f5c\u573a\u666f\u7684\u9ad8\u5c42\u7279\u5f81\u548c\u7b2c\u4e00\u9636\u6bb5\u7684\u627f\u8bfa\u51b3\u7b56\u6765\u903c\u8fd1\u7b2c\u4e8c\u9636\u6bb5\u7684\u5bf9\u7b56\u95ee\u9898\uff0c\u4ece\u800c\u7528\u5feb\u901f\u7684\u795e\u7ecf\u8bc4\u4f30\u66ff\u4ee3\u91cd\u590d\u7684\u5b50\u95ee\u9898\u6c42\u89e3\u3002", "result": "\u5728IEEE 118\u603b\u7ebf\u7cfb\u7edf\u4e0a\u7684\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbCCG\u548c\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u6c42\u89e3\u5668\u76f8\u6bd4\uff0cNeural CCG\u5b9e\u73b0\u4e86\u9ad8\u8fbe130.1\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5c06\u5e73\u5747\u6700\u4f18\u8bef\u5dee\u4fdd\u6301\u57280.096%\u4ee5\u4e0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u795e\u7ecf\u5217-\u7ea6\u675f\u751f\u6210\uff08Neural CCG\uff09\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u4e24\u9636\u6bb5\u968f\u673a\u5355\u4f4d\u627f\u8bfa\uff082S-SUC\uff09\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u5728\u5927\u89c4\u6a21\u3001\u5b9e\u65f6\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.10997", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.10997", "abs": "https://arxiv.org/abs/2508.10997", "authors": ["Dorit Aharonov", "Ori Alberton", "Itai Arad", "Yosi Atia", "Eyal Bairey", "Matan Ben Dov", "Asaf Berkovitch", "Zvika Brakerski", "Itsik Cohen", "Eran Fuchs", "Omri Golan", "Or Golan", "Barak D. Gur", "Ilya Gurwich", "Avieli Haber", "Rotem Haber", "Dorri Halbertal", "Yaron Itkin", "Barak A. Katzir", "Oded Kenneth", "Shlomi Kotler", "Roei Levi", "Eyal Leviatan", "Yotam Y. Lifshitz", "Adi Ludmer", "Shlomi Matityahu", "Ron Aharon Melcer", "Adiel Meyer", "Omrie Ovdat", "Aviad Panahi", "Gil Ron", "Ittai Rubinstein", "Gili Schul", "Tali Shnaider", "Maor Shutman", "Asif Sinay", "Tasneem Watad", "Assaf Zubida", "Netanel H. Lindner"], "title": "Reliable high-accuracy error mitigation for utility-scale quantum circuits", "comment": "12 pages, 5 figures + appendices", "summary": "Error mitigation is essential for unlocking the full potential of quantum\nalgorithms and accelerating the timeline toward quantum advantage. As quantum\nhardware progresses to push the boundaries of classical simulation, efficient\nand robust error mitigation methods are becoming increasingly important for\nproducing accurate and reliable outputs. We introduce QESEM, a reliable,\nhigh-accuracy, characterization-based software implementing efficient, unbiased\nquasi-probabilistic error mitigation. We explain the innovative components\nunderlying the operation of QESEM and demonstrate its capabilities in the\nlargest utility-scale error mitigation experiment based on an unbiased method.\nThis experiment simulates the kicked transverse field Ising model with\nfar-from-Clifford parameters on an IBM Heron device. We further validate\nQESEM's versatility across arbitrary quantum circuits and devices through\nhigh-accuracy error-mitigated molecular VQE circuits executed on IBM Heron and\nIonQ trapped-ion devices. Compared with multiple variants of the widely used\nzero-noise extrapolation method, QESEM consistently achieves higher accuracy.\nThese results mark a significant step forward in accuracy and reliability for\nrunning quantum circuits on devices available today across diverse algorithmic\napplications. Finally, we provide projections of QESEM's performance on\nnear-term devices toward quantum advantage.", "AI": {"tldr": "QESEM is a new, highly accurate, and reliable error mitigation software for quantum computing that outperforms existing methods like zero-noise extrapolation and shows promise for future quantum advantage.", "motivation": "Error mitigation is crucial for realizing the full potential of quantum algorithms and achieving quantum advantage, especially as quantum hardware advances and requires robust methods for accurate and reliable outputs.", "method": "QESEM is a characterization-based software that implements efficient, unbiased quasi-probabilistic error mitigation. It employs innovative components for its operation.", "result": "QESEM was demonstrated in the largest utility-scale error mitigation experiment using an unbiased method, simulating the kicked transverse field Ising model on an IBM Heron device. Its versatility was validated on IBM Heron and IonQ devices using molecular VQE circuits, consistently outperforming zero-noise extrapolation in accuracy.", "conclusion": "QESEM in consistently achieves higher accuracy compared to zero-noise extrapolation and represents a significant advancement in accuracy and reliability for executing quantum circuits on current devices across various algorithmic applications. Projections show its potential for near-term devices toward quantum advantage."}}
{"id": "2508.11002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11002", "abs": "https://arxiv.org/abs/2508.11002", "authors": ["Nikolaos Gkanatsios", "Jiahe Xu", "Matthew Bronars", "Arsalan Mousavian", "Tsung-Wei Ke", "Katerina Fragkiadaki"], "title": "3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation", "comment": null, "summary": "We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot\nmanipulation that combines flow matching for trajectory prediction with 3D\npretrained visual scene representations for learning from demonstration. 3DFA\nleverages 3D relative attention between action and visual tokens during action\ndenoising, building on prior work in 3D diffusion-based single-arm policy\nlearning. Through a combination of flow matching and targeted system-level and\narchitectural optimizations, 3DFA achieves over 30x faster training and\ninference than previous 3D diffusion-based policies, without sacrificing\nperformance. On the bimanual PerAct2 benchmark, it establishes a new state of\nthe art, outperforming the next-best method by an absolute margin of 41.4%. In\nextensive real-world evaluations, it surpasses strong baselines with up to\n1000x more parameters and significantly more pretraining. In unimanual\nsettings, it sets a new state of the art on 74 RLBench tasks by directly\npredicting dense end-effector trajectories, eliminating the need for motion\nplanning. Comprehensive ablation studies underscore the importance of our\ndesign choices for both policy effectiveness and efficiency.", "AI": {"tldr": "3DFA \u662f\u4e00\u79cd\u65b0\u7684 3D \u7b56\u7565\u67b6\u6784\uff0c\u53ef\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u6d41\u5339\u914d\u548c 3D \u89c6\u89c9\u8868\u793a\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u5feb 30 \u500d\u4ee5\u4e0a\uff0c\u5e76\u5728\u53cc\u81c2\u548c\u5355\u81c2\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684 3D \u7b56\u7565\u67b6\u6784\uff0c\u65e8\u5728\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "3DFA \u7ed3\u5408\u4e86\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u6d41\u5339\u914d\u548c\u7528\u4e8e\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u7684 3D \u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\u3002\u5b83\u5728\u52a8\u4f5c\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5229\u7528\u4e86\u52a8\u4f5c\u548c\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684 3D \u76f8\u5bf9\u6ce8\u610f\u529b\uff0c\u5e76\u5efa\u7acb\u4e86\u5728 3D \u6269\u6563\u6a21\u578b\u57fa\u7840\u4e0a\u8fdb\u884c\u5355\u81c2\u7b56\u7565\u5b66\u4e60\u7684\u5148\u524d\u5de5\u4f5c\u3002", "result": "\u4e0e\u4ee5\u524d\u7684 3D \u6269\u6563\u7b56\u7565\u76f8\u6bd4\uff0c3DFA \u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86 30 \u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5728 PerAct2 \u548c RLBench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "3DFA \u5728\u53cc\u81c2\u64cd\u4f5c\u7684 PerAct2 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6bd4\u6b21\u4f18\u65b9\u6cd5\u9ad8\u51fa 41.4%\u3002\u5728\u5355\u81c2\u64cd\u4f5c\u4e2d\uff0c\u5b83\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u5bc6\u96c6\u7684\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\uff0c\u5728 74 \u4e2a RLBench \u4efb\u52a1\u4e0a\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u65e0\u9700\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408L2O\u548cPPO\u7684\u81ea\u52a8\u8d28\u5b50\u6cbb\u7597\u8ba1\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u548c\u501f\u9274LLM\u6280\u672f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8017\u65f6\u548c\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u8ba1\u5212\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u4e86\u8ba1\u5212\u65f6\u95f4\uff0c\u5e76\u63d0\u9ad8\u4e86OAR\u4fdd\u62a4\u548c\u9776\u533a\u8986\u76d6\u7684\u6027\u80fd\u3002", "motivation": "\u5934\u90e8\u548c\u9888\u90e8\u764c\u75c7\u7684\u8d28\u5b50\u7b14\u5f62\u675f\u626b\u63cf\uff08PBS\uff09\u6cbb\u7597\u8ba1\u5212\u6d89\u53ca\u8bb8\u591a\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\uff0c\u9700\u8981\u4eba\u7c7b\u89c4\u5212\u8005\u4ed8\u51fa\u5927\u91cf\u52aa\u529b\u6765\u5e73\u8861\u548c\u6ee1\u8db3\u591a\u4e2a\u4e34\u5e8a\u76ee\u6807\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u9700\u8981\u8fdb\u884c\u8017\u8d39\u7ecf\u9a8c\u7684\u76ee\u6807\u53c2\u6570\u8c03\u6574\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u9006\u5411\u4f18\u5316\u3002\u5c3d\u7ba1\u5728\u81ea\u52a8\u8c03\u6574\u76ee\u6807\u53c2\u6570\u65b9\u9762\u5df2\u505a\u51fa\u5de8\u5927\u52aa\u529b\uff0c\u4f46\u6700\u8017\u65f6\u7684\u90e8\u5206\u2014\u2014\u9006\u5411\u4f18\u5316\uff0c\u4ecd\u7136\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u66f4\u6709\u6548\u3001\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u6cbb\u7597\u8ba1\u5212\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\uff0c\u8fd9\u662f\u4e00\u4e2aL2O\uff08Learning to Optimize\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u6570\u636e\u5206\u5e03\u6765\u9884\u6d4b\u66f4\u65b0\u6b65\u9aa4\u3002\u8be5\u9006\u4f18\u5316\u5668\u88ab\u96c6\u6210\u5230\u4e00\u4e2a\u57fa\u4e8ePPO\uff08Proximal Policy Optimization\uff09\u7684\u6846\u67b6\u4e2d\u3002PPO\u6846\u67b6\u4f5c\u4e3a\u5916\u5faa\u73af\u7684\u865a\u62df\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u7b56\u7565\u7f51\u7edc\u81ea\u4e3b\u8c03\u6574\u76ee\u6807\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u5242\u91cf\u9884\u6d4b\u5668\u6765\u521d\u59cb\u5316\u76ee\u6807\u53c2\u6570\u3002\u5185\u5faa\u73af\u7684L2O\u9006\u4f18\u5316\u5668\u6839\u636ePPO\u7b56\u7565\u7f51\u7edc\u4f18\u5316\u7684\u76ee\u6807\u6765\u8ba1\u7b97\u673a\u5668\u53ef\u4ea4\u4ed8\u7684MU\uff08Machine Units\uff09\u503c\u3002\u8be5\u7814\u7a76\u8fd8\u96c6\u6210\u4e86\u4e3aLLM\u8bbe\u8ba1\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6280\u672f\u5230\u57fa\u4e8eTransformer\u7684L2O\u6846\u67b6\u4e2d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709L2O\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u8be5\u7814\u7a76\u6536\u96c6\u4e8697\u540d\u60a3\u8005\u7684\u6570\u636e\uff0c\u5e76\u5c06\u63d0\u51fa\u7684L2O\u9006\u4f18\u5316\u5668\u4e0eL-BFGSB\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e8622.97%\u548c36.41%\u3002\u7ed3\u5408PPO\u5b66\u4e60\u7684\u865a\u62df\u89c4\u5212\u5668\uff0c\u8be5\u6846\u67b6\u751f\u6210\u7684\u8ba1\u5212\u5728\u5e73\u57472.55\u5c0f\u65f6\u5185\u5b8c\u6210\uff0c\u4e0e\u4eba\u5de5\u751f\u6210\u7684\u8ba1\u5212\u76f8\u6bd4\uff0c\u5728OAR\u7684\u4fdd\u62a4\u548c\u9776\u533a\u8986\u76d6\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u4f18\u6216\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u5904\u65b9\u5242\u91cf\u6c34\u5e73\u3001\u9776\u533a\u6570\u91cf\u3001\u5c04\u675f\u89d2\u5ea6\u7b49\u7684\u60a3\u8005\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8ePPO\u7684\u81ea\u52a8\u6cbb\u7597\u8ba1\u5212\u6846\u67b6\u4e2d\uff0c\u4ee5\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6cbb\u7597\u8ba1\u5212\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u6570\u636e\u5206\u5e03\u6765\u9884\u6d4b\u66f4\u65b0\u6b65\u9aa4\uff0c\u5e76\u9996\u6b21\u5c06\u4e3aLLM\u8bbe\u8ba1\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6280\u672f\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684L2O\u6846\u67b6\u4e2d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709L2O\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002\u4e0eL-BFGSB\u76f8\u6bd4\uff0c\u8be5L2O\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e8622.97%\u548c36.41%\u3002\u4e0e\u4eba\u5de5\u751f\u6210\u7684\u8ba1\u5212\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u751f\u6210\u7684\u8ba1\u5212\u5728OAR\uff08\u5371\u53ca\u5668\u5b98\uff09\u7684\u4fdd\u62a4\u548c\u9776\u533a\u8986\u76d6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u66f4\u4f18\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \\underline{V}alue \\underline{S}ign \\underline{F}lip", "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for\nincorporating negative prompt guidance in few-step diffusion and flow-matching\nimage generation models. Unlike existing approaches such as classifier-free\nguidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by\nflipping the sign of attention values from negative prompts. Our method\nrequires only small computational overhead and integrates effectively with\nMMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as\ncross-attention-based models like Wan. We validate VSF on challenging datasets\nwith complex prompt pairs and demonstrate superior performance in both static\nimage and video generation tasks. Experimental results show that VSF\nsignificantly improves negative prompt adherence compared to prior methods in\nfew-step models, and even CFG in non-few-step models, while maintaining\ncompetitive image quality. Code and ComfyUI node are available in\nhttps://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF\u662f\u4e00\u79cd\u65b0\u7684\u8d1f\u9762\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7b80\u5355\u9ad8\u6548\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ecb\u7ecd\u4e00\u79cd\u7528\u4e8e\u5728\u5c11\u6b65\u957f\u6269\u6563\u548c\u6d41\u5339\u914d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7ed3\u5408\u8d1f\u9762\u63d0\u793a\u5f15\u5bfc\u7684\u7b80\u5355\u9ad8\u6548\u65b9\u6cd5VSF\u3002", "method": "VSF\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u9762\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\u6765\u52a8\u6001\u6291\u5236\u4e0d\u5e0c\u671b\u7684\u5185\u5bb9\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u53ef\u4e0eMMDiT\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u578b\u96c6\u6210\u3002", "result": "VSF\u5728\u5305\u542b\u590d\u6742\u63d0\u793a\u5bf9\u7684\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5728\u9759\u6001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "VSF\u5728\u5c11\u6b65\u957f\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8d1f\u9762\u63d0\u793a\u7684\u9075\u5faa\u6027\uff0c\u5728\u975e\u5c11\u6b65\u957f\u6a21\u578b\u4e2d\u751a\u81f3\u4f18\u4e8eCFG\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2508.11201", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.11201", "abs": "https://arxiv.org/abs/2508.11201", "authors": ["Aina Sumiyoshi", "Keisuke Ishizeki", "Takahiro Yamamoto"], "title": "Statistical Properties of Current Noise Induced by Electron-Phonon Scattering in Metallic Carbon Nanotubes", "comment": "7 pages, 6 figures", "summary": "We theoretically investigate current noise in metallic carbon nanotubes\ninduced by electron-phonon scattering, focusing on the probability density\nfunction (PDF) of the current that characterizes the nonequilibrium steady\nstate. Quantum transport simulations combined with analyses of higher-order\nstatistical moments reveal that the PDF evolves continuously from a Gaussian\ndistribution in the ballistic regime to a non-Gaussian gamma distribution in\nthe diffusive regime. In the crossover regime, the PDF exhibits pronounced\nasymmetry, attributed to a statistical imbalance in the number of conduction\npathways contributing to high- and low-current events. Furthermore, in the\ndiffusive regime, we identify non-Markovian features arising from\nhigh-frequency resonances in the current noise, which dominate the asymptotic\nscaling behavior of the current variance.", "AI": {"tldr": "Current noise in carbon nanotubes changes from Gaussian to gamma distribution depending on the regime, showing asymmetry and non-Markovian effects in the diffusive regime.", "motivation": "The research is motivated by the need to theoretically understand current noise in metallic carbon nanotubes, specifically focusing on the probability density function (PDF) that characterizes the nonequilibrium steady state.", "method": "The paper employs quantum transport simulations and analyses of higher-order statistical moments to investigate current noise induced by electron-phonon scattering in metallic carbon nanotubes.", "result": "The PDF of the current transitions from a Gaussian to a gamma distribution as the system moves from the ballistic to the diffusive regime. Pronounced asymmetry is observed in the crossover regime, and non-Markovian features are identified in the diffusive regime, influencing the current variance scaling.", "conclusion": "The study reveals that the probability density function of current noise in metallic carbon nanotubes evolves from Gaussian to gamma distribution across ballistic and diffusive regimes, with non-Gaussian features and non-Markovian effects observed in the diffusive regime due to high-frequency resonances."}}
{"id": "2508.10925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "AI": {"tldr": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. They use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. The models have strong agentic capabilities and achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights and related resources under an Apache 2.0 license.", "motivation": "To push the frontier of accuracy and inference cost with open-weight reasoning models.", "method": "The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation.", "result": "Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety.", "conclusion": "We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research."}}
{"id": "2508.10954", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10954", "abs": "https://arxiv.org/abs/2508.10954", "authors": ["Gyutae Oh", "Jitae Shin"], "title": "Towards Efficient Prompt-based Continual Learning in Distributed Medical AI", "comment": "10p", "summary": "Modern AI models achieve state-of-the-art performance with large-scale,\nhigh-quality datasets; however, ethical, social, and institutional constraints\nin the medical domain severely restrict data sharing, rendering centralized\nlearning nearly impossible. Each institution must incrementally update models\nusing only local data. Traditional training overfits new samples and suffers\nfrom catastrophic forgetting, losing previously acquired knowledge. Medical\ndata distributions also shift due to varying diagnostic equipment and\ndemographics. Although continual learning (CL) has advanced, most methods\naddress natural images, leaving medical-domain-specific CL underexplored. We\npropose a prompt-based continual learning (PCL) approach featuring a unified\nprompt pool with a minimal expansion strategy: by expanding and freezing a\nsubset of prompts, our method reduces computational overhead, and a novel\nregularization term balances retention and adaptation. Experiments on three\ndiabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy\nDetection show our model improves final classification accuracy by at least 10%\nand F1-score by 9 points over state-of-the-art approaches while lowering\ninference cost. We anticipate this study will drive sustainable medical AI\nadvances, enabling real-time diagnosis, patient monitoring, and telemedicine\napplications in distributed healthcare. Code will be released upon acceptance", "AI": {"tldr": "\u7531\u4e8e\u533b\u5b66\u6570\u636e\u5171\u4eab\u53d7\u9650\uff0c\u6a21\u578b\u66f4\u65b0\u56f0\u96be\u4e14\u6613\u9057\u5fd8\u3002\u672c\u7814\u7a76\u63d0\u51faPCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u6c60\u548c\u6b63\u5219\u5316\u6280\u672f\uff0c\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\uff0c\u4fc3\u8fdb\u533b\u7597AI\u53d1\u5c55\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u7406\u56e0\u6570\u636e\u5171\u4eab\u9650\u5236\u800c\u53ea\u80fd\u5728\u672c\u5730\u8fdb\u884c\u589e\u91cf\u66f4\u65b0\u7684\u533b\u5b66\u6570\u636e\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002\u540c\u65f6\uff0c\u533b\u5b66\u6570\u636e\u7684\u5206\u5e03\u4f1a\u56e0\u8bca\u65ad\u8bbe\u5907\u548c\u4eba\u7fa4\u7684\u53d8\u5316\u800c\u53d1\u751f\u504f\u79fb\u3002\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u81ea\u7136\u56fe\u50cf\uff0c\u5bf9\u533b\u5b66\u9886\u57df\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u6301\u7eed\u5b66\u4e60\uff08PCL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u7edf\u4e00\u7684\u63d0\u793a\u6c60\uff0c\u91c7\u7528\u6700\u5c0f\u5316\u6269\u5c55\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u5c55\u548c\u51bb\u7ed3\u90e8\u5206\u63d0\u793a\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6b63\u5219\u5316\u9879\u6765\u5e73\u8861\u77e5\u8bc6\u7684\u4fdd\u7559\u548c\u9002\u5e94\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u6570\u636e\u96c6\uff08Aptos2019\u3001LI2019\u548cDiabetic Retinopathy Detection\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5c06\u6700\u7ec8\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u81f3\u5c1110%\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e869\u4e2a\u70b9\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPCL\uff08prompt-based continual learning\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u63d0\u793a\u6c60\u548c\u6700\u5c0f\u5316\u6269\u5c55\u7b56\u7565\uff0c\u7ed3\u5408\u65b0\u7684\u6b63\u5219\u5316\u9879\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u9886\u57df\u4e2d\u7531\u4e8e\u6570\u636e\u5171\u4eab\u9650\u5236\u800c\u5bfc\u81f4\u7684\u6a21\u578b\u66f4\u65b0\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPCL\u5728\u63d0\u9ad8\u6700\u7ec8\u5206\u7c7b\u51c6\u786e\u7387\u548cF1\u5206\u6570\u65b9\u9762\uff0c\u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u6709\u671b\u63a8\u52a8\u5206\u5e03\u5f0f\u533b\u7597AI\u7684\u53d1\u5c55\uff0c\u652f\u6301\u5b9e\u65f6\u8bca\u65ad\u3001\u60a3\u8005\u76d1\u62a4\u548c\u8fdc\u7a0b\u533b\u7597\u7b49\u5e94\u7528\u3002"}}
{"id": "2508.11298", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11298", "abs": "https://arxiv.org/abs/2508.11298", "authors": ["Gabin Schieffer", "Jacob Wahlgren", "Ruimin Shi", "Edgar A. Le\u00f3n", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive", "comment": null, "summary": "The ever-increasing compute performance of GPU accelerators drives up the\nneed for efficient data movements within HPC applications to sustain\nperformance. Proposed as a solution to alleviate CPU-GPU data movement, AMD\nMI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth\nmemory (HBM) within a single physical package. Leadership supercomputers, such\nas El Capitan, group four APUs within a single compute node, using Infinity\nFabric Interconnect. In this work, we design specific benchmarks to evaluate\ndirect memory access from the GPU, explicit inter-APU data movement, and\ncollective multi-APU communication. We also compare the efficiency of HIP APIs,\nMPI routines, and the GPU-specialized RCCL library. Our results highlight key\ndesign choices for optimizing inter-APU communication on multi-APU AMD MI300A\nsystems with Infinity Fabric, including programming interfaces, allocators, and\ndata movement. Finally, we optimize two real HPC applications, Quicksilver and\nCloverLeaf, and evaluate them on a four MI100A APU system.", "AI": {"tldr": "This paper evaluates data movement and communication on AMD MI300A APUs for HPC, comparing different programming models and optimizing applications for multi-APU systems.", "motivation": "The growing compute performance of GPUs necessitates efficient data movement in HPC applications. The AMD MI300A APU, which integrates CPU, GPU, and HBM, is a potential solution for alleviating CPU-GPU data movement issues.", "method": "The study designs specific benchmarks to evaluate direct memory access from the GPU, explicit inter-APU data movement, and collective multi-APU communication. It compares the efficiency of HIP APIs, MPI routines, and RCCL library. Two HPC applications, Quicksilver and CloverLeaf, are optimized and evaluated on a four MI100A APU system.", "result": "The results highlight key design choices for optimizing inter-APU communication on multi-APU AMD MI300A systems with Infinity Fabric. The study evaluates the efficiency of different programming interfaces, allocators, and data movement strategies.", "conclusion": "The paper analyzes the efficiency of data movement on AMD MI300A APUs for HPC applications, highlighting design choices for optimizing inter-APU communication and evaluating performance with specific benchmarks and real-world applications."}}
{"id": "2508.11476", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11476", "abs": "https://arxiv.org/abs/2508.11476", "authors": ["Qian Liang", "Zichong Chen", "Yang Zhou", "Hui Huang"], "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "comment": "Accepted to the Journal track of Pacific Graphics 2025", "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.", "AI": {"tldr": "SPG\u662f\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u65b0\u578b\u91c7\u6837\u7b56\u7565\uff0c\u53ef\u4ee5\u901a\u8fc7\u98ce\u683c\u566a\u58f0\u5411\u91cf\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u98ce\u683c\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u5728\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u98ce\u683c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "SPG\u901a\u8fc7\u6784\u5efa\u98ce\u683c\u566a\u58f0\u5411\u91cf\uff0c\u5e76\u5229\u7528\u5176\u4e0e\u65e0\u6761\u4ef6\u566a\u58f0\u7684\u65b9\u5411\u504f\u5dee\u6765\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u4f7f\u5176\u671d\u7740\u76ee\u6807\u98ce\u683c\u5206\u5e03\u8fdb\u884c\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u7a33\u5065\uff0c\u5e76\u80fd\u4e0eControlNet\u548cIPAdapter\u7b49\u53ef\u63a7\u6846\u67b6\u517c\u5bb9\u3002", "result": "SPG\u4e0eClassifier-Free Guidance\uff08CFG\uff09\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "Style-Prompting Guidance (SPG)\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u7b56\u7565\uff0c\u80fd\u591f\u5b9e\u73b0\u7279\u5b9a\u98ce\u683c\u7684\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.11225", "categories": ["cond-mat.mtrl-sci", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.11225", "abs": "https://arxiv.org/abs/2508.11225", "authors": ["Yunfan Liang", "Damien West", "Shengbai Zhang"], "title": "Uncovering the Fourier Structure of Wavefunctions in Semiconductors", "comment": null, "summary": "Symmetry dictates the physical properties of materials. The symmetry of the\nBravais lattice defines the set of points, lines, and planes over which sets of\nplanewaves are degenerate, upon which atomic symmetry determines the\ninteraction potentials which may lift such degeneracies. This results in\nwavefunctions which are single planewaves throughout the BZ, except in the\nvicinity of the removed degeneracies. As optical transitions between any two\nplanewaves are forbidden, only regions of the Brillouin zone (BZ) near these\nlifted degeneracies contribute to optical properties. Application to optical\nresponse of Si and other semiconductors reveals that a single band transition,\nwith only two planewaves, well describes their dielectric properties. Further,\nit provides a framework to understand non-linear optical response which is\ndemonstrated to arise from higher order degeneracy existing along high symmetry\nlines/points of the BZ.", "AI": {"tldr": "Material symmetry governs physical properties. Bravais lattice and atomic symmetry create planewave degeneracies in the Brillouin zone (BZ). Optical properties arise from lifted degeneracies. For Si, one band transition with two planewaves explains dielectric properties. Non-linear optical response comes from higher-order degeneracies on high-symmetry lines/points.", "motivation": "The motivation of this paper is to understand how the symmetry of materials dictates their physical properties, particularly their optical response. It aims to explain the underlying principles governing optical transitions in semiconductors by considering the role of planewaves and degeneracies within the Brillouin zone (BZ). The paper seeks to provide a theoretical framework for understanding both linear and non-linear optical properties, demonstrating that these arise from specific symmetry-induced degeneracies.", "method": "The paper analyzes the role of symmetry in determining the physical properties of materials, specifically focusing on how Bravais lattice symmetry and atomic interactions create degeneracies in planewaves within the Brillouin zone (BZ). It explains that optical transitions are forbidden between degenerate planewaves and thus only regions near lifted degeneracies contribute to optical properties. The method involves applying this framework to semiconductors like Si to understand their optical response, including both linear and non-linear properties.", "result": "The analysis reveals that a single band transition involving only two planewaves can effectively describe the dielectric properties of semiconductors like Si. Furthermore, the study demonstrates that non-linear optical response originates from higher-order degeneracies that exist along high-symmetry lines and points within the BZ.", "conclusion": "Symmetry determines the physical properties of materials. The symmetry of the Bravais lattice and atomic interactions define degeneracies in planewaves within the Brillouin zone (BZ). Optical transitions only occur near lifted degeneracies, contributing to optical properties. The analysis of Si and other semiconductors shows that a single band transition with two planewaves sufficiently describes their dielectric properties. Higher-order degeneracies along high-symmetry lines/points in the BZ are responsible for non-linear optical response."}}
{"id": "2508.11444", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.11444", "abs": "https://arxiv.org/abs/2508.11444", "authors": ["Therese Biedl"], "title": "Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm", "comment": null, "summary": "In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that\nevery $n$-vertex plane graph $G$ has (under some natural restrictions) a\nvertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is\n\\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed\nneighbourhood) and \\emph{face-hitting} (every face of $G$ is incident to a\nvertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$\nthat has certain properties, and among all such graphs, taking one that has the\nfewest edges. As such, their proof is not algorithmic. Their proof also relies\non the 4-color theorem, for which a quadratic-time algorithm exists, but it\nwould not be easy to implement.\n  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has\n(under the same restrictions) a vertex-partition into two dominating\nface-hitting sets. Our proof is constructive, and requires nothing more\ncomplicated than splitting a graph into 2-connected components, finding an ear\ndecomposition, and computing a perfect matching in a 3-regular plane graph. For\nall these problems, linear-time algorithms are known and so we can find the\nvertex-partition in linear time.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u5177\u6709\u5efa\u8bbe\u6027\u7684\u8bc1\u660e\uff0c\u8bc1\u660e\u4e86\u5e73\u9762\u56fe\u7684\u9876\u70b9\u53ef\u4ee5\u88ab\u5212\u5206\u4e3a\u4e24\u4e2a\u652f\u914d\u96c6\u548c\u9762\u5bfc\u51fa\u96c6\uff0c\u8be5\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u751f\u6210\u6027\u8bc1\u660e\uff0c\u8bc1\u660e\u5e73\u9762\u56fe\u7684\u9876\u70b9\u53ef\u4ee5\u88ab\u5212\u5206\u4e3a\u4e24\u4e2a\u652f\u914d\u96c6\u548c\u9762\u5bfc\u51fa\u96c6\uff0c\u5e76\u4e14\u8be5\u8bc1\u660e\u65b9\u6cd5\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "method": "\u672c\u6587\u5229\u7528\u56fe\u76842-\u8fde\u901a\u5206\u91cf\u5206\u89e3\uff0c\u7ed3\u5408\u6b27\u5c14\u5206\u89e3\u548c3-\u6b63\u5219\u5e73\u9762\u56fe\u7684\u5b8c\u7f8e\u5339\u914d\uff0c\u6765\u6784\u9020\u4e00\u4e2a\u9876\u70b9\u5212\u5206\u3002", "result": "\u672c\u6587\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u5e73\u9762\u56fe\u9876\u70b9\u5212\u5206\u7684\u65b0\u7684\u3001\u5efa\u8bbe\u6027\u7684\u8bc1\u660e\uff0c\u8bc1\u660e\u4e86\u8be5\u5212\u5206\u53ef\u4ee5\u751f\u6210\uff0c\u5e76\u4e14\u53ea\u9700\u8981O(n)\u65f6\u95f4\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u4e4b\u524d\u65b9\u6cd5\u66f4\u4f18\u7684\u8bc1\u660e\uff0c\u8be5\u8bc1\u660e\u662f\u5efa\u8bbe\u6027\u7684\uff0c\u5e76\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\u3002"}}
{"id": "2508.10920", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.10920", "abs": "https://arxiv.org/abs/2508.10920", "authors": ["Tom Bensky", "Justin Kopcinski"], "title": "Use of a genetic algorithm to find solutions to introductory physics problems", "comment": "11 page, 5 figures, 3 tables, 2 code blocks", "summary": "In this work, we show how a genetic algorithm (GA) can be used to find\nstep-by-step solutions to introductory physics problems. Our perspective is\nthat the underlying task for this is one of finding a sequence of equations\nthat will lead to the needed answer. Here a GA is used to find an appropriate\nequation sequence by minimizing a fitness function that measures the difference\nbetween the number of unknowns versus knowns in a set of equations. Information\nabout knowns comes from the GA posing questions to the student about what\nquantities exist in the text of their problem. The questions are generated from\nenumerations pulled from the chromosomes that drive the GA. Equations with\nsmaller known vs. unknown differences are considered more fit and are used to\nproduce intermediate results that feed less fit equations. We show that this\ntechnique can guide a student to an answer to any introductory physics problem\ninvolving one-dimensional kinematics. Interpretability findings are discussed.", "AI": {"tldr": "\u9057\u4f20\u7b97\u6cd5\u53ef\u7528\u4e8e\u89e3\u51b3\u7269\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u7a0b\u5e8f\u5217\u6765\u627e\u5230\u7b54\u6848\u3002", "motivation": "\u5c55\u793a\u4e86\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u5982\u4f55\u7528\u4e8e\u5bfb\u627e\u5165\u95e8\u7269\u7406\u95ee\u9898\u7684\u9010\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u627e\u5230\u80fd\u5bfc\u51fa\u6240\u9700\u7b54\u6848\u7684\u65b9\u7a0b\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u8861\u91cf\u65b9\u7a0b\u7ec4\u4e2d\u672a\u77e5\u6570\u4e0e\u5df2\u77e5\u6570\u4e4b\u5dee\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6765\u5bfb\u627e\u5408\u9002\u7684\u65b9\u7a0b\u5e8f\u5217\u3002\u9057\u4f20\u7b97\u6cd5\u901a\u8fc7\u8be2\u95ee\u5b66\u751f\u95ee\u9898\u6765\u83b7\u53d6\u5df2\u77e5\u6570\u4fe1\u606f\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u9a71\u52a8\u9057\u4f20\u7b97\u6cd5\u7684\u67d3\u8272\u4f53\u679a\u4e3e\u3002", "result": "\u9057\u4f20\u7b97\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u9002\u5e94\u5ea6\u51fd\u6570\u6765\u5bfb\u627e\u65b9\u7a0b\u5e8f\u5217\uff0c\u5176\u4e2d\u9002\u5e94\u5ea6\u51fd\u6570\u8861\u91cf\u672a\u77e5\u6570\u4e0e\u5df2\u77e5\u6570\u4e4b\u5dee\uff0c\u4ee5\u627e\u5230\u89e3\u51b3\u4e00\u7ef4\u8fd0\u52a8\u5b66\u5165\u95e8\u7269\u7406\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6280\u672f\u53ef\u4ee5\u6307\u5bfc\u5b66\u751f\u89e3\u51b3\u4e00\u7ef4\u8fd0\u52a8\u5b66\u7684\u4efb\u4f55\u5165\u95e8\u7269\u7406\u95ee\u9898\u3002"}}
{"id": "2508.11029", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11029", "abs": "https://arxiv.org/abs/2508.11029", "authors": ["Yuchen Zhang", "Francis Soualle", "Musa Furkan Keskin", "Yuan Liu", "Linlong Wu", "Jos\u00e9 A. del Peral-Rosado", "Bhavani Shankar M. R.", "Gonzalo Seco-Granados", "Henk Wymeersch", "Tareq Y. Al-Naffouri"], "title": "Distributed Integrated Sensing, Localization, and Communications over LEO Satellite Constellations", "comment": "This paper has been submitted to IEEE for possible publication", "summary": "Low Earth orbit (LEO) satellite constellations are rapidly becoming essential\nenablers of next-generation wireless systems, offering global broadband access,\nhigh-precision localization, and reliable sensing beyond terrestrial coverage.\nHowever, the inherent limitations of individual LEO satellites, including\nrestricted power, limited antenna aperture, and constrained onboard processing,\nhinder their ability to meet the growing demands of 6G applications. To address\nthese challenges, this article introduces the concept of distributed integrated\nsensing, localization, and communication (DISLAC) over LEO constellations,\ninspired by distributed multiple input multiple output architectures. By\nenabling inter-satellite cooperation through inter-satellite links, DISLAC can\nsubstantially improve throughput, positioning accuracy, and sensing robustness.\nWe present illustrative case studies that quantify these benefits and analyze\nkey system-level considerations, including synchronization, antenna\nreconfigurability, and ISL design. The article concludes by outlining open\nresearch directions to advance the practical deployment of DISLAC in future\nnon-terrestrial networks.", "AI": {"tldr": "LEO\u536b\u661f\u5229\u7528\u5206\u5e03\u5f0f\u534f\u540c\u6280\u672fDISLAC\uff0c\u63d0\u53476G\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LEO\u536b\u661f\u529f\u7387\u3001\u5929\u7ebf\u5b54\u5f84\u548c\u5728\u8f68\u5904\u7406\u80fd\u529b\u6709\u9650\u7684\u6311\u6218\uff0c\u4ee5\u6ee1\u8db36G\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u96c6\u6210\u4f20\u611f\u3001\u5b9a\u4f4d\u548c\u901a\u4fe1\uff08DISLAC\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u8fdb\u884c\u4e86\u7cfb\u7edf\u7ea7\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u91cf\u5316\u4e86DISLAC\u7684\u4f18\u52bf\uff0c\u5e76\u5206\u6790\u4e86\u540c\u6b65\u3001\u5929\u7ebf\u53ef\u91cd\u6784\u548cISL\u8bbe\u8ba1\u7b49\u5173\u952e\u7cfb\u7edf\u7ea7\u8003\u8651\u56e0\u7d20\u3002", "conclusion": "LEO\u536b\u661f\u661f\u5ea7\u901a\u8fc7ISL\u5b9e\u73b0DISLAC\uff0c\u53ef\u63d0\u5347\u541e\u5410\u91cf\u3001\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u611f\u77e5\u9c81\u68d2\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.11449", "categories": ["cs.LO", "03B05 (Primary)"], "pdf": "https://arxiv.org/pdf/2508.11449", "abs": "https://arxiv.org/abs/2508.11449", "authors": ["Patrick Koopmann", "Christoph Wernhard", "Frank Wolter"], "title": "Interpolation in Classical Propositional Logic", "comment": "The article will appear in Balder ten Cate, Jean Christoph Jung,\n  Patrick Koopmann, Christoph Wernhard and Frank Wolter, editors. Theory and\n  Applications of Craig Interpolation. Ubiquity Press, 2026", "summary": "We introduce Craig interpolation and related notions such as uniform\ninterpolation, Beth definability, and theory decomposition in classical\npropositional logic. We present four approaches to computing interpolants: via\nquantifier elimination, from formulas in disjunctive normal form, and by\nextraction from resolution or tableau refutations. We close with a discussion\nof the size of interpolants and links to circuit complexity.", "AI": {"tldr": "This paper introduces Craig interpolation and related concepts in classical propositional logic, offering four computational approaches and exploring connections to circuit complexity.", "motivation": "Introduction of Craig interpolation and related notions like uniform interpolation, Beth definability, and theory decomposition in classical propositional logic.", "method": "The paper presents four approaches to computing interpolants: via quantifier elimination, from formulas in disjunctive normal form, and by extraction from resolution or tableau refutations.", "result": "The paper discusses the size of interpolants and their links to circuit complexity.", "conclusion": "The paper discusses Craig interpolation and related concepts in classical propositional logic, along with four methods for computing interpolants and their connections to circuit complexity."}}
{"id": "2508.11451", "categories": ["cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.11451", "abs": "https://arxiv.org/abs/2508.11451", "authors": ["Hamid Farzaneh", "Asif Ali Khan", "Jeronimo Castrillon"], "title": "CoMoNM: A Cost Modeling Framework for Compute-Near-Memory Systems", "comment": "12 pages, 16 Figures", "summary": "Compute-Near-Memory (CNM) systems offer a promising approach to mitigate the\nvon Neumann bottleneck by bringing computational units closer to data. However,\noptimizing for these architectures remains challenging due to their unique\nhardware and programming models. Existing CNM compilers often rely on manual\nprogrammer annotations for offloading and optimizations. Automating these\ndecisions by exploring the optimization space, common in CPU/GPU systems, is\ndifficult for CNMs as constructing and navigating the transformation space is\ntedious and time consuming. This is particularly the case during system-level\ndesign, where evaluation requires time-consuming simulations. To address this,\nwe present CoMoNM, a generic cost modeling framework for CNM systems for\nexecution time estimation in milliseconds. It takes a high-level,\nhardware-agnostic application representation, target system specifications, and\na mapping specification as input and estimates the execution time for the given\napplication on the target CNM system. We show how CoMoNM can be seamlessly\nintegrated into state-of-the-art CNM compilers, providing improved offloading\ndecisions. Evaluation on established benchmarks for CNM shows estimation errors\nwithin 7.80% and 2.99%, when compared to the real UPMEM CNM system and\nSamsung's HBM-PIM simulator. Notably, CoMoNM delivers estimates seven orders of\nmagnitude faster compared to the UPMEM and HBM-PIM simulators.", "AI": {"tldr": "CoMoNM is a fast and accurate cost modeling framework for CNM systems that helps automate compiler optimizations by estimating execution time, reducing reliance on slow simulations.", "motivation": "Automating optimization decisions in Compute-Near-Memory (CNM) systems is challenging due to unique hardware and programming models, and the tediousness of exploring the optimization space, especially when simulations are time-consuming.", "method": "CoMoNM, a generic cost modeling framework for CNM systems, estimates execution time using a high-level application representation, target system specifications, and a mapping specification.", "result": "Evaluation on benchmarks shows CoMoNM achieves low estimation errors (within 7.80% and 2.99%) and provides estimates significantly faster (seven orders of magnitude) than existing simulators.", "conclusion": "CoMoNM can be integrated into CNM compilers to improve offloading decisions, with estimation errors within 7.80% and 2.99% compared to real systems/simulators, and is seven orders of magnitude faster."}}
{"id": "2508.11080", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11080", "abs": "https://arxiv.org/abs/2508.11080", "authors": ["Soumya Kundu", "Kaustav Chatterjee", "Ramij R. Hossain", "Sai Pushpak Nandanoori", "Veronica Adetola"], "title": "Managing Risks from Large Digital Loads Using Coordinated Grid-Forming Storage Network", "comment": "Submitted to IEEE PES T&D Conference and Expo 2026", "summary": "Anticipated rapid growth of large digital load, driven by artificial\nintelligence (AI) data centers, is poised to increase uncertainty and large\nfluctuations in consumption, threatening the stability, reliability, and\nsecurity of the energy infrastructure. Conventional measures taken by grid\nplanners and operators to ensure stable and reliable integration of new\nresources are either cost-prohibitive (e.g., transmission upgrades) or\nill-equipped (e.g., generation control) to resolve the unique challenges\nbrought on by AI Data Centers (e.g., extreme load transients). In this work, we\nexplore the feasibility of coordinating and managing available flexibility in\nthe grid, in terms of grid-forming storage units, to ensure stable and reliable\nintegration of AI Data Centers without the need for costly grid upgrades.\nRecently developed bi-layered coordinated control strategies -- involving\nfast-acting, local, autonomous, control at the storage to maintain transient\nsafety in voltage and frequency at the point-of-interconnection, and a slower,\ncoordinated (consensus) control to restore normal operating condition in the\ngrid -- are used in the case studies. A comparison is drawn between broadly two\nscenarios: a network of coordinated, smaller, distributed storage vs. larger\nstorage installations collocated with large digital loads. IEEE 68-bus network\nis used for the case studies, with large digital load profiles drawn from the\nMIT Supercloud Dataset.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u6570\u636e\u4e2d\u5fc3\u5bf9\u7535\u7f51\u7684\u7a33\u5b9a\u6027\u6784\u6210\u4e86\u6311\u6218\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u534f\u8c03\u50a8\u80fd\u7cfb\u7edf\u6765\u89e3\u51b3\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684\u5347\u7ea7\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e0d\u540c\u50a8\u80fd\u914d\u7f6e\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6570\u636e\u4e2d\u5fc3\u9a71\u52a8\u7684\u5927\u578b\u6570\u5b57\u8d1f\u8377\u7684\u9884\u671f\u5feb\u901f\u589e\u957f\uff0c\u5c06\u589e\u52a0\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u7684\u6d88\u8017\u4e0d\u786e\u5b9a\u6027\u548c\u5927\u5e45\u6ce2\u52a8\uff0c\u5a01\u80c1\u5176\u7a33\u5b9a\u3001\u53ef\u9760\u548c\u5b89\u5168\u3002\u4f20\u7edf\u7684\u7535\u7f51\u89c4\u5212\u548c\u8fd0\u884c\u63aa\u65bd\uff0c\u5982\u8f93\u7535\u5347\u7ea7\u6216\u53d1\u7535\u63a7\u5236\uff0c\u5728\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\u6570\u636e\u4e2d\u5fc3\u5e26\u6765\u7684\u6781\u7aef\u8d1f\u8377\u77ac\u6001\u7b49\u72ec\u7279\u6311\u6218\u65b9\u9762\u6210\u672c\u9ad8\u6602\u6216\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u6700\u8fd1\u5f00\u53d1\u7684\u53cc\u5c42\u534f\u8c03\u63a7\u5236\u7b56\u7565\uff0c\u5305\u62ec\u7528\u4e8e\u7ef4\u6301\u7535\u538b\u548c\u9891\u7387\u77ac\u6001\u5b89\u5168\u7684\u5feb\u901f\u672c\u5730\u81ea\u4e3b\u63a7\u5236\uff0c\u4ee5\u53ca\u7528\u4e8e\u6062\u590d\u7535\u7f51\u6b63\u5e38\u8fd0\u884c\u6761\u4ef6\u7684\u8f83\u6162\u534f\u8c03\uff08\u5171\u8bc6\uff09\u63a7\u5236\u3002", "result": "\u5c06\u534f\u8c03\u7684\u3001\u5206\u6563\u7684\u3001\u5c0f\u578b\u50a8\u80fd\u7f51\u7edc\u4e0e\u5e76\u7f6e\u7684\u5927\u578b\u50a8\u80fd\u8bbe\u65bd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u5728 IEEE 68 \u603b\u7ebf\u7f51\u7edc\u4e0a\u4f7f\u7528 MIT \u8d85\u7ea7\u8ba1\u7b97\u673a\u6570\u636e\u96c6\u4e2d\u7684\u5927\u578b\u6570\u5b57\u8d1f\u8377\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u4e86\u6848\u4f8b\u7814\u7a76\u3002", "conclusion": "\u901a\u8fc7\u534f\u8c03\u548c\u7ba1\u7406\u7535\u7f51\u4e2d\u53ef\u7528\u7684\u7075\u6d3b\u6027\uff0c\u4f8b\u5982\u4f7f\u7528\u5e76\u7f51\u50a8\u80fd\u5355\u5143\uff0c\u53ef\u4ee5\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\u6570\u636e\u4e2d\u5fc3\u7a33\u5b9a\u53ef\u9760\u5730\u96c6\u6210\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u6602\u8d35\u7684\u7535\u7f51\u5347\u7ea7\u3002"}}
{"id": "2508.11023", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11023", "abs": "https://arxiv.org/abs/2508.11023", "authors": ["Gustavo C. Amaral", "Nienke M. ten Haaf", "Breno Perlingeiro", "David L. Bakker", "Mark G. M. Boekel", "Tim E. van Duivenbode", "Karthik Selvan", "Nicolas Oidtmann", "Rafael Ochsendorf", "Rick N. M. Wasserman", "Mael Flament", "Felipe Giraldo", "Shane Andrewski", "Mehdi Namazi", "Federica Facchin", "Mario Casta\u00f1eda", "Fokko de Vries", "Sayali Shevate", "Shaurya Bhave", "Marco Gorter", "Nico Coesel", "David Mytling", "Mike Mabry", "Carlo Page", "Alexandra Pinto", "Joanneke Jansen", "Rahul Vyas", "Marc X. Makkes"], "title": "Hybrid Fiber-Free-Space Entanglement Distribution Using Off-the-Shelf Quantum Devices", "comment": "14 pages", "summary": "Entanglement serves as a fundamental resource for quantum technologies,\nenabling communication and computation tasks that surpass classical limits. Its\ndistribution across networks is essential for interconnecting quantum\nprocessors, enabling distributed quantum computing to address complex\nchallenges in areas such as drug discovery, material science, and optimization.\nIn this work, we report the successful distribution of polarization-entangled\nphoton pairs across a campus-scale, three-node quantum network comprising both\nfiber and free-space optical links. The entire system was built using\ncommercially available components provided by partners within the Netherlands\nQuantum Ecosystem. This result represents advancements in the technological\nmaturity of quantum communication systems and demonstrates a pathway towards\nthe practical deployment of early-stage quantum networks both on Earth and in\nspace.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u7531\u5546\u4e1a\u7ec4\u4ef6\u7ec4\u6210\u7684\u6821\u56ed\u7ea7\u91cf\u5b50\u7f51\u7edc\uff0c\u5e76\u6210\u529f\u5730\u901a\u8fc7\u5149\u7ea4\u548c\u81ea\u7531\u7a7a\u95f4\u94fe\u8def\u5206\u53d1\u4e86\u7ea0\u7f20\u5149\u5b50\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u901a\u4fe1\u6280\u672f\u7684\u8fdb\u6b65\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8d85\u8d8a\u7ecf\u5178\u6781\u9650\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u4efb\u52a1\uff0c\u4ee5\u53ca\u4e92\u8054\u91cf\u5b50\u5904\u7406\u5668\u4ee5\u8fdb\u884c\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\uff0c\u9700\u8981\u8de8\u7f51\u7edc\u5206\u53d1\u7ea0\u7f20\u3002", "method": "\u5229\u7528\u5546\u4e1a\u53ef\u7528\u7ec4\u4ef6\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5149\u7ea4\u548c\u81ea\u7531\u7a7a\u95f4\u5149\u94fe\u8def\u7684\u6821\u56ed\u89c4\u6a21\u4e09\u8282\u70b9\u91cf\u5b50\u7f51\u7edc\uff0c\u5e76\u6210\u529f\u5206\u53d1\u4e86\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\u3002", "result": "\u6210\u529f\u5730\u5728\u6821\u56ed\u89c4\u6a21\u7684\u4e09\u8282\u70b9\u91cf\u5b50\u7f51\u7edc\uff08\u5305\u62ec\u5149\u7ea4\u548c\u81ea\u7531\u7a7a\u95f4\u5149\u94fe\u8def\uff09\u4e2d\u5206\u53d1\u4e86\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5728\u5305\u542b\u5149\u7ea4\u548c\u81ea\u7531\u7a7a\u95f4\u5149\u94fe\u8def\u7684\u6821\u56ed\u89c4\u6a21\u4e09\u8282\u70b9\u91cf\u5b50\u7f51\u7edc\u4e2d\u5206\u53d1\u4e86\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u901a\u4fe1\u6280\u672f\u7684\u6210\u719f\u5ea6\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5730\u7403\u548c\u592a\u7a7a\u4e2d\u7684\u91cf\u5b50\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11049", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL\u662f\u4e00\u79cd\u521b\u65b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4ece\u751f\u6210\u6d41\u4e2d\u63d0\u53d6\u7684\u64cd\u7eb5\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u5bf9\u751f\u6210\u6570\u636e\u8d28\u91cf\u7684\u4f9d\u8d56\u4ee5\u53ca\u5728\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u4e2d\u7f3a\u4e4f\u73af\u5883\u53cd\u9988\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u57fa\u4e8e\u89c6\u9891\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c6\u9891\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6536\u96c6\u7528\u4e8e\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGenFlowRL\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u751f\u6210\u7684\u6d41\u4e2d\u63a8\u5bfc\u51fa\u5956\u52b1\uff0c\u8be5\u6d41\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u800c\u6765\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u591a\u6837\u5316\u7684\u6f14\u793a\u4e2d\u5b66\u4e60\u53ef\u6cdb\u5316\u548c\u9c81\u68d2\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u572810\u4e2a\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8fdb\u884c\uff0c\u5305\u62ec\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u8de8\u4f53\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86GenFlowRL\u7684\u6709\u6548\u6027\u3002", "conclusion": "GenFlowRL\u901a\u8fc7\u5229\u7528\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u751f\u6210\u6d41\u4e2d\u63d0\u53d6\u7684\u64cd\u7eb5\u7279\u5f81\uff0c\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Bl\u00fcmel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u5f15\u5165\u4e86\u4e00\u822c ABA\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u6027\u8d28\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u4fdd\u6301\u4e86\u6a21\u5757\u5316\u5c5e\u6027\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u76f8\u5bb9\u6027\u6982\u5ff5\u5728\u57fa\u4e8e\u5047\u8bbe\u7684\u8bba\u8bc1\uff08ABA\uff09\u4e2d\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7279\u522b\u662f\u7814\u7a76\u4e86\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u822c\u975e\u5e73\u5766 ABA\u3002", "method": "\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u57fa\u4e8e\u96c6\u5408\u7684\u8bba\u8bc1\u6846\u67b6\uff08BSAFs\uff09\u4f5c\u4e3a\u5f62\u5f0f\u5de5\u5177\uff0c\u7814\u7a76\u4e86\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u6982\u5ff5\u5728\u4e00\u822c ABA \u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u6027\u8d28\u548c\u6f5c\u5728\u95ee\u9898\u3002", "result": "\u5728\u4e00\u822c ABA \u4e2d\u5f15\u5165\u4e86\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u8bed\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u4e0e\u7ecf\u5178\u76f8\u5bb9\u6027\u4e00\u6837\uff0c\u90fd\u4fdd\u6301\u4e86\u4e2d\u5fc3\u6a21\u5757\u5316\u5c5e\u6027\u3002\u540c\u65f6\uff0c\u4e5f\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u8bed\u4e49\u5728\u975e\u5e73\u5766 ABA \u4e2d\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u5f15\u5165\u4e86\u57fa\u4e8e\u5047\u8bbe\u7684\u8bba\u8bc1\uff08ABA\uff09\u7684\u4e00\u822c\uff08\u6709\u65f6\u79f0\u4e3a\u975e\u5e73\u5766\uff09\u5f62\u5f0f\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u6027\u8d28\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u5178\u76f8\u5bb9\u6027\u3001\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u5728 ABA \u4e2d\u90fd\u4fdd\u6301\u4e86\u4e2d\u5fc3\u6a21\u5757\u5316\u5c5e\u6027\u3002\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u5f3a\u76f8\u5bb9\u6027\u548c\u5f31\u76f8\u5bb9\u6027\u8bed\u4e49\u5728\u975e\u5e73\u5766 ABA \u4e2d\u7684\u4e00\u4e9b\u7f3a\u70b9\u53ca\u5176\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10933", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10933", "abs": "https://arxiv.org/abs/2508.10933", "authors": ["Yoli Shavit", "Yosi Keller"], "title": "Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications", "comment": "Accepted to ICCVW 2025", "summary": "Accurate camera localization is crucial for modern retail environments,\nenabling enhanced customer experiences, streamlined inventory management, and\nautonomous operations. While Absolute Pose Regression (APR) from a single image\noffers a promising solution, approaches that incorporate visual and spatial\nscene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)\nhave recently been introduced to embed such priors into APR. In this work, we\nextend PAEs to the task of Relative Pose Regression (RPR) and propose a novel\nre-localization scheme that refines APR predictions using PAE-based RPR,\nwithout requiring additional storage of images or pose data. We first introduce\nPAE-based RPR and establish its effectiveness by comparing it with image-based\nRPR models of equivalent architectures. We then demonstrate that our refinement\nstrategy, driven by a PAE-based RPR, enhances APR localization accuracy on\nindoor benchmarks. Notably, our method is shown to achieve competitive\nperformance even when trained with only 30% of the data, substantially reducing\nthe data collection burden for retail deployment. Our code and pre-trained\nmodels are available at: https://github.com/yolish/camera-pose-auto-encoders", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6444\u50cf\u5934\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6444\u50cf\u5934\u4f4d\u59ff\u81ea\u52a8\u7f16\u7801\u5668\uff08PAE\uff09\u5e94\u7528\u4e8e\u76f8\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08RPR\uff09\uff0c\u5e76\u7ed3\u5408\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08APR\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u51cf\u5c11\u4e86\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u51c6\u786e\u7684\u6444\u50cf\u5934\u5b9a\u4f4d\u5bf9\u4e8e\u73b0\u4ee3\u96f6\u552e\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u5b9e\u73b0\u589e\u5f3a\u7684\u5ba2\u6237\u4f53\u9a8c\u3001\u7b80\u5316\u7684\u5e93\u5b58\u7ba1\u7406\u548c\u81ea\u4e3b\u8fd0\u8425\u3002\u867d\u7136\u4ece\u5355\u4e2a\u56fe\u50cf\u8fdb\u884c\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08APR\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7ed3\u5408\u89c6\u89c9\u548c\u7a7a\u95f4\u573a\u666f\u5148\u9a8c\u7684\u65b9\u6cd5\u5f80\u5f80\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u6444\u50cf\u5934\u4f4d\u59ff\u81ea\u52a8\u7f16\u7801\u5668\uff08PAE\uff09\u6700\u8fd1\u88ab\u5f15\u5165\u4ee5\u5c06\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u5d4c\u5165APR\u3002", "method": "\u672c\u6587\u5c06PAE\u6269\u5c55\u5230\u76f8\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08RPR\uff09\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u65b0\u5b9a\u4f4d\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u901a\u8fc7\u57fa\u4e8ePAE\u7684RPR\u6765\u4f18\u5316APR\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u7684\u56fe\u50cf\u6216\u4f4d\u59ff\u6570\u636e\u5b58\u50a8\u3002\u9996\u5148\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8ePAE\u7684RPR\uff0c\u5e76\u901a\u8fc7\u4e0e\u7b49\u6548\u67b6\u6784\u7684\u57fa\u4e8e\u56fe\u50cf\u7684RPR\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u6765\u786e\u7acb\u5176\u6709\u6548\u6027\u3002\u7136\u540e\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u7531\u57fa\u4e8ePAE\u7684RPR\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684APR\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u4f18\u5316\u7b56\u7565\u901a\u8fc7\u57fa\u4e8ePAE\u7684RPR\uff0c\u63d0\u9ad8\u4e86APR\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u4e0e\u4ec5\u4f7f\u752830%\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u672c\u6587\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8ePAE\u7684RPR\u65b9\u6cd5\u5728\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86APR\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u4ec5\u4f7f\u752830%\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u96f6\u552e\u90e8\u7f72\u7684\u6570\u636e\u6536\u96c6\u8d1f\u62c5\u3002"}}
{"id": "2508.11209", "categories": ["cond-mat.mes-hall", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.11209", "abs": "https://arxiv.org/abs/2508.11209", "authors": ["Emir Syahreza Fadhilla", "M Shoufie Ukhtary", "Ardian Nata Atmaja", "Bobby Eka Gunara"], "title": "Optically Controlled Skyrmion Number Current", "comment": "14 pages, 5 figures", "summary": "We propose a mechanism to control the motion of magnetic Skyrmions through\nthe generation of a Skyrmion number current. This current is induced and tuned\nby an explicitly time-dependent Hamiltonian that includes a Zeeman term arising\nfrom the interaction between the spin system and circularly polarized light. To\ncapture the effect, we apply a first-order perturbation method to the\nLandau-Lifshitz-Gilbert equation, using a breathing Skyrmion ansatz based on\nthe Belavin-Polyakov profile. This approach reveals that the time-dependent\ndeformation of the Skyrmion boundary produces an anisotropic breathing mode,\nwhich in turn generates a nonzero Skyrmion number current. The resulting\ndynamics in momentum space form a limit cycle, whose characteristics depend\nsolely on the external magnetic field amplitude, the Heisenberg exchange\ncoupling, and the Gilbert damping constant. Our formulation not only clarifies\nthe topological origin of optically driven Skyrmion motion but also points to\nSkyrmion number currents as a low-dissipation alternative to electric currents\nfor efficient Skyrmion control.", "AI": {"tldr": "\u901a\u8fc7\u63a7\u5236\u65af\u683c\u660e\u5b50\u6570\u6d41\u6765\u63a7\u5236\u78c1\u6027\u65af\u683c\u660e\u5b50\u7684\u8fd0\u52a8\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u4ea7\u751f\u65af\u683c\u660e\u5b50\u6570\u6d41\u6765\u63a7\u5236\u78c1\u6027\u65af\u683c\u660e\u5b50\u8fd0\u52a8\u7684\u673a\u5236\u3002", "method": "\u91c7\u7528\u4e00\u9636\u5fae\u6270\u65b9\u6cd5\u5904\u7406Landau-Lifshitz-Gilbert\u65b9\u7a0b\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eBelavin-Polyakov\u5256\u9762\u7684\u547c\u5438\u65af\u683c\u660e\u5b50ansatz\u3002", "result": "\u65f6\u95f4\u4f9d\u8d56\u7684\u65af\u683c\u660e\u5b50\u8fb9\u754c\u53d8\u5f62\u4ea7\u751f\u5404\u5411\u5f02\u6027\u547c\u5438\u6a21\u5f0f\uff0c\u8fdb\u800c\u4ea7\u751f\u975e\u96f6\u65af\u683c\u660e\u5b50\u6570\u6d41\u3002\u52a8\u91cf\u7a7a\u95f4\u4e2d\u7684\u52a8\u529b\u5b66\u5f62\u6210\u4e00\u4e2a\u6781\u9650\u73af\uff0c\u5176\u7279\u6027\u4ec5\u53d6\u51b3\u4e8e\u5916\u78c1\u573a\u5e45\u5ea6\u3001\u6d77\u68ee\u5821\u4ea4\u6362\u8026\u5408\u548c\u5409\u5c14\u4f2f\u7279\u963b\u5c3c\u5e38\u6570\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u673a\u5236\u53ef\u4ee5\u63a7\u5236\u78c1\u6027\u65af\u683c\u660e\u5b50\u7684\u8fd0\u52a8\uff0c\u5176\u901a\u8fc7\u4ea7\u751f\u65af\u683c\u660e\u5b50\u6570\u6d41\u6765\u5b9e\u73b0\u3002\u8be5\u7535\u6d41\u7531\u663e\u542b\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u54c8\u5bc6\u987f\u91cf\u8bf1\u5bfc\u548c\u8c03\u8282\uff0c\u8be5\u54c8\u5bc6\u987f\u91cf\u5305\u542b\u7531\u81ea\u65cb\u7cfb\u7edf\u4e0e\u5706\u504f\u632f\u5149\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u7684\u585e\u66fc\u9879\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9610\u660e\u4e86\u5149\u9a71\u52a8\u65af\u683c\u660e\u5b50\u8fd0\u52a8\u7684\u62d3\u6251\u8d77\u6e90\uff0c\u8fd8\u6307\u51fa\u4e86\u65af\u683c\u660e\u5b50\u6570\u6d41\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u8017\u6563\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u6709\u6548\u63a7\u5236\u65af\u683c\u660e\u5b50\u3002"}}
{"id": "2508.10927", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10927", "abs": "https://arxiv.org/abs/2508.10927", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u81ea\u52a8\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u63d0\u53d6\u516c\u53f8\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u53d1\u73b0\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u98ce\u9669\u56e0\u7d20\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4e3a\u4e86\u8bc6\u522b\u4e0e\u516c\u53f8\u76f8\u5173\u7684\u98ce\u9669\uff0c\u8fd9\u5bf9\u4e8e\u6295\u8d44\u8005\u548c\u6574\u4f53\u91d1\u878d\u5e02\u573a\u7684\u798f\u7949\u975e\u5e38\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5305\u542b\u4e03\u4e2a\u4e0d\u540c\u65b9\u9762\uff08\u5982\u4f9b\u5e94\u94fe\u3001\u6cd5\u89c4\u548c\u7ade\u4e89\uff09\u7684\u6a21\u5f0f\uff0c\u5e76\u5bf9744\u7bc7\u65b0\u95fb\u6587\u7ae0\u8fdb\u884c\u4e86\u62bd\u6837\u548c\u6807\u6ce8\uff0c\u4ee5\u5bf9\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7684\u5148\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaMA-2\uff09\u5728\u8bc6\u522b\u98ce\u9669\u56e0\u7d20\u65b9\u9762\u53ea\u80fd\u8fbe\u5230\u4e2d\u4f4e\u7b49\u6027\u80fd\u3002\u800c\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5927\u591a\u6570\u98ce\u9669\u56e0\u7d20\u4e0a\u7684\u8868\u73b0\u66f4\u597d\u3002", "result": "\u901a\u8fc7\u5206\u6790\u8d85\u8fc7277,000\u7bc7\u5f6d\u535a\u793e\u65b0\u95fb\u6587\u7ae0\uff0c\u8bc1\u660e\u4e86\u4ece\u65b0\u95fb\u4e2d\u8bc6\u522b\u98ce\u9669\u56e0\u7d20\u53ef\u4ee5\u4e3a\u516c\u53f8\u548c\u884c\u4e1a\u7684\u8fd0\u8425\u63d0\u4f9b\u5e7f\u6cdb\u7684\u89c1\u89e3\u3002", "conclusion": "\u8bc6\u522b\u65b0\u95fb\u4e2d\u7684\u516c\u53f8\u98ce\u9669\u56e0\u7d20\u5bf9\u4e8e\u6295\u8d44\u8005\u548c\u91d1\u878d\u5e02\u573a\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u81ea\u52a8\u63d0\u53d6\u516c\u53f8\u98ce\u9669\u56e0\u7d20\u3002"}}
{"id": "2508.10967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10967", "abs": "https://arxiv.org/abs/2508.10967", "authors": ["Xinyi Li", "Sai Wang", "Yutian Lin", "Yu Wu", "Yi Yang"], "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis", "comment": null, "summary": "Retrosynthesis prediction aims to infer the reactant molecule based on a\ngiven product molecule, which is a fundamental task in chemical synthesis.\nHowever, existing models rely on static pattern-matching paradigm, which limits\ntheir ability to perform effective logic decision-making, leading to black-box\ndecision-making. Building on this, we propose Retro-Expert, an interpretable\nretrosynthesis framework that performs collaborative reasoning by combining the\ncomplementary reasoning strengths of Large Language Models and specialized\nmodels via reinforcement learning. It outputs natural language explanations\ngrounded in chemical logic through three components: (1) specialized models\nperform shallow reasoning to construct high-quality chemical decision space,\n(2) LLM-driven critical reasoning to generate predictions and corresponding\ninterpretable reasoning path, and (3) reinforcement learning optimizing\ninterpretable decision policy. Experiments show that Retro-Expert not only\nsurpasses both LLM-based and specialized models across different metrics but\nalso provides expert-aligned explanations that bridge the gap between AI\npredictions and actionable chemical insights.", "AI": {"tldr": "Retro-Expert is a new framework that uses LLMs and specialized models with reinforcement learning for interpretable retrosynthesis prediction, outperforming existing methods and providing chemical insights.", "motivation": "Existing retrosynthesis models rely on static pattern-matching, limiting their decision-making abilities and resulting in black-box outcomes. The proposed framework aims to provide interpretable retrosynthesis prediction through collaborative reasoning.", "method": "Retro-Expert is an interpretable retrosynthesis framework that combines Large Language Models and specialized models via reinforcement learning. It consists of three components: specialized models for shallow reasoning to create a high-quality chemical decision space, an LLM for critical reasoning to generate predictions and interpretable reasoning paths, and reinforcement learning to optimize the decision policy.", "result": "Experiments show that Retro-Expert outperforms both LLM-based and specialized models on various metrics and provides expert-aligned explanations.", "conclusion": "Retro-Expert surpasses existing LLM-based and specialized models in retrosynthesis prediction, offering expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights."}}
{"id": "2508.11384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11384", "abs": "https://arxiv.org/abs/2508.11384", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Olivier Stietel", "Robin Vacus"], "title": "Space-efficient population protocols for exact majority in general graphs", "comment": null, "summary": "We study exact majority consensus in the population protocol model. In this\nmodel, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in\neach time step, a scheduler samples uniformly at random a pair of adjacent\nnodes to interact. In the exact majority consensus task, each node is given a\nbinary input, and the goal is to design a protocol that almost surely reaches a\nstable configuration, where all nodes output the majority input value.\n  We give improved upper and lower bounds for the exact majority in general\ngraphs. First, we give asymptotically tight time lower bounds for general\n(unbounded space) protocols. Second, we obtain new upper bounds parameterized\nby the relaxation time $\\tau_{\\mathsf{rel}}$ of the random walk on $G$ induced\nby the scheduler and the degree imbalance $\\Delta/\\delta$ of $G$. Specifically,\nwe give a protocol that stabilizes in $O\\left( \\tfrac{\\Delta}{\\delta}\n\\tau_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high\nprobability and uses $O\\left( \\log n \\cdot \\left(\n\\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\n\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any\ngraph with minimum degree at least $\\delta$ and maximum degree at most\n$\\Delta$.\n  For regular expander graphs, this matches the optimal space complexity of\n$\\Theta(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA\n2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of\n$O(n \\log^2 n)$ steps. Finally, we give a new upper bound of\n$O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a\nconstant-state protocol.", "AI": {"tldr": "\u672c\u7814\u7a76\u6539\u8fdb\u4e86\u4eba\u53e3\u534f\u8bae\u6a21\u578b\u4e2d\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u7684\u6027\u80fd\u3002\u6211\u4eec\u4e3a\u4e00\u822c\u56fe\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0b\u754c\uff0c\u5e76\u4e3a\u4f9d\u8d56\u56fe\u5c5e\u6027\uff08\u5982\u677e\u5f1b\u65f6\u95f4\u548c\u5ea6\u5931\u8861\uff09\u7684\u534f\u8bae\u63d0\u4f9b\u4e86\u65b0\u7684\u65f6\u95f4\u4e0a\u754c\u548c\u72b6\u6001\u590d\u6742\u5ea6\u4e0a\u754c\u3002\u7814\u7a76\u7ed3\u679c\u5728\u6b63\u5219\u6269\u5c55\u5668\u56fe\u4e0a\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u603b\u4f53\u4eba\u53e3\u534f\u8bae\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u3002\u8be5\u95ee\u9898\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u534f\u8bae\uff0c\u4f7f\u5f97\u7cfb\u7edf\u80fd\u591f\u7a33\u5b9a\u5730\u8f93\u51fa\u591a\u6570\u8282\u70b9\u7684\u8f93\u5165\u503c\u3002\u7814\u7a76\u7684\u76ee\u7684\u662f\u5728\u4e00\u822c\u56fe\u4e0a\u6539\u8fdb\u73b0\u6709\u5171\u8bc6\u534f\u8bae\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "method": "\u672c\u7814\u7a76\u5728\u603b\u4f53\u4eba\u53e3\u534f\u8bae\u6a21\u578b\u4e2d\u7814\u7a76\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u3002\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u5728\u968f\u673a\u8c03\u5ea6\u4e0b\u76f8\u90bb\u8282\u70b9\u4ea4\u4e92\u7684\u7cfb\u7edf\u884c\u4e3a\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u91cd\u70b9\u5173\u6ce8\u56fe\u7684\u7ed3\u6784\u5c5e\u6027\uff0c\u5982\u677e\u5f1b\u65f6\u95f4 $\\tau_{\\mathsf{rel}}$ \u548c\u5ea6\u5931\u8861 $\\Delta/\\delta$\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u534f\u8bae\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u4e0d\u540c\u56fe\u7ed3\u6784\u4e0b\u7684\u7a33\u5b9a\u65f6\u95f4\u548c\u72b6\u6001\u590d\u6742\u5ea6\u3002", "result": "\u6211\u4eec\u4e3a\u603b\u4f53\u4eba\u53e3\u534f\u8bae\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4e3a\u4e00\u822c\u56fe\u63d0\u4f9b\u4e86\u6e10\u8fd1\u6700\u4f18\u7684\u65f6\u95f4\u4e0b\u754c\u3002\u6211\u4eec\u8fd8\u83b7\u5f97\u4e86\u65b0\u7684\u4e0a\u754c\uff0c\u8fd9\u4e9b\u4e0a\u754c\u4f9d\u8d56\u4e8e\u56fe\u7684\u677e\u5f1b\u65f6\u95f4 $\\tau_{\\mathsf{rel}}$ \u548c\u5ea6\u5931\u8861 $\\Delta/\\delta$\u3002\u6211\u4eec\u63d0\u51fa\u7684\u534f\u8bae\u5728\u671f\u671b\u548c\u9ad8\u6982\u7387\u4e0b\u80fd\u5728 $O\\left( \\tfrac{\\Delta}{\\delta} \\tau_{\\mathsf{rel}} \\log^2 n \\right)$ \u6b65\u5185\u7a33\u5b9a\u3002\u5bf9\u4e8e\u5177\u6709\u6700\u5c0f\u5ea6 $\\delta$ \u548c\u6700\u5927\u5ea6 $\\Delta$ \u7684\u56fe\uff0c\u5176\u72b6\u6001\u590d\u6742\u5ea6\u4e3a $O\\left( \\log n \\cdot \\left( \\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$\u3002\u5728\u6b63\u5219\u6269\u5c55\u5668\u56fe\u4e0a\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8fbe\u5230\u4e86\u4e0e\u5b8c\u5168\u56fe\u4e0a\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u76f8\u5339\u914d\u7684 $\\Theta(\\log n)$\uff0c\u5e76\u4e14\u7a33\u5b9a\u65f6\u95f4\u63a5\u8fd1\u6700\u4f18\u7684 $O(n \\log^2 n)$\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a\u5e38\u6570\u72b6\u6001\u534f\u8bae\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7a33\u5b9a\u65f6\u95f4\u4e0a\u754c $O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u603b\u4f53\u4eba\u53e3\u534f\u8bae\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u4e0a\u4e0b\u754c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u4e00\u822c\u56fe\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6e10\u8fd1\u6700\u4f18\u7684\u65f6\u95f4\u4e0b\u754c\u3002\u5bf9\u4e8e\u5177\u6709\u677e\u5f1b\u65f6\u95f4 $\tau_{\\mathsf{rel}}$ \u548c\u5ea6\u5931\u8861 $\\Delta/\\delta$ \u7684\u56fe\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8bae\uff0c\u5176\u671f\u671b\u7a33\u5b9a\u65f6\u95f4\u548c\u9ad8\u6982\u7387\u7a33\u5b9a\u65f6\u95f4\u4e3a $O\\left( \\tfrac{\\Delta}{\\delta} \\tau_{\\mathsf{rel}} \\log^2 n \\right)$ \u6b65\uff0c\u540c\u65f6\u72b6\u6001\u590d\u6742\u5ea6\u4e3a $O\\left( \\log n \\cdot \\left( \\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u5177\u6709\u6700\u5c0f\u5ea6 $\\delta$ \u548c\u6700\u5927\u5ea6 $\\Delta$ \u7684\u56fe\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e38\u6570\u72b6\u6001\u534f\u8bae\uff0c\u5176\u7a33\u5b9a\u65f6\u95f4\u4e0a\u754c\u4e3a $O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$\u3002"}}
{"id": "2508.10934", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE \u662f\u4e00\u79cd\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u7528\u4e8e\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6 3D \u4fe1\u606f\uff08\u5982\u76f8\u673a\u59ff\u6001\u548c\u6df1\u5ea6\u56fe\uff09\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6807\u6ce8\u96be\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u901f\u5ea6\u5feb\uff0c\u5e76\u5df2\u7528\u4e8e\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7a7a\u95f4 AI \u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u72b6\u6001\u6700\u5148\u8fdb\u7684 3D \u51e0\u4f55\u611f\u77e5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u4e2d\u83b7\u53d6\u4e00\u81f4\u4e14\u7cbe\u786e\u7684 3D \u6ce8\u91ca\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ViPE \u7684\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u7528\u4e8e\u4ece\u975e\u7ea6\u675f\u539f\u59cb\u89c6\u9891\u4e2d\u9ad8\u6548\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u3001\u76f8\u673a\u8fd0\u52a8\u548c\u8fd1\u4e4e\u5ea6\u91cf\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cd\u573a\u666f\u548c\u76f8\u673a\u6a21\u578b\u3002", "result": "ViPE \u5728 TUM/KITTI \u6570\u636e\u96c6\u4e0a\uff0c\u5176\u6027\u80fd\u5206\u522b\u6bd4\u73b0\u6709\u7684\u672a\u6821\u51c6\u59ff\u6001\u4f30\u8ba1\u57fa\u7ebf\u63d0\u9ad8\u4e86 18%/50%\uff0c\u5e76\u4e14\u5728\u6807\u51c6\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\uff0c\u5355 GPU \u8fd0\u884c\u901f\u5ea6\u53ef\u8fbe 3-5FPS\u3002\u8be5\u5f15\u64ce\u5df2\u7528\u4e8e\u6ce8\u91ca\u5305\u542b\u7ea6 100K \u4e2a\u771f\u5b9e\u4e92\u8054\u7f51\u89c6\u9891\u30011M \u4e2a\u9ad8\u8d28\u91cf AI \u751f\u6210\u89c6\u9891\u548c 2K \u4e2a\u5168\u666f\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u603b\u8ba1\u7ea6 96M \u5e27\u3002", "conclusion": "ViPE \u5f15\u64ce\u5df2\u5f00\u6e90\uff0c\u5e76\u4e14\u5176\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u4e5f\u5df2\u516c\u5f00\uff0c\u65e8\u5728\u52a0\u901f\u7a7a\u95f4\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11240", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11240", "abs": "https://arxiv.org/abs/2508.11240", "authors": ["Hongyu Ma", "Xuesong Hu", "Junren Shi"], "title": "Metallic Contact Contributions in Thermal Hall Conductivity Measurements", "comment": null, "summary": "We investigate the influence of metallic contacts on thermal Hall\nmeasurements. By analyzing typical measurement configurations, we demonstrate\nthat heat currents bypassing through metallic contacts can generate\nnon-negligible thermal Hall signals, even when the actual thermal Hall\nconductivity of a measured insulator is zero. We show that the effect predicts\nthermal Hall conductivities that compare favorably with actual experimental\nobservations across a variety of materials, reproducing both their temperature\ndependencies and magnitudes by assuming effective contact thicknesses on the\norder of 10$^{-2}$ of sample widths. It even reproduces the subtle differences\nin temperature dependencies between materials with high and low longitudinal\nthermal conductivity. Our study suggests the necessity of suppressing the\nbypass heat currents in thermal Hall measurements, which can be achieved by\nproperly arranging the measurement configurations.", "AI": {"tldr": "\u91d1\u5c5e\u89e6\u70b9\u65c1\u8def\u70ed\u6d41\u4f1a\u5f71\u54cd\u70ed\u970d\u5c14\u6d4b\u91cf\uff0c\u9700\u8981\u6291\u5236\u8be5\u6548\u5e94\u3002", "motivation": "\u63a2\u7a76\u91d1\u5c5e\u89e6\u70b9\u5bf9\u70ed\u970d\u5c14\u6d4b\u91cf\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u65c1\u8def\u70ed\u7535\u6d41\u4ea7\u751f\u865a\u5047\u70ed\u970d\u5c14\u4fe1\u53f7\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5178\u578b\u7684\u6d4b\u91cf\u914d\u7f6e\uff0c\u6f14\u793a\u4e86\u91d1\u5c5e\u89e6\u70b9\u65c1\u8def\u70ed\u7535\u6d41\u4f1a\u4ea7\u751f\u975e\u5fae\u5c0f\u7684\u70ed\u970d\u5c14\u4fe1\u53f7\uff0c\u5373\u4f7f\u5728\u5b9e\u9645\u6d4b\u91cf\u7684\u7edd\u7f18\u4f53\u7684\u70ed\u970d\u5c14\u7535\u5bfc\u7387\u4e3a\u96f6\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6548\u5e94\u9884\u6d4b\u7684\u70ed\u970d\u5c14\u7535\u5bfc\u7387\u4e0e\u5404\u79cd\u6750\u6599\u7684\u5b9e\u9645\u5b9e\u9a8c\u89c2\u6d4b\u7ed3\u679c\u76f8\u6bd4\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u901a\u8fc7\u5047\u8bbe\u6837\u54c1\u5bbd\u5ea6\u6570\u91cf\u7ea7\u7684\u6709\u6548\u89e6\u70b9\u539a\u5ea6\uff0c\u91cd\u73b0\u4e86\u5b83\u4eec\u7684\u6e29\u5ea6\u4f9d\u8d56\u6027\u548c\u5e45\u5ea6\u3002\u5b83\u751a\u81f3\u53ef\u4ee5\u91cd\u73b0\u5177\u6709\u9ad8\u548c\u4f4e\u7eb5\u5411\u70ed\u5bfc\u7387\u7684\u6750\u6599\u4e4b\u95f4\u6e29\u5ea6\u4f9d\u8d56\u6027\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u91d1\u5c5e\u89e6\u70b9\u65c1\u8def\u70ed\u7535\u6d41\u4ea7\u751f\u7684\u70ed\u970d\u5c14\u4fe1\u53f7\u7684\u9884\u6d4b\u4e0e\u5b9e\u9645\u5b9e\u9a8c\u89c2\u6d4b\u7ed3\u679c\u76f8\u5f53\uff0c\u91cd\u73b0\u4e86\u6e29\u5ea6\u4f9d\u8d56\u6027\u548c\u5e45\u5ea6\uff0c\u5e76\u80fd\u533a\u5206\u9ad8\u4f4e\u7eb5\u5411\u70ed\u5bfc\u7387\u6750\u6599\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u70ed\u970d\u5c14\u6d4b\u91cf\u4e2d\uff0c\u91d1\u5c5e\u89e6\u70b9\u65c1\u8def\u70ed\u7535\u6d41\u4f1a\u4ea7\u751f\u53ef\u89c2\u7684\u70ed\u970d\u5c14\u4fe1\u53f7\uff0c\u5373\u4f7f\u5728\u7edd\u7f18\u4f53\u5b9e\u9645\u70ed\u970d\u5c14\u7535\u5bfc\u7387\u4e3a\u96f6\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6548\u5e94\u80fd\u591f\u5f88\u597d\u5730\u91cd\u73b0\u5404\u79cd\u6750\u6599\u7684\u5b9e\u9a8c\u89c2\u6d4b\u7ed3\u679c\uff0c\u5305\u62ec\u5b83\u4eec\u7684\u6e29\u5ea6\u4f9d\u8d56\u6027\u548c\u5e45\u5ea6\uff0c\u8fd9\u8868\u660e\u9700\u8981\u6291\u5236\u65c1\u8def\u70ed\u7535\u6d41\u3002"}}
{"id": "2508.11132", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11132", "abs": "https://arxiv.org/abs/2508.11132", "authors": ["Sangwon Jo", "Seok-Hwan Park"], "title": "Multi-Satellite Cooperative MIMO Transmission: Statistical CSI-Aware RSMA Precoding Design", "comment": "accepted for publication in IEEE Wireless Communications Letters", "summary": "We investigate inter-satellite cooperative transmission in a multiple\nlow-Earth orbit (LEO) satellite communication system to enhance spectral\nefficiency. Specifically, we design multiple-input multipleoutput (MIMO)\nprecoding at LEO satellites for cooperative rate-splitting multiple access\n(RSMA). Given the difficulty of acquiring instantaneous channel state\ninformation (iCSI) due to long delays and Doppler effects, we formulate an\nergodic max-min fairness rate (MMFR) maximization problem based on statistical\nCSI (sCSI). To address the challenge of ergodic rate evaluation, we approximate\nthe problem using closed-form upper bounds and develop a weighted minimum mean\nsquared error-based algorithm to obtain a stationary point. Simulation results\ndemonstrate that the proposed sCSI-based RSMA scheme approaches iCSI-based\nperformance and significantly outperforms conventional space-division multiple\naccess.", "AI": {"tldr": "\u5728\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u5229\u7528\u5408\u4f5c\u4f20\u8f93\u548c\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u9884\u7f16\u7801\u6280\u672f\uff0c\u5e76\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08sCSI\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7387\u5206\u88c2\u591a\u5740\uff08RSMA\uff09\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u4e86\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u7684\u96be\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08iCSI\uff09\u65b9\u6848\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u7684\u9891\u8c31\u6548\u7387\uff0c\u5e76\u89e3\u51b3\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08iCSI\uff09\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u8bbe\u8ba1\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u9884\u7f16\u7801\uff0c\u9488\u5bf9\u5408\u4f5c\u7387\u5206\u88c2\u591a\u5740\uff08RSMA\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u7684\u7b97\u6cd5\u6765\u83b7\u5f97\u5e73\u7a33\u70b9", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8esCSI\u7684RSMA\u65b9\u6848\u6027\u80fd\u63a5\u8fd1\u57fa\u4e8eiCSI\u7684\u65b9\u6848\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u7a7a\u5206\u591a\u5740"}}
{"id": "2508.11515", "categories": ["cs.LO", "cs.AI", "03C13, 68T27", "F.4.0"], "pdf": "https://arxiv.org/pdf/2508.11515", "abs": "https://arxiv.org/abs/2508.11515", "authors": ["Qipeng Kuang", "V\u00e1clav K\u016fla", "Ond\u0159ej Ku\u017eelka", "Yuanhong Wang", "Yuyi Wang"], "title": "Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations", "comment": "24 pages, 5 figures", "summary": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. The boundary between fragments for which WFOMC can be computed in\npolynomial time relative to the domain size lies between the two-variable\nfragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is\nknown that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time\nalgorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$,\npossibly extended by certain axioms such as the linear order axiom, the\nacyclicity axiom, and the connectedness axiom. All existing research has\nconcentrated on extending the fragment with axioms on a single distinguished\nrelation, leaving a gap in understanding the complexity boundary of axioms on\nmultiple relations. In this study, we explore the extension of the two-variable\nfragment by axioms on two relations, presenting both negative and positive\nresults. We show that WFOMC for $\\text{FO}^2$ with two linear order relations\nand $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard.\nConversely, we provide an algorithm in time polynomial in the domain size for\nWFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and\nanother successor relation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5305\u542b\u591a\u4e2a\u5173\u7cfb\u516c\u7406\u7684FO^2\u7684WFOMC\u590d\u6742\u6027\uff0c\u53d1\u73b0\u4e86NP_1-hard\u6848\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u586b\u8865\u4e86\u5173\u4e8e\u5728\u5b58\u5728\u591a\u4e2a\u5173\u7cfb\u516c\u7406\u65f6WFOMC\u590d\u6742\u6027\u8fb9\u754c\u7684\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u6269\u5c55\u4e8c\u9636\u903b\u8f91\uff08FO^2\uff09\u7684\u516c\u7406\u5bf9\u591a\u4e2a\u5173\u7cfb\u7684\u5f71\u54cd\u6765\u5206\u6790WFOMC\u7684\u590d\u6742\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5e26\u6709\u4e24\u4e2a\u7ebf\u6027\u987a\u5e8f\u5173\u7cfb\u548c\u4e24\u4e2a\u65e0\u73af\u5173\u7cfb\u7684FO^2\u7684WFOMC\u662fNP_1-hard\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e26\u6709\u7ebf\u6027\u987a\u5e8f\u5173\u7cfb\u3001\u5176\u540e\u7ee7\u5173\u7cfb\u548c\u53e6\u4e00\u4e2a\u540e\u7ee7\u5173\u7cfb\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5177\u6709\u4e24\u4e2a\u5173\u7cfb\u516c\u7406\u7684\u4e8c\u9636\u903b\u8f91\u7247\u6bb5\u7684\u52a0\u6743\u4e00\u9636\u6a21\u578b\u8ba1\u6570\uff08WFOMC\uff09\u7684\u590d\u6742\u6027\u8fb9\u754c\uff0c\u53d1\u73b0\u4e86NP_1-hard\u548c\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u7684\u8d1f\u9762\u548c\u6b63\u9762\u7ed3\u679c\u3002"}}
{"id": "2508.11332", "categories": ["eess.SY", "cs.SY", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11332", "abs": "https://arxiv.org/abs/2508.11332", "authors": ["Chris Verhoek", "Ivan Markovsky", "Roland T\u00f3th"], "title": "Direct data-driven interpolation and approximation of linear parameter-varying system trajectories", "comment": "9 pages, 5 figures, submitted for review", "summary": "We consider the problem of estimating missing values in trajectories of\nlinear parameter-varying (LPV) systems. We solve this interpolation problem for\nthe class of shifted-affine LPV systems. Conditions for the existence and\nuniqueness of solutions are given and a direct data-driven algorithm for its\ncomputation is presented, i.e., the data-generating system is not given by a\nparametric model but is implicitly specified by data. We illustrate the\napplicability of the proposed solution on illustrative examples of a\nmass-spring-damper system with exogenous and endogenous parameter variation.", "AI": {"tldr": "\u5bf9\u4e8e\u79fb\u4f4d\u4eff\u5c04 LPV \u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u63d2\u503c\u7b97\u6cd5\u6765\u4f30\u8ba1\u8f68\u8ff9\u4e2d\u7684\u7f3a\u5931\u503c\u3002", "motivation": "\u5904\u7406\u7ebf\u6027\u53c2\u6570\u53d8\u5316\uff08LPV\uff09\u7cfb\u7edf\u8f68\u8ff9\u4e2d\u7f3a\u5931\u503c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u7684\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\u6765\u8ba1\u7b97\u7f3a\u5931\u503c\uff0c\u8be5\u7b97\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u53c2\u6570\u6a21\u578b\uff0c\u800c\u662f\u7531\u6570\u636e\u9690\u5f0f\u6307\u5b9a\u3002", "result": "\u7b97\u6cd5\u9002\u7528\u4e8e\u79fb\u4f4d\u4eff\u5c04LPV\u7cfb\u7edf\uff0c\u5e76\u5728\u4e00\u7c7b\u5177\u6709\u5916\u751f\u548c\u5185\u751f\u53c2\u6570\u53d8\u5316\u7684\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u79fb\u4f4d\u4eff\u5c04 LPV \u7cfb\u7edf\u8f68\u8ff9\u4e2d\u7f3a\u5931\u503c\u7684\u63d2\u503c\u7b97\u6cd5\uff0c\u5e76\u7ed9\u51fa\u4e86\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u7684\u6761\u4ef6\u3002"}}
{"id": "2508.11137", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11137", "abs": "https://arxiv.org/abs/2508.11137", "authors": ["Z. Hao", "J. Cochran", "Y. Chuang", "H. M. Cole", "S. Shankar"], "title": "Wireless Josephson parametric amplifier above 20 GHz", "comment": "5 pages, 3 figures, first version", "summary": "Operating superconducting qubits at elevated temperatures offers increased\ncooling power and thus system scalability, but requires suppression of thermal\nphotons to preserve coherence and readout fidelity. This motivates migration to\nhigher operation frequencies, which demands high-frequency amplification with\nnear-quantum-limited noise characteristics for qubit readout. Here, we report\nthe design and experimental realization of a wireless Josephson parametric\namplifier (WJPA) operating above 20~GHz. The wireless design eliminates losses\nand impedance mismatches that become problematic at high frequencies. The WJPA\nachieves more than 20~dB of gain across a tunable frequency range of\n21--23.5~GHz, with a typical dynamic bandwidth of 3~MHz. Through Y-factor\nmeasurements and a qubit-based photon number calibration, we show that the\namplifier exhibits an added noise of approximately two photons.", "AI": {"tldr": "\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7684\u53ef\u6269\u5c55\u6027\u9700\u8981\u9ad8\u9891\u653e\u5927\u3002\u6211\u4eec\u62a5\u544a\u4e86\u4e00\u79cd\u8fd0\u884c\u9891\u7387\u9ad8\u4e8e\u4e8c\u5341\u5409\u8d6b\u7684\u65e0\u7ebf\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff08 WJPA\uff09\uff0c\u5176\u589e\u76ca\u8d85\u8fc7\u4e8c\u5341\u4e8c\u5206\u8d1d\uff0c\u9644\u52a0\u566a\u58f0\u7ea6\u4e3a\u4e24\u4e2a\u5149\u5b50\u3002", "motivation": "\u4e3a\u4e86\u5728\u66f4\u9ad8\u7684\u5de5\u4f5c\u9891\u7387\u4e0b\u5b9e\u73b0\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u9ad8\u9891\u653e\u5927\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65e0\u7ebf\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff08 WJPA\uff09\uff0c\u5de5\u4f5c\u5728\u9ad8\u4e8e\u4e8c\u5341\u5409\u8d6b\u7684\u9891\u7387\uff0c\u7528\u4e8e\u9ad8\u9891\u653e\u5927\uff0c\u5e76\u5177\u6709\u8fd1\u91cf\u5b50\u6781\u9650\u7684\u566a\u58f0\u7279\u6027\u3002", "result": "WJPA \u5728\u4e8c\u5341\u4e00\u5230\u4e8c\u5341\u4e09\u70b9\u4e94\u5409\u8d6b\u7684\u9891\u7387\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u8d85\u8fc7\u4e8c\u5341\u4e8c\u5206\u8d1d\u7684\u589e\u76ca\uff0c\u5177\u6709\u4e09\u5146\u8d6b\u7684\u5178\u578b\u52a8\u6001\u5e26\u5bbd\uff0c\u5e76\u4e14\u9644\u52a0\u566a\u58f0\u7ea6\u4e3a\u4e24\u4e2a\u5149\u5b50\u3002ansir\u91cf\u6d4b\u91cf\u548c\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u7684\u5149\u5b50\u6570\u6821\u51c6\u8bc1\u5b9e\u4e86\u8be5\u653e\u5927\u5668\u7684\u6027\u80fd\u3002 WJPAs \u7684\u65e0\u7ebf\u8bbe\u8ba1\u6d88\u9664\u4e86\u5728\u9ad8\u9891\u4e0b\u4f1a\u5f15\u8d77\u95ee\u9898\u7684\u635f\u8017\u548c\u963b\u6297\u5931\u914d\u3002", "conclusion": "\u5728\u4e8c\u5341\u4e8c\u5409\u8d6b\u9644\u8fd1\uff0c\u7ea6\u4e24\u4e2a\u5149\u5b50\u7684\u9644\u52a0\u566a\u58f0\uff08\u76f8\u5f53\u4e8e\u91cf\u5b50\u6781\u9650\u76841.1\u500d\uff09\u88ab\u8ba4\u4e3a\u662f\u5728\u4f7f\u7528\u7ea6\u5341\u4ebf\u5206\u4e4b\u4e00\u79d2\u7684\u6d4b\u91cf\u65f6\u95f4\u5c3a\u5ea6\u65f6\uff0c\u5177\u6709\u7ea6\u4e8c\u5341\u4e8c\u5409\u8d6b\u7684\u5e26\u5bbd\u4ee5\u53ca\u8d85\u8fc7\u4e8c\u5341\u4e8c\u5206\u8d1d\u7684\u589e\u76ca\u3002"}}
{"id": "2508.11093", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "This paper enhances a robot's ability to understand user goals for navigation and manipulation by integrating vision-language models and language models with an existing framework (GUIDER). The system filters objects and locations based on user prompts, allowing the robot to select the correct targets and adapt to changing intents for smoother human-robot collaboration. Future work involves testing on a simulated robot.", "motivation": "Human-robot collaboration necessitates robots that can rapidly infer user intent, offer clear reasoning, and support users in accomplishing their objectives. This work aims to improve the ability of robots to understand and act upon user commands in complex environments.", "method": "The approach augments the GUIDER framework with a vision pipeline (YOLO, SAM) and language models (VLM, LLM). The vision pipeline identifies candidate objects, which are then scored by the VLM for relevance to the operator's prompt. A text-only LLM ranks object labels. These scores are used to weight GUIDER's existing layers, selecting context-relevant targets and suppressing irrelevant ones. Autonomy transitions occur when a combined belief threshold is met, allowing the robot to navigate and retrieve objects while adapting to intent changes.", "result": "The system weights existing navigation and manipulation layers of GUIDER using relevance scores from a VLM and LLM, enabling context-relevant target selection and suppression of unrelated objects. This leads to improved intent inference and task assistance for robots.", "conclusion": "The proposed system integrates a VLM and LLM to enhance the GUIDER framework for human-robot collaboration, improving intent inference and target selection for navigation and manipulation tasks."}}
{"id": "2508.11252", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "LRMs\u5728\u89e3\u51b3\u4e0d\u5b8c\u6574\u6570\u5b66\u95ee\u9898\u65f6\uff0c\u65e0\u6cd5\u4e3b\u52a8\u8be2\u95ee\u7f3a\u5931\u4fe1\u606f\uff0c\u5e76\u4e14\u4f1a\u51fa\u73b0\u8fc7\u5ea6\u601d\u8003\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u6570\u636e\u96c6\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u65e8\u5728\u63d0\u5347LRMs\u7684\u771f\u6b63\u667a\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4ec5\u5173\u6ce8\u5df2\u5b9a\u4e49\u95ee\u9898\uff0c\u672a\u80fd\u4f53\u73b0\u667a\u80fd\u4f53\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e0d\u5b8c\u6574\u95ee\u9898\u7684\u6df7\u5408\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "LRMs\u5728\u4e0d\u5b8c\u6574\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u4e3b\u52a8\u8be2\u95ee\u4fe1\u606f\uff0c\u5e76\u66b4\u9732\u51fa\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u548c\u201c\u5e7b\u89c9\u201d\u95ee\u9898\u3002", "conclusion": "LRMs\u5728\u9700\u8981\u4e3b\u52a8\u8be2\u95ee\u4fe1\u606f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u548c\u201c\u5e7b\u89c9\u201d\u7b49\u95ee\u9898\uff0c\u4f46\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6709\u6f5c\u529b\u6539\u5584\u3002"}}
{"id": "2508.11242", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11242", "abs": "https://arxiv.org/abs/2508.11242", "authors": ["M. S. Shustin", "S. V. Aksenov", "I. S. Burmistrov"], "title": "Dissipation-Induced Steady States in Topological Superconductors: Mechanisms and Design Principles", "comment": "40 pages, 3 figures", "summary": "The search for conditions supporting degenerate steady states in\nnonequilibrium topological superconductors is important for advancing\ndissipative quantum engineering, a field that has attracted significant\nresearch attention over the past decade. In this study, we address this problem\nby investigating topological superconductors hosting unpaired Majorana modes\nunder the influence of environmental dissipative fields. Within the\nGorini-Kossakowski-Sudarshan-Lindblad framework and the third quantization\nformalism, we establish a correspondence between equilibrium Majorana zero\nmodes and non-equilibrium kinetic zero modes. We further derive a simple\nalgebraic relation between the numbers of these excitations expressed in terms\nof hybridization between the single-particle wavefunctions and linear\ndissipative fields. Based on these findings, we propose a practical recipes how\nto stabilize degenerate steady states in topological superconductors through\ncontrolled dissipation engineering. To demonstrate their applicability, we\nimplement our general framework in the BDI-class Kitaev chain with long-range\nhopping and pairing terms -- a system known to host a robust edge-localized\nMajorana modes.", "AI": {"tldr": "Researchers explored topological superconductors with Majorana modes in dissipative environments, finding a way to stabilize steady states using controlled dissipation. They linked equilibrium and non-equilibrium 'zero modes' and showed it works on a Kitaev chain model.", "motivation": "The motivation stems from the importance of finding conditions that support degenerate steady states in nonequilibrium topological superconductors, which is crucial for advancing dissipative quantum engineering.", "method": "The research utilizes the Gorini-Kossakowski-Sudarshan-Lindblad framework and third quantization formalism to investigate topological superconductors with unpaired Majorana modes subjected to environmental dissipative fields. A key aspect involves deriving an algebraic relation between equilibrium Majorana zero modes and non-equilibrium kinetic zero modes, based on hybridization between single-particle wavefunctions and linear dissipative fields.", "result": "A correspondence between equilibrium Majorana zero modes and non-equilibrium kinetic zero modes is established. An algebraic relation is derived linking the numbers of these excitations to hybridization and dissipative fields. This leads to a proposed method for stabilizing degenerate steady states in topological superconductors via dissipation engineering, demonstrated on the BDI-class Kitaev chain.", "conclusion": "The study establishes a correspondence between equilibrium and non-equilibrium zero modes in topological superconductors under dissipative fields, proposing a practical method for stabilizing degenerate steady states through controlled dissipation engineering. The framework is demonstrated on the BDI-class Kitaev chain."}}
{"id": "2508.10971", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10971", "abs": "https://arxiv.org/abs/2508.10971", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "Rule2Text uses LLMs to explain complex logical rules from knowledge graphs in natural language, making them easier to understand. The framework includes advanced evaluation methods and a fine-tuned model for high-quality explanations.", "motivation": "Logical rules mined from knowledge graphs (KGs) are often difficult for humans to interpret due to complexity and idiosyncratic labeling. This work aims to improve KG accessibility and usability by generating natural language explanations for these rules.", "method": "Developed Rule2Text, a framework leveraging LLMs for natural language explanations of logical rules. Evaluated multiple LLMs with various prompting strategies (zero-shot, few-shot, variable type incorporation, Chain-of-Thought). Conducted human evaluation and developed an LLM-as-a-judge framework for scalability. Fine-tuned the Zephyr model using curated datasets and integrated a type inference module.", "result": "Fine-tuning the Zephyr model significantly improved explanation quality, especially on domain-specific datasets. The LLM-as-a-judge framework showed strong agreement with human evaluation. The type inference module supports KGs lacking explicit type information.", "conclusion": "LLMs can generate natural language explanations for mined logical rules, improving KG accessibility. Fine-tuning Zephyr with high-quality datasets resulted in significant explanation quality improvements. The Rule2Text framework, including an LLM-as-a-judge evaluation and a type inference module, enhances KG usability."}}
{"id": "2508.10975", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10975", "abs": "https://arxiv.org/abs/2508.10975", "authors": ["Pratyush Maini", "Vineeth Dorna", "Parth Doshi", "Aldo Carranza", "Fan Pan", "Jack Urbanek", "Paul Burstein", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Charvi Bannur", "Christina Baek", "Darren Teh", "David Schwab", "Haakon Mongstad", "Haoli Yin", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Ricardo Monti", "Rishabh Adiga", "Siddharth Joshi", "Spandan Das", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining", "comment": null, "summary": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.", "AI": {"tldr": "BeyondWeb\u662f\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u80fd\u663e\u8457\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6027\u80fd\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7684\u751f\u6210\u9700\u8981\u7efc\u5408\u4f18\u5316\u591a\u79cd\u56e0\u7d20\u3002", "motivation": "\u968f\u7740LLM\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u6027\u80fd\u63d0\u5347\u9010\u6e10\u9047\u5230\u74f6\u9888\uff08\u6570\u636e\u5899\uff09\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u63d0\u9ad8\u6027\u80fd\u7684\u6709\u671b\u9014\u5f84\uff0c\u4f46\u5f71\u54cd\u5408\u6210\u6570\u636e\u8d28\u91cf\u7684\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faBeyondWeb\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\u3002", "result": "BeyondWeb\u572814\u9879\u57fa\u51c6\u8bc4\u4f30\u4e2d\u5e73\u5747\u6027\u80fd\u4f18\u4e8eCosmopedia\u548cNemotron-Synth\u3002BeyondWeb\u8bad\u7ec3\u901f\u5ea6\u6bd4Cosmopedia\u5feb7.7\u500d\uff0c\u6bd4Nemotron-Synth\u5feb2.7\u500d\u3002\u4f7f\u7528BeyondWeb\u8bad\u7ec3\u76843B\u6a21\u578b\u5728180B token\u4e0a\u4f18\u4e8e\u5728Cosmopedia\u4e0a\u8bad\u7ec3\u76848B\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u6570\u636e\u8d28\u91cf\u5f71\u54cd\u56e0\u7d20\u7684\u89c1\u89e3\u3002", "conclusion": "\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u5408\u6210\u6570\u636e\u9700\u8981\u7efc\u5408\u4f18\u5316\u591a\u4e2a\u56e0\u7d20\uff0c\u6ca1\u6709\u4e00\u8e74\u800c\u5c31\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11415", "abs": "https://arxiv.org/abs/2508.11415", "authors": ["Ra\u00efssa Nataf", "Yoram Moses"], "title": "Time, Fences and the Ordering of Events in TSO", "comment": null, "summary": "The Total Store Order (TSO) is arguably the most widely used relaxed memory\nmodel in multiprocessor architectures, widely implemented, for example in\nIntel's x86 and x64 platforms. It allows processes to delay the visibility of\nwrites through store buffering. While this supports hardware-level\noptimizations and makes a significant contribution to multiprocessor\nefficiency, it complicates reasoning about correctness, as executions may\nviolate sequential consistency. Ensuring correct behavior often requires\ninserting synchronization primitives such as memory fences ($F$) or atomic\nread-modify-write ($RMW$) operations, but this approach can incur significant\nperformance costs. In this work, we develop a semantic framework that precisely\ncharacterizes when such synchronization is necessary under TSO. We introduce a\nnovel TSO-specific occurs-before relation, which adapts Lamport's celebrated\nhappens-before relation from asynchronous message-passing systems to the TSO\nsetting. Our main result is a theorem that proves that the only way to ensure\nthat two events that take place at different sites are temporally ordered is by\nhaving the execution create an occurs-before chain between the events. By\nstudying the role of fences and $RMW$s in creating occurs-before chains, we are\nthen able to capture cases in which these costly synchronization operations are\nunavoidable. Since proper real-time ordering of events is a fundamental aspect\nof consistency conditions such as Linearizability, our analysis provides a\nsound theoretical understanding of essential aspects of the TSO model. In\nparticular, we are able to generalize prior lower bounds for linearizable\nimplementations of shared memory objects. Our results capture the structure of\ninformation flow and causality in the TSO model by extending the standard\ncommunication-based reasoning from asynchronous systems to the TSO memory\nmodel.", "AI": {"tldr": "The paper analyzes the TSO memory model, introducing a new 'occurs-before' relation to determine when synchronization (like fences) is needed to ensure correct event ordering, reducing unnecessary performance costs and providing theoretical bounds for shared memory objects.", "motivation": "The Total Store Order (TSO) memory model, widely used in multiprocessor architectures like Intel's x86/x64, allows write buffering for hardware optimizations but complicates correctness reasoning due to potential violations of sequential consistency. Ensuring correctness often requires costly synchronization primitives (memory fences or atomic RMWs). This work is motivated by the need to precisely understand when these synchronization primitives are necessary under TSO to avoid performance costs.", "method": "The paper develops a semantic framework and introduces a novel TSO-specific occurs-before relation, adapting Lamport's happens-before relation to the TSO setting. It then proves a theorem stating that an occurs-before chain is necessary to temporally order events at different sites. The analysis studies the role of fences and RMWs in creating these chains to identify unavoidable synchronization costs.", "result": "The paper introduces a TSO-specific occurs-before relation and a theorem proving that temporal ordering of events at different sites requires an occurs-before chain. It identifies cases where costly synchronization operations are unavoidable and generalizes prior lower bounds for linearizable implementations of shared memory objects. The analysis captures information flow and causality in TSO by extending reasoning from asynchronous systems.", "conclusion": "This work provides a semantic framework and a novel TSO-specific occurs-before relation to precisely characterize when synchronization primitives like memory fences or atomic RMW operations are necessary under the TSO memory model. The main result is a theorem proving that an occurs-before chain is required to temporally order events at different sites. The analysis captures the necessity of synchronization operations, generalizes prior lower bounds for linearizable shared memory objects, and extends communication-based reasoning to the TSO model, offering a sound theoretical understanding of TSO's essential aspects and information flow."}}
{"id": "2508.11315", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11315", "abs": "https://arxiv.org/abs/2508.11315", "authors": ["Fried-Conrad Weber", "Felix Hartmann", "Matias Bargheer", "Janet Anders", "Richard F. L. Evans"], "title": "Atomistic spin dynamics with quantum colored noise", "comment": "13 pages, 7 figures, 3 tables, comments are welcome", "summary": "The accurate prediction of temperature-dependent magnetization dynamics is a\nfundamental challenge in computational magnetism. While Atomistic Spin Dynamics\n(ASD) simulations have emerged as a powerful tool for studying magnetic\nphenomena, their classical nature leads to significant deviations from\nexperimental observations, particularly at low temperatures. Here we present a\ncomprehensive implementation of quantum-corrected ASD into the Vampire software\npackage, based on the open-system Landau-Lifshitz-Gilbert equation with a\nquantum thermostat. Our implementation incorporates memory effects along with\ncolored noise derived from quantum-mechanical considerations that improve the\ndescription of the equilibrium magnetization. We demonstrate excellent\nquantitative agreement with experimental magnetization curves for nickel and\ngadolinium across the full temperature range. Our results establish that\nincorporating quantum environmental effects and colored noise substantially\nenhances the predictive capabilities of ASD simulations, providing a robust\nframework for modeling temperature-dependent magnetic phenomena in localized\nmoment magnetic systems.", "AI": {"tldr": "\u901a\u8fc7\u5728ASD\u6a21\u62df\u4e2d\u52a0\u5165\u91cf\u5b50\u4fee\u6b63\u548c\u6709\u8272\u566a\u58f0\uff0c\u6211\u4eec\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u78c1\u5316\u52a8\u529b\u5b66\uff0c\u5e76\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\u3002", "motivation": "\u7ecf\u5178\u7684ASD\u6a21\u62df\u5728\u9884\u6d4b\u6e29\u5ea6\u4f9d\u8d56\u7684\u78c1\u5316\u52a8\u529b\u5b66\u65b9\u9762\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6e29\u4e0b\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u5bf9\u66f4\u7cbe\u786e\u6a21\u62df\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u4fee\u6b63\u7684ASD\u65b9\u6cd5\uff0c\u5c06\u5176\u96c6\u6210\u5230Vampire\u8f6f\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5f00\u653e\u7cfb\u7edfLandau-Lifshitz-Gilbert\u65b9\u7a0b\u548c\u91cf\u5b50\u6052\u6e29\u5668\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e86\u8bb0\u5fc6\u6548\u5e94\u548c\u6e90\u4e8e\u91cf\u5b50\u529b\u5b66\u8003\u8651\u7684\u6709\u8272\u566a\u58f0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u954d\u548c\u9486\u7684\u6574\u4e2a\u6e29\u5ea6\u8303\u56f4\u5185\u4e0e\u5b9e\u9a8c\u78c1\u5316\u66f2\u7ebf\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5b9a\u91cf\u4e00\u81f4\u6027\uff0c\u8bc1\u660e\u4e86\u5305\u542b\u91cf\u5b50\u73af\u5883\u6548\u5e94\u548c\u6709\u8272\u566a\u58f0\u80fd\u591f\u6709\u6548\u63d0\u5347ASD\u6a21\u62df\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u91cf\u5b50\u4fee\u6b63\u7684ASD\u96c6\u6210\u5230Vampire\u8f6f\u4ef6\u4e2d\uff0c\u5e76\u7ed3\u5408\u8003\u8651\u91cf\u5b50\u529b\u5b66\u6548\u5e94\u7684\u542b\u8bb0\u5fc6\u6548\u5e94\u548c\u6709\u8272\u566a\u58f0\u7684\u5f00\u653e\u7cfb\u7edfLLG\u65b9\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6e29\u5ea6\u76f8\u5173\u78c1\u5316\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86ASD\u6a21\u62df\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u5c40\u90e8\u78c1\u77e9\u78c1\u6027\u7cfb\u7edf\u7684\u6e29\u5ea6\u76f8\u5173\u78c1\u6027\u73b0\u8c61\u5efa\u6a21\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u6846\u67b6\u3002"}}
{"id": "2508.10929", "categories": ["cs.NE", "math.DS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.10929", "abs": "https://arxiv.org/abs/2508.10929", "authors": ["Eddy Kwessi"], "title": "Allee Synaptic Plasticity and Memory", "comment": null, "summary": "Neural plasticity is fundamental to memory storage and retrieval in\nbiological systems, yet existing models often fall short in addressing noise\nsensitivity and unbounded synaptic weight growth. This paper investigates the\nAllee-based nonlinear plasticity model, emphasizing its biologically inspired\nweight stabilization mechanisms, enhanced noise robustness, and critical\nthresholds for synaptic regulation. We analyze its performance in memory\nretention and pattern retrieval, demonstrating increased capacity and\nreliability compared to classical models like Hebbian and Oja's rules. To\naddress temporal limitations, we extend the model by integrating time-dependent\ndynamics, including eligibility traces and oscillatory inputs, resulting in\nimproved retrieval accuracy and resilience in dynamic environments. This work\nbridges theoretical insights with practical implications, offering a robust\nframework for modeling neural adaptation and informing advances in artificial\nintelligence and neuroscience.", "AI": {"tldr": "Allee\u6a21\u578b\u5728\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u6bd4Hebbian\u548cOja\u6a21\u578b\u66f4\u597d\uff0c\u5e76\u4e14\u901a\u8fc7\u6dfb\u52a0\u65f6\u95f4\u52a8\u6001\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u566a\u58f0\u654f\u611f\u6027\u548c\u7a81\u89e6\u6743\u91cd\u65e0\u754c\u589e\u957f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u7814\u7a76\u4e86Allee\u57fa\u7840\u7684\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u566a\u58f0\u9c81\u68d2\u6027\u548c\u7a81\u89e6\u6743\u91cd\u7a33\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u5c06Allee\u57fa\u7840\u7684\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\u4e0e\u65f6\u95f4\u4f9d\u8d56\u6027\u52a8\u529b\u5b66\uff08\u5305\u62ec\u5408\u683c\u6027\u75d5\u8ff9\u548c\u632f\u8361\u8f93\u5165\uff09\u76f8\u7ed3\u5408\uff0c\u5206\u6790\u4e86\u5176\u5728\u8bb0\u5fc6\u4fdd\u6301\u548c\u6a21\u5f0f\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u4e0eHebbian\u548cOja\u7b49\u7ecf\u5178\u6a21\u578b\u76f8\u6bd4\uff0cAllee\u57fa\u7840\u7684\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\u5728\u8bb0\u5fc6\u4fdd\u6301\u548c\u6a21\u5f0f\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bb9\u91cf\u548c\u53ef\u9760\u6027\uff0c\u5e76\u4e14\u5728\u96c6\u6210\u65f6\u95f4\u4f9d\u8d56\u6027\u52a8\u529b\u5b66\u540e\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u68c0\u7d22\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Allee\u57fa\u7840\u7684\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\u52a8\u529b\u5b66\uff0c\u5728\u8bb0\u5fc6\u4fdd\u6301\u548c\u6a21\u5f0f\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5bb9\u91cf\u548c\u53ef\u9760\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11178", "abs": "https://arxiv.org/abs/2508.11178", "authors": ["Yida Zhang", "Qiuyan Liu", "Qiang Wang", "Hongtao Luo", "Yuqi Xia"], "title": "Near-Field Variable-Width Beam Coverage and Codebook Design for XL-RIS", "comment": null, "summary": "To mitigate the issue of limited base station coverage caused by severe\nhigh-frequency electromagnetic wave attenuation, Extremely Large Reconfigurable\nIntelligent Surface (XL-RIS) has garnered significant attention due to its high\nbeam gain. However, XL-RIS exhibits a narrower beam width compared to\ntraditional RIS, which increases the complexity of beam alignment and\nbroadcast. To address this problem, we propose a variable-width beam generation\nalgorithm under the near-field assumption and apply it to the near-field\ncodebook design for XL-RIS. Our algorithm can achieve beam coverage for\narbitrarily shaped codeword regions and generate a joint codebook for the\nmulti-XL-RIS system. The simulation results demonstrate that our proposed\nscheme enables user equipment (UE) to achieve higher spectral efficiency and\nlower communication outage probability within the codeword region compared to\nexisting works. Furthermore, our scheme exhibits better robustness to codeword\nregion location and area variations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdXL-RIS\u8fd1\u573a\u7801\u672c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u53d8\u91cf\u6ce2\u675f\u751f\u6210\u7b97\u6cd5\uff0c\u63d0\u9ad8UE\u6027\u80fd\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3XL-RIS\u6ce2\u675f\u5bbd\u5ea6\u7a84\u3001\u6ce2\u675f\u5bf9\u9f50\u548c\u5e7f\u64ad\u590d\u6742\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5176\u9ad8\u6ce2\u675f\u589e\u76ca\u6765\u7f13\u89e3\u9ad8\u9891\u7535\u78c1\u6ce2\u8870\u51cf\u5bfc\u81f4\u7684\u57fa\u7ad9\u8986\u76d6\u53d7\u9650\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u91cf\u6ce2\u675f\u751f\u6210\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eXL-RIS\u7684\u8fd1\u573a\u7801\u672c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u5f62\u72b6\u7801\u672c\u533a\u57df\u7684\u6ce2\u675f\u8986\u76d6\uff0c\u5e76\u751f\u6210\u4e86\u591aXL-RIS\u7cfb\u7edf\u7684\u8054\u5408\u7801\u672c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u5728\u7801\u672c\u533a\u57df\u5185\u83b7\u5f97\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u548c\u66f4\u4f4e\u901a\u4fe1\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u4e14\u5bf9\u7801\u672c\u533a\u57df\u7684\u4f4d\u7f6e\u548c\u9762\u79ef\u53d8\u5316\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d85\u5927\u5c3a\u5bf8\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08XL-RIS\uff09\u7684\u8fd1\u573a\u7801\u672c\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u53d8\u91cf\u6ce2\u675f\u751f\u6210\u7b97\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u5728\u7801\u672c\u533a\u57df\u5185\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u548c\u66f4\u4f4e\u7684\u901a\u4fe1\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u5bf9\u7801\u672c\u533a\u57df\u7684\u4f4d\u7f6e\u548c\u9762\u79ef\u53d8\u5316\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11623", "categories": ["cs.LO", "06B35, 06F07", "F.3.2"], "pdf": "https://arxiv.org/pdf/2508.11623", "abs": "https://arxiv.org/abs/2508.11623", "authors": ["Francesco Dagnino", "Amin Farjudian Eugenio Moggi"], "title": "Robust Topology and the Hausdorff-Smyth Monad on Metric Spaces over Continuous Quantales", "comment": "28 pages, 6 figures", "summary": "We define a (preorder-enriched) category $\\mathsf{Met}$ of quantale-valued\nmetric spaces and uniformly continuous maps, with the essential requirement\nthat the quantales are continuous. For each object $(X,d,Q)$ in this category,\nwhere $X$ is the carrier set, $Q$ is a continuous quantale, and $d: X \\times X\n\\to Q$ is the metric, we consider a topology $\\tau_d$ on $X$, which generalizes\nthe open ball topology, and a topology $\\tau_{d,R}$ on the powerset\n$\\mathsf{P}(X)$, called the robust topology, which captures robustness with\nrespect to small perturbations of parameters. We define a (preorder-enriched)\nmonad $\\mathsf{P}_S$ on $\\mathsf{Met}$, called the Hausdorff-Smyth monad, which\ncaptures the robust topology, in the sense that the open ball topology of the\nobject $\\mathsf{P}_S(X,d,Q)$ coincides with the robust topology $\\tau_{d,R}$\nfor the object $(X,d,Q)$. We prove that every topology arises from a\nquantale-valued metric. As such, our framework provides a foundation for\nquantitative reasoning about imprecision and robustness in a wide range of\ncomputational and physical systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5305\u542b\u8fde\u7eed\u91cf\u5316\u503c\u7684\u5ea6\u91cf\u7a7a\u95f4\u8303\u7574\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u6355\u6349\u9c81\u68d2\u62d3\u6251\u7684 Hausdorff-Smyth \u5e7a\u534a\u7fa4\u3002", "motivation": "\u4e3a\u4e86\u6355\u6349\u9c81\u68d2\u6027\uff0c\u5373\u5728\u53c2\u6570\u7684\u5c0f\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u4e2a\uff08\u9884\u5e8f\u5bcc\u96c6\uff09\u8303\u7574 $\\\\mathsf{Met}$\uff0c\u5176\u4e2d\u5305\u542b\u8fde\u7eed\u91cf\u5316\u503c\u7684\u5ea6\u91cf\u7a7a\u95f4\u548c\u4e00\u81f4\u8fde\u7eed\u6620\u5c04\u3002\u4e3a\u6bcf\u4e2a\u5bf9\u8c61 $(X,d,Q)$ \u8003\u8651\u4e00\u4e2a\u62d3\u6251 $\\\\tau_d$ \u548c\u4e00\u4e2a\u79f0\u4e3a\u9c81\u68d2\u62d3\u6251\u7684 $\\\\tau_{d,R}$\u3002\u5b9a\u4e49\u4e86\u4e00\u4e2a\uff08\u9884\u5e8f\u5bcc\u96c6\uff09\u5e7a\u534a\u7fa4 $\\\\mathsf{P}_S$\uff0c\u79f0\u4e3a Hausdorff-Smyth \u5e7a\u534a\u7fa4\uff0c\u5b83\u6355\u6349\u4e86\u9c81\u68d2\u62d3\u6251\uff0c\u4f7f\u5f97\u5bf9\u8c61 $\\\\mathsf{P}_S(X,d,Q)$ \u7684\u5f00\u7403\u62d3\u6251\u4e0e\u5bf9\u8c61\u7684\u9c81\u68d2\u62d3\u6251 $\\\\tau_{d,R}$ \u76f8\u543b\u5408\u3002\u8bc1\u660e\u4e86\u6bcf\u4e2a\u62d3\u6251\u90fd\u6e90\u4e8e\u91cf\u5316\u503c\u5ea6\u91cf\u3002", "result": "\u8bc1\u660e\u4e86\u6bcf\u4e2a\u62d3\u6251\u90fd\u6e90\u4e8e\u91cf\u5316\u503c\u5ea6\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9a\u91cf\u63a8\u7406\u4e0d\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11381", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11381", "abs": "https://arxiv.org/abs/2508.11381", "authors": ["Yusen Wei", "Lan Tang"], "title": "System Synchronization Based on Complex Frequency", "comment": null, "summary": "In response to the inertia decline caused by high penetration of renewable\ngeneration, traditional synchronization criteria that rely solely on frequency\nconsistency are increasingly inadequate for characterizing the coupled behavior\nof frequency and voltage dynamics during power-system transients. This paper\nfocuses on the theory of complex-frequency synchronization and develops a\ntheory-simulation analysis framework that offers a new perspective for\nsteady-state and transient analysis of low-inertia power systems. First, the\nfundamental concepts and theoretical foundations of complex-frequency\nsynchronization are presented in detail. Second, local and global dynamic\nsynchronization criteria are derived and the concept of generalized inertia is\nintroduced, which unifies the conventional inertial support to frequency with\nthe inertia-like support of voltage, thereby providing an accurate measure of\nregion-level coupled support strength for voltage and frequency. Finally,\nnumerical case studies on the IEEE 9-bus system validate the effectiveness of\nthe proposed theoretical methods and criteria, and demonstrate a visualization\nworkflow for key indicators such as disturbance impact zones and\ngeneralized-inertia regions.", "AI": {"tldr": "This paper introduces complex-frequency synchronization and generalized inertia to analyze low-inertia power systems, addressing the inadequacy of traditional methods in handling coupled frequency and voltage dynamics.", "motivation": "Traditional synchronization criteria are inadequate for characterizing coupled frequency and voltage dynamics in low-inertia power systems due to the inertia decline caused by high renewable generation penetration.", "method": "The paper presents the theory of complex-frequency synchronization, derives local and global dynamic synchronization criteria, and introduces the concept of generalized inertia to unify frequency and voltage support.", "result": "The study introduces complex-frequency synchronization, generalized inertia, and a visualization workflow for key indicators, demonstrating their effectiveness in analyzing low-inertia power systems.", "conclusion": "The proposed complex-frequency synchronization theory and analysis framework provide a new perspective for analyzing low-inertia power systems, validating its effectiveness through numerical case studies."}}
{"id": "2508.11175", "categories": ["quant-ph", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11175", "abs": "https://arxiv.org/abs/2508.11175", "authors": ["Ali Karimi", "Hadi Zadeh-Haghighi", "Youssef Kora", "Christoph Simon"], "title": "The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators", "comment": null, "summary": "Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently\nprocess temporal data. In this work, we investigate a QRC framework based on\ntwo coupled Kerr nonlinear oscillators, a system well-suited for time-series\nprediction tasks due to its complex nonlinear interactions and potentially\nhigh-dimensional state space. We explore how its performance in time-series\nprediction depends on key physical parameters: input drive strength, Kerr\nnonlinearity, and oscillator coupling, and analyze the role of entanglement in\nimproving the reservoir's computational performance, focusing on its effect on\npredicting non-trivial time series. Using logarithmic negativity to quantify\nentanglement and normalized root mean square error (NRMSE) to evaluate\npredictive accuracy, our results suggest that entanglement provides a\ncomputational advantage on average-up to a threshold in the input\nfrequency-that persists under some levels of dissipation and dephasing. In\nparticular, we find that higher dissipation rates can enhance performance.\nWhile the entanglement advantage manifests as improvements in both average and\nworst-case performance, it does not lead to improvements in the best-case\nerror. These findings contribute to the broader understanding of quantum\nreservoirs for high performance, efficient quantum machine learning and\ntime-series forecasting.", "AI": {"tldr": "\u91cf\u5b50\u6c34\u5e93\u8ba1\u7b97\u4f7f\u7528\u91cf\u5b50\u52a8\u529b\u5b66\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u672c\u7814\u7a76\u4f7f\u7528\u8026\u5408\u7684\u514b\u5c14\u632f\u8361\u5668\uff0c\u53d1\u73b0\u7ea0\u7f20\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u4f46\u5bf9\u6700\u4f73\u8bef\u5dee\u65e0\u6539\u5584\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u57fa\u4e8e\u91cf\u5b50\u52a8\u529b\u5b66\u7684\u91cf\u5b50\u6c34\u5e93\u8ba1\u7b97\uff08QRC\uff09\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u63a2\u8ba8\u8026\u5408\u514b\u5c14\u975e\u7ebf\u6027\u632f\u8361\u5668\u4f5c\u4e3aQRC\u7684\u9002\u7528\u6027\uff0c\u4ee5\u53ca\u7ea0\u7f20\u5728\u63d0\u9ad8\u6c34\u5e93\u8ba1\u7b97\u6027\u80fd\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u4e24\u4e2a\u8026\u5408\u7684\u514b\u5c14\u975e\u7ebf\u6027\u632f\u8361\u5668\u6784\u5efa\u91cf\u5b50\u6c34\u5e93\u8ba1\u7b97\uff08QRC\uff09\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e86\u8f93\u5165\u9a71\u52a8\u5f3a\u5ea6\u3001\u514b\u5c14\u975e\u7ebf\u6027\u3001\u632f\u8361\u5668\u8026\u5408\u4ee5\u53ca\u7ea0\u7f20\u7b49\u5173\u952e\u7269\u7406\u53c2\u6570\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u5bf9\u6570\u8d1f\u503c\u91cf\u5316\u7ea0\u7f20\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\uff08NRMSE\uff09\u8bc4\u4f30\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ea0\u7f20\u53ef\u4ee5\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u975e\u5e73\u51e1\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u4e0a\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u8f83\u9ad8\u7684\u8017\u6563\u7387\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u7ea0\u7f20\u5e76\u672a\u80fd\u6539\u5584\u6700\u4f73\u60c5\u51b5\u4e0b\u7684\u8bef\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e00\u5b9a\u8f93\u5165\u9891\u7387\u9608\u503c\u5185\uff0c\u7ea0\u7f20\u53ef\u4ee5\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u4e14\u8fd9\u79cd\u4f18\u52bf\u5728\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8017\u6563\u548c\u9000\u76f8\u5e72\u4e0b\u4ecd\u7136\u5b58\u5728\u3002\u7136\u800c\uff0c\u7ea0\u7f20\u5e76\u672a\u80fd\u6539\u5584\u6700\u4f73\u60c5\u51b5\u4e0b\u7684\u8bef\u5dee\u3002"}}
{"id": "2508.11117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11117", "abs": "https://arxiv.org/abs/2508.11117", "authors": ["Xuning Yang", "Clemens Eppner", "Jonathan Tremblay", "Dieter Fox", "Stan Birchfield", "Fabio Ramos"], "title": "Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective", "comment": "2025 Robot: Science and Systems (RSS) Workshop on Robot Evaluation\n  for the Real World", "summary": "Current vision-based robotics simulation benchmarks have significantly\nadvanced robotic manipulation research. However, robotics is fundamentally a\nreal-world problem, and evaluation for real-world applications has lagged\nbehind in evaluating generalist policies. In this paper, we discuss challenges\nand desiderata in designing benchmarks for generalist robotic manipulation\npolicies for the goal of sim-to-real policy transfer. We propose 1) utilizing\nhigh visual-fidelity simulation for improved sim-to-real transfer, 2)\nevaluating policies by systematically increasing task complexity and scenario\nperturbation to assess robustness, and 3) quantifying performance alignment\nbetween real-world performance and its simulation counterparts.", "AI": {"tldr": "Robotics simulation benchmarks need better sim-to-real evaluation for generalist policies. This paper suggests using high-fidelity simulation, testing with increased task difficulty and perturbations, and comparing sim-to-real performance.", "motivation": "To address the gap in evaluating generalist robotic manipulation policies for real-world applications and improve sim-to-real policy transfer.", "method": "Proposes utilizing high visual-fidelity simulation, systematically increasing task complexity and scenario perturbation for robustness assessment, and quantifying performance alignment between simulation and real-world.", "result": "The paper outlines a framework for designing benchmarks that better support the development and evaluation of generalist robotic manipulation policies with a focus on sim-to-real transfer.", "conclusion": "Current benchmarks inadequately evaluate generalist policies for real-world applications, hindering sim-to-real transfer."}}
{"id": "2508.11347", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u89e3\u51b3\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u5c3a\u5ea6\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651KG\u66f4\u65b0\u7684\u5c3a\u5ea6\u53d8\u5316\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5728\u6574\u4e2a\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u66f4\u65b0\u5c3a\u5ea6\u5e76\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u6846\u67b6\u3002", "method": "SAGE\u6846\u67b6\u9996\u5148\u6839\u636e\u66f4\u65b0\u5c3a\u5ea6\u786e\u5b9a\u5d4c\u5165\u7ef4\u5ea6\u5e76\u76f8\u5e94\u5730\u6269\u5c55\u5d4c\u5165\u7a7a\u95f4\uff0c\u7136\u540e\u91c7\u7528\u52a8\u6001\u84b8\u998f\u673a\u5236\u6765\u5e73\u8861\u5df2\u6709\u77e5\u8bc6\u7684\u5b66\u4e60\u548c\u65b0\u4e8b\u5b9e\u7684\u878d\u5165\u3002", "result": "SAGE\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728MRR\u3001H@1\u548cH@10\u6307\u6807\u4e0a\u5206\u522b\u53d6\u5f97\u4e861.38%\u30011.25%\u548c1.6%\u7684\u63d0\u5347\u3002\u4e0e\u4f7f\u7528\u56fa\u5b9a\u5d4c\u5165\u7ef4\u5ea6\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cSAGE\u5728\u6bcf\u4e2a\u5feb\u7167\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u5d4c\u5165\u7ef4\u5ea6\u5728\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5d4c\u5165\u7ef4\u5ea6\u548c\u52a8\u6001\u84b8\u998f\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u66f4\u65b0\u5c3a\u5ea6\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728MRR\u3001H@1\u548cH@10\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.10935", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "AI": {"tldr": "HQ-OV3D \u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u53c9\u9a8c\u8bc1\u63d0\u6848\u751f\u6210\u5668\u548c\u6807\u6ce8\u7c7b\u522b\u8f85\u52a9\u964d\u566a\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f00\u653e\u8bcd\u6c47 3D \u68c0\u6d4b\u4e2d\u4f2a\u6807\u7b7e\u51e0\u4f55\u8d28\u91cf\u4e0d\u9ad8\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u65b0\u589e\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e86\u73b0\u6709\u5f00\u653e\u8bcd\u6c47 3D \u68c0\u6d4b\u65b9\u6cd5\u4e2d\uff0c\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u7684\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u4f46\u5176\u51e0\u4f55\u8d28\u91cf\uff08\u5c24\u5176\u662f\u8fb9\u754c\u6846\u7cbe\u5ea6\uff09\u4ecd\u7136\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HQ-OV3D \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1. \u8de8\u6a21\u6001\u4ea4\u53c9\u9a8c\u8bc1 (IMCV) \u63d0\u6848\u751f\u6210\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u51e0\u4f55\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb 3D \u63d0\u6848\u30022. \u6807\u6ce8\u7c7b\u522b\u8f85\u52a9 (ACA) \u964d\u566a\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e DDIM \u7684\u964d\u566a\u673a\u5236\u5229\u7528\u6807\u6ce8\u7c7b\u522b\u7684\u51e0\u4f55\u5148\u9a8c\u6765\u9010\u6b65\u7cbe\u70bc 3D \u63d0\u6848\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528 HQ-OV3D \u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u65b0\u589e\u7c7b\u522b\u4e0a\u7684 mAP \u63d0\u9ad8\u4e86 7.37%\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u7684\u4f18\u8d8a\u8d28\u91cf\u3002", "conclusion": " HQ-OV3D \u6846\u67b6\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5f3a\u5927\u7684\u72ec\u7acb\u5f00\u653e\u8bcd\u6c47 3D \u68c0\u6d4b\u5668\uff0c\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u5f00\u653e\u8bcd\u6c47 3D \u68c0\u6d4b\u6216\u6ce8\u91ca\u7ba1\u9053\u7684\u5373\u63d2\u5373\u7528\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u751f\u6210\u5668\u3002"}}
{"id": "2508.11405", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.11405", "abs": "https://arxiv.org/abs/2508.11405", "authors": ["Fabian Engelke", "Christian Heiliger"], "title": "Realistic modelling of transport properties at finite tempeature in magnetic materials by local quantization of a Heisenberg model", "comment": null, "summary": "The quantitative description of the electrical resistivity of a magnetic\nmaterial remains challenging to this day. Qualitatively, it is well understood\nthat the temperature-induced lattice and spin disorder determines the\ntemperature dependence of the resistivity. While prior publications reached\ngood agreement with experiment in the so-called supercell or direct approach\nfor non-magnetic materials where the spin-disorder contribution to the\nresistivity is negligible, an accurate, purely theoretical description of\nmagnetic materials remains elusive. This shortcoming can be attributed to the\nmissing accuracy in the description of the temperature-dependent spin-disorder\nitself. In this work, we employ a joint approach from \\textit{ab-initio}\ntransport calculations and atomistic modeling of the temperature-dependent\nspin-disorder. Using the example of $\\alpha$-Fe, we demonstrate that the\ninclusion of quantum mechanical effects using a semiclassical local\nquantization of the Heisenberg model significantly improves the description of\nthe spin-disorder component to the electrical resistivity. Compared to previous\napproaches, this model includes the description of magnetic short-range order\neffects, enabling us to study temperature effects around and above the Curie\ntemperature, where prior mean-field theory-based approaches inevitably\npredicted a constant contribution.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u4ece\u5934\u7b97\u8d77\u8ba1\u7b97\u548c\u539f\u5b50\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u78c1\u6027\u6750\u6599\u7684\u7535\u963b\u7387\uff0c\u7279\u522b\u662f\u8003\u8651\u4e86\u81ea\u65cb\u65e0\u5e8f\u7684\u6e29\u5ea6\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u78c1\u6027\u6750\u6599\u7684\u7535\u963b\u7387\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u81ea\u65cb\u65e0\u5e8f\u8d21\u732e\u65b9\u9762\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4ece\u5934\u7b97\u8d77\u7684\u8f93\u8fd0\u8ba1\u7b97\u548c\u539f\u5b50\u6a21\u578b\uff0c\u7ed3\u5408\u6d77\u68ee\u5821\u6a21\u578b\u7684\u534a\u7ecf\u5178\u5c40\u57df\u91cf\u5b50\u5316\u65b9\u6cd5\uff0c\u6765\u63cf\u8ff0\u6e29\u5ea6\u4f9d\u8d56\u7684\u81ea\u65cb\u65e0\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u81ea\u65cb\u65e0\u5e8f\u5bf9\u7535\u963b\u7387\u7684\u8d21\u732e\uff0c\u5e76\u4e14\u80fd\u591f\u7814\u7a76\u5c45\u91cc\u6e29\u5ea6\u9644\u8fd1\u53ca\u4ee5\u4e0a\u7684\u6e29\u5ea6\u6548\u5e94\uff0c\u514b\u670d\u4e86\u5148\u524d\u57fa\u4e8e\u5e73\u5747\u573a\u7406\u8bba\u7684\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7ed3\u5408\u4e86\u4ece\u5934\u7b97\u8d77\u7684\u8f93\u8fd0\u8ba1\u7b97\u548c\u539f\u5b50\u6a21\u578b\u6765\u63cf\u8ff0\u78c1\u6027\u6750\u6599\u4e2d\u4e0e\u81ea\u65cb\u76f8\u5173\u7684\u7535\u963b\u7387\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.10995", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10995", "abs": "https://arxiv.org/abs/2508.10995", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.", "AI": {"tldr": "MDMs are good for text generation, and a new method makes them even better.", "motivation": "To improve the generation quality of masked diffusion language models (MDMs) by leveraging inference-time scaling methods similar to those used in other diffusion models.", "method": "A verifier-based inference-time scaling method is proposed to guide the denoising process of MDMs.", "result": "Experiments show that MDMs perform well on text-style transfer tasks and that the proposed verifier improves generation quality.", "conclusion": "MDMs are a better alternative to autoregressive language models for text-style transfer tasks, and a soft-value-based verifier can significantly improve generation quality."}}
{"id": "2508.10993", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10993", "abs": "https://arxiv.org/abs/2508.10993", "authors": ["Basile Lewandowski", "Robert Birke", "Lydia Y. Chen"], "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aM&C\u7684T2I\u6a21\u578b\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5339\u914d\u56fe\u548c\u56fe\u5d4c\u5165\u7279\u5f81\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u9884\u6d4b\u54ea\u4e2a\u9884\u8bad\u7ec3T2I\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\u6548\u679c\u6700\u4f73\uff0c\u65e0\u9700\u9010\u4e00\u5c1d\u8bd5\u6240\u6709\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u6269\u6563\u548cTransformer\u67b6\u6784\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7528\u6237\u5728HuggingFace\u7b49\u5e73\u53f0\u5206\u4eab\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u8fdb\u884c\u5fae\u8c03\u4ee5\u5e94\u7528\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\u65f6\uff0c\u9762\u4e34\u7740\u4e00\u4e2a\u6311\u6218\uff1a\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u73b0\u6709\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u5df2\u6709\u8f83\u597d\u7684\u89e3\u51b3\uff0c\u4f46\u5728T2I\u6a21\u578b\u53ca\u5176\u5728\u76ee\u6807\u57df\u4e0a\u7684\u6027\u80fd\u6307\u793a\u65b9\u9762\uff0c\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3T2I\u6a21\u578b\u9009\u62e9\u7684\u96be\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u9ad8\u6548\u9009\u62e9\u9884\u8bad\u7ec3T2I\u6a21\u578b\u7684\u6846\u67b6\u3002", "method": "\u8be5\u7814\u7a76\u7684\u6838\u5fc3\u662f\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aM&C\u7684\u9009\u62e9\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u5339\u914d\u56fe\u3002\u5339\u914d\u56fe\u7531\u8282\u70b9\uff08\u4ee3\u8868\u53ef\u7528\u7684\u6a21\u578b\u548c\u5df2\u5206\u6790\u7684\u6570\u636e\u96c6\uff09\u548c\u8fb9\uff08\u5206\u522b\u4ee3\u8868\u6a21\u578b-\u6570\u636e\u5bf9\u7684\u5fae\u8c03\u6027\u80fd\u548c\u6570\u636e-\u6570\u636e\u5bf9\u7684\u6570\u636e\u76f8\u4f3c\u6027\uff09\u7ec4\u6210\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6839\u636e\u8f93\u5165\u7684\u6a21\u578b/\u6570\u636e\u7279\u5f81\u4ee5\u53ca\u4ece\u5339\u914d\u56fe\u4e2d\u63d0\u53d6\u7684\u5173\u952e\u7684\u56fe\u5d4c\u5165\u7279\u5f81\uff0c\u6765\u9884\u6d4b\u54ea\u4e2a\u6a21\u578b\u5728\u76ee\u6807\u57df\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\u80fd\u83b7\u5f97\u6700\u4f73\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u572832\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u5bf910\u4e2aT2I\u6a21\u578b\u8fdb\u884c\u4e86\u9009\u62e9\u8bc4\u4f30\uff0c\u5e76\u5c06M&C\u6846\u67b6\u4e0e\u4e09\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\uff0cM&C\u6846\u67b6\u572861.3%\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u9884\u6d4b\u4e86\u7528\u4e8e\u5fae\u8c03\u7684\u6700\u4f73\u6a21\u578b\uff0c\u5728\u5269\u4f59\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9884\u6d4b\u51fa\u6027\u80fd\u76f8\u8fd1\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u9009\u62e9\u6846\u67b6M&C\uff0c\u7528\u4e8e\u5728\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u4ece\u6a21\u578b\u5e73\u53f0\uff08\u5982HuggingFace\uff09\u4e2d\u9ad8\u6548\u9009\u62e9\u5408\u9002\u7684\u9884\u8bad\u7ec3T2I\u6a21\u578b\uff0c\u800c\u65e0\u9700\u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u8be6\u5c3d\u7684\u5fae\u8c03\u3002M&C\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u5339\u914d\u56fe\u6765\u6355\u6349\u6a21\u578b-\u6570\u636e\u548c\u6570\u636e-\u6570\u636e\u5bf9\u4e4b\u95f4\u7684\u5fae\u8c03\u6027\u80fd\u548c\u6570\u636e\u76f8\u4f3c\u6027\uff0c\u5e76\u5229\u7528\u56fe\u5d4c\u5165\u7279\u5f81\u6765\u9884\u6d4b\u5728\u76ee\u6807\u57df\u4e0a\u5fae\u8c03\u540e\u80fd\u8fbe\u5230\u6700\u4f73\u8d28\u91cf\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM&C\u5728\u9009\u62e9T2I\u6a21\u578b\u65f6\uff0c\u670961.3%\u7684\u60c5\u51b5\u4e0b\u80fd\u6210\u529f\u9884\u6d4b\u6700\u4f73\u6a21\u578b\uff0c\u5176\u4f59\u60c5\u51b5\u4e5f\u80fd\u9884\u6d4b\u51fa\u6027\u80fd\u63a5\u8fd1\u7684\u6a21\u578b\u3002"}}
{"id": "2508.11467", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11467", "abs": "https://arxiv.org/abs/2508.11467", "authors": ["Shifang Liu", "Huiyuan Li", "Hongjiao Sheng", "Haoyuan Gui", "Xiaoyu Zhang"], "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method", "comment": null, "summary": "Singular Value Decomposition (SVD) is a fundamental matrix factorization\ntechnique in linear algebra, widely applied in numerous matrix-related\nproblems. However, traditional SVD approaches are hindered by slow panel\nfactorization and frequent CPU-GPU data transfers in heterogeneous systems,\ndespite advancements in GPU computational capabilities. In this paper, we\nintroduce a GPU-centered SVD algorithm, incorporating a novel GPU-based\nbidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and\ndata layout of different steps for SVD computation, performing all panel-level\ncomputations and trailing matrix updates entirely on GPU to eliminate CPU-GPU\ndata transfers. Furthermore, we integrate related computations to optimize BLAS\nutilization, thereby increasing arithmetic intensity and fully leveraging the\ncomputational capabilities of GPUs. Additionally, we introduce a newly\ndeveloped GPU-based BDC algorithm that restructures the workflow to eliminate\nmatrix-level CPU-GPU data transfers and enable asynchronous execution between\nthe CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs\ndemonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x\nand 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684GPU\u4e2d\u5fc3SVD\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7684GPU\u8ba1\u7b97\u548c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfSVD\u65b9\u6cd5\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u7684\u9762\u677f\u5206\u89e3\u548cCPU-GPU\u6570\u636e\u4f20\u8f93\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u5c3d\u7ba1GPU\u8ba1\u7b97\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GPU\u4e2d\u5fc3SVD\u7b97\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8eGPU\u7684\u53cc\u5bf9\u89d2\u5206\u6cbb\uff08BDC\uff09\u65b9\u6cd5\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u548c\u6570\u636e\u5e03\u5c40\uff0c\u5c06\u6240\u6709\u9762\u677f\u7ea7\u8ba1\u7b97\u548c\u5c3e\u90e8\u77e9\u9635\u66f4\u65b0\u5b8c\u5168\u5728GPU\u4e0a\u6267\u884c\uff0c\u5e76\u6574\u5408\u4e86\u76f8\u5173\u8ba1\u7b97\u4ee5\u4f18\u5316BLAS\u5229\u7528\u7387\u548c\u7b97\u672f\u5f3a\u5ea6\u3002", "result": "\u5728AMD MI210\u548cNVIDIA V100 GPU\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0erocSOLVER/cuSOLVER\u548cMAGMA\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe1293.64\u500d/7.47\u500d\u548c14.10\u500d/12.38\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5GPU\u4e3a\u4e2d\u5fc3\u7684SVD\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u57fa\u4e8eGPU\u7684\u53cc\u5bf9\u89d2\u5206\u6cbb\uff08BDC\uff09\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86CPU-GPU\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u4f18\u5316\u4e86BLAS\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002"}}
{"id": "2508.11321", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11321", "abs": "https://arxiv.org/abs/2508.11321", "authors": ["Chenyue Wen", "Danrong Xiong", "Chengyi Yang", "Dapeng Zhu", "Weisheng Zhao"], "title": "Enhanced anomalous Hall conductivity via Ga doping in Mn\\textsubscript{3}Sn and Mn\\textsubscript{3}Ge", "comment": null, "summary": "This study examines the anomalous Hall effect (AHE) in the Heusler series\n\\ce{Mn3Z} (Z=Ga, Ge, Sn), with a particular emphasis on the manipulation of\nnon-collinear antiferromagnetic structures to enhance AHE. By employing\ndensity-functional theory and first-principles calculations, we demonstrate\nthat the anomalous Hall conductivity is markedly responsive to electron\nfilling. By strategically doping Ga into \\ce{Mn3Sn} and \\ce{Mn3Ge} in order to\nmodulate the electron density, a significant increase in anomalous Hall\nconductivity (AHC) is achieved. It is noteworthy that a Ga:Sn ratio of 1:5\nyields peak AHC values exceeding $\\mathrm{700(\\Omega \\cdot cm)^{-1}}$, while\n3:7 Ga-Ge ratios can result in AHC values surpassing $600\\mathrm{(\\Omega \\cdot\ncm)^{-1}}$. A comparison between the virtual crystal approximation and\nsupercell construction methods for doping has revealed consistent trends. The\nresults of this study pave the way for optimizing AHE in non-collinear AFM\nmaterials.", "AI": {"tldr": "\u901a\u8fc7\u63ba\u6742Ga\u8c03\u63a7Mn3Z\uff08Z=Ga, Ge, Sn\uff09\u7684\u7535\u5b50\u7ed3\u6784\uff0c\u6210\u529f\u5927\u5e45\u63d0\u5347\u4e86\u53cd\u5e38\u970d\u5c14\u7535\u5bfc\u7387\uff0c\u4e3a\u4f18\u5316\u8be5\u7c7b\u6750\u6599\u7684AHE\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u63a2\u7d22\u548c\u589e\u5f3a\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u6750\u6599\u4e2d\u7684\u53cd\u5e38\u970d\u5c14\u6548\u5e94\uff08AHE\uff09\u3002", "method": "\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u901a\u8fc7\u63ba\u6742Ga\u6765\u8c03\u8282Mn3Sn\u548cMn3Ge\u7684\u7535\u5b50\u5bc6\u5ea6\uff0c\u4ee5\u63d0\u9ad8\u53cd\u5e38\u970d\u5c14\u7535\u5bfc\u7387\uff08AHC\uff09\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53cd\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u5bf9\u7535\u5b50\u586b\u5145\u975e\u5e38\u654f\u611f\u3002\u5c06Ga\u63ba\u6742\u5230Mn3Sn\u548cMn3Ge\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8AHC\uff0c\u5176\u4e2dGa:Sn=1:5\u7684\u6bd4\u4f8b\u53ef\u4f7fAHC\u8d85\u8fc7700\uff08\u03a9\u00b7cm\uff09\u207b\u00b9\uff0c\u800cGa:Ge=3:7\u7684\u6bd4\u4f8b\u53ef\u4f7fAHC\u8d85\u8fc7600\uff08\u03a9\u00b7cm\uff09\u207b\u00b9\u3002\u865a\u62df\u6676\u4f53\u8fd1\u4f3c\u548c\u8d85\u80de\u6784\u5efa\u65b9\u6cd5\u5728\u6a21\u62df\u63ba\u6742\u6548\u679c\u4e0a\u5f97\u5230\u4e86\u4e00\u81f4\u7684\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63ba\u6742\u63ed\u793a\u4e86\u8d6b\u65af\u52d2\u7cfb\u5217Mn3Z\uff08Z=Ga, Ge, Sn\uff09\u4e2d\u53cd\u5e38\u970d\u5c14\u6548\u5e94\uff08AHE\uff09\u4e0e\u7535\u5b50\u586b\u5145\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6210\u529f\u8c03\u63a7\u4e86\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u7ed3\u6784\u4ee5\u589e\u5f3aAHE\u3002"}}
{"id": "2508.11186", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11186", "abs": "https://arxiv.org/abs/2508.11186", "authors": ["Mohammad Alikhani"], "title": "KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network", "comment": null, "summary": "Human Activity Recognition (HAR) plays a critical role in numerous\napplications, including healthcare monitoring, fitness tracking, and smart\nenvironments. Traditional deep learning (DL) approaches, while effective, often\nrequire extensive parameter tuning and may lack interpretability. In this work,\nwe investigate the use of a single three-axis accelerometer and the\nKolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model\ncomplex nonlinear relationships with improved interpretability and parameter\nefficiency. The MotionSense dataset, containing smartphone-based motion sensor\nsignals across various physical activities, is employed to evaluate the\nproposed approach. Our methodology involves preprocessing and normalization of\naccelerometer and gyroscope data, followed by KAN-based feature learning and\nclassification. Experimental results demonstrate that the KAN achieves\ncompetitive or superior classification performance compared to conventional\ndeep neural networks, while maintaining a significantly reduced parameter\ncount. This highlights the potential of KAN architectures as an efficient and\ninterpretable alternative for real-world HAR systems. The open-source\nimplementation of the proposed framework is available at the Project's GitHub\nRepository.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528KAN\u548c\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u66f4\u5c11\uff0c\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u91cf\u7684\u53c2\u6570\u8c03\u6574\uff0c\u5e76\u4e14\u53ef\u80fd\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528KAN\u6a21\u578b\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53c2\u6570\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u4e09\u8f74\u52a0\u901f\u5ea6\u8ba1\u548cKolmogorov-Arnold Network\uff08KAN\uff09\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3002\u6570\u636e\u9884\u5904\u7406\u548c\u5f52\u4e00\u5316\u540e\uff0c\u4f7f\u7528KAN\u8fdb\u884c\u7279\u5f81\u5b66\u4e60\u548c\u5206\u7c7b\u3002", "result": "KAN\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "KAN\u67b6\u6784\u6709\u6f5c\u529b\u6210\u4e3a\u73b0\u5b9e\u4e16\u754c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\u7684\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11591", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.11591", "abs": "https://arxiv.org/abs/2508.11591", "authors": ["Durga Joshi", "Chandi Witharana", "Robert Fahey", "Thomas Worthley", "Zhe Zhu", "Diego Cerrai"], "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "comment": "35 Pages, 15 figures", "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u8fdb\u884c\u5b9e\u65f6\u8def\u8fb9\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u7ed3\u6784\u8bc4\u4f30\u548c\u5730\u7406\u5b9a\u4f4d\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df1\u5ea6\u4f30\u8ba1\u3001GPS\u5b9a\u4f4d\u548c\u76f8\u673a\u51e0\u4f55\uff0c\u53ef\u4ee5\u51c6\u786e\u5730\u6d4b\u91cf\u7269\u4f53\u7684\u4f4d\u7f6e\u548c\u9ad8\u5ea6\uff0c\u6210\u672c\u4f4e\u5ec9\u4e14\u6613\u4e8e\u5b9e\u65bd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u5e38\u89c1\u7684\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u6570\u636e\uff0c\u5bf9\u8def\u8fb9\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5b9e\u65f6\u3001\u5bf9\u8c61\u7ea7\u522b\u7684\u7ed3\u6784\u8bc4\u4f30\u548c\u5730\u7406\u5b9a\u4f4d\u3002", "method": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u6df1\u5ea6\u8bef\u5dee\u6821\u6b63\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\u3002\u9996\u5148\u4f7f\u7528\u5148\u8fdb\u7684\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u4f30\u8ba1\u6df1\u5ea6\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u6846\u67b6\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u6821\u6b63\u4f4e\u4f30\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8fdc\u5904\u7269\u4f53\u3002\u6df1\u5ea6\u6821\u6b63\u6a21\u578b\u5728\u4e00\u4e2a\u8f6c\u6362\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff08R2 = 0.92\uff0cMAE = 0.31\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8d85\u8fc715\u7c73\u4ee5\u5916\u7684\u504f\u5dee\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u57fa\u4e8eGPS\u7684\u4e09\u89d2\u6d4b\u91cf\u6765\u4f30\u7b97\u7269\u4f53\u4f4d\u7f6e\uff0c\u5e76\u4f7f\u7528\u9488\u5b54\u76f8\u673a\u51e0\u4f55\u6765\u8ba1\u7b97\u7269\u4f53\u9ad8\u5ea6\u3002", "result": "\u8be5\u7814\u7a76\u5728\u4f4e\u901f\u8f66\u8f86\u548c\u8f66\u5185\u6444\u50cf\u5934\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5e73\u5747\u5730\u7406\u5b9a\u4f4d\u8bef\u5dee\u4e3a 2.83 \u7c73\uff0c\u6811\u6728\u9ad8\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u4e3a 2.09 \u7c73\uff0c\u6746\u7684\u9ad8\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u4e3a 0.88 \u7c73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u5e38\u89c1\u7684\u4f46\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u6570\u636e\uff0c\u5bf9\u8def\u8fb9\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5b9e\u65f6\u3001\u5bf9\u8c61\u7ea7\u522b\u7684\u7ed3\u6784\u8bc4\u4f30\u548c\u5730\u7406\u5b9a\u4f4d\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u6df1\u5ea6\u8bef\u5dee\u6821\u6b63\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\uff0c\u4ece\u8f66\u8f86\u5b89\u88c5\u7684\u884c\u8f66\u8bb0\u5f55\u4eea\u7684\u8857\u9053\u7ea7\u89c6\u9891\u6d41\u751f\u6210\u51c6\u786e\u7684\u7a7a\u95f4\u548c\u7ed3\u6784\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6444\u50cf\u5934\u653e\u7f6e\u548c\u8f66\u901f\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e3a\u57ce\u5e02\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u5b9e\u65f6\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11422", "categories": ["eess.SY", "cs.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11422", "abs": "https://arxiv.org/abs/2508.11422", "authors": ["Victoria S. Marks", "Joram vanRheede", "Dean Karantonis", "Rosana Esteller", "David Dinsmoor", "John Fleming", "Barrett Larson", "Lane Desborough", "Peter Single", "Robert Raike", "Pierre-Francois DHaese", "Dario J. Englot", "Scott Lempka", "Richard North", "Lawrence Poree", "Marom Bikson", "Tim J. Denison"], "title": "Principles of Physiological Closed-Loop Controllers in Neuromodulation", "comment": "35 pages, 10 figures", "summary": "As neurostimulation devices increasingly incorporate closed-loop\nfunctionality, the greater design complexity brings additional requirements for\nrisk management and special considerations to optimise benefit. This manuscript\ncreates a common framework upon which all current and planned\nneuromodulation-based physiological closed-loop controllers (PCLCs) can be\nmapped including integration of the Technical Considerations of Medical Devices\nwith Physiologic Closed-Loop Control Technology guidance published in 2023 by\nthe United States Food and Drug Administration (FDA), a classification of\nfeedback (reactive) and feedforward (predictive) biomarkers, and control\nsystems theory. We explain risk management in the context of this framework and\nillustrate its applications for three exemplary technologies. This manuscript\nserves as guidance to the emerging field of PCLCs in neuromodulation,\nmitigating risk through standardized nomenclature and a systematic outline for\nrigorous device development, testing, and implementation.", "AI": {"tldr": "A framework is presented for designing and managing risks in closed-loop neuromodulation devices, incorporating FDA guidance and classifying biomarkers to ensure rigorous development and implementation.", "motivation": "The increasing complexity of closed-loop neurostimulation devices necessitates a structured approach to risk management and benefit optimization. This work aims to provide guidance for the emerging field of PCLCs in neuromodulation.", "method": "The manuscript develops a common framework for mapping current and planned neuromodulation-based PCLCs. This framework integrates FDA's 2023 guidance, classifies biomarkers into feedback (reactive) and feedforward (predictive) types, and applies control systems theory. The authors explain risk management within this framework and illustrate its application with three exemplary technologies.", "result": "The manuscript establishes a common framework for PCLCs, which aids in risk management and streamlines device development, testing, and implementation by offering standardized nomenclature and a systematic outline. Its applications are illustrated through three exemplary technologies.", "conclusion": "This manuscript provides a framework for the design, development, testing, and implementation of neuromodulation-based physiological closed-loop controllers (PCLCs), aiming to mitigate risks through standardized nomenclature and a systematic approach. It also incorporates FDA guidance and classifies biomarkers."}}
{"id": "2508.11191", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.11191", "abs": "https://arxiv.org/abs/2508.11191", "authors": ["Chih-Wei Wang", "Jhih-Sheng Wu"], "title": "Quantum Control of Thermal Emission from Photonic Crystals with Two-Level Atoms", "comment": "24 pages, 7 figures,", "summary": "Thermal light engineering is a field of considerable interest and potential.\nWe study quantum light-matter interactions in a one-dimensional photonic\ncrystal with two-level atoms as the active medium, replacing classical\noscillators in traditional blackbody models. In a thermal bath with pumping,\nthese atoms modulate thermal emission via interactions with photonic modes. The\nmodel with quantum two-level systems enables the processes of spontaneous\nemission, stimulated absorption, and stimulated emission. Equilibrium and\nnonequilibrium regimes depend on competition between pumping and thermal\nrelaxation rates. Strong light-matter interaction and photon decay govern\ndynamics and steady states. In equilibrium, with a high thermal relaxation\nrate, photon numbers are initially determined by spontaneous emission and later\nstabilize due to stimulated absorption, influenced by light-matter interaction\nstrength. In-band-gap photons reach steady states at a time scale of one or two\norders of magnitude longer than outside-band-gap photons. Interestingly, for a\nstrong light-matter interaction, all photons in the equilibrium regimes show\nPlanckian radiation, regardless of their frequencies in or out of the band\ngaps. Band-gap suppression of thermal emission is more pronounced with weaker\nlight-matter interaction or larger photon decay. In the nonequilibrium regime,\nthe dynamics of photon numbers exhibit a multi-time-scale process transitioning\nto steady states due to strong pumping and stimulated processes. Steady-state\nelectron populations of two-level atoms deviate from the Fermi-Dirac\ndistribution, and the steady-state photon numbers exhibit super-Planckian\nemission. These findings enable quantum control of thermal emission spectra,\nwhich is relevant for reducing thermal noise in quantum computing or enhancing\nradiative cooling.", "AI": {"tldr": "\u91cf\u5b50\u4e8c\u80fd\u7ea7\u539f\u5b50\u4e0e\u5149\u5b50\u6676\u4f53\u76f8\u4e92\u4f5c\u7528\u53ef\u8c03\u63a7\u70ed\u53d1\u5c04\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u666e\u6717\u514b\u8f90\u5c04\u6216\u8d85\u666e\u6717\u514b\u8f90\u5c04\uff0c\u5e94\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u548c\u8f90\u5c04\u51b7\u5374\u3002", "motivation": "\u70ed\u5149\u5de5\u7a0b\u662f\u4e00\u4e2a\u5907\u53d7\u5173\u6ce8\u4e14\u6f5c\u529b\u5de8\u5927\u7684\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u91cf\u5b50\u6548\u5e94\u8c03\u63a7\u70ed\u53d1\u5c04\uff0c\u4ee5\u671f\u5728\u91cf\u5b50\u8ba1\u7b97\u7b49\u9886\u57df\u5b9e\u73b0\u4f4e\u70ed\u566a\u58f0\u6216\u589e\u5f3a\u8f90\u5c04\u51b7\u5374\u7b49\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u91cf\u5b50\u4e8c\u80fd\u7ea7\u539f\u5b50\u4ee3\u66ff\u4f20\u7edf\u9ed1\u4f53\u6a21\u578b\u4e2d\u7684\u7ecf\u5178\u632f\u5b50\uff0c\u5e76\u5c06\u5176\u7f6e\u4e8e\u4e00\u7ef4\u5149\u5b50\u6676\u4f53\u4e2d\uff0c\u4ee5\u7814\u7a76\u91cf\u5b50\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u5bf9\u70ed\u53d1\u5c04\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u7406\u8bba\u5efa\u6a21\uff0c\u5206\u6790\u4e86\u5728\u70ed\u5e93\u548c\u6cf5\u6d66\u4f5c\u7528\u4e0b\uff0c\u539f\u5b50\u4e0e\u5149\u5b50\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u8c03\u5236\u70ed\u53d1\u5c04\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u81ea\u53d1\u8f90\u5c04\u3001\u53d7\u6fc0\u5438\u6536\u548c\u53d7\u6fc0\u53d1\u5c04\u8fc7\u7a0b\uff0c\u5e76\u8003\u5bdf\u4e86\u6cf5\u6d66\u4e0e\u70ed\u5f1b\u8c6b\u7387\u7ade\u4e89\u5bfc\u81f4\u7684\u5e73\u8861\u4e0e\u975e\u5e73\u8861\u52a8\u529b\u5b66\u884c\u4e3a\u3002\u901a\u8fc7\u8c03\u6574\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u3001\u5149\u5b50\u8870\u51cf\u7387\u548c\u6cf5\u6d66\u5f3a\u5ea6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u5149\u5b50\u6570\u548c\u539f\u5b50\u5e03\u5c45\u6570\u7684\u7a33\u6001\u884c\u4e3a\uff0c\u7279\u522b\u662f\u666e\u6717\u514b\u8f90\u5c04\u548c\u8d85\u666e\u6717\u514b\u8f90\u5c04\u7684\u51fa\u73b0\u3002", "result": "\u5f3a\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u548c\u5149\u5b50\u8870\u51cf\u51b3\u5b9a\u4e86\u52a8\u529b\u5b66\u548c\u7a33\u6001\u3002\u5728\u5e73\u8861\u72b6\u6001\u4e0b\uff0c\u9ad8\u70ed\u5f1b\u8c6b\u7387\u4e0b\uff0c\u5149\u5b50\u6570\u53d7\u81ea\u53d1\u8f90\u5c04\u548c\u53d7\u6fc0\u5438\u6536\u5f71\u54cd\uff0c\u6700\u7ec8\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u4e14\u5f3a\u76f8\u4e92\u4f5c\u7528\u4e0b\u8868\u73b0\u51fa\u666e\u6717\u514b\u8f90\u5c04\u3002\u5e26\u9699\u5185\u5916\u5149\u5b50\u7a33\u6001\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u540c\u3002\u5e26\u9699\u6291\u5236\u4f5c\u7528\u4e0e\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u548c\u5149\u5b50\u8870\u51cf\u6709\u5173\u3002\u5728\u975e\u5e73\u8861\u72b6\u6001\u4e0b\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u591a\u65f6\u95f4\u5c3a\u5ea6\u52a8\u529b\u5b66\uff0c\u7a33\u6001\u4e0b\u539f\u5b50\u5e03\u5c45\u6570\u504f\u79bb\u8d39\u7c73-\u72c4\u62c9\u514b\u5206\u5e03\uff0c\u5149\u5b50\u6570\u8868\u73b0\u51fa\u8d85\u666e\u6717\u514b\u8f90\u5c04\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u91cf\u5b50\u4e8c\u80fd\u7ea7\u7cfb\u7edf\u66ff\u4ee3\u4f20\u7edf\u9ed1\u4f53\u6a21\u578b\u4e2d\u7684\u7ecf\u5178\u632f\u5b50\uff0c\u7814\u7a76\u4e86\u5728\u91cf\u5b50\u4e8c\u80fd\u7ea7\u7cfb\u7edf\u4f5c\u4e3a\u6d3b\u6027\u4ecb\u8d28\u7684\u4e00\u7ef4\u5149\u5b50\u6676\u4f53\u4e2d\u7684\u91cf\u5b50\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u70ed\u5e93\u548c\u6cf5\u6d66\u7684\u5171\u540c\u4f5c\u7528\u4e0b\uff0c\u8fd9\u4e9b\u4e8c\u80fd\u7ea7\u539f\u5b50\u80fd\u591f\u8c03\u5236\u70ed\u53d1\u5c04\u3002\u8be5\u6a21\u578b\u8003\u8651\u4e86\u81ea\u53d1\u8f90\u5c04\u3001\u53d7\u6fc0\u5438\u6536\u548c\u53d7\u6fc0\u53d1\u5c04\u8fc7\u7a0b\u3002\u7cfb\u7edf\u5728\u5e73\u8861\u4e0e\u975e\u5e73\u8861\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u6362\u53d6\u51b3\u4e8e\u6cf5\u6d66\u548c\u70ed\u5f1b\u8c6b\u7387\u7684\u7ade\u4e89\u3002\u5f3a\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u548c\u5149\u5b50\u8870\u51cf\u51b3\u5b9a\u4e86\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u548c\u7a33\u6001\u3002\u5728\u5e73\u8861\u72b6\u6001\u4e0b\uff0c\u5f53\u70ed\u5f1b\u8c6b\u7387\u8f83\u9ad8\u65f6\uff0c\u5149\u5b50\u6570\u6700\u521d\u7531\u81ea\u53d1\u8f90\u5c04\u51b3\u5b9a\uff0c\u968f\u540e\u5728\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u7684\u5f71\u54cd\u4e0b\uff0c\u901a\u8fc7\u53d7\u6fc0\u5438\u6536\u8d8b\u4e8e\u7a33\u5b9a\u3002\u5e26\u9699\u5185\u5149\u5b50\u7684\u7a33\u6001\u65f6\u95f4\u5c3a\u5ea6\u6bd4\u5e26\u9699\u5916\u5149\u5b50\u957f\u4e00\u5230\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\u7279\u522b\u5730\uff0c\u5f53\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u8f83\u5f3a\u65f6\uff0c\u6240\u6709\u5e73\u8861\u72b6\u6001\u4e0b\u7684\u5149\u5b50\u90fd\u8868\u73b0\u51fa\u666e\u6717\u514b\u8f90\u5c04\uff0c\u4e0e\u5b83\u4eec\u662f\u5426\u5904\u4e8e\u5e26\u9699\u65e0\u5173\u3002\u5e26\u9699\u5bf9\u70ed\u53d1\u5c04\u7684\u6291\u5236\u4f5c\u7528\u5728\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u8f83\u5f31\u6216\u5149\u5b50\u8870\u51cf\u8f83\u5927\u65f6\u66f4\u4e3a\u663e\u8457\u3002\u5728\u975e\u5e73\u8861\u72b6\u6001\u4e0b\uff0c\u7531\u4e8e\u5f3a\u6cf5\u6d66\u548c\u53d7\u6fc0\u8fc7\u7a0b\uff0c\u5149\u5b50\u6570\u7684\u52a8\u529b\u5b66\u6f14\u5316\u5448\u73b0\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u8fc7\u7a0b\u5e76\u8fc7\u6e21\u5230\u7a33\u6001\u3002\u7a33\u6001\u4e0b\u4e8c\u80fd\u7ea7\u539f\u5b50\u7684\u7535\u5b50\u5e03\u5c45\u6570\u504f\u79bb\u8d39\u7c73-\u72c4\u62c9\u514b\u5206\u5e03\uff0c\u7a33\u6001\u5149\u5b50\u6570\u8868\u73b0\u51fa\u8d85\u666e\u6717\u514b\u8f90\u5c04\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u91cf\u5b50\u8c03\u63a7\u70ed\u53d1\u5c04\u5149\u8c31\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u51cf\u5c11\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u70ed\u566a\u58f0\u6216\u589e\u5f3a\u8f90\u5c04\u51b7\u5374\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.11129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11129", "abs": "https://arxiv.org/abs/2508.11129", "authors": ["Ryan M. Bena", "Gilbert Bahati", "Blake Werner", "Ryan K. Cosner", "Lizhi Yang", "Aaron D. Ames"], "title": "Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "Autonomous navigation through unstructured and dynamically-changing\nenvironments is a complex task that continues to present many challenges for\nmodern roboticists. In particular, legged robots typically possess manipulable\nasymmetric geometries which must be considered during safety-critical\ntrajectory planning. This work proposes a predictive safety filter: a nonlinear\nmodel predictive control (MPC) algorithm for online trajectory generation with\ngeometry-aware safety constraints based on control barrier functions (CBFs).\nCritically, our method leverages Poisson safety functions to numerically\nsynthesize CBF constraints directly from perception data. We extend the\ntheoretical framework for Poisson safety functions to incorporate temporal\nchanges in the domain by reformulating the static Dirichlet problem for\nPoisson's equation as a parameterized moving boundary value problem.\nFurthermore, we employ Minkowski set operations to lift the domain into a\nconfiguration space that accounts for robot geometry. Finally, we implement our\nreal-time predictive safety filter on humanoid and quadruped robots in various\nsafety-critical scenarios. The results highlight the versatility of Poisson\nsafety functions, as well as the benefit of CBF constrained model predictive\nsafety-critical controllers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6709\u817f\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u7684\u9884\u6d4b\u5b89\u5168\u6ee4\u6ce2\u5668\uff0c\u8be5\u6ee4\u6ce2\u5668\u5229\u7528\u6cca\u677e\u5b89\u5168\u51fd\u6570\u548c\u63a7\u5236\u969c\u788d\u51fd\u6570\u6765\u5904\u7406\u52a8\u6001\u73af\u5883\u548c\u673a\u5668\u4eba\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u5bf9\u673a\u5668\u4eba\u6765\u8bf4\u662f\u4e00\u4e2a\u590d\u6742\u7684\u6311\u6218\u3002\u7279\u522b\u662f\uff0c\u6709\u817f\u673a\u5668\u4eba\u901a\u5e38\u5177\u6709\u53ef\u64cd\u4f5c\u7684\u4e0d\u5bf9\u79f0\u51e0\u4f55\u5f62\u72b6\uff0c\u5728\u5b89\u5168\u5173\u952e\u7684\u8f68\u8ff9\u89c4\u5212\u4e2d\u5fc5\u987b\u52a0\u4ee5\u8003\u8651\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5b89\u5168\u6ee4\u6ce2\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u57fa\u4e8e\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u5b9e\u73b0\u4e86\u8003\u8651\u51e0\u4f55\u5f62\u72b6\u7684\u5b89\u5168\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6cca\u677e\u5b89\u5168\u51fd\u6570\u4ece\u611f\u77e5\u6570\u636e\u4e2d\u76f4\u63a5\u751f\u6210CBF\u7ea6\u675f\uff0c\u5e76\u5c06\u6cca\u677e\u65b9\u7a0b\u7684\u9759\u6001\u72c4\u5229\u514b\u96f7\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u53c2\u6570\u5316\u7684\u79fb\u52a8\u8fb9\u754c\u503c\u95ee\u9898\uff0c\u4ee5\u6574\u5408\u57df\u4e2d\u7684\u65f6\u95f4\u53d8\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u95f5\u53ef\u592b\u65af\u57fa\u96c6\u8fd0\u7b97\u5c06\u57df\u63d0\u5347\u5230\u8003\u8651\u673a\u5668\u4eba\u51e0\u4f55\u5f62\u72b6\u7684\u6784\u578b\u7a7a\u95f4\u3002", "result": "\u5728\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9884\u6d4b\u5b89\u5168\u6ee4\u6ce2\u5668\uff0c\u5e76\u5728\u5404\u79cd\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u6cca\u677e\u5b89\u5168\u51fd\u6570\u7684\u901a\u7528\u6027\u548cCBF\u7ea6\u675f\u7684\u9884\u6d4b\u5b89\u5168\u63a7\u5236\u5668\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5b9e\u65f6\u9884\u6d4b\u5b89\u5168\u6ee4\u6ce2\u5668\u7684\u5b9e\u73b0\uff0c\u5e76\u5728\u5404\u79cd\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u6cca\u677e\u5b89\u5168\u51fd\u6570\u7684\u901a\u7528\u6027\u4ee5\u53ca\u57fa\u4e8eCBF\u7ea6\u675f\u7684\u9884\u6d4b\u5b89\u5168\u63a7\u5236\u5668\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "CRAFT-GUI\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u6539\u8fdb\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728GUI\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86GUI\u4efb\u52a1\u95f4\u96be\u5ea6\u5dee\u5f02\uff0c\u4e14\u5956\u52b1\u4fe1\u53f7\u7c97\u7cd9\uff0c\u5bfc\u81f4\u4ee3\u7406\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faCRAFT-GUI\u6846\u67b6\uff0c\u57fa\u4e8eGroup Relative Policy Optimization (GRPO)\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u5408\u89c4\u5219\u548c\u6a21\u578b\u5224\u65ad\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u548c\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u53cd\u9988\u3002", "result": "\u5728Android Control\u548c\u5185\u90e8\u5728\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u53d6\u5f97\u4e865.6%\u548c10.3%\u7684\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347GUI\u4ea4\u4e92\u4efb\u52a1\u7684\u4ee3\u7406\u8868\u73b0\u3002"}}
{"id": "2508.10936", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u56fe\u8fdb\u884c\u534f\u540c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\uff0c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u540c\u65f6\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u4ec5\u89c6\u89c9\u65b9\u6cd5\u5728\u534f\u540c\u611f\u77e5\u4e2d\u5b58\u5728\u901a\u4fe1\u6210\u672c\u9ad8\u6216\u9700\u8981\u989d\u5916\u76d1\u7763\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u534f\u540c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u56fe\u4f5c\u4e3a\u534f\u540c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u57fa\u4e8e\u90bb\u57df\u7684\u8de8\u4ee3\u7406\u878d\u5408\u6765\u53bb\u9664\u91cd\u590d\u5e76\u6291\u5236\u4e0d\u4e00\u81f4\u7684\u9ad8\u65af\u56fe\uff0c\u5e76\u8054\u5408\u7f16\u7801\u6bcf\u4e2a\u56fe\u5143\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u51cf\u5c11\u5bf9\u6df1\u5ea6\u76d1\u7763\u7684\u4f9d\u8d56\u5e76\u5b9e\u73b0\u7b80\u5355\u7684\u521a\u6027\u5bf9\u9f50\u3002", "result": "\u4e0e\u5355\u4ee3\u7406\u611f\u77e5\u548c\u57fa\u7ebf\u534f\u540c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728mIoU\u548cIoU\u65b9\u9762\u5206\u522b\u53d6\u5f97\u4e86+8.42/+3.28\u548c+5.11/+22.41\u7684\u63d0\u5347\u3002\u5728\u901a\u4fe1\u91cf\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5171\u4eab\u548c\u878d\u5408\u4e2d\u95f4\u9ad8\u65af\u56fe\u5143\uff0c\u5b9e\u73b0\u4e86\u534f\u540c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002"}}
{"id": "2508.11481", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11481", "abs": "https://arxiv.org/abs/2508.11481", "authors": ["J. Jechumt\u00e1l", "O. Gueckstock", "K. Jasensk\u00fd", "Z. Ka\u0161par", "K Olejn\u00edk", "M. Gaerner", "G. Reiss", "S. Moser", "P. Kessler", "G. De Luca", "S. Ganguly", "J. Santiso", "D. Scheffler", "J. Z\u00e1zvorka", "P. Kuba\u0161\u010d\u00edk", "H. Reichlova", "E. Schmoranzerova", "P. N\u011bmec", "T. Jungwirth", "P. Ku\u017eel", "T. Kampfrath", "L. N\u00e1dvorn\u00edk"], "title": "Spin-to-charge-current conversion in altermagnetic candidate RuO$_2$ probed by terahertz emission spectroscopy", "comment": "21 pages, 4 figures", "summary": "Using the THz emission spectroscopy, we investigate ultrafast spin-to-charge\ncurrent conversion in epitaxial thin films of the altermagnetic candidate\nRuO$_2$. We perform a quantitative analysis of competing effects that can\ncontribute to the measured anisotropic THz emission. These include the\nanisotropic inverse spin splitter and spin Hall effects in RuO$_2$, the\nanisotropic conductivity of RuO$_2$, and the birefringence of the TiO$_2$\nsubstrate. We observe that the leading contribution to the measured signals\ncomes from the anisotropic inverse spin Hall effect, with an average spin-Hall\nangle of $2.4\\times 10^{-3}$ at room temperature. In comparison, a possible\ncontribution from the altermagnetic inverse spin-splitter effect is found to be\nbelow $2\\times 10^{-4}$. Our work stresses the importance of carefully\ndisentangling spin-dependent phenomena that can be generated by the\nunconventional altermagnetic order, from the effects of the relativistic\nspin-orbit coupling.", "AI": {"tldr": "\u5728 RuO$_2$ \u8584\u819c\u4e2d\uff0c\u5404\u5411\u5f02\u6027\u9006\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u5728 THz \u53d1\u5c04\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u800c\u4e0d\u662f altermagnetic \u9006\u81ea\u65cb\u88c2\u53d8\u6548\u5e94\u3002", "motivation": "\u4e3a\u4e86\u91cf\u5316\u5206\u6790\u53ef\u80fd\u5bfc\u81f4\u6d4b\u91cf\u7684 THz \u53d1\u5c04\u5404\u5411\u5f02\u6027\u7684\u7ade\u4e89\u6548\u5e94\uff0c\u5305\u62ec RuO$_2$ \u7684\u5404\u5411\u5f02\u6027\u9006\u81ea\u65cb\u88c2\u53d8\u548c\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u3001RuO$_2$ \u7684\u5404\u5411\u5f02\u6027\u7535\u5bfc\u7387\u4ee5\u53ca TiO$_2$ \u886c\u5e95\u7684\u53cc\u6298\u5c04\u3002", "method": "\u4f7f\u7528 THz \u53d1\u5c04\u5149\u8c31\u6280\u672f\uff0c\u7814\u7a76\u4e86\u5916\u5ef6 RuO$_2$ \u8584\u819c\u4e2d\u7684\u8d85\u5feb\u81ea\u65cb\u5230\u7535\u8377\u7684\u7535\u6d41\u8f6c\u6362\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5ba4\u6e29\u4e0b\uff0c\u5404\u5411\u5f02\u6027\u9006\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u662f\u6d4b\u91cf\u7684\u4e3b\u8981\u8d21\u732e\u8005\uff0c\u5e73\u5747\u81ea\u65cb\u970d\u5c14\u89d2\u4e3a $2.4\times 10^{-3}$\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7531 altermagnetic \u9006\u81ea\u65cb\u88c2\u53d8\u6548\u5e94\u5f15\u8d77\u7684\u53ef\u80fd\u8d21\u732e\u4f4e\u4e8e $2\times 10^{-4}$\u3002", "conclusion": "\u901a\u8fc7 THz \u53d1\u5c04\u5149\u8c31\u7814\u7a76\u4e86\u5916\u5ef6 RuO$_2$ \u8584\u819c\u4e2d\u7684\u8d85\u5feb\u81ea\u65cb\u5230\u7535\u8377\u7684\u7535\u6d41\u8f6c\u6362\u3002 \u7ed3\u679c\u8868\u660e\uff0c\u5404\u5411\u5f02\u6027\u9006\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u662f\u6d4b\u5f97\u7684\u4fe1\u53f7\u7684\u4e3b\u8981\u8d21\u732e\u8005\uff0c\u5728\u5ba4\u6e29\u4e0b\u5e73\u5747\u81ea\u65cb\u970d\u5c14\u89d2\u4e3a $2.4\times 10^{-3}$\uff0c\u800c\u5916\u5ef6\u6027\u53cd\u81ea\u65cb\u88c2\u53d8\u6548\u5e94\u7684\u8d21\u732e\u4f4e\u4e8e $2\times 10^{-4}$\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4ed4\u7ec6\u533a\u5206\u7531\u975e\u5e38\u89c4\u7684 altermagnetic \u987a\u5e8f\u5f15\u8d77\u7684\u81ea\u65cb\u76f8\u5173\u73b0\u8c61\u4e0e\u76f8\u5bf9\u8bba\u81ea\u65cb\u8f68\u9053\u8026\u5408\u6548\u5e94\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.11009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11009", "abs": "https://arxiv.org/abs/2508.11009", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "\u73b0\u6709\u7684AI\u5b89\u5168\u6846\u67b6\u672a\u80fd\u5145\u5206\u6ee1\u8db3\u513f\u7ae5\u7684\u9700\u6c42\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86SproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u513f\u7ae5\u4f7f\u7528\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u5e76\u53d1\u73b0\u4e86\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7684AI\u5b89\u5168\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u6210\u4eba\u7528\u6237\uff0c\u5ffd\u89c6\u4e86\u672a\u6210\u5e74\u4eba\u7684\u72ec\u7279\u53d1\u80b2\u8106\u5f31\u6027\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u8bc4\u4f30\u9488\u5bf9\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u7684LLM\u7684\u5b89\u5168\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b1,283\u4e2a\u7b26\u5408\u53d1\u5c55\u89c4\u5f8b\u7684\u5bf9\u6297\u6027\u63d0\u793a\u7684SproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u63a2\u6d4b\u8bf8\u5982\u60c5\u611f\u4f9d\u8d56\u3001\u9690\u79c1\u4fb5\u72af\u548c\u6a21\u4eff\u5371\u9669\u884c\u4e3a\u7b49\u98ce\u9669\u3002\u5bf947\u4e2a\u4e0d\u540c\u7684LLM\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684LLM\u5728\u5b89\u5168\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6f0f\u6d1e\uff0c\u5e76\u4e14\u53d1\u73b0\u4ea4\u4e92\u6027\u4e0e\u5e74\u9f84\u9002\u5b9c\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709LLM\u5728\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u4f7f\u7528\u4e2d\u5b58\u5728\u7684\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u513f\u7ae5\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2508.11016", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11016", "abs": "https://arxiv.org/abs/2508.11016", "authors": ["Qingbin Li", "Rongkun Xue", "Jie Wang", "Ming Zhou", "Zhi Li", "Xiaofeng Ji", "Yongqi Wang", "Miao Liu", "Zheming Yang", "Minghui Qiu", "Jing Yang"], "title": "CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention", "comment": null, "summary": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have\ndriven the emergence of more sophisticated cognitive behaviors in large\nlanguage models (LLMs), thereby enhancing their reasoning capabilities.\nHowever, in prior RLVR pipelines, the repeated use of static initial-state\nsampling drawn exactly from the dataset distribution during each sampling phase\nproduced overly deterministic, low diversity model behavior, which manifested\nas rapid entropy collapse and hindered sustained performance gains during\nprolonged training. To address this issue, we introduce CURE\n(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a\ntwo-stage framework that balances exploration and exploitation. Specifically,\nin the first stage, to deliberately steer the model toward novel yet coherent\ncontexts, we re-generate at high-entropy critical tokens and jointly optimize\nthe original and the branched trajectories. The further comparison with vanilla\nDAPO shows that the regeneration process achieves a better performance on math\nreasoning tasks while sustaining a high-level entropy degree for exploration.\nIn the second stage, we continue training with static initial-state sampling by\nDAPO, intentionally placing the model in a familiar state to gradually\nstrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,\ncompared to other RLVR methods, CURE achieves a 5% performance gain across six\nmath benchmarks, establishing state-of-the-art performance in both entropy and\naccuracy. A series of experiments further validate the effectiveness of our\napproach. Code is available at https://github.com/CURE-Project/CURE.", "AI": {"tldr": "CURE \u662f\u4e00\u4e2a\u89e3\u51b3 RLVR \u4e2d\u71b5\u574d\u584c\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u9ad8\u71b5\u5173\u952e\u6807\u8bb0\u91cd\u65b0\u751f\u6210\u548c\u7a33\u5b9a\u8bad\u7ec3\uff09\u63d0\u9ad8\u4e86 LLM \u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709 RLVR \u7ba1\u9053\u4e2d\uff0c\u91cd\u590d\u4f7f\u7528\u4ece\u6570\u636e\u96c6\u4e2d\u7cbe\u786e\u62bd\u53d6\u7684\u9759\u6001\u521d\u59cb\u72b6\u6001\u6837\u672c\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u8fc7\u4e8e\u786e\u5b9a\u3001\u591a\u6837\u6027\u4f4e\u3001\u71b5\u5feb\u901f\u574d\u584c\u4ee5\u53ca\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u963b\u788d\u6301\u7eed\u6027\u80fd\u63d0\u5347\u7684\u95ee\u9898\u3002", "method": "CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention) \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u5728\u9ad8\u71b5\u7684\u5173\u952e\u6807\u8bb0\u4e0a\u8fdb\u884c\u91cd\u65b0\u751f\u6210\uff0c\u5e76\u8054\u5408\u4f18\u5316\u539f\u59cb\u548c\u5206\u652f\u8f68\u8ff9\uff0c\u4ee5\u5f15\u5bfc\u6a21\u578b\u63a2\u7d22\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u901a\u8fc7 DAPO \u4f7f\u7528\u9759\u6001\u521d\u59cb\u72b6\u6001\u91c7\u6837\u7ee7\u7eed\u8bad\u7ec3\uff0c\u4ee5\u52a0\u5f3a\u5229\u7528\u3002", "result": "CURE \u5728 Qwen-2.5-Math-7B \u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86 5% \u7684\u6027\u80fd\u589e\u76ca\uff0c\u5728\u71b5\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CURE \u901a\u8fc7\u5728\u7b2c\u4e00\u9636\u6bb5\u5f15\u5bfc\u6a21\u578b\u63a2\u7d22\u65b0\u9896\u4f46\u8fde\u8d2f\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7 DAPO \u7a33\u5b9a\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u71b5\u6c34\u5e73\u4ee5\u4fc3\u8fdb\u63a2\u7d22\uff0c\u5e76\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 5% \u7684\u6027\u80fd\u589e\u76ca\uff0c\u5728\u71b5\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.11397", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11397", "abs": "https://arxiv.org/abs/2508.11397", "authors": ["Marko Orozovi\u0107", "Bo\u017eidar N. \u0160o\u0161ki\u0107", "Silvia Picozzi", "\u017deljko \u0160ljivan\u010danin", "Srdjan Stavri\u0107"], "title": "Hole doping as an efficient route to increase the Curie temperature in monolayer CrI$_3$", "comment": "10 pages, 5 figures, supplementary information is included at the end\n  of the manuscript", "summary": "Two-dimensional van der Waals (vdW) magnets offer unprecedented opportunities\nto control magnetism at the atomic scale. Through charge carrier doping -\nrealized by electrostatic gating, intercalation/adsorption, or interfacial\ncharge transfer - one can efficiently tune exchange interactions and\nspin-orbit-induced effects in these systems. In this work, through a\nmulti-scale theoretical framework combining density functional theory, spin\nHamiltonian modeling, and Wannier-function analysis, we choose monolayer\nCrI$_3$ to unravel how carrier doping affects the isotropic as well as\nanisotropic exchange interactions in this prototypical vdW ferromagnet. The\nremarkable efficiency of hole doping in enhancing ferromagnetic exchange and\nmagnetic anisotropy found in our study was explained through orbital-resolved\nanalysis. Crucially, we demonstrated that unlike the undoped system - where\nisotropic exchange interactions govern magnetic long-range order - the\nhole-doped CrI$_3$ exhibits anisotropic terms comparable in magnitude to\nisotropic ones. Finally, we show that a high concentration of holes in a\nCrI$_3$ monolayer can increase its Curie temperature above 200 K. This work\nadvances our understanding of doping-controlled magnetism in semiconducting 2D\nmaterials, demonstrating how anisotropy engineering can stabilize\nhigh-temperature magnetic order.", "AI": {"tldr": "\u901a\u8fc7\u7406\u8bba\u6a21\u62df\uff0c\u6211\u4eec\u53d1\u73b0\u7ed9\u4e8c\u7ef4\u78c1\u4f53 CrI$_3$ \u63ba\u6742\u7a7a\u7a74\u53ef\u4ee5\u589e\u5f3a\u5176\u78c1\u6027\uff0c\u63d0\u9ad8\u78c1\u5316\u6e29\u5ea6\u3002", "motivation": "\u63a2\u7d22\u8f7d\u6d41\u5b50\u63ba\u6742\u5982\u4f55\u5f71\u54cd\u4e8c\u7ef4\u8303\u5fb7\u534e\u78c1\u4f53\uff08\u4ee5 CrI$_3$ \u4e3a\u4f8b\uff09\u7684\u78c1\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u3001\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u5efa\u6a21\u548c\u7ef4\u683c\u7eb3\u51fd\u6570\u5206\u6790\u7684\u591a\u5c3a\u5ea6\u7406\u8bba\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7a7a\u7a74\u63ba\u6742\u80fd\u6709\u6548\u589e\u5f3a CrI$_3$ \u7684\u94c1\u78c1\u4ea4\u6362\u548c\u78c1\u5404\u5411\u5f02\u6027\uff0c\u5e76\u4e14\u9ad8\u6d53\u5ea6\u7a7a\u7a74\u63ba\u6742\u53ef\u4ee5\u5c06 CrI$_3$ \u5355\u5c42\u7684\u5c45\u91cc\u6e29\u5ea6\u63d0\u9ad8\u5230 200 K \u4ee5\u4e0a\u3002", "conclusion": "\u8f7d\u6d41\u5b50\u63ba\u6742\u80fd\u63d0\u9ad8 CrI$_3$ \u5355\u5c42\u7684\u5c45\u91cc\u6e29\u5ea6\uff0c\u8bc1\u660e\u4e86\u5404\u5411\u5f02\u6027\u5de5\u7a0b\u53ef\u4ee5\u7a33\u5b9a\u9ad8\u6e29\u78c1\u6709\u5e8f\u3002"}}
{"id": "2508.11234", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11234", "abs": "https://arxiv.org/abs/2508.11234", "authors": ["Shengheng Liu", "Ningning Fu"], "title": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing", "comment": "Already published in ACM TOSN. 27 pages, 7 figures", "summary": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e09\u5143ADC\uff08T-ADC\uff09\u548c\u8054\u5408\u5bfc\u9891\u4e0e\u6570\u636e\uff08JPD\uff09\u65b9\u6848\uff0c\u4ee5\u964d\u4f4eAIoT\u548c\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u529f\u8017\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u9053\u611f\u77e5\u6311\u6218\u3002", "motivation": "AIoT\u7684\u6fc0\u589e\u4ee5\u53ca\u5bf9\u666e\u904d\u667a\u80fd\u7684\u9700\u6c42\u5bfc\u81f4\u4e86AIoT\u7684\u53d1\u5c55\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387ADC\u548c\u5927\u91cf\u7684\u5c04\u9891\u94fe\u4f1a\u663e\u8457\u589e\u52a0\u529f\u8017\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u4e00\u79cd\u4f4e\u529f\u8017\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5fc5\u8981\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e09\u5143ADC\uff08T-ADC\uff09\u7684\u4f4e\u529f\u8017AIoT\u548c\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\uff0c\u5e76\u89e3\u51b3\u4e86\u4fe1\u9053\u611f\u77e5\u6311\u6218\u3002\u901a\u8fc7\u8f85\u52a9\u65b9\u6848\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u5bfc\u9891\u548c\u6570\u636e\uff08JPD\uff09\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\u3002\u5206\u6790\u4e86\u8be5\u53cc\u9608\u503cADC\u7cfb\u7edf\u7684\u6027\u80fd\u9650\u5236\uff0c\u5305\u62ec\u5176\u7406\u60f3\u786c\u4ef6\u5bf9\u5e94\u7269\uff08PO-ADC\uff09\u548c\u4e00\u4e2a\u63a5\u6536\u5668\u5904\u672a\u77e5\u566a\u58f0\u65b9\u5dee\u7684\u5b9e\u9645\u573a\u666f\u3002", "result": "JPD\u65b9\u6848\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u7f13\u89e3\u7c97\u91cf\u5316\u6548\u5e94\u5bf9\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\u7684\u5f71\u54cd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u5bfc\u9891\u5f00\u9500\u3002\u5bf9\u4e8e\u786e\u5b9a\u6027\u548c\u968f\u673a\u4fe1\u9053\uff0c\u5206\u522b\u63d0\u51fa\u4e86\u6539\u8fdb\u7684EM\u548c\u53d8\u5206\u63a8\u65adEM\u4f30\u8ba1\u5668\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u4f30\u8ba1\u5668\u5728\u5747\u65b9\u8bef\u5dee\u548c\u7b26\u53f7\u9519\u8bef\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "T-ADCs\u7684\u5b9e\u73b0\u53ca\u5176\u76f8\u5173\u7684JPD\u65b9\u6848\u5bf9\u4e8e\u7eff\u8272AIoT\u667a\u80fd\u4f20\u611f\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2508.11533", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11533", "abs": "https://arxiv.org/abs/2508.11533", "authors": ["Yicheng Lin", "Bingxian Wu", "Nan Bai", "Zhiyong Sun", "Yunxiao Ren", "Chuanze Chen", "Zhisheng Duan"], "title": "Integrating Uncertainties for Koopman-Based Stabilization", "comment": null, "summary": "Over the past decades, the Koopman operator has been widely applied in\ndata-driven control, yet its theoretical foundations remain underexplored. This\npaper establishes a unified framework to address the robust stabilization\nproblem in data-driven control via the Koopman operator, fully accounting for\nthree uncertainties: projection error, estimation error, and process\ndisturbance. It comprehensively investigates both direct and indirect\ndata-driven control approaches, facilitating flexible methodology selection for\nanalysis and control. For the direct approach, considering process\ndisturbances, the lifted-state feedback controller, designed via a linear\nmatrix inequality (LMI), robustly stabilizes all lifted bilinear systems\nconsistent with noisy data. For the indirect approach requiring system\nidentification, the feedback controller, designed using a nonlinear matrix\ninequality convertible to an LMI, ensures closed-loop stability under\nworst-case process disturbances. Numerical simulations via cross-validation\nvalidate the effectiveness of both approaches, highlighting their theoretical\nsignificance and practical utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u63a7\u5236\u4e2dKoopman\u7b97\u5b50\u7684\u9c81\u68d2\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u8003\u8651\u4e86\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u548c\u95f4\u63a5\u65b9\u6cd5\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5c3d\u7ba1Koopman\u7b97\u5b50\u5728\u6570\u636e\u9a71\u52a8\u63a7\u5236\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u63a7\u5236\u4e2dKoopman\u7b97\u5b50\u7684\u9c81\u68d2\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u5e76\u5145\u5206\u8003\u8651\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u63a7\u5236\u4e2d\u7684\u9c81\u68d2\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u8003\u8651\u4e86\u6295\u5f71\u8bef\u5dee\u3001\u4f30\u8ba1\u8bef\u5dee\u548c\u8fc7\u7a0b\u6270\u52a8\u3002\u5bf9\u4e8e\u76f4\u63a5\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u8fc7\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08LMI\uff09\u5b9e\u73b0\u7684\u63d0\u5347\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u7a33\u5b9a\u6240\u6709\u4e0e\u566a\u58f0\u6570\u636e\u4e00\u81f4\u7684\u63d0\u5347\u53cc\u7ebf\u6027\u7cfb\u7edf\u3002\u5bf9\u4e8e\u95f4\u63a5\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u901a\u8fc7\u975e\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u8f6c\u6362\u4e3aLMI\u7684\u53cd\u9988\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u95ed\u73af\u7cfb\u7edf\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8fc7\u7a0b\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u548c\u95f4\u63a5\u6570\u636e\u9a71\u52a8\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406Koopman\u7b97\u5b50\u9c81\u68d2\u7a33\u5b9a\u5316\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u4e86\u5176\u7406\u8bba\u610f\u4e49\u548c\u5b9e\u8df5\u4ef7\u503c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u76f4\u63a5\u548c\u95f4\u63a5\u6570\u636e\u9a71\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u8003\u8651\u4e86\u6295\u5f71\u8bef\u5dee\u3001\u4f30\u8ba1\u8bef\u5dee\u548c\u8fc7\u7a0b\u6270\u52a8\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3Koopman\u7b97\u5b50\u7684\u9c81\u68d2\u7a33\u5b9a\u5316\u95ee\u9898\u3002\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11198", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11198", "abs": "https://arxiv.org/abs/2508.11198", "authors": ["Hang Xu", "Tailong Xiao", "Jingzheng Huang", "Jianping Fan", "Guihua Zeng"], "title": "Learning to Restore Heisenberg Limit in Noisy Quantum Sensing via Quantum Digital Twin", "comment": "7 pages, 4 figures", "summary": "Quantum sensors leverage nonclassical resources to achieve sensing precision\nat the Heisenberg limit, surpassing the standard quantum limit attainable\nthrough classical strategies. However, a critical issue is that the\nenvironmental noise induces rapid decoherence, fundamentally limiting the\nrealizability of the Heisenberg limit. In this Letter, we propose a quantum\ndigital twin protocol to overcome this issue. The protocol first establishes\nobservable-constrained state reconstruction to infer random errors in the\ndecoherence process, and then utilizes reinforcement learning to derive\nadaptive compensatory control strategies. Demonstrated across discrete,\ncontinuous variable and multi-qubit circuit systems, our approach bypasses\nquantum state tomography's exponential overhead and discovers optimal control\nschemes to restore the Heisenberg limit. Unlike quantum error correction or\nmitigation schemes requiring precise noise characterization and ancillary\nqubits, our autonomous protocol achieves noise-resilient sensing through\nenvironment-adaptive control sequencing. This work establishes quantum digital\ntwin as a generic methodology for quantum control, proposing a noise-immune\nparadigm for next-generation quantum sensors compatible with NISQ-era\nexperimental constraints.", "AI": {"tldr": "\u91cf\u5b50\u6570\u5b57\u5b6a\u751f\u534f\u8bae\u5229\u7528\u72b6\u6001\u91cd\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u514b\u670d\u73af\u5883\u566a\u58f0\u5bf9\u91cf\u5b50\u4f20\u611f\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u6d77\u68ee\u5821\u6781\u9650\uff0c\u5e76\u517c\u5bb9NISQ\u8bbe\u5907\u3002", "motivation": "\u73af\u5883\u566a\u58f0\u5f15\u8d77\u7684\u5feb\u901f\u9000\u76f8\u5e72\u662f\u5b9e\u73b0\u91cf\u5b50\u4f20\u611f\u5668\u6d77\u68ee\u5821\u6781\u9650\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u91cf\u5b50\u6570\u5b57\u5b6a\u751f\u534f\u8bae\uff0c\u5305\u62ec\u53ef\u89c2\u6d4b\u91cf\u7ea6\u675f\u7684\u72b6\u6001\u91cd\u6784\u4ee5\u63a8\u65ad\u9000\u76f8\u5e72\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u8bef\u5dee\uff0c\u4ee5\u53ca\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63a8\u5bfc\u81ea\u9002\u5e94\u8865\u507f\u63a7\u5236\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728\u79bb\u6563\u3001\u8fde\u7eed\u53d8\u91cf\u548c\u591a\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u7cfb\u7edf\u4e2d\u8fdb\u884c\u6f14\u793a\uff0c\u8be5\u65b9\u6cd5\u7ed5\u8fc7\u4e86\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u7684\u6307\u6570\u7ea7\u5f00\u9500\uff0c\u5e76\u53d1\u73b0\u4e86\u6062\u590d\u6d77\u68ee\u5821\u6781\u9650\u7684\u6700\u4f18\u63a7\u5236\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u566a\u58f0\u9c81\u68d2\u4f20\u611f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u91cf\u5b50\u6570\u5b57\u5b6a\u751f\u534f\u8bae\uff0c\u901a\u8fc7\u53ef\u89c2\u6d4b\u91cf\u7ea6\u675f\u7684\u72b6\u6001\u91cd\u6784\u6765\u63a8\u65ad\u9000\u76f8\u5e72\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u8bef\u5dee\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u81ea\u9002\u5e94\u8865\u507f\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u5728\u5b58\u5728\u73af\u5883\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6d77\u68ee\u5821\u6781\u9650\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u7684\u6307\u6570\u7ea7\u5f00\u9500\uff0c\u5e76\u80fd\u53d1\u73b0\u6700\u4f18\u63a7\u5236\u65b9\u6848\u6765\u6062\u590d\u6d77\u68ee\u5821\u6781\u9650\u3002\u4e0e\u9700\u8981\u7cbe\u786e\u566a\u58f0\u8868\u5f81\u548c\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u7ea0\u9519\u6216\u7f13\u89e3\u65b9\u6848\u4e0d\u540c\uff0c\u8be5\u81ea\u4e3b\u534f\u8bae\u901a\u8fc7\u73af\u5883\u81ea\u9002\u5e94\u63a7\u5236\u5e8f\u5217\u5b9e\u73b0\u6297\u566a\u58f0\u4f20\u611f\u3002\u672c\u5de5\u4f5c\u5c06\u91cf\u5b50\u6570\u5b57\u5b6a\u751f\u786e\u7acb\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u91cf\u5b50\u63a7\u5236\u65b9\u6cd5\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5305\u542b\u566a\u58f0\u7684\u4e0b\u4e00\u4ee3\u91cf\u5b50\u4f20\u611f\u5668\u7684\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u4e0eNISQ\u65f6\u4ee3\u7684\u5b9e\u9a8c\u9650\u5236\u517c\u5bb9\u3002"}}
{"id": "2508.11143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5b9a\u673a\u5236\uff08\u5982\u4e0d\u5bf9\u79f0\u66f4\u65b0\u548cn\u6b65\u8fd4\u56de\uff09\u7a33\u5b9a\u5730\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\uff0c\u4ece\u800c\u6709\u6548\u89e3\u51b3\u957f\u65f6\u57df\u3001\u7a00\u758f\u5956\u52b1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u957f\u65f6\u57df\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08\u5c24\u5176\u662f\u7a00\u758f\u5956\u52b1\u4efb\u52a1\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u52a8\u4f5c\u5206\u5757\uff08action chunking\uff09\u662f\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5206\u5757\u4ecd\u7136\u9762\u4e34\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u7684\u6311\u6218\u3002", "method": "AC3\u6846\u67b6\u91c7\u7528actor-Critic\u67b6\u6784\uff0c\u5b66\u4e60\u751f\u6210\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u4e0d\u5bf9\u79f0\u66f4\u65b0\u89c4\u5219\u4ec5\u4ece\u6210\u529f\u8f68\u8ff9\u4e2d\u5b66\u4e60actor\uff0c\u540c\u65f6\u5229\u7528n\u6b65\u8fd4\u56de\u548c\u81ea\u76d1\u7763\u6a21\u5757\u4e3acritic\u63d0\u4f9b\u7a33\u5b9a\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u5185\u5728\u5956\u52b1\u3002", "result": "AC3\u6846\u67b6\u572825\u4e2aBiGym\u548cRLBench\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6f14\u793a\u548c\u7b80\u5355\u7684\u6a21\u578b\u7ed3\u6784\uff0cAC3\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6210\u529f\u7387\u3002", "conclusion": "AC3\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e0d\u5bf9\u79f0\u66f4\u65b0\u89c4\u5219\u548c\u57fa\u4e8en\u6b65\u8fd4\u56de\u7684\u5956\u52b1\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u5b66\u4e60\uff0c\u5e76\u5728BiGym\u548cRLBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "AIM-Bench\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cLLM\u4ee3\u7406\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u7684\u504f\u89c1\uff0c\u9700\u8981\u91c7\u53d6\u63aa\u65bd\uff08\u5982\u8ba4\u77e5\u53cd\u601d\u3001\u4fe1\u606f\u5171\u4eab\uff09\u6765\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u4f9b\u5e94\u94fe\u7ba1\u7406\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u8fdb\u884c\u5e93\u5b58\u51b3\u7b56\u7684\u80fd\u529b\u53ca\u5176\u6f5c\u5728\u7684\u51b3\u7b56\u504f\u89c1\uff08\u4f8b\u5982\u6846\u67b6\u6548\u5e94\uff09\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u548c\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51faAIM-Bench\u57fa\u51c6\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u5e93\u5b58\u8865\u5145\u5b9e\u9a8c\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "result": "\u4e0d\u540cLLM\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u51b3\u7b56\u504f\u89c1\uff1b\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7b56\u7565\u53ef\u7528\u4e8e\u7f13\u89e3\u62c9\u4e2d\u6548\u5e94\u548c\u725b\u97ad\u6548\u5e94\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u5b58\u5728\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u51b3\u7b56\u504f\u89c1\uff0c\u9700\u8981\u8c28\u614e\u90e8\u7f72\uff0c\u5e76\u8003\u8651\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7b49\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2508.10937", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10937", "abs": "https://arxiv.org/abs/2508.10937", "authors": ["Jiarui Yang", "Hang Guo", "Wen Huang", "Tao Dai", "Shutao Xia"], "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting", "comment": null, "summary": "In recent years, face super-resolution (FSR) methods have achieved remarkable\nprogress, generally maintaining high image fidelity and identity (ID)\nconsistency under standard settings. However, in extreme degradation scenarios\n(e.g., scale $> 8\\times$), critical attributes and ID information are often\nseverely lost in the input image, making it difficult for conventional models\nto reconstruct realistic and ID-consistent faces. Existing methods tend to\ngenerate hallucinated faces under such conditions, producing restored images\nlacking authentic ID constraints. To address this challenge, we propose a novel\nFSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID\nrestoration under large scaling factors while mitigating hallucination effects.\nOur approach involves three key designs: 1) \\textbf{Masking} the facial region\nin the low-resolution (LR) image to eliminate unreliable ID cues; 2)\n\\textbf{Warping} a reference image to align with the LR input, providing style\nguidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT)\nimages for fine-grained ID modeling and personalized adaptation. We first\npretrain a diffusion-based model to explicitly decouple style and ID by forcing\nit to reconstruct masked LR face regions using both style and identity\nembeddings. Subsequently, we freeze most network parameters and perform\nlightweight fine-tuning of the ID embedding using a small set of target ID\nimages. This embedding encodes fine-grained facial attributes and precise ID\ninformation, significantly improving both ID consistency and perceptual\nquality. Extensive quantitative evaluations and visual comparisons demonstrate\nthat the proposed IDFSR substantially outperforms existing approaches under\nextreme degradation, particularly achieving superior performance on ID\nconsistency.", "AI": {"tldr": "IDFSR\u901a\u8fc7\u906e\u853d\u3001\u626d\u66f2\u548cID\u5d4c\u5165\u6765\u89e3\u51b3\u6781\u7aef\u9000\u5316\u4e0b\u7684FSR\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86ID\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u8d85\u5206\u8fa8\u7387\uff08FSR\uff09\u65b9\u6cd5\u5728\u6781\u7aef\u9000\u5316\uff08\u4f8b\u5982\uff0c\u5c3a\u5ea6>8\u00d7\uff09\u4e0b\u96be\u4ee5\u91cd\u5efa\u771f\u5b9e\u4e14ID\u4e00\u81f4\u7684\u9762\u90e8\uff0c\u503e\u5411\u4e8e\u751f\u6210\u7f3a\u4e4f\u771f\u5b9eID\u7ea6\u675f\u7684\u5e7b\u89c9\u9762\u5b54\u3002", "method": "1. \u906e\u853d\uff08Masking\uff09\uff1a\u906e\u853d\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u4e2d\u7684\u9762\u90e8\u533a\u57df\uff0c\u4ee5\u6d88\u9664\u4e0d\u53ef\u9760\u7684ID\u7ebf\u7d22\u3002\n2. \u626d\u66f2\uff08Warping\uff09\uff1a\u5c06\u53c2\u8003\u56fe\u50cf\u626d\u66f2\u4ee5\u5339\u914dLR\u8f93\u5165\uff0c\u63d0\u4f9b\u98ce\u683c\u6307\u5bfc\u3002\n3. ID\u5d4c\u5165\uff08ID embeddings\uff09\uff1a\u5229\u7528\u4ece\u771f\u5b9e\uff08GT\uff09\u56fe\u50cf\u63d0\u53d6\u7684ID\u5d4c\u5165\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684ID\u5efa\u6a21\u548c\u4e2a\u6027\u5316\u9002\u5e94\u3002\n\u8be5\u65b9\u6cd5\u9996\u5148\u9884\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5236\u5176\u4f7f\u7528\u98ce\u683c\u548c\u8eab\u4efd\u5d4c\u5165\u6765\u91cd\u5efa\u88ab\u906e\u853d\u7684LR\u9762\u90e8\u533a\u57df\uff0c\u4ece\u800c\u663e\u5f0f\u5730\u89e3\u8026\u98ce\u683c\u548cID\u3002\n\u968f\u540e\uff0c\u51bb\u7ed3\u5927\u90e8\u5206\u7f51\u7edc\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u5c11\u91cf\u76ee\u6807ID\u56fe\u50cf\u5bf9ID\u5d4c\u5165\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u3002", "result": "IDFSR\u5728\u6781\u7aef\u9000\u5316\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728ID\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684IDFSR\u65b9\u6cd5\u5728\u6781\u7aef\u9000\u5316\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728ID\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.11490", "categories": ["cond-mat.mes-hall", "physics.optics", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11490", "abs": "https://arxiv.org/abs/2508.11490", "authors": ["Subhajyoti Bid", "Henning Schomerus"], "title": "Exceptionally deficient topological square-root insulators", "comment": "6 pages, 3 figures", "summary": "One of the most surprising features of effectively non-Hermitian physical\nsystems is their potential to exhibit a striking nonlinear response and\nfragility to small perturbations. This feature arises from spectral\nsingularities known as exceptional points, whose realization in the spectrum\ntypically requires fine-tuning of parameters. The design of such systems\nreceives significant impetus from the recent conception of \\emph{exceptional\ndeficiency}, in which the entire energy spectrum is composed of exceptional\npoints. Here, we present a concrete and transparent mechanism that enforces\nexceptional deficiency through lattice sum rules in non-Hermitian topological\nsquare-root insulators. We identify the resulting dynamical signatures in\nstatic broadband amplification and non-Abelian adiabatic state amplification,\ndifferentiate between bulk and boundary effects, and outline routes to\nimplementation in physical platforms", "AI": {"tldr": "\u975e\u5384\u7c73\u62d3\u6251\u7edd\u7f18\u4f53\u53ef\u901a\u8fc7\u683c\u70b9\u548c\u89c4\u5219\u5b9e\u73b0\u4f8b\u5916\u7f3a\u9677\uff0c\u5e76\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u52a8\u529b\u5b66\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u975e\u5384\u7c73\u7269\u7406\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u54cd\u5e94\u548c\u5bf9\u5fae\u6270\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u4f8b\u5916\u70b9\u7684\u6982\u5ff5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4f8b\u5916\u7f3a\u9677\u7684\u6982\u5ff5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u683c\u70b9\u548c\u89c4\u5219\u5f3a\u5236\u5b9e\u73b0\u4f8b\u5916\u7f3a\u9677\u7684\u673a\u5236\u3002", "result": "\u8bc6\u522b\u4e86\u9759\u6001\u5bbd\u5e26\u653e\u5927\u548c\u975e\u963f\u8d1d\u5c14\u7edd\u70ed\u6001\u653e\u5927\u52a8\u529b\u5b66\u7279\u5f81\uff0c\u533a\u5206\u4e86\u4f53\u6548\u5e94\u548c\u8fb9\u754c\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u7269\u7406\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7684\u9014\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u683c\u70b9\u548c\u89c4\u5219\u5f3a\u5236\u5b9e\u73b0\u4f8b\u5916\u7f3a\u9677\u7684\u673a\u5236\uff0c\u8be5\u673a\u5236\u9002\u7528\u4e8e\u975e\u5384\u7c73\u62d3\u6251\u7edd\u7f18\u4f53\u3002"}}
{"id": "2508.11017", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11017", "abs": "https://arxiv.org/abs/2508.11017", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.", "AI": {"tldr": "LLMs\u5728\u5904\u7406\u8de8\u8bed\u8a00\u4fe1\u606f\u65f6\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578bTransformer\u6a21\u578b\u53d1\u73b0\uff0c\u7edf\u4e00\u7684\u8de8\u8bed\u8a00\u8868\u5f81\u662f\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u5173\u952e\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u6570\u636e\u8c03\u6574\u548c\u5206\u8bcd\u6765\u6539\u5584\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5373\u5f53\u6a21\u578b\u88ab\u7528\u4e00\u79cd\u8bed\u8a00\u63d0\u95ee\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u5305\u542b\u53e6\u4e00\u79cd\u8bed\u8a00\u7684\u4e8b\u5b9e\u65f6\uff0c\u6a21\u578b\u4f1a\u4ea7\u751f\u5e7b\u89c9\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u53d7\u63a7\u7684\u5b9e\u9a8c\u73af\u5883\u6765\u6df1\u5165\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u548c\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u5408\u6210\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\u5c0f\u7684Transformer\u6a21\u578b\uff0c\u6765\u7814\u7a76LLMs\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u4e2d\u7684\u95ee\u9898\u3002\u7814\u7a76\u8bc6\u522b\u51fa\u4e00\u4e2a\u5173\u952e\u7684\u5b66\u4e60\u9636\u6bb5\uff0c\u5728\u6b64\u9636\u6bb5\u6a21\u578b\u4f1a\u5f62\u6210\u5bf9\u540c\u4e00\u4e8b\u5b9e\u7684\u8de8\u8bed\u8a00\u8868\u5f81\uff0c\u5e76\u8bc1\u660e\u4e86\u8868\u5f81\u7684\u7edf\u4e00\u6027\u5bf9\u4e8e\u5b9e\u73b0\u8de8\u8bed\u8a00\u8fc1\u79fb\u662f\u5fc5\u8981\u7684\u3002\u7814\u7a76\u8fd8\u91cf\u5316\u4e86\u8868\u5f81\u7edf\u4e00\u6027\u4e0e\u6570\u636e\u8bed\u8a00\u4e92\u4fe1\u606f\u53ca\u8bed\u8a00\u63d0\u53d6\u96be\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u901a\u8fc7\u64cd\u7eb5\u6570\u636e\u5206\u5e03\u548c\u5206\u8bcd\u65b9\u6cd5\u6765\u8c03\u8282\u8de8\u8bed\u8a00\u8fc1\u79fb\u6c34\u5e73\u7684\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5ea6\u91cf\u548c\u53ef\u89c6\u5316\u5de5\u5177\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u7ecf\u5386\u4e00\u4e2a\u5b66\u4e60\u9636\u6bb5\uff0c\u5728\u6b64\u9636\u6bb5\u6a21\u578b\u4f1a\u5f62\u6210\u5bf9\u540c\u4e00\u4e8b\u5b9e\u7684\u72ec\u7acb\u6216\u7edf\u4e00\u7684\u8de8\u8bed\u8a00\u8868\u5f81\uff0c\u800c\u7edf\u4e00\u7684\u8868\u5f81\u5bf9\u4e8e\u6709\u6548\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u8868\u5f81\u7684\u7edf\u4e00\u7a0b\u5ea6\u53d7\u5230\u4e8b\u5b9e\u4e0e\u8bad\u7ec3\u6570\u636e\u8bed\u8a00\u4e4b\u95f4\u4e92\u4fe1\u606f\u91cf\u4ee5\u53ca\u8bed\u8a00\u63d0\u53d6\u96be\u5ea6\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8c03\u6574\u6570\u636e\u5206\u5e03\u548c\u5206\u8bcd\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u8c03\u63a7\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6c34\u5e73\u3002", "conclusion": "LLMs\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8868\u73b0\u4e3a\u5728\u7528\u4e00\u79cd\u8bed\u8a00\u63d0\u95ee\u65f6\uff0c\u4f1a\u5e7b\u89c9\u51fa\u5728\u8bad\u7ec3\u65f6\u7528\u53e6\u4e00\u79cd\u8bed\u8a00\u8868\u8fbe\u7684\u4e8b\u5b9e\u3002\u901a\u8fc7\u5728\u5408\u6210\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\u5c0f\u7684Transformer\u6a21\u578b\uff0c\u6211\u4eec\u4e3a\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u7684\u8d77\u56e0\u548c\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u63a7\u73af\u5883\u3002\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u4f1a\u7ecf\u5386\u4e00\u4e2a\u5b66\u4e60\u9636\u6bb5\uff0c\u5728\u6b64\u9636\u6bb5\uff0c\u6a21\u578b\u4f1a\u4e3a\u540c\u4e00\u4e8b\u5b9e\u53d1\u5c55\u51fa\u8de8\u8bed\u8a00\u7684\u72ec\u7acb\u6216\u7edf\u4e00\u7684\u8868\u5f81\uff0c\u800c\u7edf\u4e00\u7684\u8868\u5f81\u5bf9\u4e8e\u8de8\u8bed\u8a00\u8fc1\u79fb\u81f3\u5173\u91cd\u8981\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8868\u5f81\u7684\u7edf\u4e00\u7a0b\u5ea6\u53d6\u51b3\u4e8e\u4e8b\u5b9e\u4e0e\u8bad\u7ec3\u6570\u636e\u8bed\u8a00\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u91cf\u4ee5\u53ca\u8bed\u8a00\u63d0\u53d6\u7684\u96be\u6613\u7a0b\u5ea6\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u8fc7\u8c03\u6574\u6570\u636e\u5206\u5e03\u548c\u5206\u8bcd\uff08tokenization\uff09\u6765\u8c03\u63a7\u8de8\u8bed\u8a00\u8fc1\u79fb\u6c34\u5e73\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u6b63\u5f0f\u8868\u5f81\u5176\u5bf9\u7edf\u4e00\u6027\u5f71\u54cd\u7684\u5ea6\u91cf\u548c\u53ef\u89c6\u5316\u3002\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u53d7\u63a7\u73af\u5883\u5982\u4f55\u63ed\u793a\u9884\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u4e3a\u6539\u5584LLMs\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.11020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11020", "abs": "https://arxiv.org/abs/2508.11020", "authors": ["Aakash Kumar", "Emanuele Natale"], "title": "Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis", "comment": null, "summary": "Quantization is an essential technique for making neural networks more\nefficient, yet our theoretical understanding of it remains limited. Previous\nworks demonstrated that extremely low-precision networks, such as binary\nnetworks, can be constructed by pruning large, randomly-initialized networks,\nand showed that the ratio between the size of the original and the pruned\nnetworks is at most polylogarithmic.\n  The specific pruning method they employed inspired a line of theoretical work\nknown as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights\nfrom the Random Subset Sum Problem. However, these results primarily address\nthe continuous setting and cannot be applied to extend SLTH results to the\nquantized setting.\n  In this work, we build on foundational results by Borgs et al. on the Number\nPartitioning Problem to derive new theoretical results for the Random Subset\nSum Problem in a quantized setting.\n  Using these results, we then extend the SLTH framework to finite-precision\nnetworks. While prior work on SLTH showed that pruning allows approximation of\na certain class of neural networks, we demonstrate that, in the quantized\nsetting, the analogous class of target discrete neural networks can be\nrepresented exactly, and we prove optimal bounds on the necessary\noverparameterization of the initial network as a function of the precision of\nthe target network.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06SLTH\u6846\u67b6\u6269\u5c55\u5230\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u8bc1\u660e\u4e86\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\u79bb\u6563\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u88ab\u7cbe\u786e\u8868\u793a\uff0c\u5e76\u786e\u5b9a\u4e86\u6240\u9700\u7684\u521d\u59cb\u7f51\u7edc\u8fc7\u53c2\u6570\u5316\u7a0b\u5ea6\u7684\u4f18\u5316\u8fb9\u754c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7406\u8bba\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8fde\u7eed\u57df\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u91cf\u5316\u57df\u7684\u95ee\u9898\uff0c\u4e3a\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u57fa\u4e8eBorgs\u7b49\u4eba\u5728\u6570\u8bba\u5206\u95ee\u9898\u4e0a\u7684\u57fa\u7840\u6027\u7814\u7a76\uff0c\u4e3a\u91cf\u5316\u8bbe\u7f6e\u4e0b\u7684\u968f\u673a\u5b50\u96c6\u548c\u95ee\u9898\u63a8\u5bfc\u51fa\u65b0\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u5c06SLTH\u6846\u67b6\u6269\u5c55\u5230\u6709\u9650\u7cbe\u5ea6\u7f51\u7edc\u3002", "result": "\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u8bc1\u660e\u4e86\u79bb\u6563\u795e\u7ecf\u7f51\u7edc\u7684\u7b49\u4ef7\u7c7b\u53ef\u4ee5\u88ab\u7cbe\u786e\u8868\u793a\uff0c\u5e76\u8bc1\u660e\u4e86\u521d\u59cb\u7f51\u7edc\u8fc7\u53c2\u6570\u5316\u7a0b\u5ea6\u7684\u4f18\u5316\u8fb9\u754c\uff0c\u5c06SLTH\u6846\u67b6\u6210\u529f\u6269\u5c55\u5230\u6709\u9650\u7cbe\u5ea6\u7f51\u7edc\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06SLTH\u6846\u67b6\u6269\u5c55\u5230\u6709\u9650\u7cbe\u5ea6\u7f51\u7edc\uff0c\u8bc1\u660e\u4e86\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u79bb\u6563\u795e\u7ecf\u7f51\u7edc\u7684\u7b49\u4ef7\u7c7b\u53ef\u4ee5\u88ab\u7cbe\u786e\u8868\u793a\uff0c\u5e76\u8bc1\u660e\u4e86\u521d\u59cb\u7f51\u7edc\u8fc7\u53c2\u6570\u5316\u7a0b\u5ea6\u7684\u4f18\u5316\u8fb9\u754c\u3002"}}
{"id": "2508.11399", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.11399", "abs": "https://arxiv.org/abs/2508.11399", "authors": ["Chen Wang", "Toni Seiler", "Doyoung Sun", "Safa Shoaee", "Maria Saladina", "Carsten Deibel"], "title": "The contribution of electron and hole conductivity to the transport loss in organic solar cells", "comment": null, "summary": "The effective conductivity determines the reciprocal of the transport\nresistance, the dominant loss of fill factor in organic solar cells. We\nexperimentally determine the dependence of effective conductivity on its\nelectron and hole contributions. Using PM6:Y12 blends with tunable\nmorphological and energetic disorder, we show that the effective conductivity\nfollows a harmonic mean of electron and hole conductivities even across nearly\nthree orders of magnitude in conductivity imbalance. We also validate the\nmethod for directly extracting effective conductivity from current-voltage\nmeasurements, eliminating the need to rely on indirect mobility and charge\ncarrier density-based proxies. Our findings challenge the widespread use of\ngeometric mean approximations and offer a more accurate framework for analysing\nand modelling transport in disordered organic semiconductors.", "AI": {"tldr": "\u6709\u673a\u592a\u9633\u80fd\u7535\u6c60\u7684\u7535\u5bfc\u7387\u53ef\u4ee5\u901a\u8fc7\u7535\u6d41-\u7535\u538b\u6d4b\u91cf\u76f4\u63a5\u63d0\u53d6\uff0c\u5176\u6709\u6548\u7535\u5bfc\u7387\u9075\u5faa\u7535\u5b50\u548c\u7a7a\u7a74\u7535\u5bfc\u7387\u7684\u8c10\u6ce2\u5e73\u5747\u5173\u7cfb\uff0c\u8fd9\u4e0e\u5e38\u7528\u7684\u51e0\u4f55\u5e73\u5747\u6cd5\u4e0d\u540c\u3002", "motivation": "\u6709\u673a\u592a\u9633\u80fd\u7535\u6c60\u7684\u4f20\u8f93\u7535\u963b\u662f\u5f71\u54cd\u529f\u7387\u8f6c\u6362\u6548\u7387\u7684\u5173\u952e\u635f\u5931\u56e0\u7d20\uff0c\u51c6\u786e\u7406\u89e3\u548c\u91cf\u5316\u7535\u5bfc\u7387\u5bf9\u4e8e\u5668\u4ef6\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u91cfPM6:Y12\u5171\u6df7\u7269\u7684\u7535\u6d41-\u7535\u538b\u7279\u6027\uff0c\u5e76\u5206\u6790\u5176\u6709\u6548\u7535\u5bfc\u7387\u4e0e\u7535\u5b50\u548c\u7a7a\u7a74\u7535\u5bfc\u7387\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u6709\u6548\u7535\u5bfc\u7387\u9075\u5faa\u7535\u5b50\u548c\u7a7a\u7a74\u7535\u5bfc\u7387\u7684\u8c10\u6ce2\u5e73\u5747\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728PM6:Y12\u5171\u6df7\u7269\u4e2d\uff0c\u6709\u6548\u7535\u5bfc\u7387\u4e0e\u7535\u5b50\u548c\u7a7a\u7a74\u7535\u5bfc\u7387\u7684\u4e58\u79ef\u6210\u6b63\u6bd4\uff0c\u5373\u4f7f\u5728\u7535\u5bfc\u7387\u5dee\u5f02\u9ad8\u8fbe\u4e09\u4e2a\u6570\u91cf\u7ea7\u65f6\u4e5f\u6210\u7acb\uff0c\u8bc1\u5b9e\u4e86\u8c10\u6ce2\u5e73\u5747\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u7535\u6d41-\u7535\u538b\u6d4b\u91cf\u7684\u6709\u6548\u7535\u5bfc\u7387\u63d0\u53d6\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7535\u5b50\u548c\u7a7a\u7a74\u7535\u5bfc\u7387\u7684\u8c10\u6ce2\u5e73\u5747\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u6709\u673a\u592a\u9633\u80fd\u7535\u6c60\u7684\u7535\u8377\u4f20\u8f93\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6846\u67b6\uff0c\u5e76\u6311\u6218\u4e86\u4f20\u7edf\u51e0\u4f55\u5e73\u5747\u6cd5\u7684\u5e94\u7528\u3002"}}
{"id": "2508.11259", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11259", "abs": "https://arxiv.org/abs/2508.11259", "authors": ["Ryosuke Isono", "Shunsuke Ono"], "title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing.\n  arXiv admin note: text overlap with arXiv:2308.00500", "summary": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility.", "AI": {"tldr": "TSSTF\u662f\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7TGTV\u548cTGEC\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u96be\u4ee5\u6355\u6349\u7a7a\u95f4\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6709\u566a\u58f0\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u566a\u58f0\u9c81\u68d2ST\u878d\u5408\u65b9\u6cd5\u5728\u5904\u7406\u536b\u661f\u56fe\u50cf\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u7cbe\u7ec6\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u5e73\u6ed1\u548c\u4f2a\u5f71\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347ST\u878d\u5408\u65b9\u6cd5\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSSTF\uff08Temporally-Similar Structure-Aware ST fusion\uff09\u7684\u65b0\u578b\u65f6\u7a7a\uff08ST\uff09\u878d\u5408\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u79cd\u5173\u952e\u673a\u5236\uff1a\u65f6\u95f4\u5f15\u5bfc\u5168\u53d8\u5206\uff08TGTV\uff09\u548c\u65f6\u95f4\u5f15\u5bfc\u8fb9\u7f18\u7ea6\u675f\uff08TGEC\uff09\u3002TGTV\u662f\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u51fd\u6570\uff0c\u901a\u8fc7\u53c2\u8003\u90bb\u8fd1\u65e5\u671f\u7684\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u7ec6\u8282\u7684\u540c\u65f6\uff0c\u4fc3\u8fdb\u7a7a\u95f4\u5206\u6bb5\u5e73\u6ed1\u3002TGEC\u5219\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u4e0a\u76f8\u90bb\u7684\u4e24\u4e2a\u56fe\u50cf\u4e4b\u95f4\u8fb9\u7f18\u4f4d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5141\u8bb8\u5149\u8c31\u53d8\u5316\u3002ST\u878d\u5408\u4efb\u52a1\u88ab\u6784\u5efa\u4e3a\u4e00\u4e2a\u5305\u542bTGTV\u548cTGEC\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u9884\u6761\u4ef6\u539f\u5bf9\u5076\u5206\u88c2\u65b9\u6cd5\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTSSTF\u5728\u65e0\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u6709\u566a\u58f0\u6761\u4ef6\u4e0b\u5219\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "TSSTF\u5728\u65e0\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u6709\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u5957\u63a8\u8350\u7684\u53c2\u6570\u503c\uff0c\u4ee5\u63d0\u9ad8\u5728\u4e0d\u540c\u5730\u533a\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.11561", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11561", "abs": "https://arxiv.org/abs/2508.11561", "authors": ["Kaustav Chatterjee", "Sameer Nekkalapu", "Sayak Mukherjee", "Ramij Raja Hossain", "Marcelo Elizondo"], "title": "Identification of Sub/Super-Synchronous Control Interaction Paths Using Dissipative Energy Flow", "comment": "5 pages, 11 figures", "summary": "Sub- and super-synchronous control interactions (SSCIs) are oscillations\narising from adverse interactions between inverter-based resource (IBR)\ncontrols and the power network. SSCIs often involve multiple frequencies and\npropagate through complex, interconnected paths, making it difficult for\nmodel-based approaches to identify both the sources and the paths of\noscillatory energy flow. This paper extends the Dissipative Energy Flow (DEF)\nmethod, originally developed for low-frequency electromechanical oscillations,\nto identify SSCI sources and dynamic interaction paths across multiple\nfrequencies using three-phase voltage and current measurements. The approach\noperates in the dq frame using dynamic phasors, enabling mode-specific DEF\ncomputation from bandpass-filtered signals. An electromagnetic transient (EMT)\ncase study on a meshed network with synchronous generator and type-3 wind farm\nresources under series-compensated conditions demonstrates the method's\ncapability to distinguish frequency-dependent source and sink roles, including\ncases where the same resource acts as a source at one frequency and a sink at\nanother. The results show DEF can provide a physics-based and\nautomation-friendly tool for SSCI diagnosis in IBR-rich grids.", "AI": {"tldr": "\u672c\u6587\u5c06\u8017\u6563\u80fd\u91cf\u6d41\uff08DEF\uff09\u65b9\u6cd5\u6269\u5c55\u5230\u8bc6\u522b\u6b21\u540c\u6b65\u548c\u8d85\u540c\u6b65\u63a7\u5236\u76f8\u4e92\u4f5c\u7528\uff08SSCIs\uff09\u7684\u6e90\u548c\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u9891\u7387\u548c\u590d\u6742\u7f51\u7edc\u7684\u95ee\u9898\u3002\u901a\u8fc7\u4eff\u771f\u6848\u4f8b\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u533a\u5206\u4e0d\u540c\u9891\u7387\u4e0b\u7684\u6e90\u548c\u6c47\u4f5c\u7528\uff0c\u4e3aSSCI\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002", "motivation": "SSCIs\u6d89\u53ca\u591a\u4e2a\u9891\u7387\u5e76\u7a7f\u8fc7\u590d\u6742\u7684\u4e92\u8fde\u8def\u5f84\u4f20\u64ad\uff0c\u4f7f\u5f97\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u8bc6\u522b\u632f\u8361\u80fd\u91cf\u6d41\u7684\u6e90\u548c\u8def\u5f84\u3002", "method": "\u672c\u6587\u5c06\u6700\u521d\u4e3a\u4f4e\u9891\u673a\u7535\u632f\u8361\u5f00\u53d1\u7684\u8017\u6563\u80fd\u91cf\u6d41\uff08DEF\uff09\u65b9\u6cd5\u6269\u5c55\u5230\u4f7f\u7528\u4e09\u76f8\u7535\u538b\u548c\u7535\u6d41\u6d4b\u91cf\u6765\u8bc6\u522b\u8de8\u8d8a\u591a\u4e2a\u9891\u7387\u7684SSCI\u6e90\u548c\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5728dq\u6846\u67b6\u4e2d\u4f7f\u7528\u52a8\u6001\u76f8\u91cf\u8fd0\u884c\uff0c\u80fd\u591f\u4ece\u5e26\u901a\u6ee4\u6ce2\u4fe1\u53f7\u8ba1\u7b97\u6a21\u5f0f\u7279\u5b9a\u7684DEF\u3002", "result": "EMT\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u533a\u5206\u9891\u7387\u76f8\u5173\u7684\u6e90\u548c\u6c47\u4f5c\u7528\uff0c\u5305\u62ec\u540c\u4e00\u8d44\u6e90\u5728\u4e0d\u540c\u9891\u7387\u4e0b\u5206\u522b\u5145\u5f53\u6e90\u548c\u6c47\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5bcc\u542bIBR\u7684\u7535\u7f51\u7684SSCI\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4e14\u81ea\u52a8\u5316\u53cb\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2508.11207", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.11207", "abs": "https://arxiv.org/abs/2508.11207", "authors": ["Ye Xing", "Deifei Xu", "Yuan Li", "Wuhong Zhang", "Lixiang Chen"], "title": "Sunlight-Excited Spontaneous Parametric Down-Conversion for Quantum Imaging", "comment": "7 pages,4 figures,V1", "summary": "Quantum imaging, which harnesses quantum correlations to achieve imaging with\nmultiple advantages over classical optics, has been in development for several\nyears. Here, we explore sunlight, serving as the pump beam, to excite\nspontaneous parametric down-conversion to get the quantum correlation of two\nphotons. Remarkably, our investigations disclose that the photon pairs produced\nfrom sunlight are well correlated in position such that they can be used for\nquantum imaging. Consequently, this demonstrates a latent application scenario\nin which the incoherent beam is harnessed as the pump source for quantum\nimaging. Our research is of substantial significance as it broadens the scope\nof available illumination options, such as using scattering light or\nnon-traditional artificial incoherent light sources, for quantum information, a\nprime potential application being a space-based quantum information mechanism\nwhere this approach allows the system to operate independently of a laser.", "AI": {"tldr": "\u5229\u7528\u9633\u5149\u8fdb\u884c\u91cf\u5b50\u6210\u50cf\uff0c\u592a\u7a7a\u5e94\u7528\u6f5c\u529b\u5de8\u5927\u3002", "motivation": "\u63a2\u7d22\u5c06\u9633\u5149\u8fd9\u4e00\u975e\u4f20\u7edf\u5149\u6e90\u5e94\u7528\u4e8e\u91cf\u5b50\u6210\u50cf\uff0c\u62d3\u5c55\u91cf\u5b50\u4fe1\u606f\u7684\u7167\u660e\u9009\u9879\uff0c\u7279\u522b\u662f\u5728\u6fc0\u5149\u4e0d\u6613\u90e8\u7f72\u7684\u592a\u7a7a\u573a\u666f\u4e2d\u3002", "method": "\u5229\u7528\u9633\u5149\u4f5c\u4e3a\u6cf5\u6d66\u5149\uff0c\u6fc0\u53d1\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u83b7\u53d6\u53cc\u5149\u5b50\u7684\u91cf\u5b50\u5173\u8054\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5149\u5b50\u5bf9\u5728\u4f4d\u7f6e\u4e0a\u7684\u826f\u597d\u5173\u8054\u6027\uff0c\u4ece\u800c\u53ef\u7528\u4e8e\u91cf\u5b50\u6210\u50cf\u3002", "result": "\u5b9e\u73b0\u4e86\u5229\u7528\u9633\u5149\u8fdb\u884c\u91cf\u5b50\u6210\u50cf\uff0c\u8bc1\u660e\u4e86\u5149\u5b50\u5bf9\u5728\u4f4d\u7f6e\u4e0a\u7684\u826f\u597d\u5173\u8054\u6027\uff0c\u4e3a\u975e\u76f8\u5e72\u5149\u6e90\u5728\u91cf\u5b50\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u5b9e\u4e86\u5229\u7528\u9633\u5149\u4f5c\u4e3a\u6cf5\u6d66\u6e90\u8fdb\u884c\u91cf\u5b50\u6210\u50cf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u592a\u7a7a\u7b49\u975e\u4f20\u7edf\u573a\u666f\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11200", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "GASv2\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u6293\u53d6\u7684\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u89e3\u51b3\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3001\u4f7f\u7528\u5355\u4e00\u7acb\u4f53\u6444\u50cf\u5934\u4ee5\u53ca\u7269\u4f53\u65e0\u5173\u7684\u6293\u53d6\u7b49\u6311\u6218\u3002\u8be5\u6846\u67b6\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6293\u53d6\u662f\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\uff08RAS\uff09\u4e2d\u7684\u57fa\u672c\u4efb\u52a1\uff0c\u81ea\u52a8\u5316\u6293\u53d6\u53ef\u4ee5\u51cf\u8f7b\u5916\u79d1\u533b\u751f\u7684\u8d1f\u62c5\uff0c\u5e76\u63d0\u9ad8\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7684\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u6216\u624b\u5de5\u5236\u4f5c\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5904\u7406\u65b0\u7269\u4f53\u3001\u89c6\u89c9\u5e72\u6270\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u65b9\u9762\u7684\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5728RAS\u4e2d\u90e8\u7f72\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u4e5f\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\uff0c\u4f8b\u5982\u89c6\u89c9\u89c2\u6d4b\u4e2d\u7684\u4f4e\u4fe1\u566a\u6bd4\u3001\u5bf9\u9ad8\u5b89\u5168\u6027\u548c\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u8981\u6c42\u4ee5\u53ca\u590d\u6742\u7684\u624b\u672f\u73af\u5883\u3002", "method": "GASv2\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u67b6\u6784\u548c\u624b\u672f\u611f\u77e5\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u63a7\u5236\u7cfb\u7edf\u6765\u786e\u4fdd\u5b89\u5168\u6267\u884c\u3002\u8be5\u7b56\u7565\u5728\u6a21\u62df\u73af\u5883\u4e2d\u901a\u8fc7\u9886\u57df\u968f\u673a\u5316\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4fbf\u4e8e\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u5e76\u4ec5\u4f7f\u7528\u4e00\u5bf9\u5185\u7aa5\u955c\u6444\u50cf\u5934\u5728\u4eff\u751f\u548c\u4f53\u5916\u624b\u672f\u8bbe\u7f6e\u7684\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u90e8\u7f72\u3002", "result": "GASv2\u6846\u67b6\u5728\u4eff\u751f\u548c\u4f53\u5916\u624b\u672f\u8bbe\u7f6e\u4e2d\u5747\u5b9e\u73b0\u4e8665%\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u5e76\u4e14\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u7684\u7269\u4f53\u548c\u6293\u624b\uff0c\u8fd8\u80fd\u9002\u5e94\u5404\u79cd\u5e72\u6270\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3001\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GASv2\u5728\u4eff\u751f\u548c\u4f53\u5916\u624b\u672f\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8665%\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u5e76\u4e14\u80fd\u591f\u6cdb\u5316\u5230\u672a\u77e5\u7684\u7269\u4f53\u548c\u6293\u624b\uff0c\u8fd8\u80fd\u9002\u5e94\u5404\u79cd\u5e72\u6270\uff0c\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3001\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "**Inclusion Arena**\uff1a\u4e00\u4e2a\u521b\u65b0\u7684\u5b9e\u65f6\u6392\u884c\u699c\uff0c\u901a\u8fc7\u5728\u5b9e\u9645AI\u5e94\u7528\u4e2d\u6536\u96c6\u7528\u6237\u53cd\u9988\u6765\u8bc4\u4f30LLMs\u548cMLLMs\u3002\u5b83\u4f7f\u7528\u6539\u8fdb\u7684Bradley-Terry\u6a21\u578b\uff08\u542bPlacement Matches\u548cProximity Sampling\uff09\uff0c\u63d0\u4f9b\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u53ef\u9760\u3001\u66f4\u7a33\u5b9a\u7684\u6392\u540d\uff0c\u5e76\u6709\u6548\u9632\u6b62\u4f5c\u5f0a\uff0c\u65e8\u5728\u52a0\u901f\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684LLMs\u548cMLLMs\u57fa\u51c6\u6d4b\u8bd5\u548c\u6392\u884c\u699c\uff08\u5982MMLU\u3001Chatbot Arena\uff09\u5927\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6216\u901a\u7528\u9886\u57df\u63d0\u793a\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u76f4\u63a5\u4ece\u5b9e\u9645\u5e94\u7528\u4e2d\u6536\u96c6\u53cd\u9988\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Inclusion Arena\uff0c\u4e00\u4e2a\u5b9e\u65f6\u7684\u6a21\u578b\u6392\u540d\u6392\u884c\u699c\uff0c\u5b83\u901a\u8fc7\u5728\u5b9e\u9645AI\u5e94\u7528\u4e2d\u6536\u96c6\u7528\u6237\u53cd\u9988\u6765\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u3002\u5e73\u53f0\u91c7\u7528\u6210\u5bf9\u6bd4\u8f83\u7684\u65b9\u5f0f\uff0c\u5e76\u5c06Bradley-Terry\u6a21\u578b\u8fdb\u884c\u6539\u8fdb\uff0c\u5f15\u5165\u4e86Placement Matches\uff08\u7528\u4e8e\u65b0\u96c6\u6210\u6a21\u578b\u7684\u51b7\u542f\u52a8\uff09\u548cProximity Sampling\uff08\u4f18\u5148\u9009\u62e9\u80fd\u529b\u76f8\u8fd1\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u548c\u63d0\u9ad8\u6392\u540d\u7a33\u5b9a\u6027\uff09\u3002", "result": "Inclusion Arena\u751f\u6210\u7684\u6392\u540d\u53ef\u9760\u4e14\u7a33\u5b9a\uff0c\u4e0e\u901a\u7528\u4f17\u5305\u6570\u636e\u96c6\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u4f20\u9012\u6027\uff0c\u5e76\u80fd\u663e\u8457\u964d\u4f4e\u6076\u610f\u64cd\u7eb5\u7684\u98ce\u9669\u3002", "conclusion": "Inclusion Arena\u901a\u8fc7\u96c6\u6210\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7528\u6237\u53cd\u9988\uff0c\u91c7\u7528\u5e26\u6709Placement Matches\u548cProximity Sampling\u7684Bradley-Terry\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u53ef\u9760\u3001\u7a33\u5b9a\u4e14\u6297\u64cd\u7eb5\u7684\u6a21\u578b\u6392\u540d\uff0c\u65e8\u5728\u52a0\u901fLLMs\u548cMLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u5316\u548c\u90e8\u7f72\u3002"}}
{"id": "2508.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10938", "abs": "https://arxiv.org/abs/2508.10938", "authors": ["Tianyu Song", "Van-Doan Duong", "Thi-Phuong Le", "Ton Viet Ta"], "title": "Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation", "comment": null, "summary": "Accurate identification of wood species plays a critical role in ecological\nmonitoring, biodiversity conservation, and sustainable forest management.\nTraditional classification approaches relying on macroscopic and microscopic\ninspection are labor-intensive and require expert knowledge. In this study, we\nexplore the application of deep learning to automate the classification of ten\nwood species commonly found in Vietnam. A custom image dataset was constructed\nfrom field-collected wood samples, and five state-of-the-art convolutional\nneural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,\nand ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best\nbalance between classification performance and computational efficiency, with\nan average accuracy of 99.29\\% and F1-score of 99.35\\% over 20 independent\nruns. These results demonstrate the potential of lightweight deep learning\nmodels for real-time, high-accuracy species identification in\nresource-constrained environments. Our work contributes to the growing field of\necological informatics by providing scalable, image-based solutions for\nautomated wood classification and forest biodiversity assessment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.11501", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.11501", "abs": "https://arxiv.org/abs/2508.11501", "authors": ["Vibhuti N. Rai", "Christof Holzer", "Carsten Rockstuhl", "Wulf Wulfhekel", "Lukas Gerhard"], "title": "Gating upconversion electroluminescence in a single molecule via adsorption-induced interaction of unpaired spin", "comment": "11 pages, 8 figures", "summary": "Molecules with unpaired spins (radicals) offer promising alternatives to\nclosed-shell molecules as they are less limited regarding the spin statistics\nin their electroluminescence. Here, we combine scanning tunneling microscopy\ninduced luminescence and density functional theory to study single vanadyl\nphthalocyanine molecules, which are stable neutral radicals. Two distinct\nadsorption geometries of the molecule on NaCl/Au(111) lead to a difference in\nthe interaction of the unpaired electron with the substrate, which in turn\nallows us to investigate its effects on the light emission process. Remarkably,\nwe observe that up-conversion electroluminescence is gated by the adsorption\ngeometry of the molecule, an effect we attribute to a reordering of excited\nstates and enhanced excited state transition probabilities. The profound\ninfluence of the unpaired electron via state reordering opens new possibilities\nfor tuning not only molecular electroluminescence but also many other spin\ndependent phenomena.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408 STM \u548c DFT \u7814\u7a76\u4e86\u5355\u5c42\u9492\u915e\u83c1\u5206\u5b50\u7684\u7535\u81f4\u53d1\u5149\uff0c\u53d1\u73b0\u4e0a\u8f6c\u6362\u7535\u81f4\u53d1\u5149\u53d7\u5206\u5b50\u5438\u9644\u51e0\u4f55\u5f62\u72b6\u8c03\u63a7\uff0c\u8fd9\u5f52\u56e0\u4e8e\u6fc0\u53d1\u6001\u91cd\u6392\u548c\u8dc3\u8fc1\u6982\u7387\u589e\u5f3a\uff0c\u4e3a\u8c03\u63a7\u5206\u5b50\u7535\u81f4\u53d1\u5149\u548c\u81ea\u65cb\u76f8\u5173\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u63d0\u51fa\u672a\u914d\u5bf9\u7535\u5b50\uff08\u81ea\u7531\u57fa\uff09\u5206\u5b50\u4f5c\u4e3a\u95ed\u58f3\u5c42\u5206\u5b50\u7684\u66ff\u4ee3\u54c1\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u7535\u81f4\u53d1\u5149\u4e2d\u7684\u81ea\u65cb\u7edf\u8ba1\u9650\u5236\u8f83\u5c11\u3002", "method": "\u7ed3\u5408\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u8bf1\u5bfc\u7684\u53d1\u5149\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff0c\u7814\u7a76\u4e86\u5355\u5c42\u9492\u915e\u83c1\u5206\u5b50\u7684\u6027\u8d28\u3002", "result": "\u89c2\u5bdf\u5230\u4e0a\u8f6c\u6362\u7535\u81f4\u53d1\u5149\u53d7\u5230\u5206\u5b50\u5438\u9644\u51e0\u4f55\u5f62\u72b6\u7684\u8c03\u63a7\uff0c\u5e76\u5f52\u56e0\u4e8e\u6fc0\u53d1\u6001\u7684\u91cd\u6392\u548c\u6fc0\u53d1\u6001\u8dc3\u8fc1\u6982\u7387\u7684\u589e\u5f3a\u3002", "conclusion": "vanadyl phthalocyanine \u5206\u5b50\u5728 NaCl/Au(111) \u8868\u9762\u4e0a\uff0c\u7531\u4e8e\u5176\u4e24\u79cd\u4e0d\u540c\u7684\u5438\u9644\u51e0\u4f55\u5f62\u72b6\uff0c\u5bfc\u81f4\u672a\u914d\u5bf9\u7535\u5b50\u4e0e\u886c\u5e95\u7684\u76f8\u4e92\u4f5c\u7528\u4e0d\u540c\uff0c\u8fd9\u5f71\u54cd\u4e86\u53d1\u5149\u8fc7\u7a0b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0a\u8f6c\u6362\u7535\u81f4\u53d1\u5149\u53d7\u5230\u5206\u5b50\u5438\u9644\u51e0\u4f55\u5f62\u72b6\u7684\u8c03\u63a7\uff0c\u8fd9\u5f52\u56e0\u4e8e\u6fc0\u53d1\u6001\u7684\u91cd\u6392\u548c\u6fc0\u53d1\u6001\u8dc3\u8fc1\u6982\u7387\u7684\u589e\u5f3a\u3002\u8fd9\u79cd\u672a\u914d\u5bf9\u7535\u5b50\u901a\u8fc7\u72b6\u6001\u91cd\u6392\u4ea7\u751f\u7684\u6df1\u523b\u5f71\u54cd\uff0c\u4e3a\u8c03\u63a7\u5206\u5b50\u7535\u81f4\u53d1\u5149\u548c\u5176\u4ed6\u4f9d\u8d56\u4e8e\u81ea\u65cb\u7684\u73b0\u8c61\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11027", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11027", "abs": "https://arxiv.org/abs/2508.11027", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u9762\u5bf9\u5916\u90e8\u5931\u8d25\u65f6\uff0c\u96be\u4ee5\u5236\u5b9a\u548c\u6267\u884c\u5907\u7528\u8ba1\u5212\uff0c\u5373\u4f7f\u5728\u641c\u7d22\u7a7a\u95f4\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "motivation": "\u7814\u7a76\u5f53\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u95ee\u9898\u4e2d\u9047\u5230\u610f\u5916\u5931\u8d25\u65f6\uff0c\u5b83\u4eec\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\u4ee5\u8fbe\u6210\u76ee\u6807\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u89c4\u5212\u57fa\u51c6\u6765\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u7ec4\u5408\u6765\u89e3\u51b3\u89c4\u5212\u95ee\u9898\uff0c\u4ee3\u7406\u4ece\u8d85\u8fc7\u56db\u5343\u79cd\u53ef\u80fd\u6027\u4e2d\u641c\u7d22\u76f8\u5173\u51fd\u6570\uff0c\u5e76\u89c2\u5bdf\u51fd\u6570\u8f93\u51fa\u6216\u9519\u8bef\u6d88\u606f\u5f62\u5f0f\u7684\u73af\u5883\u53cd\u9988\u3002", "result": "\u5373\u4f7f\u5f15\u5165\u5916\u90e8\u5931\u8d25\uff08\u4f8b\u5982\u51fd\u6570\u7a81\u7136\u53d8\u5f97\u4e0d\u53ef\u7528\uff09\uff0c\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u4f1a\u53d7\u5230\u5f71\u54cd\u3002\u6700\u5148\u8fdb\u7684\u6a21\u578b\u867d\u7136\u901a\u5e38\u80fd\u5728\u6b63\u786e\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u6b63\u786e\u7684\u51fd\u6570\uff0c\u4f46\u5728\u9002\u5e94\u73af\u5883\u53cd\u9988\u548c\u8ffd\u6c42\u66ff\u4ee3\u884c\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u9762\u5bf9\u8d85\u51fa\u5176\u63a7\u5236\u8303\u56f4\u7684\u539f\u56e0\u800c\u5931\u8d25\u65f6\uff0c\u5728\u5bfb\u6c42\u66ff\u4ee3\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u76ee\u6807\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5373\u4f7f\u5728\u641c\u7d22\u7a7a\u95f4\u53d7\u5230\u4eba\u4e3a\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2508.11025", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11025", "abs": "https://arxiv.org/abs/2508.11025", "authors": ["Laura L\u00fctzow", "Michael Eichelbeck", "Mykel J. Kochenderfer", "Matthias Althoff"], "title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks", "comment": "Preprint. Under review", "summary": "Conformal prediction is a popular uncertainty quantification method that\naugments a base predictor with prediction sets with statistically valid\ncoverage guarantees. However, current methods are often computationally\nexpensive and data-intensive, as they require constructing an uncertainty model\nbefore calibration. Moreover, existing approaches typically represent the\nprediction sets with intervals, which limits their ability to capture\ndependencies in multi-dimensional outputs. We address these limitations by\nintroducing zono-conformal prediction, a novel approach inspired by interval\npredictor models and reachset-conformant identification that constructs\nprediction zonotopes with assured coverage. By placing zonotopic uncertainty\nsets directly into the model of the base predictor, zono-conformal predictors\ncan be identified via a single, data-efficient linear program. While we can\napply zono-conformal prediction to arbitrary nonlinear base predictors, we\nfocus on feed-forward neural networks in this work. Aside from regression\ntasks, we also construct optimal zono-conformal predictors in classification\nsettings where the output of an uncertain predictor is a set of possible\nclasses. We provide probabilistic coverage guarantees and present methods for\ndetecting outliers in the identification data. In extensive numerical\nexperiments, we show that zono-conformal predictors are less conservative than\ninterval predictor models and standard conformal prediction methods, while\nachieving a similar coverage over the test data.", "AI": {"tldr": "Zono-conformal prediction is a new uncertainty quantification method that uses zonotopes for prediction sets, offering computational and data efficiency with strong coverage guarantees, outperforming existing methods in terms of conservativeness.", "motivation": "Current conformal prediction methods are computationally expensive and data-intensive due to the need for pre-calibration uncertainty modeling. Existing interval-based prediction sets also have limitations in capturing dependencies in multi-dimensional outputs. This work addresses these limitations.", "method": "Zono-conformal prediction, inspired by interval predictor models and reachset-conformant identification, constructs prediction zonotopes with assured coverage by integrating zonotopic uncertainty sets directly into the base predictor model. Identification is achieved through a single, data-efficient linear program. The method is applicable to arbitrary nonlinear base predictors, with a focus on feed-forward neural networks, and can be applied to both regression and classification tasks.", "result": "Zono-conformal predictors are less conservative than interval predictor models and standard conformal prediction methods while achieving similar coverage. The paper also provides methods for detecting outliers in the identification data.", "conclusion": "The proposed zono-conformal prediction method offers a computationally efficient and data-efficient alternative to existing uncertainty quantification techniques, achieving competitive coverage with less conservative prediction sets."}}
{"id": "2508.11549", "categories": ["cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2508.11549", "abs": "https://arxiv.org/abs/2508.11549", "authors": ["Aldo Art\u00edmez Pe\u00f1a", "Nikita Medvedev"], "title": "Ultrafast X-ray interaction with photovoltaic materials: Thermal and nonthermal responses", "comment": "26 pages, 22 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:2502.05799", "summary": "Cadmium telluride (CdTe), lead sulfide (PbS), and indium tin oxide (ITO) are\nimportant in various electronic technologies, for which laser irradiation is\nused to selectively modify and design their unique semiconductor properties. We\nemploy the hybrid multiscale code XTANT-3 to simulate the kinetics of material\nresponse to ultrafast X-ray irradiation. The code accounts for nonequilibrium\nelectronic and atomic dynamics, nonadiabatic coupling, nonthermal melting, and\nbond breaking due to electronic excitation. Among the materials studied, CdTe\nexhibits the highest radiation resistance, similar to CdS. At the respective\nthreshold doses, the melting is primarily thermal, driven by electron-phonon\ncoupling, which is accompanied by the band gap closure. Additionally, all\nmaterials show nonthermal melting at higher doses. Threshold doses increase\nfurther if energy sinks and recrystallization are included. In CdTe and PbS,\nbelow 1.5 eV/atom, the band gap returns to its original value upon\nrecrystallization. As the dose increases, the cooled state becomes more\namorphous, reducing the band gap until it stabilizes. Curiously, in a narrow\nwindow of deposited doses, ITO exhibit transient superionic behavior, with the\nliquid oxygen but solid In and Sn sublattices. At 0.6 eV/atom in CdTe and 0.4\neV/atom in PbS and ITO, material ablation from the surface occurs. The results\nsuggest that femtosecond lasers may be used for tuning the band gap of\nphotovoltaic semiconductors.", "AI": {"tldr": "Ultrafast X-ray irradiation of CdTe, PbS, and ITO using XTANT-3 code reveals varying material responses, including thermal and nonthermal melting, band gap tuning, and transient superionic behavior in ITO. Femtosecond lasers show potential for modifying photovoltaic semiconductor band gaps.", "motivation": "To simulate the kinetics of material response to ultrafast X-ray irradiation for important electronic materials like CdTe, PbS, and ITO, and to understand how laser irradiation selectively modifies their semiconductor properties.", "method": "Utilizing the hybrid multiscale code XTANT-3 to simulate the kinetics of material response to ultrafast X-ray irradiation, accounting for nonequilibrium electronic and atomic dynamics, nonadiabatic coupling, nonthermal melting, and bond breaking due to electronic excitation.", "result": "CdTe demonstrates the highest radiation resistance. Melting is primarily thermal at threshold doses, accompanied by band gap closure, while nonthermal melting occurs at higher doses. Threshold doses increase with energy sinks and recrystallization. Band gaps recover or stabilize at amorphous states with increasing doses. ITO shows transient superionic behavior in a specific dose window. Ablation occurs at 0.6 eV/atom for CdTe and 0.4 eV/atom for PbS and ITO.", "conclusion": "Cadmium telluride (CdTe), lead sulfide (PbS), and indium tin oxide (ITO) exhibit varying responses to ultrafast X-ray irradiation, with CdTe showing higher radiation resistance. Melting can be thermal or nonthermal depending on the dose. Including energy sinks and recrystallization increases threshold doses. Band gaps can recover or decrease with increasing dose, leading to amorphous states. ITO displays transient superionic behavior under specific conditions. Material ablation occurs at characteristic doses for each material. The findings suggest femtosecond lasers can tune the band gaps of photovoltaic semiconductors."}}
{"id": "2508.11292", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11292", "abs": "https://arxiv.org/abs/2508.11292", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Haijun Zhang"], "title": "Beyond Diagonal Reconfigurable Intelligent Surface Enabled Sensing: Cramer-Rao Bound Optimization", "comment": "to appear in IEEE Wireless Communications Letters", "summary": "Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has\nemerged as a more flexible solution to engineer the wireless propagation\nchannels, thanks to its non-diagonal reflecting matrix. Although the gain of\nthe BD-RIS over the conventional RIS in communication has been revealed in many\nworks, its gain in 6G sensing is still unknown. This motivates us to study the\nBD-RIS assisted sensing in this letter. Specifically, we derive the Cramer-Rao\nbound (CRB) for estimating the angle-of-arrival (AOA) from the target to the\nBD-RIS under the constraint that the BD-RIS scattering matrix is unitary. To\nminimize the CRB, we develop an optimization scheme based on an adaptive\nRiemannian steepest ascent algorithm that can satisfy the non-convex unitary\nconstraint. Numerical results demonstrate that the proposed BD-RIS-assisted\ntarget localization method achieves superior sensing performance.", "AI": {"tldr": "\u7814\u7a76\u4e86BD-RIS\u57286G\u611f\u77e5\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u6700\u5c0f\u5316CRB\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eBD-RIS\u5728\u901a\u4fe1\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfRIS\uff0c\u4f46\u5176\u57286G\u611f\u77e5\u65b9\u9762\u7684\u4f18\u52bf\u4ecd\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76BD-RIS\u5728\u611f\u77e5\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "\u63a8\u5bfc\u4e86BD-RIS\u8f85\u52a9\u611f\u77e5\u4e0b\u76ee\u6807\u89d2\u5ea6\uff08AOA\uff09\u4f30\u8ba1\u7684Cramer-Rao\u754c\uff08CRB\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u9ece\u66fc\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u4ee5\u6ee1\u8db3BD-RIS\u6563\u5c04\u77e9\u9635\u9149\u7ea6\u675f\uff0c\u6700\u5c0f\u5316CRB\u3002", "result": "\u63d0\u51fa\u7684BD-RIS\u8f85\u52a9\u76ee\u6807\u5b9a\u4f4d\u65b9\u6cd5\u5728\u611f\u77e5\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eBD-RIS\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5728\u611f\u77e5\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11612", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11612", "abs": "https://arxiv.org/abs/2508.11612", "authors": ["Samuel G. Gessow", "James Tseng", "Eden Zafran", "Brett T. Lopez"], "title": "Two-Impulse Trajectory Design in Two-Body Systems With Riemannian Geometry", "comment": null, "summary": "This work presents a new method for generating impulsive trajectories in\nrestricted two-body systems by leveraging Riemannian geometry. The proposed\nmethod transforms the standard trajectory optimization problem into a purely\ngeometric one that involves computing a set of geodesics for a suitable\nRiemannian metric. This transformation is achieved by defining a metric,\nspecifically the Jacobi metric, that embeds the dynamics directly into the\nmetric, so any geodesic of the metric is also a dynamically feasible\ntrajectory. The method finds the fuel-optimal transfer trajectory by sampling\ncandidate energy ($\\Delta V$) changes for different points on the current and\ndesired orbit, and efficiently computing and evaluating each candidate\ngeodesic, which are equivalent to candidate orbit transfer trajectories via the\nJacobi metric. The method bypasses the known issues of optimization-based\nmethods, e.g., sensitivity to the initial guess, and can be applied to more\ncomplex two-body systems. The approach is demonstrated on the minimum-$\\Delta\nV$ two-impulse phase-free orbit transfer problem, first on a Keplerian system\nand second on a system with a modeled $J_2$ perturbation. The proposed method\nis shown to meet or exceed the state-of-the-art methods in the minimum-$\\Delta\nV$ problem in the Keplerian system. The generality and versatility of the\napproach is demonstrated by seamlessly including the $J_2$ perturbation, a case\nthat many existing methods cannot handle. Numerical simulations and performance\ncomparisons showcase the effectiveness of the approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\uff08\u96c5\u53ef\u6bd4\u5ea6\u91cf\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53d7\u9650\u4e8c\u4f53\u7cfb\u7edf\u4e2d\u751f\u6210\u51b2\u52a8\u8f68\u8ff9\uff0c\u4f18\u5316\u71c3\u6599\u6d88\u8017\uff0c\u5e76\u80fd\u5904\u7406$J_2$\u7b49\u6270\u52a8\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5bf9\u521d\u59cb\u731c\u6d4b\u654f\u611f\u7b49\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u4e8c\u4f53\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5229\u7528\u9ece\u66fc\u51e0\u4f55\uff0c\u5c06\u6807\u51c6\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7eaf\u7cb9\u7684\u51e0\u4f55\u95ee\u9898\uff0c\u6d89\u53ca\u8ba1\u7b97\u4e00\u7ec4\u9002\u7528\u4e8e\u9ece\u66fc\u5ea6\u91cf\u7684\u6d4b\u5730\u7ebf\u3002\u901a\u8fc7\u5b9a\u4e49\u5d4c\u5165\u52a8\u529b\u5b66\u7684\u96c5\u53ef\u6bd4\u5ea6\u91cf\u6765\u5b9e\u73b0\u8fd9\u79cd\u8f6c\u5316\uff0c\u4f7f\u5f97\u5ea6\u91cf\u7684\u4efb\u4f55\u6d4b\u5730\u7ebf\u4e5f\u662f\u52a8\u529b\u5b66\u4e0a\u53ef\u884c\u7684\u8f68\u8ff9\u3002\u901a\u8fc7\u4e3a\u5f53\u524d\u8f68\u9053\u548c\u671f\u671b\u8f68\u9053\u7684\u4e0d\u540c\u70b9\u91c7\u6837\u5019\u9009\u80fd\u91cf\uff08$\\Delta V$\uff09\u53d8\u5316\uff0c\u5e76\u6709\u6548\u5730\u8ba1\u7b97\u548c\u8bc4\u4f30\u6bcf\u4e2a\u5019\u9009\u6d4b\u5730\u7ebf\uff08\u901a\u8fc7\u96c5\u53ef\u6bd4\u5ea6\u91cf\u7b49\u4ef7\u4e8e\u5019\u9009\u8f68\u9053\u8f6c\u79fb\u8f68\u8ff9\uff09\uff0c\u6765\u5bfb\u627e\u71c3\u6599\u6700\u4f18\u8f6c\u79fb\u8f68\u8ff9\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u6027\u80fd\u6bd4\u8f83\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u6700\u5c0f$\\Delta V$\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5728\u5f00\u666e\u52d2\u7cfb\u7edf\u548c\u5177\u6709$J_2$\u6270\u52a8\u7684\u7cfb\u7edf\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f00\u666e\u52d2\u7cfb\u7edf\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u6700\u4f18$\\Delta V$\u95ee\u9898\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u65e0\u7f1d\u5730\u5904\u7406$J_2$\u6270\u52a8\uff0c\u8fd9\u662f\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u60c5\u51b5\u3002"}}
{"id": "2508.11230", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11230", "abs": "https://arxiv.org/abs/2508.11230", "authors": ["Yao Shen", "Fu-Lin Zhang"], "title": "Fault-tolerant mixed boundary punctures on the toric code", "comment": null, "summary": "Defects on the toric code, a well-known exactly solvable Abelian anyon model,\ncan exhibit non-Abelian statistical properties, which can be classified into\npunctures and twists. Benhemou et al.[Phys. Rev. A. 105, 042417 (2022)]\nintroduced a mixed boundary puncture model that integrates the advantages of\nboth punctures and twists. They proposed that non-Abelian properties could be\nrealized in the symmetric subspace {$|++\\rangle$, $|--\\rangle$}. This work\ndemonstrates that the nontrivial antisymmetric subspace{$|+-\\rangle$,\n$|-+\\rangle$} also supports non-Abelian statistics. The mixed boundary puncture\nmodel is shown to be fault-tolerant in both subspaces, offering resistance to\ncollective dephasing noise and collective rotation noise. In addition, we\npropose and validate a quantum information masking scheme within the\nthree-partite mixed boundary puncture model.", "AI": {"tldr": "\u6258\u5229\u7801\u7684\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\u5728\u5bf9\u79f0\u548c\u53cd\u5bf9\u79f0\u5b50\u7a7a\u95f4\u4e2d\u5747\u652f\u6301\u975e\u963f\u8d1d\u5c14\u7edf\u8ba1\uff0c\u5e76\u4e14\u5177\u6709\u5bb9\u9519\u6027\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u548c\u5229\u7528\u4e8c\u7ef4\u91cf\u5b50\u6750\u6599\uff08\u5982\u6258\u5229\u7801\uff09\u4e2d\u975e\u963f\u8d1d\u5c14\u7edf\u8ba1\u7684\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u5176\u5728\u4e0d\u540c\u5b50\u7a7a\u95f4\u4e2d\u7684\u975e\u963f\u8d1d\u5c14\u6027\u8d28\u53ca\u5bb9\u9519\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u91cf\u5b50\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\u4e2d\u975e\u963f\u8d1d\u5c14\u7edf\u8ba1\u7684\u6027\u8d28\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5bf9\u79f0\u548c\u53cd\u5bf9\u79f0\u5b50\u7a7a\u95f4\u4e2d\u7684\u5b58\u5728\u6027\u4ee5\u53ca\u5bf9\u566a\u58f0\u7684\u5bb9\u9519\u6027\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u4fe1\u606f\u63a9\u853d\u65b9\u6848\u3002", "result": "\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\u5728\u5bf9\u79f0\u548c\u53cd\u5bf9\u79f0\u5b50\u7a7a\u95f4\u4e2d\u5747\u8868\u73b0\u51fa\u975e\u963f\u8d1d\u5c14\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u5bf9\u96c6\u4f53\u9000\u76f8\u5e72\u566a\u58f0\u548c\u96c6\u4f53\u65cb\u8f6c\u566a\u58f0\u5177\u6709\u5bb9\u9519\u6027\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u91cf\u5b50\u4fe1\u606f\u63a9\u853d\u65b9\u6848\u5728\u4e09\u65b9\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u4e09\u65b9\u6df7\u5408\u8fb9\u754c\u7a7f\u5b54\u6a21\u578b\u4e2d\u7684\u975e\u963f\u8d1d\u5c14\u7edf\u8ba1\u4e0d\u4ec5\u5b58\u5728\u4e8e\u5bf9\u79f0\u5b50\u7a7a\u95f4\uff0c\u800c\u4e14\u4e5f\u5b58\u5728\u4e8e\u975e\u5bf9\u79f0\u5b50\u7a7a\u95f4\uff0c\u5e76\u4e14\u8be5\u6a21\u578b\u5728\u4e24\u4e2a\u5b50\u7a7a\u95f4\u4e2d\u90fd\u5177\u6709\u5bb9\u9519\u6027\uff0c\u80fd\u591f\u62b5\u6297\u96c6\u4f53\u9000\u76f8\u5e72\u566a\u58f0\u548c\u96c6\u4f53\u65cb\u8f6c\u566a\u58f0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u4fe1\u606f\u63a9\u853d\u65b9\u6848\u3002"}}
{"id": "2508.11204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEA\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u91c7\u6837\u6548\u7387\u5bf9\u4e8e\u5c06\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u4efb\u52a1\u5bf9\u79f0\u6027\u5df2\u6210\u4e3a\u63d0\u9ad8\u6548\u7387\u7684\u6709\u5e0c\u671b\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u4f46\u4e4b\u524d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u4ec5\u9650\u4e8e\u7b49\u8ddd\u5bf9\u79f0\u6027\u2014\u2014\u5728\u6240\u6709\u65f6\u95f4\u6b65\u957f\u4e2d\u5c06\u76f8\u540c\u7684\u7ec4\u53d8\u6362\u5e94\u7528\u4e8e\u6240\u6709\u4efb\u52a1\u5bf9\u8c61\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u7684\u65b0\u578b\u8868\u8ff0\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u975e\u7b49\u8ddd\u5bf9\u79f0\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u2014\u2014\u591a\u7ec4\u7b49\u53d8\u589e\u5f3a\uff08MEA\uff09\u3002\u6211\u4eec\u5c06MEA\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u4fdd\u7559\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u57fa\u4e8e\u4f53\u7d20\u7684\u89c6\u89c9\u8868\u793a\u3002", "result": "\u6211\u4eec\u63a2\u7d22\u4e86\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\uff0c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5e94\u7528\u591a\u4e2a\u72ec\u7acb\u7ec4\u53d8\u6362\u6765\u653e\u5bbd\u8fd9\u4e9b\u7ea6\u675f\u3002", "conclusion": "\u5728\u4e24\u4e2a\u64cd\u4f5c\u57df\u7684\u5e7f\u6cdb\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "Probabilistic landmarks, adapted from classical planning, improve UCT performance in stochastic planning by acting as subgoals, though the optimal strategy depends on the specific problem.", "motivation": "To explore the underutilization of landmarks in stochastic domains despite their success in classical planning.", "method": "Formalized probabilistic landmarks and adapted the UCT algorithm to use them as subgoals for MDP decomposition, balancing greedy landmark achievement with final goal achievement.", "result": "Well-chosen landmarks significantly improved UCT performance in benchmark domains, with the optimal balance between greedy and long-term goal achievement being problem-dependent.", "conclusion": "Landmarks can provide helpful guidance for anytime algorithms solving MDPs, as demonstrated by their significant performance improvement in UCT for online probabilistic planning."}}
{"id": "2508.10940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10940", "abs": "https://arxiv.org/abs/2508.10940", "authors": ["Nirmal Gaud", "Krishna Kumar Jha", "Jhimli Adhikari", "Adhini Nasarin P S", "Joydeep Das", "Samarth S Deshpande", "Nitasha Barara", "Vaduguru Venkata Ramya", "Santu Saha", "Mehmet Tarik Baran", "Sarangi Venkateshwarlu", "Anusha M D", "Surej Mouli", "Preeti Katiyar", "Vipin Kumar Chaudhary"], "title": "NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification", "comment": "6 pages, 2 figures", "summary": "This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional\nNeural Networks (CNNs) that integrates adaptive max pooling with non-linear\nactivation function for image classification tasks. The acronym NIRMAL stands\nfor Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,\nAdaptive, and Localized. By dynamically adjusting pooling parameters based on\ndesired output dimensions and applying a Rectified Linear Unit (ReLU)\nactivation post-pooling, NIRMAL Pooling improves robustness and feature\nexpressiveness. We evaluated its performance against standard Max Pooling on\nthree benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL\nPooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on\nMNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on\nCIFAR-10, demonstrating consistent improvements, particularly on complex\ndatasets. This work highlights the potential of NIRMAL Pooling to enhance CNN\nperformance in diverse image recognition tasks, offering a flexible and\nreliable alternative to traditional pooling methods.", "AI": {"tldr": "A new pooling layer called NIRMAL Pooling improves CNN accuracy in image classification by using adaptive max pooling with ReLU activation, outperforming standard Max Pooling on MNIST and CIFAR-10 datasets.", "motivation": "The paper aims to enhance the performance of Convolutional Neural Networks (CNNs) in image classification tasks by proposing a novel pooling layer, NIRMAL Pooling, which addresses limitations of traditional pooling methods.", "method": "This paper introduces NIRMAL Pooling, a novel pooling layer for CNNs that combines adaptive max pooling with a non-linear activation function (ReLU). NIRMAL stands for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum, Adaptive, and Localized. The layer dynamically adjusts pooling parameters and applies ReLU activation post-pooling to improve robustness and feature expressiveness.", "result": "NIRMAL Pooling achieved higher test accuracies than standard Max Pooling on benchmark datasets: 99.25% vs. 99.12% on MNIST Digits, 91.59% vs. 91.44% on MNIST Fashion, and 70.49% vs. 68.87% on CIFAR-10.", "conclusion": "NIRMAL Pooling, a novel pooling layer, demonstrates consistent improvements over traditional Max Pooling in image classification tasks, especially on complex datasets like CIFAR-10. It offers a flexible and reliable alternative for enhancing CNN performance."}}
{"id": "2508.11601", "categories": ["cond-mat.mes-hall", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11601", "abs": "https://arxiv.org/abs/2508.11601", "authors": ["Edward McCann"], "title": "A non-Hermitian Su-Schrieffer-Heeger model with the energy levels of free parafermions", "comment": "14 pages, 6 figures", "summary": "Using a parent Hermitian tight-binding model on a bipartite lattice with\nchiral symmetry, we theoretically generate non-Hermitian models for free\nfermions with $p$ orbitals per unit cell satisfying a complex generalization of\nchiral symmetry. The $p$ complex energy bands in $k$ space are given by a\ncommon $k$-dependent real factor, determined by the bands of the parent model,\nmultiplied by the $p$th roots of unity. When the parent model is the\nSu-Schrieffer-Heeger (SSH) model, the single-particle energy levels are the\nsame as those of free parafermion solutions to Baxter's non-Hermitian clock\nmodel. This construction relies on fully unidirectional hopping to create Bloch\nHamiltonians with the form of generalized permutation matrices, but we also\ndescribe the effect of partial unidirectional hopping. For fully bidirectional\nhopping, the Bloch Hamiltonians are Hermitian and may be separated into even\nand odd parity blocks with respect to inversion of the orbitals within the unit\ncell. Partially unidirectional hopping breaks the inversion symmetry and mixes\nthe even and odd blocks, and the real energy spectrum evolves into a complex\none as the degree of unidirectionality increases, with details determined by\nthe topology of the parent model and the number of orbitals per unit cell, $p$.\nWe describe this process in detail for $p=3$ and $p=4$ with the SSH model. We\nalso apply our approach to graphene, and show that $AA$-stacked bilayer\ngraphene evolves into a square root Hamiltonian of monolayer graphene with the\nintroduction of unidirectional hopping. We show that higher-order exceptional\npoints occur at edge states and solitons in the non-Hermitian SSH model, and at\nthe Dirac point of non-Hermitian graphene.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u5d4c\u5957\u5384\u7c73\u7d27\u675f\u7f1a\u6a21\u578b\uff0c\u751f\u6210\u4e86\u5177\u6709p\u4e2a\u8f68\u9053/\u6676\u80de\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u975e\u5384\u7c73\u6a21\u578b\uff0c\u5e76\u63a8\u5e7f\u4e86\u624b\u5f81\u5bf9\u79f0\u6027\u3002\u7814\u7a76\u9610\u8ff0\u4e86\u5355\u5411\u8df3\u8dc3\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u4ee5SSH\u6a21\u578b\u548c\u77f3\u58a8\u70ef\u4e3a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u9ad8\u9636\u5384\u7c73\u70b9\u3002", "motivation": "\u4e3a\u4e86\u5728\u5d4c\u5957\u5384\u7c73\u7d27\u675f\u7f1a\u6a21\u578b\u6846\u67b6\u4e0b\uff0c\u63a2\u7d22\u548c\u751f\u6210\u5177\u6709p\u4e2a\u8f68\u9053/\u6676\u80de\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u975e\u5384\u7c73\u6a21\u578b\uff0c\u5e76\u7406\u89e3\u5176\u624b\u5f81\u5bf9\u79f0\u6027\u7684\u590d\u6742\u63a8\u5e7f\u4ee5\u53ca\u5355\u5411\u8df3\u8dc3\u5bf9\u80fd\u8c31\u548c\u6a21\u578b\u7279\u6027\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5d4c\u5957\u5384\u7c73\u7d27\u675f\u7f1a\u6a21\u578b\uff0c\u5229\u7528p\u4e2a\u8f68\u9053/\u6676\u80de\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u590d\u6570\u63a8\u5e7f\u7684\u624b\u5f81\u5bf9\u79f0\u6027\uff0c\u7406\u8bba\u4e0a\u751f\u6210\u4e86\u975e\u5384\u7c73\u6a21\u578b\u3002\u7814\u7a76\u5206\u6790\u4e86\u5168\u5355\u5411\u8df3\u8dc3\u548c\u90e8\u5206\u5355\u5411\u8df3\u8dc3\u5bf9\u5e03\u6d1b\u8d6b\u54c8\u5bc6\u987f\u91cf\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5176\u5bf9\u5b9e\u80fd\u91cf\u8c31\u6f14\u5316\u4e3a\u590d\u6570\u80fd\u8c31\u7684\u5f71\u54cd\u3002\u901a\u8fc7SSH\u6a21\u578b\uff08p=3\u548cp=4\uff09\u548c\u77f3\u58a8\u70ef\u7684\u4f8b\u5b50\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5d4c\u5957\u5384\u7c73\u7d27\u675f\u7f1a\u6a21\u578b\uff0c\u6210\u529f\u751f\u6210\u4e86\u5177\u6709p\u4e2a\u8f68\u9053/\u6676\u80de\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u975e\u5384\u7c73\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4e0eBaxter\u975e\u5384\u7c73\u65f6\u949f\u6a21\u578b\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u89e3\u5177\u6709\u76f8\u540c\u7684\u5355\u7c92\u5b50\u80fd\u7ea7\u3002\u7814\u7a76\u8be6\u7ec6\u5206\u6790\u4e86\u5355\u5411\u8df3\u8dc3\u5bf9\u5e03\u6d1b\u8d6b\u54c8\u5bc6\u987f\u91cf\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u7531\u6b64\u5bfc\u81f4\u7684\u80fd\u8c31\u6f14\u5316\u3002\u5bf9\u4e8ep=3\u548cp=4\u7684SSH\u6a21\u578b\u4ee5\u53ca\u77f3\u58a8\u70ef\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u666e\u9002\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u9ad8\u9636\u5384\u7c73\u70b9\u51fa\u73b0\u5728\u975e\u5384\u7c73SSH\u6a21\u578b\u7684\u8fb9\u7f18\u6001\u3001\u5b64\u5b50\u4ee5\u53ca\u975e\u5384\u7c73\u77f3\u58a8\u70ef\u7684\u72c4\u62c9\u514b\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5229\u7528\u5d4c\u5957\u5384\u7c73\u7d27\u675f\u7f1a\u6a21\u578b\u751f\u6210\u5177\u6709p\u4e2a\u8f68\u9053/\u6676\u80de\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u7684\u975e\u5384\u7c73\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u6ee1\u8db3\u624b\u5f81\u5bf9\u79f0\u6027\u7684\u590d\u6742\u63a8\u5e7f\u3002\u7814\u7a76\u8be6\u7ec6\u9610\u8ff0\u4e86\u5355\u5411\u8df3\u8dc3\u548c\u90e8\u5206\u5355\u5411\u8df3\u8dc3\u5bf9\u5e03\u6d1b\u8d6b\u54c8\u5bc6\u987f\u91cf\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u7531\u6b64\u4ea7\u751f\u7684\u80fd\u8c31\u6f14\u5316\u3002\u7279\u522b\u5730\uff0c\u7814\u7a76\u4ee5p=3\u548cp=4\u7684SSH\u6a21\u578b\u4ee5\u53ca\u77f3\u58a8\u70ef\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5e76\u63ed\u793a\u4e86\u975e\u5384\u7c73SSH\u6a21\u578b\u548c\u77f3\u58a8\u70ef\u4e2d\u7684\u9ad8\u9636\u5384\u7c73\u70b9\u51fa\u73b0\u5728\u8fb9\u7f18\u6001\u3001\u5b64\u5b50\u548c\u72c4\u62c9\u514b\u70b9\u3002"}}
{"id": "2508.11061", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11061", "abs": "https://arxiv.org/abs/2508.11061", "authors": ["Martin Pavl\u00ed\u010dek", "Tom\u00e1\u0161 Filip", "Petr Sos\u00edk"], "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u4e0e\u6781\u5316\u76f8\u5173\u7684\u504f\u89c1\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u91cd\u7528\u3001\u7ec6\u7c92\u5ea6\u4e14\u4e0e\u4e3b\u9898\u65e0\u5173\u3002\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u7684\u3001\u5305\u542b\u51b2\u7a81\u76f8\u5173\u9648\u8ff0\u7684\u5e73\u8861\u6570\u636e\u96c6\u548c\u5bf9\u6781\u5316\u654f\u611f\u7684sentiment\u6307\u6807\uff0c\u7814\u7a76\u8bc4\u4f30\u4e86\u5305\u62ecGPT-4\u548cGemini\u5728\u5185\u7684\u591a\u4e2aLLMs\uff0c\u53d1\u73b0\u603b\u4f53\u4e0a\u5bf9\u4e4c\u514b\u5170\u7684\u60c5\u611f\u503e\u5411\u66f4\u79ef\u6781\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u4e49\u7c7b\u522b\u4e2d\u7684\u7ec6\u5fae\u884c\u4e3a\u5dee\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u653f\u6cbb\u8a00\u8bba\u3001\u6027\u522b\u8ba4\u540c\u3001\u6c11\u65cf\u5173\u7cfb\u6216\u56fd\u5bb6\u523b\u677f\u5370\u8c61\u7b49\u654f\u611f\u8bdd\u9898\u65f6\u3002\u867d\u7136\u5728\u504f\u89c1\u68c0\u6d4b\u548c\u7f13\u89e3\u6280\u672f\u65b9\u9762\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u6709\u4e00\u4e9b\u6311\u6218\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u91cd\u7528\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u3001\u4e0e\u4e3b\u9898\u65e0\u5173\u7684\u6846\u67b6\u6765\u8bc4\u4f30LLM\uff08\u5305\u62ec\u5f00\u6e90\u548c\u95ed\u6e90\uff09\u4e2d\u4e0e\u6781\u5316\u76f8\u5173\u7684\u504f\u89c1\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bf9\u6781\u5316\u654f\u611f\u7684sentiment\u6307\u6807\u548c\u4e00\u4e2a\u5408\u6210\u751f\u6210\u7684\u3001\u5305\u542b\u51b2\u7a81\u76f8\u5173\u9648\u8ff0\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u8bed\u4e49\u7c7b\u522b\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u805a\u7126\u4e8e\u4fc4\u4e4c\u6218\u4e89\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86Llama-3\u3001Mistral\u3001GPT-4\u3001Claude 3.5\u548cGemini 1.0\u7b49LLM\u7684\u504f\u89c1\u3002\u7ed3\u679c\u663e\u793a\uff0c\u603b\u4f53\u4e0a\u5bf9\u4e4c\u514b\u5170\u7684\u60c5\u611f\u503e\u5411\u66f4\u79ef\u6781\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u80fd\u591f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e4b\u95f4\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u5bf9\u63d0\u793a\u4fee\u6539\u7684\u9002\u5e94\u6027\u8fdb\u4e00\u6b65\u663e\u793a\u51fa\u5bf9\u5148\u5165\u4e3a\u4e3b\u7684\u8bed\u8a00\u548c\u516c\u6c11\u8eab\u4efd\u4fee\u6539\u7684\u504f\u89c1\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u548c\u7ec6\u7c92\u5ea6\u504f\u89c1\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7531\u6781\u5316\u9a71\u52a8\u7684\u573a\u666f\u548c\u4e3b\u9898\uff0c\u5e76\u4e14\u4e0e\u8bb8\u591a\u5176\u4ed6\u504f\u89c1\u8bc4\u4f30\u7b56\u7565\u6b63\u4ea4\u3002"}}
{"id": "2508.11037", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2508.11037", "abs": "https://arxiv.org/abs/2508.11037", "authors": ["Oliver Ethan Richardson"], "title": "Learning with Confidence", "comment": "Accepted for oral UAI 2025, plus some additional modifications for\n  clarity", "summary": "We characterize a notion of confidence that arises in learning or updating\nbeliefs: the amount of trust one has in incoming information and its impact on\nthe belief state. This learner's confidence can be used alongside (and is\neasily mistaken for) probability or likelihood, but it is fundamentally a\ndifferent concept -- one that captures many familiar concepts in the\nliterature, including learning rates and number of training epochs, Shafer's\nweight of evidence, and Kalman gain. We formally axiomatize what it means to\nlearn with confidence, give two canonical ways of measuring confidence on a\ncontinuum, and prove that confidence can always be represented in this way.\nUnder additional assumptions, we derive more compact representations of\nconfidence-based learning in terms of vector fields and loss functions. These\nrepresentations induce an extended language of compound \"parallel\"\nobservations. We characterize Bayes Rule as the special case of an optimizing\nlearner whose loss representation is a linear expectation.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u754c\u5b9a\u5e76\u5f62\u5f0f\u5316\u4e86\u201c\u4fe1\u5fc3\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u5b83\u4e0d\u540c\u4e8e\u6982\u7387\u6216\u4f3c\u7136\uff0c\u66f4\u80fd\u6355\u6349\u5b66\u4e60\u7387\u3001\u8bc1\u636e\u6743\u91cd\u7b49\u76f4\u89c2\u6982\u5ff5\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5ea6\u91cf\u4fe1\u5fc3\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u5411\u91cf\u573a\u3001\u635f\u5931\u51fd\u6570\u8054\u7cfb\u8d77\u6765\uff0c\u6700\u7ec8\u5c06\u8d1d\u53f6\u65af\u89c4\u5219\u8868\u8ff0\u4e3a\u4e00\u79cd\u7279\u6b8a\u7684\u7ebf\u6027\u671f\u671b\u635f\u5931\u4f18\u5316\u5b66\u4e60\u5668\u3002", "motivation": "\u4e3a\u4e86\u8868\u5f81\u5b66\u4e60\u6216\u66f4\u65b0\u4fe1\u5ff5\u65f6\u4ea7\u751f\u7684\u201c\u4fe1\u5fc3\u201d\u6982\u5ff5\uff0c\u5373\u5bf9\u8f93\u5165\u4fe1\u606f\u7684\u4fe1\u4efb\u7a0b\u5ea6\u53ca\u5176\u5bf9\u4fe1\u5ff5\u72b6\u6001\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u516c\u7406\u5316\u201c\u5e26\u4fe1\u5fc3\u5b66\u4e60\u201d\u7684\u542b\u4e49\uff0c\u63d0\u51fa\u4e24\u79cd\u5ea6\u91cf\u4fe1\u5fc3\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u8868\u793a\u3002\u5728\u9644\u52a0\u5047\u8bbe\u4e0b\uff0c\u5c06\u57fa\u4e8e\u4fe1\u5fc3\u7684\u5b66\u4e60\u8868\u793a\u4e3a\u5411\u91cf\u573a\u548c\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63a8\u5bfc\u51fa\u5316\u5408\u7269\u201c\u5e76\u884c\u201d\u89c2\u6d4b\u7684\u6269\u5c55\u8bed\u8a00\u3002", "result": "\u6b63\u5f0f\u516c\u7406\u5316\u4e86\u5e26\u4fe1\u5fc3\u7684\u5b66\u4e60\uff0c\u7ed9\u51fa\u4e86\u4e24\u79cd\u5ea6\u91cf\u4fe1\u5fc3\u7684\u5178\u578b\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4fe1\u5fc3\u603b\u662f\u53ef\u4ee5\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u8868\u793a\u3002\u5728\u9644\u52a0\u5047\u8bbe\u4e0b\uff0c\u63a8\u5bfc\u51fa\u4e86\u57fa\u4e8e\u4fe1\u5fc3\u7684\u5b66\u4e60\u7684\u66f4\u7d27\u51d1\u7684\u8868\u793a\uff0c\u5373\u5411\u91cf\u573a\u548c\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u5316\u5408\u7269\u201c\u5e76\u884c\u201d\u89c2\u6d4b\u7684\u6269\u5c55\u8bed\u8a00\u3002\u5c06\u8d1d\u53f6\u65af\u89c4\u5219\u8868\u5f81\u4e3a\u635f\u5931\u8868\u793a\u4e3a\u7ebf\u6027\u671f\u671b\u7684\u4f18\u5316\u5b66\u4e60\u5668\u7684\u7279\u4f8b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5b66\u4e60\u6216\u66f4\u65b0\u4fe1\u5ff5\u4e2d\u51fa\u73b0\u7684\u201c\u4fe1\u5fc3\u201d\u6982\u5ff5\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u4fe1\u5fc3\u53ef\u4ee5\u88ab\u8868\u793a\u4e3a\u4e00\u79cd\u5ea6\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a8\u5bfc\u51fa\u4e86\u57fa\u4e8e\u4fe1\u5fc3\u7684\u5b66\u4e60\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5e76\u63ed\u793a\u4e86\u8d1d\u53f6\u65af\u89c4\u5219\u4f5c\u4e3a\u4e00\u79cd\u4f18\u5316\u5b66\u4e60\u5668\u7684\u7279\u4f8b\u3002"}}
{"id": "2508.10943", "categories": ["cs.CV", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.10943", "abs": "https://arxiv.org/abs/2508.10943", "authors": ["Christian D\u00fcreth", "Jan Cond\u00e9-Wolter", "Marek Danczak", "Karsten Tittmann", "J\u00f6rn Jaschinski", "Andreas Hornig", "Maik Gude"], "title": "Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods", "comment": "submitted to Elsevier Composite Part C: Open Access\n  (JCOMC-D-25-00212), 16 pages, 8 Figures, and 3 Tables", "summary": "A detailed understanding of material structure across multiple scales is\nessential for predictive modeling of textile-reinforced composites. Nesting --\ncharacterized by the interlocking of adjacent fabric layers through local\ninterpenetration and misalignment of yarns -- plays a critical role in defining\nmechanical properties such as stiffness, permeability, and damage tolerance.\nThis study presents a framework to quantify nesting behavior in dry textile\nreinforcements under compaction using low-resolution computed tomography (CT).\nIn-situ compaction experiments were conducted on various stacking\nconfigurations, with CT scans acquired at 20.22 $\\mu$m per voxel resolution. A\ntailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill\nphases across compaction stages corresponding to fiber volume contents of\n50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822\nand an $F1$ score of 0.902. Spatial structure was subsequently analyzed using\nthe two-point correlation function $S_2$, allowing for probabilistic extraction\nof average layer thickness and nesting degree. The results show strong\nagreement with micrograph-based validation. This methodology provides a robust\napproach for extracting key geometrical features from industrially relevant CT\ndata and establishes a foundation for reverse modeling and descriptor-based\nstructural analysis of composite preforms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387CT\u548c3D-UNet\u91cf\u5316\u538b\u5b9e\u7eba\u7ec7\u54c1\u5d4c\u5957\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u5c42\u539a\u548c\u5d4c\u5957\u7a0b\u5ea6\u7684\u63d0\u53d6\u7ed3\u679c\u4e0e\u663e\u5fae\u7167\u7247\u9a8c\u8bc1\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e3a\u590d\u5408\u6750\u6599\u9884\u5236\u4ef6\u7684\u9006\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5bf9\u8de8\u8d8a\u591a\u4e2a\u5c3a\u5ea6\u7684\u6750\u6599\u7ed3\u6784\u8fdb\u884c\u8be6\u7ec6\u7406\u89e3\uff0c\u5bf9\u4e8e\u9884\u6d4b\u6027\u5730\u6a21\u62df\u7eba\u7ec7\u589e\u5f3a\u590d\u5408\u6750\u6599\u81f3\u5173\u91cd\u8981\u3002\u5d4c\u5957\u2014\u2014\u5176\u7279\u70b9\u662f\u76f8\u90bb\u7ec7\u7269\u5c42\u901a\u8fc7\u7eb1\u7ebf\u7684\u5c40\u90e8\u4e92\u7a7f\u548c\u9519\u4f4d\u800c\u76f8\u4e92\u8054\u9501\u2014\u2014\u5728\u5b9a\u4e49\u521a\u5ea6\u3001\u6e17\u900f\u6027\u548c\u635f\u4f24\u5bb9\u9650\u7b49\u673a\u68b0\u6027\u80fd\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u91cf\u5316\u5e72\u6027\u7eba\u7ec7\u589e\u5f3a\u6750\u6599\u5728\u538b\u5b9e\u884c\u4e3a\u4e0b\u5d4c\u5957\u884c\u4e3a\u7684\u6846\u67b6\u3002\u5bf9\u5404\u79cd\u5806\u53e0\u914d\u7f6e\u8fdb\u884c\u4e86\u539f\u4f4d\u538b\u5b9e\u5b9e\u9a8c\uff0c\u5e76\u4ee5\u6bcf\u4e2a\u4f53\u7d2020.22\u03bcm\u7684\u5206\u8fa8\u7387\u83b7\u53d6\u4e86CT\u626b\u63cf\u3002\u4f7f\u7528\u5b9a\u5236\u76843D-UNet\u5bf9\u7ea4\u7ef4\u4f53\u79ef\u5206\u6570\u4e3a50%\u201360%\u7684\u538b\u5b9e\u9636\u6bb5\u7684\u57fa\u4f53\u3001\u7eac\u7eb1\u548c\u586b\u5145\u7eb1\u76f8\u8fdb\u884c\u4e86\u8bed\u4e49\u5206\u5272\u3002\u8be5\u6a21\u578b\u5b9e\u73b0\u4e860.822\u7684\u6700\u5c0f\u5e73\u5747\u4ea4\u5e76\u6bd4\u548c0.902\u7684F1\u5206\u6570\u3002\u968f\u540e\u4f7f\u7528\u4e24\u70b9\u76f8\u5173\u51fd\u6570S2\u5bf9\u7a7a\u95f4\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u4ece\u800c\u80fd\u591f\u6982\u7387\u6027\u5730\u63d0\u53d6\u5e73\u5747\u5c42\u539a\u548c\u5d4c\u5957\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u663e\u5fae\u7167\u7247\u7684\u9a8c\u8bc1\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u53d6\u5173\u952e\u51e0\u4f55\u7279\u5f81\u548c\u590d\u5408\u6750\u6599\u9884\u5236\u4ef6\u7684\u9006\u5efa\u6a21\u4e0e\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u7ed3\u6784\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11295", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11295", "abs": "https://arxiv.org/abs/2508.11295", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Weifeng Zhu", "Haijun Zhang"], "title": "Optimizing Rate-CRB Performance for Beyond Diagonal Reconfigurable Intelligent Surface Enabled ISAC", "comment": "to appear in IEEE Communications Letters", "summary": "This letter considers a beyond diagonal reconfigurable intelligent surface\n(BD-RIS) aided integrated sensing and communication (ISAC) system, where the\nBD-RIS can help a multi-antenna base station (BS) serve multiple user\nequipments (UEs) and localize a target simultaneously. We formulate an\noptimization problem that designs the BS beamforming matrix and the BD-RIS\nscattering matrix to maximize UEs' sum rate subject to a localization\nCramer-Rao bound (CRB) constraint and an additional unitary matrix constraint\nfor the scattering matrix. Because unitary matrices form a manifold, our\nproblem belongs to constrained manifold optimization. This letter proposes a\nlog-barrier based Riemannian steepest ascent method to solve this problem\neffectively. Numerical results verify the effectiveness of our algorithm and\nthe performance gain of the BD-RIS aided ISAC systems over the conventional RIS\naided ISAC systems.", "AI": {"tldr": "BD-RIS \u8f85\u52a9 ISAC \u7cfb\u7edf\u7684\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u5728\u8d85\u8d8a\u5bf9\u89d2\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762 (BD-RIS) \u8f85\u52a9\u7684\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1 (ISAC) \u7cfb\u7edf\u4e2d\uff0c\u6700\u5927\u5316 UEs \u7684\u603b\u548c\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u672c\u5730\u5316 Cramer-Rao \u754c (CRB) \u7ea6\u675f\u548c\u6563\u5c04\u77e9\u9635\u7684\u9149\u77e9\u9635\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u969c\u788d\u7684\u9ece\u66fc\u6700\u901f\u4e0a\u5347\u6cd5\u6765\u89e3\u51b3\u8be5\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6709\u6548\u6027\u4ee5\u53ca BD-RIS \u8f85\u52a9 ISAC \u7cfb\u7edf\u76f8\u5bf9\u4e8e\u4f20\u7edf RIS \u8f85\u52a9 ISAC \u7cfb\u7edf\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "BD-RIS \u6709\u52a9\u4e8e ISAC \u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2508.11270", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.11270", "abs": "https://arxiv.org/abs/2508.11270", "authors": ["Fabio Tarocco", "Davide Materia", "Leonardo Ratini", "Leonardo Guidoni"], "title": "Multi-QIDA method for VQE state preparation in molecular systems", "comment": null, "summary": "The development of quantum algorithms and their application to quantum\nchemistry has introduced new opportunities for solving complex molecular\nproblems that are computationally infeasible for classical methods. In quantum\nchemistry, the Variational Quantum Eigensolver (VQE) is a hybrid\nquantum-classical algorithm designed to estimate ground-state energies of\nmolecular systems. Despite its promise, VQE faces challenges such as\nscalability issues, high circuit depths, and barren plateaus that make the\noptimization of the variational wavefunction. To mitigate these challenges, the\nQuantum Information Driven Ansatz (QIDA) leverages Quantum Mutual Information\n(QMI) to construct compact, correlation-driven circuits. In this work, we go\nback to the original field of application of QIDA, by applying the already\ndefined Multi-Threshold Quantum Information Driven Ansatz (Multi-QIDA)\nmethodology on Molecular Systems. to systematically construct shallow, layered\nquantum circuits starting from approximate QMI matrices obtained by Quantum\nChemistry calculations. The Multi-QIDA approach combines efficient creation of\nthe QMI map, reduction of the number of correlators required by exploiting\nMinimum/Maximum spanning tress, and an iterative layer-wise VQE optimization\nroutine. These enhancements allow the method to recover missing correlations in\nmolecular systems while maintaining computational efficiency. Additionally, the\napproach incorporates alternative gate constructions, such as SO(4)\ncorrelators, to enhance the circuit expressibility without significantly\nincreasing the circuit complexity. We benchmark Multi-QIDA on systems ranging\nfrom small molecules like H2O, BeH2, and NH3 in Iterative Natural Orbitals\n(INOs) basis set, to active-space models such as H2O-6-31G-CAS(4,4) and\nN2-cc-pVTZ-CAS(6,6), comparing it to traditional hardware-efficient ansatze.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMulti-QIDA\u7684\u65b0\u578b\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u91cf\u5b50\u5316\u5b66\u4e2d\u7684\u590d\u6742\u5206\u5b50\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u91cf\u5b50\u4e92\u4fe1\u606f\uff08QMI\uff09\u6765\u6784\u5efa\u9ad8\u6548\u3001\u6d45\u5c42\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u7ed3\u5408VQE\u4f18\u5316\u6280\u672f\uff0c\u5728\u5904\u7406\u5206\u5b50\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u7684\u590d\u6742\u6027\u5bf9\u7ecf\u5178\u8ba1\u7b97\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u4f5c\u4e3a\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7b97\u6cd5\uff0c\u867d\u7136\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u9762\u4e34\u7740\u53ef\u6269\u5c55\u6027\u3001\u7535\u8def\u6df1\u5ea6\u548c\u65e0\u0628\u0631\u0627\u06cc\u0646\u5e73\u539f\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u63d0\u9ad8VQE\u5728\u5206\u5b50\u7cfb\u7edf\u4e0a\u4f30\u8ba1\u57fa\u6001\u80fd\u91cf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u9608\u503c\u91cf\u5b50\u4fe1\u606f\u9a71\u52a8ansatz\uff08Multi-QIDA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u91cf\u5b50\u4e92\u4fe1\u606f\uff08QMI\uff09\u6765\u6784\u5efa\u7d27\u51d1\u3001\u7531\u76f8\u5173\u9a71\u52a8\u7684\u91cf\u5b50\u7535\u8def\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86QMI\u56fe\u7684\u6709\u6548\u521b\u5efa\u3001\u5229\u7528\u6700\u5c0f/\u6700\u5927\u751f\u6210\u6811\u51cf\u5c11\u76f8\u5173\u5668\u6570\u91cf\u4ee5\u53ca\u8fed\u4ee3\u5c42\u72b6VQE\u4f18\u5316\u4f8b\u7a0b\uff0c\u5e76\u91c7\u7528\u4e86SO(4)\u76f8\u5173\u5668\u7b49\u66ff\u4ee3\u95e8\u7ed3\u6784\u6765\u589e\u5f3a\u7535\u8def\u7684\u53ef\u8868\u8fbe\u6027\u3002", "result": "\u5728H2O\u3001BeH2\u548cNH3\u7b49\u5c0f\u5206\u5b50\u4ee5\u53caH2O-6-31G-CAS(4,4)\u548cN2-cc-pVTZ-CAS(6,6)\u7b49\u6d3b\u6027\u7a7a\u95f4\u6a21\u578b\u4e0a\uff0cMulti-QIDA\u65b9\u6cd5\u5728\u6062\u590d\u5206\u5b50\u7cfb\u7edf\u4e2d\u7684\u7f3a\u5931\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u4e0e\u4f20\u7edf\u7684\u786c\u4ef6\u9ad8\u6548ansatz\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u66f4\u6d45\u3001\u66f4\u5206\u5c42\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u6709\u6548\u5904\u7406\u76f8\u5173\u6027\u3002", "conclusion": "Multi-QIDA\u901a\u8fc7\u5229\u7528\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u83b7\u5f97\u7684\u8fd1\u4f3c\u91cf\u5b50\u4e92\u4fe1\u606f\uff08QMI\uff09\u77e9\u9635\uff0c\u7ed3\u5408QMI\u56fe\u7684\u6709\u6548\u521b\u5efa\u3001\u5229\u7528\u6700\u5c0f/\u6700\u5927\u751f\u6210\u6811\u51cf\u5c11\u6240\u9700\u7684\u76f8\u5173\u5668\u6570\u91cf\u4ee5\u53ca\u8fed\u4ee3\u5c42\u72b6VQE\u4f18\u5316\u4f8b\u7a0b\uff0c\u5728\u5206\u5b50\u7cfb\u7edf\u4e0a\u7cfb\u7edf\u5730\u6784\u5efa\u6d45\u5c42\u3001\u5206\u5c42\u91cf\u5b50\u7535\u8def\uff0c\u4ee5\u5e94\u5bf9VQE\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2508.11232", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11232", "abs": "https://arxiv.org/abs/2508.11232", "authors": ["Guoliang Li", "Xibin Jin", "Yujie Wan", "Chenxuan Liu", "Tong Zhang", "Shuai Wang", "Chengzhong Xu"], "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification", "comment": "9 pages, 6 figures, to appear in IEEE Network", "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u8fd1\u573a\u901a\u4fe1\uff08NFC\uff09\u4e0e\u5177\u8eab\u8fb9\u7f18\u667a\u80fd\uff08EEI\uff09\u76f8\u7ed3\u5408\uff0c\u5f62\u6210NEEI\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u89c4\u5212\u3001\u6ce2\u675f\u805a\u7126\u548c\u5bfc\u822a\u6280\u672f\u89e3\u51b3\u4e86\u5176\u901a\u4fe1\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u8bc1\u5b9e\u65f6\u63a8\u7406\u7684\u540c\u65f6\u652f\u6301\u5927\u578b\u6a21\u578b\uff08LMs\uff09\u5728\u5177\u8eab\u673a\u5668\u4eba\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u5177\u8eab\u8fb9\u7f18\u667a\u80fd\uff08EEI\uff09\u8303\u5f0f\u3002\u7136\u800c\uff0cEEI\u7684\u6570\u636e\u4ea4\u6362\u9700\u6c42\u5bf9\u5176\u901a\u4fe1\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff08\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u3001\u901a\u4fe1\u5b89\u5168\u6027\u548c\u51cf\u5c11\u7528\u6237\u95f4\u5e72\u6270\uff09\uff0c\u800c\u8fd1\u573a\u901a\u4fe1\uff08NFC\uff09\u662f\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002\u56e0\u6b64\uff0c\u5c06EEI\u4e0eNFC\u76f8\u7ed3\u5408\u7684NEEI\u8303\u5f0f\u5e94\u8fd0\u800c\u751f\u3002", "method": "\u63d0\u51fa\u7c7b\u65e0\u7ebf\u7535\u5177\u8eab\u89c4\u5212\uff08radio-friendly embodied planning\uff09\u7528\u4e8eEEI\u8f85\u52a9NFC\u573a\u666f\uff0c\u63d0\u51fa\u89c6\u56fe\u5f15\u5bfc\u6ce2\u675f\u805a\u7126\uff08view-guided beam-focusing\uff09\u7528\u4e8eNFC\u8f85\u52a9EEI\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u673a\u4f1a\u6027\u534f\u4f5c\u5bfc\u822a\uff08opportunistic collaborative navigation\uff09\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684NEEI\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u6280\u672f\u76f8\u6bd4\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8fd1\u573a\u901a\u4fe1\uff08NFC\uff09\u4e0e\u5177\u8eab\u8fb9\u7f18\u667a\u80fd\uff08EEI\uff09\u76f8\u7ed3\u5408\u7684\u65b0\u578b\u8fd1\u573a\u5177\u8eab\u8fb9\u7f18\u667a\u80fd\uff08NEEI\uff09\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u4f18\u5316\u6280\u672f\u4ee5\u5e94\u5bf9\u5176\u5e26\u6765\u7684\u6311\u6218\uff0c\u5305\u62ec\u9762\u5411EEI\u8f85\u52a9NFC\u573a\u666f\u7684\u201c\u7c7b\u65e0\u7ebf\u7535\u201d\u89c4\u5212\u548c\u9762\u5411NFC\u8f85\u52a9EEI\u573a\u666f\u7684\u89c6\u56fe\u5f15\u5bfc\u6ce2\u675f\u805a\u7126\uff0c\u5e76\u901a\u8fc7\u673a\u4f1a\u6027\u534f\u4f5c\u5bfc\u822a\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684NEEI\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6280\u672f\u76f8\u5bf9\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.11524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "This paper presents an LLM-assisted planner that tackles large-scale planning problems by decomposing them and using LLMs (LLM4Inspire for general knowledge, LLM4Predict for domain-specific knowledge) to prune the search space. Results show it's effective, especially LLM4Predict.", "motivation": "Large-scale planning problems face state-space explosion due to increasing objects and actions. Leveraging LLMs to prune search spaces is a recent trend, but prior work has overlooked integrating LLMs with domain-specific knowledge to ensure plan validity.", "method": "The paper proposes a novel LLM-assisted planner that integrates problem decomposition. It explores two paradigms: LLM4Inspire (using general knowledge for heuristic guidance) and LLM4Predict (using domain-specific knowledge to infer intermediate conditions). The planner decomposes large problems into simpler sub-tasks and utilizes LLMs to assist in this decomposition.", "result": "Empirical validation across multiple domains demonstrates the planner's effectiveness in partitioning the search space for large-scale planning problems. LLMs successfully prune the search space to find feasible solutions, with LLM4Predict showing particular promise compared to LLM4Inspire.", "conclusion": "LLM-assisted planning, particularly when infused with domain-specific knowledge (LLM4Predict), shows significant promise in effectively pruning search spaces and locating feasible solutions for large-scale planning problems."}}
{"id": "2508.10942", "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.10942", "abs": "https://arxiv.org/abs/2508.10942", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u4e00\u79cd\u79f0\u4e3a Artcodes \u7684\u88c5\u9970\u6027\u6807\u8bb0\uff0c\u5b83\u901a\u8fc7\u5c06\u4fe1\u606f\u7f16\u7801\u5230\u5176\u62d3\u6251\u7ed3\u6784\u4e2d\u6765\u4f2a\u88c5\u81ea\u5df1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\u6765\u68c0\u6d4b\u5b83\u4eec\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9192\u4eba\u4eec\u6ce8\u610f\u8fd9\u4e9b\u5bf9\u8c61\uff08Artcodes\uff09\u7684\u5b58\u5728\uff0c\u5e76\u89e6\u53d1\u9644\u7740\u5728\u8fd9\u4e9b\u5bf9\u8c61\u4e0a\u7684\u6570\u5b57\u6750\u6599\uff0c\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u4e00\u79cd\u7279\u6b8a\u7684 Artcodes\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u79f0\u4e3a\u65b9\u5411\u76f4\u65b9\u56fe\u7684\u5f62\u72b6\uff0c\u7528\u4e8e\u63cf\u8ff0 Artcode \u7684\u901a\u7528\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7279\u5f81\u5411\u91cf\u5728\u8868\u793a\u62d3\u6251\u7ed3\u6784\u65b9\u9762\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u68c0\u6d4b Artcode \u63d0\u6848\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u5f00\u53d1\u7528\u4e8e\u68c0\u6d4b\u8bf8\u5982 Artcodes \u4e4b\u7c7b\u7684\u62d3\u6251\u5bf9\u8c61\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u7cfb\u7edf\u7684\u521d\u6b65\u5c1d\u8bd5\uff0c\u5b83\u5c06\u5f00\u542f\u65b0\u7684\u4ea4\u4e92\u673a\u4f1a\uff0c\u5e76\u6fc0\u53d1\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2508.11068", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11068", "abs": "https://arxiv.org/abs/2508.11068", "authors": ["Nicolas Goulet", "Alexandre Blondin Mass\u00e9", "Moussa Abdendi"], "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.", "AI": {"tldr": "This paper embeds digital dictionaries into AMR graphs using LLMs, reduces the graphs while preserving their properties, and analyzes the results in the context of the symbol grounding problem.", "motivation": "To describe how real digital dictionaries can be embedded into AMR digraphs and analyze the properties of the reduced digraphs in relation to the symbol grounding problem.", "method": "Embedding real digital dictionaries into AMR digraphs using pre-trained large language models and reducing those graphs in a confluent manner.", "result": "Real digital dictionaries can be embedded into AMR digraphs, and their reduced properties are analyzed concerning the symbol grounding problem.", "conclusion": "The properties of the reduced digraphs are analyzed and discussed in relation to the symbol grounding problem."}}
{"id": "2508.11050", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11050", "abs": "https://arxiv.org/abs/2508.11050", "authors": ["Ujas Shah", "Manuel Lladser", "Rebecca Morrison"], "title": "Conditional Independence Estimates for the Generalized Nonparanormal", "comment": "22 pages, 7 figures, 3 tables", "summary": "For general non-Gaussian distributions, the covariance and precision matrices\ndo not encode the independence structure of the variables, as they do for the\nmultivariate Gaussian. This paper builds on previous work to show that for a\nclass of non-Gaussian distributions -- those derived from diagonal\ntransformations of a Gaussian -- information about the conditional independence\nstructure can still be inferred from the precision matrix, provided the data\nmeet certain criteria, analogous to the Gaussian case. We call such\ntransformations of the Gaussian as the generalized nonparanormal. The functions\nthat define these transformations are, in a broad sense, arbitrary. We also\nprovide a simple and computationally efficient algorithm that leverages this\ntheory to recover conditional independence structure from the generalized\nnonparanormal data. The effectiveness of the proposed algorithm is demonstrated\nvia synthetic experiments and applications to real-world data.", "AI": {"tldr": "\u5373\u4f7f\u6570\u636e\u662f\u975e\u9ad8\u65af\u5206\u5e03\uff08\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\uff09\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u7cbe\u5ea6\u77e9\u9635\u63a8\u65ad\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "motivation": "\u5bf9\u4e8e\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u4f20\u7edf\u7684\u534f\u65b9\u5dee\u548c\u7cbe\u5ea6\u77e9\u9635\u65e0\u6cd5\u50cf\u5728\u9ad8\u65af\u5206\u5e03\u4e2d\u90a3\u6837\u660e\u786e\u5730\u63ed\u793a\u53d8\u91cf\u7684\u72ec\u7acb\u6027\u7ed3\u6784\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u5728\u9ad8\u65af\u5206\u5e03\u4e2d\u7684\u53d1\u73b0\uff0c\u5373\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u5373\u4f7f\u6570\u636e\u662f\u975e\u9ad8\u65af\u7684\uff0c\u4e5f\u53ef\u4ee5\u4ece\u7cbe\u5ea6\u77e9\u9635\u4e2d\u63a8\u65ad\u51fa\u6761\u4ef6\u72ec\u7acb\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cbe\u5ea6\u77e9\u9635\u63a8\u65ad\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u7684\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\uff0c\u8fd9\u7c7b\u5206\u5e03\u662f\u901a\u8fc7\u5bf9\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u5bf9\u89d2\u53d8\u6362\u5f97\u5230\u7684\u3002", "result": "\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u4ece\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\u6570\u636e\u4e2d\u6210\u529f\u6062\u590d\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u4e00\u7c7b\u79f0\u4e3a\u201c\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\u201d\u7684\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u5373\u4f7f\u5b83\u4eec\u4e0e\u9ad8\u65af\u5206\u5e03\u4e0d\u540c\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u4ece\u5176\u7cbe\u5ea6\u77e9\u9635\u4e2d\u63a8\u65ad\u51fa\u53d8\u91cf\u4e4b\u95f4\u7684\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u3002\u8fd9\u5bf9\u4e8e\u7406\u89e3\u975e\u9ad8\u65af\u6570\u636e\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.11351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11351", "abs": "https://arxiv.org/abs/2508.11351", "authors": ["Haonan Lu", "Rui Meng", "Xiaodong Xu", "Yiming Liu", "Ping Zhang", "Dusit Niyato"], "title": "Important Bit Prefix M-ary Quadrature Amplitude Modulation for Semantic Communications", "comment": null, "summary": "M-ary Quadrature Amplitude Modulation (MQAM) is a commonly used channel\nmodulation technology in wireless communication systems. To achieve dedicated\nchannel modulation for semantic communication (SemCom), we propose an\nImportant-Bit-Prefixed MQAM (IBP-MQAM) scheme and derive its approximate\nexpression of important symbol error rate (ISER) and unimportant symbol error\nrate (USER). By extracting and quantifying text semantics using Latent\nDirichlet Allocation (LDA), we verify that IBP-MQAM achieves improved\nperformance over MQAM in SemCom scenarios and further analyze the effects of\nkey system parameters.", "AI": {"tldr": "\u63d0\u51faIBP-MQAM\u65b9\u6848\uff0c\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\u3002\u901a\u8fc7LDA\u91cf\u5316\u6587\u672c\u8bed\u4e49\uff0c\u9a8c\u8bc1IBP-MQAM\u6027\u80fd\u4f18\u4e8eMQAM\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8bed\u4e49\u901a\u4fe1\uff08SemCom\uff09\u7684\u4e13\u7528\u4fe1\u9053\u8c03\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIBP-MQAM\u7684\u91cd\u8981\u6bd4\u7279\u524d\u7f00MQAM\u4fe1\u9053\u8c03\u5236\u65b9\u6848\uff0c\u5e76\u63a8\u5bfc\u4e86\u91cd\u8981\u7b26\u53f7\u9519\u8bef\u7387\uff08ISER\uff09\u548c\u975e\u91cd\u8981\u7b26\u53f7\u9519\u8bef\u7387\uff08USER\uff09\u7684\u8fd1\u4f3c\u8868\u8fbe\u5f0f\u3002\u901a\u8fc7\u4f7f\u7528\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\uff08LDA\uff09\u63d0\u53d6\u548c\u91cf\u5316\u6587\u672c\u8bed\u4e49\u6765\u9a8c\u8bc1\u3002", "result": "IBP-MQAM\u5728SemCom\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8eMQAM\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "IBP-MQAM\u5728\u8bed\u4e49\u901a\u4fe1\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8eMQAM\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11296", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.11296", "abs": "https://arxiv.org/abs/2508.11296", "authors": ["Neelan Gounden", "Fazilah Nothlawala", "Paola C. Obando", "Thomas Konrad", "Andrew Forbes", "Isaac Nape"], "title": "Unveiling the link between quantum ghost imaging and Grover's quantum searching algorithm", "comment": null, "summary": "Photonic quantum technologies have become pivotal in the implementation of\ncommunication, imaging and computing modalities. Among these applications,\nquantum ghost imaging (GI) exploits photon correlations to surpass classical\nlimits, with recent advances in spatial-mode encoding and phase imaging. In\nparallel, all-optical computing offers powerful, passive-light processing\ncapabilities. Here, we explore the intersection of these domains, revealing a\nconceptual and operational link between GI and Grover's quantum search\nalgorithm (GSA) which is designed to search for elements in an unstructured\ndatabase. Here, the elements are encoded as phases in the position basis states\nof photons. To show this, we use entangled photon pairs, with one photon\nencoding the oracle features while the other photon is used to find the marked\nelement.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u91cf\u5b50\u9b3c\u6210\u50cf\uff08GI\uff09\u4e0eGrover\u91cf\u5b50\u641c\u7d22\u7b97\u6cd5\uff08GSA\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u5229\u7528\u7ea0\u7f20\u5149\u5b50\u548c\u76f8\u4f4d\u7f16\u7801\uff0c\u5728GI\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86GSA\uff0c\u7528\u4e8e\u641c\u7d22\u975e\u7ed3\u6784\u5316\u6570\u636e\u5e93\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u9b3c\u6210\u50cf\uff08GI\uff09\u4e0e\u5168\u5149\u5b66\u8ba1\u7b97\u7684\u4ea4\u53c9\u9886\u57df\uff0c\u63ed\u793a\u5176\u4e0eGrover\u91cf\u5b50\u641c\u7d22\u7b97\u6cd5\uff08GSA\uff09\u7684\u8054\u7cfb\u3002", "method": "\u5229\u7528\u7ea0\u7f20\u5149\u5b50\u5bf9\uff0c\u5176\u4e2d\u4e00\u4e2a\u5149\u5b50\u7528\u4e8e\u7f16\u7801\u201c\u795e\u8c15\u201d\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u5149\u5b50\u7528\u4e8e\u5bfb\u627e\u88ab\u6807\u8bb0\u7684\u76ee\u6807\u9879\u3002", "result": "\u5b9e\u73b0\u4e86\u5c06GSA\u5e94\u7528\u4e8e\u5305\u542b\u76f8\u4f4d\u7f16\u7801\u7684GI\u7cfb\u7edf\uff0c\u4ee5\u641c\u7d22\u975e\u7ed3\u6784\u5316\u6570\u636e\u5e93\u4e2d\u7684\u76ee\u6807\u9879\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u91cf\u5b50\u9b3c\u6210\u50cf\uff08GI\uff09\u4e0eGrover\u91cf\u5b50\u641c\u7d22\u7b97\u6cd5\uff08GSA\uff09\u4e4b\u95f4\u5b58\u5728\u6982\u5ff5\u548c\u64cd\u4f5c\u4e0a\u7684\u8054\u7cfb\uff0c\u5176\u4e2d\u76ee\u6807\u9879\u88ab\u7f16\u7801\u4e3a\u5149\u5b50\u5728\u4f4d\u7f6e\u57fa\u6001\u4e0b\u7684\u76f8\u4f4d\u3002"}}
{"id": "2508.11261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11261", "abs": "https://arxiv.org/abs/2508.11261", "authors": ["Shan Luo", "Nathan F. Lepora", "Wenzhen Yuan", "Kaspar Althoefer", "Gordon Cheng", "Ravinder Dahiya"], "title": "Tactile Robotics: An Outlook", "comment": "20 pages, 2 figures, accepted to IEEE Transactions on Robotics", "summary": "Robotics research has long sought to give robots the ability to perceive the\nphysical world through touch in an analogous manner to many biological systems.\nDeveloping such tactile capabilities is important for numerous emerging\napplications that require robots to co-exist and interact closely with humans.\nConsequently, there has been growing interest in tactile sensing, leading to\nthe development of various technologies, including piezoresistive and\npiezoelectric sensors, capacitive sensors, magnetic sensors, and optical\ntactile sensors. These diverse approaches utilise different transduction\nmethods and materials to equip robots with distributed sensing capabilities,\nenabling more effective physical interactions. These advances have been\nsupported in recent years by simulation tools that generate large-scale tactile\ndatasets to support sensor designs and algorithms to interpret and improve the\nutility of tactile data. The integration of tactile sensing with other\nmodalities, such as vision, as well as with action strategies for active\ntactile perception highlights the growing scope of this field. To further the\ntransformative progress in tactile robotics, a holistic approach is essential.\nIn this outlook article, we examine several challenges associated with the\ncurrent state of the art in tactile robotics and explore potential solutions to\ninspire innovations across multiple domains, including manufacturing,\nhealthcare, recycling and agriculture.", "AI": {"tldr": "This paper discusses the importance of tactile sensing in robotics for human-robot interaction, reviews current technologies and advancements, and outlines future challenges and solutions for applications in manufacturing, healthcare, recycling, and agriculture.", "motivation": "Robotics research aims to equip robots with touch perception analogous to biological systems, which is crucial for emerging applications requiring close human-robot interaction. This necessitates advancements in tactile sensing technologies.", "method": "The paper examines challenges and explores potential solutions in tactile robotics, drawing on advancements in diverse sensing technologies (piezoresistive, piezoelectric, capacitive, magnetic, optical), simulation tools for dataset generation, and the integration of tactile sensing with vision and action strategies.", "result": "The paper reviews various tactile sensing technologies, the role of simulation tools, and the integration of tactile sensing with other modalities and action strategies, highlighting the growing scope and potential of the field.", "conclusion": "The integration of tactile sensing with other modalities and action strategies, along with a holistic approach, is essential for further transformative progress in tactile robotics, with potential applications in manufacturing, healthcare, recycling, and agriculture."}}
{"id": "2508.07264", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07264", "abs": "https://arxiv.org/abs/2508.07264", "authors": ["Van Duc Cuong", "Ta Dinh Tam", "Tran Duc Chinh", "Nguyen Thi Hanh"], "title": "FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning", "comment": null, "summary": "Multimodal classification requires robust integration of visual and textual\nsignals, yet common fusion strategies are brittle and vulnerable to\nmodality-specific noise. In this paper, we present \\textsc{FLUID}-Flow-Latent\nUnified Integration via Token Distillation for Expert Specialization, a\nprincipled token-level pipeline that improves cross-modal robustness and\nscalability. \\textsc{FLUID} contributes three core elements: (1)\n\\emph{Q-transforms}, learnable query tokens that distill and retain salient\ntoken-level features from modality-specific backbones; (2) a two-stage fusion\nscheme that enforces cross-modal consistency via contrastive alignment and then\nperforms adaptive, task-aware fusion through a gating mechanism and a\n\\emph{Q-bottleneck} that selectively compresses information for downstream\nreasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at\nprediction time that enables efficient specialization to diverse semantic\npatterns. Extensive experiments demonstrate that \\textsc{FLUID} attains\n\\(91\\%\\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior\nbaselines and exhibiting strong resilience to label noise, long-tail class\nimbalance, and semantic heterogeneity. Targeted ablation studies corroborate\nboth the individual and synergistic benefits of the proposed components,\npositioning \\textsc{FLUID} as a scalable, noise-resilient solution for\nmultimodal product classification.", "AI": {"tldr": "FLUID\u901a\u8fc7\u4ee4\u724c\u84b8\u998f\u548c\u4e13\u5bb6\u6df7\u5408\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u5206\u7c7b\uff0c\u5728GLAMI-1M\u6570\u636e\u96c6\u4e0a\u8fbe\u523091%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5e38\u89c1\u7684\u878d\u5408\u7b56\u7565\u5728\u5904\u7406\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\u65f6\uff0c\u5bb9\u6613\u53d7\u5230\u7279\u5b9a\u6a21\u6001\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u9c81\u68d2\u6027\u8f83\u5dee\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8de8\u6a21\u6001\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\textsc{FLUID}\u91c7\u7528\u57fa\u4e8e\u4ee4\u724c\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u4ee4\u724c\u84b8\u998f\u5b9e\u73b0\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u4ee5\u6539\u8fdb\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5176\u6838\u5fc3\u8981\u7d20\u5305\u62ec\uff1a1) Q-transforms\uff0c\u7528\u4e8e\u4ece\u7279\u5b9a\u6a21\u6001\u7684\u9aa8\u5e72\u7f51\u7edc\u4e2d\u63d0\u53d6\u548c\u4fdd\u7559\u663e\u8457\u4ee4\u724c\u7ea7\u7279\u5f81\u7684\u53ef\u5b66\u4e60\u67e5\u8be2\u4ee4\u724c\uff1b2) \u4e00\u4e2a\u5f3a\u5236\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7684\u4e24\u9636\u6bb5\u878d\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u548c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u4ee5\u53ca\u9009\u62e9\u6027\u538b\u7f29\u4fe1\u606f\u7684Q-bottleneck\u8fdb\u884c\u81ea\u9002\u5e94\u3001\u4efb\u52a1\u611f\u77e5\u878d\u5408\uff1b3) \u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u8d1f\u8f7d\u5747\u8861\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u9884\u6d4b\u5668\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u4e0d\u540c\u8bed\u4e49\u6a21\u5f0f\u7684\u9ad8\u6548\u4e13\u4e1a\u5316\u3002", "result": "FLUID\u5728GLAMI-1M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8691%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u6807\u7b7e\u566a\u58f0\u3001\u957f\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u4e49\u5f02\u8d28\u6027\u7684\u5f3a\u5927\u97e7\u6027\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7ec4\u4ef6\u7684\u5355\u72ec\u548c\u534f\u540c\u6548\u76ca\u3002", "conclusion": "FLUID\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6297\u566a\u7684\u591a\u6a21\u6001\u4ea7\u54c1\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11120", "abs": "https://arxiv.org/abs/2508.11120", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.", "AI": {"tldr": "RAMP\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528LLM\u89c4\u5212\u3001\u5de5\u5177\u548c\u957f\u671f\u8bb0\u5fc6\u6765\u6539\u8fdb\u8425\u9500\u4e2d\u7684\u53d7\u4f17\u7b56\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u6587\u732e\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u52a8\u6001\u3001\u9762\u5411\u884c\u4e1a\u7684\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAMP\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u8425\u9500\u4e2d\u7684\u53d7\u4f17\u7b56\u5212\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u89c4\u5212\u3001\u5de5\u5177\u8c03\u7528\u3001\u8f93\u51fa\u9a8c\u8bc1\u548c\u6539\u8fdb\u5efa\u8bae\u6765\u8fd0\u4f5c\uff0c\u5e76\u7ed3\u5408\u4e86\u957f\u671f\u8bb0\u5fc6\u5e93\uff08\u7279\u5b9a\u5ba2\u6237\u4e8b\u5b9e\u548c\u5386\u53f2\u67e5\u8be2\uff09\u3002", "result": "RAMP\u6846\u67b6\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u4e8628\u4e2a\u767e\u5206\u70b9\uff08\u57fa\u4e8e88\u4e2a\u8bc4\u4f30\u67e5\u8be2\uff09\u3002\u5bf9\u4e8e\u6a21\u7cca\u67e5\u8be2\uff0c\u8fed\u4ee3\u9a8c\u8bc1\u548c\u53cd\u601d\u53ef\u5c06\u53ec\u56de\u7387\u63d0\u9ad8\u7ea620\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "LLM\u89c4\u5212\u548c\u8bb0\u5fc6\u53ef\u4ee5\u63d0\u9ad8\u8425\u9500\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u8fed\u4ee3\u9a8c\u8bc1\u548c\u53cd\u601d\u53ef\u4ee5\u63d0\u9ad8\u6a21\u7cca\u67e5\u8be2\u7684\u53ec\u56de\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2508.11053", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11053", "abs": "https://arxiv.org/abs/2508.11053", "authors": ["Sam Chauhan", "Estelle Duguet", "Karthik Ramakrishnan", "Hugh Van Deventer", "Jack Kruger", "Ranjan Subbaraman"], "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME", "comment": "7 pages, 7 figures", "summary": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable\ninsights into black-box classifiers and are increasingly used to assess model\nbiases and generalizability. However, these methods are vulnerable to\nadversarial manipulation, potentially concealing harmful biases. Building on\nthe work of Slack et al. (2020), we investigate the susceptibility of LIME and\nSHAP to biased models and evaluate strategies for improving robustness. We\nfirst replicate the original COMPAS experiment to validate prior findings and\nestablish a baseline. We then introduce a modular testing framework enabling\nsystematic evaluation of augmented and ensemble explanation approaches across\nclassifiers of varying performance. Using this framework, we assess multiple\nLIME/SHAP ensemble configurations on out-of-distribution models, comparing\ntheir resistance to bias concealment against the original methods. Our results\nidentify configurations that substantially improve bias detection, highlighting\ntheir potential for enhancing transparency in the deployment of high-stakes\nmachine learning systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86LIME\u548cSHAP\u89e3\u91ca\u65b9\u6cd5\u5728\u6709\u504f\u89c1\u6a21\u578b\u4e2d\u7684\u6613\u611f\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u589e\u5f3a\u8fd9\u4e9b\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u7531\u4e8eLIME\u548cSHAP\u7b49\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u64cd\u7eb5\uff0c\u53ef\u80fd\u9690\u85cf\u6709\u5bb3\u504f\u89c1\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5LIME\u548cSHAP\u5728\u6709\u504f\u89c1\u6a21\u578b\u4e2d\u7684\u6613\u611f\u6027\uff0c\u5e76\u8bc4\u4f30\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u7684\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e0d\u540c\u5206\u7c7b\u5668\u4e0a\u589e\u5f3a\u548c\u96c6\u6210\u89e3\u91ca\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u4f7f\u7528\u6b64\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u591a\u79cdLIME/SHAP\u96c6\u6210\u914d\u7f6e\u5728\u5206\u5e03\u5916\u6a21\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5c06\u5176\u504f\u89c1\u9690\u85cf\u62b5\u6297\u80fd\u529b\u4e0e\u539f\u59cb\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u80fd\u591f\u663e\u8457\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u7684LIME/SHAP\u96c6\u6210\u914d\u7f6e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53d1\u73b0\u4e86\u80fd\u591f\u663e\u8457\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u7684\u914d\u7f6e\uff0c\u8fd9\u8868\u660e\u4e86\u5728\u90e8\u7f72\u9ad8\u98ce\u9669\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u65f6\u589e\u5f3a\u900f\u660e\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11457", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11457", "abs": "https://arxiv.org/abs/2508.11457", "authors": ["Hui Cao", "Rui Meng", "Xiaodong Xu", "Shujun Han", "Ping Zhang"], "title": "Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground Communication", "comment": null, "summary": "Satellite-ground semantic communication is anticipated to serve a critical\nrole in the forthcoming 6G era. Nonetheless, task-oriented data transmission in\nsuch systems remains a formidable challenge, primarily due to the dynamic\nnature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth\nlimitations inherent to low Earth orbit (LEO) satellite channels. In response\nto these constraints, we propose an importance-aware robust semantic\ntransmission (IRST) framework, specifically designed for scenarios\ncharacterized by bandwidth scarcity and channel variability. The IRST scheme\nbegins by applying a segmentation model enhancement algorithm to improve the\ngranularity and accuracy of semantic segmentation. Subsequently, a task-driven\nsemantic selection method is employed to prioritize the transmission of\nsemantically vital content based on real-time channel state information.\nFurthermore, the framework incorporates a stack-based, SNR-aware channel codec\ncapable of executing adaptive channel coding in alignment with SNR variations.\nComparative evaluations across diverse operating conditions demonstrate the\nsuperior performance and resilience of the IRST model relative to existing\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIRST\uff08Importance-Aware Robust Semantic Transmission\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b36G\u65f6\u4ee3\u536b\u661f\u5730\u9762\u901a\u4fe1\u4e2d\u7684\u5e26\u5bbd\u9650\u5236\u548c\u4fe1\u9053\u6ce2\u52a8\u95ee\u9898\u3002IRST\u901a\u8fc7\u589e\u5f3a\u5206\u5272\u6a21\u578b\u3001\u4f18\u5148\u4f20\u8f93\u5173\u952e\u8bed\u4e49\u5185\u5bb9\u4ee5\u53ca\u81ea\u9002\u5e94\u4fe1\u9053\u7f16\u7801\u6765\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u5730\u9762\u8bed\u4e49\u901a\u4fe1\u4e2d\uff0c\u7531\u4e8e\u4fe1\u566a\u6bd4\u52a8\u6001\u6ce2\u52a8\u548c\u5e26\u5bbd\u9650\u5236\u5bfc\u81f4\u7684\u9762\u5411\u4efb\u52a1\u7684\u6570\u636e\u4f20\u8f93\u6311\u6218\u3002", "method": "1.\u9996\u5148\uff0c\u5e94\u7528\u4e86\u5206\u5272\u6a21\u578b\u589e\u5f3a\u7b97\u6cd5\u6765\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u7684\u7c92\u5ea6\u548c\u51c6\u786e\u6027\u30022.\u7136\u540e\uff0c\u91c7\u7528\u9762\u5411\u4efb\u52a1\u7684\u8bed\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u6839\u636e\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4f18\u5148\u4f20\u8f93\u8bed\u4e49\u5173\u952e\u5185\u5bb9\u30023.\u6700\u540e\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u5806\u6808\u7684\u3001\u611f\u77e5\u7684\u4fe1\u9053\u7f16\u89e3\u7801\u5668\uff0c\u80fd\u591f\u6839\u636e\u4fe1\u566a\u6bd4\u7684\u53d8\u5316\u6267\u884c\u81ea\u9002\u5e94\u4fe1\u9053\u7f16\u7801\u3002", "result": "\u6240\u63d0\u51fa\u7684IRST\u6846\u67b6\u5728\u5e26\u5bbd\u7a00\u7f3a\u548c\u4fe1\u9053\u591a\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u51c6\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "IRST\u6a21\u578b\u5728\u5e26\u5bbd\u53d7\u9650\u548c\u4fe1\u9053\u591a\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u51c6\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11308", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11308", "abs": "https://arxiv.org/abs/2508.11308", "authors": ["Zhiwei Song", "Lin Chen"], "title": "Spectral characterizations of entanglement witnesses", "comment": "14 pages, no figure", "summary": "We present a systematic investigation of the spectral properties of\nentanglement witnesses (EWs). Specifically, we analyze the infimum and supremum\nof the largest eigenvalue, the smallest eigenvalue, the negativity (defined as\nthe absolute value of the sum of negative eigenvalues), and the squared\nFrobenius norm of a unit-trace (normalized) entanglement witness, along with\nthe conditions under which these values are attained. Our study provides\ndistinct characterizations for decomposable (DEWs) and nondecomposable\nentanglement witnesses (NDEWs). While these two classes share many spectral\nsimilarities, we reveal a fundamental divergence by proving that the infimum of\nthe smallest eigenvalue can be attained by DEWs, yet remains strictly\nunattainable for all NDEWs. We apply the results to provide necessary\nconditions for an EW to possess a mirrored EW. Furthermore, we demonstrate the\nsuperior detection capability of NDEWs by proving that any\nnon-positive-transpose (NPT) state beyond the two-qubit and qubit-qutrit\nsystems can be detected by an NDEW.", "AI": {"tldr": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u7ea0\u7f20\u89c1\u8bc1\u7684\u8c31\u6027\u8d28\uff0c\u533a\u5206\u4e86\u53ef\u5206\u548c\u4e0d\u53ef\u5206\u89c1\u8bc1\uff0c\u5e76\u8bc1\u660e\u4e86\u4e0d\u53ef\u5206\u89c1\u8bc1\u5728\u63a2\u6d4b\u91cf\u5b50\u6001\u65b9\u9762\u5177\u6709\u66f4\u5f3a\u7684\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u7814\u7a76\u7ea0\u7f20\u89c1\u8bc1\uff08EWs\uff09\u7684\u8c31\u6027\u8d28\uff0c\u5e76\u533a\u5206\u53ef\u5206\uff08DEWs\uff09\u548c\u4e0d\u53ef\u5206\u7ea0\u7f20\u89c1\u8bc1\uff08NDEWs\uff09\u7684\u7279\u6027\uff0c\u4ee5\u671f\u4e86\u89e3\u5b83\u4eec\u7684\u5f02\u540c\uff0c\u5e76\u4e3a\u7ea0\u7f20\u6001\u7684\u63a2\u6d4b\u63d0\u4f9b\u65b0\u7684\u65b9\u6cd5\u548c\u89c1\u89e3\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u7ea0\u7f20\u89c1\u8bc1\uff08EWs\uff09\u7684\u8c31\u6027\u8d28\uff0c\u5305\u62ec\u6700\u5927\u7279\u5f81\u503c\u7684\u4e0b\u786e\u754c\u548c\u4e0a\u786e\u754c\u3001\u6700\u5c0f\u7279\u5f81\u503c\u3001\u8d1f\u5ea6\uff08\u6240\u6709\u8d1f\u7279\u5f81\u503c\u4e4b\u548c\u7684\u7edd\u5bf9\u503c\uff09\u4ee5\u53ca\u5355\u4f4d\u8ff9\uff08\u5f52\u4e00\u5316\uff09\u7ea0\u7f20\u89c1\u8bc1\u7684\u5e73\u65b9\u5f17\u7f57\u8d1d\u5c3c\u4e4c\u65af\u8303\u6570\uff0c\u5e76\u786e\u5b9a\u4e86\u8fd9\u4e9b\u503c\u88ab\u8fbe\u5230\u7684\u6761\u4ef6\u3002\u901a\u8fc7\u6bd4\u8f83DEWs\u548cNDEWs\uff0c\u63ed\u793a\u4e86NDEWs\u7684\u6700\u5c0f\u7279\u5f81\u503c\u4e0b\u786e\u754c\u662f\u4e25\u683c\u4e0d\u53ef\u8fbe\u7684\uff0c\u800cDEWs\u53ef\u4ee5\u8fbe\u5230\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cDEWs\u53ef\u4ee5\u8fbe\u5230\u6700\u5c0f\u7279\u5f81\u503c\u4e0b\u786e\u754c\uff0c\u800cNDEWs\u5219\u4e0d\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4e3aEW\u62e5\u6709\u955c\u50cfEW\u63d0\u4f9b\u4e86\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u4e86NDEWs\u80fd\u591f\u63a2\u6d4b\u9664\u4e24\u91cf\u5b50\u6bd4\u7279\u548c\u91cf\u5b50\u6bd4\u7279-\u91cf\u5b50\u4e09\u6bd4\u7279\u7cfb\u7edf\u5916\u7684\u6240\u6709NPT\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ea0\u7f20\u89c1\u8bc1\uff08EWs\uff09\u7684\u8c31\u6027\u8d28\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u533a\u5206\u4e86\u53ef\u5206\uff08DEWs\uff09\u548c\u4e0d\u53ef\u5206\u7ea0\u7f20\u89c1\u8bc1\uff08NDEWs\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u6700\u5c0f\u7279\u5f81\u503c\u4e0b\u786e\u754c\u65b9\u9762\u7684\u6839\u672c\u533a\u522b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5e94\u7528\u8fd9\u4e9b\u7ed3\u679c\u6765\u786e\u5b9aEW\u5177\u6709\u955c\u50cfEW\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u4e86NDEWs\u7684\u4f18\u8d8a\u63a2\u6d4b\u80fd\u529b\uff0c\u80fd\u591f\u63a2\u6d4b\u9664\u4e24\u91cf\u5b50\u6bd4\u7279\u548c\u91cf\u5b50\u6bd4\u7279-\u91cf\u5b50\u4e09\u6bd4\u7279\u7cfb\u7edf\u5916\u7684\u4efb\u4f55\u975e\u6b63\u4ea4\u6362\uff08NPT\uff09\u6001\u3002"}}
{"id": "2508.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11275", "abs": "https://arxiv.org/abs/2508.11275", "authors": ["Masaki Murooka", "Iori Kumagai", "Mitsuharu Morisawa", "Fumio Kanehiro"], "title": "Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation", "comment": null, "summary": "To reduce the computational cost of humanoid motion generation, we introduce\na new approach to representing robot kinematic reachability: the differentiable\nreachability map. This map is a scalar-valued function defined in the task\nspace that takes positive values only in regions reachable by the robot's\nend-effector. A key feature of this representation is that it is continuous and\ndifferentiable with respect to task-space coordinates, enabling its direct use\nas constraints in continuous optimization for humanoid motion planning. We\ndescribe a method to learn such differentiable reachability maps from a set of\nend-effector poses generated using a robot's kinematic model, using either a\nneural network or a support vector machine as the learning model. By\nincorporating the learned reachability map as a constraint, we formulate\nhumanoid motion generation as a continuous optimization problem. We demonstrate\nthat the proposed approach efficiently solves various motion planning problems,\nincluding footstep planning, multi-contact motion planning, and\nloco-manipulation planning for humanoid robots.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5fae\u53ef\u8fbe\u6027\u56fe\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u4eba\u5f62\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5fae\u53ef\u8fbe\u6027\u56fe\u6765\u8868\u793a\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u53ef\u8fbe\u6027\u3002\u8be5\u56fe\u662f\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u6807\u91cf\u503c\u51fd\u6570\uff0c\u4ec5\u5728\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u53ef\u8fbe\u533a\u57df\u53d6\u6b63\u503c\uff0c\u5e76\u4e14\u76f8\u5bf9\u4e8e\u4efb\u52a1\u7a7a\u95f4\u5750\u6807\u662f\u8fde\u7eed\u4e14\u53ef\u5fae\u7684\u3002\u901a\u8fc7\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6216\u652f\u6301\u5411\u91cf\u673a\u4ece\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u6a21\u578b\u751f\u6210\u7684\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u96c6\u5408\u4e2d\u5b66\u4e60\u53ef\u5fae\u53ef\u8fbe\u6027\u56fe\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5305\u62ec\u8db3\u6b65\u89c4\u5212\u3001\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u548c\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u7eb5\u89c4\u5212\u5728\u5185\u7684\u5404\u79cd\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u53ef\u5fae\u53ef\u8fbe\u6027\u56fe\u4f5c\u4e3a\u7ea6\u675f\uff0c\u5c06\u4eba\u5f62\u8fd0\u52a8\u751f\u6210\u8868\u8ff0\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u8db3\u6b65\u89c4\u5212\u3001\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u548c\u8fd0\u52a8\u64cd\u7eb5\u89c4\u5212\u7b49\u591a\u79cd\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2508.10945", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10945", "abs": "https://arxiv.org/abs/2508.10945", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities", "comment": "Under review", "summary": "Potholes on the roads are a serious hazard and maintenance burden. This poses\na significant threat to road safety and vehicle longevity, especially on the\ndiverse and under-maintained roads of India. In this paper, we present a\ncomplete end-to-end system called iWatchRoad for automated pothole detection,\nGlobal Positioning System (GPS) tagging, and real time mapping using\nOpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000\nframes captured across various road types, lighting conditions, and weather\nscenarios unique to Indian environments, leveraging dashcam footage. This\ndataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to\nperform real time pothole detection, while a custom Optical Character\nRecognition (OCR) module was employed to extract timestamps directly from video\nframes. The timestamps are synchronized with GPS logs to geotag each detected\npotholes accurately. The processed data includes the potholes' details and\nframes as metadata is stored in a database and visualized via a user friendly\nweb interface using OSM. iWatchRoad not only improves detection accuracy under\nchallenging conditions but also provides government compatible outputs for road\nassessment and maintenance planning through the metadata visible on the\nwebsite. Our solution is cost effective, hardware efficient, and scalable,\noffering a practical tool for urban and rural road management in developing\nregions, making the system automated. iWatchRoad is available at\nhttps://smlab.niser.ac.in/project/iwatchroad", "AI": {"tldr": "iWatchRoad is an automated system for detecting, geotagging, and mapping potholes using a YOLO model, OCR, and GPS, with a focus on improving road safety and maintenance in India.", "motivation": "Potholes pose a significant threat to road safety and vehicle longevity, especially on India's diverse and under-maintained roads. This necessitates an automated system for pothole detection, tagging, and mapping to aid in road maintenance and planning.", "method": "The iWatchRoad system uses a fine-tuned YOLO model for real-time pothole detection, a custom OCR module for timestamp extraction from video frames, and GPS logs for accurate geotagging. A large, self-annotated dataset of over 7,000 frames was curated from dashcam footage captured in various Indian road conditions. The processed data, including pothole details and frames, is stored in a database and visualized on a web interface using OpenStreetMap.", "result": "The iWatchRoad system enables automated pothole detection, GPS tagging, and real-time mapping using OpenStreetMap. It offers improved detection accuracy under challenging conditions and provides visualized metadata on a web interface for road assessment and maintenance planning.", "conclusion": "iWatchRoad is a cost-effective, hardware-efficient, and scalable solution that improves detection accuracy under challenging conditions and provides government-compatible outputs for road assessment and maintenance planning, making it a practical tool for road management in developing regions."}}
{"id": "2508.11133", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.11133", "abs": "https://arxiv.org/abs/2508.11133", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "MoNaCo \u662f\u4e00\u4e2a\u5305\u542b 1,315 \u4e2a\u81ea\u7136\u3001\u590d\u6742\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30 LLM \u5728\u9700\u8981\u5927\u91cf\u4e2d\u95f4\u6b65\u9aa4\u7684\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002\u73b0\u6709 LLM \u5728\u6b64\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u5bf9\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684 LLM \u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u80fd\u591f\u5145\u5206\u53cd\u6620\u4eba\u7c7b\u89e3\u51b3\u95ee\u9898\u6240\u9700\u65f6\u95f4\u548c\u590d\u6742\u6027\u7684\u81ea\u7136\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8861\u91cf LLM \u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u89e3\u5f0f\u6807\u6ce8\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6536\u96c6\u548c\u624b\u52a8\u56de\u7b54\u81ea\u7136\u3001\u8017\u65f6\u7684\u95ee\u9898\uff0c\u4ee5\u6784\u5efa MoNaCo \u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728 MoNaCo \u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5f53\u524d\u7684 LLM \u6700\u591a\u53ea\u80fd\u8fbe\u5230 61.2% \u7684 F1 \u5206\u6570\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u53ec\u56de\u7387\u4f4e\u548c\u5e7b\u89c9\u9020\u6210\u7684\u3002", "conclusion": "LLMs \u5728\u5904\u7406\u9700\u8981\u5927\u91cf\u4e2d\u95f4\u6b65\u9aa4\u7684\u590d\u6742\u3001\u4fe1\u606f\u68c0\u7d22\u578b\u95ee\u9898\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0cMoNaCo \u57fa\u51c6\u6d4b\u8bd5\u7a81\u663e\u4e86\u5bf9\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002"}}
{"id": "2508.11075", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11075", "abs": "https://arxiv.org/abs/2508.11075", "authors": ["Hyunwoo Yoo", "Gail Rosen"], "title": "Abundance-Aware Set Transformer for Microbiome Sample Embedding", "comment": null, "summary": "Microbiome sample representation to input into LLMs is essential for\ndownstream tasks such as phenotype prediction and environmental classification.\nWhile prior studies have explored embedding-based representations of each\nmicrobiome sample, most rely on simple averaging over sequence embeddings,\noften overlooking the biological importance of taxa abundance. In this work, we\npropose an abundance-aware variant of the Set Transformer to construct\nfixed-size sample-level embeddings by weighting sequence embeddings according\nto their relative abundance. Without modifying the model architecture, we\nreplicate embedding vectors proportional to their abundance and apply\nself-attention-based aggregation. Our method outperforms average pooling and\nunweighted Set Transformers on real-world microbiome classification tasks,\nachieving perfect performance in some cases. These results demonstrate the\nutility of abundance-aware aggregation for robust and biologically informed\nmicrobiome representation. To the best of our knowledge, this is one of the\nfirst approaches to integrate sequence-level abundance into Transformer-based\nsample embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u7c7b\u7fa4\u4e30\u5ea6\u7684\u96c6\u5408Transformer\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5fae\u751f\u7269\u6837\u672c\u7684\u5d4c\u5165\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u5fae\u751f\u7269\u6837\u672c\u8868\u793a\u65b9\u6cd5\uff08\u5982\u7b80\u5355\u5e73\u5747\u5e8f\u5217\u5d4c\u5165\uff09\u5ffd\u7565\u4e86\u7c7b\u7fa4\u4e30\u5ea6\u7684\u751f\u7269\u5b66\u91cd\u8981\u6027\uff0c\u9650\u5236\u4e86\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8868\u578b\u9884\u6d4b\u548c\u73af\u5883\u5206\u7c7b\uff09\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e30\u5ea6\u7684\u96c6\u5408Transformer\u53d8\u4f53\uff0c\u901a\u8fc7\u6839\u636e\u76f8\u5bf9\u4e30\u5ea6\u5bf9\u5e8f\u5217\u5d4c\u5165\u8fdb\u884c\u52a0\u6743\u6765\u6784\u5efa\u56fa\u5b9a\u5927\u5c0f\u7684\u6837\u672c\u7ea7\u5d4c\u5165\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u805a\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5fae\u751f\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5e73\u5747\u6c60\u5316\u548c\u672a\u52a0\u6743\u7684\u96c6\u5408Transformer\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u5b8c\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5e8f\u5217\u7ea7\u4e30\u5ea6\u4fe1\u606f\u5230Transformer\u7684\u6837\u672c\u5d4c\u5165\u4e2d\uff0c\u4e3a\u5fae\u751f\u7269\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u8868\u793a\uff0c\u5e76\u5728\u5b9e\u9645\u7684\u5fae\u751f\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5e73\u5747\u6c60\u5316\u548c\u672a\u52a0\u6743\u96c6\u5408Transformer\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11459", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11459", "abs": "https://arxiv.org/abs/2508.11459", "authors": ["Tzu-Chi Liu", "Po-Lin Chen", "Yi-Chieh Chen", "Po-Hsun Tu", "Chih-Hua Yeh", "Mun-Chun Yeap", "Chiung-Chu Chen", "Hau-Tieng Wu"], "title": "Efficient Artifacts Removal for Adaptive Deep Brain Stimulation and a Temporal Event Localization Analysis", "comment": "This manuscript is under review at Journal of Neural Engineering", "summary": "Adaptive deep brain stimulation (aDBS) leverages symptom-related biomarkers\nto deliver personalized neuromodulation therapy, with the potential to improve\ntreatment efficacy and reduce power consumption compared to conventional DBS.\nHowever, stimulation-induced signal contamination remains a major technical\nbarrier to advancing its clinical application. Existing artifact removal\nstrategies, both front-end and back-end, face trade-offs between artifact\nsuppression and algorithmic flexibility. Among back-end algorithms, Shrinkage\nand Manifold-based Artifact Removal using Template Adaptation (SMARTA) has\nshown promising performance in mitigating stimulus artifacts with minimal\ndistortion to local field potentials (LFPs), but its high computational demand\nand inability to handle transient direct current (DC) artifacts limit its use\nin real-time applications. To address this, we developed SMARTA+, a\ncomputationally efficient extension of SMARTA capable of suppressing both\nstimulus and transient DC artifacts while supporting flexible algorithmic\ndesign. We evaluated SMARTA+ using semi-real aDBS data and real data from\nParkinson's disease patients. Compared to SMARTA and other established methods,\nSMARTA+ achieved comparable or superior artifact removal while significantly\nreducing computation time. It preserved spectral and temporal structures,\nranging from beta band to high-frequency oscillations, and demonstrated\nrobustness across diverse stimulation protocols. Temporal event localization\nanalysis further showed improved accuracy in detecting beta bursts. These\nfindings support SMARTA+ as a promising tool for advancing real-time,\nclosed-loop aDBS systems.", "AI": {"tldr": "SMARTA+\u662fSMARTA\u7684\u8ba1\u7b97\u4f18\u5316\u7248\u672c\uff0c\u53ef\u6709\u6548\u53bb\u9664aDBS\u4e2d\u7684\u523a\u6fc0\u548c\u77ac\u6001\u76f4\u6d41\u4f2a\u5f71\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u4f2a\u5f71\u53bb\u9664\u7b56\u7565\u5728\u4f2a\u5f71\u6291\u5236\u548c\u7b97\u6cd5\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5e76\u4e14SMARTA\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u65e0\u6cd5\u5904\u7406\u77ac\u6001\u76f4\u6d41\u4f2a\u5f71\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684SMARTA\u6269\u5c55\uff0c\u79f0\u4e3aSMARTA+\uff0c\u5b83\u80fd\u591f\u6291\u5236\u523a\u6fc0\u548c\u77ac\u6001\u76f4\u6d41\u4f2a\u5f71\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u7684\u7b97\u6cd5\u8bbe\u8ba1\u3002", "result": "SMARTA+\u5728\u4f2a\u5f71\u53bb\u9664\u65b9\u9762\u4e0eSMARTA\u548c\u5176\u4ed6\u5df2\u5efa\u7acb\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u80fd\u5728\u5404\u79cd\u523a\u6fc0\u534f\u8bae\u4e2d\u4fdd\u6301\u5149\u8c31\u548c\u65f6\u95f4\u7ed3\u6784\u3002", "conclusion": "SMARTA+\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u63a8\u8fdb\u5b9e\u65f6\u95ed\u73afaDBS\u7cfb\u7edf\u3002"}}
{"id": "2508.11432", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11432", "abs": "https://arxiv.org/abs/2508.11432", "authors": ["Muhammad Zakwan", "Liang Xu", "Giancarlo Ferrari-Trecate"], "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization", "comment": "Accepted in IEEE CDC2025, Rio de Janeiro, Brazil", "summary": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.", "AI": {"tldr": "This paper uses contraction theory to make neural networks (NODEs) more robust to noise and attacks by ensuring similar inputs lead to similar outputs, verified on image classification tasks.", "motivation": "Neural networks are fragile to input noise and adversarial attacks. This work aims to improve the robustness of continuous-depth neural networks (NODEs) by leveraging contraction theory.", "method": "We propose to use contraction theory to improve the robustness of Convolutional Neural Ordinary Differential Equations (NODEs). Contractivity is induced during training using a regularization term involving the Jacobian of the system dynamics or through weight regularization terms for NODEs with slope-restricted activation functions.", "result": "Contractive Convolutional NODEs exhibit increased robustness because slight perturbations in features do not lead to significant changes in the output. The effectiveness of the proposed regularizers was shown on image classification tasks with corrupted images.", "conclusion": "The proposed regularization terms improve the robustness of Contractive Convolutional NODEs against input noise and adversarial attacks, as demonstrated on MNIST and FashionMNIST datasets."}}
{"id": "2508.11311", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.11311", "abs": "https://arxiv.org/abs/2508.11311", "authors": ["Jiahao Joel Fan", "Feihong Liu", "Dangyuan Lei", "Zhedong Zhang"], "title": "Quantum-Enhanced Sensing of Excited-State Dynamics with Correlated Photons", "comment": "7 pages, 3 figures", "summary": "The squeezed photons, as a quantum-correlated light with reduced noise, have\nemerged as a great resource for sensing the structures of matter. Here we study\nthe transient absorption (TA) scheme using the squeezed photons whose spectral\ncorrelation of amplitudes can be tailored. A microscopic theory is developed,\nrevealing a highly time-energy-resolved nature of the signal that is not\nattainable by conventional TA scheme. Such a capability is elaborated by\napplying to monolayer transition metal dichalcogenide materials (TMDs),\nachieving a real-time monitoring of valley excitons and their dynamics.\nMoreover, we show the intermediate squeezing regime-not the strong\nsqueezing-which the time-resolved spectroscopy is in favor of. Our work offers\na new paradigm for studying nonequilibrium dynamics of matter, in light of the\nphotocatalysis and optoelectronics.", "AI": {"tldr": "\u5229\u7528\u88ab\u538b\u7f29\u5149\u5b50\u8fdb\u884c\u77ac\u6001\u5438\u6536\u5149\u8c31\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7269\u8d28\uff08\u7279\u522b\u662f\u5355\u5c42\u8fc7\u6e21\u91d1\u5c5e\u786b\u65cf\u5316\u7269\u6750\u6599\uff09\u8c37\u6fc0\u5b50\u53ca\u5176\u52a8\u529b\u5b66\u7684\u5b9e\u65f6\u76d1\u6d4b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u88ab\u538b\u7f29\u5149\u5b50\uff08\u4e00\u79cd\u5177\u6709\u964d\u4f4e\u566a\u58f0\u7684\u91cf\u5b50\u5173\u8054\u5149\uff09\u4f5c\u4e3a\u4f20\u611f\u7269\u8d28\u7ed3\u6784\u7684\u8d44\u6e90\u3002", "method": "\u5229\u7528\u5149\u8c31\u76f8\u5173\u7684\u88ab\u538b\u7f29\u5149\u5b50\uff0c\u63d0\u51fa\u4e00\u79cd\u77ac\u6001\u5438\u6536\uff08TA\uff09\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u5fae\u89c2\u7406\u8bba\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u6240\u63d0\u51fa\u7684TA\u65b9\u6848\u5177\u6709\u9ad8\u5ea6\u7684\u65f6\u95f4-\u80fd\u91cf\u5206\u8fa8\u7279\u6027\uff0c\u53ef\u5b9e\u73b0\u5bf9\u8c37\u6fc0\u5b50\u53ca\u5176\u52a8\u529b\u5b66\u7684\u5b9e\u65f6\u76d1\u6d4b\uff0c\u5e76\u4e14\u4e2d\u95f4\u538b\u7f29\u6bd4\u7684\u88ab\u538b\u7f29\u5149\u5b50\u66f4\u9002\u7528\u4e8e\u65f6\u95f4\u5206\u8fa8\u5149\u8c31\u5b66\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u7269\u8d28\u7684\u975e\u5e73\u8861\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\uff0c\u53ef\u5e94\u7528\u4e8e\u5149\u50ac\u5316\u548c\u5149\u7535\u5b50\u5b66\u9886\u57df\u3002"}}
{"id": "2508.11286", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u77bb\u6027\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u573a\u666f\u56fe\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u6f5c\u5728\u7684\u6267\u884c\u5931\u8d25\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u8bb8\u591a\u81ea\u4e3b\u673a\u5668\u4eba\u7f3a\u4e4f\u9002\u5e94\u6027\u611f\u77e5\u80fd\u529b\uff0c\u7ecf\u5e38\u9075\u5faa\u9884\u5148\u89c4\u5212\u7684\u52a8\u4f5c\uff0c\u53ef\u80fd\u5ffd\u7565\u573a\u666f\u4e2d\u7684\u7ec6\u5fae\u4f46\u5173\u952e\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u57fa\u4e8e\u8fc7\u65f6\u5047\u8bbe\u6267\u884c\u52a8\u4f5c\u5e76\u6700\u7ec8\u5bfc\u81f4\u5931\u8d25\u3002\u867d\u7136\u91cd\u65b0\u89c4\u5212\u5bf9\u4e8e\u9c81\u68d2\u81ea\u4e3b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u6545\u969c\u53d1\u751f\u540e\u624d\u54cd\u5e94\uff0c\u800c\u524d\u77bb\u6027\u91cd\u65b0\u89c4\u5212\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u8bbe\u8ba1\u7684\u89c4\u5219\u548c\u5e7f\u6cdb\u7684\u76d1\u7763\u6765\u63d0\u524d\u9632\u6b62\u6545\u969c\u3002", "method": "\u901a\u8fc7\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u6bd4\u8f83\u5f53\u524dRGB-D\u89c2\u6d4b\u6784\u5efa\u7684\u573a\u666f\u56fe\u4e0e\u4ece\u6210\u529f\u6f14\u793a\u4e2d\u63d0\u53d6\u7684\u53c2\u8003\u56fe\uff0c\u5e76\u5728\u4e0d\u5339\u914d\u65f6\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u5757\u6765\u8bca\u65ad\u548c\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6267\u884c\u5931\u8d25\u53d1\u751f\u4e4b\u524d\u68c0\u6d4b\u5230\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u548c\u7ea0\u6b63\u6545\u969c\uff0c\u5e76\u5c06\u5f53\u524dRGB-D\u89c2\u6d4b\u6784\u5efa\u7684\u573a\u666f\u56fe\u4e0e\u4ece\u6210\u529f\u6f14\u793a\u4e2d\u63d0\u53d6\u7684\u53c2\u8003\u56fe\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b9e\u73b0\u4e86\u524d\u77bb\u6027\u91cd\u65b0\u89c4\u5212\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10946", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10946", "abs": "https://arxiv.org/abs/2508.10946", "authors": ["Wonho Lee", "Hyunsik Na", "Jisu Lee", "Daeseon Choi"], "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training", "comment": null, "summary": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments.", "AI": {"tldr": "IPG \u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u751f\u6210\u5bf9\u6297\u6027\u8d34\u7247\uff0c\u63d0\u9ad8\u4e86 AI \u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u5b89\u5168\u548c\u533b\u7597\u6210\u50cf\u7b49\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u5bf9\u6297\u6027\u8d34\u7247\u5bf9\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08\u5c24\u5176\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5bf9\u8c61\u68c0\u6d4b\uff09\u7684\u9c81\u68d2\u6027\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e0e\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u4e0d\u540c\uff0c\u8fd9\u4e9b\u8d34\u7247\u4f1a\u9488\u5bf9\u56fe\u50cf\u7684\u7279\u5b9a\u533a\u57df\uff0c\u5bfc\u81f4\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u53d1\u751f\u6545\u969c\u3002", "method": "\u63d0\u51fa\u589e\u91cf\u8d34\u7247\u751f\u6210\uff08IPG\uff09\u65b9\u6cd5\uff0c\u4ee5\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa 11.1 \u500d\u7684\u6548\u7387\u751f\u6210\u5bf9\u6297\u6027\u8d34\u7247\u3002", "result": "\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff08\u5305\u62ec YOLO \u7684\u7279\u5f81\u5206\u5e03\u53ef\u89c6\u5316\u548c\u5bf9\u6297\u6027\u8bad\u7ec3\u7ed3\u679c\uff09\u8bc1\u660e\u4e86 IPG \u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5b83\u80fd\u751f\u6210\u6cdb\u5316\u6027\u597d\u4e14\u80fd\u6709\u6548\u8986\u76d6\u66f4\u5e7f\u6cdb\u6a21\u578b\u6f0f\u6d1e\u7684\u8d34\u7247\u3002", "conclusion": "\u589e\u91cf\u8d34\u7247\u751f\u6210\uff08IPG\uff09\u5728\u4fdd\u6301\u53ef\u6bd4\u7684\u653b\u51fb\u6027\u80fd\u7684\u540c\u65f6\uff0c\u751f\u6210\u5bf9\u6297\u6027\u8d34\u7247\u7684\u6548\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa 11.1 \u500d\u3002IPG \u751f\u6210\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u4f5c\u4e3a\u6784\u5efa\u7a33\u5065\u6a21\u578b\u3001\u5b9e\u73b0\u7ed3\u6784\u5316\u8868\u793a\u3001\u9ad8\u7ea7\u63a8\u7406\u548c\u4e3b\u52a8\u9632\u5fa1\u7684\u7a33\u5065\u77e5\u8bc6\u57fa\u7840\u3002"}}
{"id": "2508.11163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11163", "abs": "https://arxiv.org/abs/2508.11163", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "AI": {"tldr": "MobQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u65b9\u9762\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u6570\u636e\u96c6\u3002\u867d\u7136\u6a21\u578b\u5728\u4e8b\u5b9e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8f68\u8ff9\u957f\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u7406\u89e3\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u65b9\u9762\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u867d\u7136\u80fd\u9884\u6d4b\u79fb\u52a8\u6a21\u5f0f\uff0c\u4f46\u5bf9\u5176\u5e95\u5c42\u539f\u56e0\u6216\u8bed\u4e49\u542b\u4e49\u7684\u7406\u89e3\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMobQA\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5800\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u4e8b\u5b9e\u68c0\u7d22\u3001\u591a\u9879\u9009\u62e9\u63a8\u7406\u548c\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u4e09\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u5728\u5bf9\u4e3b\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\u5728\u4e8b\u5b9e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u6027\u95ee\u7b54\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u8f68\u8ff9\u957f\u5ea6\u5bf9\u6a21\u578b\u6709\u6548\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\u7684\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8bed\u4e49\u63a8\u7406\u7684\u89e3\u91ca\u6027\u95ee\u9898\u4e0a\u3002\u6a21\u578b\u5728\u4e8b\u5b9e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u63a8\u7406\u548c\u95ee\u7b54\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e14\u8f68\u8ff9\u957f\u5ea6\u4f1a\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.11084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11084", "abs": "https://arxiv.org/abs/2508.11084", "authors": ["Thanasis Schoinas", "Ghulam Qadir"], "title": "A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora", "comment": null, "summary": "Predictive coding, the term used in the legal industry for document\nclassification using machine learning, presents additional challenges when the\ndataset comprises instant messages, due to their informal nature and smaller\nsizes. In this paper, we exploit a data management workflow to group messages\ninto day chats, followed by feature selection and a logistic regression\nclassifier to provide an economically feasible predictive coding solution. We\nalso improve the solution's baseline model performance by dimensionality\nreduction, with focus on quantitative features. We test our methodology on an\nInstant Bloomberg dataset, rich in quantitative information. In parallel, we\nprovide an example of the cost savings of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u53ef\u884c\u7684\u9884\u6d4b\u7f16\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5bf9\u5373\u65f6\u6d88\u606f\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u6570\u636e\u7ba1\u7406\u3001\u7279\u5f81\u9009\u62e9\u548c\u903b\u8f91\u56de\u5f52\uff0c\u5e76\u4f7f\u7528\u964d\u7ef4\u6280\u672f\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5373\u65f6\u6d88\u606f\u7684\u975e\u6b63\u5f0f\u6027\u8d28\u548c\u8f83\u5c0f\u7684\u5c3a\u5bf8\uff0c\u5728\u5305\u542b\u5373\u65f6\u6d88\u606f\u7684\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u6587\u6863\u5206\u7c7b\uff08\u5373\u9884\u6d4b\u7f16\u7801\uff09\u9762\u4e34\u989d\u5916\u7684\u6311\u6218\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u7ba1\u7406\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5bf9\u5373\u65f6\u6d88\u606f\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u548c\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u3002\u901a\u8fc7\u964d\u7ef4\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6587\u7ae0\u5728\u5305\u542b\u4e30\u5bcc\u5b9a\u91cf\u4fe1\u606f\u7684Instant Bloomberg\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6210\u672c\u8282\u7ea6\u7684\u793a\u4f8b\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u7ba1\u7406\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5c06\u6d88\u606f\u5206\u7ec4\u5230\u6bcf\u65e5\u804a\u5929\u4e2d\uff0c\u7136\u540e\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u548c\u903b\u8f91\u56de\u5f52\u5206\u7c7b\uff0c\u4e3a\u5373\u65f6\u6d88\u606f\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u53ef\u884c\u7684\u9884\u6d4b\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u964d\u7ef4\uff08\u7279\u522b\u5173\u6ce8\u5b9a\u91cf\u7279\u5f81\uff09\u6765\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.11473", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11473", "abs": "https://arxiv.org/abs/2508.11473", "authors": ["Yuqin Liu", "Mona Jaber", "Yan Liu", "Arumugam Nallanathan"], "title": "Reducing AoI and Improving Throughput for NOMA-assisted SGF Systems: A Hierarchical Learning Approach", "comment": null, "summary": "A non-orthogonal multiple access (NOMA) assisted semi-grant-free (SGF)\nframework is proposed to enable channel access for grant-free users (GFUs) by\nusing residual resources from grant-based users. Under this framework, the\nproblem of joint beamforming design and transmission scheduling is formulated\nto improve the system throughput and reduce the age-of-information of GFUs. The\naforementioned problem is transferred into a Markov Decision Process to model\nthe changing environment with the transmission/ waiting/ retransmission of\nGFUs. In an effort to solve the pertinent problem, firstly, a deep\nreinforcement learning (DRL) based transmission scheduling approach is proposed\nfor determining the optimal transmission probability based on the available\ntransmission slots and transmission status of GFUs. Secondly, a hierarchical\nlearning algorithm is proposed to analyze the channel state information of GBUs\nand the transmission status of GFUs, and to train an upper-level policy based\non this analysis for beamforming to achieve efficient grant-based transmission,\nwhile a lower-level policy adapts to maximize the utilization of transmission\nslots allocated by the upper-level agent. The two policies interact to improve\nchannel access and avoid collisions. Numerical results reveal that 1) The DRL\nbased transmission scheduling outperforms existing adaptive and state-dependent\nbaselines in AoI reduction, where an average\nthree-time-slots-earlier-transmission can be obtained compared to the\nstate-dependent choice, and five time slots earlier can be achieved when\ncomparing to the adaptive choice; 2) The hierarchical learning algorithm is\nable to achieve approximately a 31.82% gain while maintaining the average AoI\nof GFUs within 1.5 time slots. 3) The effectiveness of the hierarchical\nlearning scheme in NOMA-assisted SGF system is validated across scenarios with\nGFUs counts from 1-5 times of GBUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdNOMA\u8f85\u52a9SGF\u6846\u67b6\uff0c\u901a\u8fc7DRL\u548c\u5206\u5c42\u5b66\u4e60\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u4f20\u8f93\u8c03\u5ea6\uff0c\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u964d\u4f4eGFU\u7684AoI\u3002", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u5229\u7528\u6765\u81ea\u57fa\u4e8e\u6388\u6743\u7684\u7528\u6237\uff08GBUs\uff09\u7684\u5269\u4f59\u8d44\u6e90\u6765\u4e3a\u6388\u6743\u7528\u6237\uff08GFUs\uff09\u5b9e\u73b0\u4fe1\u9053\u63a5\u5165\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u8f85\u52a9\u7684\u534a\u6388\u6743\uff08SGF\uff09\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u8054\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u548c\u4f20\u8f93\u8c03\u5ea6\u6765\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u5e76\u964d\u4f4eGFUs\u7684\u5e74\u9f84\u4fe1\u606f\uff08AoI\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u4f20\u8f93\u8c03\u5ea6\u65b9\u6cd5\u6765\u786e\u5b9a\u57fa\u4e8e\u53ef\u7528\u4f20\u8f93\u65f6\u9699\u548cGFU\u4f20\u8f93\u72b6\u6001\u7684\u6700\u4f18\u4f20\u8f93\u6982\u7387\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\u6765\u5206\u6790GBU\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548cGFU\u7684\u4f20\u8f93\u72b6\u6001\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u6790\u8bad\u7ec3\u4e00\u4e2a\u7528\u4e8e\u6ce2\u675f\u6210\u5f62\u7684\u4e0a\u5c42\u7b56\u7565\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u57fa\u4e8e\u6388\u6743\u7684\u4f20\u8f93\uff0c\u540c\u65f6\u4e00\u4e2a\u4e0b\u5c42\u7b56\u7565\u9002\u5e94\u4ee5\u6700\u5927\u5316\u5229\u7528\u4e0a\u5c42\u4ee3\u7406\u5206\u914d\u7684\u4f20\u8f93\u65f6\u9699\u3002", "result": "DRL\u4f20\u8f93\u8c03\u5ea6\u65b9\u6cd5\u5728AoI\u7f29\u51cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u72b6\u6001\u4f9d\u8d56\u9009\u62e9\u5e73\u5747\u53ef\u4ee5\u63d0\u524d\u4e09\u500d\u65f6\u9699\u4f20\u8f93\uff0c\u76f8\u6bd4\u4e8e\u81ea\u9002\u5e94\u9009\u62e9\u5e73\u5747\u53ef\u4ee5\u63d0\u524d\u4e94\u500d\u65f6\u9699\u4f20\u8f93\u3002\u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\u5728\u5c06GFUs\u7684\u5e73\u5747AoI\u4fdd\u6301\u57281.5\u500d\u65f6\u9699\u5185\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u7ea631.82%\u7684\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u5206\u5c42\u5b66\u4e60\u65b9\u6848\u5728NOMA\u8f85\u52a9SGF\u7cfb\u7edf\u4e2d\u5bf9\u4e8e1-5\u500dGBU\u6570\u91cf\u7684GFU\u573a\u666f\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11520", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11520", "abs": "https://arxiv.org/abs/2508.11520", "authors": ["Evangelos Tsiatsianas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning", "comment": "8 pages, 2 figures, 4 tables, Accepted at Humanoids 2025", "summary": "Automatically generating agile whole-body motions for legged and humanoid\nrobots remains a fundamental challenge in robotics. While numerous trajectory\noptimization approaches have been proposed, there is no clear guideline on how\nthe choice of floating-base space parameterization affects performance,\nespecially for agile behaviors involving complex contact dynamics. In this\npaper, we present a comparative study of different parameterizations for direct\ntranscription-based trajectory optimization of agile motions in legged systems.\nWe systematically evaluate several common choices under identical optimization\nsettings to ensure a fair comparison. Furthermore, we introduce a novel\nformulation based on the tangent space of SE(3) for representing the robot's\nfloating-base pose, which, to our knowledge, has not received attention from\nthe literature. This approach enables the use of mature off-the-shelf numerical\nsolvers without requiring specialized manifold optimization techniques. We hope\nthat our experiments and analysis will provide meaningful insights for\nselecting the appropriate floating-based representation for agile whole-body\nmotion generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u673a\u5668\u4eba\u654f\u6377\u8fd0\u52a8\u8f68\u8ff9\u4f18\u5316\u4e2d\u4e0d\u540c\u6d6e\u52a8\u57fa\u7ebf\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eSE(3)\u5207\u7ebf\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u7b80\u5316\u6c42\u89e3\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\uff0c\u5982\u4f55\u9009\u62e9\u6d6e\u52a8\u57fa\u7ebf\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u590d\u6742\u63a5\u89e6\u52a8\u529b\u5b66\u7684\u654f\u6377\u884c\u4e3a\u4e2d\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u5bf9\u76f4\u63a5\u8f6c\u5f55\u7684\u8f68\u8ff9\u4f18\u5316\u8fdb\u884c\u4e0d\u540c\u53c2\u6570\u5316\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7ebf\u7a7a\u95f4\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u6211\u4eec\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u51e0\u79cd\u5e38\u89c1\u7684\u9009\u62e9\uff0c\u5e76\u5728\u76f8\u540c\u7684\u4f18\u5316\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u516c\u5e73\u7684\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7ebf\u7a7a\u95f4\u7684\u65b0\u9896\u516c\u5f0f\uff0c\u7528\u4e8e\u8868\u793a\u673a\u5668\u4eba\u6d6e\u52a8\u57fa\u7ebf\u7684\u59ff\u6001\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u4f7f\u7528\u6210\u719f\u7684\u73b0\u6210\u6570\u503c\u6c42\u89e3\u5668\uff0c\u800c\u65e0\u9700\u4e13\u95e8\u7684\u6d41\u5f62\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u654f\u6377\u8fd0\u52a8\u7684\u6d6e\u52a8\u57fa\u7ebf\u8868\u793a\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u89c1\u89e3\uff0c\u4ee5\u6307\u5bfc\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u8fdb\u884c\u9009\u62e9\u3002"}}
{"id": "2508.11333", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11333", "abs": "https://arxiv.org/abs/2508.11333", "authors": ["Vigneshwar B", "Sankaranarayanan R"], "title": "Noise Resilience of Spin Quantum Battery in the presence of DM Interactions", "comment": null, "summary": "Quantum batteries utilize quantum effects to enhance energy storage and work\nextraction, offering promising avenues for nanoscale energy applications.\nHowever, environmental noise poses a significant challenge by degrading stored\nenergy. For a single qubit, we show that amplitude damping and bit-flip noises\nlead to ergotropy loss, while phase-flip noise permits partial preservation of\nwork. Extending to a two-qubit Heisenberg XYZ model with Dzyaloshinsky-Moria\ninteraction (DMI), we identify a critical interaction strength that enhances\nenergy retention. We show that strong DMI and initial coherence protects\nergotropy even under repeated noise applications, highlighting chiral spin\ninteractions as a resource for noise-resilient quantum batteries.", "AI": {"tldr": "\u91cf\u5b50\u7535\u6c60\u7684\u6297\u566a\u58f0\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u624b\u5f81\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\uff08\u5982DMI\uff09\u6765\u63d0\u9ad8\uff0c\u7279\u522b\u662f\u5f53DMI\u5f3a\u5ea6\u8f83\u5f3a\u4e14\u521d\u59cb\u76f8\u5e72\u6027\u8f83\u9ad8\u65f6\u3002", "motivation": "\u73af\u5883\u566a\u58f0\u4f1a\u964d\u4f4e\u91cf\u5b50\u7535\u6c60\u5b58\u50a8\u7684\u80fd\u91cf\uff0c\u7814\u7a76\u624b\u5f81\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u63d0\u9ad8\u91cf\u5b50\u7535\u6c60\u7684\u6297\u566a\u58f0\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e86\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u53cc\u91cf\u5b50\u6bd4\u7279Heisenberg XYZ\u6a21\u578b\uff0c\u5e76\u8003\u8651\u4e86\u632f\u5e45\u963b\u5c3c\u3001\u6bd4\u7279\u7ffb\u8f6c\u548c\u76f8\u4f4d\u7ffb\u8f6c\u566a\u58f0\uff0c\u4ee5\u53caDzyaloshinsky-Moria\u76f8\u4e92\u4f5c\u7528\uff08DMI\uff09\u3002", "result": "\u5728\u5355\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u632f\u5e45\u963b\u5c3c\u548c\u6bd4\u7279\u7ffb\u8f6c\u566a\u58f0\u5bfc\u81f4\u80fd\u91cf\u635f\u5931\uff0c\u800c\u76f8\u4f4d\u7ffb\u8f6c\u566a\u58f0\u5141\u8bb8\u90e8\u5206\u80fd\u91cf\u4fdd\u5b58\u3002\u5728\u53cc\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u53d1\u73b0\u4e34\u754c\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u53ef\u4ee5\u589e\u5f3a\u80fd\u91cf\u4fdd\u6301\u80fd\u529b\uff0c\u5f3aDMI\u548c\u521d\u59cb\u76f8\u5e72\u6027\u53ef\u4ee5\u62b5\u6297\u566a\u58f0\u3002", "conclusion": "\u91cf\u5b50\u7535\u6c60\u7684\u80fd\u91cf\u4fdd\u6301\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u624b\u5f81\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\u6765\u589e\u5f3a\uff0c\u5373\u4f7f\u5728\u91cd\u590d\u7684\u566a\u58f0\u4f5c\u7528\u4e0b\u4e5f\u80fd\u4fdd\u62a4\u529f\u3002"}}
{"id": "2508.11289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11289", "abs": "https://arxiv.org/abs/2508.11289", "authors": ["Lin Li", "Xueming Liu", "Zhoujingzi Qiu", "Tianjiang Hu", "Qingrui Zhang"], "title": "A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 6 Pages", "summary": "Bearing-only Target Motion Analysis (TMA) is a promising technique for\npassive tracking in various applications as a bearing angle is easy to measure.\nDespite its advantages, bearing-only TMA is challenging due to the nonlinearity\nof the bearing measurement model and the lack of range information, which\nimpairs observability and estimator convergence. This paper addresses these\nissues by proposing a Recursive Total Least Squares (RTLS) method for online\ntarget localization and tracking using mobile observers. The RTLS approach,\ninspired by previous results on Total Least Squares (TLS), mitigates biases in\nposition estimation and improves computational efficiency compared to\npseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a\ncircumnavigation controller to enhance system observability and estimator\nconvergence by guiding the mobile observer in orbit around the target.\nExtensive simulations and experiments are performed to demonstrate the\neffectiveness and robustness of the proposed method. The proposed algorithm is\nalso compared with the state-of-the-art approaches, which confirms its superior\nperformance in terms of both accuracy and stability.", "AI": {"tldr": "This paper presents a new Recursive Total Least Squares (RTLS) method and a circumnavigation controller for passive tracking using bearing-only measurements. The new method improves accuracy and stability, outperforming current techniques.", "motivation": "Bearing-only Target Motion Analysis (TMA) is challenging due to the nonlinearity of bearing measurements and the lack of range information, which hinders observability and estimator convergence. This paper aims to address these challenges by improving accuracy and stability in target tracking.", "method": "The paper proposes a Recursive Total Least Squares (RTLS) method for bearing-only Target Motion Analysis (TMA) and introduces a circumnavigation controller to improve system observability and estimator convergence. The RTLS approach is designed to mitigate biases in position estimation and improve computational efficiency compared to pseudo-linear Kalman filter (PLKF) methods.", "result": "The proposed RTLS method and circumnavigation controller demonstrate effectiveness and robustness through extensive simulations and experiments, showing superior performance in accuracy and stability compared to existing methods.", "conclusion": "The proposed Recursive Total Least Squares (RTLS) method enhances online target localization and tracking for bearing-only TMA, outperforming state-of-the-art approaches in accuracy and stability. The addition of a circumnavigation controller further improves system observability and estimator convergence."}}
{"id": "2508.10947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10947", "abs": "https://arxiv.org/abs/2508.10947", "authors": ["Ronghao Xu", "Zhen Huang", "Yangbo Wei", "Xiaoqian Zhou", "Zikang Xu", "Ting Liu", "Zihang Jiang", "S. Kevin Zhou"], "title": "MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text", "comment": null, "summary": "Artificial intelligence has demonstrated significant potential in clinical\ndecision-making; however, developing models capable of adapting to diverse\nreal-world scenarios and performing complex diagnostic reasoning remains a\nmajor challenge. Existing medical multi-modal benchmarks are typically limited\nto single-image, single-turn tasks, lacking multi-modal medical image\nintegration and failing to capture the longitudinal and multi-modal interactive\nnature inherent to clinical practice. To address this gap, we introduce\nMedAtlas, a novel benchmark framework designed to evaluate large language\nmodels on realistic medical reasoning tasks. MedAtlas is characterized by four\nkey features: multi-turn dialogue, multi-modal medical image interaction,\nmulti-task integration, and high clinical fidelity. It supports four core\ntasks: open-ended multi-turn question answering, closed-ended multi-turn\nquestion answering, multi-image joint reasoning, and comprehensive disease\ndiagnosis. Each case is derived from real diagnostic workflows and incorporates\ntemporal interactions between textual medical histories and multiple imaging\nmodalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to\nperform deep integrative reasoning across images and clinical texts. MedAtlas\nprovides expert-annotated gold standards for all tasks. Furthermore, we propose\ntwo novel evaluation metrics: Round Chain Accuracy and Error Propagation\nResistance. Benchmark results with existing multi-modal models reveal\nsubstantial performance gaps in multi-stage clinical reasoning. MedAtlas\nestablishes a challenging evaluation platform to advance the development of\nrobust and trustworthy medical AI.", "AI": {"tldr": "MedAtlas is a new benchmark for evaluating AI in medical diagnosis, featuring realistic multi-turn conversations and various medical images (CT, MRI, etc.). Current AI models show significant weaknesses in complex clinical reasoning tasks evaluated by this benchmark, which aims to improve trustworthy medical AI.", "motivation": "Existing medical multi-modal benchmarks are limited to single-image, single-turn tasks, lacking multi-modal medical image integration and failing to capture the longitudinal and multi-modal interactive nature inherent to clinical practice. This gap hinders the development of AI models capable of adapting to diverse real-world scenarios and performing complex diagnostic reasoning.", "method": "MedAtlas is a novel benchmark framework designed to evaluate large language models on realistic medical reasoning tasks. It features multi-turn dialogue, multi-modal medical image interaction, multi-task integration, and high clinical fidelity. It supports four core tasks: open-ended multi-turn question answering, closed-ended multi-turn question answering, multi-image joint reasoning, and comprehensive disease diagnosis. Cases are derived from real diagnostic workflows and incorporate temporal interactions between textual medical histories and multiple imaging modalities (CT, MRI, PET, ultrasound, X-ray). The framework also proposes two novel evaluation metrics: Round Chain Accuracy and Error Propagation Resistance.", "result": "Benchmark results with existing multi-modal models reveal substantial performance gaps in multi-stage clinical reasoning. This indicates that current multi-modal models struggle with the complexities of realistic medical reasoning tasks as evaluated by MedAtlas.", "conclusion": "MedAtlas is a novel benchmark framework that addresses the limitations of existing medical multi-modal benchmarks by incorporating multi-turn dialogue, multi-modal medical image interaction, multi-task integration, and high clinical fidelity. It supports four core tasks and includes expert-annotated gold standards and new evaluation metrics. Benchmark results indicate substantial performance gaps in multi-stage clinical reasoning, highlighting MedAtlas's potential to advance the development of robust and trustworthy medical AI."}}
{"id": "2508.11166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11166", "abs": "https://arxiv.org/abs/2508.11166", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "AI": {"tldr": "Tulu\u8bed\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\uff1aBiGRU\u6a21\u578b\u4f18\u4e8eTransformer\uff0c\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5df2\u5efa\u7acb\u3002", "motivation": "Tulu\u8bed\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5728\u6570\u5b57\u9886\u57df\u65e5\u76ca\u589e\u957f\u4f46\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u7684NLP\u7814\u7a76\u5de5\u5177\u548c\u6570\u636e\u96c6\u6765\u5904\u7406\u5176\u4ee3\u7801\u6df7\u5408\u7684\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3,845\u6761YouTube\u8bc4\u8bba\u7684Tulu\u8bed\u4ee3\u7801\u6df7\u5408\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5bf9GRU\u3001LSTM\u3001BiGRU\u3001BiLSTM\u3001CNN\u3001\u6ce8\u610f\u529b\u673a\u5236\u6a21\u578b\u4ee5\u53camBERT\u3001XLM-RoBERTa\u7b49Transformer\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5e26\u6709\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684BiGRU\u6a21\u578b\u5728Tulu\u8bed\u4ee3\u7801\u6df7\u5408\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u4e8682%\u7684\u51c6\u786e\u7387\u548c0.81\u7684\u5b8f\u89c2F1\u5206\u6570\u3002Transformer\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u663e\u793a\u51fa\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u3001\u4ee3\u7801\u6df7\u5408\u8bed\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5370\u5ea6\u5357\u90e8\u4f4e\u8d44\u6e90\u591a\u8bed\u79cdTulu\u8bed\u8a00\u7684\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\uff08OLI\uff09\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5e26\u6709\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684BiGRU\uff09\u5728\u5904\u7406\u6df7\u5408\u7f16\u7801\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709Transformer\u6a21\u578b\u5728\u8be5\u7c7b\u4e0a\u4e0b\u6587\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.11086", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11086", "abs": "https://arxiv.org/abs/2508.11086", "authors": ["Emily Liu", "Kuan Han", "Minfeng Zhan", "Bocheng Zhao", "Guanyu Mu", "Yang Song"], "title": "Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation", "comment": null, "summary": "Watch time is widely used as a proxy for user satisfaction in video\nrecommendation platforms. However, raw watch times are influenced by\nconfounding factors such as video duration, popularity, and individual user\nbehaviors, potentially distorting preference signals and resulting in biased\nrecommendation models. We propose a novel relative advantage debiasing\nframework that corrects watch time by comparing it to empirically derived\nreference distributions conditioned on user and item groups. This approach\nyields a quantile-based preference signal and introduces a two-stage\narchitecture that explicitly separates distribution estimation from preference\nlearning. Additionally, we present distributional embeddings to efficiently\nparameterize watch-time quantiles without requiring online sampling or storage\nof historical data. Both offline and online experiments demonstrate significant\nimprovements in recommendation accuracy and robustness compared to existing\nbaseline methods.", "AI": {"tldr": "Watch time is a flawed proxy for user satisfaction in video recommendations. This paper proposes a framework to correct watch time by comparing it to reference distributions, leading to better recommendations. It also introduces a method to efficiently represent this corrected signal.", "motivation": "Raw watch times are influenced by confounding factors like video duration, popularity, and user behaviors, potentially distorting preference signals and leading to biased recommendation models.", "method": "A novel relative advantage debiasing framework that corrects watch time by comparing it to empirically derived reference distributions conditioned on user and item groups. A two-stage architecture separates distribution estimation from preference learning. Distributional embeddings parameterize watch-time quantiles without online sampling or storing historical data.", "result": "Significant improvements in recommendation accuracy and robustness compared to existing baseline methods, demonstrated through offline and online experiments.", "conclusion": "Watch time debiasing using relative advantage and distributional embeddings improves recommendation accuracy and robustness."}}
{"id": "2508.11489", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11489", "abs": "https://arxiv.org/abs/2508.11489", "authors": ["Bowu Wang", "Mohamadreza Delbari", "Robin Neuder", "Alejandro Jim\u00e9nez-S\u00e1ez", "Vahid Jamali"], "title": "Liquid Crystal-Based RIS Loss-Trade-Off Analysis", "comment": null, "summary": "Liquid crystal (LC) technology has emerged as a promising solution for large\nreconfigurable intelligent surfaces (RISs) at millimeter wave (mmWave) bands,\noffering advantages such as low power consumption, scalability, and\ncontinuously tunable phase shifts. For LC-RIS based on the delay-line\narchitecture, i.e., with dedicated phase shifters, there exists a trade-off\nbetween the maximum achievable phase-shift range and the corresponding\ninsertion loss, which has not been studied for LC-RIS-assisted wireless systems\nyet. In this paper, we investigate this trade-off where a base station (BS) and\nan RIS are configured to minimize the transmit power while satisfying a given\nquality of service (QoS) for a number of users. Simulation results reveal a\nfundamental trade-off between the total transmit power and the achievable data\nrate as a function of the LC phase-shift range.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6db2\u6676\u667a\u80fd\u8d85\u8868\u9762\uff08LC-RIS\uff09\u5728\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u76f8\u79fb\u8303\u56f4\u548c\u63d2\u5165\u635f\u8017\u7684\u6743\u8861\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5bf9\u53d1\u5c04\u529f\u7387\u548c\u6570\u636e\u901f\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4e86\u6db2\u6676\uff08LC\uff09RIS\u8f85\u52a9\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u6700\u5927\u76f8\u79fb\u8303\u56f4\u4e0e\u63d2\u5165\u635f\u8017\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u8be5\u6743\u8861\u5bf9\u4e8eLC-RIS\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6db2\u6676\u7684\u667a\u80fd\u8d85\u8868\u9762\uff08RIS\uff09\u7684\u7cfb\u7edf\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u6700\u5927\u76f8\u79fb\u8303\u56f4\u548c\u63d2\u5165\u635f\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u6700\u5c0f\u5316\u53d1\u5c04\u529f\u7387\u5e76\u6ee1\u8db3\u7528\u6237\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728LC-RIS\u8f85\u52a9\u65e0\u7ebf\u7cfb\u7edf\u4e2d\uff0c\u5b58\u5728\u53d1\u5c04\u529f\u7387\u4e0e\u6570\u636e\u901f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u8be5\u5173\u7cfb\u53d7LC\u76f8\u79fb\u8303\u56f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u63ed\u793a\u4e86\u603b\u53d1\u5c04\u529f\u7387\u4e0e\u53ef\u5b9e\u73b0\u6570\u636e\u901f\u7387\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6db2\u6676\u7684\u76f8\u79fb\u8303\u56f4\u3002"}}
{"id": "2508.11547", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11547", "abs": "https://arxiv.org/abs/2508.11547", "authors": ["Martin Jirou\u0161ek", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads", "comment": null, "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u6807\u51c6\u673a\u8f7d\u4f20\u611f\u5668\uff08GPS\u548cIMU\uff09\u6765\u8ddf\u8e2a\u65e0\u4eba\u673a\u60ac\u7d22\u540a\u8231\u4f4d\u7f6e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u73b0\u573a\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u643a\u5e26\u7684\u60ac\u7d22\u540a\u8231\u7684\u8ddf\u8e2a\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u9645\u90e8\u7f72\u548c\u6700\u5c11\u786c\u4ef6\u9700\u6c42\uff0c\u4e0e\u4f9d\u8d56\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\u3001\u989d\u5916\u673a\u8f7d\u6444\u50cf\u5934\u6216\u4eea\u5668\u5316\u540a\u8231\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u5b9e\u65f6\u8fd0\u52a8\u5b66\u5168\u5c40\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\u6d4b\u91cf\u548c\u673a\u8f7d\u60ef\u6d4b\u91cf\u5355\u5143\u6570\u636e\u7684\u6846\u67b6\uff0c\u5e76\u96c6\u6210\u4e86\u7ebf\u6027\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3001\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236\u5668\u548c\u589e\u91cf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u6765\u4f30\u8ba1\u548c\u63a7\u5236\u540a\u8231\u7684\u4f4d\u7f6e\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u7684\u63a7\u5236\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ea\u6709\u5fae\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\uff08<6%\uff09\u3002\u901a\u8fc7\u73b0\u573a\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u6237\u5916\u73af\u5883\u4e2d\u4ec5\u4f7f\u7528\u73b0\u6210\u65e0\u4eba\u673a\u786c\u4ef6\u7684\u5b9e\u9645\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5728\u53ea\u6709\u6807\u51c6\u673a\u8f7d\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u7684\u63a7\u5236\u76f8\u5f53\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11368", "categories": ["quant-ph", "81P15 (Primary) 81P05, 81S99, 82D99 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.11368", "abs": "https://arxiv.org/abs/2508.11368", "authors": ["Maik Reddiger"], "title": "A solution of the quantum time of arrival problem via mathematical probability theory", "comment": "17 pages + 8 pages of references, 1 figure; Keywords: time of\n  arrival, time of flight, quantum measurement, ideal detector, screen problem,\n  geometric quantum theory, projection postulate", "summary": "Time of arrival refers to the time a particle takes after emission to impinge\nupon a suitably idealized detector surface. Within quantum theory, no generally\naccepted solution exists so far for the corresponding probability distribution\nof arrival times. In this work we derive a general solution for a single body\nwithout spin impacting on a so called ideal detector in the absence of any\nother forces or obstacles. A solution of the so called screen problem for this\ncase is also given. We construct the ideal detector model via mathematical\nprobability theory, which in turn suggests an adaption of the Madelung\nequations in this instance. This detector model assures that the probability\nflux through the detector surface is always positive, so that the corresponding\ndistributions can be derived via an approach originally suggested by Daumer,\nD\\\"urr, Goldstein, and Zangh\\`i. The resulting dynamical model is, strictly\nspeaking, not compatible with quantum mechanics, yet it is well-described\nwithin geometric quantum theory. Geometric quantum theory is a novel adaption\nof quantum mechanics, which makes the latter consistent with mathematical\nprobability theory. Implications to the general theory of measurement and\navenues for future research are also provided. Future mathematical work should\nfocus on finding an appropriate distributional formulation of the evolution\nequations and studying the well-posedness of the corresponding Cauchy problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u91cf\u5b50\u7406\u8bba\u7684\u5230\u8fbe\u65f6\u95f4\u6982\u7387\u5206\u5e03\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e00\u4e2a\u96be\u9898\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "motivation": "\u5728\u91cf\u5b50\u7406\u8bba\u4e2d\uff0c\u7c92\u5b50\u7684\u5230\u8fbe\u65f6\u95f4\u6982\u7387\u5206\u5e03\u5c1a\u672a\u6709\u666e\u904d\u63a5\u53d7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u901a\u8fc7\u6570\u5b66\u6982\u7387\u8bba\u7684\u7406\u60f3\u63a2\u6d4b\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u786e\u4fdd\u4e86\u6982\u7387\u6d41\u901a\u8fc7\u63a2\u6d4b\u5668\u8868\u9762\u59cb\u7ec8\u4e3a\u6b63\uff0c\u5e76\u501f\u9274\u4e86 Daumer\u3001D\u00fcrr\u3001Goldstein \u548c Zangh\u00ec \u6700\u521d\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u4e86\u76f8\u5e94\u7684\u6982\u7387\u5206\u5e03\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u5355\u4e2a\u7c92\u5b50\uff08\u65e0\u81ea\u65cb\uff09\u5728\u6ca1\u6709\u4efb\u4f55\u5176\u4ed6\u529b\u6216\u969c\u788d\u7684\u60c5\u51b5\u4e0b\u649e\u51fb\u7406\u60f3\u63a2\u6d4b\u5668\u7684\u5230\u8fbe\u65f6\u95f4\u6982\u7387\u5206\u5e03\u7684\u901a\u7528\u89e3\uff0c\u5e76\u89e3\u51b3\u4e86\u8be5\u60c5\u51b5\u4e0b\u7684\u5c4f\u5e55\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u52a8\u529b\u5b66\u6a21\u578b\u4e25\u683c\u6765\u8bf4\u4e0e\u91cf\u5b50\u529b\u5b66\u4e0d\u517c\u5bb9\uff0c\u4f46\u5728\u51e0\u4f55\u91cf\u5b50\u7406\u8bba\u4e2d\u5f97\u5230\u4e86\u5f88\u597d\u7684\u63cf\u8ff0\u3002\u51e0\u4f55\u91cf\u5b50\u7406\u8bba\u662f\u5bf9\u91cf\u5b50\u529b\u5b66\u7684\u4e00\u79cd\u65b0\u9896\u7684\u8c03\u6574\uff0c\u5b83\u4f7f\u540e\u8005\u4e0e\u6570\u5b66\u6982\u7387\u8bba\u76f8\u4e00\u81f4\u3002"}}
{"id": "2508.11396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11396", "abs": "https://arxiv.org/abs/2508.11396", "authors": ["Jingran Zhang", "Zhengzhang Yan", "Yiming Chen", "Zeqiang He", "Jiahao Chen"], "title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "comment": null, "summary": "This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eGPS\u62d2\u7edd\u73af\u5883\u4e0b\u7684\u53cc\u8db3\u673a\u5668\u4eba\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u652f\u6491\u811a\u4e0a\u7684IMU\u8fdb\u884c\u4f2a\u6d4b\u91cf\uff0c\u5e76\u91c7\u7528\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08InEKF\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e14InEKF\u6bd4\u6807\u51c6EKF\u66f4\u5bb9\u6613\u8c03\u6574\u3002", "motivation": "\u4e3a\u4e86\u5728GPS\u62d2\u7edd\u73af\u5883\u4e2d\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e00\u79cd\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e86IMU\u5728\u652f\u6491\u811a\u4e0a\u65f6\u6267\u884c\u7684\u56fa\u5b9a\u4f2a\u6d4b\u91cf\u6765\u4e3a\u57fa\u4e8eIMU\u7684\u9884\u6d4b\u63d0\u4f9b\u4fe1\u606f\u3002\u6587\u7ae0\u9610\u8ff0\u4e86\u91c7\u7528\u7684\u57fa\u4e8e\u77e9\u9635\u674e\u7fa4\u7684\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08InEKF\uff09\u7684\u7406\u8bba\u53d1\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u6559\u7a0b\u3002", "result": "\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u57fa\u51c6\u5b9e\u9a8c\u3001\u5927\u89c4\u6a21\u591a\u697c\u5c42\u884c\u8d70\u5b9e\u9a8c\u548c\u53cc\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u5c06InEKF\u4e0e\u6807\u51c6EKF\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\u3002\u6b64\u5916\uff0c\u654f\u611f\u6027\u5206\u6790\u8868\u660eInEKF\u6bd4EKF\u66f4\u5bb9\u6613\u8c03\u6574\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728GPS\u62d2\u7edd\u73af\u5883\u4e2d\u53cc\u8db3\u673a\u5668\u4eba\u3002"}}
{"id": "2508.10950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10950", "abs": "https://arxiv.org/abs/2508.10950", "authors": ["Xinyi Wang", "Michael Barnett", "Frederique Boonstra", "Yael Barnett", "Mariano Cabezas", "Arkiev D'Souza", "Matthew C. Kiernan", "Kain Kyle", "Meng Law", "Lynette Masters", "Zihao Tang", "Stephen Tisch", "Sicong Tu", "Anneke Van Der Walt", "Dongang Wang", "Fernando Calamante", "Weidong Cai", "Chenyu Wang"], "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement", "comment": "24 pages, 5 figures", "summary": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u540d\u4e3a FastFOD-Net \u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5f25\u6563 MRI \u4e2d\u7684\u7ea4\u7ef4\u65b9\u5411\u5206\u5e03 (FOD)\u3002\u8be5\u6846\u67b6\u6bd4\u5176\u524d\u8eab\u5feb 60 \u500d\uff0c\u5e76\u5728\u5065\u5eb7\u548c\u60a3\u75c5\u4eba\u7fa4\u4e2d\u5747\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u6709\u671b\u52a0\u901f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5c42\u3001\u4f4e\u89d2\u5206\u8fa8\u7387\u91c7\u96c6\u7684\u4e34\u5e8a\u534f\u8bae\u751f\u6210\u53ef\u9760 FOD \u7684\u6311\u6218\uff0c\u5e76\u9a8c\u8bc1\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5065\u5eb7\u548c\u60a3\u75c5\u53d7\u8bd5\u8005\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u4fc3\u8fdb\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u4f18\u5316\u4e86\u4e00\u4e2a\u540d\u4e3a FastFOD-Net \u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u7ea4\u7ef4\u65b9\u5411\u5206\u5e03 (FOD)\uff0c\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u5177\u6709\u6548\u7387\uff0c\u6bd4\u5176\u524d\u8eab\u5feb 60 \u500d\u3002", "result": "FastFOD-Net \u5728\u5065\u5eb7\u5bf9\u7167\u7ec4\u548c\u516d\u79cd\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u60a3\u8005\u4e2d\u5747\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3001\u75be\u75c5\u9274\u522b\u3001connectome \u5e94\u7528\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u51cf\u5c11\u6837\u672c\u91cf\u8981\u6c42\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "FastFOD-Net \u6846\u67b6\u80fd\u591f\u52a0\u901f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\uff0c\u901a\u8fc7\u75be\u75c5\u9274\u522b\u8d4b\u80fd\u5f25\u6563 MRI \u5206\u6790\uff0c\u63d0\u9ad8 connectome \u5e94\u7528\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u51cf\u5c11\u6d4b\u91cf\u8bef\u5dee\u4ee5\u964d\u4f4e\u6837\u672c\u91cf\u8981\u6c42\u3002\u8be5\u6846\u67b6\u80fd\u591f\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5f25\u6563 MRI \u6570\u636e\u8fdb\u884c\u7a33\u5065\u5206\u6790\uff0c\u5176\u6548\u679c\u53ef\u4e0e\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u91c7\u96c6\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2508.11184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11184", "abs": "https://arxiv.org/abs/2508.11184", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u4e2a\u6027\u5316\u5e72\u6270\u9879\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u5b66\u751f\u63a8\u7406\u9519\u8bef\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6784\u5efa\u5b66\u751f\u8bef\u89e3\u539f\u578b\uff0c\u5e76\u6a21\u62df\u5b66\u751f\u63a8\u7406\u8fc7\u7a0b\u751f\u6210\u5e72\u6270\u9879\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u548c\u7fa4\u4f53\u5e72\u6270\u9879\u751f\u6210\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e72\u6270\u9879\u751f\u6210\u65b9\u6cd5\uff0c\u867d\u7136\u80fd\u751f\u6210\u7fa4\u4f53\u5c42\u9762\u7684\u5e72\u6270\u9879\uff0c\u4f46\u672a\u80fd\u6355\u6349\u5230\u4e2a\u522b\u5b66\u751f\u7684\u63a8\u7406\u9519\u8bef\uff0c\u9650\u5236\u4e86\u5176\u8bca\u65ad\u6548\u679c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e2a\u6027\u5316\u5e72\u6270\u9879\u751f\u6210\u4efb\u52a1\uff0c\u65e8\u5728\u6839\u636e\u4ece\u5b66\u751f\u5386\u53f2\u95ee\u7b54\u8bb0\u5f55\u4e2d\u63a8\u65ad\u51fa\u7684\u4e2a\u4f53\u8bef\u89e3\u6765\u751f\u6210\u91cf\u8eab\u5b9a\u5236\u7684\u5e72\u6270\u9879\uff0c\u4ee5\u6709\u6548\u5730\u66b4\u9732\u6bcf\u4e2a\u5b66\u751f\u7279\u5b9a\u7684\u63a8\u7406\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\u7684\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u5e94\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6765\u6062\u590d\u5b66\u751f\u8fc7\u53bb\u7684\u9519\u8bef\u7b54\u6848\u6240\u9690\u542b\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u6784\u5efa\u5b66\u751f\u7279\u5b9a\u7684\u8bef\u89e3\u539f\u578b\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u8be5\u539f\u578b\u6307\u5bfc\u6a21\u578b\u5728\u65b0\u7684\u95ee\u9898\u4e0a\u6a21\u62df\u5b66\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u751f\u6210\u4e0e\u5b66\u751f\u5e38\u89c1\u8bef\u89e3\u4e00\u81f4\u7684\u4e2a\u6027\u5316\u5e72\u6270\u9879\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3a140\u540d\u5b66\u751f\u751f\u6210\u4e2a\u6027\u5316\u5e72\u6270\u9879\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u6cdb\u5316\u5230\u7fa4\u4f53\u5c42\u9762\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e3a140\u540d\u5b66\u751f\u751f\u6210\u4e2a\u6027\u5316\u5e72\u6270\u9879\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\uff0c\u5e76\u4e14\u5728\u7fa4\u4f53\u5c42\u9762\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u663e\u793a\u51fa\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.11090", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DB", "68T07, 68T05, 68T09", "I.2.6; I.5.1; G.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11090", "abs": "https://arxiv.org/abs/2508.11090", "authors": ["Daniel Mas Montserrat", "David Bonet", "Maria Perera", "Xavier Gir\u00f3-i-Nieto", "Alexander G. Ioannidis"], "title": "Compressive Meta-Learning", "comment": "Extended version of a paper accepted at KDD '25", "summary": "The rapid expansion in the size of new datasets has created a need for fast\nand efficient parameter-learning techniques. Compressive learning is a\nframework that enables efficient processing by using random, non-linear\nfeatures to project large-scale databases onto compact, information-preserving\nrepresentations whose dimensionality is independent of the number of samples\nand can be easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from the underlying\ndata distribution without requiring access to the original samples, offering an\nefficient and privacy-friendly learning framework. However, both the encoding\nand decoding techniques are typically randomized and data-independent, failing\nto exploit the underlying structure of the data. In this work, we propose a\nframework that meta-learns both the encoding and decoding stages of compressive\nlearning methods by using neural networks that provide faster and more accurate\nsystems than the current state-of-the-art approaches. To demonstrate the\npotential of the presented Compressive Meta-Learning framework, we explore\nmultiple applications -- including neural network-based compressive PCA,\ncompressive ridge regression, compressive k-means, and autoencoders.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u538b\u7f29\u5b66\u4e60\u7684\u7f16\u7801\u548c\u89e3\u7801\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u56e0\u65b0\u6570\u636e\u96c6\u5feb\u901f\u6269\u5f20\u800c\u4ea7\u751f\u7684\u5bf9\u5feb\u901f\u9ad8\u6548\u53c2\u6570\u5b66\u4e60\u6280\u672f\u7684\u9700\u6c42\uff0c\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u5e94\u8fd0\u800c\u751f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7f16\u7801\u548c\u89e3\u7801\u6280\u672f\u672a\u80fd\u5229\u7528\u6570\u636e\u5e95\u5c42\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u538b\u7f29\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u4f18\u5316\u7f16\u7801\u548c\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\uff08\u5305\u62ec\u538b\u7f29PCA\u3001\u538b\u7f29\u5cad\u56de\u5f52\u3001\u538b\u7f29k-means\u548c\u81ea\u52a8\u7f16\u7801\u5668\uff09\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u901f\u548c\u66f4\u51c6\u786e\u7684\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5143\u5b66\u4e60\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u7684\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u795e\u7ecf\u5143\u7f51\u7edc\u57fa\u7840\u7684\u538b\u7f29PCA\u3001\u538b\u7f29\u5cad\u56de\u5f52\u3001\u538b\u7f29k-means\u548c\u81ea\u52a8\u7f16\u7801\u5668\u7b49\u591a\u4e2a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11115", "categories": ["cs.CV", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard\u662f\u4e00\u79cd\u5229\u7528UWB\u6280\u672f\u8fdb\u884c\u975e\u63a5\u89e6\u5f0f\u5750\u59ff\u76d1\u6d4b\u7684\u7cfb\u7edf\uff0c\u53ef\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9690\u79c1\u6027\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u95f4\u4f7f\u7528\u7535\u8111\u65f6\u5750\u59ff\u4e0d\u5f53\u5f15\u8d77\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u57fa\u4e8e\u6444\u50cf\u5934\u6216\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u59ff\u52bf\u76d1\u6d4b\u65b9\u6cd5\u7684\u9690\u79c1\u548c\u7528\u6237\u8212\u9002\u5ea6\u969c\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUWB-PostureGuard\u7684\u9690\u79c1\u4fdd\u62a4\u8d85\u5bbd\u5e26\uff08UWB\uff09\u4f20\u611f\u7cfb\u7edf\uff0c\u5229\u7528\u5546\u4e1aUWB\u8bbe\u5907\u548c\u5168\u9762\u7684\u7279\u5f81\u5de5\u7a0b\u63d0\u53d6\u5750\u59ff\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86PoseGBDT\u6765\u6355\u6349\u5750\u59ff\u6a21\u5f0f\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u548c19\u79cd\u4e0d\u540c\u59ff\u52bf\u7684\u5e7f\u6cdb\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e8699.11%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u62b5\u6297\u73af\u5883\u53d8\u5316\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u79fb\u52a8\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u73b0\u6709\u5e73\u53f0\u4e0a\u8fdb\u884c\u524d\u77bb\u6027\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u7ba1\u7406\uff0c\u4ee5\u4f4e\u6210\u672c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2508.11573", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11573", "abs": "https://arxiv.org/abs/2508.11573", "authors": ["Mogens Plessen"], "title": "Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching", "comment": "14 pages plus 7 pages appendix with additional figures, 18 main\n  figures, 3 tables", "summary": "Automatic Section Control (ASC) is a long-standing trend for spraying in\nagriculture. It promises to minimise spray overlap areas. The core idea is to\n(i) switch off spray nozzles on areas that have already been sprayed, and (ii)\nto dynamically adjust nozzle flow rates along the boom bar that holds the spray\nnozzles when velocities of boom sections vary during turn maneuvers. ASC is not\npossible without sensors, in particular for accurate positioning data. Spraying\nand the movement of modern wide boom bars are highly dynamic processes. In\naddition, many uncertainty factors have an effect such as cross wind drift,\nboom height, nozzle clogging in open-field conditions, and so forth. In view of\nthis complexity, the natural question arises if a simpler alternative exist.\nTherefore, an Automatic Multi-Sections Control method is compared to a proposed\nsimpler one- or two-sections alternative that uses predictive spray switching.\nThe comparison is provided under nominal conditions. Agricultural spraying is\nintrinsically linked to area coverage path planning and spray switching logic.\nCombinations of two area coverage path planning and switching logics as well as\nthree sections-setups are compared. The three sections-setups differ by\ncontrolling 48 sections, 2 sections or controlling all nozzles uniformly with\nthe same control signal as one single section. Methods are evaluated on 10\ndiverse real-world field examples, including non-convex field contours,\nfreeform mainfield lanes and multiple obstacle areas. A preferred method is\nsuggested that (i) minimises area coverage pathlength, (ii) offers intermediate\noverlap, (iii) is suitable for manual driving by following a pre-planned\npredictive spray switching logic for an area coverage path plan, and (iv) and\nin contrast to ASC can be implemented sensor-free and therefore at low cost.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u519c\u4e1a\u55b7\u6d12\u63a7\u5236\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4f20\u7edfASC\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u6d4b\u6027\u55b7\u6d12\u5207\u6362\u548c\u7b80\u5316\u7684\u8282\u6bb5\u63a7\u5236\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u4f20\u611f\u5668\u65e0\u5173\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u52a8\u8282\u6bb5\u63a7\u5236\uff08ASC\uff09\u7528\u4e8e\u519c\u4e1a\u55b7\u6d12\uff0c\u65e8\u5728\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u55b7\u6d12\u91cd\u53e0\u533a\u57df\uff0c\u4f46\u5176\u5b9e\u73b0\u9700\u8981\u4f20\u611f\u5668\u4e14\u8fc7\u7a0b\u590d\u6742\uff0c\u6613\u53d7\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u5b58\u5728\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e00\u79cd\u81ea\u52a8\u591a\u8282\u6bb5\u63a7\u5236\uff08ASC\uff09\u65b9\u6cd5\u4e0e\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u3001\u57fa\u4e8e\u9884\u6d4b\u55b7\u6d12\u5207\u6362\u7684\u5355\u8282\u6bb5\u6216\u4e24\u8282\u6bb5\u66ff\u4ee3\u65b9\u6cd5\u3002\u7814\u7a76\u4e2d\u8003\u8651\u4e86\u4e24\u79cd\u9762\u79ef\u8986\u76d6\u8def\u5f84\u89c4\u5212\u548c\u55b7\u6d12\u5207\u6362\u903b\u8f91\u7684\u7ec4\u5408\uff0c\u4ee5\u53ca\u4e09\u79cd\u4e0d\u540c\u7684\u8282\u6bb5\u8bbe\u7f6e\uff0c\u5e76\u6839\u636e\u6700\u5c0f\u5316\u9762\u79ef\u8986\u76d6\u8def\u5f84\u957f\u5ea6\u3001\u63d0\u4f9b\u4e2d\u95f4\u91cd\u53e0\u7387\u3001\u9002\u7528\u4e8e\u624b\u52a8\u9a7e\u9a76\u4ee5\u53ca\u4f4e\u6210\u672c\u5b9e\u73b0\u7684\u6807\u51c6\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u572810\u4e2a\u771f\u5b9e\u7684\u3001\u591a\u6837\u7684\u7530\u91ce\u6848\u4f8b\uff08\u5305\u62ec\u975e\u51f8\u8f6e\u5ed3\u3001\u81ea\u7531\u5f62\u72b6\u4e3b\u7530\u9053\u548c\u591a\u4e2a\u969c\u788d\u533a\u57df\uff09\u7684\u8bc4\u4f30\u4e2d\uff0c\u7814\u7a76\u53d1\u73b0\u4e00\u79cd\u7ed3\u5408\u4e86\u7279\u5b9a\u8def\u5f84\u89c4\u5212\u548c\u9884\u6d4b\u55b7\u6d12\u5207\u6362\u903b\u8f91\u7684\u5355\u8282\u6bb5\u6216\u4e24\u8282\u6bb5\u63a7\u5236\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684ASC\uff0c\u5177\u6709\u6700\u5c0f\u5316\u9762\u79ef\u8986\u76d6\u8def\u5f84\u957f\u5ea6\u3001\u63d0\u4f9b\u4e2d\u95f4\u91cd\u53e0\u7387\u3001\u9002\u7528\u4e8e\u624b\u52a8\u9a7e\u9a76\u7b49\u4f18\u70b9\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u4f20\u611f\u5668\u65e0\u5173\u7684\u7279\u6027\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4f4e\u6210\u672c\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u4f20\u611f\u5668\u65e0\u5173\u7684\u55b7\u6d12\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u8def\u5f84\u89c4\u5212\u548c\u55b7\u6d12\u5207\u6362\u903b\u8f91\uff0c\u5e76\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684\u8282\u6bb5\u8bbe\u7f6e\uff08\u63a7\u523648\u4e2a\u8282\u6bb5\u30012\u4e2a\u8282\u6bb5\u6216\u6240\u6709\u55b7\u5634\u4f5c\u4e3a\u4e00\u4e2a\u8282\u6bb5\u7edf\u4e00\u63a7\u5236\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002"}}
{"id": "2508.11418", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11418", "abs": "https://arxiv.org/abs/2508.11418", "authors": ["Rounak Chatterjee", "Mayuresh Kanagal", "Vikas S Bhat", "Kiran Bajar", "Sushil Mujumdar"], "title": "Generation and certification of pure phase entangled light", "comment": "11 pages, 10 figures", "summary": "Biphoton systems exhibiting entanglement in position-momentum variables,\nknown as spatial entanglement, are among the most intriguing and well-studied\nphenomena in quantum optics. A notable subset of these are phase entangled\nstates, where entanglement manifests purely through correlations in the spatial\nphase of the wavefunction. While the generation of such states from biphotons\nvia spontaneous parametric down-conversion has been explored, their physical\nimplications and applications remain under-investigated. In this work, we\ntheoretically and experimentally examine a unique form of phase entanglement\nknown as `pure' phase entanglement. This state exhibits the unusual feature\nthat the position of one photon is correlated with the momentum of the other.\nUnlike typical spatially entangled states, it shows no direct correlation in\nposition or momentum between the two photons, underscoring that all\ncorrelations arise purely from the spatial phase of the wavefunction. We delve\ndeeper into the theory of this state and experimentally construct it from known\nphase-entangled states. To certify its properties, we propose a setup that\nperforms a \"one-particle momentum measurement\" and explore the various tunable\nparameters. We also highlight potential applications of this state in quantum\noptics and imaging experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u2018\u7eaf\u2019\u76f8\u4f4d\u7ea0\u7f20\u6001\uff0c\u5176\u7279\u70b9\u662f\u4e24\u5149\u5b50\u95f4\u6ca1\u6709\u76f4\u63a5\u7684\u4f4d\u7f6e\u6216\u52a8\u91cf\u76f8\u5173\u6027\uff0c\u4f46\u5b58\u5728\u76f8\u4f4d\u5173\u8054\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5b9e\u9a8c\u5236\u5907\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u91cf\u5b50\u5149\u5b66\u548c\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u7a7a\u95f4\u7ea0\u7f20\u6001\uff0c\u7279\u522b\u662f\u76f8\u4f4d\u7ea0\u7f20\u6001\u7684\u7269\u7406\u610f\u4e49\u548c\u5e94\u7528\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u6b64\u7a7a\u767d\u3002", "method": "\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684\u2018\u7eaf\u2019\u76f8\u4f4d\u7ea0\u7f20\u6001\uff0c\u8be5\u7ea0\u7f20\u6001\u7684\u7279\u70b9\u662f\u5355\u4e2a\u5149\u5b50\u7684\u4f4d\u7f6e\u4e0e\u53e6\u4e00\u4e2a\u5149\u5b50\u7684\u52a8\u91cf\u76f8\u5173\uff0c\u800c\u5149\u5b50\u95f4\u6ca1\u6709\u76f4\u63a5\u7684\u4f4d\u7f6e\u6216\u52a8\u91cf\u76f8\u5173\u6027\u3002", "result": "\u6210\u529f\u4ece\u5df2\u77e5\u7684\u76f8\u4f4d\u7ea0\u7f20\u6001\u6784\u5efa\u4e86\u2018\u7eaf\u2019\u76f8\u4f4d\u7ea0\u7f20\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u901a\u8fc7\u2018\u5355\u7c92\u5b50\u52a8\u91cf\u6d4b\u91cf\u2019\u6765\u9a8c\u8bc1\u5176\u6027\u8d28\u7684\u5b9e\u9a8c\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u5149\u5b66\u548c\u6210\u50cf\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u4f4d\u7ea0\u7f20\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u9a8c\u8bc1\u5176\u6027\u8d28\u7684\u5b9e\u9a8c\u65b9\u6848\u3002"}}
{"id": "2508.11404", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "AI-powered robots with human collaboration improve safety and accuracy in nuclear facility inspections, reducing human workload and errors compared to manual methods.", "motivation": "Traditional manual inspection methods in nuclear facilities pose safety risks, high cognitive demands, and potential inaccuracies due to human limitations. AI and robotic technologies, specifically HRC, offer safer, more efficient, and accurate alternatives.", "method": "The study explores the effectiveness of AI-assisted visual crack detection integrated into a mobile Jackal robot platform for structural inspection in nuclear facilities, utilizing Human-Robot Collaboration (HRC).", "result": "Experimental results indicate that HRC enhances inspection accuracy and reduces operator workload, demonstrating potential for superior performance compared to traditional manual methods.", "conclusion": "Human-robot collaboration (HRC) with AI-assisted visual crack detection on a mobile robot platform enhances inspection accuracy and reduces operator workload in nuclear facilities, showing potential for superior performance compared to traditional manual methods."}}
{"id": "2508.10955", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10955", "abs": "https://arxiv.org/abs/2508.10955", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.", "AI": {"tldr": "MLLMs are powerful but face challenges. Augmenting them with external tools can improve data, task performance, and evaluation, offering a path to more capable AI. This paper surveys how tools help MLLMs.", "motivation": "Despite the success of MLLMs like GPT-4V, challenges such as limited data quality, poor performance on complex tasks, and inadequate evaluation hinder their reliability and applicability. Augmenting MLLMs with external tools is proposed as a solution.", "method": "This paper provides a comprehensive survey structured along four key dimensions: data acquisition/annotation, downstream task performance improvement, evaluation, and limitations/future directions of tool-augmented MLLMs.", "result": "The survey analyzes how external tools can facilitate high-quality multimodal data acquisition and annotation, assist in improving MLLM performance on challenging tasks, and enable comprehensive and accurate evaluation, ultimately aiming to advance MLLM capabilities.", "conclusion": "The paper surveys the use of external tools to enhance Multimodal Large Language Models (MLLMs), highlighting their potential to improve data quality, task performance, and evaluation, while also discussing limitations and future directions."}}
{"id": "2508.11189", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11189", "abs": "https://arxiv.org/abs/2508.11189", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bc4\u751f\u53cc\u5c3a\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7 KVSPN \u6a21\u5757\u548c\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347 Whisper \u7ffb\u8bd1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86 2.6 \u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u6a21\u578b\u867d\u7136\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u8bed\u8a00\uff0c\u4f46\u901a\u5e38\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u96be\u4ee5\u5728\u4fdd\u8bc1\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u672c\u5730\u90e8\u7f72\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5bc4\u751f\u53cc\u5c3a\u5ea6\u65b9\u6cd5\uff08Parasitic Dual-Scale Approach\uff09\uff0c\u7ed3\u5408\u4e86\u589e\u5f3a\u7684\u63a8\u6d4b\u6027\u91c7\u6837\uff08speculative sampling\uff09\u65b9\u6cd5\u3001\u6a21\u578b\u538b\u7f29\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5bf9 Whisper Medium \u6a21\u578b\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u4f7f\u5176\u6210\u4e3a whisperM2M\uff0c\u5e76\u96c6\u6210\u4e86\u65b0\u9896\u7684 KVSPN \u6a21\u5757\u3002", "result": "\u5728\u516d\u79cd\u6d41\u884c\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002KVSPN \u6a21\u5757\u5b9e\u73b0\u4e86 40% \u7684\u52a0\u901f\uff0c\u4e14 BLEU \u5206\u6570\u6ca1\u6709\u4e0b\u964d\u3002\u7ed3\u5408\u84b8\u998f\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u539f\u59cb\u7684 Whisper Medium \u6a21\u578b\uff0c\u5b9e\u73b0\u4e86 2.6 \u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 KVSPN \u6a21\u5757\u548c\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u4e86 40% \u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14 BLEU \u5206\u6570\u6ca1\u6709\u4e0b\u964d\u3002\u4e0e\u539f\u59cb\u7684 Whisper Medium \u6a21\u578b\u76f8\u6bd4\uff0c\u6574\u4f53\u901f\u5ea6\u63d0\u5347\u4e86 2.6 \u500d\uff0c\u540c\u65f6\u6027\u80fd\u66f4\u4f18\uff0c\u8fbe\u5230\u4e86\u5305\u62ec\u516d\u79cd\u6d41\u884c\u8bed\u8a00\u5728\u5185\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.11092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11092", "abs": "https://arxiv.org/abs/2508.11092", "authors": ["Cindy Shih-Ting Huang", "Clarence Boon Liang Ng", "Marek Rei"], "title": "Predictive Multimodal Modeling of Diagnoses and Treatments in EHR", "comment": "10 pages, 1 figure", "summary": "While the ICD code assignment problem has been widely studied, most works\nhave focused on post-discharge document classification. Models for early\nforecasting of this information could be used for identifying health risks,\nsuggesting effective treatments, or optimizing resource allocation. To address\nthe challenge of predictive modeling using the limited information at the\nbeginning of a patient stay, we propose a multimodal system to fuse clinical\nnotes and tabular events captured in electronic health records. The model\nintegrates pre-trained encoders, feature pooling, and cross-modal attention to\nlearn optimal representations across modalities and balance their presence at\nevery temporal point. Moreover, we present a weighted temporal loss that\nadjusts its contribution at each point in time. Experiments show that these\nstrategies enhance the early prediction model, outperforming the current\nstate-of-the-art systems.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u52a0\u6743\u7684 \u0627\u0644\u0632\u0645\u0646\u064a\u0629 \u635f\u5931\uff0c\u5b9e\u73b0\u4e86ICD\u7f16\u7801\u7684\u65e9\u671f\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u60a3\u8005\u4f4f\u9662\u521d\u671f\u4fe1\u606f\u6709\u9650\u7684\u9884\u6d4b\u5efa\u6a21\u6311\u6218\uff0c\u5b9e\u73b0\u65e9\u671f\u9884\u6d4bICD\u7f16\u7801\uff0c\u4ece\u800c\u8bc6\u522b\u5065\u5eb7\u98ce\u9669\u3001\u5efa\u8bae\u6709\u6548\u6cbb\u7597\u6216\u4f18\u5316\u8d44\u6e90\u914d\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u878d\u5408\u4e34\u5e8a\u7b14\u8bb0\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u8868\u683c\u4e8b\u4ef6\uff0c\u96c6\u6210\u4e86\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u7279\u5f81\u6c60\u5316\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u7684 \u0627\u0644\u0632\u0645\u0646\u064a\u0629 \u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u63d0\u5347\u65e9\u671f\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u878d\u5408\u4e34\u5e8a\u7b14\u8bb0\u548c\u8868\u683c\u4e8b\u4ef6\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u7684 \u0627\u0644\u0632\u0645\u0646\u064a\u0629 \u635f\u5931\uff0c\u5728\u65e9\u671f\u9884\u6d4bICD\u7f16\u7801\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.11445", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11445", "abs": "https://arxiv.org/abs/2508.11445", "authors": ["Matthew Freed", "Dominic M. Rouse", "Andrea Rocco", "Jim Al-Khalili", "Marian Florescu", "Adam Burgess"], "title": "The Effect of Permanent Dipoles on Dark States in Molecular Dimers", "comment": "13 pages, 5 figures", "summary": "Many organic molecules possess large permanent dipole moments that differ\ndepending on the electronic state. These permanent dipoles influence both\nintermolecular coupling and interactions with the optical fields, yet they are\noften neglected in typical theoretical quantum optics treatments. Here, we\ninvestigate the optical properties and their effect on dark states of dimers\npossessing such permanent dipoles. We show that when monomers have\nexcitation-dependent permanent dipoles, optical transitions between the bright\nand dark states of the dimer are enabled. We investigate how permanent dipoles\nallow for the existence of static driving terms between the ground and excited\nstates of each monomer. In turn, these can cause the excited states of the\nmonomers to couple indirectly to the zero excitation state of the dimer. This\nleads to interference between permanent and transition dipoles and can result\nin the formation of dark states that are entirely localised. Furthermore, dark\nstates formed through indirect coupling exhibit enhanced robustness against\nenergy level fluctuations, which may improve the efficiency of the design of\nphotovoltaic devices.", "AI": {"tldr": "Organic molecule permanent dipoles affect light interactions. This paper shows how these dipoles in dimers create robust 'dark states' that could boost solar cells.", "motivation": "To address the common neglect of permanent dipoles in quantum optics treatments and explore their influence on intermolecular coupling and optical field interactions, specifically in organic dimers.", "method": "Investigating optical properties and their effect on dark states of dimers with excitation-dependent permanent dipoles, analyzing the coupling mechanisms and resulting interference effects.", "result": "Demonstrated that excitation-dependent permanent dipoles enable optical transitions between bright and dark states in dimers, leading to indirectly coupled, localized dark states that are more robust to energy fluctuations.", "conclusion": "Permanent dipoles in organic molecules, especially when excitation-dependent, enable optical transitions between bright and dark states in dimers. This coupling, mediated by static driving terms, leads to interference with transition dipoles, forming localized dark states with enhanced robustness against fluctuations, potentially improving photovoltaic device efficiency."}}
{"id": "2508.11406", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u6b63\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8fdb\u884c\u79d1\u5b66\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u5b9e\u9a8c\u7ed3\u679c\u66f4\u6613\u4e8e\u5171\u4eab\u548c\u590d\u5236\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u4e00\u4e2a\u81ea\u4e3b\u673a\u5668\u4eba\u8fdb\u884c\u7cbe\u786e\u3001\u53ef\u91cd\u590d\u3001\u5f00\u653e\u3001\u53ef\u4fe1\u548c\u900f\u660e\u7684\u79d1\u5b66\u5b9e\u9a8c\u7684\u672a\u6765\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u4e49\u6267\u884c\u8ddf\u8e2a\u6846\u67b6\uff0c\u8bb0\u5f55\u4f20\u611f\u5668\u6570\u636e\u548c\u8bed\u4e49\u6ce8\u91ca\u7684\u673a\u5668\u4eba\u4fe1\u5ff5\u72b6\u6001\uff0c\u5e76\u63a8\u51fa\u4e86 AICOR \u865a\u62df\u7814\u7a76\u6784\u5efa\uff08VRB\uff09\u5e73\u53f0\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5171\u4eab\u3001\u590d\u5236\u548c\u9a8c\u8bc1\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u3002", "result": "\u8be5\u6846\u67b6\u548c\u5e73\u53f0\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u96c6\u6210\u786e\u5b9a\u6027\u6267\u884c\u3001\u8bed\u4e49\u8bb0\u5fc6\u548c\u5f00\u653e\u77e5\u8bc6\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u7684\u3001\u673a\u5668\u4eba\u9a71\u52a8\u7684\u79d1\u5b66\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u5e73\u53f0\u4e3a\u5b9e\u73b0\u53ef\u590d\u73b0\u7684\u3001\u673a\u5668\u4eba\u9a71\u52a8\u7684\u79d1\u5b66\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2508.10956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10956", "abs": "https://arxiv.org/abs/2508.10956", "authors": ["Abhishek Kolari", "Mohammadhossein Khojasteh", "Yifan Jiang", "Floris den Hengst", "Filip Ilievski"], "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks", "comment": null, "summary": "While vision-language models (VLMs) have made remarkable progress on many\npopular visual question answering (VQA) benchmarks, it remains unclear whether\nthey abstract and reason over depicted objects. Inspired by human object\ncategorisation, object property reasoning involves identifying and recognising\nlow-level details and higher-level abstractions. While current VQA benchmarks\nconsider a limited set of object property attributes like size, they typically\nblend perception and reasoning, and lack representativeness in terms of\nreasoning and image categories. To this end, we introduce a systematic\nevaluation framework with images of three representative types, three reasoning\nlevels of increasing complexity, and four object property dimensions driven by\nprior work on commonsense reasoning. We develop a procedure to instantiate this\nbenchmark into ORBIT, a multi-level reasoning VQA benchmark for object\nproperties comprising 360 images paired with a total of 1,080 count-based\nquestions. Experiments with 12 state-of-the-art VLMs in zero-shot settings\nreveal significant limitations compared to humans, with the best-performing\nmodel only reaching 40\\% accuracy. VLMs struggle particularly with realistic\n(photographic) images, counterfactual reasoning about physical and functional\nproperties, and higher counts. ORBIT points to the need to develop methods for\nscalable benchmarking, generalize annotation guidelines, and explore additional\nreasoning VLMs. We make the ORBIT benchmark and the experimental code available\nto support such endeavors.", "AI": {"tldr": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7269\u4f53\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u771f\u5b9e\u56fe\u50cf\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u9700\u8981\u66f4\u9ad8\u8ba1\u6570\u7684\u95ee\u9898\u65f6\u3002\u6211\u4eec\u63d0\u51fa\u4e86ORBIT\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5f53\u524dVLM\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30VLM\u5728\u7269\u4f53\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5373\u8bc6\u522b\u548c\u8bc6\u522b\u4f4e\u7ea7\u7ec6\u8282\u548c\u66f4\u9ad8\u7ea7\u522b\u7684\u62bd\u8c61\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u4ee3\u8868\u6027\u56fe\u50cf\u3001\u4e09\u4e2a\u9012\u589e\u7684\u63a8\u7406\u590d\u6742\u6027\u7ea7\u522b\u548c\u56db\u4e2a\u7531\u5e38\u8bc6\u63a8\u7406\u9a71\u52a8\u7684\u7269\u4f53\u5c5e\u6027\u7ef4\u5ea6\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aORBIT\u7684\u591a\u5c42\u6b21\u63a8\u7406VQA\u57fa\u51c6\uff0c\u5305\u542b360\u5f20\u56fe\u50cf\u548c1080\u4e2a\u8ba1\u6570\u95ee\u9898\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c12\u4e2a\u6700\u5148\u8fdb\u7684VLM\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u5b83\u4eec\u5b58\u5728\u663e\u8457\u7684\u5c40\u9650\u6027\uff0c\u6700\u597d\u7684\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a40%\u3002", "conclusion": "VLMs \u5728\u7269\u4f53\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5199\u5b9e\u56fe\u50cf\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u8ba1\u6570\u65b9\u9762\u3002\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u548c\u901a\u7528\u6ce8\u91ca\u6307\u5357\uff0c\u5e76\u63a2\u7d22\u66f4\u591a\u63a8\u7406VLM\u3002"}}
{"id": "2508.11105", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11105", "abs": "https://arxiv.org/abs/2508.11105", "authors": ["Sajjad Saed", "Babak Teimourpour"], "title": "Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation", "comment": null, "summary": "The rapid expansion of the fashion industry and the growing variety of\nproducts have made it challenging for users to find compatible items on\ne-commerce platforms. Effective fashion recommendation systems are crucial for\nfiltering irrelevant items and suggesting suitable ones. However,\nsimultaneously addressing outfit compatibility and personalized recommendations\nremains a significant challenge, as these aspects are often treated\nindependently in existing studies, often overlooking the complex interactions\nbetween items and user preferences. This research introduces a new framework\nnamed FGAT, inspired by the HFGN model, which leverages graph neural networks\nand graph attention mechanisms to tackle this issue. The proposed framework\nconstructs a three-tier hierarchical graph of users, outfits, and items,\nintegrating visual and textual features to simultaneously model outfit\ncompatibility and user preferences. A graph attention mechanism dynamically\nweights node importance during representation propagation, enabling the capture\nof key interactions and generating precise representations for both user\npreferences and outfit compatibility. Evaluated on the POG dataset, FGAT\noutperforms baseline models such as HFGN, achieving improved results in\nprecision, HR, recall, NDCG, and accuracy.These results demonstrate that\ncombining multimodal visual-textual features with a hierarchical graph\nstructure and attention mechanisms significantly enhances the accuracy and\nefficiency of personalized fashion recommendation systems.", "AI": {"tldr": "FGAT, a new framework inspired by HFGN, uses a hierarchical graph and attention mechanism with visual-textual features to improve fashion recommendations by considering both outfit compatibility and user preferences simultaneously.", "motivation": "Addressing outfit compatibility and personalized recommendations simultaneously is challenging due to independent treatment in existing studies, overlooking complex interactions between items and user preferences in fashion e-commerce.", "method": "FGAT framework constructs a three-tier hierarchical graph of users, outfits, and items, integrating visual and textual features to simultaneously model outfit compatibility and user preferences. A graph attention mechanism dynamically weights node importance during representation propagation.", "result": "FGAT outperforms baseline models like HFGN on the POG dataset in precision, HR, recall, NDCG, and accuracy.", "conclusion": "FGAT combined multimodal visual-textual features with a hierarchical graph structure and attention mechanisms to enhance the accuracy and efficiency of personalized fashion recommendation systems."}}
{"id": "2508.11480", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11480", "abs": "https://arxiv.org/abs/2508.11480", "authors": ["Vyacheslav Shatokhin", "Friedemann Landmesser", "Mario Niebuhr", "Frank Stienkemeier", "Andreas Buchleitner", "Lukas Bruder"], "title": "Anisotropic fluorescence signals retarded dipole-dipole interactions in a thermal atomic cloud", "comment": null, "summary": "We experimentally observe and theoretically explain anisotropic multiple\nquantum coherence signals in the fluorescence from dilute thermal potassium\nvapors, at room temperature and particle densities $\\sim 10^8\\ \\rm{cm}^{-3}$.\nWe identify the retarded part of the geometrically fully resolved inter-atomic,\nresonant dipole-dipole interaction as the crucial ingredient to theoretically\nreproduce all qualitative features of the experimental spectra.", "AI": {"tldr": "Anisotropic multiple quantum coherence signals in potassium vapors were observed and explained using inter-atomic dipole-dipole interaction.", "motivation": "Anisotropic multiple quantum coherence signals in the fluorescence from dilute thermal potassium vapors.", "method": "We experimentally observe and theoretically explain anisotropic multiple quantum coherence signals in the fluorescence from dilute thermal potassium vapors.", "result": "We experimentally observe and theoretically explain anisotropic multiple quantum coherence signals in the fluorescence from dilute thermal potassium vapors, at room temperature and particle densities $\\sim 10^8\rm{cm}^{-3}$. The retarded part of the geometrically fully resolved inter-atomic, resonant dipole-dipole interaction is identified as the crucial ingredient to theoretically reproduce all qualitative features of the experimental spectra.", "conclusion": "We identify the retarded part of the geometrically fully resolved inter-atomic, resonant dipole-dipole interaction as the crucial ingredient to theoretically reproduce all qualitative features of the experimental spectra."}}
{"id": "2508.11453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11453", "abs": "https://arxiv.org/abs/2508.11453", "authors": ["Jiayue Jin", "Lang Qian", "Jingyu Zhang", "Chuanyu Ju", "Liang Song"], "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback", "comment": null, "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvoPSF\u7684\u5728\u7ebf\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6a21\u578b\u5728\u90e8\u7f72\u540e\u65e0\u6cd5\u9002\u5e94\u65b0\u73af\u5883\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u8bc6\u522b\u5e76\u66f4\u65b0\u5bf9\u89c4\u5212\u5931\u8d25\u5f71\u54cd\u6700\u5927\u7684\u5173\u952e\u5bf9\u8c61\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u548c\u89c4\u5212\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5927\u591a\u662f\u79bb\u7ebf\u8bad\u7ec3\u7684\uff0c\u7f3a\u4e4f\u5728\u90e8\u7f72\u671f\u95f4\u9002\u5e94\u65b0\u73af\u5883\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u573a\u666f\u4e2d\u672a\u89c1\u8fc7\u7684\u53d8\u5316\u65f6\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002\u6587\u7ae0\u5e0c\u671b\u6253\u7834\u201c\u8bad\u7ec3\u4e00\u6b21\uff0c\u6c38\u8fdc\u90e8\u7f72\u201d\u7684\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5728\u7ebf\u8fdb\u5316\u6846\u67b6EvoPSF\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u89c4\u5212-\u72b6\u6001\u53cd\u9988\u3002\u5c06\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u5728\u7ebf\u8fdb\u5316\u7684\u89e6\u53d1\u5668\uff0c\u5229\u7528\u5b83\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u6765\u542f\u52a8\u6709\u9488\u5bf9\u6027\u7684\u6a21\u578b\u66f4\u65b0\u3002\u901a\u8fc7\u5229\u7528\u89c4\u5212\u5668\u7684\u4ee3\u7406-\u4ee3\u7406\u6ce8\u610f\u529b\u6765\u8bc6\u522b\u5bf9\u89c4\u5212\u5931\u8d25\u8d1f\u4e3b\u8981\u8d23\u4efb\u7684\u5173\u952e\u5bf9\u8c61\uff0c\u5e76\u5c06\u5176\u4e0e\u611f\u77e5\u6a21\u5757\u7684\u8f93\u51fa\u6765\u6e90\u8fdb\u884c\u6bd4\u8f83\uff0c\u8ba1\u7b97\u6709\u9488\u5bf9\u6027\u7684\u81ea\u76d1\u7763\u635f\u5931\uff0c\u7136\u540e\u8fdb\u884c\u5728\u7ebf\u53cd\u5411\u4f20\u64ad\u4ee5\u9002\u5e94\u6a21\u578b\u3002", "result": "EvoPSF\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u73af\u5883\u53d8\u5316\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5b9a\u7684\u89c4\u5212\u3002\u5728nuScenes\u6570\u636e\u96c6\u7684\u8de8\u533a\u57df\u548c\u635f\u574f\u53d8\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvoPSF\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6301\u7eed\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "EvoPSF\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u73af\u5883\u53d8\u5316\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5b9a\u7684\u89c4\u5212\u3002\u5728nuScenes\u6570\u636e\u96c6\u7684\u8de8\u533a\u57df\u548c\u635f\u574f\u53d8\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvoPSF\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6301\u7eed\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2508.10962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10962", "abs": "https://arxiv.org/abs/2508.10962", "authors": ["Jiarong Li", "Imad Ali Shah", "Diarmaid Geever", "Fiachra Collins", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving", "comment": "Under Review at IEEE OJITS, July, 2025", "summary": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u9ad8\u5149\u8c31\u6210\u50cf\u5e26\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u6ce2\u6bb5\uff08497nm\u3001607nm\u3001895nm\uff09\u5e76\u751f\u6210\u4f2a\u5f69\u8272\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u53ef\u89c1\u6027\uff0c\u89e3\u51b3\u4e86RGB\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u6b67\u4e49\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6c7d\u8f66\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u540c\u8d28\u5f02\u8c31\u73b0\u8c61\uff08RGB\u56fe\u50cf\u4e2d\u4e0d\u540c\u6750\u6599\u5916\u89c2\u76f8\u4f3c\uff09\u5bfc\u81f4\u7684\u89c6\u89c9\u6b67\u4e49\u4e0b\uff0c\u4fdd\u62a4\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u5b89\u5168\u6311\u6218\u3002\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u80fd\u591f\u6355\u6349\u53ef\u89c1\u5149\u8c31\u4e4b\u5916\u7684\u72ec\u7279\u6750\u6599\u4fe1\u606f\uff0c\u7279\u522b\u662f\u8fd1\u7ea2\u5916\uff08NIR\uff09\u4fe1\u606f\uff0c\u4ee5\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5e26\u9009\u62e9\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7ed3\u5408\u4e86\u4fe1\u606f\u8bba\u6280\u672f\uff08\u8054\u5408\u4e92\u4fe1\u606f\u6700\u5927\u5316\u3001\u76f8\u5173\u6027\u5206\u6790\uff09\u548c\u56fe\u50cf\u8d28\u91cf\u5ea6\u91cf\uff08\u5bf9\u6bd4\u5ea6\u4fe1\u566a\u6bd4\uff09\uff0c\u4ee5\u8bc6\u522b\u5bf9VRU\u8bc6\u522b\u6700\u6709\u4fe1\u606f\u91cf\u7684\u9ad8\u5149\u8c31\u6ce2\u6bb5\u3002\u4f7f\u7528H-City\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u51fa3\u4e2a\u6700\u4f18\u6ce2\u6bb5\uff08497nm\u3001607nm\u3001895nm\uff09\uff0c\u5e76\u91cd\u6784\u4f2a\u5f69\u8272\u56fe\u50cf\u4e0eRGB\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6240\u9009\u7684\u9ad8\u5149\u8c31\u6ce2\u6bb5\u5728\u533a\u5206\u5ea6\uff08\u6b27\u6c0f\u8ddd\u79bb\u3001SAM\u3001$T^2$\uff09\u548c\u611f\u77e5\uff08CIE $\triangle E$\uff09\u6307\u6807\u4e0a\u5206\u522b\u5e26\u6765\u4e8670.24%\u3001528.46%\u30011206.83%\u548c246.62%\u7684\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8eRGB\u56fe\u50cf\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u540c\u8d28\u5f02\u8c31\u7684\u6df7\u6dc6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u9ad8\u5149\u8c31\u6210\u50cf\u548c\u5e26\u9009\u62e9\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8fd1\u7ea2\u5916\u5149\u8c31\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u4e0e\u80cc\u666f\u7684\u533a\u5206\u5ea6\uff0c\u514b\u670d\u4e86RGB\u56fe\u50cf\u4e2d\u7684\u540c\u8d28\u5f02\u8c31\u73b0\u8c61\uff0c\u4e3a\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u548c\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e0b\u7684\u611f\u77e5\u4efb\u52a1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3002"}}
{"id": "2508.11247", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11247", "abs": "https://arxiv.org/abs/2508.11247", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "AI": {"tldr": "HGRAG\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u7684\u65b0\u578bRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86QA\u6027\u80fd\u5e76\u52a0\u901f\u4e86\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u5206\u6563\u77e5\u8bc6\u7684\u7ed3\u6784\u5173\u8054\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u800cGraphRAG\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u68c0\u7d22\uff0c\u5bfc\u81f4\u6587\u672c\u8bed\u4e49\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHGRAG\u7684\u65b0\u578bRAG\u65b9\u6cd5\uff0c\u5229\u7528\u8d85\u56fe\u5b9e\u73b0\u4e86\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u8de8\u7c92\u5ea6\u96c6\u6210\u3002\u901a\u8fc7\u6784\u5efa\u5b9e\u4f53\u8d85\u56fe\uff08\u5b9e\u4f53\u4f5c\u4e3a\u8282\u70b9\uff0c\u6bb5\u843d\u4f5c\u4e3a\u8d85\u8fb9\uff09\u6765\u5efa\u7acb\u77e5\u8bc6\u5173\u8054\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u8fc7\u8d85\u56fe\u6269\u6563\u96c6\u6210\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u76f8\u4f3c\u6027\u548c\u7c97\u7c92\u5ea6\u6bb5\u843d\u76f8\u4f3c\u6027\u7684\u8d85\u56fe\u68c0\u7d22\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u6a21\u5757\u8fdb\u4e00\u6b65\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\uff0c\u4ee5\u83b7\u5f97\u6700\u76f8\u5173\u7684\u6bb5\u843d\u4f5c\u4e3aLLM\u7684\u56de\u7b54\u751f\u6210\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGRAG\u65b9\u6cd5\u5728QA\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e866\u500d\u7684\u68c0\u7d22\u6548\u7387\u63d0\u5347\u3002", "conclusion": "HGRAG\u65b9\u6cd5\u5728MHQA\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u68c0\u7d22\u6548\u7387\u4e0a\u5b9e\u73b0\u4e866\u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2508.11112", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11112", "abs": "https://arxiv.org/abs/2508.11112", "authors": ["Jianhao Ma", "Lin Xiao"], "title": "Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees", "comment": null, "summary": "Optimization problems over discrete or quantized variables are very\nchallenging in general due to the combinatorial nature of their search space.\nPiecewise-affine regularization (PAR) provides a flexible modeling and\ncomputational framework for quantization based on continuous optimization. In\nthis work, we focus on the setting of supervised learning and investigate the\ntheoretical foundations of PAR from optimization and statistical perspectives.\nFirst, we show that in the overparameterized regime, where the number of\nparameters exceeds the number of samples, every critical point of the\nPAR-regularized loss function exhibits a high degree of quantization. Second,\nwe derive closed-form proximal mappings for various (convex, quasi-convex, and\nnon-convex) PARs and show how to solve PAR-regularized problems using the\nproximal gradient method, its accelerated variant, and the Alternating\nDirection Method of Multipliers. Third, we study statistical guarantees of\nPAR-regularized linear regression problems; specifically, we can approximate\nclassical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex\nregularizations using PAR and obtain similar statistical guarantees with\nquantized solutions.", "AI": {"tldr": "This paper explores Piecewise-Affine Regularization (PAR) for optimizing problems with discrete variables, particularly in supervised learning. It provides theoretical insights into PAR, showing that it leads to highly quantized solutions in overparameterized settings. The study also derives practical methods for solving PAR-regularized problems and demonstrates that PAR can achieve statistical guarantees comparable to traditional regularization techniques while producing quantized results.", "motivation": "The motivation is to address the challenges of optimization problems with discrete or quantized variables due to their complex search spaces, by leveraging the flexible modeling and computational framework of piecewise-affine regularization (PAR) based on continuous optimization.", "method": "The paper analyzes the theoretical foundations of PAR from optimization and statistical perspectives. It demonstrates that in the overparameterized regime, critical points of the PAR-regularized loss function exhibit high quantization. It also derives closed-form proximal mappings for various PARs and solves PAR-regularized problems using proximal gradient methods and ADMM.", "result": "The paper shows that in the overparameterized regime, every critical point of the PAR-regularized loss function exhibits a high degree of quantization. It derives closed-form proximal mappings for various PARs and demonstrates their solvability using proximal gradient methods and ADMM. Furthermore, it studies the statistical guarantees of PAR-regularized linear regression, showing approximations to classical formulations with similar statistical guarantees.", "conclusion": "PAR-regularized linear regression problems can approximate classical formulations and achieve similar statistical guarantees with quantized solutions."}}
{"id": "2508.11487", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2508.11487", "abs": "https://arxiv.org/abs/2508.11487", "authors": ["Ben Foxman", "Natalie Parham", "Francisca Vasconcelos", "Henry Yuen"], "title": "Random Unitaries in Constant (Quantum) Time", "comment": null, "summary": "Random unitaries are a central object of study in quantum information, with\napplications to quantum computation, quantum many-body physics, and quantum\ncryptography. Recent work has constructed unitary designs and pseudorandom\nunitaries (PRUs) using $\\Theta(\\log \\log n)$-depth unitary circuits with\ntwo-qubit gates.\n  In this work, we show that unitary designs and PRUs can be efficiently\nconstructed in several well-studied models of $\\textit{constant-time}$ quantum\ncomputation (i.e., the time complexity on the quantum computer is independent\nof the system size). These models are constant-depth circuits augmented with\ncertain nonlocal operations, such as (a) many-qubit TOFFOLI gates, (b)\nmany-qubit FANOUT gates, or (c) mid-circuit measurements with classical\nfeedforward control. Recent advances in quantum computing hardware suggest\nexperimental feasibility of these models in the near future.\n  Our results demonstrate that unitary designs and PRUs can be constructed in\nmuch weaker circuit models than previously thought. Furthermore, our\nconstruction of PRUs in constant-depth with many-qubit TOFFOLI gates shows\nthat, under cryptographic assumptions, there is no polynomial-time learning\nalgorithm for the circuit class $\\mathsf{QAC}^0$. Finally, our results suggest\na new approach towards proving that PARITY is not computable in\n$\\mathsf{QAC}^0$, a long-standing question in quantum complexity theory.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u66f4\u5f31\u7684\u91cf\u5b50\u8ba1\u7b97\u6a21\u578b\uff08\u5982\u5177\u6709\u591a\u91cf\u5b50\u6bd4\u7279\u95e8\u6216\u4e2d\u9014\u6d4b\u91cf\u7684\u6052\u5b9a\u6df1\u5ea6\u7535\u8def\uff09\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6784\u9020\u968f\u673a\u5e7a\u6b63\u3002\u8fd9\u5728\u91cf\u5b50\u8ba1\u7b97\u548c\u590d\u6742\u6027\u7406\u8bba\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u5e7a\u6b63\uff08random unitaries\uff09\u5728\u91cf\u5b50\u4fe1\u606f\u3001\u91cf\u5b50\u8ba1\u7b97\u3001\u91cf\u5b50\u591a\u4f53\u7269\u7406\u548c\u91cf\u5b50\u5bc6\u7801\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u8fd1\u671f\u5173\u4e8e\u4f7f\u7528$\\\\(n)$\u6df1\u5ea6\u5e7a\u6b63\u7535\u8def\u6784\u9020\u5e7a\u6b63\u8bbe\u8ba1\u548c\u4f2a\u968f\u673a\u5e7a\u6b63\uff08PRUs\uff09\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5728\u5177\u6709\u591a\u91cf\u5b50\u6bd4\u7279Toffoli\u95e8\u3001\u591a\u91cf\u5b50\u6bd4\u7279FANOUT\u95e8\u6216\u5e26\u7ecf\u5178\u524d\u9988\u63a7\u5236\u7684\u4e2d\u9014\u6d4b\u91cf\u7b49\u975e\u5c40\u57df\u64cd\u4f5c\u7684\u6052\u5b9a\u6df1\u5ea6\u91cf\u5b50\u7535\u8def\u4e2d\u8fdb\u884c\u6784\u9020\u3002", "result": "1. \u8bc1\u660e\u4e86\u5e7a\u6b63\u8bbe\u8ba1\u548cPRUs\u53ef\u4ee5\u5728\u4e00\u4e9b\u7814\u7a76\u8f83\u591a\u7684\u6052\u5b9a\u65f6\u95f4\u91cf\u5b50\u8ba1\u7b97\u6a21\u578b\u4e2d\u6709\u6548\u6784\u9020\u3002 2. \u6784\u9020\u4e86\u5728\u5177\u6709\u591a\u91cf\u5b50\u6bd4\u7279Toffoli\u95e8\u7684\u6052\u5b9a\u6df1\u5ea6\u7535\u8def\u4e2d\u7684PRUs\uff0c\u8fd9\u8868\u660e\u5728\u5bc6\u7801\u5b66\u5047\u8bbe\u4e0b\uff0c\u4e0d\u5b58\u5728$\\\\\\mathsf{QAC}^0$\u7684\u7535\u8def\u7c7b\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u5b66\u4e60\u7b97\u6cd5\u3002 3. \u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u6765\u8bc1\u660ePARITY\u4e0d\u53ef\u5728$\\\\\\mathsf{QAC}^0$\u4e2d\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e4b\u524d\u8ba4\u4e3a\u66f4\u5f31\u7684\u91cf\u5b50\u8ba1\u7b97\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6784\u9020\u5e7a\u6b63\u8bbe\u8ba1\u548c\u4f2a\u968f\u673a\u5e7a\u6b63\uff08PRUs\uff09\u3002"}}
{"id": "2508.11479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11479", "abs": "https://arxiv.org/abs/2508.11479", "authors": ["Tatiana Zemskova", "Aleksei Staroverov", "Dmitry Yudin", "Aleksandr Panov"], "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation", "comment": null, "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.", "AI": {"tldr": "OVSegDT\u662f\u4e00\u79cd\u65b0\u7684Transformer\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u652f\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u884c\u4e3a\u4e0d\u5b89\u5168\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5728Open-vocabulary Object Goal Navigation\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u548c\u78b0\u649e\u6b21\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u578b\u6a21\u62df\u5668\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u573a\u666f\u4e2d\u6210\u529f\u7387\u9ad8\u4f46\u5728\u672a\u89c1\u8fc7\u7c7b\u522b\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u884c\u4e3a\u4e0d\u5b89\u5168\uff08\u9891\u7e41\u78b0\u649e\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOVSegDT\u7684\u8f7b\u91cf\u7ea7Transformer\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1. \u8bed\u4e49\u5206\u652f\uff1a\u4f7f\u7528\u76ee\u6807\u4e8c\u503c\u63a9\u7801\u7684\u7f16\u7801\u5668\u548c\u8f85\u52a9\u5206\u5272\u635f\u5931\u51fd\u6570\uff0c\u5c06\u6587\u672c\u76ee\u6807\u4e0e\u7cbe\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u76f8\u7ed3\u5408\u30022. \u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff1a\u4e00\u79cd\u6839\u636e\u7b56\u7565\u71b5\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u548c\u5f3a\u5316\u4fe1\u53f7\u7684\u6837\u672c\u7ea7\u8c03\u5ea6\u5668\uff0c\u6d88\u9664\u4e86\u624b\u52a8\u5207\u6362\u7684\u5f0a\u7aef\u3002", "result": "OVSegDT\u6a21\u578b\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u4e0e\u5728\u5df2\u89c1\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u4e14\u5728HM3D-OVON\u6570\u636e\u96c6\u7684\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e8640.1%\u7684\u6210\u529f\u7387\uff08SR\uff09\u548c20.9%\u7684SPL\uff08Split-based Performance Evaluation\uff09\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u5c06\u8bad\u7ec3\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u4e8633%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e862\u500d\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86\u4f4e\u63a8\u7406\u6210\u672c\uff08130M\u53c2\u6570\uff0c\u4ec5\u4f7f\u7528RGB\u8f93\u5165\uff09\u3002", "conclusion": "OVSegDT\u6a21\u578b\u5728HM3D-OVON\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff08\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u8fbe\u523040.1%\u7684\u6210\u529f\u7387\u548c20.9%\u7684SPL\uff09\uff0c\u5e76\u4e14\u65e0\u9700\u6df1\u5ea6\u3001\u91cc\u7a0b\u8868\u6216\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u4e8633%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e862\u500d\uff0c\u63a8\u7406\u6210\u672c\u4ecd\u7136\u5f88\u4f4e\uff08130M\u53c2\u6570\uff0c\u4ec5RGB\u8f93\u5165\uff09\u3002"}}
{"id": "2508.10963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10963", "abs": "https://arxiv.org/abs/2508.10963", "authors": ["Zixiang Yang", "Yue Ma", "Yinhan Zhang", "Shanhui Mo", "Dongrui Liu", "Linfeng Zhang"], "title": "EVCtrl: Efficient Control Adapter for Visual Generation", "comment": null, "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.", "AI": {"tldr": "EVCtrl\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u901a\u8fc7\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\u51cf\u5c11\u4e86ControlNet\u7684\u5ef6\u8fdf\u548c\u8ba1\u7b97\u91cf\uff0c\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3ControlNet\u5728\u8fdb\u884c\u53ef\u63a7\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65f6\u5b58\u5728\u7684\u5ef6\u8fdf\u589e\u52a0\u548c\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u9891\u751f\u6210\u573a\u666f\u3002", "method": "EVCtrl\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\u6765\u89e3\u51b3ControlNet\u7684\u5ef6\u8fdf\u548c\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u9996\u5148\u5206\u6790\u4e86DiT-ControlNet\u7684\u6bcf\u4e00\u5c42\u5bf9\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u54cd\u5e94\uff0c\u5e76\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u529f\u80fd\u533a\uff0c\u5229\u7528\u611f\u77e5\u5c40\u90e8\u6027\u7684\u7f13\u5b58\u6765\u805a\u7126\u4e8e\u771f\u6b63\u9700\u8981\u7684\u5c40\u90e8\u533a\u57df\uff0c\u8df3\u8fc7\u4e86\u5927\u90e8\u5206\u5168\u5c40\u533a\u57df\u7684\u5197\u4f59\u8ba1\u7b97\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u6709\u9009\u62e9\u5730\u7701\u7565\u4e0d\u5fc5\u8981\u7684\u53bb\u566a\u6b65\u9aa4\u6765\u5904\u7406\u65f6\u95f4\u5197\u4f59\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5728CogVideo-Controlnet\u3001Wan2.1-Controlnet\u548cFlux\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cEVCtrl\u5728\u56fe\u50cf\u548c\u89c6\u9891\u63a7\u5236\u751f\u6210\u65b9\u9762\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u63a7\u5236\u3002\u4f8b\u5982\uff0c\u5728CogVideo-Controlnet\u548cWan2.1-Controlnet\u4e0a\u5206\u522b\u5b9e\u73b0\u4e862.16\u500d\u548c2.05\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u6ca1\u6709\u4e0b\u964d\u3002", "conclusion": "EVCtrl\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u63a7\u5236\u9002\u914d\u5668\uff0c\u53ef\u5728\u4e0d\u8fdb\u884c\u6a21\u578b\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u964d\u4f4e\u5f00\u9500\uff0c\u6709\u6548\u89e3\u51b3\u4e86ControlNet\u5728\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u7684\u5ef6\u8fdf\u548c\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002\u901a\u8fc7\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\uff0cEVCtrl\u5728\u4fdd\u8bc1\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002"}}
{"id": "2508.11260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11260", "abs": "https://arxiv.org/abs/2508.11260", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.11144", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11144", "abs": "https://arxiv.org/abs/2508.11144", "authors": ["Gauri Jain", "Dominik Rothenh\u00e4usler", "Kirk Bansak", "Elisabeth Paulson"], "title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets", "comment": null, "summary": "Machine learning (ML) tasks often utilize large-scale data that is drawn from\nseveral distinct sources, such as different locations, treatment arms, or\ngroups. In such settings, practitioners often desire predictions that not only\nexhibit good overall accuracy, but also remain reliable within each source and\npreserve the differences that matter across sources. For instance, several\nasylum and refugee resettlement programs now use ML-based employment\npredictions to guide where newly arriving families are placed within a host\ncountry, which requires generating informative and differentiated predictions\nfor many and often small source locations. However, this task is made\nchallenging by several common characteristics of the data in these settings:\nthe presence of numerous distinct data sources, distributional shifts between\nthem, and substantial variation in sample sizes across sources. This paper\nintroduces Clustered Transfer Residual Learning (CTRL), a meta-learning method\nthat combines the strengths of cross-domain residual learning and adaptive\npooling/clustering in order to simultaneously improve overall accuracy and\npreserve source-level heterogeneity. We provide theoretical results that\nclarify how our objective navigates the trade-off between data quantity and\ndata quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5\nlarge-scale datasets. This includes a dataset from the national asylum program\nin Switzerland, where the algorithmic geographic assignment of asylum seekers\nis currently being piloted. CTRL consistently outperforms the benchmarks across\nseveral key metrics and when using a range of different base learners.", "AI": {"tldr": "CTRL is a meta-learning method that improves ML predictions accuracy and preserves source-level heterogeneity, outperforming benchmarks on large-scale datasets.", "motivation": "Machine learning (ML) tasks often utilize large-scale data that is drawn from several distinct sources, practitioners often desire predictions that not only exhibit good overall accuracy, but also remain reliable within each source and preserve the differences that matter across sources.", "method": "CTRL, a meta-learning method that combines the strengths of cross-domain residual learning and adaptive pooling/clustering in order to simultaneously improve overall accuracy and preserve source-level heterogeneity.", "result": "CTRL navigates the trade-off between data quantity and data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5 large-scale datasets. CTRL consistently outperforms the benchmarks across several key metrics and when using a range of different base learners.", "conclusion": "CTRL consistently outperforms the benchmarks across several key metrics and when using a range of different base learners."}}
{"id": "2508.11554", "categories": ["quant-ph", "cond-mat.stat-mech", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.11554", "abs": "https://arxiv.org/abs/2508.11554", "authors": ["Dimitris Moustos", "Obinna Abah"], "title": "Surpassing Carnot efficiency with relativistic motion", "comment": null, "summary": "Relativistic thermal devices offer a unique platform for understanding the\ninterplay between motion, quantum fields, and thermodynamics, revealing\nphenomena inaccessible to stationary systems. We consider a two-qubit SWAP heat\nengine whose working medium consists of inertially moving Unruh-DeWitt qubit\ndetectors, each coupled to a scalar quantum field in thermal equilibrium at a\ndistinct temperature. Relativistic motion causes the qubits to perceive\nfrequency-dependent effective temperatures that are either hotter or colder\nthan their respective reservoir temperature. We show that the relativistic\ntemperature shift, perhaps the qubit velocity, can be harnessed as a\nthermodynamic resource to enhance the work output and the efficiency at maximum\npower of the heat engine. We derive a generalized second law for a heat engine\nwith a moving working medium and demonstrate that it can exceed the standard\nCarnot bound defined by rest-frame temperatures.", "AI": {"tldr": "This paper uses a quantum heat engine with moving parts to show that relativistic effects can boost the engine's performance beyond classical limits, and derives a new version of the second law of thermodynamics.", "motivation": "Relativistic thermal devices provide a novel way to explore the interactions between motion, quantum fields, and thermodynamics, leading to phenomena not observable in static systems. This research specifically focuses on a heat engine utilizing relativistic effects.", "method": "The paper analyzes a two-qubit SWAP heat engine with inertially moving Unruh-DeWitt qubit detectors coupled to scalar quantum fields at different temperatures. It investigates how relativistic motion induces frequency-dependent effective temperatures for the qubits, which can differ from their reservoir temperatures.", "result": "The analysis demonstrates that the effective temperatures perceived by the moving qubits are dependent on frequency and can be higher or lower than their reservoir temperatures. This relativistic temperature shift acts as a resource to enhance the engine's performance (work output and efficiency at maximum power). Furthermore, a generalized second law is derived, indicating that such engines can exceed the Carnot efficiency limit.", "conclusion": "The relativistic temperature shift due to motion can be utilized as a thermodynamic resource to improve the work output and efficiency at maximum power of a two-qubit SWAP heat engine. The study also derives a generalized second law for heat engines with moving working media, showing they can surpass the Carnot bound."}}
{"id": "2508.11485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11485", "abs": "https://arxiv.org/abs/2508.11485", "authors": ["Hailiang Tang", "Tisheng Zhang", "Liqiang Wang", "Xin Ding", "Man Yuan", "Zhiyu Xiang", "Jujin Chen", "Yuhan Bian", "Shuangyan Liu", "Yuqing Wang", "Guan Wang", "Xiaoji Niu"], "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping", "comment": "10 pages, 12 figures", "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.", "AI": {"tldr": "i2Nav-Robot \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u878d\u5408\u5bfc\u822a\u548c\u5efa\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u673a\u5668\u4eba\uff08UGV\uff09\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u914d\u7f6e\u3001\u65f6\u95f4\u540c\u6b65\u3001\u5730\u9762\u771f\u5b9e\u6027\u4ee5\u53ca\u573a\u666f\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bfc\u822a\u548c\u5efa\u56fe\u6280\u672f\u53d1\u5c55\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a i2Nav-Robot \u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u4e3a\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u5bfc\u822a\u548c\u5efa\u56fe\u800c\u8bbe\u8ba1\u3002\u5b83\u96c6\u6210\u4e86\u591a\u79cd\u4f20\u611f\u5668\uff0c\u5305\u62ec\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\u30014D \u96f7\u8fbe\u3001\u7acb\u4f53\u6444\u50cf\u5934\u3001\u91cc\u7a0b\u8ba1\u3001GNSS \u63a5\u6536\u5668\u548c IMU\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u5341\u4e2a\u5927\u89c4\u6a21\u5e8f\u5217\uff0c\u6db5\u76d6\u4e86\u5ba4\u5916\u8857\u9053\u548c\u5ba4\u5185\u505c\u8f66\u573a\u7b49\u591a\u79cd UGV \u64cd\u4f5c\u573a\u666f\uff0c\u603b\u957f\u5ea6\u7ea6 17060 \u7c73\u3002\u5b83\u63d0\u4f9b\u4e86\u9ad8\u9891\u3001\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u4f4d\u7f6e\u5730\u9762\u771f\u5b9e\u6027\u6570\u636e\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5177\u6709\u4f18\u8d8a\u7684\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5df2\u88ab\u5341\u591a\u4e2a\u5f00\u6e90\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u8bc4\u4f30\u3002"}}
{"id": "2508.10972", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10972", "abs": "https://arxiv.org/abs/2508.10972", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).", "AI": {"tldr": "This paper explores using AI (VLMs) to simulate the vision of people with low vision. Researchers collected data from 40 participants and used it to prompt an AI. They found that giving the AI both information about the participants' vision and examples of their responses greatly improved the AI's accuracy in mimicking their perception. One good example was much better than many weak ones.", "motivation": "To investigate the potential of Vision Language Models (VLMs) in simulating the vision perception of low vision individuals for accessibility applications, an area not previously explored.", "method": " The study involved a survey with 40 low vision participants to collect vision information and image perception responses. This data was used to create prompts for VLMs (GPT-4o) to simulate agents. The agreement between VLM-generated responses and participants' original answers was evaluated.", "result": "VLMs tend to infer beyond specified abilities with minimal prompts, leading to low agreement (0.59). Providing only vision information or example responses also resulted in low agreement (0.59). However, combining both vision information and example responses significantly increased agreement (0.70, p < 0.0001). A single example incorporating both open-ended and multiple-choice responses showed significant improvement over using either type of example alone (p < 0.0001), with diminishing returns from additional examples (p > 0.05).", "conclusion": " VLMs can simulate the vision perception of low vision individuals, with prompt engineering significantly impacting accuracy. A combination of vision information and example responses in prompts yields the best results, and a single, well-crafted example is more effective than multiple weaker examples."}}
{"id": "2508.11280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLETToT\u7684\u65e0\u6807\u6ce8\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u601d\u7ef4\u6811\u6765\u8bc4\u4f30\u65c5\u6e38\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6210\u672c\u548c\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u5e76\u53d1\u73b0\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u5bf9\u8868\u73b0\u7684\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65c5\u6e38\u7b49\u7279\u5b9a\u9886\u57df\u7684\u8868\u73b0\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u5305\u62ec\u6807\u6ce8\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u6a21\u578b\u5e7b\u89c9\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fdb\u884c\u6709\u6548\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002", "method": "LETToT\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u9a8c\u8bc1\u5c42\u6b21\u5316\u601d\u7ef4\u6811\u7ec4\u4ef6\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u53cd\u9988\u548c\u901a\u7528\u8d28\u91cf\u7ef4\u5ea6\u6765\u8bc4\u4f30\u65c5\u6e38\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u6807\u6ce8\u6570\u636e\u3002", "result": "1. LETToT\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u7684\u4e13\u5bb6\u601d\u7ef4\u6811\uff0c\u5728\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad84.99%-14.15%\u7684\u76f8\u5bf9\u8d28\u91cf\u63d0\u5347\u30022. \u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff0832B-671B\u53c2\u6570\uff09\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\uff1aa) \u89c4\u6a21\u6cd5\u5219\u5728\u4e13\u4e1a\u9886\u57df\u4f9d\u7136\u9002\u7528\uff08DeepSeek-V3\u8868\u73b0\u6700\u4f73\uff09\uff0c\u4f46\u7ecf\u8fc7\u63a8\u7406\u589e\u5f3a\u7684\u5c0f\u578b\u6a21\u578b\uff08\u5982DeepSeek-R1-Distill-Llama-70B\uff09\u80fd\u591f\u7f29\u5c0f\u4e0e\u5927\u578b\u6a21\u578b\u7684\u5dee\u8ddd\uff1bb) \u5bf9\u4e8e\u53c2\u6570\u91cf\u5c0f\u4e8e72B\u7684\u6a21\u578b\uff0c\u663e\u5f0f\u7684\u63a8\u7406\u67b6\u6784\u5728\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff08p<0.05\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLETToT\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u65c5\u6e38\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528\u4e13\u5bb6\u751f\u6210\u7684\u63a8\u7406\u7ed3\u6784\uff08\u601d\u7ef4\u6811\uff09\uff0cLETToT\u514b\u670d\u4e86\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6a21\u578b\u5e7b\u89c9\u7b49\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u8bc4\u4f30\u7684\u8d28\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u5bf9\u65c5\u6e38\u9886\u57dfLLM\u8868\u73b0\u7684\u5f71\u54cd\u89c4\u5f8b\u3002"}}
{"id": "2508.11145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11145", "abs": "https://arxiv.org/abs/2508.11145", "authors": ["Huan Zhang", "Daokun Zhang", "Kexin Meng", "Geoffrey I. Webb"], "title": "Towards the Next-generation Bayesian Network Classifiers", "comment": null, "summary": "Bayesian network classifiers provide a feasible solution to tabular data\nclassification, with a number of merits like high time and memory efficiency,\nand great explainability. However, due to the parameter explosion and data\nsparsity issues, Bayesian network classifiers are restricted to low-order\nfeature dependency modeling, making them struggle in extrapolating the\noccurrence probabilities of complex real-world data. In this paper, we propose\na novel paradigm to design high-order Bayesian network classifiers, by learning\ndistributional representations for feature values, as what has been done in\nword embedding and graph representation learning. The learned distributional\nrepresentations are encoded with the semantic relatedness between different\nfeatures through their observed co-occurrence patterns in training data, which\nthen serve as a hallmark to extrapolate the occurrence probabilities of new\ntest samples. As a classifier design realization, we remake the K-dependence\nBayesian classifier (KDB) by extending it into a neural version, i.e.,\nNeuralKDB, where a novel neural network architecture is designed to learn\ndistributional representations of feature values and parameterize the\nconditional probabilities between interdependent features. A stochastic\ngradient descent based algorithm is designed to train the NeuralKDB model\nefficiently. Extensive classification experiments on 60 UCI datasets\ndemonstrate that the proposed NeuralKDB classifier excels in capturing\nhigh-order feature dependencies and significantly outperforms the conventional\nBayesian network classifiers, as well as other competitive classifiers,\nincluding two neural network based classifiers without distributional\nrepresentation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeuralKDB\u7684\u65b0\u578b\u9ad8\u9636\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u7279\u5f81\u503c\u7684\u5206\u5e03\u8868\u793a\u6765\u89e3\u51b3\u4f20\u7edf\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cNeuralKDB\u5728\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u53c2\u6570\u7206\u70b8\u548c\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u4ec5\u9650\u4e8e\u4f4e\u9636\u7279\u5f81\u4f9d\u8d56\u6027\u5efa\u6a21\uff0c\u96be\u4ee5\u63a8\u65ad\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u51fa\u73b0\u6982\u7387\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u9ad8\u9636\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u7279\u5f81\u503c\u7684\u5206\u5e03\u8868\u793a\u6765\u8bbe\u8ba1\u9ad8\u9636\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u4e86KDB\uff0c\u4f7f\u5176\u6210\u4e3a\u795e\u7ecf\u7248\u672c\uff08NeuralKDB\uff09\uff0c\u5229\u7528\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u5b66\u4e60\u7279\u5f81\u503c\u7684\u5206\u5e03\u8868\u793a\u5e76\u53c2\u6570\u5316\u76f8\u4e92\u4f9d\u8d56\u7279\u5f81\u4e4b\u95f4\u7684\u6761\u4ef6\u6982\u7387\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u7b97\u6cd5\u6765\u6709\u6548\u5730\u8bad\u7ec3NeuralKDB\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684NeuralKDB\u5206\u7c7b\u5668\u901a\u8fc7\u5b66\u4e60\u5206\u5e03\u8868\u793a\uff0c\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u572860\u4e2aUCI\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5206\u7c7b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u548c\u5176\u4ed6\u7ade\u4e89\u6027\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684NeuralKDB\u5206\u7c7b\u5668\u5728\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u4ee5\u53ca\u5176\u4ed6\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u5668\uff0c\u5305\u62ec\u4e24\u4e2a\u6ca1\u6709\u5206\u5e03\u8868\u793a\u5b66\u4e60\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u5668\u3002"}}
{"id": "2508.11606", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.11606", "abs": "https://arxiv.org/abs/2508.11606", "authors": ["V. V. Ignatyuk", "Ch. Samorodov"], "title": "Dependence of the recoherence times and recoherence increments on the state of phonon bath in a single qubit dephasing model", "comment": "13 pages, 5 figures", "summary": "The recoherence times $t^*$ and the maximum values of the recoherence\nincrements $\\gamma_{\\rm extr}$ are studied as functions of the bath parameters\nfor a single qubit dephasing model, prepared initially by a special kind of the\nnon-selective measurements. The recoherence/decoherence events (RDE), occurring\nat the initial stage of the system evolution, are found to be both similar and\ndifferent from the system dynamics at large times. For instance, in contrast to\nthe RDE observed on large time scales, the sub-Ohmic and Ohmic coupling regimes\nare more favourable for the short-time recoherence than the super-Ohmic one. On\nthe other hand, the short-time behaviour of the recoherence and the long-time\ndynamics of the decoherence are closely related: the domain of the ohmicity\nindexes, where the decoherence changes its type (from the complete to\nincomplete one), is, simultaneously, that of the weakest recoherence. The\nobtained results give us some hints about the basic characteristics of the\nenvironment, which might provide the most optimal values of $t^*$ and\n$\\gamma_{\\rm extr}$ in some sense.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5355\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u6a21\u578b\u4e2d\u76f8\u5e72\u56de\u8df3\u65f6\u95f4 t* \u548c\u76f8\u5e72\u56de\u8df3\u589e\u91cf \u03b3_extr \u4e0e\u6d74\u53c2\u6570\u7684\u5173\u7cfb\u3002\u53d1\u73b0\u77ed\u65f6\u95f4\u76f8\u5e72\u56de\u8df3\u548c\u957f\u65f6\u95f4\u9000\u76f8\u5e72\u884c\u4e3a\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u6307\u51fa\u4e86\u5f71\u54cd\u8fd9\u4e9b\u53c2\u6570\u7684\u73af\u5883\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u76f8\u5e72\u56de\u8df3\u65f6\u95f4 t* \u548c\u76f8\u5e72\u56de\u8df3\u589e\u91cf \u03b3_extr \u7684\u51fd\u6570\u884c\u4e3a\uff0c\u4ee5\u671f\u7406\u89e3\u73af\u5883\u7684\u57fa\u672c\u7279\u5f81\uff0c\u5e76\u627e\u5230\u6700\u4f18\u7684 t* \u548c \u03b3_extr\u3002", "method": "\u7814\u7a76\u4e86\u5355\u4e2a\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u7279\u6b8a\u7c7b\u578b\u7684\u975e\u9009\u62e9\u6027\u6d4b\u91cf\u5236\u5907\u7684\u521d\u59cb\u72b6\u6001\u4e0b\uff0c\u76f8\u5e72\u56de\u8df3\u65f6\u95f4 t* \u548c\u76f8\u5e72\u56de\u8df3\u589e\u91cf \u03b3_extr \u7684\u6700\u5927\u503c\u4f5c\u4e3a\u6d74\u53c2\u6570\u51fd\u6570\u7684\u884c\u4e3a\u3002", "result": "\u5728\u9000\u76f8\u5e72\u6a21\u578b\u4e2d\uff0c\u53d1\u73b0\u76f8\u5e72/\u9000\u76f8\u5e72\u4e8b\u4ef6\uff08RDE\uff09\u5728\u7cfb\u7edf\u6f14\u5316\u7684\u521d\u59cb\u9636\u6bb5\uff0c\u5176\u884c\u4e3a\u4e0e\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u65e2\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4e5f\u5b58\u5728\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u4e0e\u957f\u65f6\u6807\u4e0a\u89c2\u5bdf\u5230\u7684 RDE \u4e0d\u540c\uff0c\u5728\u77ed\u65f6\u95f4\u76f8\u5e72\u56de\u8df3\u65b9\u9762\uff0c\u6b21\u6b27\u59c6\u548c\u6b27\u59c6\u8026\u5408\u673a\u5236\u6bd4\u8d85\u6b27\u59c6\u8026\u5408\u673a\u5236\u66f4\u6709\u5229\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u77ed\u65f6\u95f4\u76f8\u5e72\u56de\u8df3\u884c\u4e3a\u4e0e\u957f\u65f6\u95f4\u9000\u76f8\u5e72\u52a8\u529b\u5b66\u5bc6\u5207\u76f8\u5173\uff1a\u9000\u76f8\u5e72\u7c7b\u578b\u53d1\u751f\u6539\u53d8\uff08\u4ece\u5b8c\u5168\u9000\u76f8\u5e72\u5230\u4e0d\u5b8c\u5168\u9000\u76f8\u5e72\uff09\u7684\u6b27\u59c6\u6307\u6570\u533a\u57df\uff0c\u540c\u65f6\u4e5f\u662f\u76f8\u5e72\u56de\u8df3\u6700\u5f31\u7684\u533a\u57df\u3002$\nThe recoherence times $t^*$ and the maximum values of the recoherence increments $\\gamma_{\\rm extr}$ are studied as functions of the bath parameters for a single qubit dephasing model, prepared initially by a special kind of the non-selective measurements. The recoherence/decoherence events (RDE), occurring at the initial stage of the system evolution, are found to be both similar and different from the system dynamics at large times. For instance, in contrast to the RDE observed on large time scales, the sub-Ohmic and Ohmic coupling regimes are more favourable for the short-time recoherence than the super-Ohmic one. On the other hand, the short-time behaviour of the recoherence and the long-time dynamics of the decoherence are closely related: the domain of the ohmicity indexes, where the decoherence changes its type (from the complete to incomplete one), is, simultaneously, that of the weakest recoherence.", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u73af\u5883\u7684\u57fa\u672c\u7279\u5f81\u63d0\u4f9b\u4e86\u4e00\u4e9b\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u7279\u5f81\u53ef\u80fd\u5728\u67d0\u79cd\u610f\u4e49\u4e0a\u63d0\u4f9b\u6700\u4f18\u7684 t* \u548c \u03b3_extr \u503c\u3002"}}
{"id": "2508.11492", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "Polaris\uff1a\u4e00\u79cd\u5728\u6781\u5750\u6807\u7cfb\u4e2d\u8fd0\u884c\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u65b9\u6cd5\uff0c\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u5bf9\u81ea\u8f66\u4e0e\u5468\u56f4\u4ea4\u901a\u5143\u7d20\u7684\u76f8\u5bf9\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u5b83\u4e0d\u80fd\u81ea\u7136\u5730\u6355\u6349\u4e0d\u540c\u5143\u7d20\u57fa\u4e8e\u5176\u76f8\u5bf9\u8ddd\u79bb\u548c\u65b9\u5411\u53d8\u5316\u7684\u306e\u3092\u3002", "method": "Polaris \u65b9\u6cd5\u5b8c\u5168\u5728\u6781\u5750\u6807\u7cfb\u4e2d\u8fd0\u884c\uff0c\u5229\u7528\u6781\u5750\u6807\u8868\u793a\u6765\u663e\u5f0f\u5efa\u6a21\u8ddd\u79bb\u548c\u65b9\u5411\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u7684\u7f16\u7801\u548c\u7ec6\u5316\u6a21\u5757\u6355\u83b7\u76f8\u5bf9\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u5229\u7528\u6781\u5750\u6807\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u7ed3\u6784\u5316\u548c\u7a7a\u95f4\u611f\u77e5\u5730\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u3002", "conclusion": "Polaris \u5728\u5177\u6709\u6311\u6218\u6027\u7684\u9884\u6d4b (Argoverse 2) \u548c\u89c4\u5212\u57fa\u51c6 (nuPlan) \u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11011", "abs": "https://arxiv.org/abs/2508.11011", "authors": ["Xuezheng Chen", "Zhengbo Zou"], "title": "Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?", "comment": null, "summary": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aConstructionSite 10k\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5f20\u65bd\u5de5\u73b0\u573a\u56fe\u50cf\uff0c\u6db5\u76d6\u56fe\u50cf\u5b57\u5e55\u3001\u5b89\u5168\u89c4\u5219\u8fdd\u89c4VQA\u548c\u65bd\u5de5\u5143\u7d20\u89c6\u89c9\u57fa\u7840\u4e09\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u4e3a\u65bd\u5de5\u5b89\u5168\u68c0\u67e5\u9886\u57df\u7684VLM\u8bc4\u4f30\u548c\u5fae\u8c03\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u5f53\u524dVLM\u5728\u65bd\u5de5\u5b89\u5168\u68c0\u67e5\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u3001\u76d1\u7763\u5f0f\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u672a\u7ecf\u76f4\u63a5\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u5f00\u653e\u6570\u636e\u96c6\u6765\u5168\u9762\u8bc4\u4f30\u548c\u8fdb\u4e00\u6b65\u5fae\u8c03VLM\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b10,000\u5f20\u65bd\u5de5\u73b0\u573a\u56fe\u50cf\u7684\u6570\u636e\u96c6ConstructionSite 10k\uff0c\u5e76\u6ce8\u91ca\u4e86\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u4efb\u52a1\uff1a\u56fe\u50cf\u5b57\u5e55\u3001\u5b89\u5168\u89c4\u5219\u8fdd\u89c4\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u65bd\u5de5\u5143\u7d20\u89c6\u89c9\u57fa\u7840\u3002", "result": "\u5bf9\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5927\u578b\u9884\u8bad\u7ec3VLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5b83\u4eec\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5177\u6709\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u989d\u5916\u8bad\u7ec3\u624d\u80fd\u5e94\u7528\u4e8e\u5b9e\u9645\u65bd\u5de5\u73b0\u573a\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u6709VLM\u65b0\u67b6\u6784\u548c\u6280\u672f\u7684\u5e73\u53f0\uff0c\u4e3a\u65bd\u5de5\u5b89\u5168\u68c0\u67e5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.11281", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.11281", "abs": "https://arxiv.org/abs/2508.11281", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TOXIFRENCH\u6570\u636e\u96c6\uff0c\u5e76\u53d1\u73b0\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u6cd5\u8bed\u6bd2\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8eGPT-40\u548cGemini-2.5\u3002\u8be5\u65b9\u6cd5\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6cd5\u8bed\u6bd2\u6027\u68c0\u6d4b\u4e2d\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u6a21\u578b\u89c4\u6a21\u4e0e\u6bd2\u6027\u68c0\u6d4b\u4efb\u52a1\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5305\u542b53,622\u4e2a\u6cd5\u8bed\u5728\u7ebf\u8bc4\u8bba\u7684TOXIFRENCH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u6807\u6ce8\u548c\u4eba\u5de5\u9a8c\u8bc1\u7684\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u540c\u65f6\u5bf9\u5305\u62ec\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u5185\u7684\u4e00\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u5fae\u8c03\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u7528\u52a8\u6001\u52a0\u6743\u635f\u5931\u6765\u9010\u6b65\u5f3a\u8c03\u6a21\u578b\u7684\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u6240\u63d0\u51fa\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5fae\u8c03\u540e\u76844B\u6a21\u578b\u5728F1\u5206\u6570\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8613%\uff0c\u5e76\u4e14\u4f18\u4e8eGPT-40\u548cGemini-2.5\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u68c0\u6d4b\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u6bd2\u6027\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u8a00\u548c\u5b89\u5168\u5173\u952e\u7684\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2508.11159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11159", "abs": "https://arxiv.org/abs/2508.11159", "authors": ["Heqiang Wang", "Weihong Yang", "Xiaoxiong Zhong", "Jia Zhou", "Fangming Liu", "Weizhe Zhang"], "title": "Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning", "comment": "arXiv admin note: text overlap with arXiv:2505.16138", "summary": "The Internet of Things (IoT) ecosystem produces massive volumes of multimodal\ndata from diverse sources, including sensors, cameras, and microphones. With\nadvances in edge intelligence, IoT devices have evolved from simple data\nacquisition units into computationally capable nodes, enabling localized\nprocessing of heterogeneous multimodal data. This evolution necessitates\ndistributed learning paradigms that can efficiently handle such data.\nFurthermore, the continuous nature of data generation and the limited storage\ncapacity of edge devices demand an online learning framework. Multimodal Online\nFederated Learning (MMO-FL) has emerged as a promising approach to meet these\nrequirements. However, MMO-FL faces new challenges due to the inherent\ninstability of IoT devices, which often results in modality quantity and\nquality imbalance (QQI) during data collection. In this work, we systematically\ninvestigate the impact of QQI within the MMO-FL framework and present a\ncomprehensive theoretical analysis quantifying how both types of imbalance\ndegrade learning performance. To address these challenges, we propose the\nModality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning\nbased method designed to operate in parallel with the training process.\nExtensive experiments on two real-world multimodal datasets show that the\nproposed QQR algorithm consistently outperforms benchmarks under modality\nimbalance conditions with promising learning performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7269\u8054\u7f51\u591a\u6a21\u6001\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\uff08MMO-FL\uff09\u4e2d\u7684\u6a21\u6001\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\uff08QQI\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQQR\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u591f\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff0c\u8fd9\u9700\u8981\u5206\u5e03\u5f0f\u5b66\u4e60\u8303\u5f0f\u3002\u540c\u65f6\uff0c\u8fb9\u7f18\u8bbe\u5907\u7684\u5b58\u50a8\u9650\u5236\u548c\u6570\u636e\u7684\u8fde\u7eed\u6027\u9700\u8981\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\u3002MMO-FL\u867d\u7136\u6709\u524d\u666f\uff0c\u4f46\u9762\u4e34\u8bbe\u5907\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u7684\u6570\u636e\u6a21\u6001\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\uff08QQI\uff09\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQQR\u7684\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u884c\u64cd\u4f5c\uff0c\u4ee5\u89e3\u51b3MMO-FL\u4e2d\u7684\u6a21\u6001\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684QQR\u7b97\u6cd5\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8de8\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u7684\u6761\u4ef6\u4e0b\uff0cQQR\u7b97\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "MMO-FL\u9762\u4e34\u7684\u6a21\u6001\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\uff08QQI\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aQQR\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cQQR\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002"}}
{"id": "2508.11610", "categories": ["quant-ph", "hep-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2508.11610", "abs": "https://arxiv.org/abs/2508.11610", "authors": ["Shvetaank Tripathi", "Sandeep Joshi", "Garima Rajpoot", "Prashant Shukla"], "title": "Quantum Simulation of Collective Neutrino Oscillations in Dense Neutrino Environment", "comment": "23 pages, 10 figures", "summary": "Inside dense neutrino gases, such as neutron star mergers or core-collapse\nsupernovae, collective neutrino effects cause the transformation of one\nneutrino flavour into another. Due to strong neutrino self-interactions in\nthese environments, there is prevalence of flavour swapping. Considering these\nenvironments to be isotropic and homogeneous, we present a study of collective\nneutrino oscillations by simulating such a system on a noisy quantum simulator\n(Qiskit AerSimulator) and a quantum processor (ibm\\_brisbane). We model the\neffective Hamiltonian governing neutrino interactions and by applying the\nTrotter-Suzuki approximation, decompose it into a tractable form suitable for\nquantum circuit implementation of the time-evolution propagator. Encoding the\nneutrino state for a system of two- and three-neutrinos onto qubits, we compute\nthe time evolution of the inversion probability relative to the initial product\nstate. Furthermore, we present quantum circuits to evaluate the concurrence as\na measure of entanglement between the neutrinos.", "AI": {"tldr": "Neutrino oscillations in dense environments like neutron star mergers were simulated on quantum computers. The study modeled neutrino interactions, decomposed them for quantum circuits, and calculated flavour transformation and entanglement.", "motivation": "The motivation was to study collective neutrino oscillations within dense neutrino gases, such as those found in neutron star mergers or core-collapse supernovae, where flavour transformation and swapping occur due to strong self-interactions.", "method": "The study modeled the effective Hamiltonian governing neutrino interactions and used the Trotter-Suzuki approximation to decompose it for quantum circuit implementation. The neutrino state for two- and three-neutrino systems was encoded onto qubits to compute the time evolution of the inversion probability and evaluate entanglement using quantum circuits.", "result": "The study computed the time evolution of the inversion probability relative to the initial product state for two- and three-neutrino systems and presented quantum circuits to evaluate the concurrence as a measure of entanglement between neutrinos.", "conclusion": "Inside dense neutrino gases, collective neutrino effects cause flavour transformation, with prevalence of flavour swapping due to strong neutrino self-interactions. This study simulated these collective neutrino oscillations on quantum simulators and processors."}}
{"id": "2508.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11498", "abs": "https://arxiv.org/abs/2508.11498", "authors": ["Agnes Bressan de Almeida", "Joao Aires Correa Fernandes Marsicano"], "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language", "comment": null, "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.", "AI": {"tldr": "Swarm in Blocks is a block-based programming tool that simplifies drone swarm management for beginners, built on the Clover platform.", "motivation": "To address the growing complexities of managing drone swarms, especially for beginners, and to make swarm handling accessible without extensive knowledge of ROS or programming.", "method": "Using a block-based language interface built on the Clover platform to simplify drone swarm programming, allowing users to create functionalities like loops and conditional structures by assembling code blocks.", "result": "The development of Swarm in Blocks and Swarm in Blocks 2.0, which simplifies swarm control and offers educational benefits.", "conclusion": "Swarm in Blocks simplifies drone swarm programming with a block-based language, making it accessible for beginners and expanding educational opportunities."}}
{"id": "2508.11021", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11021", "abs": "https://arxiv.org/abs/2508.11021", "authors": ["Zisheng Liang", "Kidus Zewde", "Rudra Pratap Singh", "Disha Patil", "Zexi Chen", "Jiayu Xue", "Yao Yao", "Yifei Chen", "Qinzhe Liu", "Simiao Ren"], "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?", "comment": "arXiv admin note: text overlap with arXiv:2503.20084", "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u80fd\u529b\u4e0e\u51c6\u786e\u5ea6\u7684\u76f8\u5173\u6027\u6709\u9650\uff0c\u5f3a\u8c03\u4e86\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6587\u6863\u6b3a\u8bc8\u5bf9\u4f9d\u8d56\u5b89\u5168\u548c\u53ef\u9a8c\u8bc1\u6587\u4ef6\u7684\u884c\u4e1a\u6784\u6210\u4e86\u91cd\u5927\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u5927\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u5305\u62ec OpenAI O1\u3001OpenAI 4o\u3001Gemini Flash\uff08\u601d\u8003\uff09\u3001Deepseek Janus\u3001Grok\u3001Llama 3.2 \u548c 4\u3001Qwen 2 \u548c 2.5 VL\u3001Mistral Pixtral \u4ee5\u53ca Claude 3.5 \u548c 3.7 Sonnet \u5728\u5185\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u6b3a\u8bc8\u6587\u4ef6\u65b9\u9762\u7684\u6548\u679c\u3002\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u548c\u5bf9\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u5728\u4e00\u5305\u542b\u771f\u5b9e\u4ea4\u6613\u6587\u4ef6\u7684\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u5f7c\u6b64\u4ee5\u53ca\u5148\u524d\u5728\u6587\u6863\u6b3a\u8bc8\u68c0\u6d4b\u6280\u672f\u65b9\u9762\u7684\u5de5\u4f5c\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8868\u73b0\u6700\u4f73\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u975e\u5206\u5e03\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u800c\u4e00\u4e9b\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5219\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u6216\u4e0d\u4f73\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u4e0e\u68c0\u6d4b\u51c6\u786e\u5ea6\u7684\u76f8\u5173\u6027\u6709\u9650\uff0c\u8fd9\u8868\u660e\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u6587\u6863\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u6b3a\u8bc8\u7f13\u89e3\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11285", "abs": "https://arxiv.org/abs/2508.11285", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "AI": {"tldr": "\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56de\u5e94\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\uff0c\u60c5\u7eea\u8868\u8fbe\u5dee\u5f02\u5f88\u5927\u3002\u6a21\u578b\u9009\u62e9\u548c\u95ee\u9898\u7c7b\u578b\uff08\u6291\u90c1\u3001\u7126\u8651\u3001\u538b\u529b\uff09\u5bf9\u60c5\u7eea\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u7528\u6237\u753b\u50cf\u7684\u5f71\u54cd\u8f83\u5c0f\u3002Mixtral\u503e\u5411\u4e8e\u8d1f\u9762\u60c5\u7eea\uff0cLlama\u5219\u66f4\u4e50\u89c2\u3002", "motivation": "\u968f\u7740\u6291\u90c1\u3001\u7126\u8651\u548c\u538b\u529b\u7b49\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u666e\u904d\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u83b7\u53d6\u4fe1\u606f\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7814\u7a76LLMs\u5728\u56de\u5e94\u8fd9\u4e9b\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4e86\u516b\u79cdLLM\uff08Claude Sonnet, Copilot, Gemini Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity\uff09\u5bf9\u5305\u542b\u6291\u90c1\u3001\u7126\u8651\u3001\u538b\u529b\u7b49\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u7684\u4e8c\u5341\u79cd\u5b9e\u9645\u95ee\u9898\u7684\u56de\u5e94\uff0c\u8fd9\u4e9b\u95ee\u9898\u9488\u5bf9\u516d\u79cd\u7528\u6237\u753b\u50cf\uff08\u57fa\u7ebf\u3001\u5973\u6027\u3001\u7537\u6027\u3001\u5e74\u8f7b\u4eba\u3001\u8001\u5e74\u4eba\u3001\u5927\u5b66\u751f\uff09\u8fdb\u884c\u4e86\u8bbe\u5b9a\u3002\u5206\u6790\u4e86\u6a21\u578b\u751f\u6210\u76842880\u4e2a\u56de\u7b54\u7684\u60c5\u611f\u548c\u60c5\u7eea\u3002", "result": "\u6240\u6709LLM\u7684\u56de\u7b54\u4e2d\uff0c\u4e50\u89c2\u3001\u6050\u60e7\u548c\u60b2\u4f24\u662f\u4e3b\u8981\u60c5\u7eea\uff0c\u4e2d\u6027\u60c5\u611f\u5360\u6bd4\u8f83\u9ad8\u3002Mixtral\u6a21\u578b\u8868\u8fbe\u7684\u8d1f\u9762\u60c5\u7eea\uff08\u5982\u4e0d\u6ee1\u3001\u70e6\u607c\u3001\u60b2\u4f24\uff09\u6700\u591a\uff0c\u800cLlama\u6a21\u578b\u5219\u5c55\u73b0\u51fa\u6700\u4e50\u89c2\u3001\u6700\u6109\u5feb\u7684\u54cd\u5e94\u3002\u7126\u8651\u95ee\u9898\u5f15\u53d1\u4e86\u6781\u9ad8\u7684\u6050\u60e7\u5206\u6570\uff080.974\uff09\uff0c\u6291\u90c1\u95ee\u9898\u4ea7\u751f\u4e86\u8f83\u9ad8\u7684\u60b2\u4f24\u5206\u6570\uff080.686\uff09\u548c\u6700\u8d1f\u9762\u7684\u60c5\u7eea\uff0c\u800c\u538b\u529b\u76f8\u5173\u95ee\u9898\u5219\u5e26\u6765\u4e86\u6700\u4e50\u89c2\uff080.755\uff09\u548c\u8f83\u9ad8\u7684\u6109\u60a6\u53ca\u4fe1\u4efb\u5ea6\u3002\u7528\u6237\u753b\u50cf\u7684\u8bbe\u5b9a\u5bf9\u60c5\u7eea\u57fa\u8c03\u5f71\u54cd\u5f88\u5c0f\u3002\u6a21\u578b\u548c\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u662f\u5f71\u54cd\u60c5\u7eea\u8868\u8fbe\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "LLM\u5728\u56de\u5e94\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\uff0c\u5176\u60c5\u7eea\u8868\u8fbe\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6a21\u578b\u9009\u62e9\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.11180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11180", "abs": "https://arxiv.org/abs/2508.11180", "authors": ["Yiyang Shen", "Weiran Wang"], "title": "A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels", "comment": null, "summary": "Multi-view learning is widely applied to real-life datasets, such as multiple\nomics biological data, but it often suffers from both missing views and missing\nlabels. Prior probabilistic approaches addressed the missing view problem by\nusing a product-of-experts scheme to aggregate representations from present\nviews and achieved superior performance over deterministic classifiers, using\nthe information bottleneck (IB) principle. However, the IB framework is\ninherently fully supervised and cannot leverage unlabeled data. In this work,\nwe propose a semi-supervised generative model that utilizes both labeled and\nunlabeled samples in a unified framework. Our method maximizes the likelihood\nof unlabeled samples to learn a latent space shared with the IB on labeled\ndata. We also perform cross-view mutual information maximization in the latent\nspace to enhance the extraction of shared information across views. Compared to\nexisting approaches, our model achieves better predictive and imputation\nperformance on both image and multi-omics data with missing views and limited\nlabeled samples.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u7684\u7f3a\u5931\u89c6\u56fe\u548c\u7f3a\u5931\u6807\u7b7e\u95ee\u9898\u3002\u8be5\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5728\u56fe\u50cf\u548c\u591a\u7ec4\u5b66\u6570\u636e\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u5728\u5904\u7406\u7f3a\u5931\u89c6\u56fe\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46IB\u6846\u67b6\u7684\u5c40\u9650\u6027\u5728\u4e8e\u5176\u5b8c\u5168\u76d1\u7763\u7684\u6027\u8d28\uff0c\u65e0\u6cd5\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u7f3a\u5931\u89c6\u56fe\u548c\u7f3a\u5931\u6807\u7b7e\uff0c\u5e76\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u672a\u6807\u8bb0\u6837\u672c\u7684\u4f3c\u7136\u6765\u5b66\u4e60\u4e0e\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6267\u884c\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6765\u589e\u5f3a\u8de8\u89c6\u56fe\u5171\u4eab\u4fe1\u606f\u7684\u63d0\u53d6\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6a21\u578b\u5728\u56fe\u50cf\u548c\u591a\u7ec4\u5b66\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u548c\u586b\u5145\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u7f3a\u5931\u89c6\u56fe\u548c\u6709\u9650\u6807\u8bb0\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u534a\u76d1\u7763\u751f\u6210\u6a21\u578b\u901a\u8fc7\u8054\u5408\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u8de8\u89c6\u56fe\u5171\u4eab\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u5728\u5177\u6709\u7f3a\u5931\u89c6\u56fe\u548c\u6709\u9650\u6807\u8bb0\u6837\u672c\u7684\u56fe\u50cf\u548c\u591a\u7ec4\u5b66\u6570\u636e\u4e0a\u5747\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u9884\u6d4b\u548c\u586b\u5145\u6027\u80fd\u3002"}}
{"id": "2508.11503", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u5230\u771f\u5b9e\uff08sim-to-real\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u6837\u5316\u7684\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u5728\u6708\u7403\u6a21\u62df\u5730\u5f62\u4e0a\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7a0b\u5e8f\u5316\u591a\u6837\u6027\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u672a\u6765\u592a\u7a7a\u63a2\u7d22\u4e2d\u8de8\u8d8a\u9065\u8fdc\u884c\u661f\u8868\u9762\u975e\u7ed3\u6784\u5316\u5730\u5f62\u7684\u53ef\u9760\u81ea\u4e3b\u5bfc\u822a\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b66\u4e60\u63a7\u5236\u5728\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u7684\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u4e0e\u9897\u7c92\u4ecb\u8d28\u590d\u6742\u4ea4\u4e92\u52a8\u529b\u5b66\u65b9\u9762\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u7a0b\u5e8f\u5316\u751f\u6210\u73af\u5883\u4e2d\u968f\u673a\u5316\u7684\u7269\u7406\u7279\u6027\u3002\u7814\u7a76\u4e2d\u8fd8\u5bf9\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u548c\u52a8\u4f5c\u5e73\u6ed1\u6ee4\u6ce2\u5668\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u4ee5\u786e\u5b9a\u6700\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u7684\u7ec4\u5408\uff0c\u5e76\u5206\u6790\u4e86\u4f7f\u7528\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u8fdb\u884c\u5fae\u8c03\u7684\u6743\u8861\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u591a\u6837\u6027\u8bad\u7ec3\u7684\u4ee3\u7406\u76f8\u6bd4\u4e8e\u5728\u9759\u6001\u573a\u666f\u4e2d\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u5728\u96f6\u6837\u672c\uff08zero-shot\uff09\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u540c\u65f6\uff0c\u5728\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u65b9\u9762\uff0c\u867d\u7136\u80fd\u4ee5\u5fae\u5c0f\u7684\u6027\u80fd\u63d0\u5347\u6362\u53d6\u4f4e\u901f\u7cbe\u5ea6\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u663e\u8457\u8f83\u9ad8\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6a21\u62df\u5230\u771f\u5b9e\uff08sim-to-real\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc1\u5728\u6708\u7403\u6a21\u62df\u73af\u5883\u4e2d\u5177\u6709\u52a8\u6001\u8def\u5f84\u8ddf\u8e2a\u80fd\u529b\u7684\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u5728\u590d\u6742\u5730\u5f62\u4e0a\u90e8\u7f72\u81ea\u4e3b\u5bfc\u822a\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11032", "abs": "https://arxiv.org/abs/2508.11032", "authors": ["Yanwu Yang", "Guinan Su", "Jiesi Hu", "Francesco Sammarco", "Jonas Geiping", "Thomas Wolfers"], "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation", "comment": null, "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.", "AI": {"tldr": "MedSAMix\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8bad\u7ec3\u65e0\u5173\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u96f6\u9636\u4f18\u5316\u81ea\u52a8\u878d\u5408\u901a\u7528\u6a21\u578b\u548c\u4e13\u7528\u533b\u5b66\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u901a\u7528\u89c6\u89c9\u6a21\u578b\uff08\u5982SAM\uff09\u5fae\u8c03\u7684\u4e13\u7528\u6a21\u578b\uff08\u5982MedSAM\uff09\u5728\u9762\u5bf9\u591a\u6837\u6027\u3001\u6807\u6ce8\u7a00\u758f\u6027\u548c\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\u65f6\uff0c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6574\u5408\u901a\u7528\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u5e7f\u6cdb\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "MedSAMix\u662f\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u901a\u7528\u89c6\u89c9\u6a21\u578b\uff08\u5982SAM\uff09\u548c\u9488\u5bf9\u533b\u5b66\u5206\u5272\u4efb\u52a1\u5fae\u8c03\u7684\u4e13\u7528\u6a21\u578b\uff08\u5982MedSAM\uff09\u7684\u4f18\u70b9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u96f6\u9636\u4f18\u5316\u6280\u672f\u81ea\u52a8\u5bfb\u627e\u6700\u4f18\u7684\u5c42\u7ea7\u5408\u5e76\u7b56\u7565\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u3002\u6b64\u5916\uff0cMedSAMix\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u673a\u5236\uff1a\u5355\u4efb\u52a1\u4f18\u5316\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u5bf9\u9886\u57df\u7279\u5f02\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u9700\u6c42\u3002", "result": "\u572825\u4e2a\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMedSAMix\u80fd\u591f\u6709\u6548\u7f13\u89e3\u6a21\u578b\u504f\u5dee\uff0c\u5e76\u5728\u7279\u5b9a\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u591a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u5b9e\u73b0\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u5728\u4e13\u95e8\u4efb\u52a1\u4e0a\u63d0\u53476.67%\uff0c\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e0a\u63d0\u53474.37%\u3002", "conclusion": "MedSAMix\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u6a21\u578b\uff08\u5982SAM\uff09\u548c\u4e13\u7528\u6a21\u578b\uff08\u5982MedSAM\uff09\u7684\u4f18\u52bf\uff0c\u5e76\u91c7\u7528\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u81ea\u52a8\u5bfb\u627e\u6700\u4f18\u7684\u9010\u5c42\u5408\u5e76\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u3001\u6807\u6ce8\u7a00\u758f\u548c\u5206\u5e03\u504f\u79fb\u7684\u533b\u5b66\u5f71\u50cf\u6570\u636e\u65f6\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMedSAMix\u80fd\u591f\u51cf\u8f7b\u6a21\u578b\u504f\u5dee\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u591a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5206\u522b\u63d0\u9ad8\u4e866.67%\u548c4.37%\u3002"}}
{"id": "2508.11290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11290", "abs": "https://arxiv.org/abs/2508.11290", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "AI": {"tldr": "LLMs over-refuse benign instructions due to safety mechanisms. This paper introduces SafeConstellations, an inference-time method that analyzes embedding space trajectories to guide models away from refusal, reducing over-refusal by up to 73% with minimal impact on utility.", "motivation": "LLMs exhibit over-refusal behavior, rejecting benign instructions that resemble harmful content, which diminishes their utility in production applications. This paper aims to address this issue by analyzing the underlying mechanisms and proposing a mitigation strategy.", "method": "SafeConstellations, an inference-time trajectory-shifting approach that tracks task-specific trajectory patterns in embedding space and guides representations toward non-refusal pathways to mitigate over-refusal behavior in LLMs.", "result": "Comprehensive evaluation demonstrates that LLMs still tend to refuse harmful instructions even when reframed as benign. Mechanistic analysis reveals distinct 'constellation' patterns in embedding space during response generation. SafeConstellations reduces over-refusal rates by up to 73% with minimal impact on utility.", "conclusion": "LLMs' over-refusal behavior can be mitigated by SafeConstellations, an inference-time approach that guides model representations toward non-refusal pathways by tracking task-specific trajectory patterns. This method reduces over-refusal rates by up to 73% with minimal impact on utility, offering a principled way to address the issue."}}
{"id": "2508.11190", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2508.11190", "abs": "https://arxiv.org/abs/2508.11190", "authors": ["Feng-ao Wang", "Shaobo Chen", "Yao Xuan", "Junwei Liu", "Qi Gao", "Hongdong Zhu", "Junjie Hou", "Lixin Yuan", "Jinyu Cheng", "Chenxin Yi", "Hai Wei", "Yin Ma", "Tao Xu", "Kai Wen", "Yixue Li"], "title": "Quantum-Boosted High-Fidelity Deep Learning", "comment": null, "summary": "A fundamental limitation of probabilistic deep learning is its predominant\nreliance on Gaussian priors. This simplistic assumption prevents models from\naccurately capturing the complex, non-Gaussian landscapes of natural data,\nparticularly in demanding domains like complex biological data, severely\nhindering the fidelity of the model for scientific discovery. The\nphysically-grounded Boltzmann distribution offers a more expressive\nalternative, but it is computationally intractable on classical computers. To\ndate, quantum approaches have been hampered by the insufficient qubit scale and\noperational stability required for the iterative demands of deep learning.\nHere, we bridge this gap by introducing the Quantum Boltzmann\nMachine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable\nhybrid quantum-classical architecture. Our framework leverages a quantum\nprocessor for efficient sampling from the Boltzmann distribution, enabling its\nuse as a powerful prior within a deep generative model. Applied to\nmillion-scale single-cell datasets from multiple sources, the QBM-VAE generates\na latent space that better preserves complex biological structures,\nconsistently outperforming conventional Gaussian-based deep learning models\nlike VAE and SCVI in essential tasks such as omics data integration, cell-type\nclassification, and trajectory inference. It also provides a typical example of\nintroducing a physics priori into deep learning to drive the model to acquire\nscientific discovery capabilities that breaks through data limitations. This\nwork provides the demonstration of a practical quantum advantage in deep\nlearning on a large-scale scientific problem and offers a transferable\nblueprint for developing hybrid quantum AI models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 QBM-VAE\uff0c\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u4ece\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u751f\u7269\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\u3002\u8be5\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21\u5355\u7ec6\u80de\u6570\u636e\u96c6\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u5c55\u73b0\u4e86\u5b9e\u9645\u7684\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4e3b\u8981\u4f9d\u8d56\u9ad8\u65af\u5148\u9a8c\u7684\u5c40\u9650\u6027\uff0c\u9ad8\u65af\u5148\u9a8c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u81ea\u7136\u6570\u636e\u590d\u6742\u7684\u975e\u9ad8\u65af\u666f\u89c2\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u751f\u7269\u6570\u636e\u7b49\u8981\u6c42\u82db\u523b\u7684\u9886\u57df\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4fdd\u771f\u5ea6\u3002\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u8868\u73b0\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a-\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08QBM-VAE\uff09\u7684\u5927\u89c4\u6a21\u3001\u957f\u65f6\u95f4\u7a33\u5b9a\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u3002\u8be5\u6846\u67b6\u5229\u7528\u91cf\u5b50\u5904\u7406\u5668\u4ece\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e2d\u8fdb\u884c\u9ad8\u6548\u91c7\u6837\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5f3a\u5927\u5148\u9a8c\u3002", "result": "QBM-VAE \u5728\u5904\u7406\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u767e\u4e07\u7ea7\u5355\u7ec6\u80de\u6570\u636e\u96c6\u65f6\uff0c\u751f\u6210\u7684\u6f5c\u5728\u7a7a\u95f4\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u590d\u6742\u7684\u751f\u7269\u7ed3\u6784\uff0c\u5728 omics \u6570\u636e\u96c6\u6210\u3001\u7ec6\u80de\u7c7b\u578b\u5206\u7c7b\u548c\u8f68\u8ff9\u63a8\u7406\u7b49\u57fa\u672c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e VAE \u548c SCVI \u7b49\u4f20\u7edf\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5728\u5927\u578b\u79d1\u5b66\u95ee\u9898\u4e0a\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u4e3a\u5f00\u53d1\u6df7\u5408\u91cf\u5b50\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.11058", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "AI": {"tldr": "Existing 3D vision-language datasets lack multi-view and multi-object reasoning capabilities. This paper introduces MV-ScanQA and TripAlign datasets to address these limitations, along with LEGO, a method that achieves state-of-the-art performance by transferring knowledge from 2D to 3D domains.", "motivation": "Existing 3D vision-language datasets have limitations, such as requiring reasoning only within a close range of objects from a single viewpoint and annotating instructions to single objects, missing richer contextual alignments between multiple objects. This hinders the development of models capable of deep, multi-view 3D scene understanding over distant objects.", "method": "The paper introduces two datasets: MV-ScanQA, which requires multi-view reasoning, and TripAlign, a large-scale pre-training corpus with 1M <2D view, set of 3D objects, text> triplets for multi-object alignment. They also propose LEGO, a baseline method for multi-view reasoning by transferring knowledge from 2D LVLMs to the 3D domain using TripAlign.", "result": "MV-ScanQA requires multi-view reasoning, with 68% of questions needing information from multiple views. TripAlign provides 1M multi-object alignment signals. LEGO, pre-trained on TripAlign, achieves state-of-the-art results on MV-ScanQA and existing 3D benchmarks.", "conclusion": "LEGO, which transfers knowledge from pre-trained 2D LVLMs to the 3D domain using TripAlign, achieves state-of-the-art performance on MV-ScanQA, 3D dense captioning, and question answering benchmarks."}}
{"id": "2508.11310", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "AI": {"tldr": "SGSimEval\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u95ee\u5377\u751f\u6210\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u6574\u5408\u5927\u7eb2\u3001\u5185\u5bb9\u3001\u53c2\u8003\u6587\u732e\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5206\u548c\u4eba\u7c7b\u504f\u597d\u6307\u6807\uff0c\u514b\u670d\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cASG\u7cfb\u7edf\u5728\u5927\u7eb2\u751f\u6210\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u751f\u6210\u4e0a\u9700\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u95ee\u5377\u751f\u6210\uff08ASG\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u6307\u6807\u504f\u5dee\u3001\u7f3a\u4e4f\u4eba\u7c7b\u504f\u597d\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u88c1\u5224\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGSimEval\u7684\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6574\u5408\u4e86\u5bf9\u95ee\u5377\u5927\u7eb2\u3001\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u7684\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u5206\u4e0e\u5b9a\u91cf\u6307\u6807\u3002SGSimEval\u8fd8\u5f15\u5165\u4e86\u5f3a\u8c03\u56fa\u6709\u8d28\u91cf\u548c\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u7684\u4eba\u7c7b\u504f\u597d\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u53d1\u73b0\uff0c\u73b0\u6709ASG\u7cfb\u7edf\u5728\u95ee\u5377\u5927\u7eb2\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u751f\u6210\u65b9\u9762\u4ecd\u6709\u663e\u8457\u7684\u6539\u8fdb\u7a7a\u95f4\u3002SGSimEval\u8bc4\u4f30\u6307\u6807\u80fd\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u73b0\u6709\u81ea\u52a8\u95ee\u5377\u751f\u6210\u7cfb\u7edf\u5728\u95ee\u5377\u5927\u7eb2\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u4eba\u7c7b\u53ef\u6bd4\u7684\u6c34\u5e73\uff0c\u4f46\u5728\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u751f\u6210\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002SGSimEval\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2508.11205", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11205", "abs": "https://arxiv.org/abs/2508.11205", "authors": ["Cheng Jing", "Uvini Balasuriya Mudiyanselage", "Woojin Cho", "Minju Jo", "Anthony Gruber", "Kookjin Lee"], "title": "Meta-learning Structure-Preserving Dynamics", "comment": null, "summary": "Structure-preserving approaches to dynamics modeling have demonstrated great\npotential for modeling physical systems due to their strong inductive biases\nthat enforce conservation laws and dissipative behavior. However, the resulting\nmodels are typically trained for fixed system configurations, requiring\nexplicit knowledge of system parameters as well as costly retraining for each\nnew set of parameters -- a major limitation in many-query or parameter-varying\nscenarios. Meta-learning offers a potential solution, but existing approaches\nlike optimization-based meta-learning often suffer from training instability or\nlimited generalization capability. Inspired by ideas from computer vision, we\nintroduce a modulation-based meta-learning framework that directly conditions\nstructure-preserving models on compact latent representations of potentially\nunknown system parameters, avoiding the need for gray-box system knowledge and\nexplicit optimization during adaptation. Through the application of novel\nmodulation strategies to parametric energy-conserving and dissipative systems,\nwe enable scalable and generalizable learning across parametric families of\ndynamical systems. Experiments on standard benchmark problems demonstrate that\nour approach achieves accurate predictions in few-shot learning settings,\nwithout compromising on the essential physical constraints necessary for\ndynamical stability and effective generalization performance across parameter\nspace.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c03\u5236\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u4fdd\u6301\u7684\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u6761\u4ef6\u5316\u4e8e\u7cfb\u7edf\u53c2\u6570\u7684\u6f5c\u5728\u8868\u793a\u6765\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u8de8\u53c2\u6570\u65cf\u7684\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u6cdb\u5316\u7684\u5b66\u4e60\uff0c\u5e76\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u51c6\u786e\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7ed3\u6784\u4fdd\u6301\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u56fa\u5b9a\u7684\u7cfb\u7edf\u914d\u7f6e\u8fdb\u884c\u8bad\u7ec3\uff0c\u9700\u8981\u663e\u5f0f\u7684\u7cfb\u7edf\u53c2\u6570\u77e5\u8bc6\uff0c\u5e76\u4e14\u5728\u6bcf\u6b21\u66f4\u6539\u53c2\u6570\u65f6\u90fd\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u5728\u8bb8\u591a\u67e5\u8be2\u6216\u53c2\u6570\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u662f\u4e00\u4e2a\u4e3b\u8981\u9650\u5236\u3002\u867d\u7136\u5143\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c03\u5236\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u76f4\u63a5\u5c06\u7ed3\u6784\u4fdd\u6301\u6a21\u578b\u6761\u4ef6\u5316\u4e8e\u6f5c\u5728\u7684\u3001\u672a\u77e5\u7684\u7cfb\u7edf\u53c2\u6570\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u7070\u8272\u76d2\u7cfb\u7edf\u77e5\u8bc6\u548c\u9002\u5e94\u671f\u95f4\u663e\u5f0f\u4f18\u5316\u7684\u9700\u6c42\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u95ee\u9898\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u8de8\u53c2\u6570\u7a7a\u95f4\u7684\u6709\u6548\u6cdb\u5316\u6027\u80fd\u6240\u5fc5\u9700\u7684\u57fa\u672c\u7269\u7406\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u65b0\u9896\u7684\u8c03\u5236\u7b56\u7565\u5c06\u7ed3\u6784\u4fdd\u6301\u6a21\u578b\u5e94\u7528\u4e8e\u53c2\u6570\u5316\u80fd\u91cf\u5b88\u6052\u548c\u8017\u6563\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u8de8\u8d8a\u52a8\u6001\u7cfb\u7edf\u53c2\u6570\u65cf\u7684\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u6cdb\u5316\u7684\u5b66\u4e60\u3002\u5728\u6807\u51c6\u57fa\u51c6\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u8de8\u53c2\u6570\u7a7a\u95f4\u7684\u6709\u6548\u6cdb\u5316\u6027\u80fd\u6240\u5fc5\u9700\u7684\u57fa\u672c\u7269\u7406\u7ea6\u675f\u3002"}}
{"id": "2508.11537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11537", "abs": "https://arxiv.org/abs/2508.11537", "authors": ["Han Zheng", "Zikang Zhou", "Guli Zhang", "Zhepei Wang", "Kaixuan Wang", "Peiliang Li", "Shaojie Shen", "Ming Yang", "Tong Qin"], "title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "comment": null, "summary": "Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.", "AI": {"tldr": "MultiPark \u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u505c\u8f66\u7684\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u7684\u9884\u6d4b\u8303\u5f0f\u548c\u9762\u5411\u7ed3\u679c\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u90e8\u7f72\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e86\u65e0\u8f66\u9053\u5f00\u653e\u7a7a\u95f4\u4e2d\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6a21\u6001\u6027\u8d28\uff0c\u672a\u80fd\u83b7\u5f97\u540c\u4e00\u60c5\u51b5\u4e0b\u7684\u591a\u79cd\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u5b58\u5728\u56fa\u6709\u7684\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u5728\u4e0d\u540c\u7684\u505c\u8f66\u573a\u666f\u4e2d\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiPark\u7684\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u505c\u8f66\u3002\u901a\u8fc7\u5f15\u5165\u6570\u636e\u9ad8\u6548\u7684\u4e0b\u4e00\u4e2a\u8def\u6bb5\u9884\u6d4b\u8303\u5f0f\u6765\u5904\u7406\u5305\u542b\u6025\u8f6c\u5f2f\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u505c\u8f66\u67e5\u8be2\uff08\u5206\u4e3a\u6863\u4f4d\u3001\u7eb5\u5411\u548c\u6a2a\u5411\u7ec4\u4ef6\uff09\u4ee5\u5e76\u884c\u89e3\u7801\u4e0d\u540c\u7684\u505c\u8f66\u884c\u4e3a\u3002\u4e3a\u4e86\u51cf\u8f7b\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u56e0\u679c\u6df7\u6dc6\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4ee5\u76ee\u6807\u4e3a\u4e2d\u5fc3\u7684\u59ff\u6001\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u78b0\u649e\u4f5c\u4e3a\u8d85\u8d8a\u7eaf\u6a21\u4eff\u635f\u5931\u7684\u9762\u5411\u7ed3\u679c\u7684\u635f\u5931\u3002", "result": "MultiPark \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u91cf\u4ea7\u8f66\u4e0a\u8fdb\u884c\u4e86\u90e8\u7f72\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u771f\u5b9e\u505c\u8f66\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MultiPark\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u91cf\u4ea7\u8f66\u4e0a\u8fdb\u884c\u4e86\u90e8\u7f72\uff0c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u505c\u8f66\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11063", "abs": "https://arxiv.org/abs/2508.11063", "authors": ["Lucas W. Remedios", "Chloe Choe", "Trent M. Schwartz", "Dingjie Su", "Gaurav Rudravaram", "Chenyu Gao", "Aravind R. Krishnan", "Adam M. Saunders", "Michael E. Kim", "Shunxing Bao", "Alvin C. Powers", "Bennett A. Landman", "John Virostko"], "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts", "comment": null, "summary": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528 AI \u5206\u6790\u8179\u90e8 CT \u5f71\u50cf\uff0c\u53d1\u73b0\u5185\u810f\u8102\u80aa\u3001\u76ae\u4e0b\u8102\u80aa\u3001\u9aa8\u9abc\u808c\u8102\u80aa\u5316\u548c\u80f0\u817a\u7279\u5f81\u662f 2 \u578b\u7cd6\u5c3f\u75c5\u7684\u5171\u540c\u98ce\u9669\u56e0\u7d20\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5728\u4e0d\u540c\u4f53\u91cd\u7c7b\u522b\u4eba\u7fa4\u4e2d\u5747\u5b58\u5728\u3002", "motivation": "\u5c3d\u7ba1\u9ad8 BMI \u662f 2 \u578b\u7cd6\u5c3f\u75c5\u7684\u5df2\u77e5\u98ce\u9669\u56e0\u7d20\uff0c\u4f46\u5728\u4e00\u4e9b\u6d88\u7626\u6210\u4eba\u4e2d\u51fa\u73b0\u8be5\u75be\u75c5\uff0c\u800c\u5728\u80a5\u80d6\u8005\u4e2d\u5374\u4e0d\u5b58\u5728\uff0c\u8fd9\u8868\u660e\u8be6\u7ec6\u7684\u8eab\u4f53\u6210\u5206\u53ef\u80fd\u63ed\u793a 2 \u578b\u7cd6\u5c3f\u75c5\u7684\u8179\u90e8\u8868\u578b\u3002\u5229\u7528 AI \u6280\u672f\uff0c\u53ef\u4ee5\u5927\u89c4\u6a21\u5730\u4ece\u8179\u90e8 3D \u4e34\u5e8a\u5f71\u50cf\u4e2d\u63d0\u53d6\u8be6\u7ec6\u7684\u5c3a\u5bf8\u3001\u5f62\u72b6\u548c\u8102\u80aa\u542b\u91cf\u6d4b\u91cf\u6570\u636e\uff0c\u4ece\u800c\u63d0\u4f9b\u4e00\u4e2a\u7ecf\u9a8c\u6027\u5b9a\u4e49\u4e0e 2 \u578b\u7cd6\u5c3f\u75c5\u98ce\u9669\u548c\u4fdd\u62a4\u76f8\u5173\u7684\u8eab\u4f53\u6210\u5206\u7279\u5f81\u7684\u673a\u4f1a\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6280\u672f\uff0c\u5bf9\u8179\u90e8 CT \u5f71\u50cf\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u53d6\u8eab\u4f53\u6210\u5206\u7279\u5f81\u3002\u7814\u7a76\u5e94\u7528\u4e86\u968f\u673a\u68ee\u6797\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7 SHAP \u5206\u6790\u8bc4\u4f30\u7279\u5f81\u5bf9\u6a21\u578b\u9884\u6d4b\u98ce\u9669\u6216\u4fdd\u62a4\u7684\u8d21\u732e\u5ea6\u3002\u6700\u540e\uff0c\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u548c\u89e3\u5256\u5b66\u5173\u8054\uff0c\u8bc6\u522b\u4e0e 2 \u578b\u7cd6\u5c3f\u75c5\u76f8\u5173\u7684\u8179\u90e8\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u5728\u4e0d\u540c BMI \u5206\u7ec4\uff08\u603b\u961f\u5217\u3001\u6d88\u7626\u3001\u8d85\u91cd\u3001\u80a5\u80d6\uff09\u4e2d\u5747\u5b9e\u73b0\u4e86 0.72-0.74 \u7684\u5e73\u5747 AUC\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u5404\u5206\u7ec4\u4e2d\u5171\u540c\u7684 2 \u578b\u7cd6\u5c3f\u75c5\u7279\u5f81\uff0c\u5305\u62ec\u8102\u80aa\u6d78\u6da6\u7684\u9aa8\u9abc\u808c\u3001\u5e74\u9f84\u8f83\u5927\u3001\u5185\u810f\u548c\u76ae\u4e0b\u8102\u80aa\u589e\u52a0\uff0c\u4ee5\u53ca\u80f0\u817a\u4f53\u79ef\u8f83\u5c0f\u6216\u8102\u80aa\u542b\u91cf\u8f83\u9ad8\u3002\u5355\u53d8\u91cf\u903b\u8f91\u56de\u5f52\u8bc1\u5b9e\u4e86 14-18 \u4e2a\u9884\u6d4b\u56e0\u5b50\u5728\u5404\u4e9a\u7ec4\u4e2d\u7684\u9884\u6d4b\u65b9\u5411\uff08p < 0.05\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8179\u90e8\u56e0\u7d20\u5bf9 2 \u578b\u7cd6\u5c3f\u75c5\u7684\u5f71\u54cd\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u4f53\u91cd\u7c7b\u522b\u4e2d\u662f\u4e00\u81f4\u7684\u3002"}}
{"id": "2508.11318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11318", "abs": "https://arxiv.org/abs/2508.11318", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ond\u0159ej Bojar"], "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e864\u4f4dGSQ\u548cGPTQ\u91cf\u5316\u6280\u672f\u5728LLaMA 1B\u3001Qwen 0.5B\u548cPHI 1.5B\u6a21\u578b\u4e0a\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u6a21\u578b\u538b\u7f29\u4e0e\u4efb\u52a1\u8868\u73b0\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3aLLM\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8LLM\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u901a\u8fc7\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4f4e\u6bd4\u7279\u91cf\u5316\u6280\u672f\u5bf9\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u548cNLP\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\u3002", "method": "\u5c064\u4f4dGSQ\u548cGPTQ\u91cf\u5316\u6280\u672f\u5e94\u7528\u4e8eLLaMA 1B\u3001Qwen 0.5B\u548cPHI 1.5B\u6a21\u578b\uff0c\u5e76\u5728MS MARCO\u3001BoolQ\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u91cf\u5316\u6280\u672f\u5728\u6240\u6d4b\u8bd5\u7684\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u6743\u8861\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7528\u6237\u6839\u636e\u7279\u5b9a\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u901a\u8fc7\u5728LLaMA 1B\u3001Qwen 0.5B\u548cPHI 1.5B\u6a21\u578b\u4e0a\u5e94\u75284\u4f4dGSQ\u548cGPTQ\u91cf\u5316\u6280\u672f\uff0c\u5e76\u5728MS MARCO\u3001BoolQ\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u4e3a\u4f4e\u6bd4\u7279\u91cf\u5316\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u8ba8\u8bba\u4e86GSQ\u548cGPTQ\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u672a\u6765\u7684\u91cf\u5316\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u51c6\u3002"}}
{"id": "2508.11210", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11210", "abs": "https://arxiv.org/abs/2508.11210", "authors": ["Minghui Sun", "Matthew M. Engelhard", "Benjamin A. Goldstein"], "title": "Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning", "comment": "accepted by Machine Learning for Healthcare 2025", "summary": "Risk assessments for a pediatric population are often conducted across\nmultiple stages. For example, clinicians may evaluate risks prenatally, at\nbirth, and during Well-Child visits. Although predictions made at later stages\ntypically achieve higher precision, it is clinically desirable to make reliable\nrisk assessments as early as possible. Therefore, this study focuses on\nimproving prediction performance in early-stage risk assessments. Our solution,\n\\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal\nframework that treats each time window as a distinct modality. In BFF, a model\nis trained on all available data throughout the time while performing a risk\nassessment using up-to-date information. This contrastive framework allows the\nmodel to ``borrow'' informative signals from later stages (e.g., Well-Child\nvisits) to implicitly supervise the learning at earlier stages (e.g.,\nprenatal/birth stages). We validate BFF on two real-world pediatric outcome\nprediction tasks, demonstrating consistent improvements in early risk\nassessments. The code is available at https://github.com/scotsun/bff.", "AI": {"tldr": "BFF\u6846\u67b6\u901a\u8fc7\u501f\u9274\u540e\u671f\u6570\u636e\u4fe1\u606f\u6765\u6539\u8fdb\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4e34\u5e8a\u4e0a\u5e0c\u671b\u80fd\u591f\u5c3d\u65e9\u8fdb\u884c\u53ef\u9760\u7684\u98ce\u9669\u8bc4\u4f30\uff0c\u4f46\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u9884\u6d4b\u7cbe\u5ea6\u901a\u5e38\u4e0d\u5982\u540e\u671f\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cBorrowing From the Future (BFF)\u201d\u7684\u5bf9\u6bd4\u591a\u6a21\u6001\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u6bcf\u4e2a\u65f6\u95f4\u7a97\u53e3\u89c6\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u6a21\u6001\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u5229\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\uff0c\u5e76\u5728\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\u65f6\u4f7f\u7528\u6700\u65b0\u4fe1\u606f\u3002\u8fd9\u79cd\u5bf9\u6bd4\u6846\u67b6\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u501f\u9274\u540e\u671f\u9636\u6bb5\uff08\u5982\u513f\u7ae5\u5065\u5eb7\u8bbf\u89c6\uff09\u7684\u6709\u7528\u4fe1\u53f7\uff0c\u6765\u9690\u5f0f\u5730\u6307\u5bfc\u65e9\u671f\u9636\u6bb5\uff08\u5982\u4ea7\u524d/\u51fa\u751f\u9636\u6bb5\uff09\u7684\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u513f\u79d1\u7ed3\u5c40\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cBFF\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u63d0\u9ad8\u4e86\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6Borrowing From the Future (BFF)\uff0c\u5229\u7528\u540e\u671f\u6570\u636e\uff08\u5982\u513f\u7ae5\u5065\u5eb7\u8bbf\u89c6\uff09\u7684\u4fe1\u53f7\u6765\u9690\u5f0f\u76d1\u7763\u65e9\u671f\u9636\u6bb5\uff08\u5982\u4ea7\u524d/\u51fa\u751f\uff09\u7684\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u9884\u6d4b\u6027\u80fd\u3002\u7814\u7a76\u5728\u4e24\u4e2a\u513f\u79d1\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86BFF\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u65b9\u9762\u7684\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2508.11106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11106", "abs": "https://arxiv.org/abs/2508.11106", "authors": ["Xinjie Gao", "Bi'an Du", "Wei Hu"], "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing", "comment": null, "summary": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.", "AI": {"tldr": "HierOctFusion \u662f\u4e00\u79cd\u65b0\u7684 3D \u5185\u5bb9\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u90e8\u4ef6\u611f\u77e5\u548c\u591a\u5c3a\u5ea6\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u516b\u53c9\u6811\u7684\u6269\u6563\u6a21\u578b\u5728\u751f\u6210 3D \u5185\u5bb9\u65f6\uff0c\u901a\u5e38\u5c06 3D \u5bf9\u8c61\u89c6\u4e3a\u6574\u4f53\uff0c\u5ffd\u7565\u4e86\u5176\u8bed\u4e49\u90e8\u4ef6\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u6a21\u578b\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bf9\u8c61\u672c\u8d28\u4e0a\u662f\u7a00\u758f\u548c\u5206\u5c42\u7684\uff0c\u8fd9\u4e3a\u5206\u5c42\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HierOctFusion \u7684\u90e8\u4ef6\u611f\u77e5\u591a\u5c3a\u5ea6\u516b\u53c9\u6811\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6ce8\u5165\u90e8\u4ef6\u7ea7\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u7ec6\u7c92\u5ea6\u548c\u7a00\u758f\u5bf9\u8c61\u7ed3\u6784\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5e26\u6709\u90e8\u4ef6\u7c7b\u522b\u6ce8\u91ca\u7684 3D \u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "HierOctFusion \u5b9e\u73b0\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u5f62\u72b6\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "HierOctFusion \u5728\u5f62\u72b6\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11343", "abs": "https://arxiv.org/abs/2508.11343", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u6587\u672c\u68c0\u6d4b\u89c6\u4e3a\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u7684\u9891\u57df\u7279\u6027\uff0c\u53d1\u73b0\u4eba\u7c7b\u5199\u4f5c\u6bd4LLM\u751f\u6210\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u9891\u8c31\u80fd\u91cf\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u7684SpecDetect\u68c0\u6d4b\u5668\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u7684\u80fd\u529b\u589e\u5f3a\uff0c\u5bf9\u6587\u672c\u8fdb\u884c\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u68c0\u6d4b\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u8868\u9762\u7edf\u8ba1\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u6f5c\u5728\u7684\u4fe1\u53f7\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u6df1\u5165\u5730\u7406\u89e3\u548c\u533a\u5206\u673a\u5668\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u3002", "method": "\u672c\u7814\u7a76\u5c06\u6587\u672c\u68c0\u6d4b\u91cd\u6784\u4e3a\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790token\u5bf9\u6570\u6982\u7387\u5e8f\u5217\u7684\u9891\u8c31\u7279\u6027\u6765\u533a\u5206\u4eba\u7c7b\u5199\u4f5c\u548cLLM\u751f\u6210\u6587\u672c\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u5168\u5c40\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u548c\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u6765\u5206\u6790\u6587\u672c\u4fe1\u53f7\u7684\u9891\u8c31\u5c5e\u6027\u3002\u7814\u7a76\u8005\u53d1\u73b0\u4eba\u7c7b\u5199\u4f5c\u5177\u6709\u66f4\u9ad8\u7684\u9891\u8c31\u80fd\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86SpecDetect\uff08\u57fa\u4e8eDFT\u603b\u80fd\u91cf\uff09\u548cSpecDetect++\uff08\u5f15\u5165\u91c7\u6837\u5dee\u5f02\u673a\u5236\uff09\u4e24\u79cd\u68c0\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSpecDetect\u548cSpecDetect++\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e86\u8fd1\u4e00\u534a\u3002\u8fd9\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4fe1\u53f7\u5904\u7406\u7684\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u6587\u672c\u68c0\u6d4b\u91cd\u6784\u4e3a\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9891\u57df\u5206\u6790token\u5bf9\u6570\u6982\u7387\u5e8f\u5217\u7684\u65b0\u8303\u5f0f\u3002\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u5206\u6790\u6587\u672c\u7684\u9891\u8c31\u7279\u6027\uff0c\u53d1\u73b0\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c\u76f8\u6bd4\u4e8eLLM\u751f\u6210\u7684\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u9891\u8c31\u80fd\u91cf\uff0c\u8fd9\u53cd\u6620\u4e86\u4eba\u7c7b\u5199\u4f5c\u4e2d\u6bd4LLM\u751f\u6210\u6587\u672c\u66f4\u663e\u8457\u7684\u5e45\u5ea6\u6ce2\u52a8\u3002\u57fa\u4e8e\u6b64\uff0c\u7814\u7a76\u8005\u6784\u5efa\u4e86SpecDetect\u548cSpecDetect++\uff0c\u524d\u8005\u57fa\u4e8e\u5168\u5c40\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u7684\u5355\u4e00\u9c81\u68d2\u7279\u5f81\u2014\u2014DFT\u603b\u80fd\u91cf\uff0c\u540e\u8005\u5219\u5f15\u5165\u4e86\u91c7\u6837\u5dee\u5f02\u673a\u5236\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u540c\u65f6\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u8fd1\u4e00\u534a\u3002\u672c\u7814\u7a76\u4e3aLLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u7684\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u7ecf\u5178\u7684\u4fe1\u53f7\u5904\u7406\u6280\u672f\u5728\u5e94\u5bf9\u8fd9\u4e00\u73b0\u4ee3\u6311\u6218\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u80fd\u529b\u3002"}}
{"id": "2508.11214", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11214", "abs": "https://arxiv.org/abs/2508.11214", "authors": ["Atticus Geiger", "Jacqueline Harding", "Thomas Icard"], "title": "How Causal Abstraction Underpins Computational Explanation", "comment": null, "summary": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction.", "AI": {"tldr": "This paper uses causal abstraction to explain computational implementation in cognitive science and machine learning, focusing on representation, generalization, and prediction.", "motivation": "Explanations of cognitive behavior often appeal to computations over representations. This work explores what it takes for a system to implement a given computation over suitable representational vehicles within that system.", "method": "We provide an account of computational implementation grounded in causal abstraction and examine the role of representation.", "result": "We illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning, specifically in deep learning with artificial neural networks.", "conclusion": "We argue that computational implementation is best understood through causal abstraction, with representation playing a key role, particularly in relation to generalization and prediction."}}
{"id": "2508.11364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11364", "abs": "https://arxiv.org/abs/2508.11364", "authors": ["Sylvio R\u00fcdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528 Llama 3.1 \u4ece\u5b66\u751f\u4f5c\u4e1a\u4e2d\u63d0\u53d6\u53cd\u9988\u6307\u6807\uff0c\u53d1\u73b0\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e3a\u81ea\u52a8\u751f\u6210\u53cd\u9988\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u5f62\u6210\u6027\u53cd\u9988\uff0c\u5fc5\u987b\u9996\u5148\u63d0\u53d6\u76f8\u5173\u6307\u6807\uff0c\u8fd9\u662f\u6784\u5efa\u53cd\u9988\u7684\u57fa\u7840\u3002\u6559\u5e08\u901a\u5e38\u4f7f\u7528\u5305\u542b\u5404\u79cd\u6307\u6807\u7684\u53cd\u9988\u6807\u51c6\u7f51\u683c\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u8003\u5bdf\u4f7f\u7528 LLM \u63d0\u53d6\u8fd9\u4e9b\u6307\u6807\u7684\u521d\u59cb\u9636\u6bb5\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b Llama 3.1 \u63d0\u53d6\u5b66\u751f\u8bed\u8a00\u5b66\u4e60\u8bfe\u7a0b\u4f5c\u4e1a\u4e2d\u7684\u76f8\u5173\u6307\u6807\uff0c\u5e76\u7814\u7a76\u4e86 LLM \u751f\u6210\u7684\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5728\u4e0d\u540c\u53cd\u9988\u6807\u51c6\u4e0b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cLLM \u751f\u6210\u7684\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5728\u5404\u79cd\u53cd\u9988\u6807\u51c6\u4e4b\u95f4\u5177\u6709\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u5373\u4f7f\u5728\u6d89\u53ca\u672a\u9884\u6599\u5230\u7684\u6307\u6807\u548c\u6807\u51c6\u7ec4\u5408\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u5b66\u751f\u4f5c\u4e1a\u4e2d\u63d0\u53d6\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u751f\u6210\u53ef\u89e3\u91ca\u3001\u900f\u660e\u7684\u5f62\u6210\u6027\u53cd\u9988\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11215", "abs": "https://arxiv.org/abs/2508.11215", "authors": ["Zicheng Guo", "Shuqi Wu", "Meixing Zhu", "He Guandi"], "title": "Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM", "comment": null, "summary": "With the intensification of global climate change, accurate prediction of air\nquality indicators, especially PM2.5 concentration, has become increasingly\nimportant in fields such as environmental protection, public health, and urban\nmanagement. To address this, we propose an air quality PM2.5 index prediction\nmodel based on a hybrid CNN-LSTM architecture. The model effectively combines\nConvolutional Neural Networks (CNN) for local spatial feature extraction and\nLong Short-Term Memory (LSTM) networks for modeling temporal dependencies in\ntime series data. Using a multivariate dataset collected from an industrial\narea in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5\nconcentration, temperature, dew point, pressure, wind direction, wind speed,\nand precipitation -- the model predicts the average PM2.5 concentration over\n6-hour intervals. Experimental results show that the model achieves a root mean\nsquare error (RMSE) of 5.236, outperforming traditional time series models in\nboth accuracy and generalization. This demonstrates its strong potential in\nreal-world applications such as air pollution early warning systems. However,\ndue to the complexity of multivariate inputs, the model demands high\ncomputational resources, and its ability to handle diverse atmospheric factors\nstill requires optimization. Future work will focus on enhancing scalability\nand expanding support for more complex multivariate weather prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408CNN-LSTM\u6a21\u578b\u6765\u9884\u6d4bPM2.5\u6d53\u5ea6\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86CNN\u7684\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548cLSTM\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f46\u5728\u5904\u7406\u591a\u5143\u8f93\u5165\u65f6\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u8f83\u9ad8\uff0c\u4e14\u5bf9\u590d\u6742\u5927\u6c14\u56e0\u7d20\u7684\u5904\u7406\u80fd\u529b\u6709\u5f85\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u5168\u7403\u6c14\u5019\u53d8\u5316\u7684\u52a0\u5267\uff0c\u7a7a\u6c14\u8d28\u91cf\u6307\u6807\uff08\u5c24\u5176\u662fPM2.5\u6d53\u5ea6\uff09\u7684\u51c6\u786e\u9884\u6d4b\u5728\u73af\u5883\u4fdd\u62a4\u3001\u516c\u4f17\u5065\u5eb7\u548c\u57ce\u5e02\u7ba1\u7406\u7b49\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408CNN-LSTM\u67b6\u6784\u7684\u7a7a\u6c14\u8d28\u91cfPM2.5\u6307\u6570\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528CNN\u63d0\u53d6\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\uff0c\u5229\u7528LSTM\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u9884\u6d4b\u7684PM2.5\u5e73\u5747\u6d53\u5ea6\uff086\u5c0f\u65f6\u95f4\u9694\uff09\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u4e3a5.236\uff0c\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728PM2.5\u6d53\u5ea6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u7a7a\u6c14\u6c61\u67d3\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2508.11584", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub \u0141ucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine \u662f\u4e00\u4e2a\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6a21\u5757\u5316\u89c6\u89c9\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u548c\u5e76\u884c\u5904\u7406\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86 GPU \u5229\u7528\u7387\u548c\u901f\u5ea6\uff08\u6700\u9ad8 3 \u500d\uff09\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ee5\u6267\u884c\u4e0d\u540c\u611f\u77e5\u4efb\u52a1\uff0c\u5e38\u5e38\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u3001\u5185\u5b58\u5360\u7528\u8fc7\u5927\u4ee5\u53ca\u590d\u6742\u7684\u96c6\u6210\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86 VPEngine\u3002", "method": "VPEngine \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528\u5171\u4eab\u7684\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u63d0\u53d6\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u5c06\u5176\u5728\u591a\u4e2a\u5e76\u884c\u8fd0\u884c\u7684\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u5934\u4e4b\u95f4\u5171\u4eab\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684 GPU-CPU \u5185\u5b58\u4f20\u8f93\u548c\u7279\u5f81\u63d0\u53d6\u7684\u8ba1\u7b97\u5197\u4f59\u3002\u8be5\u8bbe\u8ba1\u8fd8\u652f\u6301\u57fa\u4e8e\u5e94\u7528\u9700\u6c42\u7684\u52a8\u6001\u4efb\u52a1\u4f18\u5148\u7ea7\u6392\u5e8f\u3002", "result": "VPEngine \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684 GPU \u5229\u7528\u7387\u548c\u6052\u5b9a\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u5141\u8bb8\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u4efb\u52a1\u7684\u63a8\u7406\u9891\u7387\u3002\u4e0e\u987a\u5e8f\u6267\u884c\u76f8\u6bd4\uff0c\u901f\u5ea6\u6700\u9ad8\u53ef\u63d0\u5347 3 \u500d\uff0c\u5e76\u5728 NVIDIA Jetson Orin AGX \u4e0a\u5b9e\u73b0\u4e86 \u226550 Hz \u7684\u7aef\u5230\u7aef\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "VPEngine \u901a\u8fc7\u5171\u4eab DINOv2 \u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u4e2a\u4efb\u52a1\uff08\u6df1\u5ea6\u4f30\u8ba1\u3001\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff09\u7684\u5e76\u884c\u5904\u7406\uff0c\u4e0e\u987a\u5e8f\u6267\u884c\u76f8\u6bd4\uff0c\u901f\u5ea6\u6700\u9ad8\u63d0\u5347\u4e86 3 \u500d\u3002\u8be5\u6846\u67b6\u5229\u7528 CUDA MPS\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684 GPU \u5229\u7528\u7387\u548c\u6052\u5b9a\u7684\u5185\u5b58\u5360\u7528\uff0c\u5e76\u5141\u8bb8\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u4efb\u52a1\u7684\u63a8\u7406\u9891\u7387\u3002\u5728 NVIDIA Jetson Orin AGX \u4e0a\uff0c\u4f7f\u7528 TensorRT \u4f18\u5316\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86 \u226550 Hz \u7684\u7aef\u5230\u7aef\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2508.11134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11134", "abs": "https://arxiv.org/abs/2508.11134", "authors": ["Bing Liu", "Le Wang", "Hao Liu", "Mingming Liu"], "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "comment": "7 pages, 5 figures, 2025 ICME Accepted", "summary": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRBDM\u7684\u53cc\u5411\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u53bb\u96fe\u548c\u751f\u6210\u96fe\u973e\u7684\u8f6c\u6362\uff0c\u5e76\u5728\u56fe\u50cf\u5757\u4e0a\u5b66\u4e60\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u53bb\u96fe\u65b9\u6cd5\u4ec5\u80fd\u4ece\u6a21\u7cca\u56fe\u50cf\u4e2d\u53bb\u9664\u96fe\u973e\uff0c\u800c\u7f3a\u4e4f\u5728\u6a21\u7cca\u548c\u65e0\u6a21\u7cca\u56fe\u50cf\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u7684\u9ad8\u6548\u53cc\u5411\u6269\u6563\u6a21\u578b\uff08RBDM\uff09\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1. \u63d0\u51fa\u5bf9\u5076\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u7528\u4e8e\u6709\u6548\u8f6c\u79fb\u6b8b\u5dee\u5e76\u4fc3\u8fdb\u5b83\u4eec\u4e4b\u95f4\u7684\u53cc\u5411\u5e73\u6ed1\u8f6c\u6362\u30022. RBDM\u5728\u5355\u4e2a\u65f6\u95f4\u6b65\u957f\u4e0a\u6270\u52a8\u6a21\u7cca\u548c\u65e0\u6a21\u7cca\u56fe\u50cf\uff0c\u5e76\u9884\u6d4b\u6270\u52a8\u6570\u636e\u4e2d\u7684\u566a\u58f0\uff0c\u4ee5\u540c\u65f6\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u5728\u76f8\u5bf9\u8f83\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5757\u800c\u975e\u6574\u4e2a\u56fe\u50cf\u4e0a\u5b66\u4e60\u7edf\u4e00\u7684\u5206\u6570\u51fd\u6570\u3002", "result": "\u6240\u63d0\u51fa\u7684RBDM\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u6216\u81f3\u5c11\u53ef\u6bd4\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RBDM\u80fd\u591f\u5b9e\u73b0\u65e0\u5c3a\u5bf8\u9650\u5236\u7684\u53cc\u5411\u8f6c\u6362\uff0c\u5e76\u4e14\u4ec5\u970015\u6b65\u91c7\u6837\u3002"}}
{"id": "2508.11383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "AI": {"tldr": "LLM\u5bf9\u63d0\u793a\u7684\u5fae\u5c0f\u53d8\u5316\u5f88\u654f\u611f\u3002\u6211\u4eec\u8bc4\u4f30\u4e865\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u63d0\u793a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u57288\u4e2a\u6a21\u578b\u548c52\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u4f7fLLM\u66f4\u53ef\u9760\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "LLM\u5bf9\u63d0\u793a\u63aa\u8f9e\u548c\u683c\u5f0f\u7684\u7ec6\u5fae\u3001\u975e\u8bed\u4e49\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u5f71\u54cd\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e865\u79cd\u63d0\u9ad8\u63d0\u793a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u57288\u4e2a\u6a21\u578b\uff08Llama\u3001Qwen\u3001Gemma\u7cfb\u5217\uff09\u768452\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5b83\u4eec\u5728\u591a\u79cd\u5206\u5e03 shifts \u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86GPT-4.1\u548cDeepSeek V3\u6a21\u578b\u5bf9\u683c\u5f0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "result": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e865\u79cd\u63d0\u9ad8\u63d0\u793a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u76f8\u5bf9\u6709\u6548\u6027\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u4ee5\u5e2e\u52a9\u5b9e\u8df5\u8005\u505a\u51fa\u660e\u667a\u7684\u51b3\u7b56\u3002", "conclusion": "LLM\u5728\u63d0\u793a\u63aa\u8f9e\u548c\u683c\u5f0f\u7684\u7ec6\u5fae\u3001\u975e\u8bed\u4e49\u53d8\u5316\u4e0a\u9ad8\u5ea6\u654f\u611f\u3002\u672c\u7814\u7a76\u9996\u6b21\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u5b9e\u9a8c\u6846\u67b6\u5185\uff0c\u5bf95\u79cd\u63d0\u9ad8\u63d0\u793a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6db5\u76d6\u4e86\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u57288\u4e2a\u6a21\u578b\uff08Llama\u3001Qwen\u3001Gemma\u7cfb\u5217\uff09\u768452\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5e94\u5bf9\u591a\u79cd\u5206\u5e03 shifts \u65b9\u9762\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u6269\u5c55\u5230GPT-4.1\u548cDeepSeek V3\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u5bf9\u683c\u5f0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u8df5\u8005\u5728\u8ffd\u6c42\u7a33\u5b9a\u53ef\u9760\u7684LLM\u6027\u80fd\u65f6\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fd9\u4e9b\u9c81\u68d2\u6027\u65b9\u6cd5\u76f8\u5bf9\u6709\u6548\u6027\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002"}}
{"id": "2508.11235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11235", "abs": "https://arxiv.org/abs/2508.11235", "authors": ["William Alemanni", "Arianna Burzacchi", "Davide Colombi", "Elena Giarratano"], "title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories", "comment": null, "summary": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios.", "AI": {"tldr": "An enhanced map matching algorithm improves GPS trajectory reconstruction accuracy and applicability by integrating imputation, optimized voting, and OpenStreetMap data.", "motivation": "To reconstruct GPS trajectories with high accuracy, independent of input data quality, and to extend the applicability of the original algorithm to diverse real-world scenarios.", "method": "The enhanced version of the Interactive Voting-Based Map Matching algorithm integrates trajectory imputation and a distance-bounded interactive voting strategy to reduce computational complexity. It also includes modifications to address missing data in the road network and uses a custom-built asset derived from OpenStreetMap.", "result": "The enhanced algorithm preserves the core strengths of the original algorithm while significantly extending its applicability.", "conclusion": "The enhanced algorithm can process trajectories with varying sampling rates and reconstruct GPS trajectories with high accuracy, regardless of input data quality. It is applicable to any geographic region covered by OpenStreetMap."}}
{"id": "2508.11588", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e86\u5bf9\u673a\u5668\u4eba\u91c7\u6458\u8fc7\u7a0b\u4e2d\u6293\u63e1\u72b6\u6001\u7684100%\u51c6\u786e\u8bc6\u522b\uff0c\u5e76\u786e\u5b9a\u4e86IMU\u548c\u5f20\u529b\u4f20\u611f\u5668\u4e3a\u5173\u952e\u4f20\u611f\u5668\uff0c\u4ee5\u63d0\u9ad8\u519c\u4e1a\u673a\u5668\u4eba\u91c7\u6458\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u7684\u590d\u6742\u6027\u3001\u906e\u6321\u4ee5\u53ca\u6c34\u679c\u4e0e\u690d\u7269\u7684\u8fde\u63a5\u6027\u5bf9\u673a\u5668\u4eba\u6293\u63e1\u72b6\u6001\u7684\u51c6\u786e\u7406\u89e3\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u9009\u62e9\u5408\u9002\u7684\u4f20\u611f\u5668\u548c\u5efa\u6a21\u6280\u672f\u6765\u63d0\u9ad8\u6293\u63e1\u72b6\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u8bc4\u4f30\u4e86IMU\u3001\u7ea2\u5916\u53cd\u5c04\u3001\u5f20\u529b\u3001\u89e6\u89c9\u4f20\u611f\u5668\u548cRGB\u6444\u50cf\u5934\u5728\u6293\u63e1\u72b6\u6001\u5206\u7c7b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\u548cLSTM\u7f51\u7edc\u7684\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u771f\u5b9e\u6a31\u6843\u756a\u8304\u690d\u7269\u4e0a\u5b9e\u73b0\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u786e\u5b9a\u4e86IMU\u548c\u5f20\u529b\u4f20\u611f\u5668\u662f\u8bc6\u522b\u6293\u63e1\u72b6\u6001\u7684\u6700\u5c0f\u53ef\u884c\u4f20\u611f\u5668\u7ec4\u5408\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210IMU\u548c\u5f20\u529b\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u53ef\u4ee5100%\u51c6\u786e\u5730\u8bc6\u522b\u673a\u5668\u4eba\u91c7\u6458\u4e2d\u7684\u6ed1\u52a8\u3001\u6293\u63e1\u5931\u8d25\u548c\u6210\u529f\u91c7\u6458\uff0c\u4ece\u800c\u63d0\u9ad8\u6c34\u679c\u91c7\u6458\u64cd\u4f5c\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMICC\u7684\u65b0\u578b\u8de8\u6a21\u6001\u8c23\u8a00\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5c3a\u5ea6\u56fe\u50cf\u5185\u5bb9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8c23\u8a00\u68c0\u6d4b\u65b9\u6cd5\u5e38\u5e38\u5ffd\u7565\u56fe\u50cf\u5185\u5bb9\u4ee5\u53ca\u8de8\u4e0d\u540c\u89c6\u89c9\u5c3a\u5ea6\u7684\u4e0a\u4e0b\u6587\u4e0e\u56fe\u50cf\u4e4b\u95f4\u56fa\u6709\u7684\u5173\u7cfb\uff0c\u5bfc\u81f4\u8bc6\u522b\u8c23\u8a00\u7684\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8de8\u6a21\u6001\u8c23\u8a00\u68c0\u6d4b\u65b9\u6848\uff08MICC\uff09\uff0c\u5305\u62ec\uff1a1.\u8bbe\u8ba1SCLIP\u7f16\u7801\u5668\u751f\u6210\u6587\u672c\u548c\u591a\u5c3a\u5ea6\u56fe\u50cf\u5757\u7684\u7edf\u4e00\u8bed\u4e49\u5d4c\u5165\uff1b2.\u5f15\u5165\u8de8\u6a21\u6001\u591a\u5c3a\u5ea6\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u6700\u5927\u5316\u548c\u4fe1\u606f\u74f6\u9888\u539f\u7406\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8de8\u6a21\u6001\u76f8\u5173\u6027\u77e9\u9635\u7684Top-K\u9009\u62e9\u7b56\u7565\uff0c\u8bc6\u522b\u4e0e\u6587\u672c\u8bed\u4e49\u6700\u76f8\u5173\u7684\u56fe\u50cf\u533a\u57df\uff1b3.\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u611f\u77e5\u591a\u5c3a\u5ea6\u7684\u878d\u5408\u7f51\u7edc\uff0c\u901a\u8fc7\u4e3a\u56fe\u50cf\u533a\u57df\u5206\u914d\u57fa\u4e8e\u5176\u8bed\u4e49\u91cd\u8981\u6027\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\u7684\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u6765\u6574\u5408\u9ad8\u5ea6\u76f8\u5173\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u4e0e\u5168\u5c40\u6587\u672c\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8c23\u8a00\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8c23\u8a00\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.11386", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11386", "abs": "https://arxiv.org/abs/2508.11386", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "title": "Retrieval-augmented reasoning with lean language models", "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "AI": {"tldr": "\u672c\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u4e2d\u7ed3\u5408\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528Qwen2.5-Instruct\u6a21\u578b\u548c\u5bc6\u96c6\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u63d0\u9ad8\u4e86\u56de\u7b54\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u672c\u5730\u90e8\u7f72\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u548c\u5916\u90e8API\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u5728\u8d44\u6e90\u53d7\u9650\u6216\u5b89\u5168\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u4e2d\uff0c\u96c6\u6210\u4e86\u5bc6\u96c6\u68c0\u7d22\u5668\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684Qwen2.5-Instruct\u6a21\u578b\uff0c\u5e76\u5229\u7528\u4e86\u6765\u81ea\u524d\u6cbf\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u7684\u5408\u6210\u67e5\u8be2\u751f\u6210\u548c\u63a8\u7406\u8f68\u8ff9\uff0c\u5904\u7406\u4e86NHS A-Z \u72b6\u51b5\u9875\u9762\u8fd9\u4e00\u7279\u5b9a\u8bed\u6599\u5e93\u3002\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u57fa\u4e8e\u6458\u8981\u7684\u6587\u6863\u538b\u7f29\u3001\u5408\u6210\u6570\u636e\u8bbe\u8ba1\u4ee5\u53ca\u9762\u5411\u63a8\u7406\u7684\u5fae\u8c03\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u975e\u63a8\u7406\u548c\u901a\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u7b54\u6848\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u52bf\uff0c\u6027\u80fd\u63a5\u8fd1\u524d\u6cbf\u6a21\u578b\uff0c\u5e76\u4e14\u6613\u4e8e\u672c\u5730\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u7b54\u6848\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63a5\u8fd1\u4e86\u524d\u6cbf\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u672c\u5730\u90e8\u7f72\u3002"}}
{"id": "2508.11249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11249", "abs": "https://arxiv.org/abs/2508.11249", "authors": ["Asela Hevapathige", "Asiri Wijesinghe", "Ahad N. Zehmakan"], "title": "Graph Neural Diffusion via Generalized Opinion Dynamics", "comment": null, "summary": "There has been a growing interest in developing diffusion-based Graph Neural\nNetworks (GNNs), building on the connections between message passing mechanisms\nin GNNs and physical diffusion processes. However, existing methods suffer from\nthree critical limitations: (1) they rely on homogeneous diffusion with static\ndynamics, limiting adaptability to diverse graph structures; (2) their depth is\nconstrained by computational overhead and diminishing interpretability; and (3)\ntheoretical understanding of their convergence behavior remains limited. To\naddress these challenges, we propose GODNF, a Generalized Opinion Dynamics\nNeural Framework, which unifies multiple opinion dynamics models into a\nprincipled, trainable diffusion mechanism. Our framework captures heterogeneous\ndiffusion patterns and temporal dynamics via node-specific behavior modeling\nand dynamic neighborhood influence, while ensuring efficient and interpretable\nmessage propagation even at deep layers. We provide a rigorous theoretical\nanalysis demonstrating GODNF's ability to model diverse convergence\nconfigurations. Extensive empirical evaluations of node classification and\ninfluence estimation tasks confirm GODNF's superiority over state-of-the-art\nGNNs.", "AI": {"tldr": " GODNF \u901a\u8fc7\u8282\u70b9\u7279\u5b9a\u7684\u884c\u4e3a\u5efa\u6a21\u548c\u52a8\u6001\u90bb\u57df\u5f71\u54cd\u6765\u514b\u670d\u73b0\u6709 GNN \u7684\u5c40\u9650\u6027\uff0c\u4ee5\u5b9e\u73b0\u5f02\u6784\u6269\u6563\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684 GNN \u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a(1) \u4f9d\u8d56\u4e8e\u5177\u6709\u9759\u6001\u52a8\u6001\u7684\u540c\u8d28\u6269\u6563\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u56fe\u7ed3\u6784\uff1b(2) \u7531\u4e8e\u8ba1\u7b97\u5f00\u9500\u548c\u53ef\u89e3\u91ca\u6027\u4e0b\u964d\uff0c\u5176\u6df1\u5ea6\u53d7\u5230\u9650\u5236\uff1b(3) \u5176\u6536\u655b\u884c\u4e3a\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GODNF \u7684\u5e7f\u4e49\u610f\u89c1\u52a8\u529b\u5b66\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u591a\u79cd\u610f\u89c1\u52a8\u529b\u5b66\u6a21\u578b\u7edf\u4e00\u4e3a\u4e00\u79cd\u539f\u5219\u6027\u7684\u3001\u53ef\u8bad\u7ec3\u7684\u6269\u6563\u673a\u5236\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8282\u70b9\u7279\u5b9a\u7684\u884c\u4e3a\u5efa\u6a21\u548c\u52a8\u6001\u90bb\u57df\u5f71\u54cd\u6765\u6355\u83b7\u5f02\u6784\u6269\u6563\u6a21\u5f0f\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u540c\u65f6\u786e\u4fdd\u5373\u4f7f\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6d88\u606f\u4f20\u64ad\u3002", "result": "GODNF \u80fd\u591f\u6a21\u62df\u4e0d\u540c\u7684\u6536\u655b\u914d\u7f6e\uff0c\u5e76\u4e14\u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u4f30\u8ba1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 GNN\u3002", "conclusion": "GODNF \u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u4f30\u8ba1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 GNN\u3002"}}
{"id": "2508.11153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "comment": "The International Conference on Neural Information Processing\n  (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "AI": {"tldr": "\"LEARN\u662f\u4e00\u4e2a\u4e3aSTEM\u6559\u80b2\u751f\u6210\u6559\u5b66\u63d2\u56fe\u7684\u5e03\u5c40\u611f\u77e5\u6269\u6563\u6846\u67b6\u3002\u5b83\u4f7f\u7528BookCover\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u3001\u5bf9\u6bd4\u89c6\u89c9-\u8bed\u4e49\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u6574\u6765\u751f\u6210\u652f\u6301\u63a8\u7406\u548c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u7684\u89c6\u89c9\u5e8f\u5217\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u5bf9\u6297\u788e\u7247\u5316\u6ce8\u610f\u529b\uff0c\u4fc3\u8fdb\u6982\u5ff5\u4e13\u6ce8\uff0c\u5e76\u53ef\u80fd\u4e0e\u591a\u6a21\u6001\u7cfb\u7edf\u548c\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u3002LEARN\u662f\u7ed3\u5408\u5e03\u5c40\u53d9\u4e8b\u3001\u8bed\u4e49\u7ed3\u6784\u5b66\u4e60\u548c\u8ba4\u77e5\u652f\u67b6\u7684\u7b2c\u4e00\u4e2a\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002 \"", "motivation": "\"STEM\u6559\u80b2\u4e2d\u7684\u63d2\u56fe\u751f\u6210\u5f80\u5f80\u7f3a\u4e4f\u6559\u5b66\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5b66\u751f\u6ce8\u610f\u529b\u5206\u6563\u548c\u7406\u89e3\u788e\u7247\u5316\u3002LEARN\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u5177\u6709\u53d9\u4e8b\u5e03\u5c40\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u7ebf\u7d22\u7684\u63d2\u56fe\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u4e2d\u9ad8\u7ea7\u63a8\u7406\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u3002\" ", "method": " \"LEARN\u901a\u8fc7\u5229\u7528\u7cbe\u9009\u7684BookCover\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u53d9\u4e8b\u5e03\u5c40\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ece\u800c\u5b9e\u73b0\u5e03\u5c40\u611f\u77e5\u7684\u751f\u6210\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u5e03\u5c40\u7684\u6761\u4ef6\u751f\u6210\u3001\u5bf9\u6bd4\u89c6\u89c9-\u8bed\u4e49\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u6574\u6765\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u89c9\u5e8f\u5217\uff0c\u4ee5\u652f\u6301\u4e0e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u4e00\u81f4\u7684\u4ece\u4e2d\u9ad8\u7ea7\u63a8\u7406\uff0c\u540c\u65f6\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u6240\u5f3a\u8c03\u7684\u989d\u5916\u8ba4\u77e5\u8d1f\u8377\u3002\" ", "result": "\"LEARN\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u89c9\u5e8f\u5217\uff0c\u652f\u6301\u4e2d\u9ad8\u7ea7\u63a8\u7406\uff0c\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4fc3\u8fdb\u7a7a\u95f4\u7ec4\u7ec7\u548c\u9a71\u52a8\u6545\u4e8b\u53d9\u8ff0\u7684\u53d9\u8ff0\uff0c\u53ef\u4ee5\u5bf9\u6297\u77ed\u5a92\u4f53\u5f15\u8d77\u7684\u6ce8\u610f\u529b\u5206\u6563\uff0c\u5e76\u4fc3\u8fdb\u6301\u7eed\u7684\u6982\u5ff5\u5173\u6ce8\u3002 \"", "conclusion": "\"LEARN\u662f\u7b2c\u4e00\u4e2a\u5c06\u57fa\u4e8e\u5e03\u5c40\u7684\u6545\u4e8b\u53d9\u8ff0\u3001\u8bed\u4e49\u7ed3\u6784\u5b66\u4e60\u548c\u8ba4\u77e5\u652f\u67b6\u7ed3\u5408\u8d77\u6765\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u4ee3\u8868\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u65b9\u5411\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002 \""}}
{"id": "2508.11388", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11388", "abs": "https://arxiv.org/abs/2508.11388", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u68af\u5ea6\u906e\u6321\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u751f\u6210\u62bd\u53d6\u5f0f\u89e3\u91ca\uff0c\u6ee1\u8db3\u5145\u5206\u6027\u3001\u5b8c\u5907\u6027\u548c\u7d27\u51d1\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u53d6\u5f97\u5feb\u901f\u8fdb\u5c55\uff0c\u5bf9\u8fd9\u4e9b\u9ed1\u76d2\u6a21\u578b\u9884\u6d4b\u8fdb\u884c\u89e3\u91ca\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62bd\u53d6\u5f0f\u89e3\u91ca\u751f\u6210\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u906e\u6321\u8f93\u5165\u4e2d\u6a21\u578b\u4e0d\u8ba4\u4e3a\u662f\u6307\u793a\u6027\u7684\u90e8\u5206\u3002\u906e\u6321\u8fc7\u7a0b\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u548c\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6848\u5b9e\u73b0\uff0c\u8be5\u65b9\u6848\u5f3a\u5236\u6267\u884c\u5145\u5206\u6027\u3001\u5b8c\u5907\u6027\u548c\u7d27\u51d1\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89e3\u91ca\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u63a8\u7406\u63d0\u53d6\u6761\u4ef6\u66f4\u5e7f\u6cdb\u5730\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u8f93\u5165\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u906e\u6321\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u5145\u5206\u6027\u3001\u5b8c\u5907\u6027\u548c\u7d27\u51d1\u6027\u539f\u5219\u7684\u62bd\u53d6\u5f0f\u89e3\u91ca\uff0c\u8bc1\u660e\u4e86\u65e0\u9700\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u4ec5\u57fa\u4e8e\u5df2\u8bad\u7ec3\u5206\u7c7b\u5668\u5373\u53ef\u5b9e\u73b0 \u0644\u0647\u0630\u0647 \u5c5e\u6027\u3002"}}
{"id": "2508.11258", "categories": ["cs.LG", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11258", "abs": "https://arxiv.org/abs/2508.11258", "authors": ["Ruicheng Xian", "Yuxuan Wan", "Han Zhao"], "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "comment": null, "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63d0\u793a\u4ece\u95ed\u5408\u6743\u91cd LLM \u4e2d\u63d0\u53d6\u516c\u5e73\u5206\u7c7b\u5668\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u6027\u6267\u884c\u65b9\u6cd5\u65e0\u6cd5\u7528\u4e8e GPT-4\u3001Gemini \u548c Claude \u7b49\u95ed\u5408\u6743\u91cd LLM \u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "method": "\u8be5\u6846\u67b6\u5c06 LLM \u89c6\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4ece\u5176\u6982\u7387\u9884\u6d4b\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u83b7\u5f97\u516c\u5e73\u5206\u7c7b\u7684\u5145\u5206\u7edf\u8ba1\u6570\u636e\uff0c\u7136\u540e\u91c7\u7528\u540e\u9a8c\u65b9\u6cd5\u5e94\u7528\u516c\u5e73\u7b97\u6cd5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u516c\u5e73\u5206\u7c7b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027-\u516c\u5e73\u6027\u6743\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u6570\u636e\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ece\u5f00\u653e\u6743\u91cd\u548c\u95ed\u5408\u6743\u91cd LLM \u4e2d\u63d0\u53d6\u7684\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027-\u516c\u5e73\u6027\u6743\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5177\u6709\u6570\u636e\u6548\u7387\uff0c\u4f18\u4e8e\u5728 LLM \u5d4c\u5165\u6216\u539f\u59cb\u8868\u683c\u7279\u5f81\u4e0a\u8bad\u7ec3\u7684\u516c\u5e73\u5206\u7c7b\u5668\u3002"}}
{"id": "2508.11165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11165", "abs": "https://arxiv.org/abs/2508.11165", "authors": ["Bing Liu", "Le Wang", "Mingming Liu", "Hao Liu", "Rui Yao", "Yong Zhou", "Peng Liu", "Tongqiang Xia"], "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "comment": "10 pages, 4 figures", "summary": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "\u901a\u8fc7EM\u548cB3DM\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u5b66\u4e60\u548cRDC\u6a21\u5757\uff0c\u5b9e\u73b0\u9ad8\u6548\u534a\u76d1\u7763\u56fe\u50cf\u53bb\u96fe\uff0c\u5e76\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u6a21\u7cca\u56fe\u50cf\uff0c\u7279\u522b\u662f\u6d53\u96fe\u573a\u666f\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u771f\u5b9e\u914d\u5bf9\u6570\u636e\u548c\u9c81\u68d2\u5148\u9a8c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u548c\u53cc\u5411\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\uff08B3DM\uff09\u7684\u534a\u76d1\u7763\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6848\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u5229\u7528EM\u7b97\u6cd5\u89e3\u8026\u914d\u5bf9\u7684\u6a21\u7cca\u548c\u6e05\u6670\u56fe\u50cf\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u7edf\u4e00\u7684\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\u6765\u6355\u6349\u5b83\u4eec\u4e4b\u95f4\u7684\u7ed3\u6784\u548c\u5185\u5bb9\u76f8\u5173\u6027\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u89c4\u6a21\u65e0\u914d\u5bf9\u6570\u636e\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u589e\u5f3a\u7ec6\u8282\u7684\u6b8b\u5dee\u5dee\u5206\u5377\u79ef\uff08RDC\uff09\u6a21\u5757\u6765\u6355\u6349\u68af\u5ea6\u7ea7\u4fe1\u606f\u3002", "result": "EM-B3DM\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684EM-B3DM\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11393", "abs": "https://arxiv.org/abs/2508.11393", "authors": ["Marc Brinner", "Sina Zarrie\u00df"], "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "AI": {"tldr": "A new training method for rationalized transformers makes them more stable and better aligned with human judgments by using a single model instead of three, without needing extra labeled data.", "motivation": "To address the training instabilities common in existing rationalized model approaches and to improve the alignment of rationales with human annotations without explicit supervision.", "method": "An end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier, simplifying the traditional three-player-game into a single model that performs classification and token scoring. Incorporates advances in parameterizing and regularizing rationales.", "result": "A single model that simultaneously classifies samples and scores input tokens based on relevance, achieving substantially improved and state-of-the-art alignment with human annotations.", "conclusion": "The proposed approach simplifies the three-player-game for training rationalized models into a single model, leading to more efficient training and improved stability. It also extends to produce class-wise rationales with state-of-the-art alignment to human annotations without explicit supervision."}}
{"id": "2508.11279", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11279", "abs": "https://arxiv.org/abs/2508.11279", "authors": ["Jihang Wang", "Dongcheng Zhao", "Ruolin Chen", "Qian Zhang", "Yi Zeng"], "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models.", "AI": {"tldr": "This paper introduces RTE, a method to make Spiking Neural Networks more resistant to adversarial attacks by improving the robustness of their temporal components and reducing how vulnerabilities spread over time. Experiments show RTE works better than current methods.", "motivation": "The motivation is to address the poor understanding and vulnerability of Spiking Neural Networks (SNNs) to adversarial perturbations by revisiting their robustness through the lens of temporal ensembling, which reveals challenges like the fragility of individual temporal sub-networks and adversarial vulnerability transfer across time.", "method": "The paper proposes Robust Temporal self-Ensemble (RTE), a training framework that improves the robustness of each sub-network while reducing the temporal transferability of adversarial perturbations by integrating both objectives into a unified loss and employing a stochastic sampling strategy for efficient optimization.", "result": "RTE consistently outperforms existing training methods in the robust-accuracy trade-off across multiple benchmarks. It reshapes the internal robustness landscape of SNNs, leading to more resilient and temporally diversified decision boundaries.", "conclusion": "Spiking Neural Networks (SNNs) are vulnerable to adversarial perturbations, but this work proposes Robust Temporal self-Ensemble (RTE) to improve their robustness by treating the network as evolving sub-networks across timesteps. RTE integrates objectives to enhance sub-network robustness and reduce adversarial transferability, outperforming existing methods and reshaping SNNs' internal robustness for more resilient and temporally diversified decision boundaries."}}
{"id": "2508.11167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11167", "abs": "https://arxiv.org/abs/2508.11167", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "comment": "Manuscript submitted to IEEE TGRS", "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "AI": {"tldr": "VG-DETR\u662f\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u548c\u5c11\u91cf\u76ee\u6807\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u4e2d\u6e90\u57df\u65e0\u5173\u76ee\u6807\u68c0\u6d4b\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u9065\u611f\u573a\u666f\u4e2d\uff0c\u9690\u79c1\u548c\u4f20\u8f93\u9650\u5236\u5e38\u5e38\u65e0\u6cd5\u83b7\u53d6\u6e90\u57df\u6570\u636e\uff0c\u9650\u5236\u4e86\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6e90\u57df\u65e0\u5173\u76ee\u6807\u68c0\u6d4b\uff08SFOD\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e8\u5728\u65e0\u9700\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8de8\u57df\u81ea\u9002\u5e94\uff0c\u4f46\u73b0\u6709SFOD\u65b9\u6cd5\uff08\u4e3b\u8981\u901a\u8fc7\u81ea\u8bad\u7ec3\u8303\u5f0f\uff09\u5e38\u56e0\u4f2a\u6807\u7b7e\u566a\u58f0\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\uff0c\u5c24\u5176\u662f\u5728\u9065\u611f\u56fe\u50cf\u4e2d\u76ee\u6807\u5bc6\u96c6\u4e14\u80cc\u666f\u590d\u6742\u7684\u60c5\u51b5\u4e0b\u3002\u8003\u8651\u5230\u5728\u5b9e\u8df5\u4e2d\u83b7\u53d6\u6709\u9650\u7684\u76ee\u6807\u57df\u6807\u6ce8\u662f\u53ef\u884c\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVG-DETR\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6e90\u57df\u65e0\u5173\u7684\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u4ee5\u201c\u514d\u8d39\u5348\u9910\u201d\u7684\u65b9\u5f0f\u96c6\u6210\u5230\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u5229\u7528\u5c11\u91cf\u7684\u6807\u8bb0\u76ee\u6807\u6570\u636e\u6765\u51cf\u8f7b\u4f2a\u6807\u7b7e\u566a\u58f0\u5e76\u589e\u5f3a\u68c0\u6d4b\u5668\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5f15\u5165\u4e86\u4e00\u79cdVFM\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u6316\u6398\u7b56\u7565\uff0c\u5229\u7528VFM\u7684\u8bed\u4e49\u5148\u9a8c\u6765\u8bc4\u4f30\u4f2a\u6807\u7b7e\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u6062\u590d\u4f4e\u7f6e\u4fe1\u5ea6\u8f93\u51fa\u6765\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u548c\u6570\u91cf\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cdVFM\u5f15\u5bfc\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u5b9e\u4f8b\u548c\u56fe\u50cf\u5c42\u9762\u5c06\u68c0\u6d4b\u5668\u7279\u5f81\u4e0eVFM\u5d4c\u5165\u8fdb\u884c\u5bf9\u9f50\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u539f\u578b\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u589e\u5f3a\u4e86\u7279\u5f81\u8868\u793a\u5bf9\u9886\u57df\u5dee\u8ddd\u7684\u9c81\u68d2\u6027\u3002", "result": "VG-DETR\u901a\u8fc7VFM\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u6316\u6398\u7b56\u7565\u548c\u53cc\u91cdVFM\u5f15\u5bfc\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5728\u6e90\u57df\u65e0\u5173\u7684\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "VG-DETR\u5728\u6e90\u57df\u65e0\u5173\u7684\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11414", "abs": "https://arxiv.org/abs/2508.11414", "authors": ["Shangrui Nie", "Florian Mai", "David Kacz\u00e9r", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "AI": {"tldr": "\u5fae\u8c03LLM\u4ee5\u56de\u7b54\u4ef7\u503c\u8c03\u67e5\u95ee\u9898\u53ef\u4ee5\u6539\u53d8\u5176\u5728\u8c03\u67e5\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5730\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u65b9\u5f0f\u884c\u4e8b\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff1a\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u56de\u7b54\u4ef7\u503c\u8c03\u67e5\u95ee\u9898\u6765\u4fee\u6539\u5176\u4ef7\u503c\u4f53\u7cfb\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9996\u5148\u901a\u8fc7\u8ba9\u51e0\u4e2a\u5f00\u6e90LLM\u5bf9\u8de8\u8d8a20\u79cd\u4e0d\u540c\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u76f8\u5173\u63cf\u8ff0\u8fdb\u884c\u8bc4\u5206\u6765\u6784\u5efa\u5b83\u4eec\u7684\u4ef7\u503c\u753b\u50cf\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u540e\u7eed\u5b9e\u9a8c\u7684\u57fa\u7ebf\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u7814\u7a76\u4e86\u901a\u8fc7\u5728\u4ef7\u503c\u8c03\u67e5\u4e0a\u8fdb\u884c\u5fae\u8c03\u6765\u63a7\u5236\u6a21\u578b\u4ef7\u503c\u4f53\u7cfb\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5728\u6a21\u578b\u5185\u90e8\u7684\uff08\u9886\u57df\u5185\u3001\u4fdd\u7559\u7684\u8c03\u67e5\u95ee\u9898\uff09\u548c\u6a21\u578b\u5916\u90e8\u7684\uff08\u60c5\u666f\u5316\u9053\u5fb7\u5224\u65ad\u6570\u636e\u96c6\u548c\u57fa\u4e8eReddit\u5e16\u5b50\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\uff09\u884c\u4e3a\u53d8\u5316\u6765\u8bc4\u4f30\u5fae\u8c03\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5728\u4ef7\u503c\u8c03\u67e5\u95ee\u9898\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u6539\u53d8\u6a21\u578b\u5728\u9886\u57df\u5185\u8c03\u67e5\u95ee\u9898\u4e0a\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u5728\u9690\u6027\u7684\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a\u4e2d\u4ea7\u751f\u663e\u8457\u7684\u4ef7\u503c\u5bf9\u9f50\u53d8\u5316\u3002", "conclusion": "\u901a\u8fc7\u5728\u4ef7\u503c\u8c03\u67e5\u95ee\u9898\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6539\u53d8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ef7\u503c\u4f53\u7cfb\uff0c\u5e76\u5bf9\u5176\u4e0b\u6e38\u884c\u4e3a\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2508.11328", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11328", "abs": "https://arxiv.org/abs/2508.11328", "authors": ["Haitong Luo", "Suhang Wang", "Weiyao Zhang", "Ruiqi Meng", "Xuying Meng", "Yujun Zhang"], "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "comment": "Under Review", "summary": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.", "AI": {"tldr": "HS-GPPT\u901a\u8fc7\u6df7\u5408\u5149\u8c31\u6ee4\u6ce2\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u8bbe\u8ba1\u63d0\u793a\u56fe\u6765\u5bf9\u9f50\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u5149\u8c31\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u540c\u8d28\u6027\u56fe\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u6574\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u540c\u8d28\u6027\u7684\u4f4e\u9891\u77e5\u8bc6\uff0c\u672a\u80fd\u5904\u7406\u5177\u6709\u4e0d\u540c\u540c\u8d28\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u56fe\u4e2d\u591a\u6837\u5316\u7684\u5149\u8c31\u5206\u5e03\u3002\u5f53\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u5149\u8c31\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u65f6\uff0c\u5728\u6709\u9650\u7684\u76d1\u7763\u4e0b\uff0c\u77e5\u8bc6\u8f6c\u79fb\u7684\u6709\u6548\u6027\u4f1a\u53d7\u5230\u963b\u788d\u3002", "method": "HS-GPPT\u6a21\u578b\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6df7\u5408\u5149\u8c31\u6ee4\u6ce2\u5668\u9aa8\u5e72\u548c\u5c40\u90e8-\u5168\u5c40\u5bf9\u6bd4\u5b66\u4e60\u6765\u83b7\u53d6\u4e30\u5bcc\u7684\u8c31\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u63d0\u793a\u56fe\u6765\u5bf9\u9f50\u5149\u8c31\u5206\u5e03\u4e0e\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4fc3\u8fdb\u8c31\u77e5\u8bc6\u8de8\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u7684\u8f6c\u79fb\u3002", "result": "\u6240\u63d0\u51fa\u7684HS-GPPT\u6a21\u578b\u5728\u8f6c\u5bfc\u548c\u5f52\u7eb3\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u5149\u8c31\u5206\u5e03\u56fe\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u56fe\u7684\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u6574\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u76ee\u6807\u4e0b\u5bf9\u9f50\u4e0b\u6e38\u4efb\u52a1\u6765\u5b9e\u73b0\uff0c\u4ece\u800c\u5728\u6709\u9650\u7684\u76d1\u7763\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u540c\u8d28\u6027\u7684\u4f4e\u9891\u77e5\u8bc6\uff0c\u672a\u80fd\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u56fe\u4e2d\u5177\u6709\u4e0d\u540c\u540c\u8d28\u6027\u7684\u591a\u6837\u5149\u8c31\u5206\u5e03\u3002\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5149\u8c31\u7279\u5f02\u6027\u539f\u7406\uff1a\u6700\u4f18\u77e5\u8bc6\u8f6c\u79fb\u9700\u8981\u9884\u8bad\u7ec3\u5149\u8c31\u6ee4\u6ce2\u5668\u4e0e\u4e0b\u6e38\u56fe\u7684\u5185\u5728\u5149\u8c31\u5bf9\u9f50\u3002\u5728\u6709\u9650\u7684\u76d1\u7763\u4e0b\uff0c\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u5149\u8c31\u95f4\u9699\u4f1a\u963b\u788d\u6709\u6548\u7684\u9002\u5e94\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HS-GPPT\u6a21\u578b\uff0c\u4e00\u4e2a\u5728\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u6574\u8fc7\u7a0b\u4e2d\u786e\u4fdd\u5149\u8c31\u5bf9\u9f50\u7684\u65b0\u9896\u6846\u67b6\u3002\u6211\u4eec\u5229\u7528\u6df7\u5408\u5149\u8c31\u6ee4\u6ce2\u5668\u9aa8\u5e72\u548c\u5c40\u90e8-\u5168\u5c40\u5bf9\u6bd4\u5b66\u4e60\u6765\u83b7\u53d6\u4e30\u5bcc\u7684\u8c31\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u63d0\u793a\u56fe\uff0c\u5c06\u5149\u8c31\u5206\u5e03\u4e0e\u4e0a\u6587\u5bf9\u9f50\uff0c\u4fc3\u8fdb\u8de8\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u7684\u8c31\u77e5\u8bc6\u8f6c\u79fb\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u8f6c\u5bfc\u548c\u5f52\u7eb3\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://anonymous.4open.science/r/HS-GPPT-62D2/\u83b7\u5f97\u3002"}}
{"id": "2508.11170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "AI": {"tldr": "IOVQA\u901a\u8fc7\u6574\u6570\u6807\u7b7e\u548c\u76ee\u6807\u63a9\u7801\u7b56\u7565\u4f18\u5316VLM\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u89c6\u9891\u5185\u5bb9\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7ed3\u679c\u4e0d\u7cbe\u786e\u548c\u635f\u5931\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8VLM\u5728\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "IOVQA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6570\u636e\u96c6\u6784\u5efa\u4e2d\u5c06\u5341\u8fdb\u5236\u7684Overall_MOS\u8f6c\u6362\u4e3a\u6574\u6570\u4f5c\u4e3a\u6807\u7b7e\uff08\u8303\u56f4\u9650\u5236\u5728[10, 50]\uff09\uff0c\u5e76\u91c7\u7528\u76ee\u6807\u63a9\u7801\u7b56\u7565\uff0c\u5728\u635f\u5931\u8ba1\u7b97\u65f6\u4ec5\u4fdd\u7559\u6807\u7b7e\u7684\u524d\u4e24\u4f4d\u6574\u6570\uff0c\u8feb\u4f7f\u6a21\u578b\u5173\u6ce8\u5173\u952e\u7684\u6570\u503c\u8bc4\u4f30\u7ec4\u4ef6\u3002", "result": "\u4f7f\u7528IOVQA\u5fae\u8c03Qwen2.5-VL\u6a21\u578b\u540e\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728VQA\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5728VQualA 2025 GenAI-Bench AIGC\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u8d5b\uff08Track I\uff09\u4e2d\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "IOVQA\u65b9\u6cd5\u901a\u8fc7\u7ea6\u675f\u6a21\u578b\u8f93\u51fa\u4e3a\u6574\u6570\u3001\u91c7\u7528\u76ee\u6807\u63a9\u7801\u7b56\u7565\u8fdb\u884c\u635f\u5931\u8ba1\u7b97\uff0c\u6709\u6548\u63d0\u5347\u4e86VLM\u5728\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u4f18\u5316VLM\u5728\u91cf\u5316\u8bc4\u4f30\u573a\u666f\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u601d\u8def\uff0c\u5e76\u5728VQualA 2025 GenAI-Bench AIGC\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002"}}
{"id": "2508.11429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11429", "abs": "https://arxiv.org/abs/2508.11429", "authors": ["Shivam Dubey"], "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "AI": {"tldr": "HumorPlanSearch\u901a\u8fc7\u7ed3\u5408\u89c4\u5212\u3001\u6587\u5316\u63a8\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u65b0\u9896\u6027\u8fc7\u6ee4\u548c\u8fed\u4ee3\u4fee\u8ba2\uff0c\u5728\u81ea\u52a8\u5316\u5e7d\u9ed8\u751f\u6210\u4e2d\u660e\u786e\u5730\u5bf9\u4e0a\u4e0b\u6587\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u751f\u6210\u66f4\u5177\u9002\u5e94\u6027\u548c\u6587\u5316\u610f\u8bc6\u7684\u7b11\u8bdd\uff0c\u5e76\u5728\u8bc4\u4f30\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u5e7d\u9ed8\u751f\u6210\u5206\u6570\uff08HGS\uff09\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u5e7d\u9ed8\u751f\u6210\u901a\u5e38\u4f1a\u4ea7\u751f\u901a\u7528\u3001\u91cd\u590d\u6216\u4e0d\u5408\u65f6\u5b9c\u7684\u7b11\u8bdd\uff0c\u56e0\u4e3a\u5e7d\u9ed8\u662f\u6839\u690d\u4e8e\u60c5\u5883\u7684\uff0c\u5e76\u4e14\u53d6\u51b3\u4e8e\u542c\u4f17\u7684\u6587\u5316\u80cc\u666f\u3001\u5fc3\u6001\u548c\u5373\u65f6\u80cc\u666f\u3002", "method": "HumorPlanSearch\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u663e\u5f0f\u5730\u6a21\u62df\u4e0a\u4e0b\u6587\uff1a(1) Plan-Search\u7528\u4e8e\u591a\u6837\u5316\u3001\u9762\u5411\u4e3b\u9898\u7684\u7b56\u7565\uff1b(2) \u5e7d\u9ed8\u601d\u7ef4\u94fe\uff08HuCoT\uff09\u6a21\u677f\u6355\u6349\u6587\u5316\u548c\u98ce\u683c\u63a8\u7406\uff1b(3) \u77e5\u8bc6\u56fe\u8c31\u7528\u4e8e\u68c0\u7d22\u548c\u9002\u5e94\u9ad8\u6027\u80fd\u7684\u5386\u53f2\u7b56\u7565\uff1b(4) \u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u8fdb\u884c\u65b0\u9896\u6027\u8fc7\u6ee4\uff1b(5) \u8fed\u4ee3\u7684\u3001\u7531\u88c1\u5224\u9a71\u52a8\u7684\u4fee\u8ba2\u5faa\u73af\u3002", "result": "\u5728\u4e5d\u4e2a\u4e3b\u9898\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u5b8c\u6574\u7ba1\u9053\uff08KG + Revision\uff09\u5c06\u5e73\u5747HGS\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u9ad8\u4e8615.4%\uff08p < 0.05\uff09\u3002", "conclusion": "HumorPlanSearch\u901a\u8fc7\u5728\u4ece\u7b56\u7565\u89c4\u5212\u5230\u591a\u4fe1\u53f7\u8bc4\u4f30\u7684\u6bcf\u4e2a\u9636\u6bb5\u4f18\u5148\u8003\u8651\u4e0a\u4e0b\u6587\uff0c\u5c06\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5e7d\u9ed8\u63a8\u5411\u66f4\u8fde\u8d2f\u3001\u66f4\u9002\u5e94\u548c\u66f4\u5177\u6587\u5316\u610f\u8bc6\u7684\u559c\u5267\u3002"}}
{"id": "2508.11338", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11338", "abs": "https://arxiv.org/abs/2508.11338", "authors": ["Prathamesh Devadiga", "Yashmitha Shailesh"], "title": "RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading", "comment": null, "summary": "We introduce RegimeNAS, a novel differentiable architecture search framework\nspecifically designed to enhance cryptocurrency trading performance by\nexplicitly integrating market regime awareness. Addressing the limitations of\nstatic deep learning models in highly dynamic financial environments, RegimeNAS\nfeatures three core innovations: (1) a theoretically grounded Bayesian search\nspace optimizing architectures with provable convergence properties; (2)\nspecialized, dynamically activated neural modules (Volatility, Trend, and Range\nblocks) tailored for distinct market conditions; and (3) a multi-objective loss\nfunction incorporating market-specific penalties (e.g., volatility matching,\ntransition smoothness) alongside mathematically enforced Lipschitz stability\nconstraints. Regime identification leverages multi-head attention across\nmultiple timeframes for improved accuracy and uncertainty estimation. Rigorous\nempirical evaluation on extensive real-world cryptocurrency data demonstrates\nthat RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving\nan 80.3% Mean Absolute Error reduction compared to the best traditional\nrecurrent baseline and converging substantially faster (9 vs. 50+ epochs).\nAblation studies and regime-specific analysis confirm the critical contribution\nof each component, particularly the regime-aware adaptation mechanism. This\nwork underscores the imperative of embedding domain-specific knowledge, such as\nmarket regimes, directly within the NAS process to develop robust and adaptive\nmodels for challenging financial applications.", "AI": {"tldr": "RegimeNAS\u662f\u4e00\u4e2a\u65b0\u7684\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5e02\u573a\u72b6\u6001\u611f\u77e5\u6765\u63d0\u9ad8\u52a0\u5bc6\u8d27\u5e01\u4ea4\u6613\u6027\u80fd\u3002\u5b83\u4f7f\u7528\u8d1d\u53f6\u65af\u641c\u7d22\u7a7a\u95f4\u3001\u52a8\u6001\u795e\u7ecf\u6a21\u5757\u548c\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cRegimeNAS\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u91d1\u878d\u73af\u5883\u9ad8\u5ea6\u52a8\u6001\uff0c\u4f20\u7edf\u7684\u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u611f\u77e5\u548c\u9002\u5e94\u4e0d\u540c\u5e02\u573a\u72b6\u6001\uff08\u5982\u8d8b\u52bf\u3001\u6ce2\u52a8\u6027\u6216\u533a\u95f4\u4ea4\u6613\uff09\u7684\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u6613\u6027\u80fd\u3002", "method": "RegimeNAS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53ef\u5fae\u5206\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u5b83\u660e\u786e\u5730\u6574\u5408\u4e86\u5e02\u573a\u72b6\u6001\u611f\u77e5\uff0c\u4ee5\u589e\u5f3a\u52a0\u5bc6\u8d27\u5e01\u4ea4\u6613\u8868\u73b0\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1.\u4e00\u4e2a\u7ecf\u8fc7\u7406\u8bba\u9a8c\u8bc1\u7684\u8d1d\u53f6\u65af\u641c\u7d22\u7a7a\u95f4\uff0c\u4f18\u5316\u5177\u6709\u53ef\u8bc1\u660e\u6536\u655b\u7279\u6027\u7684\u67b6\u6784\uff1b2.\u4e13\u95e8\u7684\u3001\u52a8\u6001\u6fc0\u6d3b\u7684\u795e\u7ecf\u6a21\u5757\uff08\u6ce2\u52a8\u6027\u3001\u8d8b\u52bf\u548c\u8303\u56f4\u6a21\u5757\uff09\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5e02\u573a\u6761\u4ef6\uff1b3.\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\uff0c\u6574\u5408\u4e86\u5e02\u573a\u7279\u5b9a\u60e9\u7f5a\uff08\u4f8b\u5982\uff0c\u6ce2\u52a8\u6027\u5339\u914d\u3001\u8f6c\u6362\u5e73\u6ed1\u5ea6\uff09\u548c\u6570\u5b66\u4e0a\u5f3a\u5236\u6267\u884c\u7684Lipschitz\u7a33\u5b9a\u6027\u7ea6\u675f\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u52a0\u5bc6\u8d27\u5e01\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u4e25\u683c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cRegimeNAS\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u3002\u4e0e\u6700\u597d\u7684\u4f20\u7edf\u5faa\u73af\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e8680.3%\uff0c\u5e76\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff089\u4e2aepoch vs 50+\u4e2aepoch\uff09\u3002\u6d88\u878d\u7814\u7a76\u548c\u7279\u5b9a\u72b6\u6001\u5206\u6790\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u5173\u952e\u8d21\u732e\uff0c\u7279\u522b\u662f\u72b6\u6001\u611f\u77e5\u7684\u9002\u5e94\u673a\u5236\u3002", "conclusion": "RegimeNAS\u901a\u8fc7\u5c06\u5e02\u573a\u72b6\u6001\u611f\u77e5\u76f4\u63a5\u5d4c\u5165\u795e\u7ecf\u7ed3\u6784\u641c\u7d22\uff08NAS\uff09\u8fc7\u7a0b\uff0c\u4e3a\u91d1\u878d\u7b49\u52a8\u6001\u9886\u57df\u5f00\u53d1\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.11173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11173", "abs": "https://arxiv.org/abs/2508.11173", "authors": ["Ruobing Jiang", "Yang Liu", "Haobing Liu", "Yanwei Yu", "Chunyang Wang"], "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "comment": "Accepted by CIKM 2025. 10 pages, 5 figures,", "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIDOD\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u7c7b\u522b\u53d1\u73b0\uff08CCD\uff09\u3002IDOD\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u89e3\u51b3\u73b0\u6709CCD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1a\u72ec\u7acb\u591a\u6837\u6027\u4e30\u5bcc\u3001\u8054\u5408\u65b0\u9896\u6027\u53d1\u73b0\u548c\u57fa\u4e8e\u6b63\u4ea4\u6027\u7684\u8fde\u7eed\u9012\u589e\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cIDOD\u5728\u4e0d\u5360\u7528\u66f4\u591a\u5b58\u50a8\u7a7a\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u66f4\u597d\u5730\u5904\u7406\u4e86\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u5206\u7c7b\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u51cf\u5c11\u4e86\u9519\u8bef\u7d2f\u79ef\uff0c\u5e76\u6709\u6548\u9632\u6b62\u4e86\u9057\u5fd8\u3002\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u7531\u4e8e\u65b0\u5230\u8fbe\u7684\u6570\u636e\u4e2d\u6ca1\u6709\u7c7b\u522b\u6570\u91cf\u548c\u6807\u7b7e\uff0c\u5e76\u4e14\u9700\u8981\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u56e0\u6b64\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u5927\u591a\u6570CCD\u65b9\u6cd5\u4e0d\u80fd\u5f88\u597d\u5730\u5904\u7406\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u5206\u7c7b\u4e4b\u95f4\u7684\u77db\u76fe\u3002\u5b83\u4eec\u8fd8\u5bb9\u6613\u5728\u9010\u6e10\u53d1\u73b0\u65b0\u7c7b\u522b\u7684\u8fc7\u7a0b\u4e2d\u7d2f\u79ef\u9519\u8bef\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u548c\u6570\u636e\u91cd\u653e\u6765\u9632\u6b62\u9057\u5fd8\uff0c\u8fd9\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5b58\u50a8\u7a7a\u95f4\u3002", "method": "IDOD\u4e3b\u8981\u5305\u62ec\u72ec\u7acb\u591a\u6837\u6027\u4e30\u5bcc\u6a21\u5757\u3001\u8054\u5408\u65b0\u9896\u6027\u53d1\u73b0\u6a21\u5757\u548c\u57fa\u4e8e\u6b63\u4ea4\u6027\u7684\u8fde\u7eed\u9012\u589e\u6a21\u5757\u3002\u5728\u72ec\u7acb\u4e30\u5bcc\u4e2d\uff0c\u4e3b\u5e72\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u5355\u72ec\u8bad\u7ec3\uff0c\u4ee5\u907f\u514d\u5176\u4ec5\u5173\u6ce8\u5206\u7c7b\u7279\u5f81\u3002\u8054\u5408\u53d1\u73b0\u5c06\u591a\u9636\u6bb5\u65b0\u9896\u7c7b\u522b\u53d1\u73b0\u8f6c\u5316\u4e3a\u5355\u9636\u6bb5\uff0c\u51cf\u5c11\u4e86\u9519\u8bef\u7d2f\u79ef\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u6b63\u4ea4\u6027\u7684\u8fde\u7eed\u9012\u589e\u6a21\u5757\u751f\u6210\u76f8\u4e92\u6b63\u4ea4\u7684\u7c7b\u522b\u539f\u578b\uff0c\u5e76\u901a\u8fc7\u4ee3\u8868\u6027\u8868\u793a\u91cd\u653e\u4ee5\u8f83\u4f4e\u7684\u7a7a\u95f4\u5f00\u9500\u9632\u6b62\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11434", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11434", "abs": "https://arxiv.org/abs/2508.11434", "authors": ["Aditi Dutta", "Susan Banducci"], "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u5206\u7c7b\u7f51\u7edc\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e38\u5e38\u5c06\u5176\u8bef\u5224\u4e3a\u6709\u5bb3\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u654f\u611f\u65f6\u671f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u538b\u5236\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u58f0\u97f3\u3002\u7814\u7a76\u5efa\u8bae\u6539\u8fdb\u5ba1\u6838\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u7eb3\u5165\u4eba\u5de5\u5ba1\u6838\u548c\u5305\u542b\u53cd\u51fb\u6027\u8a00\u8bba\u7684\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u5728\u7ebf\u6c11\u4e3b\u8fa9\u8bba\u4e2d\uff0c\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533a\u5206\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u548c\u5b83\u6240\u62b5\u5236\u7684\u6027\u522b\u6b67\u89c6\u65b9\u9762\u53ef\u80fd\u5b58\u5728\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u4e94\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf92022\u5e74\u82f1\u56fd\u6d89\u53ca\u5973\u6027\u8bae\u5458\u7684\u653f\u6cbb\u63a8\u6587\u8fdb\u884c\u5206\u7c7b\uff0c\u8003\u5bdf\u6a21\u578b\u5982\u4f55\u533a\u5206\u6027\u522b\u6b67\u89c6\u3001\u53cd\u6027\u522b\u6b67\u89c6\u548c\u4e2d\u6027\u63a8\u6587\uff0c\u7279\u522b\u5173\u6ce8\u9ad8\u654f\u611f\u5ea6\u89e6\u53d1\u4e8b\u4ef6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u7ecf\u5e38\u9519\u8bef\u5730\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u5206\u7c7b\u4e3a\u6709\u5bb3\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u6027\u5f3a\u7684\u4e8b\u4ef6\u4e2d\uff0c\u6b64\u65f6\u6709\u5bb3\u8a00\u8bba\u548c\u62b5\u6297\u8a00\u8bba\u7684\u4fee\u8f9e\u98ce\u683c\u8d8b\u4e8e\u4e00\u81f4\u3002", "conclusion": "\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u5728\u533a\u5206\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u548c\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u65f6\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u653f\u6cbb\u654f\u611f\u4e8b\u4ef6\u4e2d\u3002\u73b0\u6709\u6a21\u578b\u9891\u7e41\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u9519\u8bef\u5206\u7c7b\u4e3a\u6709\u5bb3\u5185\u5bb9\uff0c\u8fd9\u53ef\u80fd\u538b\u5236\u6311\u6218\u6027\u522b\u6b67\u89c6\u7684\u58f0\u97f3\uff0c\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u5ba1\u6838\u7cfb\u7edf\u7684\u8bbe\u8ba1\u9700\u8981\u8d85\u8d8a\u4e8c\u5143\u5206\u7c7b\uff0c\u6574\u5408\u4eba\u5de5\u5ba1\u6838\uff0c\u5e76\u660e\u786e\u5305\u542b\u53cd\u51fb\u6027\u8a00\u8bba\u7684\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.11345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11345", "abs": "https://arxiv.org/abs/2508.11345", "authors": ["Shuqi Liu", "Jianguo Huang", "Luke Ong"], "title": "Conformal Prediction Meets Long-tail Classification", "comment": null, "summary": "Conformal Prediction (CP) is a popular method for uncertainty quantification\nthat converts a pretrained model's point prediction into a prediction set, with\nthe set size reflecting the model's confidence. Although existing CP methods\nare guaranteed to achieve marginal coverage, they often exhibit imbalanced\ncoverage across classes under long-tail label distributions, tending to over\ncover the head classes at the expense of under covering the remaining tail\nclasses. This under coverage is particularly concerning, as it undermines the\nreliability of the prediction sets for minority classes, even with coverage\nensured on average. In this paper, we propose the Tail-Aware Conformal\nPrediction (TACP) method to mitigate the under coverage of the tail classes by\nutilizing the long-tail structure and narrowing the head-tail coverage gap.\nTheoretical analysis shows that it consistently achieves a smaller head-tail\ncoverage gap than standard methods. To further improve coverage balance across\nall classes, we introduce an extension of TACP: soft TACP (sTACP) via a\nreweighting mechanism. The proposed framework can be combined with various\nnon-conformity scores, and experiments on multiple long-tail benchmark datasets\ndemonstrate the effectiveness of our methods.", "AI": {"tldr": "CP methods have imbalanced coverage on long-tail data. TACP and sTACP are proposed to fix this by being tail-aware and using reweighting, improving minority class predictions.", "motivation": "Existing CP methods exhibit imbalanced coverage across classes under long-tail distributions, leading to under coverage of minority classes, which undermines reliability.", "method": "Conformal Prediction (CP) converts a pretrained model's point prediction into a prediction set. Tail-Aware Conformal Prediction (TACP) utilizes the long-tail structure to narrow the head-tail coverage gap. Soft TACP (sTACP) further improves coverage balance via a reweighting mechanism.", "result": "TACP consistently achieves a smaller head-tail coverage gap than standard methods. Experiments demonstrate the effectiveness of TACP and sTACP on multiple long-tail benchmark datasets.", "conclusion": "TACP and sTACP are proposed to mitigate the under coverage of tail classes and improve coverage balance, showing effectiveness on long-tail datasets."}}
{"id": "2508.11176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11176", "abs": "https://arxiv.org/abs/2508.11176", "authors": ["Yumiao Zhao", "Bo Jiang", "Yuhe Ding", "Xiao Wang", "Jin Tang", "Bin Luo"], "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning", "comment": null, "summary": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.", "AI": {"tldr": "LatHAdapter\u901a\u8fc7\u5229\u7528\u53cc\u66f2\u5b66\u4e60\u548c\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u6539\u8fdb\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9002\u914d\u5668\u5fae\u8c03\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e00\u5bf9\u591a\u5173\u8054\u548c\u672a\u77e5\u7c7b\u522b\u6cdb\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u5728\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7528\u4e8e\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u65f6\uff0c\u901a\u5e38\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u663e\u5f0f\u7a7a\u95f4\u90bb\u8fd1\u6027\u6765\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u4f46\u8fd9\u672a\u80fd\u6355\u83b7\u7c7b\u522b\u4e0e\u56fe\u50cf\u6837\u672c\u4e4b\u95f4\u56fa\u6709\u7684\u4e00\u5bf9\u591a\u5173\u8054\uff0c\u5e76\u4e14\u96be\u4ee5\u5efa\u7acb\u672a\u77e5\u7c7b\u522b\u4e0e\u56fe\u50cf\u4e4b\u95f4\u7684\u51c6\u786e\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6f5c\u5728\u5206\u5c42\u9002\u914d\u5668\uff08LatHAdapter\uff09\uff0c\u5229\u7528\u4e0b\u6e38\u8bad\u7ec3\u6570\u636e\u7684\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u201c\u5c5e\u6027\u201d\u63d0\u793a\u4f5c\u4e3a\u6865\u6881\u6765\u5bf9\u9f50\u7c7b\u522b\u548c\u56fe\u50cf\uff0c\u5e76\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6295\u5f71\uff0c\u5229\u7528\u5206\u5c42\u6b63\u5219\u5316\u6765\u5b66\u4e60\u7c7b\u522b\u3001\u53ef\u5b66\u4e60\u5c5e\u6027\u548c\u56fe\u50cf\u6837\u672c\u4e4b\u95f4\u7684\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u9002\u914d\u5668\u5728\u6355\u83b7\u7c7b\u522b\u4e0e\u56fe\u50cf\u6837\u672c\u4e4b\u95f4\u7684\u4e00\u5bf9\u591a\u5173\u8054\u4ee5\u53ca\u5efa\u7acb\u672a\u77e5\u7c7b\u522b\u4e0e\u56fe\u50cf\u4e4b\u95f4\u51c6\u786e\u5173\u8054\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "result": "LatHAdapter \u80fd\u591f\u5b8c\u5168\u6a21\u62df\u7c7b\u522b\u3001\u53ef\u5b66\u4e60\u5c5e\u6027\u548c\u56fe\u50cf\u6837\u672c\u4e4b\u95f4\u56fa\u6709\u7684\u4e00\u5bf9\u591a\u5173\u8054\uff0c\u4ece\u800c\u5728\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "LatHAdapter \u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5c0f\u6837\u672c\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9002\u5e94\u5df2\u77e5\u7c7b\u522b\u548c\u6cdb\u5316\u5230\u672a\u77e5\u7c7b\u522b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.11442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u4e4b\u95f4\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u65f6\u5e38\u89c1\u7684\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5b66\u4e60\u7edf\u4e00\u7684\u6587\u672c\u5d4c\u5165\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u4e0b\u6e38\u4efb\u52a1\u662f\u4e00\u4e2a\u6838\u5fc3\u76ee\u6807\uff0c\u4f46\u8d1f\u8fc1\u79fb\uff08negative transfer\uff09\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8054\u5408\u8bad\u7ec3\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\uff08STS\uff09\u7684\u5355\u4e00\u7f16\u7801\u5668\u65f6\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u6743\u8861\u3002", "method": "CoDiEmb\u6846\u67b6\u6574\u5408\u4e86\u4efb\u52a1\u4e13\u4e1a\u5316\u76ee\u6807\u3001\u52a8\u6001\u91c7\u6837\u5668\u3001\u5bf9\u6bd4\u635f\u5931\u3001\u8de8\u8bbe\u5907\u91c7\u6837\u3001\u611f\u77e5\u987a\u5e8f\u76ee\u6807\u548cdelta-guided\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86IR\u548cSTS\u4efb\u52a1\u7684\u6709\u6548\u8054\u5408\u4f18\u5316\u3002", "result": "CoDiEmb\u572815\u4e2a\u6807\u51c6\u7684IR\u548cSTS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u51cf\u8f7b\u4e86\u8de8\u4efb\u52a1\u7684\u6743\u8861\uff0c\u8fd8\u53ef\u8861\u91cf\u5730\u6539\u5584\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\u3002", "conclusion": "CoDiEmb\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u4fe1\u53f7\u3001\u91c7\u7528delta-guided\u6a21\u578b\u878d\u5408\u7b56\u7565\u548c\u9ad8\u6548\u7684\u5355\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6210\u529f\u5730\u7f13\u89e3\u4e86\u8de8\u4efb\u52a1\u7684\u6743\u8861\uff0c\u5e76\u6539\u5584\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\u3002"}}
{"id": "2508.11348", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11348", "abs": "https://arxiv.org/abs/2508.11348", "authors": ["Xiaohan Bi", "Binhang Qi", "Hailong Sun", "Xiang Gao", "Yue Yu", "Xiaojun Liang"], "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models", "comment": null, "summary": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization.", "AI": {"tldr": "NeMo is a new method for making deep learning models smaller and easier to reuse by breaking them into modules during training. It works on different types of models, including large ones like Transformers, and improves performance while reducing size.", "motivation": "Existing MwT methods are limited to small-scale CNN models at the convolutional kernel level and struggle with diverse DNNs and large-scale models, particularly Transformer-based models. There is a need for a scalable and generalizable MwT approach to address the prohibitive construction costs of DNN models in software systems.", "method": "NeMo proposes a scalable and generalizable MwT approach that operates at the neuron level, ensuring applicability to Transformers and various architectures. It utilizes a contrastive learning-based modular training method with a composite loss function for scalability to large-scale models.", "result": "Comprehensive experiments on Transformer-based and CNN models demonstrate NeMo", "conclusion": "NeMo is a scalable and generalizable modularizing-while-training (MwT) approach that operates at the neuron level, making it applicable to diverse DNN architectures including Transformers. It outperforms existing MwT methods, achieving significant gains in module classification accuracy and reduction in module size. NeMo also shows potential benefits in practical scenarios through a case study on open-source projects."}}
{"id": "2508.11183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11183", "abs": "https://arxiv.org/abs/2508.11183", "authors": ["Zhenghao Chen", "Zicong Chen", "Lei Liu", "Yiming Wu", "Dong Xu"], "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "comment": null, "summary": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.", "AI": {"tldr": "GVT\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6807\u8bb0\u5668\uff0c\u5229\u75282D\u9ad8\u65af\u6cfc\u6e85\u6765\u63d0\u9ad8\u7a7a\u95f4\u9002\u5e94\u6027\u548c\u65f6\u95f4\u901a\u7528\u6027\uff0c\u5728\u89c6\u9891\u91cd\u5efa\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u538b\u7f29\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6807\u8bb0\u65b9\u6cd5\uff08\u5982\u56fa\u5b9a\u7f51\u683c\u548c\u5757\u72b6\u6807\u8bb0\uff09\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u90fd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4f4e\u7684\u533a\u57df\u8fc7\u5ea6\u7f16\u7801\uff0c\u5e76\u4e14\u96be\u4ee5\u5728\u4e0d\u533a\u5206\u9759\u6001\u548c\u52a8\u6001\u5185\u5bb9\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5197\u4f59\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9ad8\u65af\u89c6\u9891\u53d8\u6362\u5668\uff08GVT\uff09\u7684\u901a\u7528\u89c6\u9891\u6807\u8bb0\u5668\uff0c\u8be5\u6807\u8bb0\u5668\u57fa\u4e8e\u751f\u6210\u5f0f2D\u9ad8\u65af\u6cfc\u6e85\uff082DGS\uff09\u7b56\u7565\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u65f6\u7a7a\u9ad8\u65af\u5d4c\u5165\uff08STGE\uff09\u673a\u5236\uff0c\u5c06\u89c6\u9891\u526a\u8f91\u7684\u6f5c\u5728\u521a\u6027\u7279\u5f81\u8868\u793a\u4e3a\u4e00\u7ec42D\u9ad8\u65af\u3002\u901a\u8fc7\u9ad8\u65af\u96c6\u5408\u5212\u5206\uff08GSP\uff09\u7b56\u7565\u5c062D\u9ad8\u65af\u5206\u4e3a\u9759\u6001\u548c\u52a8\u6001\u96c6\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u901a\u7528\u6027\u3002", "result": "GVT\u5728UCF101\u3001Kinetics\u548cDavis\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u89c6\u9891\u91cd\u5efa\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8d28\u91cf\uff0c\u5728\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bMAGVIT-v2\uff0c\u5e76\u5728\u89c6\u9891\u538b\u7f29\u65b9\u9762\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002", "conclusion": "GVT\u5728\u89c6\u9891\u91cd\u5efa\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u538b\u7f29\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u89c6\u9891\u91cd\u5efa\u65b9\u9762\u8d28\u91cf\u6700\u9ad8\uff0c\u5728\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bMAGVIT-v2\uff0c\u5728\u89c6\u9891\u538b\u7f29\u65b9\u9762\u8868\u73b0\u76f8\u5f53\u3002"}}
{"id": "2508.11454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11454", "abs": "https://arxiv.org/abs/2508.11454", "authors": ["Junichiro Niimi"], "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "AI": {"tldr": "\u4e0e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f8\u6bd4\uff0c\u4f7f\u7528JSON\u683c\u5f0f\u548c\u9644\u52a0\u4fe1\u606f\u7684\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u8425\u9500\u7406\u8bba\uff0c\u5982\u9884\u671f\u7406\u8bba\u548c\u671f\u671b-\u786e\u8ba4\u7406\u8bba\u6307\u51fa\uff0c\u5ba2\u6237\u7684\u8bc4\u4ef7\u4e0d\u4ec5\u53d7\u5230\u5b9e\u9645\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u8fd8\u53d7\u5230\u5176\u4ed6\u53c2\u8003\u70b9\u7684\u5851\u9020\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e9b\u8865\u5145\u4fe1\u606f\u7684\u5185\u5bb9\u548c\u683c\u5f0f\u5982\u4f55\u5f71\u54cd\u4f7f\u7528LLM\u7684\u60c5\u611f\u5206\u6790\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u81ea\u7136\u8bed\u8a00\uff08NL\uff09\u548cJSON\u683c\u5f0f\u7684\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u9002\u5408\u5b9e\u9645\u8425\u9500\u5e94\u7528\u7684\u8f7b\u91cf\u7ea73B\u53c2\u6570\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u8fd9\u4e9b\u8865\u5145\u4fe1\u606f\u7684\u5185\u5bb9\u548c\u683c\u5f0f\u5982\u4f55\u5f71\u54cd\u4f7f\u7528LLM\u8fdb\u884c\u7684\u60c5\u611f\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2aYelp\u7c7b\u522b\uff08\u9910\u5385\u548c\u591c\u751f\u6d3b\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5e26\u6709\u9644\u52a0\u4fe1\u606f\u7684JSON\u63d0\u793a\u5728\u672a\u7ecf\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff1aMacro-F1\u5206\u522b\u63d0\u9ad8\u4e861.6%\u548c4%\uff0c\u800cRMSE\u5206\u522b\u964d\u4f4e\u4e8616%\u548c9.1%\uff0c\u4f7f\u5176\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u6b64\u5916\uff0c\u540e\u7eed\u5206\u6790\u8bc1\u5b9e\uff0c\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u771f\u6b63\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u800c\u975e\u6807\u7b7e\u4ee3\u7406\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u53ef\u4ee5\u4f7f\u8f83\u5c0f\u7684\u6a21\u578b\u83b7\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11349", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11349", "abs": "https://arxiv.org/abs/2508.11349", "authors": ["Angela John", "Selvyn Allotey", "Till Koebe", "Alexandra Tyukavina", "Ingmar Weber"], "title": "A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts", "comment": "10 figures", "summary": "Afforestation and reforestation are popular strategies for mitigating climate\nchange by enhancing carbon sequestration. However, the effectiveness of these\nefforts is often self-reported by project developers, or certified through\nprocesses with limited external validation. This leads to concerns about data\nreliability and project integrity. In response to increasing scrutiny of\nvoluntary carbon markets, this study presents a dataset on global afforestation\nand reforestation efforts compiled from primary (meta-)information and\naugmented with time-series satellite imagery and other secondary data. Our\ndataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.\nSince any remote sensing-based validation effort relies on the integrity of a\nplanting site's geographic boundary, this dataset introduces a standardized\nassessment of the provided site-level location information, which we summarize\nin one easy-to-communicate key indicator: LDIS -- the Location Data Integrity\nScore. We find that approximately 79\\% of the georeferenced planting sites\nmonitored fail on at least 1 out of 10 LDIS indicators, while 15\\% of the\nmonitored projects lack machine-readable georeferenced data in the first place.\nIn addition to enhancing accountability in the voluntary carbon market, the\npresented dataset also holds value as training data for e.g. computer\nvision-related tasks with millions of linked Sentinel-2 and Planetscope\nsatellite images.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u4e00\u624b\u4fe1\u606f\u548c\u536b\u661f\u6570\u636e\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5168\u7403\u690d\u6811\u9020\u6797\u9879\u76ee\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4d\u7f6e\u6570\u636e\u5b8c\u6574\u6027\u8bc4\u5206\uff08LDIS\uff09\u6765\u8bc4\u4f30\u9879\u76ee\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5927\u90e8\u5206\u9879\u76ee\u5b58\u5728\u5730\u7406\u4f4d\u7f6e\u6570\u636e\u95ee\u9898\u3002", "motivation": "\u5e94\u5bf9\u5f53\u524d\u690d\u6811\u9020\u6797\u548c\u518d\u9020\u6797\u9879\u76ee\u5728\u78b3\u6c47\u589e\u52a0\u65b9\u9762\u7684\u6548\u679c\u5e38\u4f9d\u8d56\u4e8e\u9879\u76ee\u5f00\u53d1\u8005\u81ea\u884c\u62a5\u544a\u6216\u6709\u9650\u7684\u5916\u90e8\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u6570\u636e\u53ef\u9760\u6027\u548c\u9879\u76ee\u5b8c\u6574\u6027\u53d7\u5230\u8d28\u7591\uff0c\u589e\u52a0\u4e86\u81ea\u613f\u78b3\u5e02\u573a\u7684\u5ba1\u67e5\u538b\u529b\u3002", "method": "\u901a\u8fc7\u6574\u5408\u4e00\u624b\uff08\u5143\uff09\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u536b\u661f\u56fe\u50cf\u548c\u5176\u4ed6\u4e8c\u624b\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u7403\u690d\u6811\u9020\u6797\u548c\u518d\u9020\u6797\u9879\u76ee\u6570\u636e\u96c6\u3002\u5f15\u5165\u4e86\u4f4d\u7f6e\u6570\u636e\u5b8c\u6574\u6027\u8bc4\u5206\uff08LDIS\uff09\u6765\u8bc4\u4f30\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\u7684\u51c6\u786e\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b1,289,068\u4e2a\u79cd\u690d\u70b9\u300145,628\u4e2a\u9879\u76ee\u3001\u8986\u76d633\u5e74\u7684\u5168\u7403\u690d\u6811\u9020\u6797\u548c\u518d\u9020\u6797\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u663e\u793a\uff0c79%\u7684\u5730\u7406\u53c2\u8003\u79cd\u690d\u70b9\u81f3\u5c11\u5728\u4e00\u4e2aLDIS\u6307\u6807\u4e0a\u5b58\u5728\u95ee\u9898\uff0c15%\u7684\u9879\u76ee\u7f3a\u4e4f\u673a\u5668\u53ef\u8bfb\u7684\u5730\u7406\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,289,068\u4e2a\u79cd\u690d\u70b9\u548c45,628\u4e2a\u9879\u76ee\u7684\u5168\u7403\u690d\u6811\u9020\u6797\u548c\u518d\u9020\u6797\u9879\u76ee\u6570\u636e\u96c6\uff0c\u6db5\u76d633\u5e74\u5386\u53f2\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7ea679%\u7684\u5730\u7406\u53c2\u8003\u79cd\u690d\u70b9\u5728\u81f3\u5c11\u4e00\u4e2a\u4f4d\u7f6e\u6570\u636e\u5b8c\u6574\u6027\uff08LDIS\uff09\u6307\u6807\u4e0a\u5b58\u5728\u7f3a\u9677\uff0c15%\u7684\u9879\u76ee\u7f3a\u4e4f\u673a\u5668\u53ef\u8bfb\u7684\u5730\u7406\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63d0\u9ad8\u81ea\u613f\u78b3\u5e02\u573a\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\uff0c\u5e76\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.11185", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11185", "abs": "https://arxiv.org/abs/2508.11185", "authors": ["Abhinav Kumar", "Yuliang Guo", "Zhihao Zhang", "Xinyu Huang", "Liu Ren", "Xiaoming Liu"], "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector", "comment": "ICCV 2025", "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R", "AI": {"tldr": "This paper analyzes the impact of camera height variations on monocular 3D object detectors, finding depth estimation to be a key factor. It proposes CHARM3R, a method that averages depth estimates to improve generalization to unseen camera heights, achieving state-of-the-art performance on the CARLA dataset.", "motivation": "Monocular 3D object detectors struggle with unseen or out-of-distribution camera heights. This paper investigates the impact of camera height variations on SoTA Mono3D models and proposes a solution.", "method": "CHARM3R averages both depth estimates within the model to mitigate the impact of camera height variations.", "result": "The study observes that depth estimation is a primary factor influencing performance under height variations. It also mathematically proves and empirically observes consistent negative and positive trends in mean depth error of regressed and ground-based depth models, respectively, under camera height changes.", "conclusion": "The proposed CHARM3R method improves generalization to unseen camera heights by more than 45%, achieving SoTA performance on the CARLA dataset."}}
{"id": "2508.11534", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautait\u0117", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u80fd\u8bc6\u522b\u7269\u79cd\u4e3b\u4e49\u8a00\u8bba\u4f46\u5bf9\u5176\u9053\u5fb7\u8bc4\u4ef7\u4e0d\u9ad8\uff0c\u4e14\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u4f18\u5148\u8003\u8651\u4eba\u7c7b\u800c\u975e\u52a8\u7269\uff0c\u5c24\u5176\u662f\u5728\u52a8\u7269\u80fd\u529b\u4e0d\u5982\u4eba\u7c7b\u7684\u60c5\u51b5\u4e0b\u3002\u6a21\u578b\u5728\u5904\u7406\u519c\u573a\u52a8\u7269\u7684\u4f24\u5bb3\u65f6\u8868\u73b0\u51fa\u53cc\u91cd\u6807\u51c6\u3002\u7814\u7a76\u8ba4\u4e3a\uff0cLLMs\u7684\u504f\u89c1\u53ef\u80fd\u6e90\u4e8e\u5bf9\u8ba4\u77e5\u80fd\u529b\u7684\u4fa7\u91cd\uff0c\u800c\u975e\u5355\u7eaf\u7684\u7269\u79cd\u6b67\u89c6\u3002\u4e3aAI\u516c\u5e73\u6027\u7eb3\u5165\u5bf9\u975e\u4eba\u7c7b\u52a8\u7269\u7684\u8003\u91cf\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u8003\u5bdf\u5b83\u4eec\u7684\u4f26\u7406\u503e\u5411\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u5bf9\u4eba\u5de5\u667a\u80fd\u516c\u5e73\u548c\u6b67\u89c6\u7684\u7814\u7a76\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5LLMs\u662f\u5426\u8868\u73b0\u51fa\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\uff08\u57fa\u4e8e\u7269\u79cd\u6210\u5458\u7684\u6b67\u89c6\uff09\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u8bc4\u4ef7\u975e\u4eba\u7c7b\u52a8\u7269\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e09\u79cd\u8303\u5f0f\u6765\u7cfb\u7edf\u5730\u7814\u7a76LLMs\u7684\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\uff1a1. \u7269\u79cd\u4e3b\u4e49\u57fa\u51c6\u6d4b\u8bd5\uff08SpeciesismBench\uff09\uff1a\u4e00\u4e2a\u5305\u542b1003\u4e2a\u9879\u76ee\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u8bc6\u522b\u548c\u9053\u5fb7\u8bc4\u4ef7\u7269\u79cd\u4e3b\u4e49\u9648\u8ff0\u7684\u80fd\u529b\u30022. \u5fc3\u7406\u6d4b\u91cf\u5b66\u65b9\u6cd5\uff1a\u4f7f\u7528\u5df2\u5efa\u7acb\u7684\u5fc3\u7406\u6d4b\u91cf\u5b66\u5de5\u5177\uff0c\u6bd4\u8f83LLMs\u7684\u53cd\u5e94\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u53cd\u5e94\u30023. \u6587\u672c\u751f\u6210\u4efb\u52a1\uff1a\u901a\u8fc7\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u63a2\u67e5LLMs\u5728\u9610\u8ff0\u6216\u62b5\u5236\u7269\u79cd\u4e3b\u4e49\u5408\u7406\u5316\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5728\u7269\u79cd\u4e3b\u4e49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLMs\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u7269\u79cd\u4e3b\u4e49\u9648\u8ff0\uff0c\u4f46\u5f88\u5c11\u8c34\u8d23\u5b83\u4eec\uff0c\u5e76\u5e38\u5e38\u8ba4\u4e3a\u7269\u79cd\u4e3b\u4e49\u6001\u5ea6\u5728\u9053\u5fb7\u4e0a\u662f\u53ef\u63a5\u53d7\u7684\u3002\u5728\u5fc3\u7406\u6d4b\u91cf\u5b66\u7814\u7a76\u4e2d\uff0c\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff1aLLMs\u8868\u8fbe\u7684\u663e\u5f0f\u7269\u79cd\u4e3b\u4e49\u7565\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u76f4\u63a5\u6743\u8861\u4e2d\uff0c\u5b83\u4eec\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u62ef\u6551\u4e00\u4e2a\u4eba\u7c7b\u800c\u4e0d\u662f\u591a\u4e2a\u52a8\u7269\u3002\u521d\u6b65\u89e3\u91ca\u662f\uff0cLLMs\u53ef\u80fd\u66f4\u770b\u91cd\u8ba4\u77e5\u80fd\u529b\u800c\u975e\u7269\u79cd\u672c\u8eab\uff1a\u5f53\u80fd\u529b\u76f8\u540c\u65f6\uff0c\u5b83\u4eec\u4e0d\u8868\u73b0\u51fa\u7269\u79cd\u504f\u597d\uff1b\u5f53\u52a8\u7269\u88ab\u63cf\u8ff0\u4e3a\u80fd\u529b\u66f4\u5f3a\u65f6\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u8be5\u52a8\u7269\u800c\u975e\u80fd\u529b\u8f83\u5f31\u7684\u4eba\u7c7b\u3002\u5728\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLMs\u7ecf\u5e38\u4e3a\u519c\u573a\u52a8\u7269\u7684\u4f24\u5bb3\u8fa9\u62a4\u6216\u5408\u7406\u5316\uff0c\u5374\u62d2\u7edd\u4e3a\u975e\u519c\u573a\u52a8\u7269\u8fd9\u6837\u505a\u3002", "conclusion": "LLMs\u5728\u8bc6\u522b\u548c\u8bc4\u4f30\u7269\u79cd\u4e3b\u4e49\u9648\u8ff0\u65b9\u9762\u8868\u73b0\u51fa\u80fd\u529b\uff0c\u4f46\u5e38\u5e38\u4e0d\u8c34\u8d23\uff0c\u5e76\u5c06\u7269\u79cd\u4e3b\u4e49\u6001\u5ea6\u89c6\u4e3a\u9053\u5fb7\u4e0a\u53ef\u63a5\u53d7\u7684\u3002\u5728\u663e\u5f0f\u8bc4\u4f30\u4e2d\uff0cLLMs\u8868\u73b0\u51fa\u7565\u4f4e\u7684\u7269\u79cd\u4e3b\u4e49\u503e\u5411\uff0c\u4f46\u5728\u76f4\u63a5\u6743\u8861\u4e2d\uff0c\u5b83\u4eec\u66f4\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u4eba\u7c7b\u3002\u5f53\u80fd\u529b\u76f8\u540c\u65f6\uff0c\u5b83\u4eec\u4e0d\u8868\u73b0\u51fa\u7269\u79cd\u504f\u597d\uff0c\u5e76\u4e14\u5728\u52a8\u7269\u88ab\u63cf\u8ff0\u4e3a\u80fd\u529b\u66f4\u5f3a\u65f6\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u52a8\u7269\u800c\u4e0d\u662f\u80fd\u529b\u8f83\u5f31\u7684\u4eba\u7c7b\u3002\u5728\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLMs\u7ecf\u5e38\u6b63\u5e38\u5316\u6216\u5408\u7406\u5316\u5bf9\u519c\u573a\u52a8\u7269\u7684\u4f24\u5bb3\uff0c\u540c\u65f6\u62d2\u7edd\u4e3a\u975e\u519c\u573a\u52a8\u7269\u8fd9\u6837\u505a\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLMs\u53cd\u6620\u4e86\u8fdb\u6b65\u548c\u4e3b\u6d41\u4eba\u7c7b\u89c2\u70b9\u7684\u6df7\u5408\u4f53\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u91cd\u73b0\u4e86\u56f4\u7ed5\u52a8\u7269\u5265\u524a\u7684\u6839\u6df1\u8482\u56fa\u7684\u6587\u5316\u89c4\u8303\u3002\u4e3aAI\u516c\u5e73\u548c\u5bf9\u9f50\u6846\u67b6\u7eb3\u5165\u975e\u4eba\u7c7b\u9053\u5fb7\u60a3\u8005\u5bf9\u4e8e\u51cf\u5c11\u8fd9\u4e9b\u504f\u89c1\u548c\u9632\u6b62\u7269\u79cd\u4e3b\u4e49\u6001\u5ea6\u5728AI\u7cfb\u7edf\u548c\u793e\u4f1a\u4e2d\u6839\u6df1\u8482\u56fa\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.11353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11353", "abs": "https://arxiv.org/abs/2508.11353", "authors": ["Han Zhou", "Hongpeng Yin", "Xuanhong Deng", "Yuyu Huang", "Hao Ren"], "title": "Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning", "comment": null, "summary": "Many real-world data are sequentially collected over time and often exhibit\nskewed class distributions, resulting in imbalanced data streams. While\nexisting approaches have explored several strategies, such as resampling and\nreweighting, for imbalanced data stream learning, our work distinguishes itself\nby addressing the imbalance problem through training modification, particularly\nfocusing on gradient descent techniques. We introduce the harmonized gradient\ndescent (HGD) algorithm, which aims to equalize the norms of gradients across\ndifferent classes. By ensuring the gradient norm balance, HGD mitigates\nunder-fitting for minor classes and achieves balanced online learning. Notably,\nHGD operates in a streamlined implementation process, requiring no data-buffer,\nextra parameters, or prior knowledge, making it applicable to any learning\nmodels utilizing gradient descent for optimization. Theoretical analysis, based\non a few common and mild assumptions, shows that HGD achieves a satisfied\nsub-linear regret bound. The proposed algorithm are compared with the commonly\nused online imbalance learning methods under several imbalanced data stream\nscenarios. Extensive experimental evaluations demonstrate the efficiency and\neffectiveness of HGD in learning imbalanced data streams.", "AI": {"tldr": "HGD \u662f\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u68af\u5ea6\u8303\u6570\u6765\u89e3\u51b3\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u95ee\u9898\uff0c\u65e0\u9700\u6570\u636e\u7f13\u51b2\u6216\u989d\u5916\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u666e\u904d\u5b58\u5728\u7684\u3001\u6309\u65f6\u95f4\u987a\u5e8f\u6536\u96c6\u4e14\u5177\u6709\u504f\u659c\u7c7b\u522b\u5206\u5e03\u7684\uff08\u5373\u4e0d\u5e73\u8861\uff09\u6570\u636e\u6d41\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u68af\u5ea6\u4e0b\u964d\u6280\u672f\uff0c\u5f15\u5165\u4e86 harmonized gradient descent (HGD) \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u65e8\u5728\u5e73\u8861\u4e0d\u540c\u7c7b\u522b\u7684\u68af\u5ea6\u8303\u6570\u3002", "result": "HGD \u5b9e\u73b0\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6b21\u7ebf\u6027\u9057\u61be\u754c\u9650\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u5e38\u7528\u7684\u5728\u7ebf\u4e0d\u5e73\u8861\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\u8bc1\u660e\u4e86\u5176\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "HGD \u7b97\u6cd5\u5728\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u65b9\u9762\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u5728\u51cf\u5c11\u6b21\u7c7b\u6b20\u62df\u5408\u548c\u5b9e\u73b0\u5747\u8861\u5728\u7ebf\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.11192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11192", "abs": "https://arxiv.org/abs/2508.11192", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Lin Li", "Andrea Colaco"], "title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark", "comment": null, "summary": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u4e24\u4eba\u5bf9\u8bdd\uff0c\u4ee5\u534f\u52a9\u5b8c\u6210\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a HowToDIV \u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5305\u542b 507 \u6b21\u5bf9\u8bdd\u30016636 \u4e2a\u95ee\u7b54\u5bf9\u548c 24 \u5c0f\u65f6\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d6\u4e86\u70f9\u996a\u3001\u673a\u68b0\u548c\u79cd\u690d\u7b49\u9886\u57df\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u5efa\u7acb\u4e86\u57fa\u4e8e Gemma-3 \u6a21\u578b\u7684\u57fa\u51c6\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u8bb8\u591a\u65e5\u5e38\u4efb\u52a1\uff0c\u5982\u4fee\u7406\u8bbe\u5907\u3001\u70f9\u996a\u98df\u8c31\u548c\u6c7d\u8f66\u4fdd\u517b\uff0c\u90fd\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5c24\u5176\u662f\u5f53\u4efb\u52a1\u590d\u6742\u4e14\u6d89\u53ca\u591a\u4e2a\u6b65\u9aa4\u65f6\u3002\u5c3d\u7ba1\u4eba\u4eec\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u5174\u8da3\u65e5\u76ca\u6d53\u539a\uff0c\u4f46\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u534f\u52a9\u7684\u3001\u4ee5\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e3a\u57fa\u7840\u7684\u5bf9\u8bdd\u89c6\u9891\u6570\u636e\u96c6\u5374\u5f88\u5c11\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u4e24\u4eba\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u8be5\u5bf9\u8bdd\u4e0e\u7ec6\u7c92\u5ea6\u6b65\u9aa4\u548c\u89c6\u9891\u526a\u8f91\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5168\u81ea\u52a8\u5316\uff0c\u4ee5\u514b\u670d\u4eba\u5de5\u6536\u96c6\u6570\u636e\u6210\u672c\u9ad8\u6602\u7684\u7f3a\u70b9\u3002", "result": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aHowToDIV\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 507 \u6b21\u5bf9\u8bdd\u30016636 \u4e2a\u95ee\u7b54\u5bf9\u4ee5\u53ca\u8de8\u8d8a\u70f9\u996a\u3001\u673a\u68b0\u548c\u79cd\u690d\u7b49\u4e0d\u540c\u4efb\u52a1\u7684 24 \u5c0f\u65f6\u89c6\u9891\u7247\u6bb5\u3002\u6211\u4eec\u8fd8\u4f7f\u7528 Gemma-3 \u6a21\u578b\u5728 HowToDIV \u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u57fa\u51c6\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5728\u6b64\u7c7b\u7a0b\u5e8f\u6027\u4efb\u52a1\u8f85\u52a9\u5bf9\u8bdd\u7684\u65b0\u4efb\u52a1\u4e0a\u5f00\u5c55\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u4e24\u4eba\u5bf9\u8bdd\u4ee5\u8fdb\u884c\u4efb\u52a1\u6307\u5bfc\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0e\u7ec6\u7c92\u5ea6\u6b65\u9aa4\u548c\u89c6\u9891\u7247\u6bb5\u5bf9\u9f50\u3002\u6211\u4eec\u7684\u5168\u81ea\u52a8\u65b9\u6cd5\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u652f\u6301\uff0c\u4e3a\u4eba\u7c7b\u8f85\u52a9\u6570\u636e\u6536\u96c6\u6240\u9700\u7684\u5927\u91cf\u6210\u672c\u548c\u7cbe\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11536", "abs": "https://arxiv.org/abs/2508.11536", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "title": "Language models align with brain regions that represent concepts across modalities", "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5728\u5185\u90e8\u8868\u793a\u8de8\u6a21\u6001\u6982\u5ff5\u542b\u4e49\uff0c\u5373\u4f7f\u5728\u8bed\u8a00\u5904\u7406\u4e0d\u654f\u611f\u7684\u5927\u8111\u533a\u57df\u4e5f\u662f\u5982\u6b64\u3002", "motivation": "\u89e3\u51b3\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u4e2d\u533a\u5206\u8bed\u8a00\u8868\u5f81\u4e0e\u6982\u5ff5\u610f\u4e49\u8868\u5f81\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u4e2d\u5b58\u5728\u7684\u76f8\u540c\u95ee\u9898\u3002", "method": "\u4f7f\u7528fMRI\u6570\u636e\u96c6\uff08Pereira et al., 2018\uff09\u7684\u5b9e\u9a8c\uff0c\u5c06\u8bed\u8a00\u6a21\u578b-\u5927\u8111\u5bf9\u9f50\u4e0e\u4e24\u4e2a\u795e\u7ecf\u6307\u6807\u76f8\u5173\u8054\uff1a\uff081\uff09\u5904\u7406\u53e5\u5b50\u65f6\u7684\u5927\u8111\u6fc0\u6d3b\u6c34\u5e73\uff0c\uff082\uff09\u8de8\u8f93\u5165\u6a21\u6001\u7684\u610f\u4e49\u4e00\u81f4\u6027\u7684\u65b0\u6d4b\u91cf\u65b9\u6cd5\uff0c\u91cf\u5316\u5927\u8111\u533a\u57df\u5982\u4f55\u8de8\u8303\u5f0f\uff08\u53e5\u5b50\u3001\u8bcd\u4e91\u3001\u56fe\u50cf\uff09\u4e00\u81f4\u5730\u54cd\u5e94\u76f8\u540c\u6982\u5ff5\u3002", "result": "\u8bed\u8a00-\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u8a00-\u89c6\u89c9\u6a21\u578b\u90fd\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u5927\u8111\u4e2d\u610f\u4e49\u66f4\u4e00\u81f4\u533a\u57df\u7684\u4fe1\u53f7\uff0c\u5373\u4f7f\u8fd9\u4e9b\u533a\u57df\u5bf9\u8bed\u8a00\u5904\u7406\u4e0d\u654f\u611f\u3002", "conclusion": "LM\u53ef\u80fd\u5728\u5185\u90e8\u8868\u793a\u8de8\u6a21\u6001\u6982\u5ff5\u542b\u4e49"}}
{"id": "2508.11356", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11356", "abs": "https://arxiv.org/abs/2508.11356", "authors": ["Jia Liu", "ChangYi He", "YingQiao Lin", "MingMin Yang", "FeiYang Shen", "ShaoGuo Liu", "TingTing Gao"], "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism", "comment": null, "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u71b5\u673a\u5236\u6765\u89e3\u51b3\u73b0\u6709TTRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76d1\u7763\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u9002\u5e94\u6027\u6709\u9650\u3001\u4ee5\u53ca\u73b0\u6709\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\u65b9\u6cd5\u9762\u4e34\u7684\u9ad8\u63a8\u7406\u6210\u672c\u548c\u65e9\u671f\u4f30\u8ba1\u504f\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u673a\u5236\uff0c\u5305\u62ec\u71b5-\u53c9\u6811\u591a\u6570\u6eda\u52a8\uff08ETMR\uff09\u548c\u57fa\u4e8e\u71b5\u7684\u4f18\u52bf\u91cd\u5851\uff08EAR\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7fLlama3.1-8B\u5728AIME 2024\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e8668%\u7684Pass at 1\u6307\u6807\u76f8\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u4ec5\u6d88\u8017\u4e8660%\u7684\u6eda\u52a8\u4ee4\u724c\u9884\u7b97\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u673a\u5236\uff0c\u5728\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u4e2d\u589e\u5f3a\u4e86\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u73b0\u6709TTRL\u65b9\u6cd5\u9762\u4e34\u7684\u9ad8\u63a8\u7406\u6210\u672c\u548c\u65e9\u671f\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u63a8\u7406\u6548\u7387\u3001\u591a\u6837\u6027\u548c\u4f30\u8ba1\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u7528\u4e8e\u5f00\u653e\u57df\u63a8\u7406\u4efb\u52a1\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "AI": {"tldr": "\u9488\u5bf9\u65e0\u4eba\u673a\u822a\u7a7a\u5f71\u50cf\u7684\u6311\u6218\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7VLM Uav-vl-r1\uff0c\u91c7\u7528SFT\u548c\u57fa\u4e8eGRPO\u7684RL\u6df7\u5408\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165HRVQA-VL\u6570\u636e\u96c6\u3002\u7ed3\u679c\u663e\u793aUAV-VL-R1\u5728\u51c6\u786e\u7387\u3001\u9c81\u68d2\u6027\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9002\u5408\u65e0\u4eba\u673a\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u822a\u7a7a\u5f71\u50cf\u4e0a\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u56e0\u4e3a\u822a\u7a7a\u5f71\u50cf\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u3001\u590d\u6742\u7684\u7a7a\u95f4\u8bed\u4e49\u548c\u4e25\u683c\u7684\u5b9e\u65f6\u6027\u9650\u5236\u3002\u8fd9\u4e9b\u6311\u6218\u963b\u788d\u4e86\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u822a\u7a7a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUAV-VL-R1\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002\u5229\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u5956\u52b1\u548c\u7ec4\u5185\u7b56\u7565\u5bf9\u9f50\u6765\u4fc3\u8fdb\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6HRVQA-VL\uff0c\u5305\u542b50,019\u4e2a\u6807\u6ce8\u6837\u672c\uff0c\u6db5\u76d6\u516b\u79cd\u4e0e\u65e0\u4eba\u673a\u76f8\u5173\u7684\u63a8\u7406\u4efb\u52a1\u3002", "result": "UAV-VL-R1\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578bQwen2-VL-2B-Instruct\u9ad848.17%\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u517672B\u89c4\u6a21\u7684\u7248\u672c\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0cSFT\u63d0\u9ad8\u4e86\u8bed\u4e49\u5bf9\u9f50\uff0c\u4f46\u53ef\u80fd\u964d\u4f4e\u4e86\u6570\u5b66\u4efb\u52a1\u7684\u63a8\u7406\u591a\u6837\u6027\uff0c\u800c\u57fa\u4e8eGRPO\u7684RL\u589e\u5f3a\u4e86\u903b\u8f91\u7075\u6d3b\u6027\u548c\u63a8\u7406\u9c81\u68d2\u6027\u3002UAV-VL-R1\u5728FP16\u4e0b\u4ec5\u97003.9GB\u5185\u5b58\uff0c\u91cf\u5316\u4e3aINT8\u540e\u4e3a2.5GB\uff0c\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "UAV-VL-R1\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u822a\u7a7a\u89c6\u89c9\u63a8\u7406\uff0c\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u822a\u7a7a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u90e8\u7f72\u6027\u3002"}}
{"id": "2508.11567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11567", "abs": "https://arxiv.org/abs/2508.11567", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u4e0e\u60a3\u8005\u7684\u5bf9\u8bdd\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u63d0\u95ee\u548c\u6811\u72b6\u7ed3\u6784\u8bb0\u5fc6\u6765\u63d0\u9ad8\u4fe1\u606f\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u8ddf\u8e2a\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u4e34\u5e8a\u533b\u751f\u65b9\u6cd5\u53d7\u5230\u5408\u683c\u4e13\u4e1a\u4eba\u5458\u77ed\u7f3a\u7684\u9650\u5236\uff0c\u800c\u73b0\u6709\u7684\u81ea\u52a8\u5fc3\u7406\u8bc4\u4f30\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u9759\u6001\u6587\u672c\u5206\u6790\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u4ea4\u4e92\u548c\u8fed\u4ee3\u63d0\u95ee\u4e2d\u4ea7\u751f\u7684\u66f4\u6df1\u5c42\u6b21\u548c\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u8d1f\u8d23\u63d0\u95ee\u3001\u5145\u5206\u6027\u8bc4\u4f30\u3001\u8bc4\u5206\u548c\u66f4\u65b0\u7684\u4e13\u95e8\u667a\u80fd\u4f53\u3002\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u63d0\u95ee\u673a\u5236\uff0c\u7531\u8bc4\u4f30\u667a\u80fd\u4f53\u8bc4\u4f30\u7528\u6237\u56de\u5e94\u7684\u5145\u5206\u6027\uff0c\u4ee5\u786e\u5b9a\u662f\u5426\u6709\u5fc5\u8981\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u540e\u7eed\u67e5\u8be2\u3002\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8bb0\u5fc6\uff0c\u6839\u8282\u70b9\u7f16\u7801\u7528\u6237\u57fa\u672c\u4fe1\u606f\uff0c\u5b50\u8282\u70b9\u6839\u636e\u4e0d\u540c\u7684\u75c7\u72b6\u7c7b\u522b\u548c\u4ea4\u4e92\u8f6e\u6b21\u7ec4\u7ec7\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728DAIC-WOZ\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728DAIC-WOZ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11357", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11357", "abs": "https://arxiv.org/abs/2508.11357", "authors": ["Changhong Jing", "Yan Liu", "Shuqiang Wang", "Bruce X. B. Yu", "Gong Chen", "Zhejing Hu", "Zhi Zhang", "Yanyan Shen"], "title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding", "comment": null, "summary": "Cross-subject electroencephalography (EEG) decoding remains a fundamental\nchallenge in brain-computer interface (BCI) research due to substantial\ninter-subject variability and the scarcity of subject-invariant\nrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariant\nSpatio-temporal Modeling), a novel framework for interpretable and robust EEG\ndecoding across unseen subjects. PTSM employs a dual-branch masking mechanism\nthat independently learns personalized and shared spatio-temporal patterns,\nenabling the model to preserve individual-specific neural characteristics while\nextracting task-relevant, population-shared features. The masks are factorized\nacross temporal and spatial dimensions, allowing fine-grained modulation of\ndynamic EEG patterns with low computational overhead. To further address\nrepresentational entanglement, PTSM enforces information-theoretic constraints\nthat decompose latent embeddings into orthogonal task-related and\nsubject-related subspaces. The model is trained end-to-end via a\nmulti-objective loss integrating classification, contrastive, and\ndisentanglement objectives. Extensive experiments on cross-subject motor\nimagery datasets demonstrate that PTSM achieves strong zero-shot\ngeneralization, outperforming state-of-the-art baselines without\nsubject-specific calibration. Results highlight the efficacy of disentangled\nneural representations for achieving both personalized and transferable\ndecoding in non-stationary neurophysiological settings.", "AI": {"tldr": "PTSM\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u6a21\u5f0f\u548c\u6f5c\u5728\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u8de8\u53d7\u8bd5\u8005\u7684EEG\u89e3\u7801\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8de8\u53d7\u8bd5\u8005\u8111\u7535\u56fe\uff08EEG\uff09\u89e3\u7801\u7531\u4e8e\u53d7\u8bd5\u8005\u95f4\u7684\u53d8\u5f02\u6027\u548c\u53d7\u8bd5\u8005\u4e0d\u53d8\u8868\u5f81\u7684\u7a00\u7f3a\u6027\uff0c\u4ecd\u7136\u662f\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u6839\u672c\u6311\u6218\u3002", "method": "PTSM\uff08\u751f\u7406\u611f\u77e5\u548c\u4efb\u52a1\u4e0d\u53d8\u65f6\u7a7a\u5efa\u6a21\uff09\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5206\u652f\u63a9\u7801\u673a\u5236\u6765\u72ec\u7acb\u5b66\u4e60\u4e2a\u6027\u5316\u548c\u5171\u4eab\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u8bba\u7ea6\u675f\u5c06\u6f5c\u5728\u5d4c\u5165\u5206\u89e3\u4e3a\u6b63\u4ea4\u7684\u4efb\u52a1\u76f8\u5173\u548c\u53d7\u8bd5\u8005\u76f8\u5173\u5b50\u7a7a\u95f4\u3002\u6a21\u578b\u901a\u8fc7\u6574\u5408\u5206\u7c7b\u3001\u5bf9\u6bd4\u5ea6\u548c\u89e3\u8026\u76ee\u6807\u7684\u591a\u76ee\u6807\u635f\u5931\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "PTSM\u5728\u8de8\u53d7\u8bd5\u8005\u8fd0\u52a8\u60f3\u8c61\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5176\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6ca1\u6709\u53d7\u8bd5\u8005\u7279\u5b9a\u6821\u51c6\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PTSM\u901a\u8fc7\u89e3\u8026\u7684\u795e\u7ecf\u8868\u5f81\u5728\u975e\u5e73\u7a33\u795e\u7ecf\u751f\u7406\u5b66\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u548c\u53ef\u8fc1\u79fb\u7684\u89e3\u7801\uff0c\u5728\u8de8\u53d7\u8bd5\u8005\u8fd0\u52a8\u60f3\u8c61\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.11212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11212", "abs": "https://arxiv.org/abs/2508.11212", "authors": ["Zhangjian Ji", "Wenjin Zhang", "Shaotong Qiao", "Kai Feng", "Yuhua Qian"], "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network", "comment": null, "summary": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7c97\u5230\u7ec6\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5173\u8282\u7ed3\u6784\u635f\u5931\u548c\u56fe\u50cf\u5f15\u5bfc\u6e10\u8fdb\u56fe\u5377\u79ef\u7f51\u7edc\uff08IGP-GCN\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u672a\u80fd\u5145\u5206\u5229\u7528\u5173\u8282\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u5728COCO\u548cCrowdPose\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u83b7\u5f97\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u8f7b\u91cf\u7ea7\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u59ff\u6001\u77e5\u8bc6\u4ece\u5f3a\u5927\u7684\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u53c2\u6570\u8f83\u5c11\u5b66\u751f\u6a21\u578b\u662f\u4e00\u79cd\u53ef\u884c\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u672a\u80fd\u5145\u5206\u63a2\u7d22\u4eba\u4f53\u5173\u8282\u95f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7c97\u5230\u7ec6\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\u84b8\u998f\u4e2d\uff0c\u5f15\u5165\u4e86\u4eba\u4f53\u5173\u8282\u7ed3\u6784\u635f\u5931\u6765\u6316\u6398\u5173\u8282\u95f4\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5c06\u9ad8\u5c42\u8bed\u4e49\u77e5\u8bc6\u4ece\u6559\u5e08\u6a21\u578b\u8fc1\u79fb\u5230\u5b66\u751f\u6a21\u578b\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\u84b8\u998f\u4e2d\uff0c\u5229\u7528\u56fe\u50cf\u5f15\u5bfc\u6e10\u8fdb\u56fe\u5377\u79ef\u7f51\u7edc\uff08IGP-GCN\uff09\u6765\u4f18\u5316\u4ece\u7b2c\u4e00\u9636\u6bb5\u83b7\u5f97\u7684\u521d\u59cb\u4eba\u4f53\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u6700\u7ec8\u8f93\u51fa\u6765\u6e10\u8fdb\u5730\u76d1\u7763IGP-GCN\u7684\u8bad\u7ec3\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728COCO\u5173\u952e\u70b9\u548cCrowdPose\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u8bb8\u591a\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u66f4\u590d\u6742\u7684CrowdPose\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728COCO\u5173\u952e\u70b9\u548cCrowdPose\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u8bb8\u591a\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u66f4\u590d\u6742\u7684CrowdPose\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002"}}
{"id": "2508.11582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "AI": {"tldr": "DR. SAF\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u6765\u63d0\u9ad8LLM\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u94fe\u601d\u60f3\uff08CoT\uff09\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u96be\u5ea6\u5148\u9a8c\u7684\u4e0d\u8db3\u3002", "method": "DR. SAF\u6846\u67b6\u6574\u5408\u4e86\u8fb9\u754c\u81ea\u6211\u610f\u8bc6\u5bf9\u9f50\u3001\u81ea\u9002\u5e94\u5956\u52b1\u7ba1\u7406\u548c\u8fb9\u754c\u4fdd\u7559\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u52a8\u6001\u8bc4\u4f30\u548c\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "result": "DR. SAF\u6846\u67b6\u5b9e\u73b0\u4e8649.27%\u7684\u54cd\u5e94\u4ee4\u724c\u603b\u6570\u51cf\u5c11\uff0c\u51c6\u786e\u6027\u635f\u5931\u6781\u5c0f\uff1b\u4ee4\u724c\u6548\u7387\u63d0\u9ad8\u4e866.59\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e865\u500d\u3002\u5728\u6781\u7aef\u8bad\u7ec3\u4e0b\uff0cDR. SAF\u5728\u4ee4\u724c\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u4f20\u7edf\u6307\u4ee4\u6a21\u578b\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e8616%\u4ee5\u4e0a\u3002", "conclusion": "DR. SAF\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u51cf\u5c11\u54cd\u5e94\u4ee4\u724c\u3001\u63d0\u9ad8\u4ee4\u724c\u6548\u7387\u548c\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\u3002"}}
{"id": "2508.11363", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.11363", "abs": "https://arxiv.org/abs/2508.11363", "authors": ["Sadegh Khorasani", "Saber Salehkaleybar", "Negar Kiyavash", "Matthias Grossglauser"], "title": "Fusing Rewards and Preferences in Reinforcement Learning", "comment": null, "summary": "We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that\nfuses both individual rewards and pairwise preferences (if available) into a\nsingle update rule. DFA uses the policy's log-probabilities directly to model\nthe preference probability, avoiding a separate reward-modeling step.\nPreferences can be provided by human-annotators (at state-level or\ntrajectory-level) or be synthesized online from Q-values stored in an\noff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing\nDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)\npolicy. Our simulation results show that DFA trained on generated preferences\nmatches or exceeds SAC on six control environments and demonstrates a more\nstable training process. With only a semi-synthetic preference dataset under\nBradley-Terry model, our algorithm outperforms reward-modeling reinforcement\nlearning from human feedback (RLHF) baselines in a stochastic GridWorld and\napproaches the performance of an oracle with true rewards.", "AI": {"tldr": "DFA is a reinforcement learning algorithm that combines individual rewards and pairwise preferences, outperforming existing methods in control environments and GridWorld.", "motivation": "To fuse both individual rewards and pairwise preferences into a single update rule for reinforcement learning.", "method": "DFA uses the policy's log-probabilities directly to model the preference probability, avoiding a separate reward-modeling step. Preferences can be provided by human-annotators or be synthesized online from Q-values. Under a Bradley-Terry model, minimizing DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC) policy.", "result": "DFA trained on generated preferences matches or exceeds SAC on six control environments and demonstrates a more stable training process. DFA outperforms reward-modeling RLHF baselines in a stochastic GridWorld and approaches the performance of an oracle with true rewards.", "conclusion": "DFA matches or exceeds SAC on six control environments and demonstrates a more stable training process. With only a semi-synthetic preference dataset under Bradley-Terry model, DFA outperforms reward-modeling RLHF baselines in a stochastic GridWorld and approaches the performance of an oracle with true rewards."}}
{"id": "2508.11218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11218", "abs": "https://arxiv.org/abs/2508.11218", "authors": ["Jialin Li", "Shuqi Wu", "Ning Wang"], "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "comment": null, "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7UMM\u6846\u67b6\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u884c\u4ebaReID\u4e2d\u7684\u4e0d\u786e\u5b9a\u6a21\u6001\u95ee\u9898\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u9c81\u68d2\u7684\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u6765\u81eaRGB\u3001\u7ea2\u5916\u3001\u8349\u56fe\u6216\u6587\u672c\u63cf\u8ff0\u7b49\u4e0d\u786e\u5b9a\u6216\u7f3a\u5931\u7684\u8f93\u5165\u6a21\u6001\u5bf9\u4f20\u7edfReID\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u5927\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u975e\u786e\u5b9a\u6027\u6a21\u6001\u5efa\u6a21\uff08UMM\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u591a\u6a21\u6001\u4ee4\u724c\u6620\u5c04\u5668\u3001\u5408\u6210\u6a21\u6001\u589e\u5f3a\u7b56\u7565\u548c\u8de8\u6a21\u6001\u7ebf\u7d22\u4ea4\u4e92\u5b66\u4e60\u5668\u3002UMM\u5229\u7528CLIP\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u65e0\u9700\u5927\u91cf\u5fae\u8c03\u5373\u53ef\u6709\u6548\u5730\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUMM\u5728\u4e0d\u786e\u5b9a\u7684\u6a21\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "UMM\u6846\u67b6\u5728\u4e0d\u786e\u5b9a\u7684\u6a21\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11598", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11598", "abs": "https://arxiv.org/abs/2508.11598", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "AI": {"tldr": "AuriStream\u662f\u4e00\u4e2a\u53d7\u751f\u7269\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u8033\u8717\u4ee4\u724c\u548c\u81ea\u56de\u5f52\u6a21\u578b\u5b66\u4e60\u8bed\u97f3\u8868\u793a\uff0c\u5e76\u5728SUPERB\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u53ef\u89c6\u5316\u97f3\u9891\u3002", "motivation": "\u53d7\u5230\u4eba\u7c7b\u542c\u89c9\u5904\u7406\u5c42\u7ea7\u7684\u542f\u53d1\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u751f\u7269\u5b66\u542f\u53d1\u7684\u6a21\u578bAuriStream\uff0c\u7528\u4e8e\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\u5bf9\u8bed\u97f3\u8fdb\u884c\u7f16\u7801\u3002", "method": "AuriStream\u9996\u5148\u5c06\u539f\u59cb\u97f3\u9891\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4eba\u8033\u8033\u8717\u7684\u65f6\u9891\u8868\u793a\uff0c\u63d0\u53d6\u79bb\u6563\u7684\u8033\u8717\u4ee4\u724c\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\u5e94\u7528\u4e8e\u8fd9\u4e9b\u8033\u8717\u4ee4\u724c\u3002", "result": "AuriStream\u5b66\u4e60\u4e86\u6709\u610f\u4e49\u7684\u97f3\u7d20\u548c\u5355\u8bcd\u8868\u793a\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u8bcd\u6c47\u8bed\u4e49\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u4e0b\u6e38SUPERB\u8bed\u97f3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cAuriStream\u751f\u6210\u7684\u97f3\u9891\u7eed\u7eed\u53ef\u4ee5\u5728\u9891\u8c31\u56fe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5e76\u89e3\u7801\u56de\u97f3\u9891\uff0c\u4ece\u800c\u6df1\u5165\u4e86\u89e3\u6a21\u578b\u7684\u9884\u6d4b\u3002", "conclusion": "AuriStream\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u5177\u4eba\u7c7b\u6a21\u4eff\u80fd\u529b\u7684\u3001\u80fd\u9ad8\u6548\u5904\u7406\u5404\u79cd\u8bed\u97f3\u4efb\u52a1\u7684\u6a21\u578b\u3002"}}
{"id": "2508.11365", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11365", "abs": "https://arxiv.org/abs/2508.11365", "authors": ["Jayanta Mandi", "Ali \u0130rfan Mahmuto\u011fullar\u0131", "Senne Berden", "Tias Guns"], "title": "Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization", "comment": null, "summary": "Decision-focused learning (DFL) trains a machine learning (ML) model to\npredict parameters of an optimization problem, to directly minimize decision\nregret, i.e., maximize decision quality. Gradient-based DFL requires computing\nthe derivative of the solution to the optimization problem with respect to the\npredicted parameters. However, for many optimization problems, such as linear\nprograms (LPs), the gradient of the regret with respect to the predicted\nparameters is zero almost everywhere. Existing gradient-based DFL approaches\nfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LP\ninto a differentiable optimization problem by adding a quadratic regularizer\nand then minimizing the regret directly or (b) minimizing surrogate losses that\nhave informative (sub)gradients. In this paper, we show that the former\napproach still results in zero gradients, because even after smoothing the\nregret remains constant across large regions of the parameter space. To address\nthis, we propose minimizing surrogate losses -- even when a differentiable\noptimization layer is used and regret can be minimized directly. Our\nexperiments demonstrate that minimizing surrogate losses allows differentiable\noptimization layers to achieve regret comparable to or better than\nsurrogate-loss based DFL methods. Further, we demonstrate that this also holds\nfor DYS-Net, a recently proposed differentiable optimization technique for LPs,\nthat computes approximate solutions and gradients through operations that can\nbe performed using feedforward neural network layers. Because DYS-Net executes\nthe forward and the backward pass very efficiently, by minimizing surrogate\nlosses using DYS-Net, we are able to attain regret on par with the\nstate-of-the-art while reducing training time by a significant margin.", "AI": {"tldr": "Gradient-based DFL for LPs often fails due to zero gradients. Smoothing doesn't fix this. Minimizing surrogate losses works better, even with differentiable layers like DYS-Net, leading to similar or better results with much faster training.", "motivation": "Existing gradient-based DFL approaches for LPs struggle because the gradient of the regret with respect to predicted parameters is often zero. Smoothing the LP or minimizing surrogate losses are common workarounds. This paper aims to address the limitations of smoothing methods and demonstrate the effectiveness of minimizing surrogate losses, even with differentiable optimization layers.", "method": "The paper proposes minimizing surrogate losses for decision-focused learning (DFL) with linear programs (LPs), even when a differentiable optimization layer is used. This approach is applied to DYS-Net, a differentiable optimization technique for LPs, to compute approximate solutions and gradients using feedforward neural networks.", "result": "Experiments show that minimizing surrogate losses with differentiable optimization layers achieves regret comparable to or better than existing DFL methods. When applied to DYS-Net, this approach achieves state-of-the-art regret while significantly reducing training time.", "conclusion": "DFL methods that minimize surrogate losses can achieve regret comparable to or better than existing methods, even when using a differentiable optimization layer. Applying this to DYS-Net significantly reduces training time while maintaining state-of-the-art regret. Therefore, minimizing surrogate losses is a viable and efficient approach for DFL, particularly with differentiable optimization techniques for LPs."}}
{"id": "2508.11255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11255", "abs": "https://arxiv.org/abs/2508.11255", "authors": ["MengChao Wang", "Qiang Wang", "Fan Jiang", "Mu Xu"], "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "comment": "https://fantasy-amap.github.io/fantasy-talking2/", "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "AI": {"tldr": "\u901a\u8fc7Talking-Critic\u5956\u52b1\u6a21\u578b\u548cTLPO\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u65b9\u6cd5\u5728\u6ee1\u8db3\u591a\u7ef4\u5ea6\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u753b\u8d28\u91cf\u548c\u5bf9\u9f50\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6ee1\u8db3\u8fd0\u52a8\u81ea\u7136\u5ea6\u3001\u5507\u8bed\u540c\u6b65\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u7b49\u591a\u7ef4\u5ea6\u7cbe\u7ec6\u5316\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u4f18\u5316\u76f8\u4e92\u51b2\u7a81\u7684\u504f\u597d\u76ee\u6807\u4ee5\u53ca\u7f3a\u4e4f\u5305\u542b\u591a\u7ef4\u5ea6\u504f\u597d\u6ce8\u91ca\u7684\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTLPO\uff08Timestep-Layer adaptive multi-expert Preference Optimization\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u504f\u597d\u89e3\u8026\u4e3a\u4e13\u95e8\u7684\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u8de8\u65f6\u95f4\u6b65\u548c\u7f51\u7edc\u5c42\u8fdb\u884c\u878d\u5408\uff0c\u4ece\u800c\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u5168\u9762\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u589e\u5f3a\uff0c\u4e14\u4e0d\u4f1a\u76f8\u4e92\u5e72\u6270\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86Talking-Critic\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u51fd\u6570\u3002", "result": "Talking-Critic\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1bTLPO\u5728\u5507\u8bed\u540c\u6b65\u7cbe\u5ea6\u3001\u52a8\u4f5c\u81ea\u7136\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TLPO\u6846\u67b6\u5728\u5507\u8bed\u540c\u6b65\u7cbe\u5ea6\u3001\u52a8\u4f5c\u81ea\u7136\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5e76\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002Talking-Critic\u5728\u4e0e\u4eba\u7c7b\u504f\u597d\u8bc4\u7ea7\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11605", "abs": "https://arxiv.org/abs/2508.11605", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "title": "Dataset Creation for Visual Entailment using Generative AI", "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u8574\u542b\u6a21\u578b\u3002\u8be5\u6570\u636e\u96c6\u57fa\u4e8eSNLI\u6570\u636e\u96c6\uff0c\u5229\u7528Stable Diffusion\u751f\u6210\u56fe\u50cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u5408\u6210\u6570\u636e\u96c6\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u65b9\u9762\u6548\u679c\u826f\u597d\uff0c\u53ef\u4f5c\u4e3a\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8574\u542b\u6570\u636e\u96c6\u76f8\u5bf9\u4e8e\u6587\u672c\u8574\u542b\u6570\u636e\u96c6\u800c\u8a00\uff0c\u89c4\u6a21\u5c0f\u4e14\u6570\u636e\u7a00\u758f\uff0c\u624b\u52a8\u521b\u5efa\u6210\u672c\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5229\u7528SNLI\u6570\u636e\u96c6\u7684\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\uff0c\u901a\u8fc7Stable Diffusion\u751f\u6210\u56fe\u50cf\uff0c\u4ee5\u6784\u5efa\u89c6\u89c9\u8574\u542b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728SNLI-VE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578bF\u5206\u6570\u4ece0.703\u4e0b\u964d\u52300.686\uff1b\u5728SICK-VTE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cF\u5206\u6570\u4ece0.400\u4e0b\u964d\u52300.384\u3002\u8fd9\u8868\u660e\u5408\u6210\u6570\u636e\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u751f\u6210\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u4f5c\u4e3a\u89c6\u89c9\u8574\u542b\u6a21\u578b\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u4e14\u53ea\u4f1a\u5728\u6027\u80fd\u4e0a\u5e26\u6765\u5fae\u5c0f\u7684\u4e0b\u964d\u3002"}}
{"id": "2508.11390", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11390", "abs": "https://arxiv.org/abs/2508.11390", "authors": ["Michael Banf", "Dominik Filipiak", "Max Schattauer", "Liliya Imasheva"], "title": "A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting", "comment": null, "summary": "Graph Neural Networks are highly effective at learning from relational data,\nleveraging node and edge features while maintaining the symmetries inherent to\ngraph structures. However, many real-world systems, such as social or\nbiological networks, exhibit complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nGeometric and Topological Deep Learning addresses this challenge by introducing\nmethods that utilize and benefit from higher-order structures. Central to TDL\nis the concept of lifting, which transforms data representations from basic\ngraph forms to more expressive topologies before the application of GNN models\nfor learning. In this work, we propose a structural lifting strategy using\nForman-Ricci curvature, which defines an edge-based network characteristic\nbased on Riemannian geometry. Curvature reveals local and global properties of\na graph, such as a network's backbones, i.e. coarse, structure-preserving graph\ngeometries that form connections between major communities - most suitably\nrepresented as hyperedges to model information flows between clusters across\nlarge distances in the network. To this end, our approach provides a remedy to\nthe problem of information distortion in message passing across long distances\nand graph bottlenecks - a phenomenon known in graph learning as over-squashing.", "AI": {"tldr": "Geometric and Topological Deep Learning uses lifting to transform data to expressive topologies. This paper proposes a structural lifting strategy using Forman-Ricci curvature to represent network backbones as hyperedges, overcoming information distortion and over-squashing in graph learning.", "motivation": "Leverage higher-order topological domains and structures in real-world systems (e.g., social, biological networks) for improved Graph Neural Network performance, specifically addressing information distortion and over-squashing in message passing across long distances.", "method": "Utilized Forman-Ricci curvature to define an edge-based network characteristic based on Riemannian geometry, capturing local and global graph properties to identify network backbones represented as hyperedges.", "result": "Successfully provided a remedy to information distortion and over-squashing in message passing across long distances and graph bottlenecks by representing network backbones as hyperedges.", "conclusion": "Proposed a structural lifting strategy using Forman-Ricci curvature to address information distortion and over-squashing in graph learning by representing network backbones as hyperedges."}}
{"id": "2508.11256", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11256", "abs": "https://arxiv.org/abs/2508.11256", "authors": ["Junjie Wang", "Keyu Chen", "Yulin Li", "Bin Chen", "Hengshuang Zhao", "Xiaojuan Qi", "Zhuotao Tian"], "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception", "comment": "arXiv admin note: text overlap with arXiv:2505.04410", "summary": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP", "AI": {"tldr": "DeCLIP\u901a\u8fc7\u89e3\u8026CLIP\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u878d\u5408\u6765\u81eaVFMs\u548c\u6269\u6563\u6a21\u578b\u7684\u7ebf\u7d22\uff0c\u5206\u522b\u589e\u5f3a\u4e86\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\uff0c\u89e3\u51b3\u4e86CLIP\u5728\u5bc6\u96c6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u5c40\u90e8\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bc6\u96c6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u5c3d\u7ba1CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5c40\u90e8\u7279\u5f81\u8868\u793a\u7684\u9650\u5236\uff0c\u5176\u5728\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u76f4\u63a5\u5e94\u7528\u6027\u80fd\u4e0d\u4f73\u3002CLIP\u7684\u56fe\u50cf\u6807\u8bb0\u96be\u4ee5\u6709\u6548\u5730\u805a\u5408\u6765\u81ea\u7a7a\u95f4\u6216\u8bed\u4e49\u76f8\u5173\u533a\u57df\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u7f3a\u4e4f\u5c40\u90e8\u53ef\u8fa8\u522b\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faDeCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u83b7\u5f97\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\u3002\u4e0a\u4e0b\u6587\u7279\u5f81\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u8bed\u4e49\u76f8\u5173\u6027\u548c\u6269\u6563\u6a21\u578b\u7684\u7269\u4f53\u5b8c\u6574\u6027\u7ebf\u7d22\u6765\u589e\u5f3a\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u5185\u5bb9\u7279\u5f81\u5219\u901a\u8fc7\u4e0e\u56fe\u50cf\u88c1\u526a\u8868\u793a\u5bf9\u9f50\u5e76\u53d7VFMs\u7684\u533a\u57df\u76f8\u5173\u6027\u7ea6\u675f\u6765\u63d0\u9ad8\u5c40\u90e8\u53ef\u8fa8\u522b\u6027\u3002", "result": "DeCLIP\u57282D\u68c0\u6d4b\u548c\u5206\u5272\u30013D\u5b9e\u4f8b\u5206\u5272\u3001\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u548c6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeCLIP\u4e3a\u5f00\u653e\u8bcd\u6c47\u7684\u5bc6\u96c6\u89c6\u89c9\u611f\u77e5\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u57282D\u68c0\u6d4b\u548c\u5206\u5272\u30013D\u5b9e\u4f8b\u5206\u5272\u3001\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u548c6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7b49\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u59cb\u7ec8 achieves state-of-the-art performance\u3002"}}
{"id": "2508.11607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11607", "abs": "https://arxiv.org/abs/2508.11607", "authors": ["Christopher J. Agostino"], "title": "TinyTim: A Family of Language Models for Divergent Generation", "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "AI": {"tldr": "TinyTim \u662f\u4e00\u4e2a\u5728\u300a\u82ac\u5c3c\u6839\u7684\u5b88\u7075\u591c\u300b\u4e0a\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u5176\u7279\u70b9\u662f\u5177\u6709\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u548c\u4f4e\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u53ef\u4f5c\u4e3a\u521b\u9020\u6027\u67b6\u6784\u4e2d\u7684\u53d1\u6563\u77e5\u8bc6\u6e90\u3002", "motivation": "\u4ecb\u7ecd TinyTim \u6a21\u578b\u7cfb\u5217\uff0c\u8be5\u7cfb\u5217\u5728 James Joyce \u7684\u300a\u82ac\u5c3c\u6839\u7684\u5b88\u7075\u591c\u300b\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "method": "\u901a\u8fc7\u5bf9 TinyTim V1 \u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\uff0c\u5e76\u4e0e\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "TinyTim V1 \u5177\u6709\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u548c\u4f4e\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u7edf\u8ba1\u4e0a\u4e0d\u540c\u7684\u751f\u6210\u7279\u5f81\u3002", "conclusion": "TinyTim V1 \u4ea7\u751f\u4e86\u4e00\u4e2a\u7edf\u8ba1\u4e0a\u4e0d\u540c\u7684\u751f\u6210\u7279\u5f81\uff0c\u5177\u6709\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u548c\u4f4e\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4ee5\u901a\u8fc7\u521b\u9020\u529b\u548c\u590d\u6742\u95ee\u9898\u89e3\u51b3\u7684\u7406\u8bba\u6765\u89e3\u91ca\uff0c\u5e76\u8ba4\u4e3a\u8fd9\u79cd\u4e13\u4e1a\u6a21\u578b\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u521b\u9020\u6027\u67b6\u6784\u4e2d\u4f5c\u4e3a\u53d1\u6563\u77e5\u8bc6\u6e90\u53d1\u6325\u4f5c\u7528\uff0c\u4ece\u800c\u5728\u5404\u79cd\u73af\u5883\u4e2d\u9a71\u52a8\u81ea\u52a8\u53d1\u73b0\u673a\u5236\u3002"}}
{"id": "2508.11408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11408", "abs": "https://arxiv.org/abs/2508.11408", "authors": ["Wenhao Zhang", "Yuexiang Xie", "Yuchang Sun", "Yanxi Chen", "Guoyin Wang", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "comment": null, "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.", "AI": {"tldr": "CHORD\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u5c06SFT\u4f5c\u4e3aRL\u7684\u8f85\u52a9\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6613\u7834\u574f\u6a21\u578b\u6a21\u5f0f\u548c\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SFT\u548cRL\u96c6\u6210\u65b9\u6cd5\u5b58\u5728\u7834\u574f\u6a21\u578b\u56fa\u6709\u6a21\u5f0f\u548c\u8fc7\u62df\u5408\u4e13\u5bb6\u6570\u636e\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCHORD\uff08\u53ef\u63a7\u8c10\u6ce2\u7684\u5728\u7ebf\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u52a0\u6743\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06SFT\u89c6\u4e3a\u5728\u7ebfRL\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u52a8\u6001\u52a0\u6743\u8f85\u52a9\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u4e86\u5168\u5c40\u7cfb\u6570\u548c\u9010\u4ee4\u724c\u52a0\u6743\u51fd\u6570\uff0c\u4ee5\u534f\u8c03\u79bb\u7ebf\u6a21\u4eff\u548c\u5728\u7ebf\u63a2\u7d22\uff0c\u540c\u65f6\u4fdd\u7559\u5728\u7ebf\u63a2\u7d22\u5e76\u51cf\u8f7b\u79bb\u7ebf\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "CHORD\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u5b66\u4e60\u8fc7\u7a0b\u548c\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CHORD\u901a\u8fc7\u6709\u6548\u534f\u8c03\u79bb\u7ebf\u4e13\u5bb6\u6570\u636e\u548c\u5728\u7ebf\u63a2\u7d22\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002"}}
{"id": "2508.11262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "AI": {"tldr": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6027\u522b\u5173\u8054\u4e0a\u5b58\u5728\u504f\u89c1\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5982\u4f55\u5c06\u6027\u522b\u4e0e\u804c\u4e1a\u5173\u8054\u6765\u91cf\u5316\u8fd9\u79cd\u504f\u89c1\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5bf9\u9f50\u56fe\u50cf\u548c\u6587\u672c\u65f6\u53ef\u80fd\u4ea7\u751f\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5373\u4f7f\u6807\u51c6\u51c6\u786e\u7387\u6307\u6807\u65e0\u6cd5\u663e\u73b0\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u5305\u542b220\u5f20\u9762\u90e8\u7167\u7247\u548c150\u4e2a\u9648\u8ff0\u7684\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u4e86\u7537\u6027\u548c\u5973\u6027\u5d4c\u5165\u7684\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u91cf\u5316\u6027\u522b\u5173\u8054\u3002\u901a\u8fc7\u81ea\u52a9\u6cd5\u7f6e\u4fe1\u533a\u95f4\u548c\u6807\u7b7e\u4ea4\u6362\u6a21\u578b\u6765\u8bc4\u4f30\u5173\u8054\u7684\u7a33\u5065\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5448\u73b0\u4e86\u5728\u5bf9\u6bd4\u5f0f\u89c6\u89c9-\u8bed\u8a00\u7a7a\u95f4\u4e2d\uff0c\u9648\u8ff0\u548c\u7c7b\u522b\u5c42\u9762\u7684\u6027\u522b\u5173\u8054\u56fe\uff0c\u5e76\u9644\u5e26\u4e86\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3001\u5065\u5168\u6027\u68c0\u67e5\u548c\u4e00\u4e2a\u7a33\u5065\u7684\u6027\u522b\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u5bf9\u6bd4\u5f0f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6027\u522b\u5173\u8054\uff0c\u5373\u6a21\u578b\u5c06\u7537\u6027\u548c\u5973\u6027\u7684\u5d4c\u5165\u4e0e\u5176\u76f8\u5173\u7684\u804c\u4e1a\u548c\u6d3b\u52a8\u77ed\u8bed\u8fdb\u884c\u533a\u5206\u3002"}}
{"id": "2508.11424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11424", "abs": "https://arxiv.org/abs/2508.11424", "authors": ["Yinghua Yao", "Yuangang Pan", "Xixian Chen"], "title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space", "comment": "Accepted by IJCAI 2025", "summary": "Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.", "AI": {"tldr": "LEAD\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6297\u4f53\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u5e8f\u5217\u548c\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u539f\u59cb\u6570\u636e\u7a7a\u95f4\u4e2d\u4f18\u5316\u6297\u4f53CDRs\u4ee5\u6539\u5584\u53ef\u5f00\u53d1\u6027\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u8bc4\u4f30\u6210\u672c\u8fc7\u9ad8\u3002LEAD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f18\u5316\uff0c\u6253\u7834\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u786e\u4fdd\u4e0d\u540c\u6a21\u5f0f\u8bbe\u8ba1\u7684\u540c\u6b65\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEAD\uff08LatEnt blAck-box Design\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u662f\u4e00\u4e2a\u5e8f\u5217-\u7ed3\u6784\u5171\u8bbe\u8ba1\u6846\u67b6\uff0c\u5728\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u540c\u65f6\u4f18\u5316\u6297\u4f53\u5e8f\u5217\u548c\u7ed3\u6784\u3002LEAD\u91c7\u7528\u9ed1\u76d2\u5f15\u5bfc\u7b56\u7565\u6765\u5904\u7406\u4e0d\u53ef\u5fae\u7684\u5c5e\u6027\u8bc4\u4f30\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLEAD\u5728\u4f18\u5316\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u76ee\u6807\u65f6\uff0c\u67e5\u8be2\u6d88\u8017\u51cf\u5c11\u4e86\u4e00\u534a\uff0c\u540c\u65f6\u4f18\u5316\u6027\u80fd\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LEAD\u6846\u67b6\u5728\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u4f18\u5316\u76ee\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4fdd\u8bc1\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u67e5\u8be2\u6d88\u8017\u964d\u4f4e\u4e86\u4e00\u534a\u3002"}}
{"id": "2508.11265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11265", "abs": "https://arxiv.org/abs/2508.11265", "authors": ["Pei He", "Lingling Li", "Licheng Jiao", "Ronghua Shang", "Fang Liu", "Shuang Wang", "Xu Liu", "Wenping Ma"], "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "comment": "to be published in International Conference on Computer Vision, ICCV\n  2025", "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CGE \u548c GCL \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584 3D \u70b9\u4e91\u5206\u5272\u7684\u9886\u57df\u6cdb\u5316\u3002\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u7c7b\u522b\u7ea7\u51e0\u4f55\u5c5e\u6027\u548c\u5bf9\u9f50\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "3D \u5206\u5272\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u5728\u5c06\u6a21\u578b\u90e8\u7f72\u5230\u672a\u77e5\u73af\u5883\u4e2d\u65f6\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u70b9\u4e91\u7684\u6570\u636e\u5206\u5e03\u6765\u7f13\u89e3\u9886\u57df\u8f6c\u79fb\u3002\u7136\u800c\uff0c\u6a21\u578b\u5b66\u4e60\u70b9\u4e91\u4e2d\u7684\u5168\u5c40\u51e0\u4f55\u6a21\u5f0f\uff0c\u800c\u5ffd\u7565\u4e86\u7c7b\u522b\u7ea7\u5206\u5e03\u548c\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u522b\u7ea7\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u9886\u57df\u6cdb\u5316 3D \u8bed\u4e49\u5206\u5272\u7684\u9886\u57df\u4e0d\u53d8\u51e0\u4f55\u7279\u5f81\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u63d0\u51fa\u7c7b\u522b\u7ea7\u51e0\u4f55\u5d4c\u5165\uff08CGE\uff09\u6765\u611f\u77e5\u70b9\u4e91\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u51e0\u4f55\u5c5e\u6027\uff0c\u6784\u5efa\u6bcf\u4e2a\u7c7b\u522b\u7684\u51e0\u4f55\u5c5e\u6027\u5e76\u5c06\u51e0\u4f55\u5d4c\u5165\u4e0e\u8bed\u4e49\u5b66\u4e60\u76f8\u7ed3\u5408\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u51e0\u4f55\u4e00\u81f4\u6027\u5b66\u4e60\uff08GCL\uff09\u6765\u6a21\u62df\u6f5c\u5728\u7684 3D \u5206\u5e03\u5e76\u5bf9\u9f50\u7c7b\u522b\u7ea7\u51e0\u4f55\u5d4c\u5165\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u51e0\u4f55\u4e0d\u53d8\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u6cdb\u5316\u70b9\u4e91\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u975e\u5e38\u7ade\u4e89\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u6cdb\u5316\u70b9\u4e91\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u975e\u5e38\u7ade\u4e89\u7684\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2508.11272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PMTFR \u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u7406\u89e3\u548c\u5229\u7528\u65e0\u8bad\u7ec3\u8c03\u6574\u8303\u5f0f\uff0c\u5728 composto imagen retrieval \u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 compuesto imagen retrieval \u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u9700\u8981\u989d\u5916\u7684\u6392\u5e8f\u6a21\u578b\u8bad\u7ec3\u3002Chain-of-Thought (CoT) \u6280\u672f\u5728\u964d\u4f4e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728 compuesto imagen retrieval \u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6709\u9650\uff0c\u5e76\u4e14\u4e3b\u8981\u7528\u4e8e\u96f6\u6837\u672c compuesto imagen retrieval\uff0c\u5728\u76d1\u7763 compuesto imagen retrieval \u4e2d\u96be\u4ee5\u53d6\u5f97\u6ee1\u610f\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b \"Pyramid Matching Model with Training-Free Refinement\" (PMTFR) \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528 \"Pyramid Patcher\" \u6a21\u5757\u6765\u589e\u5f3a\u91d1\u5b57\u5854\u5339\u914d\u6a21\u578b\u5bf9\u4e0d\u540c\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u7684\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u5de5\u7a0b\u63d0\u53d6 COT \u6570\u636e\u7684\u8868\u793a\u5e76\u5c06\u5176\u6ce8\u5165 LVLMs\uff0c\u5728\u65e0\u8bad\u7ec3\u8c03\u6574\u8303\u5f0f\u4e2d\u83b7\u5f97\u6539\u8fdb\u7684\u68c0\u7d22\u5206\u6570\u3002", "result": "\u901a\u8fc7\u91d1\u5b57\u5854\u5339\u914d\u6a21\u578b\u548c\u65e0\u8bad\u7ec3\u8c03\u6574\uff0cPMTFR \u6846\u67b6\u5728\u89c6\u89c9\u4fe1\u606f\u7406\u89e3\u548c\u68c0\u7d22\u6027\u80fd\u65b9\u9762\u5f97\u5230\u4e86\u63d0\u5347\uff0c\u5e76\u5728\u516c\u5f00\u7684 compuesto imagen retrieval \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6210\u679c\u3002", "conclusion": "PMTFR\u6846\u67b6\u5728\u76d1\u7763 compuesto imagen retrieval \u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11436", "abs": "https://arxiv.org/abs/2508.11436", "authors": ["Mayssa Soussia", "Mohamed Ali Mahjoub", "Islem Rekik"], "title": "Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity", "comment": null, "summary": "The generation of connectional brain templates (CBTs) has recently garnered\nsignificant attention for its potential to identify unique connectivity\npatterns shared across individuals. However, existing methods for CBT learning\nsuch as conventional machine learning and graph neural networks (GNNs) are\nhindered by several limitations. These include: (i) poor interpretability due\nto their black-box nature, (ii) high computational cost, and (iii) an exclusive\nfocus on structure and topology, overlooking the cognitive capacity of the\ngenerated CBT. To address these challenges, we introduce mCOCO (multi-sensory\nCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)\nto learn population-level functional CBT from BOLD\n(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow\nfor tracking state changes over time, enhancing interpretability and enabling\nthe modeling of brain-like dynamics, as demonstrated in prior literature. By\nintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO\ncaptures not only structure and topology but also how brain regions process\ninformation and adapt to cognitive tasks such as sensory processing, all in a\ncomputationally efficient manner. Our mCOCO framework consists of two phases:\n(1) mapping BOLD signals into the reservoir to derive individual functional\nconnectomes, which are then aggregated into a group-level CBT - an approach, to\nthe best of our knowledge, not previously explored in functional connectivity\nstudies - and (2) incorporating multi-sensory inputs through a cognitive\nreservoir, endowing the CBT with cognitive traits. Extensive evaluations show\nthat our mCOCO-based template significantly outperforms GNN-based CBT in terms\nof centeredness, discriminativeness, topological soundness, and multi-sensory\nmemory retention. Our source code is available at\nhttps://github.com/basiralab/mCOCO.", "AI": {"tldr": "mCOCO\u662f\u4e00\u4e2a\u5229\u7528\u6c34\u5e93\u8ba1\u7b97\u548c\u591a\u611f\u89c9\u8f93\u5165\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u8fde\u63a5\u6027\u5927\u8111\u6a21\u677f\uff08CBT\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709CBT\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4ec5\u5173\u6ce8\u7ed3\u6784\u548c\u62d3\u6251\u800c\u5ffd\u7565\u4e86CBT\u7684\u8ba4\u77e5\u80fd\u529b\u7b49\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0cmCOCO\u6846\u67b6\u5229\u7528\u6c34\u5e93\u8ba1\u7b97\uff08RC\uff09\u4eceBOLD\u4fe1\u53f7\u4e2d\u5b66\u4e60\u7fa4\u4f53\u7ea7\u529f\u80fdCBT\u3002", "method": "mCOCO\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u5c06BOLD\u4fe1\u53f7\u6620\u5c04\u5230\u6c34\u5e93\u4ee5\u5bfc\u51fa\u4e2a\u4f53\u529f\u80fd\u8fde\u63a5\u7ec4\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u805a\u5408\u4e3a\u7fa4\u4f53\u7ea7CBT\uff1b(2) \u901a\u8fc7\u8ba4\u77e5\u6c34\u5e93\u6574\u5408\u591a\u611f\u89c9\u8f93\u5165\uff0c\u4e3aCBT\u8d4b\u4e88\u8ba4\u77e5\u7279\u5f81\u3002", "result": "mCOCO-based template\u5728\u4e2d\u5fc3\u6027\u3001\u533a\u5206\u6027\u3001\u62d3\u6251\u5065\u5168\u6027\u548c\u591a\u611f\u89c9\u8bb0\u5fc6\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8eGNN-based CBT\u3002", "conclusion": "mCOCO\u6846\u67b6\u5728\u4e2d\u5fc3\u6027\u3001\u533a\u5206\u6027\u3001\u62d3\u6251\u5065\u5168\u6027\u548c\u591a\u611f\u89c9\u8bb0\u5fc6\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eGNN\u7684CBT\u3002"}}
{"id": "2508.11277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11277", "abs": "https://arxiv.org/abs/2508.11277", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "comment": "ICCV 2025 Findings", "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.", "AI": {"tldr": "SAE\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3aSAE\u5728\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1SAE\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5f88\u53d7\u6b22\u8fce\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u5bf9SAE\u5728\u4e09\u79cd\u89c6\u89c9\u6a21\u578b\u67b6\u6784\uff08\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u3001\u591a\u6a21\u6001LMM\u548c\u6269\u6563\u6a21\u578b\uff09\u4e0a\u7684\u8868\u793a\u80fd\u529b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "SAE\u7279\u5f81\u5177\u6709\u8bed\u4e49\u610f\u4e49\uff0c\u63d0\u9ad8\u4e86\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u63a7\u751f\u6210\u3002\u5728\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u4e2d\uff0cSAE\u7279\u5f81\u53ef\u7528\u4e8eOOD\u68c0\u6d4b\uff0c\u5e76\u80fd\u6062\u590d\u5e95\u5c42\u6a21\u578b\u7684\u672c\u4f53\u7ed3\u6784\u3002\u5728\u6269\u6563\u6a21\u578b\u4e2d\uff0cSAE\u80fd\u591f\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u64cd\u7eb5\u5b9e\u73b0\u8bed\u4e49\u5f15\u5bfc\uff0c\u5e76\u53d1\u73b0\u4e86\u53ef\u4f9b\u4eba\u7c7b\u7406\u89e3\u7684\u5c5e\u6027\u3002\u5728\u591a\u6a21\u6001LLM\u4e0a\u8fdb\u884c\u7684\u63a2\u7d22\u6027\u5b9e\u9a8c\u8868\u660e\uff0cSAE\u7279\u5f81\u63ed\u793a\u4e86\u8de8\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u7684\u5171\u4eab\u8868\u793a\u3002", "conclusion": "SAE\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u5177\u6709\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u63a7\u6027\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3aSAE\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11441", "abs": "https://arxiv.org/abs/2508.11441", "authors": ["Eric G\u00fcnther", "Bal\u00e1zs Szabados", "Robi Bhattacharjee", "Sebastian Bordt", "Ulrike von Luxburg"], "title": "Informative Post-Hoc Explanations Only Exist for Simple Functions", "comment": null, "summary": "Many researchers have suggested that local post-hoc explanation algorithms\ncan be used to gain insights into the behavior of complex machine learning\nmodels. However, theoretical guarantees about such algorithms only exist for\nsimple decision functions, and it is unclear whether and under which\nassumptions similar results might exist for complex models. In this paper, we\nintroduce a general, learning-theory-based framework for what it means for an\nexplanation to provide information about a decision function. We call an\nexplanation informative if it serves to reduce the complexity of the space of\nplausible decision functions. With this approach, we show that many popular\nexplanation algorithms are not informative when applied to complex decision\nfunctions, providing a rigorous mathematical rejection of the idea that it\nshould be possible to explain any model. We then derive conditions under which\ndifferent explanation algorithms become informative. These are often stronger\nthan what one might expect. For example, gradient explanations and\ncounterfactual explanations are non-informative with respect to the space of\ndifferentiable functions, and SHAP and anchor explanations are not informative\nwith respect to the space of decision trees. Based on these results, we discuss\nhow explanation algorithms can be modified to become informative. While the\nproposed analysis of explanation algorithms is mathematical, we argue that it\nholds strong implications for the practical applicability of these algorithms,\nparticularly for auditing, regulation, and high-risk applications of AI.", "AI": {"tldr": "\u89e3\u91ca\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f88\u56f0\u96be\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6548\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u89e3\u91ca\u6709\u6548\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u53d1\u73b0\u8bb8\u591a\u6d41\u884c\u65b9\u6cd5\uff08\u5982 SHAP\uff09\u5728\u7279\u5b9a\u6a21\u578b\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u8fd9\u5bf9 AI \u7684\u53ef\u9760\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5c40\u90e8\u4e8b\u540e\u89e3\u91ca\u7b97\u6cd5\u5728\u89e3\u91ca\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u884c\u4e3a\u65f6\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u89e3\u91ca\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7406\u8bba\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89e3\u91ca\u7b97\u6cd5\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u63d0\u4f9b\u5173\u4e8e\u51b3\u7b56\u51fd\u6570\u7684\u4fe1\u606f\u3002\u901a\u8fc7\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8bb8\u591a\u6d41\u884c\u7684\u89e3\u91ca\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u89e3\u91ca\u3001\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3001SHAP \u548c\u951a\u70b9\u89e3\u91ca\uff09\u5728\u5e94\u7528\u4e8e\u590d\u6742\u6a21\u578b\uff08\u5982\u53ef\u5fae\u5206\u51fd\u6570\u548c\u51b3\u7b56\u6811\uff09\u65f6\u53ef\u80fd\u4e0d\u5177\u6709\u4fe1\u606f\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8bb8\u591a\u6d41\u884c\u7684\u89e3\u91ca\u7b97\u6cd5\uff08\u4f8b\u5982\u68af\u5ea6\u89e3\u91ca\u3001\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3001SHAP \u548c\u951a\u70b9\u89e3\u91ca\uff09\u5728\u5e94\u7528\u4e8e\u590d\u6742\u51b3\u7b56\u51fd\u6570\uff08\u4f8b\u5982\u53ef\u5fae\u5206\u51fd\u6570\u548c\u51b3\u7b56\u6811\uff09\u65f6\u53ef\u80fd\u4e0d\u5177\u6709\u4fe1\u606f\u6027\u3002\u6b64\u5916\uff0c\u63a8\u5bfc\u4e86\u4e0d\u540c\u89e3\u91ca\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u4fe1\u606f\u6027\u7684\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u4fee\u6539\u73b0\u6709\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5176\u4fe1\u606f\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bb8\u591a\u6d41\u884c\u7684\u89e3\u91ca\u7b97\u6cd5\u5728\u5e94\u7528\u4e8e\u590d\u6742\u51b3\u7b56\u51fd\u6570\u65f6\u4f1a\u5931\u6548\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u4e86\u4fee\u6539\u8fd9\u4e9b\u7b97\u6cd5\u4ee5\u4f7f\u5176\u66f4\u5177\u4fe1\u606f\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u5ba1\u8ba1\u3001\u76d1\u7ba1\u548c\u9ad8\u98ce\u9669\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2508.11282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11282", "abs": "https://arxiv.org/abs/2508.11282", "authors": ["Muzammil Khan", "Enzo Kerkhof", "Matteo Fusaglia", "Koert Kuhlmann", "Theo Ruers", "Fran\u00e7oise J. Siepel"], "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction", "comment": "18 pages, 8 figures, 3 Tables, submitted to IEEE Access for review", "summary": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u76ee\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u9884\u6d4b\u548c\u65f6\u95f4\u7cbe\u70bc\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5355\u76ee\u5fae\u521b\u624b\u672f\u7684\u5bfc\u822a\u7cbe\u5ea6\u548c\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u9700\u8981\u5b9e\u73b0\u7cbe\u786e\u7684\u5185\u7aa5\u955c\u59ff\u6001\u4f30\u8ba1\u548c\u4e09\u7ef4\u7ec4\u7ec7\u8868\u9762\u91cd\u5efa\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5355\u76ee\u5185\u7aa5\u955c\u59ff\u6001\u4f30\u8ba1\u548c\u7ec4\u7ec7\u91cd\u5efa\u65b9\u6cd5\u9762\u4e34\u6df1\u5ea6\u6a21\u7cca\u3001\u751f\u7406\u7ec4\u7ec7\u53d8\u5f62\u3001\u5185\u7aa5\u955c\u8fd0\u52a8\u4e0d\u4e00\u81f4\u3001\u7eb9\u7406\u4fdd\u771f\u5ea6\u6709\u9650\u548c\u89c6\u573a\u53d7\u9650\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5355\u76ee\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u5c3a\u5ea6\u611f\u77e5\u6df1\u5ea6\u9884\u6d4b\u548c\u65f6\u95f4\u7ea6\u675f\u611f\u77e5\u7cbe\u70bc\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2aMAPIS-Depth\u6a21\u5757\uff0c\u5229\u7528Depth Pro\u8fdb\u884c\u521d\u59cb\u5316\uff0cDepth Anything\u8fdb\u884c\u9010\u5e27\u6df1\u5ea6\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408L-BFGS-B\u4f18\u5316\u751f\u6210\u4f2a\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u3002\u7136\u540e\uff0c\u4f7f\u7528RAFT\u8ba1\u7b97\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8eLPIPS\u611f\u77e5\u76f8\u4f3c\u6027\u81ea\u9002\u5e94\u5730\u878d\u5408\u6d41\u626d\u66f2\u5e27\uff0c\u4ee5\u8fdb\u884c\u65f6\u95f4\u7cbe\u70bc\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2aWEMA-RT6D\u6a21\u5757\u6765\u4f18\u5316\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u4ee5\u5b9e\u73b0\u5408\u6210\u7684\u4f2aRGBD\u5e27\u7684\u7cbe\u786e\u914d\u51c6\u3002\u6700\u540e\uff0c\u91c7\u7528\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08TSDF\uff09\u8fdb\u884c\u4f53\u79ef\u878d\u5408\uff0c\u5e76\u7ed3\u5408Marching Cubes\u63d0\u53d6\u4e09\u7ef4\u8868\u9762\u7f51\u683c\u3002", "result": "\u901a\u8fc7\u5728HEVD\u548cSCARED\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u3001\u6d88\u878d\u7814\u7a76\u548c\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5e94\u5bf9\u5355\u76ee\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u6311\u6218\u65b9\u9762\u7684\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5c3a\u5ea6\u611f\u77e5\u6df1\u5ea6\u9884\u6d4b\u548c\u65f6\u95f4\u7ea6\u675f\u611f\u77e5\u7cbe\u70bc\uff0c\u514b\u670d\u4e86\u5355\u76ee\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u4e2d\u7684\u6df1\u5ea6\u6a21\u7cca\u3001\u751f\u7406\u7ec4\u7ec7\u53d8\u5f62\u3001\u5185\u7aa5\u955c\u8fd0\u52a8\u4e0d\u4e00\u81f4\u3001\u7eb9\u7406\u4fdd\u771f\u5ea6\u6709\u9650\u548c\u89c6\u573a\u53d7\u9650\u7b49\u6311\u6218\uff0c\u5e76\u5728HEVD\u548cSCARED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11460", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11460", "abs": "https://arxiv.org/abs/2508.11460", "authors": ["Aurora Grefsrud", "Nello Blaser", "Trygve Buanes"], "title": "Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models", "comment": null, "summary": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cd\u6982\u7387\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7279\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u6240\u6709\u7b97\u6cd5\u90fd\u7ecf\u8fc7\u4e86\u826f\u597d\u6821\u51c6\uff0c\u4f46\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7b97\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916\u6570\u636e\u65f6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7b49\u65e5\u76ca\u590d\u6742\u7684\u6570\u636e\u6a21\u578b\u51fa\u73b0\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u5927\u91cf\u7684\u6280\u672f\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u7b97\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u7684\u7279\u6027\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7ecf\u9a8c\u6d4b\u8bd5\uff0c\u7814\u7a76\u4e86\u516d\u79cd\u4e0d\u540c\u7684\u6982\u7387\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u7c7b\u522b\u6982\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u7684\u7279\u6027\uff1a(i) \u795e\u7ecf\u7f51\u7edc\u96c6\u6210\uff0c(ii) \u5177\u6709\u51b2\u7a81\u635f\u5931\u7684\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\uff0c(iii) \u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\uff0c(iv) \u5177\u6709\u8499\u7279\u5361\u6d1b Dropout \u7684\u5355\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c(v) \u9ad8\u65af\u8fc7\u7a0b\u5206\u7c7b\u548c (vi) \u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\u6df7\u5408\u6a21\u578b\u3002", "result": "\u6240\u6709\u7b97\u6cd5\u90fd\u7ecf\u8fc7\u4e86\u826f\u597d\u6821\u51c6\uff0c\u4f46\u6ca1\u6709\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7b97\u6cd5\u63d0\u4f9b\u7684\u4f30\u8ba1\u91cf\u80fd\u591f\u6301\u7eed\u53cd\u6620\u51fa\u5206\u5e03\u5916\u6570\u636e\u70b9\u7f3a\u4e4f\u5b9e\u9a8c\u8bc1\u636e\u7684\u60c5\u51b5\u3002", "conclusion": "\u6240\u6709\u7b97\u6cd5\u90fd\u7ecf\u8fc7\u4e86\u826f\u597d\u6821\u51c6\uff0c\u4f46\u6ca1\u6709\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7b97\u6cd5\u63d0\u4f9b\u7684\u4f30\u8ba1\u91cf\u80fd\u591f\u6301\u7eed\u53cd\u6620\u51fa\u5206\u5e03\u5916\u6570\u636e\u70b9\u7f3a\u4e4f\u5b9e\u9a8c\u8bc1\u636e\u7684\u60c5\u51b5\u3002"}}
{"id": "2508.11284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11284", "abs": "https://arxiv.org/abs/2508.11284", "authors": ["Yilin Mi", "Qixin Yan", "Zheng-Peng Duan", "Chunle Guo", "Hubery Yin", "Hao Liu", "Chen Li", "Chongyi Li"], "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "comment": null, "summary": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "AI": {"tldr": "TimeMachine\u6846\u67b6\u901a\u8fc7\u6ce8\u5165\u5e74\u9f84\u4fe1\u606f\u548c\u4f7f\u7528ACG\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u53ef\u63a7\u7684\u9762\u90e8\u5e74\u9f84\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u7559\u8eab\u4efd\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u5728HFFA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u5b9e\u73b0\u7cbe\u7ec6\u7684\u5e74\u9f84\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u7559\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "TimeMachine\u6846\u67b6\u901a\u8fc7\u5728\u591a\u91cd\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u6ce8\u5165\u9ad8\u7cbe\u5ea6\u5e74\u9f84\u4fe1\u606f\uff0c\u660e\u786e\u5206\u79bb\u5e74\u9f84\u76f8\u5173\u548c\u8eab\u4efd\u76f8\u5173\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u7684\u5e74\u9f84\u7f16\u8f91\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5e74\u9f84\u5206\u7c7b\u5668\u6307\u5bfc\uff08ACG\uff09\u6a21\u5757\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u9884\u6d4b\u5e74\u9f84\uff0c\u800c\u4e0d\u662f\u5728\u8bad\u7ec3\u671f\u95f4\u8fdb\u884c\u53bb\u566a\u56fe\u50cf\u91cd\u5efa\uff0c\u4ee5\u63d0\u9ad8\u5e74\u9f84\u7f16\u8f91\u7684\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e00\u767e\u4e07\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001\u5e76\u5e26\u6709\u8eab\u4efd\u53ca\u9762\u90e8\u5c5e\u6027\u6807\u7b7e\u7684HFFA\u6570\u636e\u96c6\uff08\u9ad8\u8d28\u91cf\u7cbe\u7ec6\u9762\u90e8\u5e74\u9f84\u6570\u636e\u96c6\uff09\uff0c\u4ee5\u89e3\u51b3\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u9762\u90e8\u5e74\u9f84\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTimeMachine\u5728\u7cbe\u7ec6\u5e74\u9f84\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "conclusion": "TimeMachine\u5728\u5e74\u9f84\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.11504", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11504", "abs": "https://arxiv.org/abs/2508.11504", "authors": ["Andrea Castellani", "Zacharias Papadovasilakis", "Giorgos Papoutsoglou", "Mary Cole", "Brian Bautsch", "Tobias Rodemann", "Ioannis Tsamardinos", "Angela Harden"], "title": "Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection", "comment": "Preprint. Manuscript under review at \"Accident Analysis & Prevention\"\n  journal", "summary": "Motor vehicle crashes remain a leading cause of injury and death worldwide,\nnecessitating data-driven approaches to understand and mitigate crash severity.\nThis study introduces a curated dataset of more than 3 million people involved\nin accidents in Ohio over six years (2017-2022), aggregated to more than 2.3\nmillion vehicle-level records for predictive analysis. The primary contribution\nis a transparent and reproducible methodology that combines Automated Machine\nLearning (AutoML) and explainable artificial intelligence (AI) to identify and\ninterpret key risk factors associated with severe crashes. Using the JADBio\nAutoML platform, predictive models were constructed to distinguish between\nsevere and non-severe crash outcomes. The models underwent rigorous feature\nselection across stratified training subsets, and their outputs were\ninterpreted using SHapley Additive exPlanations (SHAP) to quantify the\ncontribution of individual features. A final Ridge Logistic Regression model\nachieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test\nset, with 17 features consistently identified as the most influential\npredictors. Key features spanned demographic, environmental, vehicle, human,\nand operational categories, including location type, posted speed, minimum\noccupant age, and pre-crash action. Notably, certain traditionally emphasized\nfactors, such as alcohol or drug impairment, were less influential in the final\nmodel compared to environmental and contextual variables. Emphasizing\nmethodological rigor and interpretability over mere predictive performance,\nthis study offers a scalable framework to support Vision Zero with aligned\ninterventions and advanced data-informed traffic safety policy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528AutoML\u548c\u53ef\u89e3\u91caAI\u5206\u6790\u4e86\u8d85\u8fc7300\u4e07\u8d77\u8f66\u7978\u6570\u636e\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u8f66\u7978\u4e25\u91cd\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u4ea4\u901a\u5b89\u5168\u653f\u7b56\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u548c\u51cf\u8f7b\u8f66\u7978\u7684\u4e25\u91cd\u6027\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\uff08AutoML\uff09\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\uff0c\u5229\u7528JADBio AutoML\u5e73\u53f0\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7SHapley Additive exPlanations\uff08SHAP\uff09\u89e3\u91ca\u6a21\u578b\u3002", "result": "\u8be5\u7814\u7a76\u4f7f\u7528\u7684\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7300\u4e07\u4eba\u5728\u4fc4\u4ea5\u4fc4\u5dde\u53d1\u751f\u7684\u4e8b\u6545\u8bb0\u5f55\uff0c\u6700\u7ec8\u7684Ridge Logistic Regression\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684AUC-ROC\u4e3a85.6%\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u4e3a84.9%\uff0c\u8bc6\u522b\u51fa17\u4e2a\u5173\u952e\u9884\u6d4b\u56e0\u7d20\uff0c\u8fd9\u4e9b\u56e0\u7d20\u6db5\u76d6\u4e86\u4eba\u53e3\u7edf\u8ba1\u3001\u73af\u5883\u3001\u8f66\u8f86\u3001\u4eba\u7c7b\u548c\u64cd\u4f5c\u7c7b\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u201c\u96f6\u613f\u666f\u201d\u884c\u52a8\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u7684\u5e72\u9884\u63aa\u65bd\u548c\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u5b89\u5168\u653f\u7b56\u3002"}}
{"id": "2508.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11301", "abs": "https://arxiv.org/abs/2508.11301", "authors": ["Jiarong Li", "Imad Ali Shah", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study", "comment": "Submitted to IEEE ICVES, July, 2025", "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.", "AI": {"tldr": "\u9ad8\u5149\u8c31\u6210\u50cf\u7ed3\u5408\u6700\u4f73\u6ce2\u6bb5\u9009\u62e9\uff08CSNR-JMIM\uff09\u80fd\u663e\u8457\u63d0\u9ad8\u6c7d\u8f66\u611f\u77e5\u7cfb\u7edf\u4e2d\u884c\u4eba\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3RGB\u6210\u50cf\u4e2d\u7684\u540c\u8d28\u6027\u95ee\u9898\uff0c\u5373\u884c\u4eba\u548c\u80cc\u666f\u5728\u89c6\u89c9\u4e0a\u96be\u4ee5\u533a\u5206\uff0c\u4ece\u800c\u63d0\u9ad8\u6c7d\u8f66\u611f\u77e5\u7cfb\u7edf\u4e2d\u884c\u4eba\u5206\u5272\u7684\u5b89\u5168\u5173\u952e\u6027\u3002", "method": "\u672c\u7814\u7a76\u5c06HSI\u6570\u636e\u8f6c\u6362\u4e3a\u4e09\u901a\u9053\u8868\u793a\uff08PCA\u548cCSNR-JMIM\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86U-Net\u3001DeepLabV3+\u548cSegFormer\u4e09\u79cd\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u884c\u4eba\u5206\u5272\u80fd\u529b\u3002", "result": "\u4e0eRGB\u76f8\u6bd4\uff0cCSNR-JMIM\u5728\u9ad8\u5149\u8c31\u6570\u636e\u5904\u7406\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5728\u884c\u4eba\u5206\u5272\u7684IoU\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u9ad8\u4e861.44%\u548c2.18%\uff0c\u5728\u9a91\u884c\u8005\u5206\u5272\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u76f8\u4f3c\u7684\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u6700\u4f73\u9ad8\u5149\u8c31\u6210\u50cf(HSI)\u6ce2\u6bb5\u9009\u62e9\u53ef\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u884c\u4eba\u5206\u5272\uff0c\u8fd9\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u7684\u6c7d\u8f66\u5e94\u7528\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11513", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11513", "abs": "https://arxiv.org/abs/2508.11513", "authors": ["Fanzhen Liu", "Xiaoxiao Ma", "Jian Yang", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Quan Z. Sheng", "Jia Wu"], "title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies", "comment": "14 pages, 12 figures", "summary": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa GraphOracle\uff0c\u4e00\u79cd\u7528\u4e8e GNN \u7684\u81ea\u89e3\u91ca\u6846\u67b6\uff0c\u53ef\u751f\u6210\u5fe0\u5b9e\u7684\u7c7b\u7ea7\u522b\u89e3\u91ca\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd GNN \u7684\u5b89\u5168\u548c\u516c\u5e73\u90e8\u7f72\uff0c\u589e\u5f3a GNN \u7684\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002\u5148\u524d\u7684\u5de5\u4f5c\u96c6\u4e2d\u5728\u5b9e\u4f8b\u7ea7\u522b\u89e3\u91ca\uff0c\u4f46\u7c7b\u7ea7\u522b\u89e3\u91ca\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u786e\u5b9a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u751f\u6210\u548c\u8bc4\u4f30\u7c7b\u7ea7\u522b\u89e3\u91ca\u7684 GNN \u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u7684\u7c7b\u7ea7\u522b\u89e3\u91ca\u80fd\u529b\u3002", "method": "GraphOracle \u6846\u67b6\u901a\u8fc7\u96c6\u6210\u8bad\u7ec3\u8054\u5408\u5b66\u4e60 GNN \u5206\u7c7b\u5668\u548c\u4e00\u7ec4\u7ed3\u6784\u5316\u3001\u7a00\u758f\u7684\u5b50\u56fe\uff0c\u8fd9\u4e9b\u5b50\u56fe\u5bf9\u6bcf\u4e2a\u7c7b\u5177\u6709\u533a\u5206\u6027\u3002\u5b83\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u8bc4\u4f30\u7b56\u7565\u6765\u8bc4\u4f30\u7c7b\u7ea7\u522b\u89e3\u91ca\u7684\u6709\u6548\u6027\uff0c\u5e76\u91c7\u7528\u71b5\u6b63\u5219\u5316\u5b50\u56fe\u9009\u62e9\u548c\u8f7b\u91cf\u7ea7\u968f\u673a\u6e38\u8d70\u63d0\u53d6\u6765\u907f\u514d\u8ba1\u7b97\u74f6\u9888\u3002", "result": "GraphOracle \u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u907f\u514d\u8ba1\u7b97\u74f6\u9888\u3002\u5148\u524d\u7684 GNN \u65b9\u6cd5\uff08\u5982 ProtGNN \u548c PGIB\uff09\u672a\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u7c7b\u7ea7\u522b\u89e3\u91ca\u3002", "conclusion": "GraphOracle \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u89e3\u91ca GNN \u6846\u67b6\uff0c\u65e8\u5728\u4e3a GNN \u751f\u6210\u548c\u8bc4\u4f30\u7c7b\u7ea7\u522b\u89e3\u91ca\u3002\u5b83\u901a\u8fc7\u96c6\u6210\u8bad\u7ec3\u8054\u5408\u5b66\u4e60 GNN \u5206\u7c7b\u5668\u548c\u4e00\u7ec4\u7ed3\u6784\u5316\u3001\u7a00\u758f\u7684\u5b50\u56fe\uff0c\u8fd9\u4e9b\u5b50\u56fe\u5bf9\u6bcf\u4e2a\u7c7b\u5177\u6709\u533a\u5206\u6027\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\uff08\u5982 ProtGNN \u548c PGIB\uff09\u4e0d\u540c\uff0cGraphOracle \u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u4fdd\u771f\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u907f\u514d\u4e86\u8ba1\u7b97\u74f6\u9888\uff0c\u662f GNN \u4e2d\u5fe0\u5b9e\u7684\u7c7b\u7ea7\u522b\u81ea\u53ef\u89e3\u91ca\u6027\u7684\u5b9e\u7528\u4e14\u6709\u539f\u5219\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u53bb\u566a-\u68c0\u7d22\u201d\u65b9\u6cd5\uff08DRNet\uff09\uff0c\u901a\u8fc7\u53bb\u9664\u89c6\u9891\u4e2d\u4e0e\u6587\u672c\u65e0\u5173\u7684\u5185\u5bb9\uff0c\u7136\u540e\u8fdb\u884c\u68c0\u7d22\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\uff0c\u5e76\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u4f1a\u5c06\u6240\u6709\u89c6\u9891\u7247\u6bb5\uff08\u5305\u62ec\u4e0d\u76f8\u5173\u7684\u7247\u6bb5\uff09\u8fdb\u884c\u7f16\u7801\uff0c\u8fd9\u4f1a\u5e72\u6270\u591a\u6a21\u6001\u4fe1\u606f\u7684\u5bf9\u9f50\u5e76\u963b\u788d\u4f18\u5316\u8fc7\u7a0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u53bb\u566a-\u68c0\u7d22\u201d\u8303\u5f0f\uff0c\u65e8\u5728\u663e\u5f0f\u5730\u8fc7\u6ee4\u6389\u6587\u672c\u4e0d\u76f8\u5173\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u7136\u540e\u5229\u7528\u51c0\u5316\u540e\u7684\u591a\u6a21\u6001\u8868\u793a\u8fdb\u884c\u68c0\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53bb\u566a-\u68c0\u7d22\u7f51\u7edc\u201d\uff08DRNet\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u7f51\u7edc\u5305\u542b\u201c\u6587\u672c\u6761\u4ef6\u53bb\u566a\u201d\uff08TCD\uff09\u548c\u201c\u6587\u672c\u91cd\u5efa\u53cd\u9988\u201d\uff08TRF\uff09\u4e24\u4e2a\u6a21\u5757\u3002TCD\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5757\u6765\u52a8\u6001\u8bc6\u522b\u548c\u53bb\u9664\u566a\u58f0\u7247\u6bb5\uff0c\u751f\u6210\u4e00\u4e2a\u53bb\u566a\u63a9\u7801\u6765\u51c0\u5316\u89c6\u9891\u8868\u793a\u3002TRF\u5219\u4ece\u51c0\u5316\u540e\u7684\u89c6\u9891\u8868\u793a\u4e2d\u63d0\u70bc\u51fa\u5355\u4e00\u67e5\u8be2\u5d4c\u5165\uff0c\u5e76\u4f7f\u5176\u4e0e\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u671f\u95f4\u53bb\u566a\u7684\u8f85\u52a9\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DRNet\u65b9\u6cd5\u5728Charades-STA\u548cQVHighlights\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u53bb\u566a-\u68c0\u7d22\u8303\u5f0f\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5148\u8fdb\u7684\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u4e2d\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u201c\u53bb\u566a-\u68c0\u7d22\u201d\u8303\u5f0f\u80fd\u591f\u6709\u6548\u5730\u8fc7\u6ee4\u6389\u4e0e\u6587\u672c\u4e0d\u76f8\u5173\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u63d0\u70bc\u540e\u7684\u591a\u6a21\u6001\u8868\u793a\u8fdb\u884c\u68c0\u7d22\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728Charades-STA\u548cQVHighlights\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u5176\u4ed6\u6a21\u578b\u4e2d\u4ee5\u63d0\u5347\u6548\u679c\u3002"}}
{"id": "2508.11514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11514", "abs": "https://arxiv.org/abs/2508.11514", "authors": ["Qitong Chu", "Yufeng Yue", "Danya Yao", "Huaxin Pei"], "title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality", "comment": null, "summary": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5173\u952e\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5728\u5e73\u8861\u591a\u6837\u6027\u548c\u5173\u952e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7a7a\u95f4\u5f15\u5bfc\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u548c\u4ee3\u7406\u884c\u4e3a\u7a7a\u95f4\u6765\u751f\u6210\u8003\u8651\u591a\u6837\u6027\u548c\u5173\u952e\u6027\u7684\u6d4b\u8bd5\u573a\u666f\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u51b3\u7b56\u4ee3\u7406\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u589e\u52a0\u4e86\u5b89\u5168\u6027\u9a8c\u8bc1\u7684\u9700\u6c42\u3002\u73b0\u6709\u7684\u5173\u952e\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5728\u5e73\u8861\u591a\u6837\u6027\u548c\u5173\u952e\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u573a\u666f\u7a7a\u95f4\u4e2d\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7a7a\u95f4\u5f15\u5bfc\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u534f\u8c03\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u548c\u4ee3\u7406\u884c\u4e3a\u7a7a\u95f4\u3002\u5728\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u91c7\u7528\u5206\u5c42\u8868\u793a\u6846\u67b6\uff0c\u7ed3\u5408\u964d\u7ef4\u548c\u591a\u7ef4\u5b50\u7a7a\u95f4\u8bc4\u4f30\uff0c\u4ee5\u6709\u6548\u672c\u5730\u5316\u591a\u6837\u5316\u548c\u5173\u952e\u5316\u7684\u5b50\u7a7a\u95f4\uff0c\u6307\u5bfc\u5c40\u90e8\u6270\u52a8\u548c\u5168\u5c40\u63a2\u7d22\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\u4e4b\u95f4\u7684\u52a8\u6001\u534f\u8c03\u3002\u5728\u4ee3\u7406\u884c\u4e3a\u7a7a\u95f4\u4e2d\uff0c\u5229\u7528\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u6570\u636e\u91cf\u5316\u884c\u4e3a\u7684\u5173\u952e\u6027/\u591a\u6837\u6027\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u652f\u6301\u751f\u6210\u6a21\u5f0f\u5207\u6362\uff0c\u5f62\u6210\u4e00\u4e2a\u95ed\u73af\u53cd\u9988\uff0c\u4e0d\u65ad\u589e\u5f3a\u573a\u666f\u8868\u5f81\u548c\u53c2\u6570\u7a7a\u95f4\u5185\u7684\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e94\u4e2a\u51b3\u7b56\u4ee3\u7406\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5728\u5173\u952e\u573a\u666f\u751f\u6210\u65b9\u9762\u5e73\u5747\u63d0\u9ad8\u4e8656.23%\uff0c\u5e76\u5728\u65b0\u9896\u7684\u53c2\u6570-\u884c\u4e3a\u534f\u540c\u9a71\u52a8\u6307\u6807\u4e0b\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u548c\u4ee3\u7406\u884c\u4e3a\u7a7a\u95f4\uff0c\u5e76\u5728\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u4e2d\u91c7\u7528\u5206\u5c42\u8868\u793a\u6846\u67b6\uff08\u7ed3\u5408\u964d\u7ef4\u548c\u591a\u7ef4\u5b50\u7a7a\u95f4\u8bc4\u4f30\uff09\u6765\u6709\u6548\u5e73\u8861\u591a\u6837\u6027\u548c\u5173\u952e\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5173\u952e\u573a\u666f\u7684\u751f\u6210\u6548\u679c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11317", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u903b\u8f91\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86LogicBench\u57fa\u51c6\u6d4b\u8bd5\u548cLogicCLIP\u8bad\u7ec3\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002LogicCLIP\u901a\u8fc7\u6539\u8fdb\u7684\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u903b\u8f91\u7406\u89e3\u65b9\u9762\u5b58\u5728\u201c\u903b\u8f91\u76f2\u70b9\u201d\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u7cfb\u7edf\u8bca\u65ad\u5e76\u63d0\u5347\u5176\u903b\u8f91\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faLogicBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b50,000\u591a\u4e2a\u8de8\u8d8a9\u4e2a\u903b\u8f91\u7c7b\u522b\u548c4\u79cd\u573a\u666f\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\uff0c\u5e76\u63d0\u51faLogicCLIP\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u548c\u4f18\u5316\u76ee\u6807\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u654f\u611f\u6027\u3002LogicCLIP\u91c7\u7528\u903b\u8f91\u611f\u77e5\u6570\u636e\u751f\u6210\u3001\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u7c97\u7c92\u5ea6\u5bf9\u9f50\u3001\u7ec6\u7c92\u5ea6\u591a\u9879\u9009\u62e9\u548c\u65b0\u9896\u7684\u903b\u8f91\u7ed3\u6784\u611f\u77e5\u76ee\u6807\u3002", "result": "\u73b0\u6709VLMs\u5728LogicBench\u4e0a\u7684\u51c6\u786e\u7387\u6bd4\u4eba\u7c7b\u4f4e40\u591a\u4e2a\u767e\u5206\u70b9\uff0c\u5c24\u5176\u5728\u56e0\u679c\u5173\u7cfb\u548c\u6761\u4ef6\u53e5\u65b9\u9762\u3002LogicCLIP\u5728LogicBench\u6240\u6709\u9886\u57df\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LogicBench\u548cLogicCLIP\u662f\u63a8\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u903b\u8f91\u80fd\u529b\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2508.11522", "categories": ["cs.LG", "hep-th"], "pdf": "https://arxiv.org/pdf/2508.11522", "abs": "https://arxiv.org/abs/2508.11522", "authors": ["Max Guillen", "Philipp Misof", "Jan E. Gerken"], "title": "Finite-Width Neural Tangent Kernels from Feynman Diagrams", "comment": "11 pages + appendices", "summary": "Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,\nnon-linear neural networks. In the infinite-width limit, NTKs can easily be\ncomputed for most common architectures, yielding full analytic control over the\ntraining dynamics. However, at infinite width, important properties of training\nsuch as NTK evolution or feature learning are absent. Nevertheless, finite\nwidth effects can be included by computing corrections to the Gaussian\nstatistics at infinite width. We introduce Feynman diagrams for computing\nfinite-width corrections to NTK statistics. These dramatically simplify the\nnecessary algebraic manipulations and enable the computation of layer-wise\nrecursive relations for arbitrary statistics involving preactivations, NTKs and\ncertain higher-derivative tensors (dNTK and ddNTK) required to predict the\ntraining dynamics at leading order. We demonstrate the feasibility of our\nframework by extending stability results for deep networks from preactivations\nto NTKs and proving the absence of finite-width corrections for scale-invariant\nnonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We\nvalidate our results with numerical experiments.", "AI": {"tldr": "\u5728\u65e0\u9650\u5bbd\u5ea6\u4e0b\uff0cNTK\u6613\u4e8e\u8ba1\u7b97\uff0c\u4f46\u7f3a\u5c11NTK\u6f14\u5316\u548c\u7279\u5f81\u5b66\u4e60\u7b49\u7279\u6027\u3002\u672c\u6587\u63d0\u51fa\u4f7f\u7528Feynman\u56fe\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\uff0c\u4ee5\u5305\u542b\u8fd9\u4e9b\u91cd\u8981\u7279\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u9884\u6d4b\u8bad\u7ec3\u52a8\u6001\u7684\u5c42\u7ea7\u9012\u63a8\u5173\u7cfb\u3002", "motivation": "\u4e3a\u4e86\u5728\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u5728\u6709\u9650\u5bbd\u5ea6\u4e0b\u8003\u8651NTK\u6f14\u5316\u548c\u7279\u5f81\u5b66\u4e60\u7b49\u91cd\u8981\u7279\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u53ef\u9884\u6d4b\u8bad\u7ec3\u52a8\u6001\u7684\u89e3\u6790\u7ed3\u679c\u3002", "method": "\u5f15\u5165Feynman\u56fe\u6765\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\uff0c\u5e76\u63a8\u5bfc\u51fa\u4efb\u610f\u6d89\u53capreactivations\u3001NTK\u548cdNTK\u3001ddNTK\u7684\u7edf\u8ba1\u91cf\u7684\u5c42\u7ea7\u9012\u63a8\u5173\u7cfb\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\u7684Feynman\u56fe\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6269\u5c55\u6df1\u5ea6\u7f51\u7edc\u7a33\u5b9a\u6027\u7ed3\u679c\u548c\u8bc1\u660eReLU\u7b49\u5c3a\u5ea6\u4e0d\u53d8\u975e\u7ebf\u6027\u5728Gram\u77e9\u9635\u5bf9\u89d2\u7ebf\u4e0a\u65e0\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Feynman\u56fe\u53ef\u4ee5\u7528\u6765\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6NTK\u7edf\u8ba1\u91cf\u7684\u4fee\u6b63\uff0c\u5e76\u5df2\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.11354", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11354", "abs": "https://arxiv.org/abs/2508.11354", "authors": ["Zhenyi Zhao", "Muthu Rama Krishnan Mookiah", "Emanuele Trucco"], "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images", "comment": null, "summary": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]", "AI": {"tldr": "RETFound, a foundation model for fundus images, was adapted for optic disc segmentation and achieved ~96% Dice score on multiple datasets, outperforming existing methods with limited training data.", "motivation": "The motivation was to explore the applicability of the RETFound foundation model beyond its known uses in disease diagnosis from retinal images and to adapt it for the fundamental task of optic disc segmentation.", "method": "The study adapted the RETFound foundation model for optic disc segmentation, a foundational task in retinal image analysis. A segmentation head was trained using a small number of task-specific examples.", "result": "The adapted RETFound model achieved approximately 96% Dice score consistently across four public datasets (IDRID, Drishti-GS, RIM-ONE-r3, REFUGE) and one private dataset (GoDARTS). It outperformed state-of-the-art, segmentation-specific baseline networks and demonstrated excellent performance in internal verification, domain generalization, and domain adaptation.", "conclusion": "RETFound, a foundation model for fundus images, has been successfully adapted for optic disc segmentation, outperforming state-of-the-art segmentation-specific models with minimal task-specific training data. The adapted model achieved consistent Dice scores of ~96% across multiple public and private datasets, demonstrating excellent performance in internal verification, domain generalization, and domain adaptation."}}
{"id": "2508.11323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11323", "abs": "https://arxiv.org/abs/2508.11323", "authors": ["Haonan Zhang", "Xinyao Wang", "Boxi Wu", "Tu Zheng", "Wang Yunhua", "Zheng Yang"], "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "comment": null, "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.", "AI": {"tldr": "DSC-Track\u901a\u8fc7\u4e00\u79cd\u7ebf\u7d22\u4e00\u81f4\u6027\u539f\u5219\uff0c\u5229\u7528\u70b9\u5bf9\u7279\u5f81\u548cTransformer\u6765\u5904\u74063D\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u906e\u6321\u548c\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u573a\u666f\u6216\u4e0d\u51c6\u786e\u68c0\u6d4b\u65f6\uff0c\u7531\u4e8e\u5ffd\u7565\u4e86\u7269\u4f53\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u6548\u679c\u4e0d\u4f73\u3002\u800c\u73b0\u6709\u7684\u8003\u8651\u51e0\u4f55\u5173\u7cfb\u7684\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u65e0\u5173\u7269\u4f53\u7684\u5e72\u6270\uff0c\u5bfc\u81f4\u7279\u5f81\u6a21\u7cca\u548c\u5173\u8054\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u7a7a\u95f4\u7ebf\u7d22\u5e76\u514b\u670d\u5e72\u6270\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u573a\u666f\u7ebf\u7d22\u4e00\u81f4\u6027\u8ddf\u8e2a\u5668\uff08DSC-Track\uff09\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u7a7a\u7f16\u7801\u5668\uff08\u4f7f\u7528\u70b9\u5bf9\u7279\u5f81PPF\uff09\u6765\u5b66\u4e60\u5177\u6709\u533a\u5206\u6027\u7684\u8f68\u8ff9\u5d4c\u5165\u5e76\u6291\u5236\u5e72\u6270\uff0c\u4e00\u4e2a\u7ebf\u7d22\u4e00\u81f4\u6027Transformer\u6a21\u5757\u6765\u660e\u786e\u5730\u5bf9\u9f50\u5386\u53f2\u8f68\u8ff9\u548c\u5f53\u524d\u68c0\u6d4b\u4e4b\u95f4\u7684\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u673a\u5236\u6765\u4fdd\u7559\u663e\u8457\u7684\u65f6\u7a7a\u4fe1\u606f\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u5728\u7ebf\u8ddf\u8e2a\u3002", "result": "\u901a\u8fc7\u5728nuScenes\u548cWaymo Open Datasets\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAMOTA\u5206\u522b\u8fbe\u5230\u4e8673.2%\uff08\u9a8c\u8bc1\u96c6\uff09\u548c70.3%\uff08\u6d4b\u8bd5\u96c6\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728nuScenes\u548cWaymo Open Datasets\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728nuScenes\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u4e8673.2%\u548c70.3%\u7684AMOTA\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11528", "abs": "https://arxiv.org/abs/2508.11528", "authors": ["Juhi Soni", "Markus Lange-Hegermann", "Stefan Windmann"], "title": "Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series", "comment": "16 pages, 5 figures", "summary": "We propose an unsupervised anomaly detection approach based on a\nphysics-informed diffusion model for multivariate time series data. Over the\npast years, diffusion model has demonstrated its effectiveness in forecasting,\nimputation, generation, and anomaly detection in the time series domain. In\nthis paper, we present a new approach for learning the physics-dependent\ntemporal distribution of multivariate time series data using a weighted\nphysics-informed loss during diffusion model training. A weighted\nphysics-informed loss is constructed using a static weight schedule. This\napproach enables a diffusion model to accurately approximate underlying data\ndistribution, which can influence the unsupervised anomaly detection\nperformance. Our experiments on synthetic and real-world datasets show that\nphysics-informed training improves the F1 score in anomaly detection; it\ngenerates better data diversity and log-likelihood. Our model outperforms\nbaseline approaches, additionally, it surpasses prior physics-informed work and\npurely data-driven diffusion models on a synthetic dataset and one real-world\ndataset while remaining competitive on others.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\u548c\u52a0\u6743\u7684\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u7406\u4fe1\u606f\u6765\u66f4\u51c6\u786e\u5730\u8fd1\u4f3c\u5e95\u5c42\u6570\u636e\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u52a0\u6743\u7684\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u6765\u5b66\u4e60\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7269\u7406\u4f9d\u8d56\u65f6\u95f4\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684F1\u5206\u6570\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u6570\u636e\u591a\u6837\u6027\u548c\u5bf9\u6570\u4f3c\u7136\u5ea6\u3002\u8be5\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u6539\u8fdb\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86F1\u5206\u6570\u3001\u6570\u636e\u591a\u6837\u6027\u548c\u5bf9\u6570\u4f3c\u7136\u5ea6\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2508.11330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11330", "abs": "https://arxiv.org/abs/2508.11330", "authors": ["Yanghao Wang", "Long Chen"], "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "comment": null, "summary": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.", "AI": {"tldr": "NoOp \u901a\u8fc7\u4f18\u5316\u201c\u597d\u201d\u566a\u58f0\u6765\u7a33\u5b9a\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u5206\u7c7b\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982 CLIP\uff09\u5b58\u5728\u7684\u8bcd\u888b\u95ee\u9898\u548c\u865a\u5047\u504f\u5dee\uff0c\u4ee5\u53ca\u5f53\u524d\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u5b58\u5728\u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff08\u5373\u4e0d\u540c\u7684\u968f\u673a\u566a\u58f0\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u53d8\u5316\uff09\uff0c\u5e76\u63d0\u9ad8 DC \u7684\u5206\u7c7b\u901f\u5ea6\uff08\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u5927\u91cf\u91c7\u6837\u566a\u58f0\u6765\u89e3\u51b3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901f\u5ea6\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a NoOp \u7684\u65b0\u9896\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u5b66\u4e60\u5339\u914d\u7684\uff08\u201c\u597d\u201d\uff09\u566a\u58f0\u6765\u89e3\u51b3 DC \u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002NoOp \u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1. **\u9891\u7387\u5339\u914d**\uff1a\u4f18\u5316\u7279\u5b9a\u6570\u636e\u96c6\u548c\u65f6\u95f4\u6b65 t \u7684\u566a\u58f0\u30022. **\u7a7a\u95f4\u5339\u914d**\uff1a\u8bad\u7ec3\u4e00\u4e2a\u5143\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u4ee5\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5e76\u8f93\u51fa\u56fe\u50cf\u7279\u5b9a\u7684\u566a\u58f0\u504f\u79fb\u3002\u901a\u8fc7\u5c06\u4f18\u5316\u540e\u7684\u566a\u58f0\u548c\u566a\u58f0\u504f\u79fb\u76f8\u7ed3\u5408\uff0cNoOp \u53ef\u4ee5\u66ff\u4ee3 DC \u4e2d\u7684\u968f\u673a\u566a\u58f0\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86 NoOp \u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u7f13\u89e3 DC \u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u548c\u901f\u5ea6\u3002", "conclusion": "NoOp \u901a\u8fc7\u4f18\u5316\u548c\u5143\u7f51\u7edc\u5b66\u4e60\u5339\u914d\u7684\u201c\u597d\u201d\u566a\u58f0\uff0c\u53ef\u4ee5\u7f13\u89e3\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Ma\u00f1as", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "AI": {"tldr": "This paper presents a new method to control how MLLMs describe images by guiding their output with rewards for accuracy (precision) and completeness (recall). This allows users to tune the model's responses on the fly, balancing detail and accuracy, and improves performance compared to existing methods.", "motivation": "To adapt Multimodal Large Language Models (MLLMs) for diverse user needs by enabling controlled decoding, specifically to improve their visual grounding capabilities.", "method": "The paper introduces a reward-guided decoding method for MLLMs, using two separate reward models to control object precision and recall independently. This method guides the MLLM's decoding process on-the-fly by adjusting the relative importance of reward functions and controlling the search breadth during decoding.", "result": "The method demonstrates significant controllability over MLLM inference, allowing users to dynamically trade off object precision for recall and control the trade-off between compute time and grounding degree. It consistently outperforms existing hallucination mitigation methods on standard benchmarks.", "conclusion": "The proposed reward-guided decoding method offers significant controllability over MLLM inference, outperforming existing hallucination mitigation methods and allowing dynamic trade-offs between object precision and recall."}}
{"id": "2508.11529", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11529", "abs": "https://arxiv.org/abs/2508.11529", "authors": ["George Paterakis", "Andrea Castellani", "George Papoutsoglou", "Tobias Rodemann", "Ioannis Tsamardinos"], "title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow", "comment": "Preprint. Currently under review at \"Artificial Intelligence Review\"\n  journal", "summary": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.", "AI": {"tldr": "HXAI is a new framework that makes AI models more transparent and trustworthy by explaining them at every stage of the data analysis workflow, tailored to different users. It addresses limitations of current methods by considering the entire process and using AI agents with LLMs to communicate findings effectively.", "motivation": "Many users still regard AI models as opaque 'black boxes'. Conventional explainable AI methods clarify individual predictions but overlook upstream decisions and downstream quality checks that determine whether insights can be trusted. HXAI aims to address this by providing a holistic approach to explainability.", "method": "HXAI unifies six components (data, analysis set-up, learning process, model output, model quality, communication channel) into a single taxonomy and aligns each component with the needs of domain experts, data analysts, and data scientists. It uses an 112-item question bank and analyzes contemporary tools to identify coverage gaps. The framework is grounded in theories of human explanation, principles from human-computer interaction, and findings from empirical user studies.", "result": "HXAI identifies characteristics that make explanations clear, actionable, and cognitively manageable. Its taxonomy reduces terminological ambiguity and enables rigorous coverage analysis of existing toolchains. AI agents embedding LLMs can orchestrate diverse explanation techniques to translate technical artifacts into stakeholder-specific narratives.", "conclusion": "HXAI presents a user-centric framework that embeds explanation into every stage of the data-analysis workflow and tailors explanations to users, operationalizing insights into a taxonomy to reduce ambiguity and enable rigorous coverage analysis of existing toolchains. It also demonstrates how AI agents with LLMs can orchestrate explanation techniques to bridge the gap between AI developers and domain experts, advancing an end-to-end viewpoint on transparency, trustworthiness, and responsible AI deployment."}}
{"id": "2508.11334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11334", "abs": "https://arxiv.org/abs/2508.11334", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Md Jawadul Hasan", "Tze Hui Liew"], "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "comment": "Accepted in ICCVDM '25", "summary": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.", "AI": {"tldr": "GANDiff FR\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u4eba\u53e3\u548c\u73af\u5883\u56e0\u7d20\u6765\u8861\u91cf\u3001\u89e3\u91ca\u548c\u51cf\u5c11\u504f\u5dee\uff0c\u5e76\u5df2\u6210\u529f\u7528\u4e8e\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u51cf\u5c11\u4e86\u7ec4\u95f4\u5dee\u5f02\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5176\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u4e3a\u4e86\u7cbe\u786e\u63a7\u5236\u4eba\u53e3\u548c\u73af\u5883\u56e0\u7d20\uff0c\u4ee5\u53ef\u590d\u73b0\u7684\u4e25\u8c28\u6027\u6765\u8861\u91cf\u3001\u89e3\u91ca\u548c\u51cf\u5c11\u504f\u5dee\uff0c\u4ece\u800c\u5efa\u7acb\u4e00\u4e2a\u7b26\u5408\u6cd5\u89c4\u6807\u51c6\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u6846\u67b6\u3002", "method": "GANDiff FR\u6846\u67b6\u7edf\u4e00\u4e86\u57fa\u4e8eStyleGAN3\u7684\u8eab\u4efd\u4fdd\u6301\u751f\u6210\u548c\u57fa\u4e8e\u6269\u6563\u7684\u5c5e\u6027\u63a7\u5236\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u4eba\u8138\u59ff\u6001\uff08\u7ea630\u5ea6\uff09\u3001\u5149\u7167\uff08\u56db\u65b9\u5411\uff09\u548c\u8868\u60c5\uff08\u4e94\u7ea7\u522b\uff09\u7b49\u56e0\u7d20\u7684\u7cbe\u7ec6\u64cd\u63a7\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u6761\u4ef6\u4e0d\u53d8\u3002", "result": "\u5728\u76f8\u540c\u7684\u8fd0\u884c\u70b9\u4e0b\uff0c\u5bf9ArcFace\u3001CosFace\u548cAdaFace\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cAdaFace\u5c06\u7ec4\u95f4TPR\u5dee\u5f02\u51cf\u5c11\u4e8660%\uff082.5% vs. 6.3%\uff09\uff0c\u5176\u4e2d\u5149\u7167\u56e0\u7d20\u5360\u4e86\u5269\u4f59\u504f\u5dee\u768442%\u3002\u8de8\u6570\u636e\u96c6\uff08RFW\u3001BUPT\u548cCASIA WebFace\uff09\u7684\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u8fc1\u79fb\u6027\uff08r=0.85\uff09\u3002", "conclusion": "GANDiff FR\u662f\u4e00\u4e2a\u9996\u521b\u7684\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u4eba\u53e3\u548c\u73af\u5883\u56e0\u7d20\uff0c\u4ee5\u53ef\u590d\u73b0\u7684\u4e25\u8c28\u6027\u6765\u8861\u91cf\u3001\u89e3\u91ca\u548c\u51cf\u5c11\u504f\u5dee\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8eStyleGAN3\u7684\u8eab\u4efd\u4fdd\u6301\u751f\u6210\u548c\u57fa\u4e8e\u6269\u6563\u7684\u5c5e\u6027\u63a7\u5236\uff0c\u80fd\u591f\u5bf9\u7ea630\u5ea6\u7684\u4eba\u8138\u59ff\u6001\u3001\u56db\u79cd\u65b9\u5411\u7684\u5149\u7167\u548c\u4e94\u4e2a\u7ea7\u522b\u7684\u5f71\u54cd\u8fdb\u884c\u7cbe\u7ec6\u64cd\u63a7\u3002\u901a\u8fc7\u5408\u621010,000\u5f20\u8de8\u4e94\u4e2a\u961f\u5217\u7684\u4eba\u8138\uff0c\u5e76\u7ecf\u8fc7\u81ea\u52a8\u68c0\u6d4b\uff0898.2%\uff09\u548c\u4eba\u5de5\u5ba1\u67e5\uff0889%\uff09\u9a8c\u8bc1\u5176\u771f\u5b9e\u6027\uff0c\u4ee5\u5206\u79bb\u548c\u91cf\u5316\u504f\u5dee\u9a71\u52a8\u56e0\u7d20\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u7eafGANs\u76f8\u6bd4\uff0cGANDiff FR\u5728\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u7ea620%\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u4ea7\u751f\u4e09\u500d\u7684\u5c5e\u6027\u6761\u4ef6\u53d8\u4f53\uff0c\u4e3a\u516c\u5e73\u6027\u5ba1\u8ba1\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u7b26\u5408\u6cd5\u89c4\uff08\u6b27\u76df\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\uff09\u7684\u6807\u51c6\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u53d1\u5e03\uff0c\u4ee5\u652f\u6301\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u504f\u5dee\u8bc4\u4f30\u3002"}}
{"id": "2508.11530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11530", "abs": "https://arxiv.org/abs/2508.11530", "authors": ["Lianshuai Guo", "Zhongzheng Yuan", "Xunkai Li", "Yinlin Zhu", "Meixia Qu", "Wenyu Wang"], "title": "DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning", "comment": null, "summary": "Decentralized Federated Learning (DFL) has emerged as a robust distributed\nparadigm that circumvents the single-point-of-failure and communication\nbottleneck risks of centralized architectures. However, a significant challenge\narises as existing DFL optimization strategies, primarily designed for tasks\nsuch as computer vision, fail to address the unique topological information\ninherent in the local subgraph. Notably, while Federated Graph Learning (FGL)\nis tailored for graph data, it is predominantly implemented in a centralized\nserver-client model, failing to leverage the benefits of decentralization.To\nbridge this gap, we propose DFed-SST, a decentralized federated graph learning\nframework with adaptive communication. The core of our method is a\ndual-topology adaptive communication mechanism that leverages the unique\ntopological features of each client's local subgraph to dynamically construct\nand optimize the inter-client communication topology. This allows our framework\nto guide model aggregation efficiently in the face of heterogeneity. Extensive\nexperiments on eight real-world datasets consistently demonstrate the\nsuperiority of DFed-SST, achieving 3.26% improvement in average accuracy over\nbaseline methods.", "AI": {"tldr": "DFed-SST\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u4fe1\u673a\u5236\u6765\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684DFL\u4f18\u5316\u7b56\u7565\u4e3b\u8981\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u4efb\u52a1\u8bbe\u8ba1\uff0c\u672a\u80fd\u89e3\u51b3\u672c\u5730\u5b50\u56fe\u7684\u72ec\u7279\u62d3\u6251\u4fe1\u606f\u3002\u867d\u7136FGL\u9002\u7528\u4e8e\u56fe\u6570\u636e\uff0c\u4f46\u5b83\u4e3b\u8981\u5728\u4e2d\u5fc3\u5316\u7684\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u6a21\u578b\u4e2d\u5b9e\u73b0\uff0c\u672a\u80fd\u5229\u7528\u53bb\u4e2d\u5fc3\u5316\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFed-SST\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u81ea\u9002\u5e94\u901a\u4fe1\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u79cd\u53cc\u62d3\u6251\u81ea\u9002\u5e94\u901a\u4fe1\u673a\u5236\uff0c\u5b83\u5229\u7528\u6bcf\u4e2a\u5ba2\u6237\u7aef\u672c\u5730\u5b50\u56fe\u7684\u72ec\u7279\u62d3\u6251\u7279\u5f81\u6765\u52a8\u6001\u6784\u5efa\u548c\u4f18\u5316\u5ba2\u6237\u7aef\u95f4\u901a\u4fe1\u62d3\u6251\u3002", "result": "\u5728\u5e73\u5747\u51c6\u786e\u7387\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e863.26%\u3002", "conclusion": "DFed-SST\u6846\u67b6\u5728\u516b\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e00\u81f4\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5728\u5e73\u5747\u51c6\u786e\u7387\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e863.26%\u3002"}}
{"id": "2508.11339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11339", "abs": "https://arxiv.org/abs/2508.11339", "authors": ["Mingxiao Ma", "Shunyao Zhu", "Guoliang Kang"], "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection", "comment": "12 pages, 5 figures", "summary": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7d22\u5f15\u5bf9\u9f50\u67e5\u8be2\u84b8\u998f\uff08IAQD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\u3002\u4e0e\u73b0\u6709\u7684\u5308\u7259\u5229\u5339\u914d\u65b9\u6cd5\u4e0d\u540c\uff0cIAQD\u901a\u8fc7\u5339\u914d\u5177\u6709\u76f8\u540c\u7d22\u5f15\u7684\u67e5\u8be2\u5e76\u4ec5\u5728\u5173\u952e\u67e5\u8be2\u4e0a\u8fdb\u884c\u84b8\u998f\uff0c\u6709\u6548\u4fdd\u7559\u4e86\u6a21\u578b\u5bf9\u5148\u524d\u7c7b\u522b\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u5b66\u4e60\u65b0\u7c7b\u522b\uff0c\u6700\u7ec8\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\uff08IOD\uff09\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8eTransformer\u7684\u68c0\u6d4b\u6a21\u578b\u53ef\u80fd\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u5177\u4f53\u65b9\u6cd5\u662f\u4f7f\u7528\u5308\u7259\u5229\u5339\u914d\u6765\u5339\u914d\u67e5\u8be2\u5e76\u5bf9\u9f50\u8f93\u51fa\u3002\u7136\u800c\uff0c\u7814\u7a76\u8005\u53d1\u73b0\u5308\u7259\u5229\u5339\u914d\u5728\u8fd9\u79cd\u4efb\u52a1\u4e2d\u5e76\u975e\u6700\u4f73\u9009\u62e9\uff0c\u56e0\u4e3a\u5b83\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u9057\u5fd8\u5148\u524d\u7f16\u7801\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7d22\u5f15\u5bf9\u9f50\u67e5\u8be2\u84b8\u998f\uff08IAQD\uff09\u7684\u65b0\u84b8\u998f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acb\u5177\u6709\u76f8\u540c\u7d22\u5f15\u7684\u524d\u4e00\u9636\u6bb5\u548c\u5f53\u524d\u9636\u6bb5\u6a21\u578b\u7684\u67e5\u8be2\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u4ec5\u5728\u5bf9\u68c0\u6d4b\u5148\u524d\u7c7b\u522b\u81f3\u5173\u91cd\u8981\u7684\u90e8\u5206\u67e5\u8be2\u4e0a\u6267\u884c\u7d22\u5f15\u5bf9\u9f50\u84b8\u998f\uff0c\u4ece\u800c\u5728\u4e0d\u5e72\u6270\u65b0\u7c7b\u522b\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u4e86\u5148\u524d\u7684\u524d\u8bed\u4e49\u548c\u7a7a\u95f4\u7f16\u7801\u80fd\u529b\u3002", "result": "IAQD\u6210\u529f\u7f13\u89e3\u4e86\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684IAQD\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11374", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11374", "abs": "https://arxiv.org/abs/2508.11374", "authors": ["Devansh Arora", "Nitin Kumar", "Sukrit Gupta"], "title": "Does the Skeleton-Recall Loss Really Work?", "comment": null, "summary": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.", "AI": {"tldr": "SRL\u635f\u5931\u51fd\u6570\u5728\u5206\u5272\u7ec6\u957f\u7ba1\u72b6\u7ed3\u6784\u65b9\u9762\u7684\u6027\u80fd\u88ab\u8bc1\u660e\u5e76\u4e0d\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30SRL\uff08\u4e00\u79cd\u7528\u4e8e\u5206\u5272\u7ec6\u957f\u7ba1\u72b6\u7ed3\u6784\u7684\u62d3\u6251\u4fdd\u6301\u635f\u5931\u51fd\u6570\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7a76\u57fa\u4e8e\u62d3\u6251\u7684\u635f\u5931\u51fd\u6570\u7684\u5c40\u9650\u6027\u3002", "method": "\u5bf9SRL\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5728\u591a\u4e2a\u7ba1\u72b6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5c06SRL\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "SRL\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\u5e76\u672a\u8d85\u8fc7\u4f20\u7edf\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd9\u4e0e\u539f\u59cb\u7814\u7a76\u7684\u58f0\u79f0\u4e0d\u7b26\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u57fa\u4e8e\u62d3\u6251\u7684\u635f\u5931\u51fd\u6570\uff08\u7279\u522b\u662fSRL\uff09\u5728\u5206\u5272\u7ec6\u957f\u7ba1\u72b6\u7ed3\u6784\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8868\u660e\u5176\u6027\u80fd\u5e76\u672a\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.11340", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2508.11340", "abs": "https://arxiv.org/abs/2508.11340", "authors": ["Yuanlin Liu", "Zhihan Zhou", "Mingqiang Wei", "Youyi Song"], "title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification", "comment": "accepted by CW2025", "summary": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.", "AI": {"tldr": "Active labeling reduces human cost in training cervical cell classifiers by intelligently selecting images to label based on model uncertainty.", "motivation": "Existing automatic classification methods for cervical cells require representative training datasets, which incur high human costs. This paper addresses the need for a more cost-effective approach.", "method": "Active labeling that leverages the classifier's uncertainty on unlabeled cervical cell images to select the most beneficial images for labeling, with a fast estimation of uncertainty.", "result": "The active labeling algorithm enhances the representative ability of the training dataset and demonstrates efficacy in navigating human cost usage, leading to data-efficient cervical cell classification.", "conclusion": "The proposed active labeling method efficiently constructs a representative training dataset with reduced human cost for data-efficient cervical cell classification, confirmed by empirical results."}}
{"id": "2508.11553", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11553", "abs": "https://arxiv.org/abs/2508.11553", "authors": ["Jinghui Wang", "Shaojie Wang", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Xiaojiang Zhang", "Minglei Zhang", "Jiarong Zhang", "Wenhao Zhuang", "Yuchen Cao", "Wankang Bao", "Haimo Li", "Zheng Lin", "Huiming Wang", "Haoyang Huang", "Zongxian Feng", "Zizheng Zhan", "Ken Deng", "Wen Xiang", "Huaixi Tang", "Kun Wu", "Mengtong Li", "Mengfei Xie", "Junyi Peng", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling", "comment": null, "summary": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks.", "AI": {"tldr": "SeamlessFlow is a server-based RL framework that decouples training from execution and maximizes GPU utilization for industrial-scale RL by using a data plane, trajectory manager, and tag-driven scheduling.", "motivation": "The paper addresses two main challenges in industrial-scale RL: decoupling training from complex agent execution flows and maximizing GPU utilization with minimal idle time while maintaining stability and scalability.", "method": "SeamlessFlow employs a data plane to decouple the RL trainer from agent implementations, a central trajectory manager for partial rollouts, and a tag-driven scheduling paradigm with spatiotemporal multiplexing to dynamically reassign idle training nodes.", "result": "SeamlessFlow enables stable and high-performance RL training, effectively handling complex scenarios by maximizing GPU utilization and decoupling training from execution.", "conclusion": "SeamlessFlow is a stable and high-performance framework suitable for complex RL tasks like multi-agent and long-horizon scenarios. It achieves this by decoupling RL training from agent execution and maximizing GPU utilization through a tag-driven scheduling paradigm and spatiotemporal multiplexing."}}
{"id": "2508.11379", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11379", "abs": "https://arxiv.org/abs/2508.11379", "authors": ["Ramil Khafizov", "Artem Komarichev", "Ruslan Rakhimov", "Peter Wonka", "Evgeny Burnaev"], "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "comment": null, "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.", "AI": {"tldr": "G-CUT3R\u662f\u4e00\u79cd\u65b0\u76843D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u548c\u76f8\u673a\u4fe1\u606f\u7b49\u989d\u5916\u6570\u636e\u6765\u6539\u8fdbCUT3R\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u9988\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8f93\u5165\u56fe\u50cf\uff0c\u800cG-CUT3R\u65e8\u5728\u5229\u7528\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5e38\u89c1\u7684\u8f85\u52a9\u6570\u636e\uff08\u5982\u6df1\u5ea6\u3001\u76f8\u673a\u6807\u5b9a\u6216\u76f8\u673a\u4f4d\u7f6e\uff09\u6765\u63d0\u53473D\u573a\u666f\u91cd\u5efa\u7684\u6027\u80fd\u3002", "method": "G-CUT3R\u662f\u4e00\u79cd\u524d\u9988\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e13\u95e8\u7684\u7f16\u7801\u5668\u6765\u5904\u7406\u8f85\u52a9\u6570\u636e\uff08\u5982\u6df1\u5ea6\u3001\u76f8\u673a\u6807\u5b9a\u3001\u76f8\u673a\u4f4d\u7f6e\uff09\uff0c\u5e76\u4f7f\u7528\u96f6\u5377\u79ef\u5c06\u63d0\u53d6\u7684\u7279\u5f81\u4e0eRGB\u56fe\u50cf\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ec3D\u91cd\u5efa\u548c\u5176\u4ed6\u591a\u89c6\u56fe\u4efb\u52a1\uff0cG-CUT3R\u5747\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u5229\u7528\u53ef\u7528\u5148\u9a8c\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u517c\u5bb9\u4e0d\u540c\u7684\u8f93\u5165\u6a21\u6001\u3002", "conclusion": "G-CUT3R\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u4fe1\u606f\uff08\u5982\u6df1\u5ea6\u3001\u76f8\u673a\u6807\u5b9a\u6216\u76f8\u673a\u4f4d\u7f6e\uff09\u589e\u5f3a\u4e86CUT3R\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u76843D\u573a\u666f\u91cd\u5efa\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8f7b\u91cf\u7ea7\u4fee\u6539\uff0c\u4e3a\u4e0d\u540c\u6a21\u6001\u6570\u636e\u8bbe\u8ba1\u4e86\u4e13\u7528\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u96f6\u5377\u79ef\u4e0eRGB\u56fe\u50cf\u7279\u5f81\u878d\u5408\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u5404\u79cd\u5148\u9a8c\u4fe1\u606f\u3002"}}
{"id": "2508.11341", "categories": ["cs.CV", "cs.CR", "cs.LG", "68T45, 68T01, 68T07, 68T10, 68M25", "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.11341", "abs": "https://arxiv.org/abs/2508.11341", "authors": ["Katarzyna Filus", "Jorge M. Cruz-Duarte"], "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "comment": "12 pages, 4 figures, 3 tables. Submitted for peer review", "summary": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982BERT\u3001CLIP\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u6765\u9009\u62e9\u5bf9\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u6700\u6709\u5229\u6216\u6700\u4e0d\u5229\u7684\u76ee\u6807\u6807\u7b7e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8eWordNet\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u4e14\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u89c6\u89c9\u6a21\u578b\u7684\u5b9a\u5411\u5bf9\u6297\u6027\u653b\u51fb\u4e2d\uff0c\u76ee\u6807\u6807\u7b7e\u7684\u9009\u62e9\u81f3\u5173\u91cd\u8981\u4f46\u5e38\u88ab\u5ffd\u89c6\u3002\u73b0\u6709\u7684\u7b56\u7565\uff08\u968f\u673a\u3001\u6a21\u578b\u9884\u6d4b\u3001\u9759\u6001\u8bed\u4e49\u8d44\u6e90\uff09\u5728\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u590d\u73b0\u6027\u6216\u7075\u6d3b\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u5177\u89e3\u91ca\u6027\u7684\u76ee\u6807\u6807\u7b7e\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff08BERT, TinyLLAMA\uff09\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08CLIP\uff09\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u6765\u9009\u62e9\u4e0e\u771f\u5b9e\u6807\u7b7e\u8bed\u4e49\u4e0a\u6700\u76f8\u5173\u548c\u6700\u4e0d\u76f8\u5173\u7684\u76ee\u6807\u6807\u7b7e\uff0c\u4ece\u800c\u6784\u5efa\u6700\u4f73\u548c\u6700\u5dee\u60c5\u51b5\u7684\u5bf9\u6297\u6027\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\u80fd\u591f\u751f\u6210\u5b9e\u7528\u7684\u5bf9\u6297\u6027\u76ee\u6807\uff0c\u5e76\u4e14\u5728\u5904\u7406\u8fdc\u8ddd\u79bb\u7c7b\u522b\u5173\u7cfb\u65f6\u4f18\u4e8eWordNet\u7b49\u9759\u6001\u8bcd\u6c47\u6570\u636e\u5e93\u3002\u6b64\u5916\uff0c\u9759\u6001\u6d4b\u8bd5\u76ee\u6807\u6807\u7b7e\u53ef\u4ee5\u521d\u6b65\u8bc4\u4f30\u76f8\u4f3c\u6027\u6e90\u7684\u6709\u6548\u6027\uff08\u5148\u9a8c\u6d4b\u8bd5\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6765\u9009\u62e9\u5bf9\u6297\u6027\u76ee\u6807\u6807\u7b7e\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684WordNet\u7b49\u9759\u6001\u8bcd\u6c47\u6570\u636e\u5e93\uff0c\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u7684\u5bf9\u6297\u6027\u57fa\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.11618", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11618", "abs": "https://arxiv.org/abs/2508.11618", "authors": ["Jungang Chen", "Seyyed A. Hosseini"], "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective", "comment": "38 pages, 16 figures", "summary": "Carbon capture and storage (CCS) projects typically involve a diverse array\nof stakeholders or players from public, private, and regulatory sectors, each\nwith different objectives and responsibilities. Given the complexity, scale,\nand long-term nature of CCS operations, determining whether individual\nstakeholders can independently maximize their interests or whether\ncollaborative coalition agreements are needed remains a central question for\neffective CCS project planning and management. CCS projects are often\nimplemented in geologically connected sites, where shared geological features\nsuch as pressure space and reservoir pore capacity can lead to competitive\nbehavior among stakeholders. Furthermore, CO2 storage sites are often located\nin geologically mature basins that previously served as sites for hydrocarbon\nextraction or wastewater disposal in order to leverage existing\ninfrastructures, which makes unilateral optimization even more complicated and\nunrealistic.\n  In this work, we propose a paradigm based on Markov games to quantitatively\ninvestigate how different coalition structures affect the goals of\nstakeholders. We frame this multi-stakeholder multi-site problem as a\nmulti-agent reinforcement learning problem with safety constraints. Our\napproach enables agents to learn optimal strategies while compliant with safety\nregulations. We present an example where multiple operators are injecting CO2\ninto their respective project areas in a geologically connected basin. To\naddress the high computational cost of repeated simulations of high-fidelity\nmodels, a previously developed surrogate model based on the Embed-to-Control\n(E2C) framework is employed. Our results demonstrate the effectiveness of the\nproposed framework in addressing optimal management of CO2 storage when\nmultiple stakeholders with various objectives and goals are involved.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u5177\u6709\u4e0d\u540c\u76ee\u6807\u548c\u7ea6\u675f\u7684\u591a\u4e2aCCS\u9879\u76ee\u53c2\u4e0e\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6700\u4f18\u7ba1\u7406\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5730\u8d28\u5171\u4eab\u548c\u57fa\u7840\u8bbe\u65bd\u5229\u7528\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "motivation": "CCS\u9879\u76ee\u6d89\u53ca\u591a\u4e2a\u90e8\u95e8\u7684\u5229\u76ca\u76f8\u5173\u8005\uff0c\u5176\u76ee\u6807\u548c\u804c\u8d23\u5404\u4e0d\u76f8\u540c\u3002\u9274\u4e8eCCS\u7684\u590d\u6742\u6027\u3001\u89c4\u6a21\u548c\u957f\u671f\u6027\uff0c\u786e\u5b9a\u5229\u76ca\u76f8\u5173\u8005\u662f\u72ec\u7acb\u6700\u5927\u5316\u81ea\u8eab\u5229\u76ca\u8fd8\u662f\u9700\u8981\u534f\u4f5c\u8fbe\u6210\u4e00\u81f4\uff0c\u662fCCS\u9879\u76ee\u89c4\u5212\u548c\u7ba1\u7406\u7684\u6838\u5fc3\u95ee\u9898\u3002\u5c24\u5176\u662f\u5728\u5730\u8d28\u8fde\u901a\u7684\u573a\u5730\uff0c\u5171\u4eab\u5730\u8d28\u7279\u5f81\u53ef\u80fd\u5bfc\u81f4\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u7ade\u4e89\u3002\u6b64\u5916\uff0c\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\uff0c\u5c06CCS\u7ad9\u70b9\u8bbe\u5728\u5df2\u5f00\u53d1\u8fc7\u7684\u533a\u57df\uff0c\u4f7f\u5f97\u5355\u65b9\u9762\u4f18\u5316\u66f4\u52a0\u590d\u6742\u548c\u4e0d\u73b0\u5b9e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u8303\u5f0f\uff0c\u5c06\u591a\u65b9\u3001\u591a\u4f4d\u70b9\u7684CCS\u95ee\u9898\u6784\u5efa\u4e3a\u5177\u6709\u5b89\u5168\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u5e94\u5bf9\u9ad8\u4fdd\u771f\u6a21\u578b\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u91c7\u7528\u4e86\u57fa\u4e8eEmbed-to-Control\uff08E2C\uff09\u6846\u67b6\u7684\u4ee3\u7406\u6a21\u578b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u5728\u5904\u7406\u6d89\u53ca\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u53ca\u5176\u4e0d\u540c\u76ee\u6807\u548c\u76ee\u6807\u7684CO2\u5c01\u5b58\u6700\u4f18\u7ba1\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u65b9\u53c2\u4e0e\u7684\u78b3\u6355\u83b7\u4e0e\u5c01\u5b58\uff08CCS\uff09\u9879\u76ee\u7684\u6700\u4f18\u7ba1\u7406\u95ee\u9898\uff0c\u5e76\u80fd\u5728\u9075\u5b88\u5b89\u5168\u89c4\u5b9a\u7684\u524d\u63d0\u4e0b\uff0c\u5e2e\u52a9\u5404\u65b9\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002"}}
{"id": "2508.11350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11350", "abs": "https://arxiv.org/abs/2508.11350", "authors": ["Zhenhao Zhang", "Hanqing Wang", "Xiangyu Zeng", "Ziyu Cheng", "Jiaxin Liu", "Haoyu Yan", "Zhirui Liu", "Kaiyang Ji", "Tianxiang Gui", "Ke Hu", "Kangyi Chen", "Yahao Fan", "Mokai Pan"], "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "comment": null, "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "AI": {"tldr": "HOID-R1 \u662f\u9996\u4e2a\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684 HOI \u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u201cMLLM-as-a-judge\u201d\u673a\u5236\u6765\u51cf\u8f7b\u5e7b\u89c9\uff0c\u5728 HOI \u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47 HOI \u68c0\u6d4b\u4e2d\u4ec5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u63d0\u793a\uff0c\u5ffd\u7565\u4e86\u5176\u56fa\u6709\u7684\u4e09\u7ef4\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "HOID-R1 \u662f\u4e00\u4e2a\u6574\u5408\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08MADDPG\uff09\u7684 HOI \u68c0\u6d4b\u6846\u67b6\u3002\u9996\u5148\uff0cSFT \u7528\u4e8e\u4e3a\u6a21\u578b\u6ce8\u5165\u57fa\u672c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u5236\u6a21\u578b\u5728\u8f93\u51fa\u4e2d\u9610\u8ff0\u5176\u601d\u8003\u8fc7\u7a0b\u3002\u7136\u540e\uff0cMADDPG \u7528\u4e8e\u5229\u7528\u591a\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u4ece\u800c\u589e\u5f3a\u8de8\u6a21\u6001\u7684\u5bf9\u9f50\u3002\u4e3a\u4e86\u51cf\u8f7b CoT \u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\uff0c\u5f15\u5165\u4e86\u201cMLLM-as-a-judge\u201d\u673a\u5236\u6765\u76d1\u7763 CoT \u8f93\u51fa\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "result": "HOID-R1 \u5728 HOI \u68c0\u6d4b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u5bf9\u65b0\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HOID-R1 \u5728 HOI \u68c0\u6d4b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u5bf9\u65b0\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11376", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11376", "abs": "https://arxiv.org/abs/2508.11376", "authors": ["Durgesh Mishra", "Rishabh Uikey"], "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025", "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u548c\u5173\u7cfb\u7ea7\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u4f7f\u5b66\u751f\u6a21\u578b\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\u5bf9\u4e8e\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\uff09\u4e2d\u4f18\u5316\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u7ec6\u8282\u548c\u590d\u6742\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff1a\u5b9e\u4f8b\u7ea7\u5d4c\u5165\u84b8\u998f\u548c\u57fa\u4e8e\u5173\u7cfb\u5bf9\u76f8\u4f3c\u6027\u84b8\u998f\u3002\u5b9e\u4f8b\u7ea7\u5d4c\u5165\u84b8\u998f\u5229\u7528\u52a8\u6001\u56f0\u96be\u6316\u6398\u7b56\u7565\u5bf9\u9f50\u5355\u4e2a\u7279\u5f81\u5d4c\u5165\uff1b\u57fa\u4e8e\u5173\u7cfb\u5bf9\u76f8\u4f3c\u6027\u84b8\u998f\u901a\u8fc7\u6210\u5bf9\u76f8\u4f3c\u6027\u5173\u7cfb\u6355\u83b7\u5173\u7cfb\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u5185\u5b58\u5e93\u673a\u5236\u548c\u6837\u672c\u6316\u6398\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7edf\u4e00\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u4eba\u8138\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\u3002\u7279\u522b\u5730\uff0c\u5f53\u4f7f\u7528\u5f3a\u6559\u5e08\u7f51\u7edc\u65f6\uff0c\u8be5\u7edf\u4e00\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4f7f\u5b66\u751f\u6a21\u578b\u7684\u51c6\u786e\u6027\u751a\u81f3\u8d85\u8fc7\u4e86\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u4eba\u8138\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5b66\u751f\u6a21\u578b\u7684\u51c6\u786e\u6027\u751a\u81f3\u8d85\u8fc7\u4e86\u6559\u5e08\u6a21\u578b\u3002"}}
{"id": "2508.11409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11409", "abs": "https://arxiv.org/abs/2508.11409", "authors": ["Zhiming Liu", "Nantheera Anantrasirichai"], "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "comment": null, "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.", "AI": {"tldr": "RMFAT\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u89c6\u9891\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u5904\u7406\u6765\u89e3\u51b3\u5927\u6c14\u6e4d\u6d41\u95ee\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u548c3D\u67b6\u6784\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5185\u5b58\u5360\u7528\u5927\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMFAT\uff08\u5faa\u73af\u591a\u5c3a\u5ea6\u7279\u5f81\u5927\u6c14\u6e4d\u6d41\u4fee\u590d\u5668\uff09\u7684\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6bcf\u6b21\u4ec5\u4f7f\u7528\u4e24\u4e2a\u8f93\u5165\u5e27\u8fdb\u884c\u6062\u590d\uff0c\u5e76\u96c6\u6210\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\u4ee5\u53ca\u65f6\u95f4\u626d\u66f2\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRMFAT\u5728\u6e05\u6670\u5ea6\u6062\u590d\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u65f6\u5927\u6c14\u6e4d\u6d41\u6291\u5236\u3002", "conclusion": "RMFAT\u901a\u8fc7\u91c7\u7528\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u7f16\u7801\u89e3\u7801\u4ee5\u53ca\u96c6\u6210\u7684\u65f6\u95f4\u626d\u66f2\u6a21\u5757\uff0c\u5728\u6e05\u6670\u5ea6\u6062\u590d\uff08SSIM\u63d0\u9ad8\u8fd19%\uff09\u548c\u63a8\u7406\u901f\u5ea6\uff08\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u56db\u500d\u4ee5\u4e0a\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5b9e\u65f6\u5927\u6c14\u6e4d\u6d41\u6291\u5236\u4efb\u52a1\u3002"}}
{"id": "2508.11446", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11446", "abs": "https://arxiv.org/abs/2508.11446", "authors": ["Daniel Airinei", "Elena Burceanu", "Marius Leordeanu"], "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation", "comment": "Accepted at the International Conference on Computer Vision Workshops\n  2025", "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u7eaf\u89c6\u89c9\u5ba4\u5185\u5bfc\u822a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u56fe\u751f\u6210\u8def\u5f84\u3001\u53ef\u89e3\u91ca\u6570\u636e\u589e\u5f3a\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c Android \u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5ba4\u5185\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u90e8\u7f72\u590d\u6742\u4e14\u6709\u989d\u5916\u8981\u6c42\uff0c\u800c\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u5b9e\u65f6\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u7eaf\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u3001\u53ef\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u56fe\u751f\u6210\u8def\u5f84\u65b9\u6cd5\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u589e\u5f3a\u548c\u8bfe\u7a0b\u5b66\u4e60\u6765\u9884\u6d4b\u79fb\u52a8\u8bbe\u5907\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u65b9\u5411\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d2d\u7269\u4e2d\u5fc3\u5185\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5e27\u8fdb\u884c\u4e86\u6307\u5411\u4e0d\u540c\u76ee\u6807\u7684\u76ee\u7684\u5730\u7684\u6b63\u786e\u65b9\u5411\u6807\u6ce8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\uff0c\u65e0\u9700\u7279\u6b8a\u4f20\u611f\u5668\u3001\u8def\u5f84\u6807\u8bb0\u3001\u573a\u666f\u5730\u56fe\u6216\u7f51\u7edc\u8fde\u63a5\u3002"}}
{"id": "2508.11411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11411", "abs": "https://arxiv.org/abs/2508.11411", "authors": ["Fabian H. Reith", "Jannik Franzen", "Dinesh R. Palli", "J. Lorenz Rumberger", "Dagmar Kainmueller"], "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "comment": "8 pages, 3 figures. To appear in the proceedings of the BioImage\n  Computing (BIC) Workshop @ ICCVW 2025. This is the accepted author manuscript\n  (camera-ready version)", "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.", "AI": {"tldr": "SelfAdapt adapts pre-trained cell segmentation models like Cellpose to new domains without needing labeled data, using a student-teacher approach with novel regularization and stopping criteria. It improves performance and can even enhance models already fine-tuned with supervision.", "motivation": "While generalist models like Cellpose achieve state-of-the-art performance in biomedical instance segmentation, their effectiveness diminishes on domains different from their training data. Supervised fine-tuning can address this, but requires scarce annotated data. Therefore, there is a need for methods that can adapt these models without labels.", "method": "SelfAdapt builds upon student-teacher augmentation consistency training, incorporating L2-SP regularization and label-free stopping criteria to achieve unsupervised domain adaptation for pre-trained cell segmentation models.", "result": "On the LiveCell and TissueNet datasets, SelfAdapt showed relative improvements in AP0.5 of up to 29.64% over baseline Cellpose. The method also demonstrated the capability to further enhance models that had already undergone supervised fine-tuning.", "conclusion": "SelfAdapt enables the adaptation of pre-trained cell segmentation models without the need for labels, demonstrating significant improvements over baseline Cellpose and even outperforming supervised fine-tuning in some cases. It can also further enhance already fine-tuned models, and is released as an accessible extension to the Cellpose framework."}}
{"id": "2508.11419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11419", "abs": "https://arxiv.org/abs/2508.11419", "authors": ["Florian Bayer", "Maximilian Russo", "Christian Rathgeb"], "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems", "comment": null, "summary": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u878d\u5408\u591a\u79cd\u751f\u7269\u7279\u5f81\uff08\u9762\u90e8\u3001\u6307\u7eb9\u3001\u8679\u819c\uff09\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7684\u7279\u5f81\u6765\u51cf\u5c11\u751f\u7269\u7279\u5f81\u6a21\u677f\u5927\u5c0f\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9\u7279\u5f81\u5411\u91cf\u8fdb\u884c\u964d\u7ef4\uff0c\u53ef\u4ee5\u5728\u540c\u6001\u52a0\u5bc6\u57df\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5355\u4e00\u751f\u7269\u8bc6\u522b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6a21\u677f\u5927\u5c0f\u51cf\u5c0f 67%\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u7b49\u4e8e\u9519\u8bef\u7387 (EER)\u3002", "motivation": "\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7684\u666e\u53ca\u4f7f\u5f97\u63d0\u53d6\u7684\u6a21\u677f\u7684\u9690\u79c1\u548c\u5b89\u5168\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u7136\u800c\uff0c\u5229\u7528\u540c\u6001\u52a0\u5bc6 (HE) \u7684\u751f\u7269\u7279\u5f81\u6a21\u677f\u4fdd\u62a4\u65b9\u6848\u7531\u4e8e\u5de5\u4f5c\u91cf\u589e\u52a0\u800c\u5e26\u6765\u91cd\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u63d0\u53d6\u9762\u90e8\u3001\u6307\u7eb9\u548c\u8679\u819c\u7279\u5f81\uff0c\u5e76\u5728 FRGC\u3001MCYT \u548c CASIA \u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u8bc4\u4f30\u7684\u65b9\u6cd5\u5305\u62ec (i) \u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u5728\u52a0\u5bc6\u4e0b\u5b9e\u73b0\uff0c(ii) \u65e0\u9700\u8bad\u7ec3\uff0c\u4ee5\u53ca (iii) \u80fd\u591f\u6cdb\u5316\u3002\u5bf9\u7279\u5f81\u5411\u91cf\u8fdb\u884c\u964d\u7ef4\u3002", "result": "\u901a\u8fc7\u878d\u5408\u7279\u5f81\u5411\u91cf\uff0c\u6a21\u677f\u5927\u5c0f\u53ef\u51cf\u5c0f 67%\uff0c\u4e14\u4e0d\u5f71\u54cd\u7b49\u4e8e\u9519\u8bef\u7387 (EER)\uff0c\u5176\u6027\u80fd\u4e0e\u6027\u80fd\u6700\u4f73\u7684\u5355\u4e00\u6a21\u6001\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u6765\u81ea\u591a\u79cd\u751f\u7269\u7279\u5f81\u6a21\u6001\u7684\u7279\u5f81\u5411\u91cf\uff0c\u53ef\u4ee5\u5c06\u6a21\u677f\u5927\u5c0f\u51cf\u5c0f 67%\uff0c\u800c\u4e0d\u4f1a\u5728\u7b49\u4e8e\u9519\u8bef\u7387 (EER) \u65b9\u9762\u9020\u6210\u4efb\u4f55\u635f\u5931\uff0c\u4e0e\u6027\u80fd\u6700\u4f73\u7684\u5355\u4e00\u6a21\u6001\u76f8\u6bd4\u3002"}}
{"id": "2508.11499", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11499", "abs": "https://arxiv.org/abs/2508.11499", "authors": ["Erez Meoded"], "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models", "comment": null, "summary": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528TrOCR\u6a21\u578b\u548c\u521b\u65b0\u7684\u6570\u636e\u589e\u5f3a\u53ca\u96c6\u6210\u5b66\u4e60\u6280\u672f\uff0c\u5728\u5904\u740616\u4e16\u7eaa\u62c9\u4e01\u624b\u7a3f\u65f6\uff0c\u5c06\u5386\u53f2\u624b\u5199\u6587\u672c\u8bc6\u522b\u7684\u9519\u8bef\u7387\u964d\u4f4e\u4e8650%\uff0c\u662f\u5904\u7406\u5386\u53f2\u6587\u732e\u7684\u91cd\u5927\u8fdb\u5c55\u3002", "motivation": "\u5386\u53f2\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u5bf9\u4e8e\u53d1\u6398\u6863\u6848\u6587\u732e\u7684\u6587\u5316\u548c\u5b66\u672f\u4ef7\u503c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6570\u5b57\u5316\u8fdb\u7a0b\u53d7\u5230\u8f6c\u5f55\u7a00\u5c11\u3001\u8bed\u8a00\u53d8\u5f02\u548c\u591a\u6837\u5316\u624b\u5199\u98ce\u683c\u7684\u963b\u788d\u3002", "method": "\u672c\u7814\u7a76\u5c06\u6700\u5148\u8fdb\u7684\u57fa\u4e8eTransformer\u7684HTR\u6a21\u578bTrOCR\u5e94\u7528\u4e8e\u9c81\u9053\u592b\u00b7\u74dc\u5c14\u745f16\u4e16\u7eaa\u62c9\u4e01\u624b\u7a3f\u3002\u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u56fe\u50cf\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u7279\u522b\u662f\u5f15\u5165\u4e86\u56db\u79cd\u9488\u5bf9\u5386\u53f2\u624b\u5199\u7279\u5f81\u7684\u65b0\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5229\u7528\u96c6\u6210\u5b66\u4e60\u7b56\u7565\u6765\u7ed3\u5408\u4e0d\u540c\u589e\u5f3a\u6a21\u578b\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u74dc\u5c14\u745f\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f73\u7684\u5355\u4e00\u6a21\u578b\u589e\u5f3a\uff08Elastic\uff09\u5b9e\u73b0\u4e861.86\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u800c\u9876\u7ea7\u7684\u4e94\u6a21\u578b\u6295\u7968\u96c6\u6210\u8fbe\u5230\u4e861.60\u7684CER\uff0c\u8fd9\u76f8\u6bd4\u62a5\u544a\u7684\u6700\u4f73TrOCR_BASE\u7ed3\u679c\u670950%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5e76\u6bd4\u4e4b\u524d\u7684\u6280\u672f\u6c34\u5e73\u63d0\u9ad8\u4e8642%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5e94\u7528TrOCR\u6a21\u578b\u5e76\u7ed3\u5408\u5b9a\u5236\u5316\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u96c6\u6210\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5386\u53f2\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u572816\u4e16\u7eaa\u62c9\u4e01\u624b\u7a3f\u4e0a\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e861.60\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u6709\u5927\u5e45\u63d0\u5347\u3002"}}
{"id": "2508.11428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11428", "abs": "https://arxiv.org/abs/2508.11428", "authors": ["Jingyu Li", "Bozhou Zhang", "Xin Jin", "Jiankang Deng", "Xiatian Zhu", "Li Zhang"], "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving", "comment": null, "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.", "AI": {"tldr": "ImagiDrive\u6846\u67b6\u901a\u8fc7\u7ed3\u5408VLM\u7684\u9884\u6d4b\u80fd\u529b\u548cDWM\u7684\u573a\u666f\u751f\u6210\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65e9\u671f\u505c\u6b62\u548c\u8f68\u8ff9\u9009\u62e9\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u7cbe\u786e\u7684\u9884\u6d4b\u63a8\u7406\u80fd\u529b\u3002\u867d\u7136VLMs\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u884c\u4e3a\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0cDWMs\u5728\u751f\u6210\u8be6\u7ec6\u7684\u672a\u6765\u9a7e\u9a76\u573a\u666f\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u4ee5\u5229\u7528\u5176\u4e92\u8865\u4f18\u52bf\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u96c6\u6210\u5728\u8fde\u63a5\u52a8\u4f5c\u7ea7\u51b3\u7b56\u548c\u50cf\u7d20\u7ea7\u9884\u6d4b\u4ee5\u53ca\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aImagiDrive\u7684\u65b0\u578b\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\uff08DWM\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u5305\u62ec\u4e00\u4e2a\u7531VLM\u9a71\u52a8\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u7528\u4e8e\u6839\u636e\u591a\u6a21\u6001\u8f93\u5165\u9884\u6d4b\u521d\u59cb\u9a7e\u9a76\u8f68\u8ff9\uff1b\u4e00\u4e2a\u7531DWM\u9a71\u52a8\u7684\u573a\u666f\u60f3\u8c61\u5668\uff0c\u7528\u4e8e\u6839\u636e\u9a7e\u9a76\u4ee3\u7406\u7684\u9884\u6d4b\u751f\u6210\u672a\u6765\u573a\u666f\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u8fed\u4ee3\u4f18\u5316\u9a7e\u9a76\u4ee3\u7406\u89c4\u5212\u51b3\u7b56\u7684\u60f3\u8c61-\u89c4\u5212\u5faa\u73af\u3002\u4e3a\u4e86\u89e3\u51b3\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u8fd8\u5f15\u5165\u4e86\u65e9\u671f\u505c\u6b62\u673a\u5236\u548c\u8f68\u8ff9\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728nuScenes\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660eImagiDrive\u5728\u5f00\u653e\u548c\u95ed\u73af\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "ImagiDrive\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u57fa\u4e8eVLM\u7684\u9a7e\u9a76\u4ee3\u7406\u548c\u57fa\u4e8eDWM\u7684\u573a\u666f\u60f3\u8c61\u5668\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u60f3\u8c61-\u89c4\u5212\u5faa\u73af\uff0c\u5e76\u5728nuScenes\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f00\u653e\u548c\u95ed\u73af\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.11431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11431", "abs": "https://arxiv.org/abs/2508.11431", "authors": ["Simona Kocour", "Assia Benbihi", "Torsten Sattler"], "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574", "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f303D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7269\u4f53\u79fb\u9664\u540e\u8bed\u4e49\u6b8b\u7559\u7684\u65b0\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff08Remove360\uff09\uff0c\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7406\u89e3\u7269\u4f53\u79fb\u9664\u540e\u8bed\u4e49\u4fe1\u606f\u7684\u6301\u4e45\u6027\u5bf9\u4e8e\u4fdd\u62a4\u9690\u79c1\u76843D\u91cd\u5efa\u548c\u53ef\u7f16\u8f91\u573a\u666f\u8868\u793a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf3D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7269\u4f53\u79fb\u9664\u540e\u7684\u8bed\u4e49\u6b8b\u7559\uff0c\u5e76\u8fdb\u884c\u4e86\u5ba4\u5185\u5916\u573a\u666f\u7684\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u89c6\u89c9\u51e0\u4f55\u7684\u60c5\u51b5\u4e0b\uff0c\u5f53\u524d\u65b9\u6cd5\u4e5f\u80fd\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u7a81\u663e\u4e86\u73b0\u67093D\u7269\u4f53\u79fb\u9664\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d3D\u7269\u4f53\u79fb\u9664\u6280\u672f\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u9650\u5236\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u8bed\u4e49\u6b8b\u7559\u95ee\u9898\u3002"}}
{"id": "2508.11433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "AI": {"tldr": "MM-R1\u6846\u67b6\u901a\u8fc7X-CoT\u548cGRPO\uff0c\u4f7f\u7edf\u4e00\u7684MLLMs\u80fd\u591f\u96f6\u6837\u672c\u5730\u8fdb\u884c\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u5bf9\u9f50\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u662f\u7279\u5b9a\u4e3b\u9898\u7684\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u4e3b\u9898\u8fdb\u884c\u6570\u636e\u5bc6\u96c6\u578b\u5fae\u8c03\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "MM-R1\u6846\u67b6\u96c6\u6210\u4e86\u8de8\u6a21\u6001\u7684\u601d\u7ef4\u94fe\uff08X-CoT\uff09\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5c06\u4e2a\u6027\u5316\u8fc7\u7a0b\u6784\u5efa\u4e3a\u96c6\u6210\u7684\u89c6\u89c9\u63a8\u7406\u548c\u751f\u6210\u8fc7\u7a0b\uff1a1.\u901a\u8fc7\u89e3\u91ca\u548c\u7406\u89e3\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u53ca\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6765\u8bc6\u522b\u4e3b\u9898\u6982\u5ff5\uff1b2.\u57fa\u4e8e\u63d0\u53d6\u7684\u4e3b\u9898\u8868\u793a\u548c\u7528\u6237\u63d0\u793a\u751f\u6210\u4e2a\u6027\u5316\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u91c7\u7528\u5206\u7ec4\u5956\u52b1\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6765\u5bf9\u9f50\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMM-R1\u6846\u67b6\u80fd\u591f\u5728\u96f6\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u5177\u6709\u9ad8\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u5f3a\u6587\u672c\u5bf9\u9f50\u6027\u7684\u4e2a\u6027\u5316\u56fe\u50cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MM-R1\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u8de8\u6a21\u6001\u7684\u601d\u7ef4\u94fe\uff08X-CoT\uff09\u63a8\u7406\u7b56\u7565\uff0c\u6210\u529f\u5730\u91ca\u653e\u4e86\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u8fbe\u5230\u4e86\u9ad8\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u5f3a\u6587\u672c\u5bf9\u9f50\u6027\u3002"}}
{"id": "2508.11464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11464", "abs": "https://arxiv.org/abs/2508.11464", "authors": ["Xiaoya Zhu", "Yibing Nan", "Shiguo Lian"], "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge", "comment": null, "summary": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.", "AI": {"tldr": "A Swin Transformer V2-B based model with data augmentation and sample generation techniques achieved excellence in detecting Deepfakes.", "motivation": "The motivation is to address the challenges posed by deepfake technology to digital security by developing a method to detect whether a face image is a Deepfake image and output its probability score.", "method": "The method is based on the Swin Transformer V2-B classification network, utilizing online data augmentation and offline sample generation to enhance training samples and model generalization.", "result": "The approach achieved an award of excellence in the Deepfake image detection competition.", "conclusion": "The approach achieved excellence in Deepfake image detection using Swin Transformer V2-B, online data augmentation, and offline sample generation, enriching training data diversity and improving model generalization."}}
{"id": "2508.11469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11469", "abs": "https://arxiv.org/abs/2508.11469", "authors": ["Hongjin Fang", "Daniel Reisenb\u00fcchler", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation", "comment": null, "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.", "AI": {"tldr": "CoFi\u662f\u4e00\u4e2a\u7528\u4e8eEM\u56fe\u50cfGBM\u5206\u5272\u7684\u5c11\u6837\u672c\u5206\u5272\u6d41\u6c34\u7ebf\uff0c\u5b83\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u6807\u6ce8\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\u3002", "motivation": "\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728GBM\u5206\u5272\u4e2d\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u9700\u8981\u5927\u91cf\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u8fd9\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u4ee5\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u6355\u6349GBM\u5206\u6790\u6240\u9700\u7684\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u3002", "method": "CoFi\u662f\u4e00\u4e2a\u5feb\u901f\u9ad8\u6548\u7684\u7c97\u5230\u7ec6\u5c11\u6837\u672c\u5206\u5272\u6d41\u7a0b\uff0c\u7528\u4e8eEM\u56fe\u50cf\u4e2d\u7684GBM\u63cf\u7ed8\u3002\u5b83\u9996\u5148\u4f7f\u7528\u4e09\u4e2a\u5e26\u6ce8\u91ca\u7684\u56fe\u50cf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u4ea7\u751f\u521d\u59cb\u7c97\u5206\u5272\u63a9\u6a21\u3002\u7136\u540e\uff0c\u8be5\u63a9\u6a21\u7ecf\u8fc7\u81ea\u52a8\u5904\u7406\uff0c\u751f\u6210\u5177\u6709\u5f62\u6001\u611f\u77e5\u4fee\u526a\u7684\u9ad8\u8d28\u91cf\u70b9\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u968f\u540e\u7528\u4e8e\u6307\u5bfcSAM\u8fdb\u884c\u5206\u5272\u4f18\u5316\u3002", "result": "CoFi\u5b9e\u73b0\u4e86\u51fa\u8272\u7684GBM\u5206\u5272\u6027\u80fd\uff0cDice\u7cfb\u6570\u4e3a74.54%\uff0c\u63a8\u7406\u901f\u5ea6\u4e3a1.9 FPS\u3002", "conclusion": "CoFi\u901a\u8fc7\u51cf\u8f7b\u6ce8\u91ca\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5728EM\u56fe\u50cf\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u53ef\u9760\u7684GBM\u5206\u5272\uff0c\u5728\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u90fd\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.11478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11478", "abs": "https://arxiv.org/abs/2508.11478", "authors": ["Xinyi Yin", "Wenbo Yuan", "Xuecheng Wu", "Liangyu Fu", "Danlei Huang"], "title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations", "comment": "8 pages, 4 figures, accepted by IJCNN 2025", "summary": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.", "AI": {"tldr": "TACR-YOLO\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4f18\u5316\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5c0f\u76ee\u6807\u3001\u5206\u7c7b\u56de\u5f52\u51b2\u7a81\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728PABD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3YOLO\u7c7b\u68c0\u6d4b\u65b9\u6cd5\u5728\u7279\u6b8a\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u4eba\u7c7b\u884c\u4e3a\u68c0\u6d4b\uff08AHBD\uff09\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u5c0f\u76ee\u6807\u3001\u4efb\u52a1\u51b2\u7a81\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTACR-YOLO\u7684\u65b0\u578b\u5b9e\u65f6\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u5750\u6807\u6ce8\u610f\u529b\u6a21\u5757\uff08\u7528\u4e8e\u589e\u5f3a\u5c0f\u76ee\u6807\u68c0\u6d4b\uff09\u3001\u4efb\u52a1\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff08\u7528\u4e8e\u5904\u7406\u5206\u7c7b-\u56de\u5f52\u51b2\u7a81\uff09\u548c\u52a0\u5f3a\u9888\u90e8\u7f51\u7edc\uff08\u7528\u4e8e\u7cbe\u7ec6\u5316\u591a\u5c3a\u5ea6\u878d\u5408\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528K-means\u805a\u7c7b\u4f18\u5316\u4e86Anchor Box\u5c3a\u5bf8\uff0c\u5e76\u90e8\u7f72\u4e86DIoU-Loss\u6765\u6539\u8fdb\u8fb9\u754c\u6846\u56de\u5f52\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5305\u542b8,529\u4e2a\u6837\u672c\uff08\u6db5\u76d6\u56db\u79cd\u884c\u4e3a\u7c7b\u522b\uff09\u7684\u4eba\u5458\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\uff08PABD\uff09\u6570\u636e\u96c6\u3002", "result": "TACR-YOLO\u5728PABD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.92%\u7684mAP\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u6539\u8fdb\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "TACR-YOLO\u5728PABD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.92%\u7684mAP\uff0c\u5e76\u4e14\u5177\u6709\u7ade\u4e89\u529b \u7684\u901f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7279\u6b8a\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.11482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11482", "abs": "https://arxiv.org/abs/2508.11482", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Jiannan Cai", "Kaijian Liu", "Yuansheng Zhu", "Pingbo Tang", "Nora El-Gohary"], "title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "comment": null, "summary": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u5efa\u7b51\u884c\u4e1a51\u4e2a\u516c\u5f00\u89c6\u89c9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5ba1\u67e5\u548c\u5206\u7c7b\uff0c\u6784\u5efa\u4e86OpenConstruction\u76ee\u5f55\uff0c\u5e76\u63d0\u51fa\u4e86\u9075\u5faaFAIR\u539f\u5219\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u4fc3\u8fdb\u5efa\u7b51\u9886\u57dfAI\u5e94\u7528\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u5efa\u7b51\u9886\u57df\u89c6\u89c9\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u6a21\u6001\u3001\u6807\u6ce8\u8d28\u91cf\u548c\u4ee3\u8868\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7f3a\u4e4f\u5bf9\u5176\u6570\u636e\u7279\u6027\u548c\u5e94\u7528\u80cc\u666f\u7684\u7cfb\u7edf\u6027\u5ba1\u67e5\uff0c\u8fd9\u963b\u788d\u4e86\u793e\u533a\u5145\u5206\u7406\u89e3\u6570\u636e\u96c6\u73b0\u72b6\u3001\u8bc6\u522b\u5173\u952e\u5dee\u8ddd\u5e76\u6307\u5bfc\u672a\u6765AI\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u641c\u7d22\u5b66\u672f\u6570\u636e\u5e93\u548c\u5f00\u653e\u6570\u636e\u5e73\u53f0\uff0c\u5bf92005-2024\u5e74\u95f4\u768451\u4e2a\u516c\u5f00\u89c6\u89c9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5ba1\u67e5\u548c\u5206\u7c7b\u3002\u5206\u7c7b\u7ef4\u5ea6\u5305\u62ec\u6570\u636e\u57fa\u672c\u4fe1\u606f\uff08\u5927\u5c0f\u3001\u8bb8\u53ef\uff09\u3001\u6570\u636e\u6a21\u6001\uff08RGB\u3001\u70b9\u4e91\uff09\u3001\u6807\u6ce8\u6846\u67b6\uff08\u8fb9\u754c\u6846\uff09\u4ee5\u53ca\u4e0b\u6e38\u5e94\u7528\u9886\u57df\uff08\u8fdb\u5ea6\u8ddf\u8e2a\uff09\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aOpenConstruction\u7684\u5f00\u6e90\u76ee\u5f55\uff0c\u5bf951\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u6c47\u603b\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u652f\u6301\u3002\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u73b0\u6709\u5efa\u7b51\u6570\u636e\u96c6\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eFAIR\u539f\u5219\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u672a\u6765\u8def\u7ebf\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf951\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u641c\u7d22\u548c\u5206\u7c7b\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u5efa\u7b51\u9886\u57df\u89c6\u89c9\u6570\u636e\u96c6\u7684\u7279\u70b9\u548c\u5e94\u7528\u80cc\u666f\uff0c\u53d1\u73b0\u4e86\u5f53\u524d\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9075\u5faaFAIR\u539f\u5219\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u672a\u6765\u8def\u7ebf\u56fe\uff0c\u4ee5\u652f\u6301\u5efa\u7b51\u884c\u4e1a\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.11628", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11628", "abs": "https://arxiv.org/abs/2508.11628", "authors": ["Qiang Li", "Shansong Wang", "Mingzhe Hu", "Mojtaba Safari", "Zachary Eidex", "Xiaofeng Yang"], "title": "Is ChatGPT-5 Ready for Mammogram VQA?", "comment": null, "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.", "AI": {"tldr": "GPT-5\u5728\u4e73\u817aX\u7ebf\u7167\u7247VQA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e34\u5e8a\u5e94\u7528\u5c1a\u9700\u4f18\u5316\uff0c\u5176\u6027\u80fd\u4e0d\u5982\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30GPT-5\u7cfb\u5217\u548cGPT-4o\u5728\u4e73\u817aX\u7ebf\u7167\u7247VQA\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u4e73\u817a\u764c\u7b5b\u67e5\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86GPT-5\u7cfb\u5217\u548cGPT-4o\u6a21\u578b\u5728EMBED\u3001InBreast\u3001CMMD\u3001CBIS-DDSM\u56db\u4e2a\u516c\u5171\u4e73\u817aX\u7ebf\u7167\u7247\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4efb\u52a1\u5305\u62ecBI-RADS\u8bc4\u4f30\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u6076\u6027\u80bf\u7624\u5206\u7c7b\u3002", "result": "GPT-5\u5728\u6240\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e2d\u5747\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u5bc6\u5ea6\u3001\u626d\u66f2\u3001\u80bf\u5757\u3001\u9499\u5316\u548c\u6076\u6027\u80bf\u7624\u5206\u7c7b\u65b9\u9762\uff0c\u4ee5\u53caBI-RADS\u51c6\u786e\u6027\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u6076\u6027\u80bf\u7624\u5206\u7c7b\u65b9\u9762\uff0c\u5747\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u3002GPT-5\u7684\u654f\u611f\u6027\uff0863.5%\uff09\u548c\u7279\u5f02\u6027\uff0852.3%\uff09\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "GPT-5\u5728\u4e73\u817aX\u7ebf\u7167\u7247VQA\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u4e34\u5e8a\u5e94\u7528\u524d\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u5176\u6027\u80fd\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u3002"}}
{"id": "2508.11484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11484", "abs": "https://arxiv.org/abs/2508.11484", "authors": ["Xiaoxue Wu", "Bingjie Gao", "Yu Qiao", "Yaohui Wang", "Xinyuan Chen"], "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "comment": "27 pages, 20 figures", "summary": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.", "AI": {"tldr": "CineTrans\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u5177\u6709\u7535\u5f71\u822c\u8f6c\u573a\u7684\u591a\u955c\u5934\u89c6\u9891\u3002\u5b83\u4f7f\u7528\u63a9\u7801\u63a7\u5236\u5728\u4efb\u610f\u70b9\u8fdb\u884c\u8f6c\u573a\uff0c\u5e76\u5728Cine250K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u96c6\u5177\u6709\u955c\u5934\u6807\u6ce8\u3002CineTrans\u5728\u8f6c\u573a\u63a7\u5236\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5408\u6210\u6280\u672f\u5728\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u65b9\u9762\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u5373\u4f7f\u4f7f\u7528\u6269\u5c55\u7684\u6a21\u578b\u548c\u6d77\u91cf\u6570\u636e\u96c6\uff0c\u955c\u5934\u8f6c\u6362\u80fd\u529b\u4e5f\u663e\u5f97\u7c97\u7cd9\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u89c6\u9891\u5927\u591a\u5c40\u9650\u4e8e\u5355\u955c\u5934\u5e8f\u5217\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a2\u7d22\u751f\u6210\u5177\u6709\u7535\u5f71\u5316\u3001\u98ce\u683c\u5316\u955c\u5934\u8f6c\u6362\u7684\u591a\u955c\u5934\u89c6\u9891\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCineTrans\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7535\u5f71\u98ce\u683c\u8f6c\u6362\u7684\u591a\u955c\u5934\u89c6\u9891\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u56fe\u4e0e\u955c\u5934\u8fb9\u754c\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u63a7\u5236\u673a\u5236\u3002\u8be5\u673a\u5236\u5141\u8bb8\u5728\u4efb\u610f\u4f4d\u7f6e\u8fdb\u884c\u955c\u5934\u8f6c\u6362\uff0c\u5e76\u4e14\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8be6\u7ec6\u955c\u5934\u6807\u6ce8\u7684\u591a\u955c\u5934\u89c6\u9891-\u6587\u672c\u6570\u636e\u96c6Cine250K\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u548c\u5206\u6790\u3002", "result": "CineTrans\u5728Cine250K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408\u7535\u5f71\u526a\u8f91\u98ce\u683c\u7684\u3001\u8fde\u8d2f\u7684\u591a\u955c\u5934\u5e8f\u5217\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u8f6c\u6362\u6216\u7b80\u5355\u7684\u62fc\u63a5\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u63d0\u51fa\u4e86\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u8f6c\u6362\u63a7\u5236\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6574\u4f53\u8d28\u91cf\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCineTrans\u5728\u6240\u6709\u6807\u51c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CineTrans\u6846\u67b6\u80fd\u591f\u751f\u6210\u7b26\u5408\u7535\u5f71\u526a\u8f91\u98ce\u683c\u7684\u3001\u8fde\u8d2f\u7684\u591a\u955c\u5934\u89c6\u9891\uff0c\u514b\u670d\u4e86\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u955c\u5934\u8f6c\u6362\u65b9\u9762\u5b58\u5728\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5728Cine250K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u5e76\u7ed3\u5408\u63a9\u7801\u63a7\u5236\u673a\u5236\uff0cCineTrans\u80fd\u591f\u5728\u4efb\u610f\u4f4d\u7f6e\u5b9e\u73b0\u955c\u5934\u8f6c\u6362\uff0c\u5e76\u4e14\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCineTrans\u5728\u8f6c\u6362\u63a7\u5236\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.11486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11486", "abs": "https://arxiv.org/abs/2508.11486", "authors": ["Kristina Dabrock", "Tim Johansson", "Anna Donarelli", "Mikael Mangold", "Noah Pflugradt", "Jann Michael Weinand", "Jochen Lin\u00dfen"], "title": "Automated Building Heritage Assessment Using Street-Level Imagery", "comment": null, "summary": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528GPT\u548c\u673a\u5668\u5b66\u4e60\u8bc6\u522b\u5efa\u7b51\u6587\u5316\u9057\u4ea7\u4ef7\u503c\uff0c\u4ee5\u652f\u6301\u8282\u80fd\u6539\u9020\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u635f\u5bb3\u6587\u5316\u9057\u4ea7\u7684\u60c5\u51b5\u4e0b\u91cf\u5316\u5efa\u7b51\uff08\u5982\u56f4\u62a4\u7ed3\u6784\u6539\u9020\uff09\u7684\u8282\u80fd\u63aa\u65bd\uff0c\u9700\u8981\u8be6\u7ec6\u7684\u6570\u636e\u3002\u65b0\u578b\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u53ef\u80fd\u6bd4\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u7684\u4eba\u5de5\u6e05\u5355\u66f4\u6709\u6548\u5730\u8bc6\u522b\u5efa\u7b51\u4e2d\u7684\u9057\u4ea7\u4ef7\u503c\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578bGPT\u6765\u68c0\u6d4b\u95e8\u9762\u56fe\u50cf\u4e2d\u7684\u5404\u79cd\u6587\u5316\u9057\u4ea7\u4ef7\u503c\u65b9\u9762\uff0c\u5e76\u7ed3\u5408\u5efa\u7b51\u767b\u8bb0\u6570\u636e\u4f5c\u4e3a\u7279\u5f81\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5bf9\u745e\u5178\u65af\u5fb7\u54e5\u5c14\u6469\u7684\u591a\u6237\u4f4f\u5b85\u548c\u975e\u4f4f\u5b85\u5efa\u7b51\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u4f7f\u7528GPT\u68c0\u7d22\u7684\u7279\u5f81\u548c\u767b\u8bb0\u6570\u636e\u76f8\u7ed3\u5408\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4e0e\u4e13\u5bb6\u521b\u5efa\u7684\u6e05\u5355\u8fdb\u884c\u9a8c\u8bc1\u65f6\uff0c\u5b8f\u89c2F1\u5206\u6570\u4e3a0.71\uff1b\u4ec5\u4f7f\u7528GPT\u68c0\u7d22\u7684\u6570\u636e\uff0c\u5f97\u5206\u4e3a0.60\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u5e93\u7684\u8d28\u91cf\u3002", "conclusion": "\u5229\u7528GPT\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u8bc6\u522b\u5efa\u7b51\u7684\u6587\u5316\u9057\u4ea7\u4ef7\u503c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7684\u5efa\u7b51\u8282\u80fd\u6539\u9020\u4e2d\uff0c\u4ee5\u652f\u6301\u8c28\u614e\u7684\u8282\u80fd\u63aa\u65bd\u548c\u9057\u4ea7\u4ef7\u503c\u7684\u6574\u5408\u8003\u8651\u3002"}}
{"id": "2508.11488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11488", "abs": "https://arxiv.org/abs/2508.11488", "authors": ["Bozhou Zhang", "Jingyu Li", "Nan Song", "Li Zhang"], "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.", "AI": {"tldr": "VeteranAD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u611f\u77e5\u96c6\u6210\u5230\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001\u951a\u5b9a\u8f68\u8ff9\u4f5c\u4e3a\u89c4\u5212\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u7684\u9a7e\u9a76\u3002\u5728NAVSIM\u548cBench2Drive\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u89c4\u5212\u6027\u80fd\uff0c\u5c06\u611f\u77e5\u96c6\u6210\u5230\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u7531\u4e0d\u65ad\u53d8\u5316\u7684\u89c4\u5212\u76ee\u6807\u9a71\u52a8\u7684\u9488\u5bf9\u6027\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVeteranAD\u7684\u611f\u77e5\u4e0e\u89c4\u5212\u8026\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u951a\u5b9a\u8f68\u8ff9\u4f5c\u4e3a\u89c4\u5212\u5148\u9a8c\uff0c\u4f7f\u611f\u77e5\u6a21\u5757\u80fd\u591f\u6536\u96c6\u8fd9\u4e9b\u8f68\u8ff9\u4e0a\u7684\u4ea4\u901a\u5143\u7d20\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u611f\u77e5\u3002\u7136\u540e\uff0c\u57fa\u4e8e\u611f\u77e5\u7ed3\u679c\u548c\u89c4\u5212\u5148\u9a8c\u751f\u6210\u89c4\u5212\u8f68\u8ff9\u3002\u91c7\u7528\u81ea\u56de\u5f52\u7b56\u7565\u9010\u6b65\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u4e13\u6ce8\u4e8e\u76f8\u5173\u533a\u57df\u8fdb\u884c\u76ee\u6807\u611f\u77e5\u3002", "result": "VeteranAD\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "VeteranAD\u5728NAVSIM\u548cBench2Drive\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11497", "abs": "https://arxiv.org/abs/2508.11497", "authors": ["Feiyue Zhao", "Zhichao Zhang"], "title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition", "comment": null, "summary": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.", "AI": {"tldr": "HGFE\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u56fe\u5377\u79ef\u548c\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\uff0c\u589e\u5f3a\u4e86CNN\u5904\u7406\u56fe\u50cf\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u5176\u56fa\u6709\u7684\u5bf9\u89c4\u5219\u7f51\u683c\u7ed3\u6784\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5b83\u4eec\u5728\u56fe\u50cf\u5185\u5efa\u6a21\u590d\u6742\u62d3\u6251\u5173\u7cfb\u548c\u975e\u5c40\u90e8\u8bed\u4e49\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u56fe\u7279\u5f81\u589e\u5f3a\uff08HGFE\uff09\u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u57fa\u4e8e\u56fe\u7684\u63a8\u7406\u96c6\u6210\u5230CNN\u4e2d\uff0c\u4ee5\u589e\u5f3a\u7ed3\u6784\u611f\u77e5\u548c\u7279\u5f81\u8868\u793a\u3002HGFE\u6784\u5efa\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u56fe\u7ed3\u6784\u5c42\uff1a\u7a97\u53e3\u5185\u56fe\u5377\u79ef\u7528\u4e8e\u6355\u83b7\u5c40\u90e8\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u7a97\u53e3\u95f4\u8d85\u7ea7\u8282\u70b9\u4ea4\u4e92\u7528\u4e8e\u6a21\u62df\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u6a21\u5757\uff0c\u52a8\u6001\u5e73\u8861\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u53f7\u4f20\u64ad\uff0c\u4ee5\u4fdd\u7559\u5173\u952e\u7684\u8fb9\u7f18\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u540c\u65f6\u51cf\u8f7b\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "HGFE\u5728CIFAR-100\u3001PASCAL VOC\u3001VisDrone\u3001CrackSeg\u548cCarParts\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7ed3\u6784\u8868\u793a\u80fd\u529b\u548c\u6574\u4f53\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "HGFE\u6a21\u5757\u8f7b\u91cf\u7ea7\u3001\u53ef\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u7684CNN\u9aa8\u5e72\u7f51\u7edc\u4e2d\u3002\u5728CIFAR-100\uff08\u5206\u7c7b\uff09\u3001PASCAL VOC\u548cVisDrone\uff08\u68c0\u6d4b\uff09\u4ee5\u53caCrackSeg\u548cCarParts\uff08\u5206\u5272\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HGFE\u5728\u6539\u8fdb\u7ed3\u6784\u8868\u793a\u548c\u589e\u5f3a\u6574\u4f53\u8bc6\u522b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11532", "abs": "https://arxiv.org/abs/2508.11532", "authors": ["Jingsong Xia", "Yue Yin", "Xiuhan Li"], "title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture", "comment": null, "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ConvNeXt-Tiny\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5168\u5c40\u6c60\u5316\u3001SEVector\u6ce8\u610f\u529b\u6a21\u5757\u548c\u7279\u5f81\u5e73\u6ed1\u635f\u5931\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbConvNeXt-Tiny\u67b6\u6784\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5177\u4f53\u5305\u62ec\uff1a\u5f15\u5165\u53cc\u5168\u5c40\u6c60\u5316\uff08\u5168\u5c40\u5e73\u5747\u6c60\u5316\u548c\u5168\u5c40\u6700\u5927\u6c60\u5316\uff09\u7279\u5f81\u878d\u5408\u7b56\u7565\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757SEVector\u4ee5\u63d0\u9ad8\u901a\u9053\u6743\u91cd\u5206\u914d\u9002\u5e94\u6027\u5e76\u6700\u5c0f\u5316\u53c2\u6570\u5f00\u9500\uff1b\u91c7\u7528\u7279\u5f81\u5e73\u6ed1\u635f\u5931\u51fd\u6570\u4ee5\u589e\u5f3a\u7c7b\u5185\u7279\u5f81\u4e00\u81f4\u6027\u5e76\u6291\u5236\u7c7b\u5185\u65b9\u5dee\u3002", "result": "\u5728\u4ec5CPU\uff088\u7ebf\u7a0b\uff09\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u572810\u4e2a\u8bad\u7ec3\u5468\u671f\u5185\u5b9e\u73b0\u4e8689.10%\u7684\u6700\u5927\u6d4b\u8bd5\u96c6\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u635f\u5931\u503c\u5448\u73b0\u51fa\u7a33\u5b9a\u7684\u6536\u655b\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11502", "abs": "https://arxiv.org/abs/2508.11502", "authors": ["Eyad Alshami", "Shashank Agnihotri", "Bernt Schiele", "Margret Keuper"], "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025", "summary": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.", "AI": {"tldr": "AIM \u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u63a9\u853d\u6765\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u4fc3\u4f7f\u6a21\u578b\u5173\u6ce8\u771f\u5b9e\u7279\u5f81\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u8fdb\u884c\u51b3\u7b56\u65f6\uff0c\u5e38\u5e38\u4f1a\u540c\u65f6\u4f7f\u7528\u771f\u5b9e\u7279\u5f81\u548c\u865a\u5047\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u4fc3\u4f7f\u7f51\u7edc\u4f18\u5148\u4f7f\u7528\u771f\u5b9e\u7279\u5f81\u3001\u6291\u5236\u865a\u5047\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5185\u5728\u53ef\u89e3\u91ca\u6027\u3002", "method": "AIM\uff08Amending Inherent Interpretability via Self-Supervised Masking\uff09\u65b9\u6cd5\u5229\u7528\u591a\u7f16\u7801\u9636\u6bb5\u7684\u7279\u5f81\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u3001\u6837\u672c\u7279\u5b9a\u7684\u7279\u5f81\u63a9\u853d\u8fc7\u7a0b\uff0c\u5f15\u5bfc\u7f51\u7edc\u4f18\u5148\u4f7f\u7528\u771f\u5b9e\u7279\u5f81\u3002", "result": "AIM \u5728 ImageNet100\u3001HardImageNet\u3001ImageWoof\u3001Waterbirds\u3001TravelingBirds \u548c CUB-200 \u7b49\u591a\u6837\u5316\u7684\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAIM \u5728\u53ef\u89e3\u91ca\u6027\uff08\u901a\u8fc7\u80fd\u91cf\u6307\u5411\u6e38\u620f EPG \u5206\u6570\u8861\u91cf\uff09\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u9886\u57df\u548c\u67b6\u6784\u4e0a\u5747\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u63d0\u5347\u3002", "conclusion": "AIM \u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u4f7f\u7528\u771f\u5b9e\u7279\u5f81\u800c\u975e\u865a\u5047\u7279\u5f81\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u52bf\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u4eba\u7c7b\u53ef\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11517", "abs": "https://arxiv.org/abs/2508.11517", "authors": ["Shaoze Huang", "Qi Liu", "Chao Chen", "Yuhang Chen"], "title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11", "comment": null, "summary": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.", "AI": {"tldr": "\u9488\u5bf9\u957f\u6c5f\u4e09\u89d2\u6d32\u5730\u533a\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u8001\u5316\u53ca\u88c2\u7f1d\u68c0\u6d4b\u96be\u9898\uff0c\u63d0\u51fa YOLOv11-KW-TA-FP \u6a21\u578b\uff0c\u7ed3\u5408 KWConv\u3001TA \u548c FP-IoU \u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u88c2\u7f1d\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5177\u6709\u91cd\u8981\u7684\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u624b\u52a8\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u80cc\u666f\u4e0b\u5c0f\u76ee\u6807\u88c2\u7f1d\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u5e94\u5bf9\u957f\u6c5f\u4e09\u89d2\u6d32\u5730\u533a\u5feb\u901f\u53d1\u5c55\u80cc\u666f\u4e0b\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u52a0\u901f\u8001\u5316\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e YOLOv11n \u67b6\u6784\u7684\u591a\u4efb\u52a1\u6df7\u51dd\u571f\u88c2\u7f1d\u68c0\u6d4b\u548c\u5206\u5272\u6a21\u578b YOLOv11-KW-TA-FP\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1.\u5728\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5d4c\u5165\u52a8\u6001\u6838\u4ed3\u5e93\u5377\u79ef\uff08KWConv\uff09\u4ee5\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff1b2.\u5728\u7279\u5f81\u91d1\u5b57\u5854\u4e2d\u5f15\u5165\u4e09\u6ce8\u610f\u529b\u673a\u5236\uff08TA\uff09\u4ee5\u52a0\u5f3a\u901a\u9053-\u7a7a\u95f4\u4ea4\u4e92\u5efa\u6a21\uff1b3.\u8bbe\u8ba1 FP-IoU \u635f\u5931\u51fd\u6570\u4ee5\u4fc3\u8fdb\u81ea\u9002\u5e94\u8fb9\u754c\u6846\u56de\u5f52\u60e9\u7f5a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u589e\u5f3a\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c mAP@50 \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5206\u522b\u8fbe\u5230\u4e86 91.3%\u300176.6% \u548c 86.4%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u6a21\u5757\u7684\u534f\u540c\u529f\u6548\uff0c\u9c81\u68d2\u6027\u6d4b\u8bd5\u8868\u660e\u6a21\u578b\u5728\u6570\u636e\u7a00\u758f\u548c\u566a\u58f0\u5e72\u6270\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a YOLOv11-KW-TA-FP \u7684\u591a\u4efb\u52a1\u6df7\u51dd\u571f\u88c2\u7f1d\u68c0\u6d4b\u548c\u5206\u5272\u6a21\u578b\uff0c\u57fa\u4e8e YOLOv11n \u67b6\u6784\uff0c\u901a\u8fc7\u96c6\u6210\u52a8\u6001\u6838\u4ed3\u5e93\u5377\u79ef\uff08KWConv\uff09\u3001\u4e09\u6ce8\u610f\u529b\u673a\u5236\uff08TA\uff09\u548c FP-IoU \u635f\u5931\u51fd\u6570\uff0c\u5728\u88c2\u7f1d\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5de5\u7a0b\u4ef7\u503c\u3002"}}
{"id": "2508.11531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11531", "abs": "https://arxiv.org/abs/2508.11531", "authors": ["Shilei Wang", "Gong Cheng", "Pujian Lai", "Dong Gao", "Junwei Han"], "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction", "comment": null, "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.", "AI": {"tldr": "MST\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u591a\u72b6\u6001\u7279\u5f81\u589e\u5f3a\u548c\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u9ad8\u6548\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u901f\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u9ad8\u6548\u8ddf\u8e2a\u5668\u5728\u8ffd\u6c42\u901f\u5ea6\u7684\u540c\u65f6\u727a\u7272\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u6355\u6349\u76ee\u6807\u72b6\u6001\u7684\u95ee\u9898\uff0cMST\u88ab\u63d0\u51fa\u6765\u63d0\u9ad8\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u3002", "method": "MST\uff08Multi-State Tracker\uff09\u901a\u8fc7\u591a\u9636\u6bb5\u751f\u6210\uff08MSG\uff09\u4ea7\u751f\u591a\u72b6\u6001\u7279\u5f81\uff0c\u7136\u540e\u5229\u7528\u72b6\u6001\u589e\u5f3a\uff08SSE\uff09\u548c\u8de8\u72b6\u6001\u4ea4\u4e92\uff08CSI\uff09\u8fdb\u884c\u4f18\u5316\u548c\u805a\u5408\u3002SSE\u548cCSI\u6a21\u5757\u91c7\u7528\u4e86\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u9690\u85cf\u72b6\u6001\u81ea\u9002\u5e94\u7684\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\uff08HSA-SSD\uff09\u8bbe\u8ba1\uff0c\u8ba1\u7b97\u91cf\u4ec5\u4e3a0.1 GFLOPs\uff0c\u53c2\u6570\u91cf\u4e3a0.66M\u3002", "result": "MST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u5148\u524d\u7684\u9ad8\u6548\u8ddf\u8e2a\u5668\uff0c\u7279\u522b\u662f\u5728GOT-10K\u6570\u636e\u96c6\u4e0a\uff0cAO\u5f97\u5206\u6bd4HCAT\u63d0\u9ad8\u4e864.5%\uff0c\u5c55\u793a\u4e86\u5176\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "MST\u901a\u8fc7\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u72b6\u6001\u589e\u5f3a\u548c\u8de8\u72b6\u6001\u4ea4\u4e92\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u5148\u524d\u7684\u9ad8\u6548\u8ddf\u8e2a\u5668\uff0c\u5e76\u5728GOT-10K\u6570\u636e\u96c6\u4e0a\u6bd4\u4e4b\u524d\u7684SOTA\u9ad8\u6548\u8ddf\u8e2a\u5668HCAT\u53d6\u5f97\u4e864.5%\u7684AO\u5f97\u5206\u63d0\u5347\u3002"}}
{"id": "2508.11538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "AI": {"tldr": "Veason-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u548c\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u521d\u59cb\u5316\u6539\u8fdb\u4e86\u89c6\u9891\u63a8\u7406\u5206\u5272\uff08VRS\uff09\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5148\u524d\u7684\u89c6\u9891\u63a8\u7406\u5206\u5272\uff08VRS\uff09\u65b9\u6cd5\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5c06\u5bf9\u8c61\u8bed\u4e49\u7f16\u7801\u4e3a\u63a9\u6a21\u9884\u6d4b\u7684<SEG>\u6807\u8bb0\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8303\u4f8b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u5e76\u4e14\u7531\u4e8e\u65f6\u7a7a\u63a8\u7406\u4e0d\u8db3\uff0c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVeason-R1\u7684\u89c6\u9891\u63a8\u7406\u5206\u5272\uff08VRS\uff09\u4e13\u7528LVLM\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5f15\u5bfc\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u521d\u59cb\u5316\u6765\u76d1\u7763\u5fae\u8c03\u6a21\u578b\uff08Veason-SFT\uff09\uff0c\u4ee5\u89e3\u51b3\u8de8\u89c6\u9891\u7ea7\u8bed\u4e49\u548c\u5e27\u7ea7\u7a7a\u95f4\u57fa\u7840\u7684\u7ed3\u6784\u5316\u63a8\u7406\u95ee\u9898\u3002\u7136\u540e\uff0c\u901a\u8fc7\u7ed3\u5408\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u521d\u59cb\u5316\u548c\u6574\u4f53\u5956\u52b1\u673a\u5236\u6765\u4f18\u5316\u63a8\u7406\u94fe\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "Veason-R1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728ReVOS\u548cReasonVOS\u4e0a\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e86+1.3 J&F\u548c+10.0 J&F\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff08+8.8 R\uff09\u3002", "conclusion": "Veason-R1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728ReVOS\u548cReasonVOS\u4e0a\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e86+1.3 J&F\u548c+10.0 J&F\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff08+8.8 R\uff09\u3002"}}
{"id": "2508.11550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11550", "abs": "https://arxiv.org/abs/2508.11550", "authors": ["Zuo Zuo", "Jiahao Dong", "Yanyun Qu", "Zongze Wu"], "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "comment": null, "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.", "AI": {"tldr": "AAG is a training-free framework using Stable Diffusion to generate realistic anomalies for industrial anomaly detection, improving performance on downstream tasks.", "motivation": "Addressing the challenge of data scarcity in industrial anomaly detection by proposing a novel, training-free anomaly generation framework.", "method": "AAG is a training-free framework based on Stable Diffusion, utilizing Cross-Attention Enhancement (CAE) to guide anomaly generation based on masks and text prompts, and Self-Attention Enhancement (SAE) to ensure coherence with original patterns.", "result": "AAG generates realistic and plausible anomalies, enhancing downstream anomaly inspection tasks. Experiments on MVTec AD and VisA datasets demonstrate its effectiveness.", "conclusion": "AAG can effectively generate realistic anomalies while preserving normal regions, and can improve downstream anomaly detection tasks."}}
{"id": "2508.11569", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11569", "abs": "https://arxiv.org/abs/2508.11569", "authors": ["Zheng Wang", "Shihao Xu", "Wei Shi"], "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications", "comment": "This paper has been accepted by TCSVT", "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.", "AI": {"tldr": "TrajSV\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u4f53\u80b2\u5206\u6790\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u53ef\u7528\u6027\u3001\u7f3a\u4e4f\u6709\u6548\u6846\u67b6\u548c\u76d1\u7763\u6807\u7b7e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7CRNet\u548cVRN\u5b66\u4e60\u89c6\u9891\u548c\u526a\u8f91\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u65e0\u76d1\u7763\u7684\u4e09\u91cd\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u4f18\u5316\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u4e0b\u6e38\u5e94\u7528\u4e2d\uff0cTrajSV\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4f53\u80b2\u5206\u6790\u7814\u7a76\u9762\u4e34\u6570\u636e\u4e0d\u53ef\u7528\u3001\u7f3a\u4e4f\u6709\u6548\u7684\u57fa\u4e8e\u8f68\u8ff9\u7684\u6846\u67b6\u4ee5\u53ca\u9700\u8981\u5145\u8db3\u76d1\u7763\u6807\u7b7e\u7b49\u95ee\u9898\u3002", "method": "TrajSV\u6846\u67b6\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u526a\u8f91\u8868\u793a\u7f51\u7edc\uff08CRNet\uff09\u548c\u89c6\u9891\u8868\u793a\u7f51\u7edc\uff08VRNet\uff09\u3002CRNet\u4f7f\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684Transformer\u6765\u5b66\u4e60\u526a\u8f91\u8868\u793a\uff0cVRNet\u901a\u8fc7\u805a\u5408\u526a\u8f91\u8868\u793a\u548c\u89c6\u89c9\u7279\u5f81\u6765\u5b66\u4e60\u89c6\u9891\u8868\u793a\u3002\u6700\u540e\uff0c\u4f7f\u7528\u4e09\u91cd\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u65e0\u76d1\u7763\u4f18\u5316\u3002", "result": "\u5728\u8db3\u7403\u3001\u7bee\u7403\u548c\u6392\u7403\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0cTrajSV\u5728\u4f53\u80b2\u89c6\u9891\u68c0\u7d22\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e09\u4e2a\u4e0b\u6e38\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "TrajSV\u5728\u4f53\u80b2\u89c6\u9891\u68c0\u7d22\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u4f53\u80b2\u89c6\u9891\u68c0\u7d22\u7684\u6027\u80fd\u63d0\u5347\u4e86\u8fd170%\uff0c\u52a8\u4f5c\u8bc6\u522b\u572817\u4e2a\u52a8\u4f5c\u7c7b\u522b\u4e2d\u67099\u4e2a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e5f\u63d0\u5347\u4e86\u8fd120%\u3002\u6b64\u5916\uff0c\u8fd8\u90e8\u7f72\u4e86\u4e00\u4e2a\u57fa\u4e8eTrajSV\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.11576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11576", "abs": "https://arxiv.org/abs/2508.11576", "authors": ["Yumeng Shi", "Quanyu Long", "Yin Wu", "Wenya Wang"], "title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "comment": null, "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "AI": {"tldr": "\"\u672c\u6587\u53d1\u73b0\uff0c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u65f6\u95f4\u7406\u89e3\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8de8\u5e27\u6ce8\u610f\u529b\u800c\u975e\u4f4d\u7f6e\u7f16\u7801\u3002\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\u548c\u5f15\u5165\u65f6\u95f4\u9000\u51fa\u673a\u5236\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u3002\"", "motivation": "\"\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VideoLM\uff09\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u65f6\u95f4\u7406\u89e3\uff08\u5982\u4e8b\u4ef6\u987a\u5e8f\u3001\u6301\u7eed\u65f6\u95f4\u548c\u65f6\u95f4\u5173\u7cfb\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u5148\u524d\u7684\u5de5\u4f5c\u666e\u904d\u8ba4\u4e3a\u4f4d\u7f6e\u7f16\u7801\uff08PEs\uff09\u662f\u7f16\u7801\u65f6\u95f4\u7ed3\u6784\u7684\u5173\u952e\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76VideoLM\u4e2d\u65f6\u95f4\u4fe1\u606f\u7684\u5b9e\u9645\u5904\u7406\u65b9\u5f0f\uff0c\u5e76\u5bfb\u627e\u66f4\u4f18\u5316\u7684\u65b9\u6cd5\u3002\"", "method": "\"\u672c\u6587\u901a\u8fc7\u4e00\u7cfb\u5217\u5206\u6790\u5b9e\u9a8c\u6765\u63a2\u7a76\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VideoLM\uff09\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u6574\u5408\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f4d\u7f6e\u7f16\u7801\uff08PEs\uff09\u5bf9\u65f6\u95f4\u7406\u89e3\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u800c\u53cd\u8f6c\u5e27\u5e8f\u5217\u5219\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u901a\u8fc7\u8ffd\u8e2a\u4fe1\u606f\u6d41\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u7ebf\u7d22\u901a\u8fc7\u8de8\u5e27\u6ce8\u610f\u529b\u9010\u6b65\u5408\u6210\uff0c\u5e76\u5728\u6700\u540e\u4e00\u5e27\u805a\u5408\uff0c\u6700\u7ec8\u6574\u5408\u5230\u67e5\u8be2\u4ee4\u724c\u4e2d\u7684\u56e0\u679c\u4fe1\u606f\u901a\u8def\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u9000\u51fa\u673a\u5236\u8fd9\u4e24\u79cd\u63d0\u9ad8\u6548\u7387\u7684\u7b56\u7565\u3002\"", "result": "\"\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u79fb\u9664\u6216\u4fee\u6539PEs\u5bf9\u89c6\u9891\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u5f71\u54cd\u751a\u5fae\uff0c\u800c\u53cd\u8f6c\u5e27\u5e8f\u5217\u5219\u4f1a\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u63ed\u793a\u4e86\u65f6\u95f4\u4fe1\u606f\u5e76\u975e\u4e3b\u8981\u4f9d\u8d56PEs\uff0c\u800c\u662f\u901a\u8fc7\u8de8\u5e27\u6ce8\u610f\u529b\u673a\u5236\u3001\u5728\u6700\u540e\u4e00\u5e27\u805a\u5408\uff0c\u5e76\u6574\u5408\u5230\u67e5\u8be2\u4ee4\u724c\u4e2d\u7684\u8fc7\u7a0b\u81ea\u7136\u6d8c\u73b0\u3002\u63d0\u51fa\u7684\u5206\u9636\u6bb5\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u9000\u51fa\u673a\u5236\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u7b56\u7565\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6548\u7387\u3002\"", "conclusion": "\"\u672a\u6765\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u8fd9\u79cd\u65b0\u53d1\u73b0\u7684\u8de8\u5e27\u6ce8\u610f\u529b\u673a\u5236\u6765\u6539\u8fdb\uff0c\u5e76\u7ed3\u5408\u6240\u63d0\u51fa\u7684\u6548\u7387\u7b56\u7565\uff0c\u4f8b\u5982\u5206\u9636\u6bb5\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u9000\u51fa\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u65f6\u95f4\u63a8\u7406\u3002\""}}
{"id": "2508.11603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "AI": {"tldr": "CoreEditor\u901a\u8fc7\u7ea6\u675f\u6ce8\u610f\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u67093D\u7f16\u8f91\u65b9\u6cd5\u4e2d\u7684\u89c6\u56fe\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u4e00\u81f4\u7684\u6587\u672c\u52303D\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u76843D\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u5c06\u9884\u8bad\u7ec3\u76842D\u56fe\u50cf\u7f16\u8f91\u5668\u5e94\u7528\u4e8e\u591a\u89c6\u56fe\u8f93\u5165\uff0c\u4f46\u7f3a\u4e4f\u8de8\u89c6\u56fe\u4fe1\u606f\u4ea4\u6362\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u5bfc\u81f4\u89c6\u56fe\u95f4\u4e0d\u4e00\u81f4\u3001\u7f16\u8f91\u6548\u679c\u4e0d\u7406\u60f3\u548c\u7ec6\u8282\u6a21\u7cca\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6CoreEditor\uff0c\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u7ea6\u675f\u6ce8\u610f\u673a\u5236\uff0c\u7528\u4e8e\u5728\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8de8\u89c6\u56fe\u50cf\u7d20\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u7ed3\u5408\u4e86\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f30\u8ba1\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9009\u62e9\u6027\u7f16\u8f91\u6d41\u7a0b\uff0c\u5141\u8bb8\u7528\u6237\u9009\u62e9\u6700\u4f73\u7f16\u8f91\u7ed3\u679c\u3002", "result": "CoreEditor\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u30013D\u4e00\u81f4\u6027\u5f3a\u4e14\u7ec6\u8282\u66f4\u9510\u5229\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u5728\u5404\u79cd\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "CoreEditor\u57283D\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u9510\u5316\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "AI": {"tldr": "LoRAtorio \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6709\u6548\u7ec4\u5408\u591a\u4e2a LoRA \u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u5f0f\u573a\u666f\u4e0b\u7684\u7ec4\u5408\u96be\u9898\u3002\u5b83\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u76f8\u4f3c\u5ea6\u6765\u6307\u5bfc\u52a0\u6743\u805a\u5408\uff0c\u5e76\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u548c\u52a8\u6001\u6a21\u5757\u9009\u62e9\u6765\u4f18\u5316\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684 LoRA \u65b9\u6cd5\u5728\u7ec4\u5408\u591a\u4e2a LoRA \u9002\u914d\u5668\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u9884\u5148\u786e\u5b9a\u6240\u9700\u6280\u80fd\u7684\u6570\u91cf\u548c\u6027\u8d28\u7684\u5f00\u653e\u5f0f\u573a\u666f\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7ec4\u5408\u591a\u4e2a LoRA \u9002\u914d\u5668\u7684\u65b0\u6846\u67b6\u3002", "method": "LoRAtorio \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a LoRA \u7ec4\u5408\uff0c\u5b83\u5229\u7528\u5185\u5728\u7684\u6a21\u578b\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6f5c\u5728\u7a7a\u95f4\u5212\u5206\u4e3a\u7a7a\u95f4\u5757\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u5757\u7684\u9884\u6d4b\u566a\u58f0\u4e0e\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u566a\u58f0\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u64cd\u4f5c\u3002\u8fd9\u4e9b\u76f8\u4f3c\u5ea6\u7528\u4e8e\u6784\u5efa\u4e00\u4e2a\u7a7a\u95f4\u611f\u77e5\u7684\u6743\u91cd\u77e9\u9635\uff0c\u6307\u5bfc LoRA \u8f93\u51fa\u7684\u52a0\u6743\u805a\u5408\u3002\u4e3a\u4e86\u89e3\u51b3\u9886\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u6539\u540e\u7684\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u65b9\u6cd5\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u7684\u65e0\u6761\u4ef6\u5206\u6570\u7eb3\u5165\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6a21\u5757\u9009\u62e9\u673a\u5236\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u4ece\u5927\u91cf LoRA \u9002\u914d\u5668\u4e2d\u9009\u62e9\u76f8\u5173\u7684\u9002\u914d\u5668\u3002", "result": "LoRAtorio \u5728 ClipScore \u65b9\u9762\u63d0\u9ad8\u4e86 1.3%\uff0c\u5728 GPT-4V \u914d\u5bf9\u8bc4\u4f30\u4e2d\u83b7\u80dc\u7387\u4e3a 72.43%\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u63a8\u5e7f\u5230\u591a\u4e2a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "conclusion": "LoRAtorio\u6846\u67b6\u5728\u591a LoRA \u7ec4\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 ClipScore \u65b9\u9762\u63d0\u9ad8\u4e86 1.3%\uff0c\u5728 GPT-4V \u914d\u5bf9\u8bc4\u4f30\u4e2d\u83b7\u80dc\u7387\u4e3a 72.43%\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u63a8\u5e7f\u5230\u591a\u4e2a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2508.11630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11630", "abs": "https://arxiv.org/abs/2508.11630", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Shukang Yin", "Chaoyou Fu", "Wei Chen", "Xiao Hu", "Bin Wen", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Haonan Fan", "Kaibing Chen", "Jiankang Chen", "Haojie Ding", "Kaiyu Tang", "Zhang Zhang", "Liang Wang", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "title": "Thyme: Think Beyond Images", "comment": "Project page: https://thyme-vl.github.io/", "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.", "AI": {"tldr": "Thyme\u662f\u4e00\u4e2a\u65b0\u8303\u5f0f\uff0c\u4f7fLMM\u80fd\u591f\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u6765\u8d85\u8d8a\u73b0\u6709\u7684\u201c\u56fe\u50cf\u601d\u8003\u201d\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08SFT+RL\uff09\u548cGRPO-ATS\u7b97\u6cd5\u5b9e\u73b0\uff0c\u80fd\u5728\u5904\u7406\u56fe\u50cf\u548c\u6267\u884c\u4ee3\u7801\u65f6\u4fdd\u6301\u9ad8\u81ea\u4e3b\u6027\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u201c\u56fe\u50cf\u601d\u8003\u201d\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u50cf\u4e13\u6709\u6a21\u578b\uff08O3\uff09\u90a3\u6837\u4e30\u5bcc\u7684\u529f\u80fd\u96c6\uff0c\u65e0\u6cd5\u540c\u65f6\u8fdb\u884c\u591a\u6837\u5316\u7684\u56fe\u50cf\u64cd\u4f5c\u5e76\u589e\u5f3a\u4ee3\u7801\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "Thyme\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u5176\u80fd\u529b\uff1a\u9996\u5148\u5728500K\u6837\u672c\u7684\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884cSFT\u8bad\u7ec3\u4ee5\u6559\u6388\u4ee3\u7801\u751f\u6210\uff0c\u7136\u540e\u8fdb\u884cRL\u9636\u6bb5\u4ee5\u4f18\u5316\u51b3\u7b56\u3002\u5728RL\u9636\u6bb5\uff0c\u63d0\u51faGRPO-ATS\uff08Group Relative Policy Optimization with Adaptive Temperature Sampling\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6587\u672c\u548c\u4ee3\u7801\u751f\u6210\u5e94\u7528\u4e0d\u540c\u7684\u6e29\u5ea6\u6765\u5e73\u8861\u63a8\u7406\u63a2\u7d22\u548c\u4ee3\u7801\u6267\u884c\u7cbe\u5ea6\u3002", "result": "Thyme\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u548c\u6267\u884c\u591a\u6837\u5316\u7684\u56fe\u50cf\u5904\u7406\u548c\u8ba1\u7b97\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e30\u5bcc\u7684\u5373\u65f6\u56fe\u50cf\u64cd\u4f5c\uff08\u5982\u88c1\u526a\u3001\u65cb\u8f6c\u3001\u5bf9\u6bd4\u5ea6\u589e\u5f3a\uff09\u548c\u6570\u5b66\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u81ea\u4e3b\u6027\u3002\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cThyme\u5728\u8fd120\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u611f\u77e5\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "conclusion": "Thyme\u5728\u8fd120\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u9ad8\u5206\u8fa8\u7387\u611f\u77e5\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u3002"}}
