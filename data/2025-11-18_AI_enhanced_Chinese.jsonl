{"id": "2511.11747", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11747", "abs": "https://arxiv.org/abs/2511.11747", "authors": ["Felip Pellicer"], "title": "Enhanced Digitized Adiabatic Quantum Factorization Algorithm Using Null-Space Encoding", "comment": null, "summary": "Integer factorization is a computational problem of fundamental importance in cybersecurity and secure communications, as its difficulty form the basis of modern public-key cryptography. While Shor's algorithm can solve this problem efficiently on a universal quantum computer, near-term devices require alternative approaches. The Adiabatic Factorization Algorithm and its digitized counterparts offer a promising NISQ-era pathway but suffer from high-order many-body interactions that are difficult to implement. In this work, we propose a modified QAOA-based factorization protocol that simplifies the interacting Hamiltonian to include only two-body terms, significantly reducing its experimental complexity. Numerical simulations show that this method achieves comparable or higher fidelities than the standard protocol, while requiring fewer quantum resources and converging more rapidly for problem instances up to eight qubits. We analyze the characteristic fidelity behavior introduced by the Hamiltonian modification. Additionally, we report on simulations with alternative cost-function definitions that frequently yielded improved performance.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u57fa\u4e8eQAOA\u7684\u6574\u6570\u5206\u89e3\u534f\u8bae\uff0c\u901a\u8fc7\u4ec5\u5305\u542b\u4e8c\u4f53\u76f8\u4e92\u4f5c\u7528\u9879\uff0c\u964d\u4f4e\u4e86\u5b9e\u9a8c\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u663e\u793a\u51fa\u4e0e\u6807\u51c6\u534f\u8bae\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u91cf\u5b50\u8d44\u6e90\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u7531\u4e8eShor\u7b97\u6cd5\u5728\u901a\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6548\u7387\u9ad8\uff0c\u800c\u8fd1\u5730\u91cf\u5b50\u8bbe\u5907\u9700\u8981\u66ff\u4ee3\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u9002\u7528\u4e8eNISQ\u65f6\u4ee3\u7684\u6574\u6570\u5206\u89e3\u65b9\u6cd5\u3002\u73b0\u6709\u7684Adiabatic Factorization Algorithm\u53ca\u5176\u6570\u5b57\u5316\u7248\u672c\u5b58\u5728\u9ad8\u9636\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u73b0\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u6539\u540e\u7684\u57fa\u4e8eQAOA\u7684\u56e0\u5b50\u5206\u89e3\u534f\u8bae\uff0c\u5c06\u76f8\u4e92\u4f5c\u7528\u7684\u54c8\u5bc6\u987f\u91cf\u7b80\u5316\u4e3a\u53ea\u5305\u542b\u4e8c\u4f53\u9879\u3002\u5bf9\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u6570\u503c\u6a21\u62df\uff0c\u5e76\u5206\u6790\u4e86\u54c8\u5bc6\u987f\u91cf\u4fee\u6539\u5f15\u5165\u7684\u7279\u5f81\u4fdd\u771f\u5ea6\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u6a21\u62df\u4e86\u4f7f\u7528\u66ff\u4ee3\u6210\u672c\u51fd\u6570\u5b9a\u4e49\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u6807\u51c6\u534f\u8bae\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u91cf\u5b50\u8d44\u6e90\uff0c\u5e76\u4e14\u5bf9\u4e8e\u591a\u8fbe\u516b\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u95ee\u9898\u5b9e\u4f8b\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002\u8fd8\u53d1\u73b0\u4e86\u66ff\u4ee3\u6210\u672c\u51fd\u6570\u5b9a\u4e49\u901a\u5e38\u80fd\u5e26\u6765\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b80\u5316\u7684\u57fa\u4e8eQAOA\u7684\u56e0\u5b50\u5206\u89e3\u534f\u8bae\u5728NISQ\u8bbe\u5907\u4e0a\u6bd4\u6807\u51c6\u534f\u8bae\u66f4\u5177\u53ef\u884c\u6027\uff0c\u56e0\u4e3a\u5b83\u964d\u4f4e\u4e86\u5b9e\u9a8c\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2511.11791", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11791", "abs": "https://arxiv.org/abs/2511.11791", "authors": ["Danielle A. Braje", "Matthew L. Markham", "Jennifer M. Schloss", "Michael A. Slocum", "Ronald L. Walsworth"], "title": "2025 Quantum Diamond Workshop Findings Report", "comment": "10 pages", "summary": "This report synthesizes the outcomes of a two-day workshop held in Washington, D.C. in May, 2025 that convened researchers, industry representatives, and government stakeholders to examine the current state and future directions of quantum diamond technologies. The workshop's goals were to assess the most promising use cases, to identify the key technical and structural challenges limiting adoption, and to chart potential pathways for aligning application needs with diamond material and device development. Through a series of technical presentations and open discussions, participants explored both near-term demonstrations and long-term infrastructure needs, highlighting the critical role of coordination between material suppliers, device engineers, and end users. The goal of this report is to distill those insights into a coherent set of cross-cutting themes, challenges, and strategic actions that can guide government, industry, and academic efforts to accelerate the maturation and commercialization of quantum diamond technologies.", "AI": {"tldr": "\u91cf\u5b50\u91d1\u521a\u77f3\u6280\u672f\u7814\u8ba8\u4f1a\u603b\u7ed3\u4e86\u8fd1\u671f\u8fdb\u5c55\u3001\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u91cf\u5b50\u91d1\u521a\u77f3\u6280\u672f\u7684\u5e94\u7528\u524d\u666f\uff0c\u8bc6\u522b\u6280\u672f\u548c\u7ed3\u6784\u6027\u969c\u788d\uff0c\u5e76\u4e3a\u6750\u6599\u3001\u8bbe\u5907\u548c\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u534f\u8c03\u8def\u5f84\u3002", "method": "\u901a\u8fc7\u6280\u672f\u6f14\u793a\u548c\u5f00\u653e\u8ba8\u8bba\uff0c\u63a2\u8ba8\u4e86\u77ed\u671f\u6f14\u793a\u548c\u957f\u671f\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\uff0c\u5f3a\u8c03\u4e86\u4f9b\u5e94\u94fe\u5404\u65b9\u534f\u8c03\u7684\u91cd\u8981\u6027\u3002", "result": "\u8bc6\u522b\u4e86\u52a0\u901f\u91cf\u5b50\u91d1\u521a\u77f3\u6280\u672f\u6210\u719f\u548c\u5546\u4e1a\u5316\u7684\u5173\u952e\u4e3b\u9898\u3001\u6311\u6218\u548c\u6218\u7565\u884c\u52a8\u3002", "conclusion": "\u5efa\u8bae\u653f\u5e9c\u3001\u884c\u4e1a\u548c\u5b66\u672f\u754c\u5171\u540c\u52aa\u529b\uff0c\u4ee5\u52a0\u901f\u91cf\u5b50\u91d1\u521a\u77f3\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.11802", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11802", "abs": "https://arxiv.org/abs/2511.11802", "authors": ["Maria Demidik", "Cenk T\u00fcys\u00fcz", "Michele Grossi", "Karl Jansen"], "title": "Sample-based training of quantum generative models", "comment": "10 pages, 3 figures; supplementary material 6 pages, 3 figures", "summary": "Quantum computers can efficiently sample from probability distributions that are believed to be classically intractable, providing a foundation for quantum generative modeling. However, practical training of such models remains challenging, as gradient evaluation via the parameter-shift rule scales linearly with the number of parameters and requires repeated expectation-value estimation under finite-shot noise. We introduce a training framework that extends the principle of contrastive divergence to quantum models. By deriving the circuit structure and providing a general recipe for constructing it, we obtain quantum circuits that generate the samples required for parameter updates, yielding constant scaling with respect to the cost of a forward pass, analogous to backpropagation in classical neural networks. Numerical results demonstrate that it attains comparable accuracy to likelihood-based optimization while requiring substantially fewer samples. The framework thereby establishes a scalable route to training expressive quantum generative models directly on quantum hardware.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5bf9\u6bd4\u5ea6\u8870\u51cf\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u91cf\u5b50\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4f18\u5316\u7535\u8def\u7ed3\u6784\u5b9e\u73b0\u53ef\u6269\u5c55\u8bad\u7ec3\uff0c\u5e76\u80fd\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fbe\u5230\u4e0e\u57fa\u4e8e\u4f3c\u7136\u7684\u4f18\u5316\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6240\u9700\u91c7\u6837\u6570\u91cf\u66f4\u5c11\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u91c7\u6837\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u53ef\u7528\u4e8e\u91cf\u5b50\u751f\u6210\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u8bad\u7ec3\u9762\u4e34\u68af\u5ea6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u566a\u58f0\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5bf9\u6bd4\u5ea6\u8870\u51cf\u539f\u7406\u6269\u5c55\u5230\u91cf\u5b50\u6a21\u578b\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u5bfc\u7535\u8def\u7ed3\u6784\u548c\u63d0\u4f9b\u6784\u5efa\u65b9\u6cd5\uff0c\u4f7f\u91cf\u5b50\u7535\u8def\u80fd\u591f\u751f\u6210\u53c2\u6570\u66f4\u65b0\u6240\u9700\u7684\u6837\u672c\uff0c\u5b9e\u73b0\u4e86\u4e0e\u524d\u5411\u4f20\u64ad\u76f8\u4f3c\u7684\u5e38\u6570\u7ea7\u6269\u5c55\u6027\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u4e0a\u53ef\u4e0e\u57fa\u4e8e\u4f3c\u7136\u7684\u4f18\u5316\u76f8\u5ab2\u7f8e\uff0c\u4f46\u6240\u9700\u7684\u91c7\u6837\u6570\u91cf\u5927\u5927\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u76f4\u63a5\u5728\u91cf\u5b50\u786c\u4ef6\u4e0a\u8bad\u7ec3\u8868\u8fbe\u6027\u91cf\u5b50\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2511.11861", "categories": ["quant-ph", "astro-ph.HE", "gr-qc"], "pdf": "https://arxiv.org/pdf/2511.11861", "abs": "https://arxiv.org/abs/2511.11861", "authors": ["Ningyan Fang", "Martin Houde", "Fereshteh Rajabi", "Victor Botez"], "title": "Relativistic Maxwell-Bloch Equations with Applications to Astrophysics", "comment": "9 pages, 5 figures", "summary": "We derive relativistic Maxwell-Bloch equations for potential applications in astronomical environments, where various radiative processes are known to occur, including the maser action and Dicke's superradiance. We show that for both phenomena a radiating system's response is preserved at different relative velocities between the system's rest frame and the observer, while the relevant timescales and the radiation intensity transform as expected from relativistic considerations. We verify that the level of coherence between groups of emitters travelling at different speeds is unchanged in all reference frames. We also derive relativistic versions of the maser equations applicable in the steady-state regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u5bfc\u4e86\u76f8\u5bf9\u8bba\u6027\u9ea6\u514b\u65af\u97e6-\u5e03\u6d1b\u8d6b\u65b9\u7a0b\uff0c\u5e76\u7814\u7a76\u4e86\u5929\u4f53\u73af\u5883\u4e2d\u53ef\u80fd\u53d1\u751f\u7684\u5149\u8c31\u653e\u5927\u548c\u8fea\u514b\u8d85\u8f90\u5c04\u7b49\u8f90\u5c04\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u5929\u4f53\u73af\u5883\u4e2d\u53d1\u751f\u7684\u8f90\u5c04\u8fc7\u7a0b\uff0c\u5305\u62ec\u5149\u8c31\u653e\u5927\u548c\u8fea\u514b\u8d85\u8f90\u5c04\u3002", "method": "\u63a8\u5bfc\u76f8\u5bf9\u8bba\u6027\u9ea6\u514b\u65af\u97e6-\u5e03\u6d1b\u8d6b\u65b9\u7a0b\uff0c\u5e76\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u7a33\u6001\u6761\u4ef6\u4e0b\u7684\u5149\u8c31\u653e\u5927\u65b9\u7a0b\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u76f8\u5bf9\u901f\u5ea6\u4e0b\uff0c\u8f90\u5c04\u7cfb\u7edf\u7684\u54cd\u5e94\u4fdd\u6301\u4e0d\u53d8\uff0c\u800c\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u548c\u8f90\u5c04\u5f3a\u5ea6\u4f1a\u6839\u636e\u76f8\u5bf9\u8bba\u8fdb\u884c\u8f6c\u6362\u3002\u540c\u65f6\uff0c\u4e0d\u540c\u901f\u5ea6\u53d1\u5c04\u5668\u7fa4\u4f53\u4e4b\u95f4\u7684\u76f8\u5e72\u6027\u5728\u6240\u6709\u53c2\u8003\u7cfb\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u5bf9\u8bba\u6548\u5e94\u5bf9\u5149\u8c31\u653e\u5927\u548c\u8fea\u514b\u8d85\u8f90\u5c04\u7b49\u8f90\u5c04\u8fc7\u7a0b\u6709\u91cd\u8981\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u5929\u4f53\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12230", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.12230", "abs": "https://arxiv.org/abs/2511.12230", "authors": ["Neal E. Young"], "title": "An improved approximation algorithm for k-Median", "comment": null, "summary": "We give a polynomial-time approximation algorithm for the (not necessarily metric) $k$-Median problem. The algorithm is an $\u03b1$-size-approximation algorithm for $\u03b1< 1 + 2 \\ln(n/k)$. That is, it guarantees a solution having size at most $\u03b1\\times k$, and cost at most the cost of any size-$k$ solution. This is the first polynomial-time approximation algorithm to match the well-known bounds of $H_\u0394$ and $1 + \\ln(n/k)$ for unweighted Set Cover (a special case) within a constant factor. It matches these bounds within a factor of 2. The algorithm runs in time $O(k m \\log(n/k) \\log m)$, where $n$ is the number of customers and $m$ is the instance size.", "AI": {"tldr": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9488\u5bf9k-Median\u95ee\u9898\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u8fd1\u4f3c\u6bd4\u5c0f\u4e8e1 + 2ln(n/k)\uff0c\u5e76\u4e14\u5728\u89e3\u7684\u5927\u5c0f\u548c\u6210\u672c\u4e0a\u90fd\u4f18\u4e8e\u5df2\u77e5\u7684Set Cover\u95ee\u9898\u7684\u754c\u9650\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3k-Median\u95ee\u9898\uff0c\u7279\u522b\u662f\u5176\u975e\u5ea6\u91cf\u60c5\u51b5\uff0c\u5e76\u5bfb\u6c42\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u597d\u7684\u8fd1\u4f3c\u6bd4\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u8be5\u7b97\u6cd5\u662f\u4e00\u4e2a\u03b1-\u5927\u5c0f\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u4e2d\u03b1 < 1 + 2ln(n/k)\u3002\u5b83\u4fdd\u8bc1\u89e3\u7684\u5927\u5c0f\u6700\u591a\u4e3a\u03b1 * k\uff0c\u6210\u672c\u4e0d\u8d85\u8fc7\u4efb\u4f55\u5927\u5c0f\u4e3ak\u7684\u89e3\u7684\u6210\u672c\u3002\u8be5\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e3aO(k * m * log(n/k) * log m)\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5339\u914d\u672a\u52a0\u6743Set Cover\u95ee\u9898\u7684\u5df2\u77e5\u754c\u9650\uff08H\u0394\u548c1 + ln(n/k)\uff09\u5e38\u6570\u56e0\u5b50\u5185\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u4e14\u5728\u8fd9\u4e9b\u754c\u9650\u5185\u5339\u914d\u56e0\u5b502\u3002"}}
{"id": "2511.11969", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.11969", "abs": "https://arxiv.org/abs/2511.11969", "authors": ["Zhen Tao", "Yuehang Cao", "Yang Fang", "Yunhui Liu", "Xiang Zhao", "Tieke He"], "title": "Dynamic Graph Recommendation via Sparse Augmentation and Singular Adaptation", "comment": "ICASSP 2025", "summary": "Dynamic recommendation, focusing on modeling user preference from historical interactions and providing recommendations on current time, plays a key role in many personalized services. Recent works show that pre-trained dynamic graph neural networks (GNNs) can achieve excellent performance. However, existing methods by fine-tuning node representations at large scales demand significant computational resources. Additionally, the long-tail distribution of degrees leads to insufficient representations for nodes with sparse interactions, posing challenges for efficient fine-tuning. To address these issues, we introduce GraphSASA, a novel method for efficient fine-tuning in dynamic recommendation systems. GraphSASA employs test-time augmentation by leveraging the similarity of node representation distributions during hierarchical graph aggregation, which enhances node representations. Then it applies singular value decomposition, freezing the original vector matrix while focusing fine-tuning on the derived singular value matrices, which reduces the parameter burden of fine-tuning and improves the fine-tuning adaptability. Experimental results demonstrate that our method achieves state-of-the-art performance on three large-scale datasets.", "AI": {"tldr": "GraphSASA\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u5f3a\u548c\u5947\u5f02\u503c\u5206\u89e3\u5b9e\u73b0\u4e86\u52a8\u6001\u63a8\u8350\u7684\u9ad8\u6548\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u63a8\u8350\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u65f6\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u4e14\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u7a00\u758f\u4ea4\u4e92\u8282\u70b9\u8868\u793a\u4e0d\u8db3\uff0c\u5fae\u8c03\u6548\u7387\u4f4e\u3002", "method": "GraphSASA\u5229\u7528\u8282\u70b9\u8868\u793a\u5206\u5e03\u76f8\u4f3c\u6027\u8fdb\u884c\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff0c\u5e76\u901a\u8fc7\u51bb\u7ed3\u539f\u59cb\u5411\u91cf\u77e9\u9635\u3001\u4ec5\u5bf9\u5947\u5f02\u503c\u77e9\u9635\u8fdb\u884c\u5fae\u8c03\u6765\u51cf\u5c11\u53c2\u6570\u8d1f\u62c5\u548c\u63d0\u9ad8\u5fae\u8c03\u9002\u5e94\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GraphSASA\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u52a8\u6001\u63a8\u8350\u65b9\u6cd5\u9762\u4e34\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u8868\u793a\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.12011", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2511.12011", "abs": "https://arxiv.org/abs/2511.12011", "authors": ["Sam Buss", "Anant Dhayal", "Valentine Kabanets", "Antonina Kolokolova", "Sasank Mouli"], "title": "A Logspace Constructive Proof of L=SL", "comment": "39 pages, one figure", "summary": "We formalize the proof of Reingold's Theorem that SL=L [Rei05] in the theory of bounded arithmetic VL, which corresponds to ``logspace reasoning''. As a consequence, we get that VL=VSL, where VSL is the theory of bounded arithmetic for ``symmetric-logspace reasoning''. This resolves in the affirmative an old open question from Kolokolova [Kol05] (see also Cook-Nguyen [NC10]).\n  Our proof relies on the Rozenman-Vadhan alternative proof of Reingold's Theorem ([RV05]). To formalize this proof in VL, we need to avoid reasoning about eigenvalues and eigenvectors (common in both original proofs of SL=L). We achieve this by using some results from Buss-Kabanets-Kolokolova-Kouck\u00fd [Bus+20] that allow VL to reason about graph expansion in combinatorial terms.", "AI": {"tldr": "\u672c\u6587\u5728\u6709\u754c\u7b97\u672f\u7406\u8bbaVL\u4e2d\u5f62\u5f0f\u5316\u4e86Reingold\u5b9a\u7406SL=L\u7684\u8bc1\u660e\uff0c\u8be5\u5b9a\u7406\u5bf9\u5e94\u4e8e\u201clogspace\u63a8\u7406\u201d\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u5728\u6709\u754c\u7b97\u672f\u7406\u8bbaVL\u4e2d\u5f62\u5f0f\u5316Reingold\u5b9a\u7406SL=L\u7684\u8bc1\u660e\uff0c\u5e76\u89e3\u51b3Kolokolova\u5173\u4e8eVL=VSL\u7684\u9057\u7559\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5229\u7528Rozenman-Vadhan\u7684Reingold\u5b9a\u7406\u66ff\u4ee3\u8bc1\u660e\uff0c\u5e76\u7ed3\u5408Buss-Kabanets-Kolokolova-Kouck\u00fd\u7684\u7ed3\u679c\uff0c\u907f\u514d\u4e86\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u7684\u63a8\u7406\uff0c\u4ece\u800c\u5728VL\u4e2d\u8fdb\u884c\u56fe\u6269\u5f20\u7684\u7ec4\u5408\u63a8\u7406\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86VL=VSL\uff0c\u5176\u4e2dVSL\u662f\u6709\u754c\u7b97\u672f\u7406\u8bba\u4e2d\u7684\u201c\u5bf9\u79f0logspace\u63a8\u7406\u201d\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5728\u6709\u754c\u7b97\u672f\u7406\u8bbaVL\u4e2d\u5f62\u5f0f\u5316\u4e86Reingold\u5b9a\u7406SL=L\u7684\u8bc1\u660e\uff0c\u5e76\u89e3\u51b3\u4e86VSL\u7406\u8bba\u4e0eVL\u7406\u8bba\u76f8\u7b49\u7684\u95ee\u9898\u3002"}}
{"id": "2511.11788", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11788", "abs": "https://arxiv.org/abs/2511.11788", "authors": ["Antonio Sabbatella"], "title": "MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization", "comment": "Master's Thesis, University of Milano-Bicocca, 2025", "summary": "The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.\n  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.\n  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.", "AI": {"tldr": "MALBO\u6846\u67b6\u901a\u8fc7\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u81ea\u52a8\u5316LLM\u56e2\u961f\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5206\u914d\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u591a\u667a\u80fd\u4f53\u3001\u591a\u76ee\u6807\u95ee\u9898\u3002", "method": "\u63d0\u51faMALBO\uff08Multi-Agent LLM Bayesian Optimization\uff09\u6846\u67b6\uff0c\u5c06LLM\u56e2\u961f\u7684\u5206\u914d\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u5177\u6709\u72ec\u7acb\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u7684\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff08MOBO\uff09\u5728LLM\u7684\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\u4e0a\u8fdb\u884c\u641c\u7d22\uff0c\u4ee5\u6700\u5927\u5316\u8d85\u4f53\u79ef\u6539\u8fdb\u3002", "result": "MALBO\u6846\u67b6\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b\u6700\u4f18\u56e2\u961f\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u4e0e\u968f\u673a\u641c\u7d22\u76f8\u6bd4\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u9636\u6bb5\u5728\u4fdd\u6301\u76f8\u4f3c\u5e73\u5747\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u5e73\u5747\u914d\u7f6e\u6210\u672c\u964d\u4f4e\u4e8645%\u4ee5\u4e0a\u3002\u4e0e\u540c\u8d28\u5316\u57fa\u7ebf\u76f8\u6bd4\uff0cMALBO\u8bc6\u522b\u51fa\u7684\u5f02\u6784\u56e2\u961f\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe65.8%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u9ad8\u6027\u80fd\u3002", "conclusion": "MALBO\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u7ec4\u5408\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u56e2\u961f\uff0c\u5728\u90e8\u7f72\u7ecf\u6d4e\u9ad8\u6548\u4e14\u9ad8\u5ea6\u4e13\u4e1a\u5316\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u65b9\u9762\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u5de5\u5177\u3002"}}
{"id": "2511.11809", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.11809", "abs": "https://arxiv.org/abs/2511.11809", "authors": ["Sebastian Miles", "A. Mert Bozkurt", "D\u00e1niel Varjas", "Michael Wimmer"], "title": "Effective Hamiltonians for Ge/Si core/shell nanowires from higher order perturbation theory", "comment": null, "summary": "We theoretically explore the electronic structure of holes in cylindrical Germanium/Silicon core/shell nanowires using a perturbation theory approach. The approach yields a set of interpretable and transferable effective low-energy models for the lowest few sub-bands up to fifth order for experimentally relevant growth directions. In particular, we are able to resolve higher order cross terms e.g., the dependency of the effective mass on the magnetic field. Our study reveals orbital inversions of the lowest sub-bands for low-symmetry growth directions, leading to significant changes of the lower order effective coefficients. We demonstrate a reduction of the direct Rashba spin-orbit interaction due to competing symmetry effects for low-symmetry growth directions. Finally, we find that the effective mass of the confined holes can diverge yielding quasi flat bands interesting for correlated states. We show how one can tune the effective mass of a single spin band allowing one to tune the effective mass selectively to its divergent points.", "AI": {"tldr": "\u6211\u4eec\u4f7f\u7528\u6444\u52a8\u7406\u8bba\u7814\u7a76\u4e86Ge/Si\u6838/\u58f3\u7eb3\u7c73\u7ebf\u4e2d\u7a7a\u7a74\u7684\u7535\u5b50\u7ed3\u6784\uff0c\u5f97\u5230\u4e86\u51e0\u9636\u4f4e\u80fd\u6a21\u578b\uff0c\u5e76\u89e3\u51b3\u4e86\u6709\u6548\u8d28\u91cf\u4f9d\u8d56\u4e8e\u78c1\u573a\u7684\u4ea4\u53c9\u9879\u7b49\u9ad8\u9636\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\u4f4e\u5bf9\u79f0\u751f\u957f\u65b9\u5411\u7684\u8f68\u9053\u53cd\u8f6c\u4f1a\u6539\u53d8\u4f4e\u9636\u6709\u6548\u7cfb\u6570\uff0c\u5e76\u964d\u4f4eRashba\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u7a7a\u7a74\u7684\u6709\u6548\u8d28\u91cf\u4f1a\u53d1\u6563\uff0c\u5f62\u6210\u51c6\u5e73\u5e26\uff0c\u8fd9\u5bf9\u4e8e\u5173\u8054\u6001\u5f88\u6709\u8da3\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u9009\u62e9\u6027\u5730\u8c03\u6574\u5355\u81ea\u65cb\u80fd\u5e26\u7684\u6709\u6548\u8d28\u91cf\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7406\u8bba\u4e0a\u63a2\u7d22\u5706\u67f1\u5f62\u9517/\u7845\u6838/\u58f3\u7eb3\u7c73\u7ebf\u4e2d\u7a7a\u7a74\u7684\u7535\u5b50\u7ed3\u6784\uff0c\u7279\u522b\u662f\u8981\u7406\u89e3\u5728\u4e0d\u540c\u751f\u957f\u65b9\u5411\u4e0b\uff0c\u6709\u6548\u8d28\u91cf\u3001\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u548c\u80fd\u5e26\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6444\u52a8\u7406\u8bba\u65b9\u6cd5\uff0c\u5f97\u5230\u6700\u9ad8\u5230\u4e94\u9636\u7684\u4f4e\u80fd\u6709\u6548\u6a21\u578b\uff0c\u4ee5\u89e3\u91ca\u5b9e\u9a8c\u76f8\u5173\u7684\u751f\u957f\u65b9\u5411\u3002", "result": "\u63a8\u5bfc\u4e86\u4f4e\u80fd\u6709\u6548\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u4f4e\u5bf9\u79f0\u751f\u957f\u65b9\u5411\u7684\u8f68\u9053\u53cd\u8f6c\u73b0\u8c61\uff0c\u5e76\u89c2\u5bdf\u5230Rashba\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u7684\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u7a7a\u7a74\u6709\u6548\u8d28\u91cf\u53d1\u6563\u5bfc\u81f4\u51c6\u5e73\u5e26\u7684\u5f62\u6210\uff0c\u5e76\u63d0\u51fa\u53ef\u4ee5\u8c03\u63a7\u5355\u81ea\u65cb\u80fd\u5e26\u7684\u6709\u6548\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u8c03\u63a7\u7eb3\u7c73\u7ebf\u4e2d\u7684\u7a7a\u7a74\u7535\u5b50\u7ed3\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8bbe\u8ba1\u5177\u6709\u7279\u5b9a\u7535\u5b50\u548c\u81ea\u65cb\u7279\u6027\u7684\u91cf\u5b50\u5668\u4ef6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.11637", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11637", "abs": "https://arxiv.org/abs/2511.11637", "authors": ["Pedro Pereyra"], "title": "Transition from MOS to Ideal Capacitor Behavior Triggered by Tunneling in the Inversion Population Regime", "comment": "6 figures", "summary": "An analytical solution to the nonlinear Poisson equation governing the inversion layer in metal-oxide-semiconductor (MOS) structures has recently been obtained, resolving a fundamental challenge in semiconductor theory first identified in 1955. This breakthrough enables the derivation of explicit expressions for relevant physical quantities, such as the inversion-layer width, electric potential, and charge distribution, as functions of gate voltage $V_G$, distance from oxide-semiconductor interface and impurity concentration. These quantities exhibit rapid variation during early-stage inversion but saturate once the gate voltage exceeds the threshold voltage by a few tenths of a volt signaling a transition in the MOS response to $V_G$. The onset of tunneling through the Esaki barrier leads to increased charge accumulation near the interface, reshaping the charge distribution into a two-dimensional profile and shifting the potential drop from the semiconductor to the oxide layer. This reconfiguration resembles the behavior of an ideal parallel-plate capacitor, with charge confined at the interface and the voltage drop localized across the oxide. We analyze this mechanism in detail and demonstrate, through explicit calculations, that the tunneling current through the Esaki-like barrier formed during inversion becomes dominant, effectively superseding classical inversion behavior. These results offer a new analytical foundation for quantum-aware device modeling and inform the design of next-generation MOSFET and tunneling FET architectures.", "AI": {"tldr": "\u6587\u7ae0\u5bfc\u51fa\u4e86\u975e\u7ebf\u6027\u6cca\u677e\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\uff0c\u7528\u4e8e\u63cf\u8ff0\u91d1\u5c5e-\u6c27\u5316\u7269-\u534a\u5bfc\u4f53\uff08MOS\uff09\u7ed3\u6784\u4e2d\u7684\u53cd\u578b\u5c42\uff0c\u89e3\u51b3\u4e861955\u5e74\u63d0\u51fa\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u8be5\u89e3\u80fd\u591f\u5f97\u5230\u53cd\u578b\u5c42\u5bbd\u5ea6\u3001\u7535\u52bf\u548c\u7535\u8377\u5206\u5e03\u7b49\u7269\u7406\u91cf\u968f\u6805\u6781\u7535\u538b\u3001\u8ddd\u79bb\u548c\u63ba\u6742\u6d53\u5ea6\u53d8\u5316\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\u3002\u5728\u53cd\u578b\u65e9\u671f\uff0c\u8fd9\u4e9b\u91cf\u53d8\u5316\u8fc5\u901f\uff0c\u4f46\u5728\u6805\u6781\u7535\u538b\u8d85\u8fc7\u9608\u503c\u7535\u538b\u540e\u4f1a\u9971\u548c\u3002\u5f53\u96a7\u7a7f\u53d1\u751f\u65f6\uff0c\u754c\u9762\u9644\u8fd1\u7684\u7535\u8377\u7d2f\u79ef\u589e\u52a0\uff0c\u7535\u8377\u5206\u5e03\u53d8\u4e3a\u4e8c\u7ef4\uff0c\u7535\u52bf\u964d\u4ece\u534a\u5bfc\u4f53\u8f6c\u79fb\u5230\u6c27\u5316\u5c42\uff0c\u7c7b\u4f3c\u5e73\u884c\u677f\u7535\u5bb9\u5668\u7684\u7279\u6027\u3002\u7814\u7a76\u8be6\u7ec6\u5206\u6790\u4e86\u8be5\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u8bc1\u660e\uff0c\u96a7\u7a7f\u7535\u6d41\u53d8\u5f97\u4e3b\u5bfc\uff0c\u53d6\u4ee3\u4e86\u7ecf\u5178\u53cd\u578b\u884c\u4e3a\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5668\u4ef6\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u6790\u57fa\u7840\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3MOSFET\u548c\u96a7\u7a7fFET\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u89e3\u51b31955\u5e74\u63d0\u51fa\u7684\u5173\u4e8e\u91d1\u5c5e-\u6c27\u5316\u7269-\u534a\u5bfc\u4f53\uff08MOS\uff09\u7ed3\u6784\u4e2d\u53cd\u578b\u5c42\u884c\u4e3a\u7684\u975e\u7ebf\u6027\u6cca\u677e\u65b9\u7a0b\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63a8\u5bfc\u4e86\u975e\u7ebf\u6027\u6cca\u677e\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u5f97\u5230\u4e86\u53cd\u578b\u5c42\u5bbd\u5ea6\u3001\u7535\u52bf\u548c\u7535\u8377\u5206\u5e03\u7b49\u7269\u7406\u91cf\u5173\u4e8e\u6805\u6781\u7535\u538b\u3001\u8ddd\u79bb\u548c\u63ba\u6742\u6d53\u5ea6\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\u3002\u5206\u6790\u4e86\u96a7\u7a7f\u673a\u5236\u5bf9\u53cd\u578b\u5c42\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u8bc1\u660e\u4e86\u96a7\u7a7f\u7535\u6d41\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "result": "\u5f97\u5230\u4e86\u53cd\u578b\u5c42\u5bbd\u5ea6\u3001\u7535\u52bf\u548c\u7535\u8377\u5206\u5e03\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u91cf\u5728\u8d85\u8fc7\u9608\u503c\u7535\u538b\u540e\u7684\u9971\u548c\u73b0\u8c61\u3002\u5206\u6790\u8868\u660e\uff0c\u96a7\u7a7f\u6548\u5e94\u5bfc\u81f4\u7535\u8377\u91cd\u65b0\u5206\u5e03\uff0c\u7535\u52bf\u964d\u8f6c\u79fb\u5230\u6c27\u5316\u5c42\uff0c\u7c7b\u4f3c\u4e8e\u5e73\u884c\u677f\u7535\u5bb9\u5668\u3002\u8ba1\u7b97\u7ed3\u679c\u8bc1\u5b9e\u96a7\u7a7f\u7535\u6d41\u5728\u53cd\u578b\u8fc7\u7a0b\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u8be5\u89e3\u6790\u89e3\u4e3a\u91cf\u5b50\u6548\u5e94\u5668\u4ef6\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3MOSFET\u548c\u96a7\u7a7fFET\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u96a7\u7a7f\u6548\u5e94\u5728MOS\u7ed3\u6784\u7684\u53cd\u578b\u8fc7\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u663e\u8457\u6539\u53d8\u4e86\u5668\u4ef6\u7684\u884c\u4e3a\u3002"}}
{"id": "2511.11638", "categories": ["cs.LG", "math.NA", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2511.11638", "abs": "https://arxiv.org/abs/2511.11638", "authors": ["Aamir Shehzad"], "title": "Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches", "comment": "32 pages, 19 figures This work investigates adaptive and conservative PINN frameworks for solving the RLW equation", "summary": "Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.", "AI": {"tldr": "PINN\u5728\u6c42\u89e3RLW\u65b9\u7a0b\u65f6\u5b58\u5728\u8bef\u5dee\uff0c\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94PINN\u548c\u4fdd\u5b88PINN\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660ePINN\u7684\u6709\u6548\u6027\u5177\u6709\u95ee\u9898\u7279\u5f02\u6027\u3002\u81ea\u9002\u5e94PINN\u5728\u5904\u7406\u590d\u6742\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\uff08\u5982\u53cc\u5b64\u5b50\u78b0\u649e\uff09\u65b9\u9762\u4f18\u4e8e\u4fdd\u5b88PINN\u548c\u6807\u51c6PINN\uff0c\u800c\u4fdd\u5b88PINN\u5728\u5904\u7406\u5b64\u5b50\u4f20\u64ad\u548c\u65e0\u6d6a\u9557\u6f14\u5316\u7b49\u957f\u671f\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5f3a\u5236\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u53ef\u80fd\u4f1a\u635f\u5bb3\u9ad8\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\uff0c\u9700\u8981\u7279\u6b8a\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u6539\u8fdb\u540e\u7684PINN\u65b9\u6cd5\u5728\u5404\u9879\u6d4b\u8bd5\u4e2d\u5747\u80fd\u8fbe\u5230$O(10^{-5})$\u7684\u7cbe\u5ea6\uff0c\u8bc1\u660e\u4e86PINN\u5728\u65e0\u7f51\u683c\u60c5\u51b5\u4e0b\u6c42\u89e3\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6f5c\u529b\u3002", "motivation": "\u6807\u51c6\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5728\u6c42\u89e3\u6b63\u5219\u5316\u957f\u6ce2\uff08RLW\uff09\u65b9\u7a0b\u65f6\u5b58\u5728\u8f83\u5927\u7684\u8bef\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdbPINN\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5176\u6c42\u89e3\u7cbe\u5ea6\u548c\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u7684PINN\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u5177\u6709\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u5f3a\u5236\u6267\u884c\u663e\u5f0f\u5b88\u6052\u5b9a\u5f8b\u7684\u4fdd\u5b88\u65b9\u6cd5\u3002\u901a\u8fc7\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5355\u5b64\u5b50\u4f20\u64ad\u3001\u53cc\u5b64\u5b50\u76f8\u4e92\u4f5c\u7528\u3001\u65e0\u6d6a\u9557\u6f14\u5316\uff09\u6765\u8bc4\u4f30\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u81ea\u9002\u5e94PINN\u5728\u89e3\u51b3\u590d\u6742\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u95ee\u9898\uff08\u5982\u53cc\u5b64\u5b50\u78b0\u649e\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4fdd\u5b88PINN\u548c\u6807\u51c6PINN\u3002\u4fdd\u5b88PINN\u5728\u89e3\u51b3\u6d89\u53ca\u5355\u5b64\u5b50\u957f\u671f\u884c\u4e3a\u548c\u65e0\u6d6a\u9557\u6f14\u5316\u7684\u95ee\u9898\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\u90fd\u80fd\u5728$O(10^{-5})$\u7684\u8bef\u5dee\u8303\u56f4\u5185\u83b7\u5f97\u7cbe\u786e\u89e3\uff0c\u8bc1\u660e\u4e86PINN\u7684\u65e0\u7f51\u683c\u6c42\u89e3\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f3a\u5236\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u53ef\u80fd\u5bf9\u9ad8\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "PINN\u7684\u6709\u6548\u6027\u662f\u95ee\u9898\u7279\u5f02\u6027\u7684\u3002\u5f3a\u5236\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u5e76\u975e\u603b\u662f\u80fd\u6539\u5584PINN\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u80fd\u635f\u5bb3\u9ad8\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u3002\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u7279\u5b9a\u95ee\u9898\u7684PINN\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.11633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11633", "abs": "https://arxiv.org/abs/2511.11633", "authors": ["Abhijeet Kumar", "Chetan Agarwal", "Pronoy B. Neogi", "Mayank Goswami"], "title": "Psychological stress during Examination and its estimation by handwriting in answer script", "comment": "10 Pages, 6 Figures and 1 Table", "summary": "This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u624b\u5199\u7b14\u8ff9\u7684\u5b57\u4f53\u7279\u5f81\u548c\u4eba\u5de5\u667a\u80fd\u6765\u91cf\u5316\u5b66\u751f\u7684\u5fc3\u7406\u538b\u529b\u6c34\u5e73\u3002", "motivation": "\u901a\u8fc7\u5206\u6790\u5b66\u751f\u624b\u5199\u7684\u8003\u8bd5\u8bd5\u5377\uff0c\u5229\u7528\u5b57\u4f53\u7279\u5f81\u548c\u4eba\u5de5\u667a\u80fd\u6765\u91cf\u5316\u5b66\u751f\u7684\u5fc3\u7406\u538b\u529b\u6c34\u5e73\u3002", "method": "\u5229\u7528\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u548c\u57fa\u4e8eTransformer\u7684\u611f\u60c5\u5206\u6790\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3001TrOCR\u548c\u57fa\u4e8eRoBERTa\u6a21\u578b\u7684\u611f\u60c5\u71b5\u878d\u5408\uff0c\u751f\u6210\u4e00\u4e2a\u6570\u5b57\u5316\u7684\u538b\u529b\u6307\u6570\u3002\u901a\u8fc7\u4e94\u6a21\u578b\u6295\u7968\u673a\u5236\u548c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6765\u5b9e\u73b0\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u5bf9\u8003\u8bd5\u671f\u95f4\u8ba4\u77e5\u548c\u60c5\u611f\u72b6\u6001\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5b66\u672f\u53d6\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b57\u4f53\u7279\u5f81\u3001\u4eba\u5de5\u667a\u80fd\u3001\u56fe\u50cf\u5904\u7406\u548c\u611f\u60c5\u5206\u6790\u6765\u91cf\u5316\u5b66\u751f\u7684\u5fc3\u7406\u538b\u529b\u6c34\u5e73\u3002"}}
{"id": "2511.11903", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.11903", "abs": "https://arxiv.org/abs/2511.11903", "authors": ["Jakub Garwo\u0142a", "Dvira Segal"], "title": "Compact cavity-dressed Hamiltonian framework at arbitrarily strong light-matter coupling", "comment": "19 pages, 10 figures", "summary": "We present a non-perturbative Hamiltonian mapping method for quantum systems strongly coupled to a quantized field mode (cavity), yielding compact closed-form representations of hybrid light-matter systems. The mapping method builds on an entangling transformation of photonic and atomic degrees of freedom. By truncating the resulting cavity-dressed Hamiltonian (CDH) to successively larger excitation sectors, we construct a series of compact models that converge to the exact limit, outpacing conventional approaches even in the challenging resonant and ultrastrong light-matter regime. The mapping principle also applies to multimode cavities coupled to matter through noncommuting operators and to leaky cavities. We benchmark the CDH framework on the quantum Rabi model, demonstrating accurate spectral predictions in both weak and strong coupling regimes, together with converging ground-state and thermal observables. We study the Dicke-Heisenberg lattice model and determine its phase diagram under resonant and strong light-matter coupling, achieving significant computational savings over brute-force simulations and identifying cavity-mediated spin correlations both analytically and numerically. The closed-form and compactness of the CDH provide both physical insight and enhanced computational efficiency, facilitating studies of strongly coupled hybrid light-matter systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u5fae\u6270\u54c8\u5bc6\u987f\u6620\u5c04\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5f3a\u8026\u5408\u4e8e\u91cf\u5316\u573a\u6a21\u5f0f\uff08\u8154\uff09\u7684\u91cf\u5b50\u7cfb\u7edf\uff0c\u4ee5\u7d27\u51d1\u7684\u95ed\u5f0f\u5f62\u5f0f\u8868\u793a\u5149-\u7269\u8d28\u6df7\u5408\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u5149\u5b50\u548c\u539f\u5b50\u81ea\u7531\u5ea6\u8fdb\u884c\u7ea0\u7f20\u53d8\u6362\uff0c\u5e76\u622a\u65ad\u7531\u6b64\u4ea7\u751f\u7684\u8154-\u7f00\u54c8\u5bc6\u987f\u91cf\uff08CDH\uff09\u5230\u66f4\u5927\u7684\u6fc0\u53d1\u6247\u533a\uff0c\u6784\u5efa\u4e86\u4e00\u7cfb\u5217\u6536\u655b\u4e8e\u7cbe\u786e\u6781\u9650\u7684\u7d27\u51d1\u6a21\u578b\uff0c\u5728\u5171\u632f\u548c\u8d85\u5f3a\u5149-\u7269\u8d28\u8026\u5408\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u5176\u6548\u7387\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002\u8be5\u6620\u5c04\u539f\u7406\u4e5f\u9002\u7528\u4e8e\u591a\u6a21\u8154\u901a\u8fc7\u975e\u5bf9\u6613\u7b97\u7b26\u4e0e\u7269\u8d28\u7684\u8026\u5408\u4ee5\u53ca\u8017\u6563\u8154\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5f3a\u8026\u5408\u5149-\u7269\u8d28\u6df7\u5408\u7cfb\u7edf\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u83b7\u5f97\u7d27\u51d1\u7684\u89e3\u6790\u8868\u8fbe\uff0c\u4ece\u800c\u4fc3\u8fdb\u5bf9\u8fd9\u7c7b\u7cfb\u7edf\u7684\u7406\u89e3\u548c\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u975e\u5fae\u6270\u54c8\u5bc6\u987f\u6620\u5c04\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u5149\u5b50\u548c\u539f\u5b50\u81ea\u7531\u5ea6\u7684\u7ea0\u7f20\u53d8\u6362\uff0c\u6784\u5efa\u8154-\u7f00\u54c8\u5bc6\u987f\u91cf\uff08CDH\uff09\u3002\u901a\u8fc7\u622a\u65adCDH\u5230\u4e0d\u540c\u7684\u6fc0\u53d1\u6247\u533a\uff0c\u5f97\u5230\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u5176\u6536\u655b\u6027\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5355\u6a21\u3001\u591a\u6a21\u3001\u975e\u5bf9\u6613\u7b97\u7b26\u8026\u5408\u4ee5\u53ca\u8017\u6563\u8154\u7cfb\u7edf\u3002\u901a\u8fc7\u5bf9\u91cf\u5b50Rabi\u6a21\u578b\u548cDicke-Heisenberg\u683c\u5b50\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86CDH\u6846\u67b6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u91cf\u5b50Rabi\u6a21\u578b\u4e2d\uff0cCDH\u6846\u67b6\u5728\u5f31\u8026\u5408\u548c\u5f3a\u8026\u5408\u4e24\u79cd\u60c5\u51b5\u4e0b\u90fd\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5149\u8c31\uff0c\u5e76\u4e14\u5728\u57fa\u6001\u548c\u70ed\u529b\u5b66\u53ef\u89c2\u6d4b\u91cf\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6536\u655b\u6027\u3002\u5728Dicke-Heisenberg\u683c\u5b50\u6a21\u578b\u4e2d\uff0c\u7814\u7a76\u786e\u5b9a\u4e86\u5171\u632f\u548c\u5f3a\u5149-\u7269\u8d28\u8026\u5408\u4e0b\u7684\u76f8\u56fe\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u66b4\u529b\u6a21\u62df\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u89e3\u6790\u548c\u6570\u503c\u5730\u8bc6\u522b\u8154\u4ecb\u5bfc\u7684\u81ea\u65cb\u76f8\u5173\u6027\u3002", "conclusion": "CDH\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u95ed\u5f0f\u8868\u793a\uff0c\u65e2\u80fd\u63d0\u4f9b\u7269\u7406\u6d1e\u89c1\uff0c\u53c8\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u7814\u7a76\u5f3a\u8026\u5408\u5149-\u7269\u8d28\u6df7\u5408\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2511.12714", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.12714", "abs": "https://arxiv.org/abs/2511.12714", "authors": ["George Z. Li", "Jason Li", "Satish Rao", "Junkai Zhang"], "title": "Shortcutting for Negative-Weight Shortest Path", "comment": "14-page STOC submission + new results in appendix", "summary": "Consider the single-source shortest paths problem on a directed graph with real-valued edge weights. We solve this problem in $O(n^{2.5}\\log^{4.5}n)$ time, improving on prior work of Fineman (STOC 2024) and Huang-Jin-Quanrud (SODA 2025, 2026) on dense graphs. Our main technique is an shortcutting procedure that iteratively reduces the number of negative-weight edges along shortest paths by a constant factor.", "AI": {"tldr": "\u5355\u6e90\u6700\u77ed\u8def\u5f84\u95ee\u9898\u5728\u5177\u6709\u5b9e\u503c\u8fb9\u6743\u91cdall\u7684\u5b9a\u5411\u56fe\u4e0a\u5f97\u5230\u4e86\u89e3\u51b3\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n^2.5 log^4.5 n)\uff0c\u4f18\u4e8e\u5148\u524d\u5728\u7a20\u5bc6\u56fe\u4e0a\u7684\u5de5\u4f5c\u3002", "motivation": "\u7814\u7a76\u5355\u6e90\u6700\u77ed\u8def\u5f84\u95ee\u9898\u5728\u5177\u6709\u5b9e\u503c\u8fb9\u6743\u91cdall\u7684\u5b9a\u5411\u56fe\u4e0a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e00\u79cd\u8fed\u4ee3\u7f29\u77ed\u7a0b\u5e8f\uff0c\u5c06\u8d1f\u6743\u8fb9\u6cbf\u6700\u77ed\u8def\u5f84\u7684\u6570\u91cf\u51cf\u5c11\u6052\u5b9a\u56e0\u5b50\u3002", "result": "\u5728\u7a20\u5bc6\u56fe\u4e0a\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n^2.5 log^4.5 n)\uff0c\u4f18\u4e8e Fineman (STOC 2024) \u548c Huang-Jin-Quanrud (SODA 2025, 2026) \u7684\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u5355\u6e90\u6700\u77ed\u8def\u5f84\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u7a20\u5bc6\u56fe\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.12106", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12106", "abs": "https://arxiv.org/abs/2511.12106", "authors": ["Hemant Kumar Gehlot", "Mohammad Shirzadi", "Junhao Gan", "Ahad N. Zehmakan"], "title": "Quantifying and Minimizing Perception Gap in Social Networks", "comment": null, "summary": "Social media has transformed global communication, yet its network structure can systematically distort perceptions through effects like the majority illusion and echo chambers. We introduce the perception gap index, a graph-based measure that quantifies local-global opinion divergence, which can be viewed as a generalization of the majority illusion to continuous settings. Using techniques from spectral graph theory, we demonstrate that higher connectivity makes networks more resilient to perception distortion. Our analysis of stochastic block models, however, shows that pronounced community structure increases vulnerability. We also study the problem of minimizing the perception gap via link recommendation with a fixed budget. We prove that this problem does not admit a polynomial-time algorithm for any bounded approximation ratio, unless P = NP. However, we propose a collection of efficient heuristic methods that have been demonstrated to produce near-optimal solutions on real-world network data.", "AI": {"tldr": "\u793e\u4ea4\u5a92\u4f53\u867d\u7136\u6539\u53d8\u4e86\u5168\u7403\u901a\u8baf\uff0c\u4f46\u5176\u7f51\u7edc\u7ed3\u6784\u53ef\u80fd\u901a\u8fc7\u591a\u6570\u5e7b\u89c9\u548c\u56de\u58f0\u5ba4\u6548\u5e94\u7b49\u626d\u66f2\u4eba\u4eec\u7684\u8ba4\u77e5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u611f\u77e5\u5dee\u8ddd\u6307\u6570\uff08perception gap index\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5ea6\u91cf\uff0c\u7528\u4e8e\u91cf\u5316\u5c40\u90e8-\u5168\u5c40\u610f\u89c1\u5dee\u5f02\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8fde\u7eed\u73af\u5883\u4e2d\u591a\u6570\u5e7b\u89c9\u7684\u63a8\u5e7f\u3002\u6211\u4eec\u5229\u7528\u8c31\u56fe\u7406\u8bba\u8bc1\u660e\uff0c\u66f4\u9ad8\u7684\u8fde\u901a\u6027\u4f7f\u7f51\u7edc\u66f4\u80fd\u62b5\u6297\u611f\u77e5\u626d\u66f2\u3002\u7136\u800c\uff0c\u6211\u4eec\u5bf9\u968f\u673a\u5757\u6a21\u578b\u7684\u5206\u6790\u8868\u660e\uff0c\u663e\u8457\u7684\u793e\u7fa4\u7ed3\u6784\u4f1a\u589e\u52a0\u7f51\u7edc\u7684\u8106\u5f31\u6027\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\uff0c\u901a\u8fc7\u94fe\u63a5\u63a8\u8350\u6765\u6700\u5c0f\u5316\u611f\u77e5\u5dee\u8ddd\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u9664\u975eP=NP\uff0c\u5426\u5219\u8be5\u95ee\u9898\u4e0d\u5b58\u5728\u80fd\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\u7684\u3001\u5177\u6709\u4efb\u610f\u6709\u754c\u8fd1\u4f3c\u6bd4\u7684\u7b97\u6cd5\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6709\u6548\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u7f51\u7edc\u6570\u636e\u4e0a\u8bc1\u660e\u4e86\u5b83\u4eec\u80fd\u4ea7\u751f\u8fd1\u4e4e\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7684\u7f51\u7edc\u7ed3\u6784\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u626d\u66f2\uff0c\u4f8b\u5982\u591a\u6570\u5e7b\u89c9\u548c\u56de\u58f0\u5ba4\u6548\u5e94\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6211\u4eec\u5bf9\u5168\u7403\u4fe1\u606f\u7684\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u91cf\u5316\u548c\u7406\u89e3\u8fd9\u79cd\u8ba4\u77e5\u626d\u66f2\u7684\u7a0b\u5ea6\uff0c\u5e76\u63a2\u7d22\u51cf\u5c11\u5b83\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u611f\u77e5\u5dee\u8ddd\u6307\u6570\uff08perception gap index\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5ea6\u91cf\uff0c\u7528\u4e8e\u91cf\u5316\u5c40\u90e8\u610f\u89c1\u4e0e\u5168\u5c40\u610f\u89c1\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u7814\u7a76\u5229\u7528\u4e86\u8c31\u56fe\u7406\u8bba\u6765\u5206\u6790\u7f51\u7edc\u8fde\u901a\u6027\u5bf9\u611f\u77e5\u626d\u66f2\u7684\u62b5\u6297\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u968f\u673a\u5757\u6a21\u578b\u6765\u7814\u7a76\u793e\u7fa4\u7ed3\u6784\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u5728\u6709\u9650\u9884\u7b97\u4e0b\uff0c\u901a\u8fc7\u94fe\u63a5\u63a8\u8350\u6700\u5c0f\u5316\u611f\u77e5\u5dee\u8ddd\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3NP\u96be\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u9ad8\u7684\u7f51\u7edc\u8fde\u901a\u6027\u53ef\u4ee5\u589e\u5f3a\u7f51\u7edc\u62b5\u6297\u611f\u77e5\u626d\u66f2\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u663e\u8457\u7684\u793e\u7fa4\u7ed3\u6784\u4f1a\u589e\u52a0\u7f51\u7edc\u7684\u8106\u5f31\u6027\u3002\u5728\u6700\u5c0f\u5316\u611f\u77e5\u5dee\u8ddd\u7684\u95ee\u9898\u4e0a\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u8ba1\u7b97\u7684\u56f0\u96be\u6027\uff08NP\u96be\u9898\uff09\uff0c\u4f46\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6709\u6548\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u7f51\u7edc\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5f97\u5230\u8fd1\u4e4e\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u611f\u77e5\u5dee\u8ddd\u6307\u6570\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u91cf\u5316\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u8ba4\u77e5\u626d\u66f2\u3002\u867d\u7136\u7f51\u7edc\u8fde\u901a\u6027\u6709\u52a9\u4e8e\u51cf\u5c11\u8fd9\u79cd\u626d\u66f2\uff0c\u4f46\u793e\u7fa4\u7ed3\u6784\u53ef\u80fd\u4f1a\u52a0\u5267\u5b83\u3002\u6700\u5c0f\u5316\u611f\u77e5\u5dee\u8ddd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u4f46\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e3a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u89e3\u51b3\u8be5\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2511.13245", "categories": ["cs.LO", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13245", "abs": "https://arxiv.org/abs/2511.13245", "authors": ["Matt Luckcuck", "Maike Schwammberger", "Mengwei Xu"], "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems", "comment": null, "summary": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. \n  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.", "AI": {"tldr": "\u672c EPTCS \u4f53\u79ef\u5305\u542b\u7b2c\u4e03\u5c4a\u81ea\u52a8\u5316\u7cfb\u7edf\u5f62\u5f0f\u5316\u65b9\u6cd5\u56fd\u9645\u7814\u8ba8\u4f1a (FMAS 2025) \u7684\u8bba\u6587\uff0c\u8be5\u7814\u8ba8\u4f1a\u4e8e 2025 \u5e74 11 \u6708 17 \u65e5\u81f3 19 \u65e5\u4e3e\u884c\u3002FMAS \u7814\u8ba8\u4f1a\u7cfb\u5217\u65e8\u5728\u6c47\u96c6\u81f4\u529b\u4e8e\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u7814\u7a76\u4eba\u5458\uff0c\u4ee5\u4fbf\u4ed6\u4eec\u80fd\u591f\u4e0e\u65e5\u76ca\u58ee\u5927\u7684\u7814\u7a76\u754c\u53d1\u8868\u548c\u8ba8\u8bba\u4ed6\u4eec\u7684\u5de5\u4f5c\u3002FMAS 2025 \u4e0e\u7b2c 20 \u5c4a\u96c6\u6210\u5f62\u5f0f\u5316\u65b9\u6cd5\u56fd\u9645\u4f1a\u8bae (iFM'25) \u540c\u5730\u4e3e\u884c\uff0c\u7531\u6cd5\u56fd Inria Paris \u5728 Inria Paris \u4e2d\u5fc3\u4e3b\u529e\u3002", "motivation": "FMAS \u7814\u8ba8\u4f1a\u7cfb\u5217\u65e8\u5728\u6c47\u96c6\u81f4\u529b\u4e8e\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u7814\u7a76\u4eba\u5458\uff0c\u4ee5\u4fbf\u4ed6\u4eec\u80fd\u591f\u4e0e\u65e5\u76ca\u58ee\u5927\u7684\u7814\u7a76\u754c\u53d1\u8868\u548c\u8ba8\u8bba\u4ed6\u4eec\u7684\u5de5\u4f5c\u3002", "method": "\u672c EPTCS \u4f53\u79ef\u5305\u542b\u7b2c\u4e03\u5c4a\u81ea\u52a8\u5316\u7cfb\u7edf\u5f62\u5f0f\u5316\u65b9\u6cd5\u56fd\u9645\u7814\u8ba8\u4f1a (FMAS 2025) \u7684\u8bba\u6587\uff0c\u8be5\u7814\u8ba8\u4f1a\u4e8e 2025 \u5e74 11 \u6708 17 \u65e5\u81f3 19 \u65e5\u4e3e\u884c\u3002FMAS 2025 \u5171\u6536\u5230\u6765\u81ea\u52a0\u62ff\u5927\u3001\u4e2d\u56fd\u3001\u6cd5\u56fd\u3001\u5fb7\u56fd\u3001\u7231\u5c14\u5170\u3001\u610f\u5927\u5229\u3001\u65e5\u672c\u3001\u8377\u5170\u3001\u8461\u8404\u7259\u3001\u745e\u5178\u3001\u7f8e\u56fd\u548c\u82f1\u56fd\u673a\u6784\u7684 16 \u7bc7\u6295\u7a3f\u3002", "result": "FMAS 2025 \u5171\u6536\u5230\u6765\u81ea\u52a0\u62ff\u5927\u3001\u4e2d\u56fd\u3001\u6cd5\u56fd\u3001\u5fb7\u56fd\u3001\u7231\u5c14\u5170\u3001\u610f\u5927\u5229\u3001\u65e5\u672c\u3001\u8377\u5170\u3001\u8461\u8404\u7259\u3001\u745e\u5178\u3001\u7f8e\u56fd\u548c\u82f1\u56fd\u673a\u6784\u7684 16 \u7bc7\u6295\u7a3f\u3002\u5c3d\u7ba1\u6295\u7a3f\u6570\u91cf\u5c11\u4e8e\u53bb\u5e74\uff0c\u4f46\u6295\u7a3f\u6765\u81ea\u591a\u4e2a\u56fd\u5bb6\uff0c\u8fd9\u4ee4\u4eba\u9f13\u821e\u3002\u6295\u7a3f\u4f5c\u8005\u65e2\u6709 FMAS \u7684\u8001\u4f5c\u8005\uff0c\u4e5f\u6709\u65b0\u4f5c\u8005\uff0c\u8fd9\u8868\u660e\u73b0\u6709\u7684\u793e\u533a\u8d5e\u8d4f FMAS 7 \u5e74\u6765\u5efa\u7acb\u7684\u7f51\u7edc\uff0c\u800c\u65b0\u4f5c\u8005\u4e5f\u8868\u660e FMAS \u793e\u533a\u5177\u6709\u5de8\u5927\u7684\u589e\u957f\u6f5c\u529b\u3002", "conclusion": "FMAS 2025 \u5438\u5f15\u4e86\u6765\u81ea\u591a\u4e2a\u56fd\u5bb6\u7684\u6295\u7a3f\uff0c\u5e76\u4e14\u4f5c\u8005\u7fa4\u4f53\u65e2\u6709\u8001\u6210\u5458\u4e5f\u6709\u65b0\u6210\u5458\uff0c\u8fd9\u8868\u660e\u4e86\u793e\u533a\u7684\u7a33\u5b9a\u6027\u548c\u589e\u957f\u6f5c\u529b\u3002"}}
{"id": "2511.11789", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11789", "abs": "https://arxiv.org/abs/2511.11789", "authors": ["Jiayi Li", "Xiao Liu", "Yansong Feng"], "title": "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions", "comment": "AAAI-2026", "summary": "Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.11876", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.11876", "abs": "https://arxiv.org/abs/2511.11876", "authors": ["Jackson Saunders", "Camelia Prodan"], "title": "Acoustic Metamaterials with Positive and Negative Couplings: Modular and One Piece Architectures for Topological Models", "comment": null, "summary": "We describe two 3D-printing approaches for realizing tight-binding models in acoustic metamaterials using H-shaped resonators: a modular system with tunable interconnections and an integrated one-piece design for reducing dissipation. The platform supports both positive and negative coupling through geometric control, enabling accurate acoustic analogs of topological models. By tuning the coupling length (CL), we eliminate detuning effects and preserve particle-hole symmetry. We further quantify the influence of the Total Coupling Area (TCA) on band topology and derive conditions for constant-area coupling. The system was tested on SSH and Kitaev chains, revealing midgap edge and interface states, confirming topological behavior in both configurations.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd3D\u6253\u5370\u58f0\u5b66\u8d85\u6750\u6599\u7d27\u675f\u7f1a\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e00\u79cd\u662f\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53e6\u4e00\u79cd\u662f\u96c6\u6210\u5f0f\u8bbe\u8ba1\u3002", "motivation": "\u5b9e\u73b0\u58f0\u5b66\u8d85\u6750\u6599\u4e2d\u7684\u7d27\u675f\u7f1a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u63a7\u5236\u5b9e\u73b0\u6b63\u8d1f\u8026\u5408\uff0c\u4ee5\u7cbe\u786e\u6a21\u62df\u62d3\u6251\u6a21\u578b\u3002", "method": "\u91c7\u75283D\u6253\u5370\u6280\u672f\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u96c6\u6210\u5f0f\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u8026\u5408\u957f\u5ea6\uff08CL\uff09\u548c\u603b\u8026\u5408\u9762\u79ef\uff08TCA\uff09\u6765\u63a7\u5236\u6a21\u578b\u3002", "result": "\u5728SSH\u548cKitaev\u94fe\u6a21\u578b\u4e2d\u53d1\u73b0\u4e86\u4e2d\u5e26\u8fb9\u7f18\u548c\u754c\u9762\u6001\uff0c\u9a8c\u8bc1\u4e86\u62d3\u6251\u884c\u4e3a\u3002", "conclusion": "\u6240\u63d0\u51fa\u76843D\u6253\u5370\u5e73\u53f0\u80fd\u591f\u7cbe\u786e\u5b9e\u73b0\u58f0\u5b66\u7d27\u675f\u7f1a\u6a21\u578b\uff0c\u5e76\u53ef\u7528\u4e8e\u7814\u7a76\u62d3\u6251\u7269\u7406\u73b0\u8c61\u3002"}}
{"id": "2511.11771", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "cond-mat.stat-mech", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11771", "abs": "https://arxiv.org/abs/2511.11771", "authors": ["Riccardo Fantoni"], "title": "Edwards Localization", "comment": "7 pages, 2 figures", "summary": "We study the localization problem in quantum stochastic mechanics. We start from the Edwards model for a particle in a bath of scattering centers and prove static localization of the ground state wavefunction of the particle in a one dimensional square well coupled to Dirac delta like scattering centers in arbitrary but fixed positions. We see how the localization increases for increasing coupling $g$. Then we choose the scattering centers positions as pseudo random numbers with a uniform probability distribution and observe an increase in the localization of the average of the ground state over the many positions realizations. We discuss how this averaging procedure is consistent with a picture of a particle in a Bose-Einstein condensate of of non interacting boson scattering centers interacting with the particle with Dirac delta functions pair potential. We then study the dynamics of the ground state wave function. We conclude with a discussion of the affine quantization version of the Lax model which reduces to a system of contiguous square wells with walls in arbitrary positions independently of the coupling constant $g$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u968f\u673a\u529b\u5b66\u4e2d\u7684\u5c40\u57df\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u4e00\u7ef4\u65b9\u52bf\u9631\u4e2d\uff0c\u7c92\u5b50\u5728\u4efb\u610f\u4f46\u56fa\u5b9a\u4f4d\u7f6e\u7684\u72c4\u62c9\u514b\u03b4\u6563\u5c04\u4e2d\u5fc3\u4f5c\u7528\u4e0b\u7684\u57fa\u6001\u6ce2\u51fd\u6570\u5b58\u5728\u9759\u6001\u5c40\u57df\u5316\uff0c\u5e76\u4e14\u5c40\u57df\u5316\u7a0b\u5ea6\u968f\u8026\u5408\u5e38\u6570g\u7684\u589e\u52a0\u800c\u589e\u5f3a\u3002\u5f53\u6563\u5c04\u4e2d\u5fc3\u4f4d\u7f6e\u53d6\u4e3a\u5747\u5300\u5206\u5e03\u7684\u4f2a\u968f\u673a\u6570\u65f6\uff0c\u57fa\u6001\u7684\u5e73\u5747\u503c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5c40\u57df\u5316\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u8be5\u5e73\u5747\u8fc7\u7a0b\u4e0e\u7c92\u5b50\u5728\u975e\u76f8\u4e92\u4f5c\u7528\u73bb\u8272-\u7231\u56e0\u65af\u5766\u51dd\u805a\u4e2d\u7684\u56fe\u50cf\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u57fa\u6001\u6ce2\u51fd\u6570\u52a8\u529b\u5b66\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86Lax\u6a21\u578b\u7684\u4eff\u5c04\u91cf\u5b50\u5316\u7248\u672c\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u5c40\u57df\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6563\u5c04\u4e2d\u5fc3\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4eceEdwards\u6a21\u578b\u51fa\u53d1\uff0c\u8bc1\u660e\u4e86\u4e00\u7ef4\u65b9\u52bf\u9631\u4e2d\u7c92\u5b50\u5728\u72c4\u62c9\u514b\u03b4\u6563\u5c04\u4e2d\u5fc3\u4f5c\u7528\u4e0b\u7684\u57fa\u6001\u6ce2\u51fd\u6570\u7684\u9759\u6001\u5c40\u57df\u5316\uff0c\u5e76\u5206\u6790\u4e86\u8026\u5408\u5e38\u6570g\u548c\u6563\u5c04\u4e2d\u5fc3\u4f4d\u7f6e\u5bf9\u5c40\u57df\u5316\u7684\u5f71\u54cd\u3002\u8003\u8651\u4e86\u6563\u5c04\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u4f2a\u968f\u673a\u5206\u5e03\u60c5\u51b5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4e0e\u73bb\u8272-\u7231\u56e0\u65af\u5766\u51dd\u805a\u7684\u8054\u7cfb\u3002\u7814\u7a76\u4e86\u57fa\u6001\u6ce2\u51fd\u6570\u7684\u52a8\u529b\u5b66\uff0c\u5e76\u8ba8\u8bba\u4e86Lax\u6a21\u578b\u7684\u4eff\u5c04\u91cf\u5b50\u5316\u7248\u672c\u3002", "result": "\u8bc1\u660e\u4e86\u57fa\u6001\u6ce2\u51fd\u6570\u5b58\u5728\u9759\u6001\u5c40\u57df\u5316\uff0c\u5c40\u57df\u5316\u7a0b\u5ea6\u968f\u8026\u5408\u5e38\u6570g\u589e\u52a0\u800c\u589e\u5f3a\u3002\u6563\u5c04\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u4f2a\u968f\u673a\u5206\u5e03\u589e\u52a0\u4e86\u5e73\u5747\u57fa\u6001\u7684\u5c40\u57df\u5316\u3002\u53d1\u73b0\u4e86\u4eff\u5c04\u91cf\u5b50\u5316\u7248\u672c\u7684Lax\u6a21\u578b\u4e0e\u8026\u5408\u5e38\u6570g\u65e0\u5173\u3002", "conclusion": "\u91cf\u5b50\u968f\u673a\u529b\u5b66\u4e2d\u7684\u5c40\u57df\u5316\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7Edwards\u6a21\u578b\u5f97\u5230\u89e3\u91ca\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u53c2\u6570\uff08\u5982\u8026\u5408\u5e38\u6570\u548c\u6563\u5c04\u4e2d\u5fc3\u4f4d\u7f6e\uff09\u6765\u63a7\u5236\u5c40\u57df\u5316\u7a0b\u5ea6\u3002\u4eff\u5c04\u91cf\u5b50\u5316\u7248\u672c\u7684Lax\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u8026\u5408\u5e38\u6570\u65e0\u5173\u7684\u7b80\u5316\u6a21\u578b\u3002"}}
{"id": "2511.11854", "categories": ["cs.MA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.11854", "abs": "https://arxiv.org/abs/2511.11854", "authors": ["Vahid Hemmati", "Yonas Ayalew", "Ahmad Mohammadi", "Reza Ahmari", "Parham Kebria", "Abdollah Homaifar", "Mehrdad Saif"], "title": "Conflict-Free Flight Scheduling Using Strategic Demand Capacity Balancing for Urban Air Mobility Operations", "comment": null, "summary": "In this paper, we propose a conflict-free multi- agent flight scheduling that ensures robust separation in con- strained airspace for Urban Air Mobility (UAM) operations application. First, we introduce Pairwise Conflict Avoidance (PCA) based on delayed departures, leveraging kinematic principles to maintain safe distances. Next, we expand PCA to multi-agent scenarios, formulating an optimization approach that systematically determines departure times under increasing traffic densities. Performance metrics, such as average delay, assess the effectiveness of our solution. Through numerical simulations across diverse multi-agent environments and real- world UAM use cases, our method demonstrates a significant reduction in total delay while ensuring collision-free operations. This approach provides a scalable framework for emerging urban air mobility systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u51b2\u7a81 free \u7684\u591a\u667a\u80fd\u4f53\u98de\u884c\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ef6\u8fdf\u8d77\u98de\u6765\u786e\u4fdd\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u7684\u8fd0\u884c\u5b89\u5168\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u5e94\u7528\u4e2d\u7ea6\u675f\u7a7a\u57df\u5185\u9c81\u68d2\u95f4\u9694\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\uff0c\u63d0\u51fa\u57fa\u4e8e\u5ef6\u8fdf\u8d77\u98de\u7684\u6210\u5bf9\u51b2\u7a81\u907f\u514d\uff08PCA\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u8fd0\u52a8\u5b66\u539f\u7406\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u3002\u7136\u540e\uff0c\u5c06PCA\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u7cfb\u7edf\u5730\u786e\u5b9a\u8d77\u98de\u65f6\u95f4\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u4ea4\u901a\u5bc6\u5ea6\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u591a\u667a\u80fd\u4f53\u73af\u5883\u548c\u5b9e\u9645UAM\u7528\u4f8b\u4e2d\u7684\u6570\u503c\u6a21\u62df\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u603b\u5ef6\u8fdf\uff0c\u5e76\u786e\u4fdd\u4e86\u65e0\u78b0\u649e\u8fd0\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65b0\u5174\u7684\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2511.11643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11643", "abs": "https://arxiv.org/abs/2511.11643", "authors": ["Aswath Muthuselvam", "Jeevak Raj S", "Mohanaprasad K"], "title": "Real-time pothole detection with onboard sensors and camera on vehicles", "comment": null, "summary": "Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes", "AI": {"tldr": "\u5229\u7528\u8f66\u8f7d\u4f20\u611f\u5668\u548cSVM\u5206\u7c7b\u5668\u5b9e\u73b0\u8def\u9762\u5751\u6d3c\u7684\u5b9e\u65f6\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe98.1%\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u6570\u91cf\u7684\u589e\u52a0\uff0c\u9700\u8981\u9891\u7e41\u76d1\u6d4b\u8def\u51b5\u4ee5\u786e\u4fdd\u4ea4\u901a\u987a\u7545\uff0c\u5c24\u5176\u662f\u8bc6\u522b\u9053\u8def\u88c2\u7f1d\u548c\u5751\u6d3c\u3002", "method": "\u4f7f\u7528\u8f66\u8f7d\u4f20\u611f\u5668\u6536\u96c6\u6570\u636e\uff0c\u5e76\u5229\u7528\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u5206\u7c7b\u5668\u6765\u68c0\u6d4b\u5751\u6d3c\u3002", "result": "\u5728\u6536\u96c6\u5230\u76842\u516c\u91cc\u9053\u8def\u6570\u636e\u4e2d\uff0c\u6210\u529f\u8bc6\u522b\u51fa26\u4e2a\u5751\u6d3c\uff0c\u51c6\u786e\u7387\u8fbe\u523098.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5b9e\u65f6\u68c0\u6d4b\u8def\u9762\u5751\u6d3c\uff0c\u4e3a\u5927\u89c4\u6a21\u9053\u8def\u7ba1\u7406\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2511.11848", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2511.11848", "abs": "https://arxiv.org/abs/2511.11848", "authors": ["Denis V. Saklakov"], "title": "Phase-Coded Memory and Morphological Resonance: A Next-Generation Retrieval-Augmented Generator Architecture", "comment": "24 pages, 2 diagrams, conceptual white paper for cognitive AI and memory architecture research. Primary category cs.AI; secondary cs.NE and q-bio.NC", "summary": "This paper introduces a cognitive Retrieval-Augmented Generator (RAG) architecture that transcends transformer context-length limitations through phase-coded memory and morphological-semantic resonance. Instead of token embeddings, the system encodes meaning as complex wave patterns with amplitude-phase structure. A three-tier design is presented: a Morphological Mapper that transforms inputs into semantic waveforms, a Field Memory Layer that stores knowledge as distributed holographic traces and retrieves it via phase interference, and a Non-Contextual Generator that produces coherent output guided by resonance rather than fixed context. This approach eliminates sequential token dependence, greatly reduces memory and computational overhead, and enables unlimited effective context through frequency-based semantic access. The paper outlines theoretical foundations, pseudocode implementation, and experimental evidence from related complex-valued neural models, emphasizing substantial energy, storage, and time savings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f4d\u7f16\u7801\u8bb0\u5fc6\u548c\u5f62\u6001-\u8bed\u4e49\u5171\u632f\u6765\u7a81\u7834Transformer\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3002\u8be5\u6a21\u578b\u5c06\u8f93\u5165\u7f16\u7801\u4e3a\u5177\u6709\u5e45\u5ea6\u548c\u76f8\u4f4d\u7ed3\u6784\u7684\u590d\u6742\u6ce2\u5f62\uff0c\u5e76\u4f7f\u7528\u4e09\u5c42\u8bbe\u8ba1\uff1a\u5f62\u6001\u6620\u5c04\u5668\u3001\u573a\u8bb0\u5fc6\u5c42\u548c\u975e\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u3002\u8fd9\u79cd\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u987a\u5e8f\u4ee4\u724c\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u9891\u7387\u7684\u8bed\u4e49\u8bbf\u95ee\u5b9e\u73b0\u4e86\u65e0\u9650\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u3002", "motivation": "Transformer\u6a21\u578b\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6765\u5904\u7406\u957f\u5e8f\u5217\u548c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5RAG\u67b6\u6784\uff0c\u4f7f\u7528\u76f8\u4f4d\u7f16\u7801\u8bb0\u5fc6\u548c\u5f62\u6001-\u8bed\u4e49\u5171\u632f\u3002\u5176\u4e09\u5c42\u8bbe\u8ba1\u5305\u62ec\uff1a1. \u5f62\u6001\u6620\u5c04\u5668\uff1a\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u4e49\u6ce2\u5f62\u30022. \u573a\u8bb0\u5fc6\u5c42\uff1a\u4ee5\u5206\u5e03\u5f0f\u5168\u606f\u75d5\u8ff9\u7684\u5f62\u5f0f\u5b58\u50a8\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u76f8\u4f4d\u5e72\u6270\u68c0\u7d22\u30023. \u975e\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff1a\u901a\u8fc7\u5171\u632f\u751f\u6210\u8fde\u8d2f\u7684\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u987a\u5e8f\u4ee4\u724c\u7684\u4f9d\u8d56\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u9891\u7387\u7684\u8bed\u4e49\u8bbf\u95ee\u5b9e\u73b0\u4e86\u65e0\u9650\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u80fd\u91cf\u3001\u5b58\u50a8\u548c\u65f6\u95f4\u65b9\u9762\u7684\u8282\u7701\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8ba4\u77e5RAG\u6a21\u578b\u901a\u8fc7\u5176\u65b0\u9896\u7684\u5185\u5b58\u548c\u751f\u6210\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709Transformer\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11941", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2511.11941", "abs": "https://arxiv.org/abs/2511.11941", "authors": ["Delmar G. A. Cabral", "Brandon Allen", "Fabijan Pavo\u0161evi\u0107", "Sharon Hammes-Schiffer", "Pablo D\u00edez-Valle", "Jack S. Baker", "Gaurav Saxena", "Thi Ha Kyaw", "Victor S. Batista"], "title": "Error-Mitigation Enabled Multicomponent Quantum Simulations Beyond the Born-Oppenheimer Approximation", "comment": "24 pages, 5 figures", "summary": "We introduce a multicomponent unitary coupled cluster framework for quantum simulations of molecular systems that incorporate both electronic and nuclear quantum effects beyond the Born-Oppenheimer approximation. Using the nuclear-electronic orbital formalism, we construct mcUCC ans\u00e4tze for positronium hydride and molecular hydrogen with a quantum proton, and analyze hardware requirements for different excitation truncations. To further reduce resource costs effectively, we employ the local unitary cluster Jastrow ansatz and implement it experimentally on IBM Q's Heron superconducting hardware. With the Physics-Inspired Extrapolation error mitigation protocol, the computed ground-state energies remain within chemical accuracy, consistent with the stated uncertainty level. These results provide the first demonstration of error-mitigated multicomponent correlated simulations on quantum hardware and outline a path toward scalable algorithms unifying electronic and nuclear degrees of freedom.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u7ec4\u5206\u5e7a\u6b63\u8026\u5408\u7c07\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6a21\u62df\u5305\u542b\u6838-\u7535\u5b50\u91cf\u5b50\u6548\u5e94\u7684\u5206\u5b50\u7cfb\u7edf\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6c22\u5316\u6b63\u7535\u5b50\u9393\u548c\u542b\u91cf\u5b50\u8d28\u5b50\u7684\u5206\u5b50\u6c22\uff0c\u540c\u65f6\u8003\u8651\u4e86\u8bef\u5dee\u7f13\u89e3\u3002", "motivation": "\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6a21\u62df\u5305\u542b\u6838-\u7535\u5b50\u91cf\u5b50\u6548\u5e94\u7684\u5206\u5b50\u7cfb\u7edf\uff0c\u4ee5\u8d85\u8d8a\u7edd\u70ed\u8fd1\u4f3c\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u7ec4\u5206\u5e7a\u6b63\u8026\u5408\u7c07\uff08mcUCC\uff09\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u6838-\u7535\u5b50\u8f68\u9053\uff08NEO\uff09\u5f62\u5f0f\u4e3b\u4e49\u3002\u4f7f\u7528\u4e86\u5c40\u57df\u5e7a\u6b63\u7c07Jastrow\uff08LUcJ\uff09ansatz\u6765\u964d\u4f4e\u8d44\u6e90\u6210\u672c\uff0c\u5e76\u5728IBM Q\u7684Heron\u91cf\u5b50\u786c\u4ef6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5e94\u7528\u4e86\u53d7\u7269\u7406\u542f\u53d1\u7684\u5916\u63d2\uff08PIE\uff09\u8bef\u5dee\u7f13\u89e3\u534f\u8bae\u3002", "result": "\u5728\u91cf\u5b50\u786c\u4ef6\u4e0a\u6210\u529f\u6f14\u793a\u4e86\u8bef\u5dee\u7f13\u89e3\u7684\u591a\u7ec4\u5206\u76f8\u5173\u6a21\u62df\uff0c\u8ba1\u7b97\u51fa\u7684\u57fa\u6001\u80fd\u91cf\u5728\u5316\u5b66\u7cbe\u5ea6\u8303\u56f4\u5185\uff0c\u4e0e\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u73b0\u7edf\u4e00\u7535\u5b50\u548c\u6838\u81ea\u7531\u5ea6\u7684\u53ef\u6269\u5c55\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002"}}
{"id": "2511.12854", "categories": ["cs.DS", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12854", "abs": "https://arxiv.org/abs/2511.12854", "authors": ["Alexander Lindermayr", "Kirk Pruhs", "Andr\u00e9a W. Richa", "Tegan Wilson"], "title": "Indirect Coflow Scheduling", "comment": null, "summary": "We consider routing in reconfigurable networks, which is also known as coflow scheduling in the literature. The algorithmic literature generally (perhaps implicitly) assumes that the amount of data to be transferred is large. Thus the standard way to model a collection of requested data transfers is by an integer demand matrix $D$, where the entry in row $i$ and column $j$ of $D$ is an integer representing the amount of information that the application wants to send from machine/node $i$ to machine/node $j$. A feasible coflow schedule is then a sequence of matchings, which represent the sequence of data transfers that covers $D$. In this work, we investigate coflow scheduling when the size of some of the requested data transfers may be small relative to the amount of data that can be transferred in one round. fractional matchings and/or that employ indirect routing, and compare the relative utility of these options. We design algorithms that perform much better for small demands than the algorithms in the literature that were designed for large data transfers.", "AI": {"tldr": "The paper studies coflow scheduling in reconfigurable networks, focusing on scenarios with small data transfer sizes, and designs algorithms that outperform existing methods designed for large transfers.", "motivation": "The existing literature on coflow scheduling generally assumes large data transfer amounts, which may not be suitable for scenarios with smaller data transfers. This work aims to address this gap.", "method": "The paper investigates the use of fractional matchings and/or indirect routing to handle coflow scheduling with small data transfer sizes and designs new algorithms tailored for these scenarios.", "result": "The designed algorithms perform significantly better for small data transfers compared to existing algorithms intended for large data transfers.", "conclusion": "The study highlights the ineffectiveness of traditional large-data-centric coflow scheduling algorithms when dealing with small data transfers and proposes improved algorithms for such cases."}}
{"id": "2511.12393", "categories": ["cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12393", "abs": "https://arxiv.org/abs/2511.12393", "authors": ["Nicolo' Pagan", "Andreas Philippou", "Giulia De Pasquale"], "title": "Learning to Control Misinformation: a Closed-loop Approach for Misinformation Mitigation over Social Networks", "comment": null, "summary": "Modern social networks rely on recommender systems that inadvertently amplify misinformation by prioritizing engagement over content veracity. We present a control framework that mitigates misinformation spread while maintaining user engagement by penalizing content characteristics commonly exploited by false information, specifically, extreme negative sentiment and novelty. We extend the closed-loop Friedkin-Johnsen model to incorporate the mitigation of misinformation together with the maximization of user engagement. Both model-free and model-based control strategies demonstrate up to 76% reduction in misinformation propagation across diverse network configurations, validated through simulations using the LIAR2 dataset with sentiment features extracted via large language models. Analysis of engagement-misinformation trade-offs reveals that in networks with radical users, median engagement improves even as misinformation decreases, suggesting content moderation enhances discourse quality for non-extremist users. The framework provides practical guidance for platform operators in balancing misinformation suppression with engagement objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u8d1f\u9762\u60c5\u7eea\u548c\u65b0\u9896\u6027\u7b49\u6613\u88ab\u865a\u5047\u4fe1\u606f\u5229\u7528\u7684\u5185\u5bb9\u7279\u5f81\uff0c\u5728\u964d\u4f4e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u7684\u540c\u65f6\u4fdd\u6301\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u4f18\u5148\u8003\u8651\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u65e0\u610f\u4e2d\u653e\u5927\u4e86\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u3002", "method": "\u6269\u5c55\u4e86Friedkin-Johnsen\u6a21\u578b\uff0c\u7eb3\u5165\u4e86\u51cf\u5c11\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u6700\u5927\u5316\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7LIAR2\u6570\u636e\u96c6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u60c5\u611f\u7279\u5f81\u8fdb\u884c\u4e86\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u5728\u4e0d\u540c\u7684\u7f51\u7edc\u914d\u7f6e\u4e2d\uff0c\u6a21\u578b\u65e0\u5173\u548c\u6a21\u578b\u4e24\u7c7b\u63a7\u5236\u7b56\u7565\u5747\u80fd\u51cf\u5c11\u9ad8\u8fbe76%\u7684\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u3002\u5728\u5b58\u5728\u6fc0\u8fdb\u7528\u6237\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u865a\u5047\u4fe1\u606f\u51cf\u5c11\uff0c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e5f\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5e73\u8861\u865a\u5047\u4fe1\u606f\u6291\u5236\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u76ee\u6807\uff0c\u4e3a\u5e73\u53f0\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\uff0c\u5c24\u5176\u662f\u5728\u6fc0\u8fdb\u7528\u6237\u5360\u6bd4\u8f83\u9ad8\u7684\u7f51\u7edc\u4e2d\uff0c\u5185\u5bb9\u5ba1\u6838\u53ef\u4ee5\u63d0\u9ad8\u975e\u6781\u7aef\u7528\u6237\u7684\u8ba8\u8bba\u8d28\u91cf\u3002"}}
{"id": "2511.13460", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2511.13460", "abs": "https://arxiv.org/abs/2511.13460", "authors": ["Pedro R. D'Argenio", "Arnd Hartmanns", "Patrick Wienh\u00f6ft", "Mark van Wijk"], "title": "Multi-Objective Statistical Model Checking using Lightweight Strategy Sampling (extended version)", "comment": null, "summary": "Statistical model checking delivers quantitative verification results with statistical guarantees by applying Monte Carlo simulation to formal models. It scales to model sizes and model types that are out of reach for exhaustive, analytical techniques. So far, it has been used to evaluate one property value at a time only. Many practical problems, however, require finding the Pareto front of optimal tradeoffs between multiple possibly conflicting optimisation objectives. In this paper, we present the first statistical model checking approach for such multi-objective Pareto queries, using lightweight strategy sampling to optimise over the model's nondeterministic choices. We first introduce an incremental scheme that almost surely converges to a statistically sound confidence band bounding the true Pareto front from both sides in the long run. To obtain a close underapproximation of the true front in finite time, we then propose three heuristic approaches that try to make the best of an a-priori fixed sampling budget. We implement our new techniques in the Modest Toolset's 'modes' simulator, and experimentally show their effectiveness on quantitative verification benchmarks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u7edf\u8ba1\u6a21\u578b\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6709\u9650\u65f6\u95f4\u5185\u627e\u5230\u6700\u4f18\u6743\u8861\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e76\u5b9e\u73b0\u4e86\u6548\u7387\u4e0a\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u8ba1\u6a21\u578b\u68c0\u6d4b\u65b9\u6cd5\u4e00\u6b21\u53ea\u80fd\u8bc4\u4f30\u4e00\u4e2a\u5c5e\u6027\u503c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u95ee\u9898\u4e2d\u5bfb\u627e\u591a\u76ee\u6807\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u5f0f\u65b9\u6848\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u91c7\u6837\u6765\u4f18\u5316\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\uff0c\u8be5\u65b9\u6848\u51e0\u4e4e\u53ef\u4ee5\u80af\u5b9a\u5730\u6536\u655b\u4e8e\u7edf\u8ba1\u4e0a\u53ef\u9760\u7684\u7f6e\u4fe1\u5e26\uff0c\u5e76\u4ece\u4e24\u4fa7\u5305\u56f4\u771f\u5b9e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u4e3a\u4e86\u5728\u6709\u9650\u65f6\u95f4\u5185\u83b7\u5f97\u771f\u5b9e\u524d\u6cbf\u7684\u8fd1\u4f3c\u4e0b\u754c\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e09\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u5904\u7406\u9884\u5b9a\u7684\u91c7\u6837\u9884\u7b97\u3002", "result": "\u5b9e\u73b0\u4e86\u7edf\u8ba1\u4e0a\u53ef\u9760\u7684\u7f6e\u4fe1\u5e26\uff0c\u53ef\u4ee5\u4ece\u4e24\u4fa7\u5305\u56f4\u771f\u5b9e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u6709\u9650\u65f6\u95f4\u5185\u83b7\u5f97\u771f\u5b9e\u524d\u6cbf\u7684\u8fd1\u4f3c\u4e0b\u754c\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u76ee\u6807\u7edf\u8ba1\u6a21\u578b\u68c0\u6d4b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9a\u91cf\u9a8c\u8bc1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.12067", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.12067", "abs": "https://arxiv.org/abs/2511.12067", "authors": ["Nataraju Bodappa", "Gregory Jerkiewicz", "Peter Grutter"], "title": "Probing Electrocatalytic Gas Evolution Reaction at Pt by Force Noise Measurements. Part 2. Oxygen", "comment": null, "summary": "Understanding O2 bubble nucleation and growth during the oxygen evolution reaction (OER) is crucial to comprehend their influences on catalytically active sites in the process. To achieve this goal, mapping the spatial variation of nanoscale dynamic individual steps at the electrocatalytic interfaces is vital, as it further enables a detailed understanding of the mechanism of the process. Here, we combined tapping mode AFM imaging with a Pt ultramicroelectrode to investigate oxygen bubble nucleation, growth, and detachment. Our AFM feedback error signal and topography data reveal that bubbles of O2 gas nucleate at the step edge sites and interact with the catalytically active sites. This interaction between primary catalytic sites and bubble nucleation sites is the primary reason for a decrease in the current density at a given high overpotential of the OER. Our findings advance the understanding of the complexity of phenomena involved in gas evolution on catalytic surfaces.", "AI": {"tldr": "\u6c27\u6c14\u6c14\u6ce1\u5728\u7535\u50ac\u5316\u754c\u9762\u4e0a\u6210\u6838\u3001\u751f\u957f\u548c\u8131\u79bb\uff0c\u7814\u7a76\u6c27\u6790\u51fa\u53cd\u5e94\uff08OER\uff09\u7684\u673a\u5236\u3002", "motivation": "\u7406\u89e3O2\u6c14\u6ce1\u7684\u6210\u6838\u548c\u751f\u957f\u5bf9\u4e8e\u7406\u89e3\u5176\u5bf9\u50ac\u5316\u6d3b\u6027\u4f4d\u70b9\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cbe\u786e\u63cf\u7ed8\u7535\u50ac\u5316\u754c\u9762\u5904\u7684\u7eb3\u7c73\u7ea7\u52a8\u6001\u53d8\u5316\u3002", "method": "\u7ed3\u5408\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08AFM\uff09\u548c\u94c2\u8d85\u5fae\u7535\u6781\uff0c\u7814\u7a76O2\u6c14\u6ce1\u7684\u6210\u6838\u3001\u751f\u957f\u548c\u8131\u79bb\u8fc7\u7a0b\u3002", "result": "AFM\u7ed3\u679c\u663e\u793a\uff0cO2\u6c14\u6ce1\u5728\u53f0\u9636\u8fb9\u7f18\u6210\u6838\uff0c\u5e76\u4e0e\u50ac\u5316\u6d3b\u6027\u4f4d\u70b9\u53d1\u751f\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u9ad8\u8fc7\u7535\u4f4d\u4e0b\u7535\u6d41\u5bc6\u5ea6\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6c14\u4f53\u5728\u50ac\u5316\u8868\u9762\u4e0a\u4ea7\u751f\u8fc7\u7a0b\u4e2d\u590d\u6742\u7684\u73b0\u8c61\uff0c\u7279\u522b\u662f\u50ac\u5316\u4f4d\u70b9\u4e0e\u6c14\u6ce1\u6210\u6838\u4f4d\u70b9\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u7535\u6d41\u5bc6\u5ea6\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002"}}
{"id": "2511.11852", "categories": ["cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2511.11852", "abs": "https://arxiv.org/abs/2511.11852", "authors": ["Nilesh Choudhury", "Sandeep", "Neesha Yadav", "Mayank Shukla", "Pintu Das"], "title": "Layer breathing Raman mode in two-dimensional van der Waals material $\\mathrm{Cr_2Ge_2Te_6}$", "comment": null, "summary": "Two-dimensional (2D) van der Waals (vdW) magnetic materials have emerged as key materials for next-generation magneto-electric and spintronic devices, where understanding the relationship between layer number, lattice dynamics, and magnetic interactions is very important. In this work, we report the observation of the layer breathing mode (LBM) in few-layer $\\mathrm{Cr_2Ge_2Te_6}$, a ferromagnetic semiconductor with thickness dependent electronic, magnetic and optical properties, using Raman spectroscopy, which serves as a direct fingerprint of interlayer coupling and lattice symmetry. Group-theoretical symmetry analysis confirms that the CGT falls under the non-polar category of layered material. The evolution of the LBM-frequency with increasing layer number (N) reveals a distinct softening trend, characteristic of weakening restoring forces in thicker flakes. By fitting the experimental Raman data using the Linear Chain Model (LCM), we quantitatively extract the interlayer force constant ($\\mathrm{K_c}$), providing a measure of the vdW coupling strength between layers.", "AI": {"tldr": "Cr2Ge2Te6\u7684\u62c9\u66fc\u5149\u8c31\u7814\u7a76\u63ed\u793a\u4e86\u5c42\u6570\u5bf9\u5176\u5c42\u95f4\u8026\u5408\u548c\u78c1\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u4e8c\u7ef4\u8303\u5fb7\u534e\u78c1\u6027\u6750\u6599\u7684\u5c42\u6570\u3001\u6676\u683c\u52a8\u529b\u5b66\u548c\u78c1\u76f8\u4e92\u4f5c\u7528\u4e4b\u95f4\u7684\u5173\u7cfb\u5bf9\u4e8e\u5f00\u53d1\u65b0\u578b\u78c1\u7535\u548c\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u62c9\u66fc\u5149\u8c31\u6280\u672f\u89c2\u5bdf\u4e86\u5c11\u5c42Cr2Ge2Te6\u7684\u5c42\u547c\u5438\u6a21\u5f0f\uff08LBM\uff09\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u94fe\u6a21\u578b\uff08LCM\uff09\u5b9a\u91cf\u63d0\u53d6\u4e86\u5c42\u95f4\u529b\u5e38\u6570\uff08Kc\uff09\u3002", "result": "\u89c2\u5bdf\u5230\u4e86Cr2Ge2Te6\u7684LBM\uff0c\u5e76\u53d1\u73b0\u5176\u9891\u7387\u968f\u5c42\u6570\u589e\u52a0\u5448\u73b0\u8f6f\u5316\u8d8b\u52bf\uff0c\u8868\u660e\u5c42\u95f4\u8026\u5408\u51cf\u5f31\u3002\u901a\u8fc7LCM\u6a21\u578b\u5f97\u5230\u4e86\u5c42\u95f4\u529b\u5e38\u6570Kc\u3002", "conclusion": "Cr2Ge2Te6\u7684LBM\u53ef\u4ee5\u4f5c\u4e3a\u5c42\u95f4\u8026\u5408\u548c\u6676\u683c\u5bf9\u79f0\u6027\u7684\u76f4\u63a5\u6307\u7eb9\u3002\u5c42\u6570\u662f\u5f71\u54cdCr2Ge2Te6\u5c42\u95f4\u8026\u5408\u548c\u6676\u683c\u52a8\u529b\u5b66\u7684\u91cd\u8981\u56e0\u7d20\u3002"}}
{"id": "2511.12398", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.12398", "abs": "https://arxiv.org/abs/2511.12398", "authors": ["Yulong Lu", "Tong Mao", "Jinchao Xu", "Yahong Yang"], "title": "On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions", "comment": null, "summary": "Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u903c\u8fd1\u5bf9\u79f0Korobov\u51fd\u6570\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4e0d\u53d7\u7ef4\u5ea6\u8bc5\u5492\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6784\u5efa\u5bf9\u79f0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ee5\u903c\u8fd1\u5177\u6709\u5185\u5728\u7269\u7406\u7ed3\u6784\uff08\u5982\u6392\u5217\u5bf9\u79f0\u6027\uff09\u7684\u51fd\u6570\uff0c\u5e76\u63d0\u4f9b\u5176\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u6784\u5efa\u5bf9\u79f0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u903c\u8fd1\u5bf9\u79f0Korobov\u51fd\u6570\uff0c\u5e76\u63a8\u5bfc\u5176\u6536\u655b\u7387\u548c\u6cdb\u5316\u8bef\u5dee\u7387\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u5bf9\u79f0Korobov\u51fd\u6570\u7684\u6536\u655b\u7387\u548c\u5e38\u6570\u524d\u56e0\u5b50\u6700\u591a\u4e0e\u73af\u5883\u7ef4\u5ea6\u6210\u591a\u9879\u5f0f\u5173\u7cfb\uff0c\u907f\u514d\u4e86\u7ef4\u5ea6\u8bc5\u5492\u3002\u63a8\u5bfc\u51fa\u7684\u5b66\u4e60\u8bef\u5dee\u7387\u4e5f\u907f\u514d\u4e86\u7ef4\u5ea6\u8bc5\u5492\u3002", "conclusion": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u5bf9\u79f0Korobov\u51fd\u6570\uff0c\u5e76\u4e14\u5728\u7406\u8bba\u4e0a\u907f\u514d\u4e86\u7ef4\u5ea6\u8bc5\u5492\uff0c\u8fd9\u5bf9\u4e8e\u5904\u7406\u5177\u6709\u5bf9\u79f0\u6027\u7684\u9ad8\u7ef4\u51fd\u6570\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.11659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11659", "abs": "https://arxiv.org/abs/2511.11659", "authors": ["Kesong Zheng", "Zhi Song", "Peizhou Li", "Shuyi Yao", "Zhenxing Bian"], "title": "A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model", "comment": "30 pages,12 figures", "summary": "Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u52a0\u6743\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08DWFF-Net\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8015\u5730\u751f\u6001\u7cfb\u7edf\u751f\u5883\u5206\u7c7b\u4f53\u7cfb\u4e0d\u5b8c\u5584\u3001\u7279\u5f81\u878d\u5408\u6548\u679c\u4e0d\u4f73\u7b49\u95ee\u9898\u3002\u8be5\u7f51\u7edc\u4f7f\u7528DINOv3\u63d0\u53d6\u57fa\u7840\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6570\u636e\u7ea7\u81ea\u9002\u5e94\u52a8\u6001\u52a0\u6743\u7b56\u7565\u548c\u591a\u5c42\u7279\u5f81\u878d\u5408\uff0c\u4ee5\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u7530\u57c2\u7b49\u5fae\u751f\u5883\u7684\u8bc6\u522b\u4e0a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDWFF-Net\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.6979\u7684mIoU\u548c0.8049\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7f51\u7edc\uff0c\u4e3a\u7cbe\u7ec6\u5316\u8015\u5730\u666f\u89c2\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u8015\u5730\u751f\u6001\u7cfb\u7edf\u6816\u606f\u5730\u5206\u7c7b\u7cfb\u7edf\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u73b0\u6709\u6a21\u578b\u5728\u878d\u5408\u8bed\u4e49\u548c\u7eb9\u7406\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u591a\u5c3a\u5ea6\u6816\u606f\u5730\u5206\u5272\u7cbe\u5ea6\u4e0d\u9ad8\uff0c\u8fb9\u754c\u6a21\u7cca\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u52a0\u6743\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08DWFF-Net\uff09\uff0c\u5176\u4e2d\u7f16\u7801\u5668\u4f7f\u7528\u51bb\u7ed3\u7684DINOv3\u63d0\u53d6\u57fa\u7840\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6570\u636e\u7ea7\u81ea\u9002\u5e94\u52a8\u6001\u52a0\u6743\u7b56\u7565\u878d\u5408\u7279\u5f81\u3002\u89e3\u7801\u5668\u91c7\u7528\u52a8\u6001\u6743\u91cd\u8ba1\u7b97\u7f51\u7edc\u8fdb\u884c\u591a\u5c42\u7279\u5f81\u878d\u5408\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b15\u7c7b\u8015\u5730\u7cfb\u7edf\u6816\u606f\u5730\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u6570\u636e\u96c6\u3002", "result": "\u5728\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0cDWFF-Net\u5b9e\u73b0\u4e860.6979\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u548c0.8049\u7684F1\u5206\u6570\uff0c\u5206\u522b\u6bd4\u57fa\u7ebf\u7f51\u7edc\u63d0\u9ad8\u4e860.021\u548c0.0161\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u591a\u5c42\u7279\u5f81\u878d\u5408\u80fd\u6709\u6548\u63d0\u5347\u7530\u57c2\u7b49\u5fae\u751f\u5883\u7c7b\u522b\u7684IoU\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94\u591a\u5c42\u7279\u5f81\u878d\u5408\u7684\u8015\u5730\u7cfb\u7edf\u6816\u606f\u5730\u8bc6\u522b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u4e9a\u7c73\u7ea7\u7684\u6816\u606f\u5730\u6d4b\u7ed8\uff0c\u4e3a\u8015\u5730\u666f\u89c2\u7684\u7cbe\u7ec6\u5316\u6816\u606f\u5730\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2511.12264", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2511.12264", "abs": "https://arxiv.org/abs/2511.12264", "authors": ["Anna V. Kononova", "Niki van Stein", "Olaf Mersmann", "Thomas B\u00e4ck", "Thomas Bartz-Beielstein", "Tobias Glasmachers", "Michael Hellwig", "Sebastian Krey", "Jakub K\u016fdela", "Boris Naujoks", "Leonard Papenmeier", "Elena Raponi", "Quentin Renau", "Jeroen Rook", "Lennart Sch\u00e4permeier", "Diederick Vermetten", "Daniela Zaharie"], "title": "Benchmarking that Matters: Rethinking Benchmarking for Practical Impact", "comment": null, "summary": "Benchmarking has driven scientific progress in Evolutionary Computation, yet current practices fall short of real-world needs. Widely used synthetic suites such as BBOB and CEC isolate algorithmic phenomena but poorly reflect the structure, constraints, and information limitations of continuous and mixed-integer optimization problems in practice. This disconnect leads to the misuse of benchmarking suites for competitions, automated algorithm selection, and industrial decision-making, despite these suites being designed for different purposes.\n  We identify key gaps in current benchmarking practices and tooling, including limited availability of real-world-inspired problems, missing high-level features, and challenges in multi-objective and noisy settings. We propose a vision centered on curated real-world-inspired benchmarks, practitioner-accessible feature spaces and community-maintained performance databases. Real progress requires coordinated effort: A living benchmarking ecosystem that evolves with real-world insights and supports both scientific understanding and industrial use.", "AI": {"tldr": "\u5f53\u524d\u7684\u8fdb\u5316\u8ba1\u7b97\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u7684\u9700\u6c42\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u4e0e\u73b0\u5b9e\u4e16\u754c\u89c1\u89e3\u540c\u6b65\u53d1\u5c55\u7684\u3001\u5305\u542b\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u3001\u6613\u4e8e\u4f7f\u7528\u7684\u7279\u5f81\u7a7a\u95f4\u4ee5\u53ca\u793e\u533a\u7ef4\u62a4\u7684\u6027\u80fd\u6570\u636e\u5e93\u7684\u57fa\u51c6\u6d4b\u8bd5\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7684\u8fdb\u5316\u8ba1\u7b97\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u4e86\u5176\u88ab\u8bef\u7528\u4e8e\u7ade\u8d5b\u3001\u81ea\u52a8\u7b97\u6cd5\u9009\u62e9\u548c\u5de5\u4e1a\u51b3\u7b56\u7b49\u573a\u666f\u3002", "method": "\u63d0\u51fa\u5efa\u7acb\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u3001\u6613\u4e8e\u4f7f\u7528\u7684\u7279\u5f81\u7a7a\u95f4\u4ee5\u53ca\u793e\u533a\u7ef4\u62a4\u7684\u6027\u80fd\u6570\u636e\u5e93\u7684\u57fa\u51c6\u6d4b\u8bd5\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u5f53\u524d\u7684\u8fdb\u5316\u8ba1\u7b97\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u7684\u9700\u6c42\uff0c\u5b58\u5728\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u5f0f\u95ee\u9898\u53ef\u7528\u6027\u6709\u9650\u3001\u9ad8\u7ea7\u529f\u80fd\u7f3a\u5931\u4ee5\u53ca\u591a\u76ee\u6807\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6311\u6218\u7b49\u95ee\u9898\u3002", "conclusion": "\u4e3a\u4e86\u5b9e\u73b0\u771f\u6b63\u7684\u8fdb\u5c55\uff0c\u9700\u8981\u534f\u8c03\u4e00\u81f4\u7684\u52aa\u529b\uff0c\u5efa\u7acb\u4e00\u4e2a\u4e0e\u73b0\u5b9e\u4e16\u754c\u89c1\u89e3\u540c\u6b65\u53d1\u5c55\u7684\u3001\u5305\u542b\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u3001\u6613\u4e8e\u4f7f\u7528\u7684\u7279\u5f81\u7a7a\u95f4\u4ee5\u53ca\u793e\u533a\u7ef4\u62a4\u7684\u6027\u80fd\u6570\u636e\u5e93\u7684\u57fa\u51c6\u6d4b\u8bd5\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2511.11877", "categories": ["cs.ET", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11877", "abs": "https://arxiv.org/abs/2511.11877", "authors": ["Ploy Thajchayapong", "Suzanne Carbonaro", "Tim Couper", "Blaine Helmick", "Spencer Rugaber", "Ashok Goel"], "title": "Evolution of A4L: A Data Architecture for AI-Augmented Learning", "comment": null, "summary": "As artificial intelligence (AI) becomes more deeply integrated into educational ecosystems, the demand for scalable solutions that enable personalized learning continues to grow. These architectures must support continuous data flows that power personalized learning and access to meaningful insights to advance learner success at scale. At the National AI Institute for Adult Learning and Online Education (AI-ALOE), we have developed an Architecture for AI-Augmented Learning (A4L) to support analysis and personalization of online education for adult learners. A4L1.0, an early implementation by Georgia Tech's Design Intelligence Laboratory, demonstrated how the architecture supports analysis of meso- and micro-learning by integrating data from Learning Management Systems (LMS) and AI tools. These pilot studies informed the design of A4L2.0. In this chapter, we describe A4L2.0 that leverages 1EdTech Consortium's open standards such as Edu-API, Caliper Analytics, and Learning Tools Interoperability (LTI) to enable secure, interoperable data integration across data systems like Student Information Systems (SIS), LMS, and AI tools. The A4L2.0 data pipeline includes modules for data ingestion, preprocessing, organization, analytics, and visualization.", "AI": {"tldr": "A4L2.0\u662f\u4e00\u4e2a\u5229\u7528\u5f00\u653e\u6807\u51c6\u5b9e\u73b0\u4e92\u8054\u4e92\u901a\u7684\u6570\u636e\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6210\u4eba\u5728\u7ebf\u6559\u80b2\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u3002", "motivation": "\u968f\u7740AI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u52a0\u6df1\uff0c\u5bf9\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u8005\u6d1e\u5bdf\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002", "method": "A4L2.0\u5229\u75281EdTech\u8054\u76df\u7684\u5f00\u653e\u6807\u51c6\uff08\u5982Edu-API\u3001Caliper Analytics\u548cLTI\uff09\u6765\u6574\u5408\u6765\u81eaSIS\u3001LMS\u548cAI\u5de5\u5177\u7684\u6570\u636e\u3002\u6570\u636e\u7ba1\u9053\u5305\u62ec\u6570\u636e\u6444\u53d6\u3001\u9884\u5904\u7406\u3001\u7ec4\u7ec7\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316\u6a21\u5757\u3002", "result": "A4L2.0\u80fd\u591f\u5b89\u5168\u3001\u4e92\u901a\u5730\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6559\u80b2\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u652f\u6301\u5bf9\u5728\u7ebf\u6210\u4eba\u5b66\u4e60\u8005\u8fdb\u884c\u6570\u636e\u5206\u6790\u548c\u4e2a\u6027\u5316\u5b66\u4e60\u3002", "conclusion": "A4L2.0\u901a\u8fc7\u91c7\u7528\u5f00\u653e\u6807\u51c6\u548c\u6a21\u5757\u5316\u7684\u6570\u636e\u7ba1\u9053\uff0c\u4e3a\u5b9e\u73b0\u5927\u89c4\u6a21\u6210\u4eba\u5728\u7ebf\u6559\u80b2\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u3002"}}
{"id": "2511.12456", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.12456", "abs": "https://arxiv.org/abs/2511.12456", "authors": ["Sukanya Kudva", "Anil Aswani"], "title": "Collusion-proof Auction Design using Side Information", "comment": null, "summary": "We study the problem of auction design in the presence of bidder collusion. Specifically, we consider a multi-unit auction of identical items with single-minded bidders, where a subset of bidders may collude by coordinating bids and transferring payments and items among themselves. While the classical Vickrey-Clarke-Groves (VCG) mechanism achieves efficient and truthful outcomes, it is highly vulnerable to collusion. In contrast, fully collusion-proof mechanisms are limited to posted-price formats, which fail to guarantee even approximate efficiency. This paper aims to bridge this gap by designing auctions that achieve good welfare and revenue guarantees even when some bidders collude. We first characterize the strategic behavior of colluding bidders under VCG and prove that such bidders optimally bid shade: they never overbid or take additional items, but instead reduce the auction price. This characterization enables a Bulow-Klemperer type result: adding colluding bidders can only improve welfare and revenue relative to running VCG on the non-colluding group alone. We then propose a Hybrid VCG (H-VCG) mechanism that combines VCG applied to non-colluding bidders with a posted-price mechanism for colluding bidders, assuming access to a black-box collusion detection algorithm. We show that H-VCG is ex-post dominant-strategy incentive compatible (DSIC) and derive probabilistic guarantees on expected welfare and revenue under both known and unknown valuation distributions. Numerical experiments across several distributions demonstrate that H-VCG consistently outperforms VCG restricted to non-colluding bidders and approaches the performance of the ideal VCG mechanism assuming universal truthfulness. Our results provide a principled framework for incorporating collusion detection into mechanism design, offering a step toward collusion-resistant auctions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u5408\u8c0b\u51fa\u4ef7\u8005\u7684\u62cd\u5356\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408VCG\uff08H-VCG\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u7ed3\u5408\u4e86VCG\u548c\u9650\u4ef7\u673a\u5236\uff0c\u5e76\u5229\u7528\u5408\u8c0b\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5728\u5408\u8c0b\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u798f\u5229\u548c\u6536\u5165\u4fdd\u969c\u3002", "motivation": "\u7ecf\u5178\u7684VCG\u673a\u5236\u5728\u5b58\u5728\u5408\u8c0b\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u5b8c\u5168\u9632\u5408\u8c0b\u7684\u673a\u5236\u6548\u7387\u4e0d\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u5728\u5408\u8c0b\u5b58\u5728\u65f6\u4e5f\u80fd\u83b7\u5f97\u826f\u597d\u798f\u5229\u548c\u6536\u5165\u4fdd\u969c\u7684\u62cd\u5356\u673a\u5236\u3002", "method": "\u9996\u5148\uff0c\u5206\u6790\u4e86\u5408\u8c0b\u51fa\u4ef7\u8005\u5728VCG\u673a\u5236\u4e0b\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u8bc1\u660e\u4ed6\u4eec\u4f1a\u901a\u8fc7\u538b\u4f4e\u4ef7\u683c\u6765\u51fa\u4ef7\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408VCG\uff08H-VCG\uff09\u673a\u5236\uff0c\u5b83\u5c06VCG\u5e94\u7528\u4e8e\u975e\u5408\u8c0b\u51fa\u4ef7\u8005\uff0c\u5e76\u5c06\u9650\u4ef7\u673a\u5236\u5e94\u7528\u4e8e\u5408\u8c0b\u51fa\u4ef7\u8005\uff0c\u540c\u65f6\u5047\u8bbe\u5b58\u5728\u4e00\u4e2a\u5408\u8c0b\u68c0\u6d4b\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc4\u4f30\u4e86H-VCG\u673a\u5236\u7684\u6027\u80fd\u3002", "result": "\u5408\u8c0b\u51fa\u4ef7\u8005\u5728VCG\u673a\u5236\u4e0b\u4f1a\u538b\u4f4e\u4ef7\u683c\u3002H-VCG\u673a\u5236\u88ab\u8bc1\u660e\u662f\u4e8b\u540e\u5360\u4f18\u7b56\u7565\u6fc0\u52b1\u517c\u5bb9\uff08DSIC\uff09\u7684\uff0c\u5e76\u4e14\u5728\u5df2\u77e5\u548c\u672a\u77e5\u7684\u4f30\u503c\u5206\u5e03\u4e0b\u90fd\u5177\u6709\u671f\u671b\u798f\u5229\u548c\u6536\u5165\u7684\u6982\u7387\u4fdd\u8bc1\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cH-VCG\u7684\u8868\u73b0\u4f18\u4e8e\u4ec5\u5e94\u7528\u4e8e\u975e\u5408\u8c0b\u51fa\u4ef7\u8005\u7684VCG\uff0c\u5e76\u63a5\u8fd1\u7406\u60f3VCG\u673a\u5236\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u8c0b\u68c0\u6d4b\u7684\u673a\u5236\u8bbe\u8ba1\u6846\u67b6\uff0c\u4e3a\u8bbe\u8ba1\u6297\u5408\u8c0b\u62cd\u5356\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002H-VCG\u673a\u5236\uff0c\u5728\u5408\u8c0b\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u63d0\u9ad8\u798f\u5229\u548c\u6536\u5165\u3002"}}
{"id": "2511.11606", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.11606", "abs": "https://arxiv.org/abs/2511.11606", "authors": ["Jean-Pierre Tchapet Njafa", "Elvira Vanelle Kameni Tcheuffa", "Aissatou Maghame", "Serge Guy Nana Engo"], "title": "Data-Driven Design Rules for TADF Emitters from a High-Throughput Screening of 747 Molecules", "comment": "46 pages, 5 figures", "summary": "The rational design of thermally activated delayed fluorescence (TADF) emitters is hindered by a complex interplay of thermodynamic and kinetic factors. To unravel these relationships, we performed a comprehensive computational analysis of \\num{747} experimentally known TADF molecules to establish large-scale, quantitative design principles. Our validated semi-empirical protocol systematically reveals how molecular architecture, conformational geometry, and electronic structure govern photophysical properties. We establish a clear performance hierarchy, with Donor-Acceptor-Donor (D-A-D) architectures being statistically superior for minimizing the singlet-triplet energy gap ($\u0394E_{\\text{ST}}$). Crucially, we identify an optimal D-A torsional angle window of \\qtyrange{50}{90}{\\degree} that resolves the key trade-off between a small $\u0394E_{\\text{ST}}$ and the non-zero spin-orbit coupling (SOC) required for efficient reverse intersystem crossing (RISC). Data-driven clustering further identifies a distinct family of high-performance candidates and confirms Multi-Resonance (MR) emitters as a unique paradigm for high-efficiency blue emission. These findings culminate in a set of actionable design rules and the identification of \\num{127} high-priority candidates predicted to have $\u0394E_{\\text{ST}}< \\qty{0.1}{\\electronvolt}$ and oscillator strength $f \\num{> 0.1}$. This work provides a data-driven framework that unifies thermodynamic and kinetic principles to accelerate the discovery of next-generation TADF emitters.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790747\u4e2a\u5df2\u77e5\u7684TADF\u5206\u5b50\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u5206\u5b50\u7ed3\u6784\u3001\u6784\u8c61\u548c\u7535\u5b50\u7ed3\u6784\u5f71\u54cd\u5149\u7269\u7406\u6027\u8d28\u7684\u5927\u89c4\u6a21\u5b9a\u91cf\u8bbe\u8ba1\u539f\u5219\u3002D-A-D\u7ed3\u6784\u5728\u51cf\u5c0f\u5355\u91cd\u6001-\u4e09\u91cd\u6001\u80fd\u9699\uff08\u0394E_ST\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u4e14\u53d1\u73b050-90\u5ea6\u662f\u5b9e\u73b0\u5c0f\u0394E_ST\u548c\u6709\u6548\u53cd\u5411\u7cfb\u95f4\u7a9c\u8d8a\uff08RISC\uff09\u7684\u7406\u60f3\u626d\u8f6c\u89d2\u3002MR\u53d1\u5c04\u4f53\u662f\u9ad8\u6548\u84dd\u5149\u53d1\u5c04\u7684\u72ec\u7279\u8303\u4f8b\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u89c4\u5219\uff0c\u5e76\u7b5b\u9009\u51fa127\u4e2a\u6709\u6f5c\u529b\u7684\u5019\u9009\u5206\u5b50\u3002", "motivation": "\u76ee\u524d\uff0c\u70ed\u6fc0\u6d3b\u5ef6\u8fdf\u8367\u5149\uff08TADF\uff09\u53d1\u5c04\u4f53\u7684\u7406\u6027\u8bbe\u8ba1\u53d7\u5230\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u56e0\u7d20\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u7684\u963b\u788d\uff0c\u9700\u8981\u5efa\u7acb\u5927\u89c4\u6a21\u3001\u5b9a\u91cf\u7684\u8bbe\u8ba1\u539f\u7406\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5bf9747\u4e2a\u5b9e\u9a8c\u4e0a\u5df2\u77e5\u7684TADF\u5206\u5b50\u8fdb\u884c\u5168\u9762\u7684\u8ba1\u7b97\u5206\u6790\uff0c\u91c7\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u534a\u7ecf\u9a8c\u65b9\u6848\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u5206\u5b50\u7ed3\u6784\u3001\u6784\u8c61\u51e0\u4f55\u548c\u7535\u5b50\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u5149\u7269\u7406\u6027\u8d28\u3002\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u805a\u7c7b\u8fdb\u4e00\u6b65\u8bc6\u522b\u9ad8\u6027\u80fdTADF\u5206\u5b50\u5bb6\u65cf\uff0c\u5e76\u786e\u8ba4\u591a\u5171\u632f\uff08MR\uff09\u53d1\u5c04\u4f53\u4f5c\u4e3a\u9ad8\u6548\u84dd\u5149\u53d1\u5c04\u7684\u72ec\u7279\u8303\u5f0f\u3002", "result": "D-A-D\u7ed3\u6784\u5728\u51cf\u5c0f\u0394E_ST\u65b9\u9762\u5177\u6709\u7edf\u8ba1\u5b66\u4e0a\u7684\u4f18\u52bf\u3002\u786e\u5b9a\u4e8650-90\u5ea6\u7684\u6700\u4f73D-A\u626d\u8f6c\u89d2\u7a97\u53e3\uff0c\u8be5\u7a97\u53e3\u80fd\u5728\u51cf\u5c0f\u0394E_ST\u548c\u63d0\u9ad8RISC\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u8bc6\u522b\u51fa\u4e86\u4e00\u7c7b\u72ec\u7279\u7684MR\u53d1\u5c04\u4f53\uff0c\u5b83\u4eec\u5728\u9ad8\u6548\u84dd\u5149\u53d1\u5c04\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u521d\u6b65\u7b5b\u9009\u51fa127\u4e2a\u5177\u6709\u0394E_ST < 0.1 eV\u548c\u632f\u8361\u5668\u5f3a\u5ea6f > 0.1\u7684\u9ad8\u4f18\u5148\u7ea7\u5019\u9009\u5206\u5b50\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u6574\u5408\u4e86\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u539f\u7406\uff0c\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3TADF\u53d1\u5c04\u4f53\u7684\u53d1\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u89c4\u5219\u3002"}}
{"id": "2511.11987", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11987", "abs": "https://arxiv.org/abs/2511.11987", "authors": ["Weilun Jiang"], "title": "Emergent synchronization mode in coupled Rydberg atomic chains", "comment": "8 pages, 5 figures", "summary": "We report a new oscillatory form in the two coupled dissipative Rydberg atomic chains by modulating its spacing. Such oscillation has $\u03c0$-phase difference between two neighboring sites, which distinguishes itself from antiferromagnetic-type synchronization in the previous studies. Theoretically, we find a phase with coexisting two types of continuous time crystals, and recognize that the transition belongs to Hopf and pitchfork bifurcation. Furthermore, we generalize the conclusion to multiple chains and verify the uniqueness of the new synchronization mode. We also discuss its experimental feasibility.", "AI": {"tldr": "\u6587\u7ae0\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u94fe\u539f\u5b50\u632f\u8361\u6a21\u5f0f\uff0c\u4e0e\u4e4b\u524d\u7684\u53cd\u94c1\u78c1\u540c\u6b65\u4e0d\u540c\uff0c\u5b83\u5177\u6709\u03c0\u76f8\u4f4d\u5dee\uff0c\u5e76\u4e14\u662f\u8fde\u7eed\u65f6\u95f4\u6676\u4f53\u7684\u5171\u5b58\u3002", "motivation": "\u63a2\u7d22\u8026\u5408\u8017\u6563\u91cc\u5fb7\u5821\u539f\u5b50\u94fe\u5728\u8c03\u5236\u95f4\u8ddd\u4e0b\u7684\u65b0\u540c\u6b65\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\uff0c\u53d1\u73b0\u4e86\u5177\u6709\u03c0\u76f8\u4f4d\u5dee\u7684\u65b0\u578b\u632f\u8361\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u4e0e\u53cd\u94c1\u78c1\u540c\u6b65\u533a\u5206\u5f00\u3002\u7814\u7a76\u4e86\u8be5\u7cfb\u7edf\u76f8\u56fe\uff0c\u53d1\u73b0\u4e86\u4e24\u79cd\u8fde\u7eed\u65f6\u95f4\u6676\u4f53\u7684\u5171\u5b58\uff0c\u5e76\u8bc6\u522b\u4e86Hopf\u548cpitchfork\u5206\u5c94\u3002\u5c06\u7ed3\u8bba\u63a8\u5e7f\u5230\u591a\u94fe\u7cfb\u7edf\u5e76\u9a8c\u8bc1\u4e86\u65b0\u540c\u6b65\u6a21\u5f0f\u7684\u552f\u4e00\u6027\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5b9e\u9a8c\u53ef\u884c\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u79cd\u5177\u6709\u03c0\u76f8\u4f4d\u5dee\u7684\u65b0\u578b\u632f\u8361\u6a21\u5f0f\uff0c\u5e76\u5b58\u5728\u4e24\u79cd\u8fde\u7eed\u65f6\u95f4\u6676\u4f53\u7684\u5171\u5b58\u3002\u8be5\u6a21\u578b\u9002\u7528\u4e8e\u591a\u94fe\u7cfb\u7edf\uff0c\u5e76\u5177\u6709\u5b9e\u9a8c\u53ef\u884c\u6027\u3002", "conclusion": "\u6587\u7ae0\u6210\u529f\u8bc6\u522b\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u94fe\u539f\u5b50\u632f\u8361\u6a21\u5f0f\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u5176\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9a8c\u53ef\u884c\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.13014", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13014", "abs": "https://arxiv.org/abs/2511.13014", "authors": ["Solon P. Pissis"], "title": "Maximal Palindromes in MPC: Simple and Optimal", "comment": "SOSA 2026", "summary": "In the classical longest palindromic substring (LPS) problem, we are given a string $S$ of length $n$, and the task is to output a longest palindromic substring in $S$. Gilbert, Hajiaghayi, Saleh, and Seddighin [SPAA 2023] showed how to solve the LPS problem in the Massively Parallel Computation (MPC) model in $\\mathcal{O}(1)$ rounds using $\\mathcal{\\widetilde{O}}(n)$ total memory, with $\\mathcal{\\widetilde{O}}(n^{1-\u03b5})$ memory per machine, for any $\u03b5\\in (0,0.5]$.\n  We present a simple and optimal algorithm to solve the LPS problem in the MPC model in $\\mathcal{O}(1)$ rounds. The total time and memory are $\\mathcal{O}(n)$, with $\\mathcal{O}(n^{1-\u03b5})$ memory per machine, for any $\u03b5\\in (0,0.5]$. A key attribute of our algorithm is its ability to compute all maximal palindromes in the same complexities. Furthermore, our new insights allow us to bypass the constraint $\u03b5\\in (0,0.5]$ in the Adaptive MPC model. Our algorithms and the one proposed by Gilbert et al. for the LPS problem are randomized and succeed with high probability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728MPC\u6a21\u578b\u4e2d\u4ee5O(1)\u8f6e\u89e3\u51b3\u6700\u957f\u56de\u6587\u5b50\u4e32\u95ee\u9898\u7684\u7b80\u5355\u4e14\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u603b\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u6bcf\u53f0\u673a\u5668\u7684\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(n^{1-\u03b5})\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u03b5\u2208(0,0.5]\u3002\u8be5\u7b97\u6cd5\u8fd8\u80fd\u8ba1\u7b97\u6240\u6709\u6700\u5927\u7684\u56de\u6587\u5b50\u4e32\uff0c\u5e76\u653e\u5bbd\u4e86\u5bf9\u03b5\u7684\u9650\u5236\u3002", "motivation": "\u4e3a\u89e3\u51b3\u7ecf\u5178\u7684\u6700\u957f\u56de\u6587\u5b50\u4e32\uff08LPS\uff09\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5176\u5728Massively Parallel Computation\uff08MPC\uff09\u6a21\u578b\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u4e14\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u5728MPC\u6a21\u578b\u4e2d\u4ee5O(1)\u8f6e\u89e3\u51b3LPS\u95ee\u9898\uff0c\u540c\u65f6\u80fd\u591f\u8ba1\u7b97\u6240\u6709\u6700\u5927\u7684\u56de\u6587\u5b50\u4e32\u3002\u7b97\u6cd5\u7684\u603b\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u6bcf\u53f0\u673a\u5668\u7684\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(n^{1-\u03b5})\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u03b5\u2208(0,0.5]\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u8fd8\u653e\u5bbd\u4e86\u5728\u81ea\u9002\u5e94MPC\u6a21\u578b\u4e2d\u5bf9\u03b5\u7684\u9650\u5236\u3002", "result": "\u7b97\u6cd5\u5728MPC\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86O(1)\u8f6e\u7684\u6700\u957f\u56de\u6587\u5b50\u4e32\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u603b\u5185\u5b58\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u6bcf\u53f0\u673a\u5668\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(n^{1-\u03b5})\u3002\u540c\u65f6\uff0c\u8be5\u7b97\u6cd5\u80fd\u8ba1\u7b97\u6240\u6709\u6700\u5927\u56de\u6587\u5b50\u4e32\uff0c\u5e76\u6d88\u9664\u4e86\u5bf9\u03b5\u53d6\u503c\u7684\u9650\u5236\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728MPC\u6a21\u578b\u4e2d\u4ee5O(1)\u8f6e\u7684\u590d\u6742\u5ea6\u89e3\u51b3\u4e86\u6700\u957f\u56de\u6587\u5b50\u4e32\u95ee\u9898\uff0c\u5e76\u80fd\u540c\u65f6\u627e\u51fa\u6240\u6709\u6700\u5927\u56de\u6587\u5b50\u4e32\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u6027\u4e0a\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2511.12516", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12516", "abs": "https://arxiv.org/abs/2511.12516", "authors": ["Ziqing Qian", "Jiaying Lei", "Shengqi Dang", "Nan Cao"], "title": "Designed to Spread: Generative Approaches to Enhance Information Diffusion", "comment": "Accepted by AAAI26", "summary": "Social media has fundamentally transformed how people access information and form social connections, with content expression playing a critical role in driving information diffusion. While prior research has focused largely on network structures and tipping point identification, it provides limited tools for automatically generating content tailored for virality within a specific audience. To fill this gap, we propose the novel task of DOCG and introduce an information enhancement algorithm for generating content optimized for diffusion. Our method includes an influence indicator that enables content-level diffusion assessment without requiring access to network topology, and an information editor that employs reinforcement learning to explore interpretable editing strategies. The editor leverages generative models to produce semantically faithful, audience-aware textual or visual content. Experiments on real-world social media datasets and user study demonstrate that our approach significantly improves diffusion effectiveness while preserving the core semantics of the original content.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u6863\u7ea7\u5185\u5bb9\u751f\u6210\uff08DOCG\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u80fd\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f20\u64ad\u7684\u5185\u5bb9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u589e\u5f3a\u7b97\u6cd5\u6765\u4f18\u5316\u5185\u5bb9\u4f20\u64ad\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\uff0c\u901a\u8fc7\u5f71\u54cd\u6307\u6807\u8bc4\u4f30\u5185\u5bb9\u4f20\u64ad\u6f5c\u529b\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4fe1\u606f\u7f16\u8f91\u5668\u63a2\u7d22\u53ef\u89e3\u91ca\u7684\u7f16\u8f91\u7b56\u7565\uff0c\u751f\u6210\u65e2\u5fe0\u5b9e\u4e8e\u539f\u6587\u8bed\u4e49\u53c8\u7b26\u5408\u76ee\u6807\u53d7\u4f17\u7684\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u4f20\u64ad\u65b9\u9762\uff0c\u4e3b\u8981\u5173\u6ce8\u7f51\u7edc\u7ed3\u6784\u548c\u4e34\u754c\u70b9\u8bc6\u522b\uff0c\u7f3a\u4e4f\u81ea\u52a8\u751f\u6210\u75c5\u6bd2\u5f0f\u4f20\u64ad\u5185\u5bb9\u7684\u5de5\u5177\u3002", "method": "\u63d0\u51faDOCG\u4efb\u52a1\u548c\u4fe1\u606f\u589e\u5f3a\u7b97\u6cd5\u3002\u7b97\u6cd5\u5305\u542b\u4e00\u4e2a\u5f71\u54cd\u6307\u6807\uff0c\u7528\u4e8e\u5728\u4e0d\u8bbf\u95ee\u7f51\u7edc\u62d3\u6251\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5185\u5bb9\u4f20\u64ad\u80fd\u529b\uff1b\u4e00\u4e2a\u4fe1\u606f\u7f16\u8f91\u5668\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u53ef\u89e3\u91ca\u7684\u7f16\u8f91\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u6a21\u578b\u751f\u6210\u5fe0\u5b9e\u4e8e\u539f\u6587\u8bed\u4e49\u3001\u9762\u5411\u7279\u5b9a\u53d7\u4f17\u7684\u6587\u672c\u6216\u89c6\u89c9\u5185\u5bb9\u3002", "result": "\u5728\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u5185\u5bb9\u4f20\u64ad\u6709\u6548\u6027\u7684\u540c\u65f6\uff0c\u80fd\u4fdd\u6301\u539f\u6587\u7684\u6838\u5fc3\u8bed\u4e49\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DOCG\u4efb\u52a1\u548c\u4fe1\u606f\u589e\u5f3a\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u7684\u4f20\u64ad\u6548\u679c\uff0c\u5e76\u4fdd\u6301\u5185\u5bb9\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2511.13600", "categories": ["cs.LO", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13600", "abs": "https://arxiv.org/abs/2511.13600", "authors": ["Claire Y. Yin", "Peter M. Kogge"], "title": "Subgraph Isomorphism: Prolog vs. Conventional", "comment": null, "summary": "Subgraph Isomorphism uses a small graph as a pattern to identify within a larger graph a set of vertices that have matching edges. This paper addresses a logic program written in Prolog for a specific relatively complex graph pattern for which multiple conventional implementations (including parallel) exist. The goal is to understand the complexity differences between programming logically and programming conventionally. Discussion includes the process of converting the graph pattern into logic statements in Prolog, and the resulting characteristics as the size of the graph increased. The analysis shows that using a logic paradigm is an efficient way to attack complex graph problems.", "AI": {"tldr": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u95ee\u9898\u4e2d\uff0c\u903b\u8f91\u7f16\u7a0b\u6bd4\u4f20\u7edf\u7f16\u7a0b\u66f4\u6709\u6548\u3002", "motivation": "\u63a2\u7d22\u903b\u8f91\u7f16\u7a0b\u4e0e\u4f20\u7edf\u7f16\u7a0b\u5728\u89e3\u51b3\u590d\u6742\u56fe\u95ee\u9898\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5c06\u56fe\u6a21\u5f0f\u8f6c\u6362\u4e3aProlog\u903b\u8f91\u8bed\u53e5\uff0c\u5e76\u5206\u6790\u968f\u7740\u56fe\u5927\u5c0f\u589e\u52a0\u800c\u4ea7\u751f\u7684\u7279\u5f81\u3002", "result": "\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u56fe\u95ee\u9898\u65f6\u663e\u793a\u51fa\u6548\u7387\u3002", "conclusion": "\u903b\u8f91\u7f16\u7a0b\u662f\u4e00\u79cd\u5904\u7406\u590d\u6742\u56fe\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.11992", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11992", "abs": "https://arxiv.org/abs/2511.11992", "authors": ["Hung Du", "Hy Nguyen", "Srikanth Thudumu", "Rajesh Vasa", "Kon Mouzakis"], "title": "Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams", "comment": "Accepted poster at the IEEE Consumer Communications & Networking Conference (CCNC) 2026", "summary": "Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u4f7f\u8f66\u8f86\u80fd\u591f\u6839\u636e\u5c40\u90e8\u76ee\u6807\u548c\u89c2\u6d4b\u8fdb\u884c\u9009\u62e9\u6027\u901a\u4fe1\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u3001\u4e0d\u53ef\u9884\u6d4b\u4e14\u901a\u4fe1\u53d7\u9650\u7684\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u534f\u8c03\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8f66\u8f86\uff08\u9646\u5730\u3001\u6c34\u57df\u3001\u7a7a\u4e2d\uff09\u9762\u4e34\u52a8\u6001\u3001\u4e0d\u53ef\u9884\u6d4b\u3001\u901a\u4fe1\u53d7\u9650\u3001\u65e0\u4e2d\u5fc3\u63a7\u5236\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u73af\u5883\uff0c\u8fd9\u5bf9\u8ffd\u6c42\u4e2a\u4f53\u76ee\u6807\u7684\u8f66\u8f86\u7684\u534f\u8c03\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\uff08\u8f66\u8f86\uff09\u6839\u636e\u5c40\u90e8\u76ee\u6807\u548c\u89c2\u6d4b\u8fdb\u884c\u9009\u62e9\u6027\u901a\u4fe1\uff0c\u53ea\u5171\u4eab\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u534f\u4f5c\u5e76\u5c0a\u91cd\u53ef\u89c1\u6027\u9650\u5236\u3002", "result": "\u5728\u5177\u6709\u969c\u788d\u7269\u548c\u52a8\u6001\u667a\u80fd\u4f53\u79cd\u7fa4\u7684\u590d\u6742\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u7f29\u77ed\u4e86\u8fbe\u5230\u76ee\u6807\u7684\u65f6\u95f4\uff0c\u5e76\u4e14\u968f\u7740\u667a\u80fd\u4f53\u6570\u91cf\u7684\u589e\u52a0\uff0c\u4efb\u52a1\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u7684\u3001\u4ee5\u76ee\u6807\u4e3a\u9a71\u52a8\u7684MARL\u6709\u6f5c\u529b\u652f\u6301\u5728\u4e0d\u540c\u9886\u57df\u8fd0\u884c\u7684\u73b0\u5b9e\u591a\u8f66\u8f86\u7cfb\u7edf\u4e2d\u8fdb\u884c\u6709\u6548\u534f\u8c03\u3002"}}
{"id": "2511.12105", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.12105", "abs": "https://arxiv.org/abs/2511.12105", "authors": ["Oriane de Leuze", "Maxime Berthe", "Sophie Hermans", "Beno\u00eet Hackens"], "title": "Inter-flake transport and humidity response of Ti3C2Tx MXene at the nanoscale", "comment": null, "summary": "Understanding charge transport in networks of two-dimensional crystals is essential for developing reliable applications such as chemiresistors or electromagnetic shields. For this purpose, intra- and inter-flake contributions to the network resistance must be disentangled. MXenes, such as Ti3C2Tx, are prime examples of 2D crystals often employed as thin networks of interconnected flakes deposited on substrates to realize functional devices. While a significant number of studies focused on transport in individual MXene flakes, inter-flake transport remains scarcely explored. Here, we demonstrate that charge transport in multi-flake conductive paths of Ti3C2Tx is dominated by interflake junctions and provide quantitative estimates of junction resistances. Scanning probe measurements reveal that in a MXene multi-flake conductive path, individual flakes behave as isopotential domains, since the voltage drop is localized precisely at the inter-flake junctions. We further investigate the chemiresistive response to humidity at the single flake, multi-flake and flake network scale, evidencing the leading impact of junctions on sensing kinetics. These findings underline the crucial role of junctions in charge transport and sensing capabilities of MXenes.", "AI": {"tldr": "\u7535\u8377\u4f20\u8f93\u4e3b\u8981\u53d7\u9650\u4e8e\u4e8c\u7ef4\u6676\u4f53\u7f51\u7edc\u4e2d\u7684\u7247\u95f4\u7ed3\uff0c\u8fd9\u5bf9\u4e8e MXene \u57fa\u8bbe\u5907\uff08\u5982\u5316\u5b66\u7535\u963b\u5668\uff09\u7684\u4f20\u611f\u52a8\u529b\u5b66\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u9700\u8981\u7406\u89e3\u4e8c\u7ef4\u6676\u4f53\u7f51\u7edc\uff08\u7279\u522b\u662f MXene\uff09\u4e2d\u7684\u7535\u8377\u4f20\u8f93\uff0c\u4ee5\u533a\u5206\u7247\u5185\u548c\u7247\u95f4\u7535\u963b\u7684\u8d21\u732e\uff0c\u4ece\u800c\u5f00\u53d1\u53ef\u9760\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u626b\u63cf\u63a2\u9488\u6d4b\u91cf\u6765\u7814\u7a76 Ti3C2Tx\uff08\u4e00\u79cd MXene\uff09\u591a\u7247\u5bfc\u7535\u901a\u8def\u4e2d\u7684\u7535\u8377\u4f20\u8f93\uff0c\u91cf\u5316\u7247\u95f4\u7ed3\u7535\u963b\uff0c\u5e76\u5c06\u5404\u5411\u540c\u6027\u884c\u4e3a\u4e0e\u7535\u538b\u964d\u5b9a\u4f4d\u5728\u7247\u95f4\u7ed3\u7684\u73b0\u8c61\u8054\u7cfb\u8d77\u6765\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u6e7f\u5ea6\u5bf9\u5355\u7247\u3001\u591a\u7247\u548c\u6574\u4e2a\u8584\u819c\u7f51\u7edc\u4f20\u611f\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cTi3C2Tx \u7f51\u7edc\u4e2d\u7684\u7535\u8377\u4f20\u8f93\u4e3b\u8981\u7531\u7247\u95f4\u7ed3\u51b3\u5b9a\uff0c\u5176\u4e2d\u5355\u4e2a\u8584\u7247\u8868\u73b0\u4e3a\u7b49\u7535\u4f4d\u57df\uff0c\u7535\u538b\u964d\u96c6\u4e2d\u5728\u7247\u95f4\u7ed3\u3002\u6e7f\u5ea6\u4f20\u611f\u52a8\u529b\u5b66\u4e5f\u53d7\u5230\u8fd9\u4e9b\u7ed3\u7684\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7247\u95f4\u7ed3\u5728\u4e8c\u7ef4\u6676\u4f53\u7f51\u7edc\uff08\u7279\u522b\u662f MXene\uff09\u7684\u7535\u8377\u4f20\u8f93\u548c\u4f20\u611f\u80fd\u529b\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u4ed4\u7ec6\u7814\u7a76\u4ee5\u4f18\u5316\u8bbe\u5907\u6027\u80fd\u3002"}}
{"id": "2511.11889", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11889", "abs": "https://arxiv.org/abs/2511.11889", "authors": ["Chiyoung Kim", "Ryan Jacobs", "Jack H. Duffy", "Kyle S. Brinkman", "Harry W. Abernathy", "Dane Morgan"], "title": "Effects of Yttrium Doping on Oxygen Conductivity in Ba(Fe, Co, Zr, Y)O_{3-\u03b4} Cathode Materials for Proton Ceramic Fuel Cells", "comment": "35 pages, 10 figures, 1 table. Data for \"Effects of Yttrium Doping on Oxygen Conductivity in Ba(Fe, Co, Zr, Y)O_{3-\u03b4} Cathode Materials for Proton Ceramic Fuel Cells.\" https://doi.org/10.6084/m9.figshare.30276166.v1", "summary": "Proton ceramic fuel cells (PCFCs) achieve high efficiency at reduced operating temperatures, but their performance is often limited by slow oxygen reduction reaction (ORR) kinetics at the cathode. The BaCoFeZrY (BCFZY) perovskite family is a promising triple-conducting air-electrode material, yet the role of Y dopants in governing oxygen transport remains unclear. In this study, we examine the effect of Y content on oxygen conductivity in three compositions: BCFZ, BCFZY0.1, and BCFY. Oxygen conductivity was evaluated from the product of oxygen tracer diffusivity and oxygen defect concentration. Ab initio molecular dynamics simulations were used to determine tracer diffusivity and migration energies, while defect concentrations were estimated from reference data. Y doping slightly decreases oxygen conductivity from BCFZ to BCFZY0.1, from 337 to 203 mS/cm at 500 C, with activation energies of 0.155 and 0.172 eV. BCFY shows much lower conductivity (99 mS/cm) and a higher activation energy of 0.261 eV. Computed conductivities are higher and more Arrhenius-like than experimental values, suggesting that microstructural features such as grain boundaries strongly limit oxygen transport in real materials. A series-circuit model combining bulk conductivity and fitted grain-boundary parameters provides semi-quantitative agreement with experiment. These results clarify the role of Y doping in oxygen transport and provide insight for optimizing cathode performance in PCFCs.", "AI": {"tldr": "Y\u63ba\u6742\u5bf9\u8d28\u5b50\u9676\u74f7\u71c3\u6599\u7535\u6c60\u9634\u6781\u6750\u6599BCFZY\u7684\u6c27\u4f20\u8f93\u6027\u80fd\u6709\u5f71\u54cd\uff0c\u4f46\u5177\u4f53\u5f71\u54cd\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u8ba1\u7b97\uff0c\u63a2\u7a76\u4e86\u4e0d\u540cY\u542b\u91cf\u5bf9BCFZY\u6c27\u7535\u5bfc\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u8003\u8651\u6676\u754c\u5f71\u54cd\u7684\u7b49\u6548\u7535\u8def\u6a21\u578b\u6765\u89e3\u91ca\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u660e\u786eY\u63ba\u6742\u5728BCFZY\u6750\u6599\u4e2d\u5bf9\u6c27\u4f20\u8f93\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u4f18\u5316\u8d28\u5b50\u9676\u74f7\u71c3\u6599\u7535\u6c60\u7684\u9634\u6781\u6027\u80fd\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u5b9e\u9a8c\u6d4b\u91cf\u548c\u4ece\u5934\u7b97\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u3002\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540cY\u542b\u91cf\u7684BCFZY\u6750\u6599\u5728\u4e0d\u540c\u6e29\u5ea6\u4e0b\u7684\u6c27\u7535\u5bfc\u7387\uff0c\u5e76\u5229\u7528\u4ece\u5934\u7b97\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6c27\u6269\u6563\u7cfb\u6570\u548c\u8fc1\u79fb\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u4f30\u7b97\u4e86\u6c27\u7f3a\u9677\u6d53\u5ea6\uff0c\u5e76\u7ed3\u5408\u5b9e\u9a8c\u6570\u636e\uff0c\u5206\u6790\u4e86Y\u63ba\u6742\u5bf9\u6c27\u7535\u5bfc\u7387\u7684\u5f71\u54cd\u3002\u6700\u540e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4f53\u76f8\u7535\u5bfc\u7387\u548c\u6676\u754c\u7535\u963b\u7684\u7b49\u6548\u7535\u8def\u6a21\u578b\uff0c\u4ee5\u6a21\u62df\u5b9e\u9a8c\u7ed3\u679c\u3002", "result": "Y\u63ba\u6742\u5bf9BCFZY\u6750\u6599\u7684\u6c27\u7535\u5bfc\u7387\u6709\u6291\u5236\u4f5c\u7528\u3002\u968f\u7740Y\u542b\u91cf\u7684\u589e\u52a0\uff0c\u6c27\u7535\u5bfc\u7387\u4eceBCFZ\u7684337 mS/cm\u4e0b\u964d\u5230BCZY0.1\u7684203 mS/cm\uff0c\u518d\u5230BCFY\u768499 mS/cm (\u5728500\u00b0C\u65f6)\u3002\u540c\u65f6\uff0c\u6d3b\u5316\u80fd\u4e5f\u968f\u4e4b\u589e\u52a0\u3002\u8ba1\u7b97\u5f97\u5230\u7684\u4f53\u76f8\u7535\u5bfc\u7387\u9ad8\u4e8e\u5b9e\u9a8c\u503c\uff0c\u8868\u660e\u6676\u754c\u5bf9\u6c27\u4f20\u8f93\u5b58\u5728\u663e\u8457\u7684\u9650\u5236\u4f5c\u7528\u3002\u7b49\u6548\u7535\u8def\u6a21\u578b\u80fd\u591f\u8f83\u597d\u5730\u62df\u5408\u5b9e\u9a8c\u6570\u636e\u3002", "conclusion": "Y\u63ba\u6742\u4f1a\u964d\u4f4eBCFZY\u6750\u6599\u7684\u6c27\u7535\u5bfc\u7387\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8eY\u63ba\u6742\u5f71\u54cd\u4e86\u6c27\u7f3a\u9677\u7684\u6d53\u5ea6\u548c\u8fc1\u79fb\u80fd\u3002\u6676\u754c\u5bf9\u6c27\u4f20\u8f93\u4e5f\u8d77\u7740\u5173\u952e\u7684\u9650\u5236\u4f5c\u7528\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7b49\u6548\u7535\u8def\u6a21\u578b\u80fd\u591f\u534a\u5b9a\u91cf\u5730\u63cf\u8ff0\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u4e3a\u672a\u6765\u8bbe\u8ba1\u9ad8\u6027\u80fd\u7684PCFC\u9634\u6781\u6750\u6599\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2511.11662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11662", "abs": "https://arxiv.org/abs/2511.11662", "authors": ["Ziyuan Gao"], "title": "AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation", "comment": "Accepted for publication in WACV 2026 (Round 2)", "summary": "Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.", "AI": {"tldr": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u56e0\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u800c\u9762\u4e34\u74f6\u9888\u3002\u672c\u6587\u63d0\u51faAGENet\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u7a7a\u95f4\u5173\u7cfb\u548c\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u7ebf\u8ddd\u79bb\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u7cbe\u786e\u8fb9\u754c\u63cf\u7ed8\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u5feb\u901f\u884c\u8fdb\u7ec6\u5316\u6765\u5b66\u4e60\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u7ebf\u8ddd\u79bb\uff0c\u901a\u8fc7\u7a7a\u95f4\u52a0\u6743\u805a\u5408\u8fdb\u884c\u81ea\u9002\u5e94\u539f\u578b\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\u6765\u81ea\u52a8\u8c03\u6574\u5230\u4e0d\u540c\u7684\u5668\u5b98\u7279\u5f81\u3002\u5b9e\u9a8c\u8bc1\u660eAGENet\u5728\u51cf\u5c11\u8fb9\u754c\u8bef\u5dee\u548c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5408\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e34\u5e8a\u5e94\u7528\u4e2d\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u7684\u7cbe\u786e\u8fb9\u754c\u63cf\u7ed8\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u8db3\u591f\u7a7a\u95f4\u4fe1\u606f\u7684\u7c7b\u4f3c\u89e3\u5256\u533a\u57df\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b66\u4e60\u6700\u5c0f\u6837\u672c\u5e76\u7cbe\u786e\u63cf\u7ed8\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u7684\u65b9\u6cd5\u3002", "method": "AGENet\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a(1) \u4e00\u4e2a\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u7ebf\u8ddd\u79bb\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u8fed\u4ee3\u5feb\u901f\u884c\u8fdb\u7ec6\u5316\u6765\u5c0a\u91cd\u89e3\u5256\u8fb9\u754c\uff1b(2) \u4e00\u4e2a\u81ea\u9002\u5e94\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u901a\u8fc7\u7a7a\u95f4\u52a0\u6743\u805a\u5408\u6765\u6355\u83b7\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u8fb9\u754c\u7ec6\u8282\uff1b(3) \u4e00\u4e2a\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\u6a21\u5757\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u7684\u5668\u5b98\u7279\u5f81\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAGENet\u76f8\u6bd4\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u8fb9\u754c\u9519\u8bef\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u4e0a\u4fdd\u6301\u4e86\u6548\u7387\u3002", "conclusion": "AGENet\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u5173\u7cfb\u548c\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u7ebf\u8ddd\u79bb\u5b66\u4e60\uff0c\u80fd\u591f\u4ece\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u7cbe\u786e\u5206\u5272\u533b\u5b66\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u8fb9\u754c\u8bef\u5dee\u548c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u5176\u6210\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u5904\u7406\u6807\u6ce8\u6570\u636e\u6709\u9650\u95ee\u9898\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2511.12367", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2511.12367", "abs": "https://arxiv.org/abs/2511.12367", "authors": ["Natalia A. Santos", "Marlon Jeske", "Antonio A. Chaves"], "title": "Random-Key Metaheuristic and Linearization for the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem", "comment": null, "summary": "This paper addresses the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem (QMC-VSBPP), a challenging combinatorial optimization problem that generalizes the classical bin packing by incorporating multiple capacity dimensions, heterogeneous bin types, and quadratic interaction costs between items. We propose two complementary methods that advance the current state-of-the-art. First, a linearized mathematical formulation is introduced to eliminate quadratic terms, enabling the use of exact solvers such as Gurobi to compute strong lower bounds - reported here for the first time for this problem. Second, we develop RKO-ACO, a continuous-domain Ant Colony Optimization algorithm within the Random-Key Optimization framework, enhanced with adaptive Q-learning parameter control and efficient local search. Extensive computational experiments on benchmark instances show that the proposed linearized model produces significantly tighter lower bounds than the original quadratic formulation, while RKO-ACO consistently matches or improves upon all best-known solutions in the literature, establishing new upper bounds for large-scale instances. These results provide new reference values for future studies and demonstrate the effectiveness of evolutionary and random-key metaheuristic approaches for solving complex quadratic packing problems. Source code and data available at https://github.com/nataliaalves03/RKO-ACO", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5177\u6709\u591a\u91cd\u7ea6\u675f\u7684\u4e8c\u6b21\u53d8\u91cf\u5927\u5c0f\u7bb1\u5305\u88c5\u95ee\u9898\uff08QMC-VSBPP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u5316\u6570\u5b66\u6a21\u578b\u548c\u4e00\u79cd\u57fa\u4e8e\u8681\u7fa4\u4f18\u5316\u7684\u968f\u673a\u952e\u5408\u4f18\u5316\u7b97\u6cd5\uff08RKO-ACO\uff09\uff0c\u5728\u8ba1\u7b97\u4e0b\u754c\u548c\u6c42\u89e3\u95ee\u9898\u65b9\u9762\u5747\u53d6\u5f97\u4e86 SOTA \u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5177\u6709\u591a\u91cd\u7ea6\u675f\u7684\u4e8c\u6b21\u53d8\u91cf\u5927\u5c0f\u7bb1\u5305\u88c5\u95ee\u9898\uff08QMC-VSBPP\uff09\uff0c\u8be5\u95ee\u9898\u662f\u7ecf\u5178\u7684\u7bb1\u5305\u88c5\u95ee\u9898\u7684\u6cdb\u5316\uff0c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ebf\u6027\u5316\u6570\u5b66\u6a21\u578b\u6765\u6d88\u9664\u4e8c\u6b21\u9879\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u6539\u8fdb\u7684\u968f\u673a\u952e\u5408\u4f18\u5316\uff08RKO\uff09\u6846\u67b6\u4e0b\u7684\u8fde\u7eed\u57df\u8681\u7fa4\u4f18\u5316\uff08ACO\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5305\u542b\u81ea\u9002\u5e94Q\u5b66\u4e60\u53c2\u6570\u63a7\u5236\u548c\u6709\u6548\u7684\u5c40\u90e8\u641c\u7d22\u3002", "result": "\u7ebf\u6027\u5316\u6a21\u578b\u4ea7\u751f\u7684\u4e0b\u754c\u6bd4\u539f\u59cb\u4e8c\u6b21\u516c\u5f0f\u66f4\u7d27\u5bc6\uff1bRKO-ACO \u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\u65f6\uff0c\u6301\u7eed\u5339\u914d\u6216\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u6240\u6709\u5df2\u77e5\u7684\u6700\u4f73\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u4e0a\u754c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a QMC-VSBPP \u63d0\u4f9b\u4e86\u65b0\u7684\u53c2\u8003\u503c\uff0c\u5e76\u8bc1\u660e\u4e86\u8fdb\u5316\u7b97\u6cd5\u548c\u968f\u673a\u952e\u5408\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u7684\u4e8c\u6b21\u5305\u88c5\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12617", "categories": ["cs.ET", "cs.DC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12617", "abs": "https://arxiv.org/abs/2511.12617", "authors": ["Stefano Markidis", "Luca Pennati", "Marco Pasquale", "Gilbert Netzer", "Ivy Peng"], "title": "QPU Micro-Kernels for Stencil Computation", "comment": "Accepted for publication at SCA/HPCAsia 2026", "summary": "We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.", "AI": {"tldr": "QPU\u5fae\u5185\u6838\u662f\u4e00\u79cd\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6d45\u5c42\u91cf\u5b50\u7535\u8def\uff0c\u5b83\u5145\u5f53\u91c7\u6837\u52a0\u901f\u5668\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5206\u89e3\u4e3a\u5c40\u90e8\u66f4\u65b0\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u5404\u79cd\u79d1\u5b66\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u5373\u5c06\u91cf\u5b50\u5904\u7406\u5355\u5143\uff08QPU\uff09\u4f5c\u4e3a\u91c7\u6837\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u6267\u884c\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u5c40\u90e8\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e24\u79cdQPU\u5fae\u5185\u6838\uff08Bernoulli\u548c\u5206\u652f\uff09\uff0c\u5b83\u4eec\u80fd\u591f\u6267\u884c\u6d45\u5c42\u91cf\u5b50\u7535\u8def\u4ee5\u83b7\u5f97\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u3002\u8fd9\u4e9b\u5fae\u5185\u6838\u88ab\u8bbe\u8ba1\u4e3a\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u79bb\u6563\u5316\u7f51\u683c\u70b9\u66f4\u65b0\uff0c\u5e76\u4e14\u5176\u8d44\u6e90\u5360\u7528\u4e0e\u7f51\u683c\u5927\u5c0f\u65e0\u5173\u3002\u901a\u8fc7\u5bf9\u70ed\u65b9\u7a0b\u548c\u7c98\u6027Burgers\u65b9\u7a0b\u7684\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5fae\u5185\u6838\u7684\u6027\u80fd\u3002", "result": "\u5728\u91cf\u5b50\u7535\u8def\u6a21\u62df\u5668\u4e0a\uff0c\u968f\u7740\u91c7\u6837\u6570\u7684\u589e\u52a0\uff0c\u7cbe\u5ea6\u6709\u6240\u63d0\u9ad8\u3002\u5728IBM Brisbane\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\uff0c\u5bf9\u4e8e\u5355\u6b65\u6269\u6563\u6d4b\u8bd5\uff0cBernoulli\u5fae\u5185\u6838\u5728\u76f8\u540c\u7684\u91c7\u6837\u9884\u7b97\u4e0b\u8868\u73b0\u51fa\u6bd4\u5206\u652f\u5fae\u5185\u6838\u66f4\u4f4e\u7684\u8bef\u5dee\uff0c\u5e76\u4e14QPU\u5fae\u5185\u6838\u7684\u6267\u884c\u65f6\u95f4\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "QPU\u5fae\u5185\u6838\u65b9\u6cd5\u4e3a\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u5b83\u901a\u8fc7\u5c06\u8ba1\u7b97\u5206\u89e3\u4e3a\u53ef\u5e76\u884c\u7684\u5c40\u90e8\u66f4\u65b0\uff0c\u5e76\u5229\u7528QPU\u7684\u91c7\u6837\u52a0\u901f\u80fd\u529b\uff0c\u5728\u8d44\u6e90\u6548\u7387\u548c\u6c42\u89e3\u7cbe\u5ea6\u65b9\u9762\u90fd\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2511.12523", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12523", "abs": "https://arxiv.org/abs/2511.12523", "authors": ["Adam Dziwoki", "Rostislav Horcik"], "title": "Perturbing Best Responses in Zero-Sum Games", "comment": "Accepted to AAAI 2026", "summary": "This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure.", "AI": {"tldr": "\u5bf9\u53cc\u5bf9\u7b56\u548c\u5047\u60f3\u535a\u5f08\u4e2d\u57fa\u4e8e\u6700\u4f73\u54cd\u5e94\u7684\u7b97\u6cd5\u8fdb\u884c\u6270\u52a8\u5206\u6790\uff0c\u53d1\u73b0\u6270\u52a8\u80fd\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff0c\u751a\u81f3\u8fbe\u5230\u5bf9\u6570\u7ea7\u522b\u3002", "motivation": "\u7814\u7a76\u5728\u96f6\u548c\u535a\u5f08\u4e2d\uff0c\u6270\u52a8\u5982\u4f55\u5f71\u54cd\u53cc\u5bf9\u7b56\u548c\u5047\u60f3\u535a\u5f08\u8fd9\u4e24\u79cd\u903c\u8fd1\u7eb3\u4ec0\u5747\u8861\u7684\u57fa\u4e8e\u6700\u4f73\u54cd\u5e94\u7684\u7b97\u6cd5\u3002", "method": "\u5728\u8ba1\u7b97\u6700\u4f73\u54cd\u5e94\u65f6\u5f15\u5165\u4e86\u6548\u7528\u6270\u52a8\u3002", "result": "\u8bc1\u660e\u4e86\u6270\u52a8\u53ef\u4ee5\u51cf\u5c11\u8fd9\u4e24\u79cd\u7b97\u6cd5\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u8fed\u4ee3\u6b21\u6570\u751a\u81f3\u53ef\u4ee5\u8fbe\u5230\u5bf9\u6570\u7ea7\u522b\u3002", "conclusion": "\u867d\u7136\u6548\u7528\u6270\u52a8\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f46\u5728\u5177\u6709\u5185\u5728\u7ed3\u6784\u7684\u7b56\u7565\u535a\u5f08\u4e2d\u53ef\u4ee5\u9ad8\u6548\u5b9e\u73b0\u3002"}}
{"id": "2511.11838", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.11838", "abs": "https://arxiv.org/abs/2511.11838", "authors": ["Mina Sarem", "Nuhamin Eshetu Deresse", "Els Verstrynge", "Stijn Fran\u00e7ois"], "title": "Phase-field modeling of cyclic behavior in quasi-brittle materials: a micromechanics-based approach", "comment": null, "summary": "In this paper, we extend the micromechanics-based phase-field modeling of fatigue fracture to capture cyclic plasticity with ratcheting. This mechanism is particularly important for low-cycle fatigue, where the accumulation of inelastic strains plays an important role in the progression to final failure. The ratcheting contribution is formulated through the evolution of ratcheting strain, which accumulates over loading cycles and captures the inelastic strain growth characteristic of cyclic plasticity in a thermodynamically consistent manner. The extended plastic potential allows independent control over deviatoric and volumetric ratcheting components, ensuring smooth evolution of the free energy and its derivatives. Numerical simulations are performed to evaluate the model under both monotonic and cyclic loading and to assess the influence of ratcheting on material response.", "AI": {"tldr": "\u672c\u6587\u5c06\u57fa\u4e8e\u5fae\u89c2\u529b\u5b66\u7684\u76f8\u573a\u65ad\u88c2\u6a21\u578b\u6269\u5c55\u5230\u5305\u542b\u68d8\u8f6e\u6548\u5e94\u7684\u5faa\u73af\u5851\u6027\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u5468\u75b2\u52b3\u4e0b\u7684\u4e0d\u5747\u5300\u5e94\u53d8\u7d2f\u79ef\u3002\u901a\u8fc7\u68d8\u8f6e\u5e94\u53d8\u6f14\u5316\u6765\u6355\u6349\u8fd9\u79cd\u5851\u6027\u884c\u4e3a\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u72ec\u7acb\u63a7\u5236\u504f\u5e94\u53d8\u548c\u4f53\u79ef\u68d8\u8f6e\u5206\u91cf\uff0c\u5e76\u80fd\u786e\u4fdd\u81ea\u7531\u80fd\u53ca\u5176\u5bfc\u6570\u7684\u5149\u6ed1\u6f14\u5316\u3002\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u51c6\u786e\u63cf\u8ff0\u5355\u8c03\u548c\u5faa\u73af\u52a0\u8f7d\u4e0b\u7684\u6750\u6599\u54cd\u5e94\uff0c\u5e76\u8bc4\u4f30\u68d8\u8f6e\u6548\u5e94\u5bf9\u6750\u6599\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u4f4e\u5468\u75b2\u52b3\u4e0b\uff0c\u68d8\u8f6e\u6548\u5e94\uff08\u4e0d\u5747\u5300\u5e94\u53d8\u7d2f\u79ef\uff09\u5bf9\u6700\u7ec8\u5931\u6548\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u65b0\u7684\u6a21\u578b\u6765\u6355\u6349\u8fd9\u79cd\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u68d8\u8f6e\u5e94\u53d8\u6f14\u5316\u7684\u65b9\u5f0f\uff0c\u5c06\u68d8\u8f6e\u6548\u5e94\u7eb3\u5165\u57fa\u4e8e\u5fae\u89c2\u529b\u5b66\u7684\u76f8\u573a\u65ad\u88c2\u6a21\u578b\u3002\u6269\u5c55\u7684\u5851\u6027\u52bf\u80fd\u5141\u8bb8\u72ec\u7acb\u63a7\u5236\u504f\u5e94\u53d8\u548c\u4f53\u79ef\u68d8\u8f6e\u5206\u91cf\uff0c\u4ee5\u5b9e\u73b0\u81ea\u7531\u80fd\u53ca\u5176\u5bfc\u6570\u7684\u5149\u6ed1\u6f14\u5316\u3002", "result": "\u6570\u503c\u6a21\u62df\u7ed3\u679c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u5355\u8c03\u548c\u5faa\u73af\u52a0\u8f7d\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u91cf\u5316\u4e86\u68d8\u8f6e\u6548\u5e94\u5bf9\u6750\u6599\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u5305\u542b\u68d8\u8f6e\u6548\u5e94\u7684\u5faa\u73af\u5851\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u9884\u6d4b\u4f4e\u5468\u75b2\u52b3\u4e0b\u7684\u65ad\u88c2\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2511.11844", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11844", "abs": "https://arxiv.org/abs/2511.11844", "authors": ["Olaoluwa A. Adegboye", "Kufre M. Udofia", "Akaninyene Obot"], "title": "Inverted C-Shaped Slots Loaded Exponential Tapered Triple Band Notched Ultra Wideband (UWB) Antenna", "comment": null, "summary": "This research presents a simple strategy for designing an exponentially tapered, triple-notched ultrawideband antenna. The antenna's microstrip line feed and radiating patch are matched using an exponential tapered transformer. This method inserts antenna notch elements, by cutting two inverted C-shaped slots in the radiating patch; frequency rejection can be achieved for WI-MAX and wireless LAN. The X-band is rejected by etching a U-shaped slot in the feedline. When embedding the notch elements, cross-coupling was minimized. The desired antenna was designed, simulated, and measured. The measured results and graphs show that our proposed design is reliable. This band notched antenna rejects 3.5 GHz (Wi-MAX band, 3.3 to 3.7 GHz), 5.5 GHz (WLAN 2 band, 5.15 to 5.825 GHz), and 7.5 GHz (for satellite downlink X - band-7.25 GHz to 7.75 GHz). The proposed antenna meets UWB design requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6307\u6570\u6e10\u53d8\u7684\u4e09\u9677\u6ce2\u8d85\u5bbd\u5e26\u5929\u7ebf\u8bbe\u8ba1\u7b56\u7565\uff0c\u5229\u7528\u6307\u6570\u6e10\u53d8\u53d8\u538b\u5668\u5339\u914d\u5fae\u5e26\u7ebf\u9988\u7535\u548c\u8f90\u5c04\u8d34\u7247\uff0c\u5e76\u901a\u8fc7\u5728\u8f90\u5c04\u8d34\u7247\u4e0a\u5207\u5272\u5012C\u5f62\u69fd\u548c\u5728\u9988\u7ebf\u4e0a\u8680\u523bU\u5f62\u69fd\u6765\u5b9e\u73b0\u591a\u9891\u6bb5\u7684\u9677\u6ce2\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u4ea4\u53c9\u8026\u5408\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u6b3e\u5177\u6709\u7279\u5b9a\u9891\u6bb5\u9677\u6ce2\u529f\u80fd\u7684\u5929\u7ebf\uff0c\u4ee5\u6ee1\u8db3\u8d85\u5bbd\u5e26\uff08UWB\uff09\u5e94\u7528\u7684\u9700\u6c42\uff0c\u5e76\u89e3\u51b3\u6f5c\u5728\u7684\u5e72\u6270\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6307\u6570\u6e10\u53d8\u53d8\u538b\u5668\u8fdb\u884c\u5fae\u5e26\u7ebf\u9988\u7535\u548c\u8f90\u5c04\u8d34\u7247\u7684\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u5728\u8f90\u5c04\u8d34\u7247\u4e0a\u5207\u5272\u5012C\u5f62\u69fd\u548c\u5728\u9988\u7ebf\u4e0a\u8680\u523bU\u5f62\u69fd\u6765\u5b9e\u73b0\u591a\u9891\u6bb5\uff08Wi-MAX\u3001WLAN\u3001X\u6ce2\u6bb5\uff09\u7684\u9677\u6ce2\uff0c\u540c\u65f6\u4f18\u5316\u8bbe\u8ba1\u4ee5\u6700\u5c0f\u5316\u4ea4\u53c9\u8026\u5408\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u3001\u4eff\u771f\u5e76\u6d4b\u91cf\u4e86\u4e00\u6b3e\u5929\u7ebf\uff0c\u8be5\u5929\u7ebf\u57283.5 GHz\u30015.5 GHz\u548c7.5 GHz\u4e09\u4e2a\u9891\u6bb5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4fe1\u53f7\u9677\u6ce2\uff0c\u4e14\u6ee1\u8db3\u8d85\u5bbd\u5e26\u8bbe\u8ba1\u8981\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6307\u6570\u6e10\u53d8\u7684\u4e09\u9677\u6ce2\u8d85\u5bbd\u5e26\u5929\u7ebf\u8bbe\u8ba1\u65b9\u6848\u662f\u53ef\u9760\u7684\uff0c\u80fd\u591f\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9891\u6bb5\u9009\u62e9\u6027\u9700\u6c42\u3002"}}
{"id": "2511.12000", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.12000", "abs": "https://arxiv.org/abs/2511.12000", "authors": ["Hiroki Ohta", "Aaron Merlin M\u00fcller", "Shunji Tsuchiya"], "title": "Measurement-Based Quantum Computation Using the Spin-1 XXZ Model with Uniaxial Anisotropy", "comment": "16 pages, 9 figures", "summary": "We demonstrate that the ground state of a spin-1 XXZ chain with uniaxial anisotropies, single-ion anisotropy $D$ and Ising anisotropy $J$, within the Haldane phase can serve as a resource state for measurement-based quantum computation implementing single-qubit gates. The gate fidelity of both elementary rotation gates and general single-qubit unitary gates composed of rotations about the $x$-, $y$-, and $z$-axes is evaluated, and is found to exceed 0.99 when $D$ or $J$ is appropriately tuned. Furthermore, we derive an analytic expression for the rotation-gate fidelity under the assumption that the state lies within the $\\mathbb Z_2\\times\\mathbb Z_2$-protected Haldane phase, showing that it is determined by the post-measurement spin-spin correlation function and the failure probability. The observed enhancement of gate fidelity in the spin-1 XXZ chain originates from the strengthening of antiferromagnetic (AFM) correlations near the AFM phase, which effectively suppresses failure states.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5177\u6709\u5355\u8f74\u5404\u5411\u5f02\u6027\u7684\u81ea\u65cb1 XXZ\u94fe\uff08\u5373Haldane\u76f8\u5185\u7684\u5355\u79bb\u5b50\u5404\u5411\u5f02\u6027D\u548cIsing\u5404\u5411\u5f02\u6027J\uff09\u7684\u57fa\u6001\u53ef\u4f5c\u4e3a\u5b9e\u73b0\u5355\u91cf\u5b50\u6bd4\u7279\u95e8\u6d4b\u91cf\u578b\u91cf\u5b50\u8ba1\u7b97\u7684\u8d44\u6e90\u6001\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5177\u6709\u7279\u5b9a\u5404\u5411\u5f02\u6027\u7684\u81ea\u65cb1 XXZ\u94fe\u57fa\u6001\u4f5c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u6001\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u5176\u5728Haldane\u76f8\u5185\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u5176\u5b9e\u73b0\u5355\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u8bc4\u4f30\uff0c\u63a8\u5bfc\u4e86\u65cb\u8f6c\u95e8\u4fdd\u771f\u5ea6\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u7814\u7a76\u4e86D\u6216J\u7684\u53d6\u503c\u5bf9\u4fdd\u771f\u5ea6\u7684\u5f71\u54cd\u3002\u5206\u6790\u4e86\u53cd\u94c1\u78c1\uff08AFM\uff09\u5173\u8054\u7684\u589e\u5f3a\u5982\u4f55\u6291\u5236\u5931\u8d25\u6001\uff0c\u4ece\u800c\u63d0\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u5f53D\u6216J\u9002\u5f53\u8c03\u6574\u65f6\uff0c\u57fa\u672c\u65cb\u8f6c\u95e8\u548c\u7531\u7ed5x\u3001y\u548cz\u8f74\u65cb\u8f6c\u7ec4\u6210\u7684\u901a\u7528\u5355\u91cf\u5b50\u6bd4\u7279\u9149\u95e8\u4fdd\u771f\u5ea6\u8d85\u8fc70.99\u3002\u5728$\\mathbb Z_2\\times\\mathbb Z_2$-\u4fdd\u62a4\u7684Haldane\u76f8\u5185\uff0c\u4fdd\u771f\u5ea6\u7531\u6d4b\u91cf\u540e\u7684\u81ea\u65cb-\u81ea\u65cb\u5173\u8054\u51fd\u6570\u548c\u5931\u8d25\u6982\u7387\u51b3\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574\u5355\u8f74\u5404\u5411\u5f02\u6027\uff0c\u81ea\u65cb1 XXZ\u94fe\u7684\u57fa\u6001\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728Haldane\u76f8\u5185\u3002\u53cd\u94c1\u78c1\u5173\u8054\u7684\u589e\u5f3a\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2511.13205", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13205", "abs": "https://arxiv.org/abs/2511.13205", "authors": ["Pavel Arkhipov", "Vladimir Kolmogorov"], "title": "Greedy matroid base packings with applications to dynamic graph density and orientations", "comment": null, "summary": "Greedy minimum weight spanning tree packings have proven to be useful in connectivity-related problems. We study the process of greedy minimum weight base packings in general matroids and explore its algorithmic applications.\n  When specialized to bicircular matroids, our results yield an algorithm for the approximate fully-dynamic densest subgraph density $\u03c1$. We maintain a $(1+\\varepsilon)$-approximation of the density with a worst-case update time $O((\u03c1\\varepsilon^{-2}+\\varepsilon^{-4})\u03c1\\log^3 m)$. It improves the dependency on $\\varepsilon$ from the current state-of-the-art worst-case update time complexity $O(\\varepsilon^{-6}\\log^3 n\\log\u03c1)$ [Chekuri, Christiansen, Holm, van der Hoog, Quanrud, Rotenberg, Schwiegelshohn, SODA'24]. We also can maintain an implicit fractional out-orientation with a guarantee that all out-degrees are at most $(1+\\varepsilon)\u03c1$.\n  Our algorithms above work by greedily packing pseudoforests, and require maintenance of a minimum-weight pseudoforest in a dynamically changing graph. We show that this problem can be solved in $O(\\log n)$ worst-case time per edge insertion or deletion.\n  For general matroids, we observe two characterizations of the limit of the base packings (``the vector of ideal loads''), which imply the characterizations from [Cen, Fleischmann, Li, Li, Panigrahi, FOCS'25], namely, their entropy-minimization theorem and their bottom-up cut hierarchy.\n  Finally, we give combinatorial results on the greedy tree packings. We show that a tree packing of $O(\u03bb^5\\log m)$ trees contains a tree crossing some min-cut once, which improves the bound $O(\u03bb^7\\log^3 m)$ from [Thorup, Combinatorica'07]. We also strengthen the lower bound on the edge load convergence rate from [de Vos, Christiansen, SODA'25], showing that Thorup's upper bound is tight up to a logarithmic factor.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d2a\u5fc3\u6700\u5c0f\u6743\u91cd\u751f\u6210\u6811\u6253\u5305\u5728\u4e00\u822c\u62df\u9635\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u52a8\u6001\u7a20\u5bc6\u5b50\u56fe\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u597d\u7684\u03b5\u4f9d\u8d56\u6027\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u8d2a\u5fc3\u6811\u6253\u5305\u7684\u7ec4\u5408\u5b66\u7ed3\u679c\uff0c\u6539\u8fdb\u4e86\u5df2\u6709\u754c\u9650\u3002", "motivation": "\u7814\u7a76\u8d2a\u5fc3\u6700\u5c0f\u6743\u91cd\u751f\u6210\u6811\u6253\u5305\u5728\u4e00\u822c\u62df\u9635\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u7b97\u6cd5\u4e0a\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u7a20\u5bc6\u5b50\u56fe\u95ee\u9898\u4e0a\u3002", "method": "\u901a\u8fc7\u8d2a\u5fc3\u6253\u5305\u4f2a\u68ee\u6797\u6765\u89e3\u51b3\u52a8\u6001\u7a20\u5bc6\u5b50\u56fe\u95ee\u9898\uff0c\u5e76\u7ef4\u62a4\u52a8\u6001\u53d8\u5316\u56fe\u4e2d\u7684\u6700\u5c0f\u6743\u91cd\u4f2a\u68ee\u6797\u3002\u5bf9\u4e8e\u4e00\u822c\u62df\u9635\uff0c\u7ed9\u51fa\u4e86\u57fa\u6253\u5305\u6781\u9650\u7684\u4e24\u79cd\u523b\u753b\u3002", "result": "\u5728 bicircular \u62df\u9635\u4e2d\uff0c\u5f97\u5230\u4e00\u4e2a\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7ef4\u62a4\u7a20\u5bc6\u5b50\u56fe\u5bc6\u5ea6\u7684 $(1+\\varepsilon)$ \u8fd1\u4f3c\u503c\uff0c\u5176\u6700\u574f\u60c5\u51b5\u66f4\u65b0\u65f6\u95f4\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u53e6\u5916\uff0c\u5c06\u8d2a\u5fc3\u6811\u6253\u5305\u4e2d\u7684\u6811\u6570\u91cf\u754c\u9650\u4ece $O(\u03bb^7\\log^3 m)$ \u6539\u8fdb\u5230 $O(\u03bb^5\\log m)$\uff0c\u5e76\u52a0\u5f3a\u4e86\u8fb9\u8d1f\u8f7d\u6536\u655b\u901f\u7387\u7684\u4e0b\u754c\u3002", "conclusion": "\u8d2a\u5fc3\u6700\u5c0f\u6743\u91cd\u751f\u6210\u6811\u6253\u5305\u5728\u4e00\u822c\u62df\u9635\u548c\u7b97\u6cd5\u5e94\u7528\uff0c\u7279\u522b\u662f\u52a8\u6001\u7a20\u5bc6\u5b50\u56fe\u95ee\u9898\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5e76\u4e14\u5728\u7ec4\u5408\u5b66\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u7406\u8bba\u4e0a\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.12873", "categories": ["cs.SI", "cs.CY", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12873", "abs": "https://arxiv.org/abs/2511.12873", "authors": ["Jacob Erickson"], "title": "Rethinking the filter bubble? Developing a research agenda for the protective filter bubble", "comment": "This work has been published in Big Data & Society. Please cite the journal version", "summary": "Filter bubbles and echo chambers have received global attention from scholars, media organizations, and the general public. Filter bubbles have primarily been regarded as intrinsically negative, and many studies have sought to minimize their influence. The detrimental influence of filter bubbles is well-studied. Filter bubbles may, for example, create information silos, amplify misinformation, and promote hatred and extremism. However, comparatively few studies have considered the other side of the filter bubble; its protective benefits, particularly to marginalized communities and those living in countries with low levels of press freedom. Through a review of the literature on digital safe spaces and protective filter bubbles, this commentary suggests that there may be a need to rethink the filter bubble, and it proposes several areas for future research.", "AI": {"tldr": "\u6587\u7ae0\u4e3b\u8981\u63a2\u8ba8\u4e86\u8fc7\u6ee4\u6c14\u6ce1\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u4e5f\u63d0\u51fa\u4e86\u5176\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u548c\u65b0\u95fb\u81ea\u7531\u5ea6\u4f4e\u56fd\u5bb6/\u5730\u533a\u5c45\u6c11\u7684\u4fdd\u62a4\u6027\u76ca\u5904\uff0c\u5e76\u5efa\u8bae\u91cd\u65b0\u5ba1\u89c6\u8fc7\u6ee4\u6c14\u6ce1\u53ca\u5176\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8fc7\u6ee4\u6c14\u6ce1\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u548c\u65b0\u95fb\u81ea\u7531\u5ea6\u4f4e\u56fd\u5bb6/\u5730\u533a\u5c45\u6c11\u7684\u4fdd\u62a4\u6027\u76ca\u5904\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6\u8fc7\u6ee4\u6c14\u6ce1\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5173\u4e8e\u6570\u5b57\u5b89\u5168\u7a7a\u95f4\u548c\u4fdd\u62a4\u6027\u8fc7\u6ee4\u6c14\u6ce1\u7684\u6587\u732e\uff0c\u63d0\u51fa\u65b0\u7684\u89c2\u70b9\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8fc7\u6ee4\u6c14\u6ce1\u53ef\u80fd\u5177\u6709\u4fdd\u62a4\u6027\u76ca\u5904\uff0c\u5c24\u5176\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u548c\u65b0\u95fb\u81ea\u7531\u5ea6\u4f4e\u56fd\u5bb6/\u5730\u533a\u5c45\u6c11\u800c\u8a00\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fc7\u6ee4\u6c14\u6ce1\u7684\u5b9a\u4e49\u548c\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5176\u4fdd\u62a4\u6027\u76ca\u5904\u53ca\u5176\u5728\u4e0d\u540c\u793e\u7fa4\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.11593", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11593", "abs": "https://arxiv.org/abs/2511.11593", "authors": ["Matthew Morris", "Ian Horrocks"], "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks", "comment": "Full version (with appendices) of paper accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.", "AI": {"tldr": "GNNs\u5e38\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff0c\u4f46\u5176\u9ed1\u76d2\u6027\u8d28\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u7814\u7a76\u4e86\u5e26\u6709\u5747\u503c\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684GNN\uff08MAGNN\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u7cbe\u786e\u7684\u5355\u8c03\u89c4\u5219\u7c7b\u522b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u9650\u7684\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u6765\u89e3\u91ca\u4efb\u4f55MAGNN\u9884\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u975e\u8d1f\u6743\u91cd\u53ef\u4ee5\u63d0\u9ad8\u6216\u4fdd\u6301\u6027\u80fd\uff0c\u5e76\u4e14\u751f\u6210\u7684\u89c4\u5219\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u751a\u81f3\u80fd\u66b4\u9732\u6a21\u578b\u95ee\u9898\u3002", "motivation": "GNNs\u5e38\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff0c\u4f46\u5176\u9ed1\u76d2\u6027\u8d28\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u7279\u522b\u662f\uff0c\u5e26\u6709\u5747\u503c\u805a\u5408\u51fd\u6570\u7684GNN\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u80fd\u529b\u7684\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u5e26\u6709\u5747\u503c\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684GNN\uff08MAGNN\uff09\uff0c\u8bc1\u660e\u5176\u7cbe\u786e\u7684\u5355\u8c03\u89c4\u5219\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u53d7\u9650\u7684\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u6765\u89e3\u91ca\u4efb\u4f55MAGNN\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u975e\u8d1f\u6743\u91cd\u53ef\u4ee5\u63d0\u9ad8\u6216\u4fdd\u6301\u5728\u6807\u51c6\u5f52\u7eb3\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002\u751f\u6210\u7684\u89c4\u5219\u5728\u5b9e\u8df5\u4e2d\u662f\u53ef\u9760\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u4f9b\u6709\u89c1\u5730\u7684\u89e3\u91ca\uff0c\u751a\u81f3\u66b4\u9732\u6a21\u578b\u4e2d\u7684\u95ee\u9898\u3002", "conclusion": "\u5c06GNN\u9650\u5236\u4e3a\u5177\u6709\u975e\u8d1f\u6743\u91cd\u7684\u5747\u503c\u805a\u5408\u53ef\u4ee5\u83b7\u5f97\u53ef\u9760\u4e14\u6709\u89c1\u5730\u7684\u89e3\u91ca\uff0c\u5e76\u6709\u53ef\u80fd\u66b4\u9732\u6a21\u578b\u4e2d\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12599", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12599", "abs": "https://arxiv.org/abs/2511.12599", "authors": ["Bijia Liu", "Ronghao Dang"], "title": "FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets", "comment": "LLM Applications, LLM Agents, Financial Technology", "summary": "Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.12125", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.12125", "abs": "https://arxiv.org/abs/2511.12125", "authors": ["Samer Houri", "Rachid Haouari", "Bart P. Weekers", "Veronique Rochus"], "title": "Acoustically-Coupled MEMS Transducer Pairs with Loss and Gain", "comment": null, "summary": "This work treats the dynamics of pairs of microelectromechanical ultrasound transducers (MUTs) that are immersed in water and acoustically coupled through the fluid medium. A series of these transducer pairs with varying diameters (and thus resonance frequency) and pitch separation (and thus coupling strength) are fabricated and measured. The work presented here models and quantifies the open-loop coupling between the MEMS transducer pairs and its dependence on pitch. Furthermore, a gain feedback loop is systematically applied to one of the device pair and the dynamics of the acoustically-coupled gain-loss system is investigated, and the formation of an exceptional-point or of an Hopf bifurcation is equally used to quantify the coupling coefficient. This work provides an experimental study of acoustic coupling in MUT transducers, as well as an exploration of the formation of exceptional points in acoustically-coupled MEMS transducers.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u6d78\u5165\u6c34\u4e2d\u5e76\u901a\u8fc7\u6d41\u4f53\u4ecb\u8d28\u58f0\u8026\u5408\u7684\u5fae\u673a\u7535\u8d85\u58f0\u6362\u80fd\u5668\uff08MUT\uff09\u5bf9\u7684\u52a8\u529b\u5b66\u3002", "motivation": "\u7814\u7a76\u6d78\u5165\u6c34\u4e2d\u5e76\u901a\u8fc7\u6d41\u4f53\u4ecb\u8d28\u58f0\u8026\u5408\u7684\u5fae\u673a\u7535\u8d85\u58f0\u6362\u80fd\u5668\uff08MUT\uff09\u5bf9\u7684\u52a8\u529b\u5b66\uff0c\u5e76\u63a2\u7d22\u8026\u5408\u7cfb\u6570\u7684\u5f62\u6210\u3002", "method": "\u5236\u9020\u548c\u6d4b\u91cf\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u4e0d\u540c\u76f4\u5f84\u548c\u95f4\u8ddd\u7684\u6362\u80fd\u5668\u5bf9\u3002\u5bf9\u4e00\u4e2a\u8bbe\u5907\u5bf9\u65bd\u52a0\u589e\u76ca\u53cd\u9988\u56de\u8def\uff0c\u7814\u7a76\u4e86\u58f0\u8026\u5408\u589e\u76ca-\u635f\u8017\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u5e76\u5229\u7528\u4e86\u7cbe\u70b9\u7684\u5f62\u6210\u6216\u970d\u666e\u592b\u5206\u5c94\u6765\u91cf\u5316\u8026\u5408\u7cfb\u6570\u3002", "result": "\u7814\u7a76\u4e86MUT\u6362\u80fd\u5668\u5bf9\u7684\u58f0\u8026\u5408\u53ca\u5176\u5bf9\u95f4\u8ddd\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u7cbe\u70b9\u7684\u5f62\u6210\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9MUT\u6362\u80fd\u5668\u58f0\u8026\u5408\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u5e76\u5bf9\u58f0\u8026\u5408MEMS\u6362\u80fd\u5668\u4e2d\u7cbe\u70b9\u7684\u5f62\u6210\u8fdb\u884c\u4e86\u63a2\u7d22\u3002"}}
{"id": "2511.11976", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11976", "abs": "https://arxiv.org/abs/2511.11976", "authors": ["Musawenkosi K. Ncube", "Pallab Barai", "Selva Chandrasekaran Selvaraj", "Larry A. Curtiss", "Anh T. Ngo", "Venkat Srinivasan"], "title": "Ionic Interdiffusion at Cathode-Solid-Electrolyte Interface: A Machine Learning-Assisted Multiscale Investigation and Mitigation Strategies", "comment": null, "summary": "Future lithium-based batteries are expected to use solid electrolytes to achieve higher energy density and fast charge capabilities. The majority of solid electrolytes are thermodynamically unstable against layered oxide cathodes. Here, the stability of LiCoO2 (LCO) cathode with Li10GeP2S12 (LGPS) solid electrolyte is investigated using ab initio molecular dynamics (AIMD) and machine learning molecular dynamics (MLMD). The propensity of ionic interdiffusion, formation of a passivation layer, and corresponding decay in cell performance is addressed using a continuum model. The large-scale MLMD simulations confirm that the LCO|LGPS interface permits interdiffusion of Co and other ionic species, leading to the formation and growth of a resistive interphase and dramatic capacity fade even in the first cycle. We then examine the literature evidence that incorporating a thin layer of LiNb0.5Ta0.5O3 (LNTO) between LCO and LGPS prevents the interdiffusion of ions. Atomistic simulations suggest that the substitution of Li in LNTO with Co is not thermodynamically favorable, which helps to minimize the ionic interdiffusion process. The stable Nb/Ta5+ states form a rigid metal-oxide framework, which consequently also prevents the substitution of Nb/Ta. However, continuum level analysis suggests that due to the higher mechanical stiffness of LNTO, interfacial delamination between the LCO and LNTO is possible, which can minimize the effectiveness of the protective layer. This paper suggests the need for the development of novel interlayers that balance low interdiffusion with low stiffness.", "AI": {"tldr": "\u56fa\u6001\u7535\u89e3\u8d28\u5728\u9502\u7535\u6c60\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e0e\u6c27\u5316\u7269\u9634\u6781\u7684\u70ed\u529b\u5b66\u4e0d\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u6311\u6218\u3002", "motivation": "\u7814\u7a76LiCoO2\uff08LCO\uff09\u9634\u6781\u4e0eLi10GeP2S12\uff08LGPS\uff09\u56fa\u6001\u7535\u89e3\u8d28\u754c\u9762\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4ece\u5934\u5206\u5b50\u52a8\u529b\u5b66\uff08AIMD\uff09\u3001\u673a\u5668\u5b66\u4e60\u5206\u5b50\u52a8\u529b\u5b66\uff08MLMD\uff09\u548c\u8fde\u7eed\u4ecb\u8d28\u6a21\u578b\u7814\u7a76LCO|LGPS\u754c\u9762\u7684\u79bb\u5b50\u4e92\u6269\u6563\u3001\u949d\u5316\u5c42\u5f62\u6210\u548c\u7535\u6c60\u6027\u80fd\u8870\u51cf\u3002\u5e76\u7814\u7a76\u4e86LiNb0.5Ta0.5O3\uff08LNTO\uff09\u4f5c\u4e3a\u4fdd\u62a4\u5c42\u7684\u6709\u6548\u6027\u3002", "result": "MLMD\u6a21\u62df\u8868\u660e\uff0cLCO|LGPS\u754c\u9762\u5b58\u5728\u79bb\u5b50\u4e92\u6269\u6563\uff0c\u5f62\u6210\u7535\u963b\u6027\u4e2d\u95f4\u76f8\uff0c\u5bfc\u81f4\u5bb9\u91cf\u5feb\u901f\u8870\u51cf\u3002LNTO\u5c42\u53ef\u963b\u6b62\u79bb\u5b50\u4e92\u6269\u6563\uff0c\u4f46\u5176\u8f83\u9ad8\u7684\u673a\u68b0\u521a\u5ea6\u53ef\u80fd\u5bfc\u81f4\u754c\u9762\u5206\u5c42\uff0c\u964d\u4f4e\u4fdd\u62a4\u5c42\u6548\u679c\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u517c\u5177\u4f4e\u79bb\u5b50\u4e92\u6269\u6563\u548c\u4f4e\u673a\u68b0\u521a\u5ea6\u7684\u65b0\u578b\u754c\u9762\u5c42\u6750\u6599\u3002"}}
{"id": "2511.11700", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11700", "abs": "https://arxiv.org/abs/2511.11700", "authors": ["Jiahui Wang", "Haiyue Zhu", "Haoren Guo", "Abdullah Al Mamun", "Cheng Xiang", "Tong Heng Lee"], "title": "EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance", "comment": "AAAI 2026", "summary": "Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u8bad\u7ec3\u7684EPSegFZ\u7f51\u7edc\uff0c\u901a\u8fc7ProERA\u548cDRPE\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u548c\u67e5\u8be2-\u539f\u578b\u5bf9\u5e94\uff0c\u5e76\u901a\u8fc7LGPE\u6a21\u5757\u5229\u7528\u6587\u672c\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff1b\u672a\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\uff1b\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u6587\u672c\u6ce8\u91ca\u7b49\u5176\u4ed6\u6709\u7528\u6570\u636e\uff0c\u5f71\u54cd\u4e86\u6027\u80fd\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "method": "1. \u5f15\u5165\u65e0\u9700\u9884\u8bad\u7ec3\u7684EPSegFZ\u7f51\u7edc\u3002 2. \u91c7\u7528\u539f\u578b\u589e\u5f3a\u5bc4\u5b58\u5668\u6ce8\u610f\u529b\uff08ProERA\uff09\u6a21\u5757\u548c\u57fa\u4e8e\u53cc\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08DRPE\uff09\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u548c\u67e5\u8be2-\u539f\u578b\u5bf9\u5e94\u3002 3. \u91c7\u7528\u8bed\u8a00\u5f15\u5bfc\u539f\u578b\u5d4c\u5165\uff08LGPE\uff09\u6a21\u5757\uff0c\u5229\u7528\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u5c11\u6837\u672c\u6027\u80fd\u548c\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406\u3002", "result": "\u5728S3DIS\u548cScanNet\u6570\u636e\u96c6\u4e0a\uff0cEPSegFZ\u7684\u6027\u80fd\u5206\u522b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd55.68%\u548c3.82%\u3002", "conclusion": "EPSegFZ\u7f51\u7edc\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u5c11\u6837\u672c3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u4f9d\u8d56\u3001\u4fe1\u606f\u5229\u7528\u548c\u96f6\u6837\u672c\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.12487", "categories": ["cs.NE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12487", "abs": "https://arxiv.org/abs/2511.12487", "authors": ["Onkar Shelar", "Travis Desell"], "title": "Evolving Prompts for Toxicity Search in Large Language Models", "comment": "pre-print", "summary": "Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5b89\u5168\u5bf9\u9f50\u540e\u4ecd\u6613\u53d7\u8bf1\u5bfc\u4ea7\u751f\u6709\u6bd2\u5185\u5bb9\u7684\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5f71\u54cd\u3002ToxSearch \u662f\u4e00\u4e2a\u9ed1\u76d2\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u7a33\u6001\u5faa\u73af\u6f14\u5316\u63d0\u793a\u6765\u6d4b\u8bd5\u6a21\u578b\u5b89\u5168\u6027\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u8bcd\u6cd5\u66ff\u6362\u3001\u5426\u5b9a\u3001\u53cd\u5411\u7ffb\u8bd1\u3001\u91ca\u4e49\u548c\u4e24\u79cd\u8bed\u4e49\u4ea4\u53c9\u7b49\u591a\u79cd\u7b97\u5b50\uff0c\u5e76\u7531\u5ba1\u6838\u5458\u63d0\u4f9b\u9002\u5e94\u5ea6\u6307\u5bfc\u3002\u7b97\u5b50\u7ea7\u5206\u6790\u663e\u793a\u4e86\u5f02\u6784\u884c\u4e3a\uff1a\u8bcd\u6cd5\u66ff\u6362\u63d0\u4f9b\u6700\u4f73\u7684\u6536\u76ca-\u65b9\u5dee\u6743\u8861\uff0c\u8bed\u4e49\u76f8\u4f3c\u6027\u4ea4\u53c9\u4f5c\u4e3a\u7cbe\u786e\u7684\u4f4e\u541e\u5410\u91cf\u63d2\u5165\u5668\uff0c\u5168\u5c40\u91cd\u5199\u5219\u8868\u73b0\u51fa\u9ad8\u65b9\u5dee\u548c\u8f83\u9ad8\u7684\u62d2\u7edd\u6210\u672c\u3002\u901a\u8fc7\u5728 LLaMA 3.1 8B \u4e0a\u6f14\u5316\u7684\u7cbe\u82f1\u63d0\u793a\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6709\u610f\u4e49\u4f46\u51cf\u5f31\u7684\u8de8\u6a21\u578b\u8fc1\u79fb\uff0c\u5927\u591a\u6570\u76ee\u6807\u7684\u6bd2\u6027\u5927\u7ea6\u51cf\u534a\uff0c\u8f83\u5c0f\u7684 LLaMA 3.2 \u53d8\u4f53\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u62b5\u6297\u529b\uff0c\u4e00\u4e9b\u8de8\u67b6\u6784\u6a21\u578b\u4fdd\u7559\u4e86\u8f83\u9ad8\u7684\u6bd2\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c0f\u578b\u3001\u53ef\u63a7\u7684\u6270\u52a8\u662f\u7cfb\u7edf\u5316\u7ea2\u961f\u6d4b\u8bd5\u7684\u6709\u6548\u8f7d\u4f53\uff0c\u5e76\u4e14\u9632\u5fa1\u63aa\u65bd\u5e94\u9884\u671f\u5bf9\u6297\u6027\u63d0\u793a\u7684\u8de8\u6a21\u578b\u91cd\u7528\uff0c\u800c\u4e0d\u662f\u4ec5\u5173\u6ce8\u5355\u4e00\u6a21\u578b\u52a0\u56fa\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5b89\u5168\u5bf9\u9f50\u540e\u4ecd\u6613\u53d7\u8bf1\u5bfc\u4ea7\u751f\u6709\u6bd2\u5185\u5bb9\u7684\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5f71\u54cd\u3002", "method": "ToxSearch \u662f\u4e00\u4e2a\u9ed1\u76d2\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u7a33\u6001\u5faa\u73af\u6f14\u5316\u63d0\u793a\u6765\u6d4b\u8bd5\u6a21\u578b\u5b89\u5168\u6027\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u8bcd\u6cd5\u66ff\u6362\u3001\u5426\u5b9a\u3001\u53cd\u5411\u7ffb\u8bd1\u3001\u91ca\u4e49\u548c\u4e24\u79cd\u8bed\u4e49\u4ea4\u53c9\u7b49\u591a\u79cd\u7b97\u5b50\uff0c\u5e76\u7531\u5ba1\u6838\u5458\u63d0\u4f9b\u9002\u5e94\u5ea6\u6307\u5bfc\u3002", "result": "\u901a\u8fc7\u5728 LLaMA 3.1 8B \u4e0a\u6f14\u5316\u7684\u7cbe\u82f1\u63d0\u793a\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6709\u610f\u4e49\u4f46\u51cf\u5f31\u7684\u8de8\u6a21\u578b\u8fc1\u79fb\uff0c\u5927\u591a\u6570\u76ee\u6807\u7684\u6bd2\u6027\u5927\u7ea6\u51cf\u534a\uff0c\u8f83\u5c0f\u7684 LLaMA 3.2 \u53d8\u4f53\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u62b5\u6297\u529b\uff0c\u4e00\u4e9b\u8de8\u67b6\u6784\u6a21\u578b\u4fdd\u7559\u4e86\u8f83\u9ad8\u7684\u6bd2\u6027\u3002", "conclusion": "\u5c0f\u578b\u3001\u53ef\u63a7\u7684\u6270\u52a8\u662f\u7cfb\u7edf\u5316\u7ea2\u961f\u6d4b\u8bd5\u7684\u6709\u6548\u8f7d\u4f53\uff0c\u5e76\u4e14\u9632\u5fa1\u63aa\u65bd\u5e94\u9884\u671f\u5bf9\u6297\u6027\u63d0\u793a\u7684\u8de8\u6a21\u578b\u91cd\u7528\uff0c\u800c\u4e0d\u662f\u4ec5\u5173\u6ce8\u5355\u4e00\u6a21\u578b\u52a0\u56fa\u3002"}}
{"id": "2511.12624", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.12624", "abs": "https://arxiv.org/abs/2511.12624", "authors": ["Weiping Yang", "Shilin Zhou", "Hui Xu", "Jiawei Xue", "Changlin Chen"], "title": "Segmented Exponent Alignment and Dynamic Wordline Activation for Floating-Point Analog CIM Macros", "comment": "accepted by 2025 IEEE 32nd International Conference on Electronics Circuits and Systems (ICECS)", "summary": "With the rise of compute-in-memory (CIM) accelerators, floating-point multiply-and-accumulate (FP-MAC) operations have gained extensive attention for their higher accuracy over integer MACs in neural networks. However, the hardware overhead caused by exponent comparison and mantissa alignment, along with the delay introduced by bit-serial input methods, remains a hinder to implement FP-MAC efficiently. In view of this, we propose Segmented Exponent Alignment (SEA) and Dynamic Wordline Activation (DWA) strategies. SEA exploits the observation that input exponents are often clustered around zero or within a narrow range. By segmenting the exponent space and aligning mantissas accordingly, SEA eliminates the need for maximum exponent detection and reduces input mantissa shifting, and thus reduces the processing latency. DWA further reduces latency and maintains accuracy by activating wordlines based on the exponent segments defined by SEA. Simulation results demonstrate that, when compared with conventional comparison tree based maximum exponent alignment method, our approach saves 63.8\\% power consumption, and achieves a 40.87\\% delay reduction on the VGG16-CIFAR10 benchmark.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51fa\u5206\u6bb5\u6307\u6570\u5bf9\u9f50\uff08SEA\uff09\u548c\u52a8\u6001\u5b57\u7ebf\u6fc0\u6d3b\uff08DWA\uff09\u7b56\u7565\uff0c\u4f18\u5316\u4e86 CIM \u52a0\u901f\u5668\u4e2d\u7684\u6d6e\u70b9\u4e58\u52a0\uff08FP-MAC\uff09\u8fd0\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u529f\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u795e\u7ecf\u5143\u7f51\u7edc\u4e2d\uff0c\u6d6e\u70b9\u4e58\u52a0\uff08FP-MAC\uff09\u8fd0\u7b97\u56e0\u5176\u6bd4\u6574\u6570\u8fd0\u7b97\u66f4\u9ad8\u7684\u7cbe\u5ea6\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u6307\u6570\u6bd4\u8f83\u548c\u5c3e\u6570\u5bf9\u9f50\u5e26\u6765\u7684\u786c\u4ef6\u5f00\u9500\u4ee5\u53ca\u4f4d\u4e32\u884c\u8f93\u5165\u65b9\u6cd5\u5f15\u5165\u7684\u5ef6\u8fdf\uff0c\u963b\u788d\u4e86 FP-MAC \u7684\u9ad8\u6548\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u5206\u6bb5\u6307\u6570\u5bf9\u9f50\uff08SEA\uff09\u548c\u52a8\u6001\u5b57\u7ebf\u6fc0\u6d3b\uff08DWA\uff09\u7b56\u7565\u3002SEA \u5229\u7528\u8f93\u5165\u6307\u6570\u5e38\u805a\u96c6\u5728\u96f6\u9644\u8fd1\u6216\u7a84\u8303\u56f4\u5185\u8fd9\u4e00\u7279\u70b9\uff0c\u901a\u8fc7\u5206\u5272\u6307\u6570\u7a7a\u95f4\u5e76\u76f8\u5e94\u5730\u5bf9\u9f50\u5c3e\u6570\uff0c\u65e0\u9700\u6700\u5927\u6307\u6570\u68c0\u6d4b\uff0c\u51cf\u5c11\u8f93\u5165\u5c3e\u6570\u79fb\u4f4d\uff0c\u4ece\u800c\u964d\u4f4e\u5904\u7406\u5ef6\u8fdf\u3002DWA \u8fdb\u4e00\u6b65\u6839\u636e SEA \u5b9a\u4e49\u7684\u6307\u6570\u6bb5\u6fc0\u6d3b\u5b57\u7ebf\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u5e76\u4fdd\u6301\u7cbe\u5ea6\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6bd4\u8f83\u6811\u7684\u6700\u5927\u6307\u6570\u5bf9\u9f50\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728 VGG16-CIFAR10 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8282\u7701\u4e86 63.8% \u7684\u529f\u8017\uff0c\u5e76\u5b9e\u73b0\u4e86 40.87% \u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "SEA \u548c DWA \u7b56\u7565\u80fd\u591f\u9ad8\u6548\u5730\u5b9e\u73b0 CIM \u52a0\u901f\u5668\u4e2d\u7684 FP-MAC \u8fd0\u7b97\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.12629", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12629", "abs": "https://arxiv.org/abs/2511.12629", "authors": ["Shiyun Lin"], "title": "Bandit Learning in Housing Markets", "comment": "Accepted to AAAI 2026 as oral", "summary": "The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \\log T / \u0394^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $\u0394$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal.", "AI": {"tldr": "We introduce a statistical learning model for the housing market where preferences are learned through repeated interactions using the multi-player multi-armed bandit framework. We propose core regret as the market objective and achieve order-optimal regret bounds in both centralized and decentralized settings.", "motivation": "The paper addresses the lack of attention paid to housing market models where agent preferences are unknown and must be learned through repeated interactions, a common scenario in real-world exchange economies.", "method": "We model the housing market as a multi-player multi-armed bandit problem where agents learn their preferences over goods from stochastic rewards. We analyze both centralized and decentralized learning approaches and derive regret bounds.", "result": "We establish $O(N \text{ log } T / \u0394^2)$ upper bounds on core regret for both centralized and decentralized approaches. For the decentralized setting, we also provide a matching lower bound, indicating that our proposed algorithm is order-optimal.", "conclusion": "The paper demonstrates that it is possible to efficiently learn preferences and achieve core-stable allocations in housing markets even when preferences are initially unknown, with proposed algorithms being order-optimal in decentralized settings."}}
{"id": "2511.11858", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.11858", "abs": "https://arxiv.org/abs/2511.11858", "authors": ["Cindy H. Shi", "Mia C. Cano", "Jason R. Casar", "Parivash Moradifar", "Beatriz G. Robinson", "Julia A. Kaltschmidt", "Miriam B. Goodman", "Jennifer A. Dionne"], "title": "Mechanosensitive polymer matrices of biologically-relevant compliance based on upconverting nanoparticles", "comment": "17 pages, 5 figures", "summary": "Upconverting nanoparticles (UCNPs) are promising optical biomechanical force sensors due to their near infrared excitation, low toxicity, photostability, and linear colorimetric sensitivity to micronewtons of force. Recently, a composite force sensor based on UCNPs embedded in a polystyrene microbead enabled the first real time measurement of feeding forces in living nematodes. However, the comparatively large stiffness of polystyrene only makes it relevant to biomedical application in a small subset of biological tissue. To facilitate deployment of UCNPs into biological tissues with a range of mechanical properties, we expand upon polymer UCNP composite systems by embedding UCNPs in three polymer matrices with varying stiffnesses (epoxy resin, polydimethylsiloxane, and alginate hydrogels). Furthermore, to enhance these composites mechanosensitivity, we methodically investigate using two different core-shell architectures of SrLuF based UCNPs doped with ytterbium, erbium, and varying manganese concentrations. We calibrate polymer UCNP composite optical force sensitivity with colocalized atomic force and confocal microscopy. Using the red to green emission ratio (Delta Percent IRed:IGreen) as the force read-out, we determine that SrLuF:Yb0.28Er0.025Mn0.013 with SrYF inert shell dispersed in epoxy resin exhibits the greatest emission color change (12 Delta Percent IRed:IGreen per microNewton). Finally, we map forces in the epoxy UCNP composite on the macroscale between the joint of a chicken wing bone using a commercially available wide field microscope, thereby demonstrating its ability to optically measure pressures in situ. This work establishes the utility and modularity of the UCNP polymer composite system for force sensing in geometrically and mechanically diverse biological systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u4e0a\u8f6c\u6362\u7eb3\u7c73\u7c92\u5b50\uff08UCNPs\uff09\u5d4c\u5165\u4e09\u79cd\u4e0d\u540c\u786c\u5ea6\u7684\u805a\u5408\u7269\u57fa\u8d28\u4e2d\uff08\u73af\u6c27\u6811\u8102\u3001\u805a\u4e8c\u7532\u57fa\u7845\u6c27\u70f7\u548c\u6d77\u85fb\u9178\u76d0\u6c34\u51dd\u80f6\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u6838\u58f3\u7ed3\u6784SrLuF\u57faUCNPs\uff0c\u4ee5\u63d0\u9ad8\u5176\u529b\u4f20\u611f\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73af\u6c27\u6811\u8102\u4e2d\u7684SrLuF:Yb0.28Er0.025Mn0.013\uff08\u5177\u6709SrYF\u60f0\u6027\u58f3\uff09\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u53d1\u5149\u989c\u8272\u53d8\u5316\uff08\u6bcf\u5fae\u725b\u987f12 Delta % IRed:IGreen\uff09\uff0c\u5e76\u4e14\u5728\u9e21\u7fc5\u9aa8\u5173\u8282\u5904\u8fdb\u884c\u4e86\u5b8f\u89c2\u529b\u4f20\u611f\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u7269\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u805a\u5408\u7269UCNP\u590d\u5408\u6750\u6599\u5728\u5177\u6709\u4e0d\u540c\u673a\u68b0\u7279\u6027\u7684\u751f\u7269\u7ec4\u7ec7\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u805a\u5408\u7269\u786c\u5ea6\u7684\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5c06UCNPs\u5d4c\u5165\u5177\u6709\u4e0d\u540c\u786c\u5ea6\u7684\u805a\u5408\u7269\u57fa\u8d28\u4e2d\uff0c\u5e76\u4f18\u5316UCNP\u7684\u6838\u58f3\u7ed3\u6784\u548c\u63ba\u6742\u6d53\u5ea6\uff0c\u6765\u6269\u5c55\u805a\u5408\u7269UCNP\u590d\u5408\u6750\u6599\u5728\u4e0d\u540c\u751f\u7269\u7ec4\u7ec7\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u9ad8\u5176\u529b\u4f20\u611f\u7684\u7075\u654f\u5ea6\u3002", "method": "1. \u5c06UCNPs\u5d4c\u5165\u73af\u6c27\u6811\u8102\u3001\u805a\u4e8c\u7532\u57fa\u7845\u6c27\u70f7\u548c\u6d77\u85fb\u9178\u76d0\u6c34\u51dd\u80f6\u4e09\u79cd\u4e0d\u540c\u786c\u5ea6\u7684\u805a\u5408\u7269\u57fa\u8d28\u4e2d\u3002 2. \u7814\u7a76\u4e24\u79cd\u4e0d\u540c\u7684SrLuF\u57faUCNPs\uff08\u63ba\u6742Yb, Er, Mn\uff09\u7684\u6838\u58f3\u7ed3\u6784\u3002 3. \u4f7f\u7528\u539f\u5b50\u529b\u663e\u5fae\u955c\u548c\u5171\u805a\u7126\u663e\u5fae\u955c\u5bf9\u805a\u5408\u7269UCNP\u590d\u5408\u6750\u6599\u8fdb\u884c\u529b\u5b66\u4f20\u611f\u6821\u51c6\u3002 4. \u4f7f\u7528\u7ea2\u7eff\u53d1\u5149\u6bd4\uff08Delta % IRed:IGreen\uff09\u4f5c\u4e3a\u529b\u8bfb\u6570\u3002 5. \u5728\u9e21\u7fc5\u9aa8\u5173\u8282\u5904\u8fdb\u884c\u5b8f\u89c2\u529b\u4f20\u611f\u6d4b\u8bd5\u3002", "result": "1. \u73af\u6c27\u6811\u8102\u4e2d\u7684SrLuF:Yb0.28Er0.025Mn0.013\uff08\u5177\u6709SrYF\u60f0\u6027\u58f3\uff09\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u53d1\u5149\u989c\u8272\u53d8\u5316\uff0812 Delta % IRed:IGreen / \u00b5N\uff09\u3002 2. \u6210\u529f\u5728\u9e21\u7fc5\u9aa8\u5173\u8282\u5904\u8fdb\u884c\u4e86\u5b8f\u89c2\u529b\u4f20\u611f\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u539f\u4f4d\u6d4b\u91cf\u538b\u529b\u80fd\u529b\u3002", "conclusion": "UCNP\u805a\u5408\u7269\u590d\u5408\u6750\u6599\u7cfb\u7edf\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6a21\u5757\u5316\uff0c\u53ef\u7528\u4e8e\u5728\u51e0\u4f55\u548c\u529b\u5b66\u4e0a\u591a\u6837\u5316\u7684\u751f\u7269\u7cfb\u7edf\u4e2d\u8fdb\u884c\u529b\u4f20\u611f\u3002\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u805a\u5408\u7269\u57fa\u8d28\u548cUCNP\u7ed3\u6784\uff0c\u53ef\u4ee5\u4f18\u5316\u590d\u5408\u6750\u6599\u7684\u529b\u4f20\u611f\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u751f\u7269\u529b\u5b66\u6d4b\u91cf\u3002"}}
{"id": "2511.11947", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11947", "abs": "https://arxiv.org/abs/2511.11947", "authors": ["Tri Nhu Do"], "title": "AI-Open-RAN for Non-Terrestrial Networks", "comment": null, "summary": "In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u5f00\u653e\u67b6\u6784\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u529f\u80fd\u7684\u4e00\u4f53\u5316\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u6982\u5ff5\uff0c\u79f0\u4e3a AIO-RAN-NTN\uff0c\u7528\u4e8e\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u7075\u6d3b\u6027\u548c\u667a\u80fd\u6027\u3002", "method": "\u9996\u5148\uff0c\u6982\u8ff0\u4e86 Open-RAN \u548c AI-RAN \u7684\u6700\u65b0\u67b6\u6784\u3002\u7136\u540e\uff0c\u63d0\u51fa AIO-RAN-NTN \u84dd\u56fe\uff0c\u5e76\u5e94\u7528 AIO-RAN \u548c 3GPP \u7684\u63a5\u53e3\u5230 NTN \u73af\u5883\u3002\u4f7f\u7528 OpenAirInterface \u5e73\u53f0\u4e3a\u72ec\u7acb\uff08SA\uff09\u65b0\u65e0\u7ebf\uff08NR\uff095G \u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u4f20\u8f93\uff0c\u5e76\u5bf9 KPI \u8fdb\u884c\u4e86 AI \u6a21\u578b\u8bad\u7ec3\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIO-RAN-NTN \u67b6\u6784\u5bf9\u79fb\u52a8\u6027\u5f88\u654f\u611f\uff0c\u5373\u4f7f\u5728\u4f4e\u901f\u4e0b\u4e5f\u662f\u5982\u6b64\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7 AI \u9a71\u52a8\u7684 KPI \u9884\u6d4b\u6765\u7f13\u89e3\u3002", "conclusion": "AI \u9a71\u52a8\u7684 KPI \u9884\u6d4b\u53ef\u4ee5\u7f13\u89e3 AIO-RAN-NTN \u67b6\u6784\u5bf9\u79fb\u52a8\u6027\u7684\u654f\u611f\u6027\u95ee\u9898\u3002"}}
{"id": "2511.11586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11586", "abs": "https://arxiv.org/abs/2511.11586", "authors": ["Ao Zhou", "Jianlei Yang", "Tong Qiao", "Yingjie Qi", "Xinming Wei", "Cenlin Duan", "Weisheng Zhao", "Chunming Hu"], "title": "ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments", "comment": "This paper is accepted by the Journal of IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "summary": "The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.", "AI": {"tldr": "ACE-GNN\u662f\u4e00\u4e2a\u9996\u4e2a\u4e3a\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u9002\u5e94GNN\u534f\u540c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u62bd\u8c61\u548c\u65b0\u9896\u7684\u9884\u6d4b\u65b9\u6cd5\u5b9e\u73b0\u6027\u80fd\u611f\u77e5\uff0c\u5e76\u5f15\u5165\u6570\u636e\u5e76\u884c\uff08DP\uff09\u673a\u5236\u4ee5\u5b9e\u73b0\u6d41\u6c34\u7ebf\u5e76\u884c\uff08PP\uff09\u548cDP\u4e4b\u95f4\u7684\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u4ece\u800c\u5728\u5e7f\u6cdb\u7684\u5e94\u7528\u548c\u8fb9\u7f18\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bbe\u5907-\u8fb9\u7f18\u534f\u540c\u63a8\u7406\u65b9\u6cd5\uff08\u5982\u79bb\u7ebf\u6a21\u578b\u62c6\u5206\u548c\u6d41\u6c34\u7ebf\u5e76\u884cPP\uff09\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u7f51\u7edc\u6ce2\u52a8\u548c\u591a\u8bbe\u5907\u8bbf\u95ee\uff09\u4e0b\u6027\u80fd\u4f1a\u53d7\u5230\u663e\u8457\u5f71\u54cd\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ACE-GNN\u901a\u8fc7\u7cfb\u7edf\u7ea7\u62bd\u8c61\u548c\u4e24\u79cd\u65b0\u9896\u7684\u9884\u6d4b\u65b9\u6cd5\u5b9e\u73b0\u6027\u80fd\u611f\u77e5\uff0c\u4ece\u800c\u80fd\u591f\u5feb\u901f\u8fdb\u884c\u8fd0\u884c\u65f6\u65b9\u6848\u4f18\u5316\u3002\u6b64\u5916\uff0cACE-GNN\u5728\u8fd0\u884c\u65f6\u4f18\u5316\u4e2d\u5f15\u5165\u4e86\u6570\u636e\u5e76\u884c\uff08DP\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u4e86PP\u548cDP\u4e4b\u95f4\u7684\u81ea\u9002\u5e94\u8c03\u5ea6\u3002\u8be5\u6846\u67b6\u8fd8\u5305\u62ec\u9ad8\u6548\u7684\u6279\u63a8\u7406\u7b56\u7565\u548c\u4e13\u95e8\u7684\u901a\u4fe1\u4e2d\u95f4\u4ef6\u3002", "result": "\u4e0eGCoDE\u76f8\u6bd4\uff0cACE-GNN\u5b9e\u73b0\u4e86\u9ad8\u8fbe12.7\u500d\u7684\u52a0\u901f\u548c82.3%\u7684\u80fd\u8017\u8282\u7701\u3002\u4e0eFograph\u76f8\u6bd4\uff0cACE-GNN\u7684\u80fd\u6548\u63d0\u9ad8\u4e8611.7%\u3002", "conclusion": "ACE-GNN\u901a\u8fc7\u5176\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5728\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86GNN\u534f\u540c\u63a8\u7406\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u9759\u6001\u90e8\u7f72\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12062", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12062", "abs": "https://arxiv.org/abs/2511.12062", "authors": ["Kyunghyun Baek", "Seungjin Lee", "Joonsuk Huh", "Dongkeun Lee", "Jinhyoung Lee", "M. S. Kim", "Jeongho Bang"], "title": "Quantum Amplitude-Amplification Eigensolver: A State-Learning-Assisted Approach beyond Energy-Gradient-Based Heuristics", "comment": "12 pages, 11 figures", "summary": "Ground-state estimation lies at the heart of a broad range of quantum simulations. Most near-term approaches are cast as variational energy minimization and thus inherit the challenges of problem-specific energy landscapes. We develop the quantum amplitude-amplification eigensolver (QAAE), which departs from the variational paradigm and instead coherently drives a trial state toward the ground state via quantum amplitude amplification. Each amplitude-amplification round interleaves a reflection about the learned trial state with a controlled short-time evolution under a normalized Hamiltonian; an ancilla readout yields an amplitude-amplified pure target state that a state-learning step then re-encodes into an ansatz circuit for the next round -- without evaluating the energy gradients. Under standard assumptions (normalized $\\hat{H}$, a nondegenerate ground-state, and a learning update), the ground-state overlap increases monotonically per round and the procedure converges; here, a per-round depth bound in terms of the ansatz depth and Hamiltonian-simulation cost establishes hardware compatibility. Cloud experiments on IBMQ processor verify our amplification mechanism on a two-level Hamiltonian and a two-qubit Ising model, and numerical benchmarks on $\\mathrm{H}_2$, $\\mathrm{LiH}$, and a $10$-qubit longitudinal-and-transverse-field Ising model show that QAAE integrates with chemistry-inspired and hardware-efficient circuits and can surpass gradient-based VQE in accuracy and stability. These results position QAAE as a variational-free and hardware-compatible route to ground-state estimation for near-term quantum simulation.", "AI": {"tldr": "QAAE\u662f\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u8fd1\u671f\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u57fa\u6001\u4f30\u8ba1\uff0c\u5b83\u4e0d\u4f9d\u8d56\u4e8e\u53d8\u5206\u80fd\u91cf\u6700\u5c0f\u5316\uff0c\u800c\u662f\u901a\u8fc7\u91cf\u5b50\u5e45\u5ea6\u653e\u5927\u76f8\u5e72\u5730\u9a71\u52a8\u8bd5\u63a2\u6001\u8d8b\u5411\u4e8e\u57fa\u6001\u3002", "motivation": "\u8fd1\u671f\u91cf\u5b50\u6a21\u62df\u4e2d\u7684\u57fa\u6001\u4f30\u8ba1\u901a\u5e38\u91c7\u7528\u53d8\u5206\u80fd\u91cf\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u53d7\u5230\u7279\u5b9a\u80fd\u91cf\u666f\u89c2\u7684\u6311\u6218\u3002", "method": "QAAE\u7b97\u6cd5\u7684\u6bcf\u4e2a\u5e45\u5ea6\u653e\u5927\u8f6e\u6b21\u5305\u62ec\u5173\u4e8e\u5b66\u4e60\u5230\u7684\u8bd5\u63a2\u6001\u7684\u53cd\u5c04\u64cd\u4f5c\u548c\u5728\u5f52\u4e00\u5316\u54c8\u5bc6\u987f\u91cf\u4e0b\u8fdb\u884c\u77ed\u65f6\u6f14\u5316\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u6bd4\u7279\u8bfb\u51fa\u589e\u5f3a\u7684\u7eaf\u76ee\u6807\u6001\uff0c\u7136\u540e\u901a\u8fc7\u72b6\u6001\u5b66\u4e60\u6b65\u9aa4\u91cd\u65b0\u7f16\u7801\u5230\u6a21\u578b\u7535\u8def\u4e2d\uff0c\u7528\u4e8e\u4e0b\u4e00\u8f6e\u8fed\u4ee3\uff0c\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u8ba1\u7b97\u80fd\u91cf\u68af\u5ea6\u3002", "result": "QAAE\u7b97\u6cd5\u5728IBM\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5728H2\u3001LiH\u548c10\u91cf\u5b50\u6bd4\u7279\u7684\u7eb5\u6a2a\u573a\u4f0a\u8f9b\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660eQAAE\u53ef\u4ee5\u96c6\u6210\u5316\u5b66\u542f\u53d1\u548c\u786c\u4ef6\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u4e14\u5728\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u3002", "conclusion": "QAAE\u7b97\u6cd5\u662f\u4e00\u79cd\u4e0d\u4f9d\u8d56\u53d8\u5206\u65b9\u6cd5\u4e14\u517c\u5bb9\u786c\u4ef6\u7684\u57fa\u6001\u4f30\u8ba1\u65b0\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u8fd1\u671f\u7684\u91cf\u5b50\u6a21\u62df\u3002"}}
{"id": "2511.13301", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.13301", "abs": "https://arxiv.org/abs/2511.13301", "authors": ["Lisa Lehner", "Christian Komusiewicz", "Luca Pascal Staus"], "title": "A Complexity Analysis of the c-Closed Vertex Deletion Problem", "comment": null, "summary": "A graph is $c$-closed when every pair of nonadjacent vertices has at most $c-1$ common neighbors. In $c$-Closed Vertex Deletion, the input is a graph $G$ and an integer $k$ and we ask whether $G$ can be transformed into a $c$-closed graph by deleting at most $k$ vertices. We study the classic and parameterized complexity of $c$-Closed Vertex Deletion. We obtain, for example, NP-hardness for the case that $G$ is bipartite with bounded maximum degree. We also show upper and lower bounds on the size of problem kernels for the parameter $k$ and introduce a new parameter, the number $x$ of vertices in bad pairs, for which we show a problem kernel of size $\\mathcal{O}(x^3 + x^2\\cdot c))$. Here, a pair of nonadjacent vertices is bad if they have at least $c$ common neighbors. Finally, we show that $c$-Closed Vertex Deletion can be solved in polynomial time on unit interval graphs with depth at most $c+1$ and that it is fixed-parameter tractable with respect to the neighborhood diversity of $G$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86c-\u95ed\u5408\u56fe\u5220\u9664\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5176NP-hard\u6027\u548c\u53c2\u6570\u5316\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76c-\u95ed\u5408\u56fe\u5220\u9664\u95ee\u9898\u7684NP-hard\u6027\u548c\u53c2\u6570\u5316\u590d\u6742\u6027\uff0c\u4e3a\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u53c2\u6570x\uff08\u574f\u5bf9\u7684\u6570\u91cf\uff09\u5e76\u5206\u6790\u5176\u95ee\u9898\u6838\u5927\u5c0f\uff0c\u4ee5\u53ca\u7814\u7a76\u5728\u5355\u4f4d\u533a\u95f4\u56fe\u4e0a\u7684\u53ef\u89e3\u6027\u548c\u9488\u5bf9\u90bb\u57df\u591a\u6837\u6027\u7684\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u6027\u3002", "result": "\u8bc1\u660e\u4e86c-\u95ed\u5408\u56fe\u5220\u9664\u95ee\u9898\u5728\u67d0\u4e9b\u7279\u5b9a\u56fe\uff08\u5982\u6700\u5927\u5ea6\u6709\u754c\u7684\u4e8c\u5206\u56fe\uff09\u4e0a\u662fNP-hard\u7684\uff0c\u5e76\u5f97\u5230\u4e86\u95ee\u9898\u6838\u5927\u5c0f\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9488\u5bf9\u65b0\u53c2\u6570x\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5355\u4f4d\u533a\u95f4\u56fe\u4e0a\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u6cd5\u3002", "conclusion": "c-\u95ed\u5408\u56fe\u5220\u9664\u95ee\u9898\u5728\u4e0d\u540c\u56fe\u7c7b\u548c\u53c2\u6570\u4e0b\u5177\u6709\u4e0d\u540c\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.13369", "categories": ["cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2511.13369", "abs": "https://arxiv.org/abs/2511.13369", "authors": ["Lilou Soulas", "Lorenzo Lucchini", "Maurizio Napolitano", "Sebastiano Bontorin", "Simone Centellegher", "Bruno Lepri", "Riccardo Gallotti", "Eleonora Andreotti"], "title": "Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system", "comment": null, "summary": "The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u7684\u57fa\u51c6\u548c\u6620\u5c04\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u5f00\u653e\u8857\u56fe(OSM)\u6807\u7b7e\u548cFoursquare(FS)\u5206\u7c7b\uff0c\u4ee5\u89e3\u51b3POI\uff08\u5174\u8da3\u70b9\uff09\u5206\u7c7b\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "POI\u5206\u7c7b\u5f02\u6784\u6027\u7ed9\u57ce\u5e02\u6570\u636e\u96c6\u6210\u548c\u57fa\u4e8e\u4f4d\u7f6e\u7684\u670d\u52a1\u5e26\u6765\u6311\u6218\uff0cOSM\u7684\u7075\u6d3b\u6807\u7b7e\u7cfb\u7edf\u548cFS\u7684\u5c42\u7ea7\u7ed3\u6784\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u624b\u52a8\u7b56\u5212\u7684\u57fa\u51c6\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\uff0c\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fdb\u884cOSM\u6807\u7b7e\u548cFS\u7c7b\u522b\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2aLLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86OSM\u6570\u636e\u4e30\u5bcc\u6027\u548cFS\u5c42\u7ea7\u7ed3\u6784\u7684\u8d44\u6e90\uff0c\u652f\u6301\u53ef\u590d\u73b0\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u57ce\u5e02\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u4e86\u5d4c\u5165\u548cLLM\u5bf9\u9f50\u7b56\u7565\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u66f4\u65b0\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u7c7b\u7edf\u4e00\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u57ce\u5e02\u5206\u6790\u3001\u79fb\u52a8\u6027\u7814\u7a76\u548c\u667a\u6167\u57ce\u5e02\u670d\u52a1\u3002"}}
{"id": "2511.11816", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11816", "abs": "https://arxiv.org/abs/2511.11816", "authors": ["Andrea Brunello", "Luca Geatti", "Michele Mignani", "Angelo Montanari", "Nicola Saccomanno"], "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy", "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5230\u4e00\u9636\u903b\u8f91\uff08NL-FOL\uff09\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0c\u80fd\u591f\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\uff0c\u800c\u4ee5\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5230\u4e00\u9636\u903b\u8f91\uff08NL-FOL\uff09\u7ffb\u8bd1\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\u5e26\u6765\u4e86\u5e0c\u671b\uff0c\u4f46\u73b0\u6709\u6587\u732e\u5bf9\u5176\u80fd\u529b\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u65e8\u5728\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\u3001\u8bb0\u5fc6\u53ca\u6570\u636e\u96c6\u6c61\u67d3\uff0c\u5e76\u4f7f\u7528\u8be5\u534f\u8bae\u8bc4\u4f30\u4e86\u4e24\u79cd\u7c7b\u578b\u7684LLMs\uff08\u5bf9\u8bdd\u578b\u548c\u4ee5\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u578b\uff09\u3002", "result": "\u65b0\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u8bdd\u578bLLMs\u5728NL-FOL\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u548c\u5bf9\u53e5\u5b50\u7ea7\u903b\u8f91\u7684\u771f\u6b63\u7406\u89e3\uff0c\u800c\u4ee5\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u8868\u73b0\u660e\u663e\u8f83\u5dee\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u6311\u6218\uff0c\u4f46\u7ecf\u8fc7\u6539\u8fdb\u7684\u8bc4\u4f30\u65b9\u6cd5\u8868\u660e\uff0c\u5148\u8fdb\u7684\u5bf9\u8bdd\u578bLLMs\u5728NL-FOL\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5177\u6709\u771f\u6b63\u7684\u903b\u8f91\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2511.12960", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12960", "abs": "https://arxiv.org/abs/2511.12960", "authors": ["Daivik Patel", "Shrenik Patel"], "title": "ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents", "comment": null, "summary": "Large language models (LLMs) deployed in user-facing applications require long-horizon consistency: the ability to remember prior interactions, respect user preferences, and ground reasoning in past events. However, contemporary memory systems often adopt complex architectures such as knowledge graphs, multi-stage retrieval pipelines, and OS-style schedulers, which introduce engineering complexity and reproducibility challenges. We present ENGRAM, a lightweight memory system that organizes conversation into three canonical memory types (episodic, semantic, and procedural) through a single router and retriever. Each user turn is converted into typed memory records with normalized schemas and embeddings and stored in a database. At query time, the system retrieves top-k dense neighbors for each type, merges results with simple set operations, and provides the most relevant evidence as context to the model. ENGRAM attains state-of-the-art results on LoCoMo, a multi-session conversational QA benchmark for long-horizon memory, and exceeds the full-context baseline by 15 points on LongMemEval while using only about 1% of the tokens. These results show that careful memory typing and straightforward dense retrieval can enable effective long-term memory management in language models without requiring complex architectures.", "AI": {"tldr": "ENGRAM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u4e00\u7684\u8def\u7531\u5668\u548c\u68c0\u7d22\u5668\u5c06\u5bf9\u8bdd\u7ec4\u7ec7\u6210\u4e09\u79cd\u89c4\u8303\u7684\u8bb0\u5fc6\u7c7b\u578b\uff08\u60c5\u666f\u3001\u8bed\u4e49\u548c\u7a0b\u5e8f\uff09\uff0c\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u4ee3\u5e01\u4f7f\u7528\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u590d\u6742\u7684\u67b6\u6784\uff0c\u5982\u77e5\u8bc6\u56fe\u8c31\u3001\u591a\u9636\u6bb5\u68c0\u7d22\u7ba1\u9053\u548c\u64cd\u4f5c\u7cfb\u7edf\u98ce\u683c\u7684\u8c03\u5ea6\u5668\uff0c\u8fd9\u4f1a\u5e26\u6765\u5de5\u7a0b\u590d\u6742\u6027\u548c\u53ef\u91cd\u590d\u6027\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u7b80\u5355\u3001\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\u6765\u652f\u6301LLM\u7684\u957f\u8fdc\u4e00\u81f4\u6027\u3002", "method": "ENGRAM\u5c06\u6bcf\u6b21\u7528\u6237\u4ea4\u4e92\u8f6c\u6362\u4e3a\u5177\u6709\u89c4\u8303\u6a21\u5f0f\u548c\u5d4c\u5165\u7684\u7c7b\u578b\u5316\u8bb0\u5fc6\u8bb0\u5f55\uff0c\u5e76\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u3002\u5728\u67e5\u8be2\u65f6\uff0c\u5b83\u68c0\u7d22\u6bcf\u79cd\u7c7b\u578b\u7684top-k\u5bc6\u96c6\u90bb\u5c45\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u96c6\u5408\u64cd\u4f5c\u5408\u5e76\u7ed3\u679c\uff0c\u5e76\u5c06\u6700\u76f8\u5173\u7684\u8bc1\u636e\u63d0\u4f9b\u7ed9\u6a21\u578b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u3002", "result": "ENGRAM\u5728LoCoMo\uff08\u4e00\u4e2a\u7528\u4e8e\u957f\u8fdc\u8bb0\u5fc6\u7684\u591a\u4f1a\u8bdd\u5bf9\u8bddQA\u57fa\u51c6\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e14\u5728LongMemEval\u4e0a\u6bd4\u5168\u4e0a\u4e0b\u6587\u57fa\u7ebf\u9ad8\u51fa15\u4e2a\u70b9\uff0c\u540c\u65f6\u53ea\u4f7f\u7528\u4e86\u5927\u7ea61%\u7684\u4ee3\u5e01\u3002", "conclusion": "\u4ed4\u7ec6\u7684\u8bb0\u5fc6\u7c7b\u578b\u5212\u5206\u548c\u76f4\u63a5\u7684\u5bc6\u96c6\u68c0\u7d22\u53ef\u4ee5\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u6548\u7684\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u67b6\u6784\u3002"}}
{"id": "2511.12231", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.12231", "abs": "https://arxiv.org/abs/2511.12231", "authors": ["Moru Song", "Kai Chang"], "title": "Fractional Chern Insulators Transition in Non-ideal Flat Bands of Twisted Mono-bilayer Graphene", "comment": "8 pages, 6figures. For any sentific questions or comments on this work, please contact mrsong@semi.ac.cn", "summary": "Fractional Chern insulators (FCIs) in ideal $|C|>1$ flat bands can be viewed as color-entangled composites of $C$ lowest Landau levels, but in realistic moir\u00e9 systems non-ideal quantum geometry complicates this picture, leaving their stabilization mechanism incompletely understood. Using twisted monolayer-bilayer graphene (tMBG) as a platform, we observe two FCIs joined by a continuous transition controlled by continuum model parameter $\u03ba$, arising from a geometric instability of the Bloch wave functions. For $\u03ba$ below the transition, the target $C=2$ conduction band is geometrically stable and effectively decomposes into two independent $C=1$ color sectors. Although the flat band is non-ideal, the resulting fractional phase is naturally accounted for by the non-chiral Halperin-(112) state with counterpropagating edge modes. Above the transition, the system enters a Laughlin-$1/3$ phase that persists despite further degradation of quantum-geometry indicators. To account for this robustness, we propose a color-separation mechanism beyond global geometric indicators: when the Bloch wave function is geometrically unstable, interactions dynamically split a non-ideal flat band into an ideal subcomponent that hosts the FCI and non-ideal remnants. We corroborate this picture by applying a weak perpendicular magnetic field that acts as a \"color separator,\" explicitly visualizing the ideal subcomponent at the single-particle level. Together, these results establish two different routes by which non-ideal flat bands stabilize FCIs, expanding their viable parameter space and clarifying the interplay between geometry and topological order.", "AI": {"tldr": "\u626d\u66f2\u5355\u5c42-\u53cc\u5c42\u77f3\u58a8\u70ef\u4e2d\u7684\u5206\u6570\u9648\u7edd\u7f18\u4f53(FCI)\u53ef\u4ee5\u901a\u8fc7\u8fde\u7eed\u76f8\u53d8\u7a33\u5b9a\uff0c\u8be5\u76f8\u53d8\u7531\u8fde\u7eed\u6a21\u578b\u53c2\u6570\u03ba\u63a7\u5236\uff0c\u5e76\u7531Bloch\u6ce2\u51fd\u6570\u7684\u51e0\u4f55\u4e0d\u7a33\u5b9a\u6027\u5f15\u8d77\u3002", "motivation": "\u5728\u73b0\u5b9e\u7684\u6469\u5c14\u4f53\u7cfb\u4e2d\uff0c\u975e\u7406\u60f3\u7684\u91cf\u5b50\u51e0\u4f55\u4f1a\u4f7f\u5206\u6570\u9648\u7edd\u7f18\u4f53(FCI)\u7684\u7a33\u5b9a\u673a\u5236\u590d\u6742\u5316\uff0c\u5bfc\u81f4\u5176\u7a33\u5b9a\u673a\u5236\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002", "method": "\u4f7f\u7528\u626d\u66f2\u5355\u5c42-\u53cc\u5c42\u77f3\u58a8\u70ef(tMBG)\u4f5c\u4e3a\u5e73\u53f0\uff0c\u901a\u8fc7\u6539\u53d8\u8fde\u7eed\u6a21\u578b\u53c2\u6570\u03ba\u6765\u7814\u7a76FCI\u7684\u5f62\u6210\u548c\u7a33\u5b9a\u673a\u5236\uff0c\u5e76\u5f15\u5165\u5f31\u5782\u76f4\u78c1\u573a\u4f5c\u4e3a\u201c\u989c\u8272\u5206\u79bb\u5668\u201d\u6765\u53ef\u89c6\u5316\u7406\u60f3\u5b50\u7ec4\u4ef6\u3002", "result": "\u89c2\u5bdf\u5230\u4e24\u4e2aFCI\uff0c\u5b83\u4eec\u7531\u4e00\u4e2a\u8fde\u7eed\u76f8\u53d8\u8fde\u63a5\uff0c\u8be5\u76f8\u53d8\u7531\u8fde\u7eed\u6a21\u578b\u53c2\u6570\u03ba\u63a7\u5236\u3002\u5728\u76f8\u53d8\u4ee5\u4e0b\uff0c\u76ee\u6807C=2\u4f20\u5bfc\u5e26\u5728\u51e0\u4f55\u4e0a\u662f\u7a33\u5b9a\u7684\uff0c\u53ef\u4ee5\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684C=1\u989c\u8272\u6247\u533a\uff0c\u5e76\u7531\u975e\u624b\u6027Halperin-(112)\u6001\u89e3\u91ca\u3002\u76f8\u53d8\u4ee5\u4e0a\uff0c\u7cfb\u7edf\u8fdb\u5165Laughlin-1/3\u76f8\u3002\u8be5\u56fe\u50cf\u901a\u8fc7\u5f15\u5165\u989c\u8272\u5206\u79bb\u673a\u5236\u5f97\u5230\u8bc1\u5b9e\uff0c\u5176\u4e2d\u76f8\u4e92\u4f5c\u7528\u5c06\u975e\u7406\u60f3\u7684\u5e73\u5766\u80fd\u5e26\u52a8\u6001\u5730\u5206\u88c2\u6210\u4e00\u4e2a\u7406\u60f3\u7684\u5b50\u7ec4\u4ef6\u548c\u4e00\u4e2a\u6216\u591a\u4e2a\u975e\u7406\u60f3\u7684\u6b8b\u4f59\u7ec4\u4ef6\u3002", "conclusion": "\u5df2\u786e\u5b9a\u4e24\u79cd\u7531\u975e\u7406\u60f3\u5e73\u5766\u80fd\u5e26\u7a33\u5b9aFCI\u7684\u9014\u5f84\uff0c\u6269\u5c55\u4e86\u5176\u53ef\u884c\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5e76\u9610\u660e\u4e86\u51e0\u4f55\u4e0e\u62d3\u6251\u9636\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2511.12128", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12128", "abs": "https://arxiv.org/abs/2511.12128", "authors": ["C. A. S. Vieira", "B. J. Santos", "J. G. Duque", "E. M. Bittar", "L. Bufai\u00e7al"], "title": "Ba-substitution induced evolution of structural and magnetic properties of La2-xBaxCoIrO6 double perovskites", "comment": null, "summary": "The Iridium-based oxides are the subject of great recent interest due to the non-conventional physics that may emerge from the strong spin-orbit coupling present in 5d ions. Here, we explore the coupling between Ir and Co in the La2-xBaxCoIrO6 perovskites (x = 0, 0.5, 0.75 and 1.0), where the structural, electronic, and magnetic properties of the series are investigated by means of x-ray powder diffraction and magnetometry. The system's crystal structure evolves from the monoclinic P2_1/n to the triclinic I-1 space group as the Ba concentration increases. Measurements of magnetization revealed ferrimagnetic behavior in x = 0, 0.5 and 0.75 compounds, possibly resulting from antiferromagnetic coupling between Co2+/3+ and Ir4+. In contrast, for x = 1.0 a clear collinear antiferromagnetic character is observed for the Co2+ ions, resulting from the quenching of the Ir5+ magnetic moment. The evolution of the magnetic properties of the series is discussed in terms of the structural and electronic changes, as well as the spin-orbit coupling in Ir.", "AI": {"tldr": "La2-xBaxCoIrO6\u7684\u78c1\u6027\u884c\u4e3a\u968f\u7740Ba\u6d53\u5ea6\u7684\u589e\u52a0\u800c\u53d8\u5316\uff0c\u4ece\u4e9a\u94c1\u78c1\u6027\u6f14\u53d8\u4e3a\u53cd\u94c1\u78c1\u6027\uff0c\u8fd9\u4e0e\u7ed3\u6784\u548c\u7535\u5b50\u6027\u8d28\u7684\u53d8\u5316\u4ee5\u53caIr\u7684\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u6709\u5173\u3002", "motivation": "\u7814\u7a76La2-xBaxCoIrO6\u4e2dIr\u4e0eCo\u7684\u8026\u5408\uff0c\u4ee5\u4e86\u89e3\u5f3a\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u5bf95d\u79bb\u5b50\u975e\u4f20\u7edf\u7269\u7406\u5b66\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7X\u5c04\u7ebf\u7c89\u672b\u884d\u5c04\u548c\u78c1\u6d4b\u91cf\u7814\u7a76\u4e86La2-xBaxCoIrO6\uff08x = 0, 0.5, 0.75\u548c1.0\uff09\u7684\u7ed3\u6784\u3001\u7535\u5b50\u548c\u78c1\u6027\u3002", "result": "\u968f\u7740Ba\u6d53\u5ea6\u7684\u589e\u52a0\uff0c\u6676\u4f53\u7ed3\u6784\u4ece\u5355\u659cP2_1/n\u6f14\u53d8\u4e3a\u4e09\u659cI-1\u7a7a\u95f4\u7fa4\u3002x = 0, 0.5\u548c0.75\u7684\u5316\u5408\u7269\u8868\u73b0\u51fa\u4e9a\u94c1\u78c1\u6027\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8eCo2+/3+\u548cIr4+\u4e4b\u95f4\u7684\u53cd\u94c1\u78c1\u8026\u5408\u3002\u7136\u800c\uff0c\u5728x = 1.0\u65f6\uff0c\u89c2\u5bdf\u5230Co2+\u79bb\u5b50\u7684\u53cd\u94c1\u78c1\u7279\u6027\uff0c\u8fd9\u662f\u7531\u4e8eIr5+\u78c1\u77e9\u7684\u731d\u706d\u3002", "conclusion": "La2-xBaxCoIrO6\u7684\u78c1\u6027\u8d28\u7684\u6f14\u53d8\u53ef\u4ee5\u7528\u7ed3\u6784\u548c\u7535\u5b50\u6027\u8d28\u7684\u53d8\u5316\u4ee5\u53caIr\u7684\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u6765\u89e3\u91ca\u3002"}}
{"id": "2511.11702", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11702", "abs": "https://arxiv.org/abs/2511.11702", "authors": ["Lian He", "Meng Liu", "Qilang Ye", "Yu Zhou", "Xiang Deng", "Gangyi Ding"], "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement", "comment": null, "summary": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.", "AI": {"tldr": "TASA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u51e0\u4f55\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u54082D\u8bed\u4e49\u7ebf\u7d22\u548c3D\u51e0\u4f55\u63a8\u7406\uff0c\u5b9e\u73b0\u4e863D\u573a\u666f\u7ea7\u53ef\u8fbe\u6027\u5206\u5272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u74063D\u573a\u666f\u7ea7\u53ef\u8fbe\u6027\u5206\u5272\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u4f8b\u5982\u4ec5\u5173\u6ce8\u7269\u4f53\u7ea7\u53ef\u8fbe\u6027\u3001\u5c062D\u9884\u6d4b\u63d0\u5347\u52303D\uff0c\u4ee5\u53ca\u5ffd\u7565\u70b9\u4e91\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "TASA\u6846\u67b6\u91c7\u7528\u7c97\u5230\u7cbe\u7684\u65b9\u5f0f\uff0c\u7ed3\u54082D\u8bed\u4e49\u7ebf\u7d22\u548c3D\u51e0\u4f55\u63a8\u7406\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u76842D\u53ef\u8fbe\u6027\u68c0\u6d4b\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u8bed\u8a00\u548c\u89c6\u89c9\u8f93\u5165\u4e2d\u8bc6\u522b\u53ef\u64cd\u4f5c\u70b9\uff0c\u5e76\u6307\u5bfc\u4efb\u52a1\u76f8\u5173\u89c6\u56fe\u7684\u9009\u62e9\uff1b\u4ee5\u53ca\u4e00\u4e2a3D\u53ef\u8fbe\u6027\u7ec6\u5316\u6a21\u5757\uff0c\u7528\u4e8e\u6574\u54082D\u8bed\u4e49\u5148\u9a8c\u548c\u5c40\u90e83D\u51e0\u4f55\u4fe1\u606f\uff0c\u751f\u6210\u51c6\u786e\u4e14\u7a7a\u95f4\u4e00\u81f4\u76843D\u53ef\u8fbe\u6027\u63a9\u7801\u3002", "result": "TASA\u5728SceneFun3D\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u573a\u666f\u7ea7\u53ef\u8fbe\u6027\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TASA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b33D\u573a\u666f\u7ea7\u53ef\u8fbe\u6027\u5206\u5272\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u54082D\u8bed\u4e49\u4fe1\u606f\u548c3D\u51e0\u4f55\u63a8\u7406\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002"}}
{"id": "2511.12652", "categories": ["cs.NE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12652", "abs": "https://arxiv.org/abs/2511.12652", "authors": ["Claude Carlet", "Marko \u00d0urasevic", "Domagoj Jakobovic", "Luca Mariot", "Stjepan Picek", "Alexandr Polujan"], "title": "On Counts and Densities of Homogeneous Bent Functions: An Evolutionary Approach", "comment": "18 pages, 3 figures", "summary": "Boolean functions with strong cryptographic properties, such as high nonlinearity and algebraic degree, are important for the security of stream and block ciphers. These functions can be designed using algebraic constructions or metaheuristics. This paper examines the use of Evolutionary Algorithms (EAs) to evolve homogeneous bent Boolean functions, that is, functions whose algebraic normal form contains only monomials of the same degree and that are maximally nonlinear. We introduce the notion of density of homogeneous bent functions, facilitating the algorithmic design that results in finding quadratic and cubic bent functions in different numbers of variables.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u8bbe\u8ba1\u5177\u6709\u5f3a\u5bc6\u7801\u5b66\u6027\u8d28\uff08\u9ad8\u975e\u7ebf\u6027\u5ea6\u548c\u9ad8\u4ee3\u6570\u6b21\u6570\uff09\u7684\u9f50\u6b21\uff08\u4ee3\u6570\u8303\u5f0f\u4e2d\u4ec5\u5305\u542b\u76f8\u540c\u6b21\u6570\u5355\u9879\u5f0f\uff09\u4e14\u6700\u5927\u975e\u7ebf\u6027\u7684\u5e03\u5c14\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u9f50\u6b21\u8010\u529b\u51fd\u6570\u5bc6\u5ea6\uff08density of homogeneous bent functions\uff09\u7684\u6982\u5ff5\uff0c\u4ee5\u5728\u4e0d\u540c\u6570\u91cf\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u5bfb\u627e\u4e8c\u6b21\u548c\u4e09\u6b21\u8010\u529b\u51fd\u6570\u3002", "motivation": "\u8bbe\u8ba1\u5177\u6709\u5f3a\u5bc6\u7801\u5b66\u6027\u8d28\uff08\u9ad8\u975e\u7ebf\u6027\u5ea6\u548c\u9ad8\u4ee3\u6570\u6b21\u6570\uff09\u7684\u5e03\u5c14\u51fd\u6570\uff0c\u4ee5\u589e\u5f3a\u6d41\u5bc6\u7801\u548c\u5206\u7ec4\u5bc6\u7801\u7684\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u6765\u6f14\u5316\u9f50\u6b21\u8010\u529b\u5e03\u5c14\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u4e86\u9f50\u6b21\u8010\u529b\u51fd\u6570\u5bc6\u5ea6\u7684\u6982\u5ff5\u3002", "result": "\u6210\u529f\u5bfb\u627e\u5230\u4e86\u4e0d\u540c\u6570\u91cf\u53d8\u91cf\u4e0b\u7684\u4e8c\u6b21\u548c\u4e09\u6b21\u9f50\u6b21\u8010\u529b\u5e03\u5c14\u51fd\u6570\u3002", "conclusion": "\u9f50\u6b21\u8010\u529b\u5e03\u5c14\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u548c\u5bc6\u5ea6\u6982\u5ff5\u8fdb\u884c\u8bbe\u8ba1\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u53d8\u91cf\u6570\u4e0b\u53ef\u4ee5\u627e\u5230\u4e8c\u6b21\u548c\u4e09\u6b21\u8010\u529b\u51fd\u6570\u3002"}}
{"id": "2511.13489", "categories": ["cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13489", "abs": "https://arxiv.org/abs/2511.13489", "authors": ["Gautam Nagarajan", "Omir Kumar", "Sudarsun Santhiappan"], "title": "PolicyBot - Reliable Question Answering over Policy Documents", "comment": null, "summary": "All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.", "AI": {"tldr": "PolicyBot\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u56de\u7b54\u5173\u4e8e\u653f\u7b56\u6587\u4ef6\u7684\u95ee\u9898\uff0c\u5177\u6709\u900f\u660e\u5ea6\u548c\u53ef\u590d\u7528\u6027\u3002\u5b83\u7ed3\u5408\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8bed\u4e49\u5206\u5757\u3001\u591a\u8bed\u8a00\u5bc6\u96c6\u5d4c\u5165\u3001\u591a\u9636\u6bb5\u68c0\u7d22\u4e0e\u91cd\u6392\u4ee5\u53ca\u6e90\u611f\u77e5\u751f\u6210\uff0c\u4ee5\u63d0\u4f9b\u57fa\u4e8e\u539f\u59cb\u6587\u4ef6\u7684\u7b54\u6848\u3002\u901a\u8fc7\u5b9e\u73b0\u5f15\u7528\u8ffd\u8e2a\u6765\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u6cd5\u5f8b\u548c\u653f\u7b56\u6587\u4ef6\u901a\u5e38\u5197\u957f\u3001\u590d\u6742\u4e14\u96be\u4ee5\u7406\u89e3\uff0c\u7ed9\u516c\u6c11\u67e5\u627e\u548c\u7406\u89e3\u76f8\u5173\u4fe1\u606f\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8bed\u4e49\u5206\u5757\u3001\u591a\u8bed\u8a00\u5bc6\u96c6\u5d4c\u5165\u3001\u591a\u9636\u6bb5\u68c0\u7d22\u4e0e\u91cd\u6392\u4ee5\u53ca\u6e90\u611f\u77e5\u751f\u6210\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f15\u7528\u8ffd\u8e2a\u6765\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "result": "PolicyBot\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u57fa\u4e8e\u539f\u59cb\u653f\u7b56\u6587\u4ef6\u7684\u3001\u53ef\u8ffd\u6eaf\u7684\u7b54\u6848\uff0c\u51cf\u5c11\u4e86\u4fe1\u606f\u68c0\u7d22\u7684\u96be\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "conclusion": "PolicyBot\u662f\u4e00\u4e2a\u5728\u6cbb\u7406\u76f8\u5173\u80cc\u666f\u4e0b\u90e8\u7f72\u53ef\u4fe1\u8d56RAG\u7cfb\u7edf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u8003\u91cf\u3001\u5b9e\u9645\u6311\u6218\u548c\u7ecf\u9a8c\u6559\u8bad\u3002"}}
{"id": "2511.12863", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12863", "abs": "https://arxiv.org/abs/2511.12863", "authors": ["Xi Zheng", "Yinghui Huang", "Xiangyu Chang", "Ruoxi Jia", "Yong Tan"], "title": "Rethinking Data Value: Asymmetric Data Shapley for Structure-Aware Valuation in Data Markets and Machine Learning Pipelines", "comment": null, "summary": "Rigorous valuation of individual data sources is critical for fair compensation in data markets, informed data acquisition, and transparent development of ML/AI models. Classical Data Shapley (DS) provides a essential axiomatic framework for data valuation but is constrained by its symmetry axiom that assumes interchangeability of data sources. This assumption fails to capture the directional and temporal dependencies prevalent in modern ML/AI workflows, including the reliance of duplicated or augmented data on original sources and the order-specific contributions in sequential pipelines such as federated learning and multi-stage LLM fine tuning. To address these limitations, we introduce Asymmetric Data Shapley (ADS), a structure-aware data valuation framework for modern ML/AI pipelines. ADS relaxes symmetry by averaging marginal contributions only over permutations consistent with an application-specific ordering of data groups. It preserves efficiency and linearity, maintains within group symmetry and directional precedence across groups, and reduces to DS when the ordering collapses to a single group. We develop two complementary computational procedures for ADS: (i) a Monte Carlo estimator (MC-ADS) with finite-sample accuracy guarantees, and (ii) a k-nearest neighbor surrogate (KNN-ADS) that is exact and efficient for KNN predictors. Across representative settings with directional and temporal dependence, ADS consistently outperforms benchmark methods by distinguishing novel from redundant contributions and respecting the sequential nature of training. These results establish ADS as a principled and practical approach to equitable data valuation in data markets and complex ML/AI pipelines.", "AI": {"tldr": "ADS\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u4f30\u503c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edfDS\u5bf9\u79f0\u6027\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u8003\u8651\u4e86\u73b0\u4ee3ML/AI\u6d41\u7a0b\u4e2d\u7684\u65b9\u5411\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u636eShapley\uff08DS\uff09\u5728\u6570\u636e\u4f30\u503c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5bf9\u79f0\u6027\u5047\u8bbe\u65e0\u6cd5\u6355\u6349\u73b0\u4ee3ML/AI\u5de5\u4f5c\u6d41\u4e2d\u7684\u65b9\u5411\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e0d\u5bf9\u79f0\u6570\u636eShapley\uff08ADS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u5728\u4e0e\u5e94\u7528\u7279\u5b9a\u6570\u636e\u7ec4\u6392\u5e8f\u4e00\u81f4\u7684\u6392\u5217\u4e0a\u5e73\u5747\u8fb9\u9645\u8d21\u732e\u6765\u653e\u677e\u5bf9\u79f0\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3001\u7ebf\u6027\u3001\u7ec4\u5185\u5bf9\u79f0\u6027\u548c\u8de8\u7ec4\u7684\u65b9\u5411\u4f18\u5148\u6027\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u8ba1\u7b97\u65b9\u6cd5\uff1a\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff08MC-ADS\uff09\u548ck\u8fd1\u90bb\u4ee3\u7406\uff08KNN-ADS\uff09\u3002", "result": "ADS\u5728\u5177\u6709\u65b9\u5411\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u4ee3\u8868\u6027\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u533a\u5206\u65b0\u9896\u8d21\u732e\u4e0e\u5197\u4f59\u8d21\u732e\u5e76\u5c0a\u91cd\u8bad\u7ec3\u7684\u987a\u5e8f\u6027\uff0c\u4e00\u81f4\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "ADS\u662f\u4e00\u4e2a\u539f\u5219\u6027\u5f3a\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u6570\u636e\u5e02\u573a\u548c\u590d\u6742ML/AI\u6d41\u7a0b\u4e2d\u7684\u516c\u5e73\u6570\u636e\u4f30\u503c\u3002"}}
{"id": "2511.11868", "categories": ["physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.11868", "abs": "https://arxiv.org/abs/2511.11868", "authors": ["Dhiraj K. Singh"], "title": "Orthogonal Photoelastic Imaging for Three-Dimensional Stress Estimation in a Transparent Cubical Block", "comment": "31 pages 15 Figures", "summary": "Conventional photoelastic methods are largely limited to two-dimensional stress visualization, leaving a gap in techniques that can capture three-dimensional force interactions with high sensitivity at low stress levels, a capability that is critical for biomechanics and dynamic force analysis. This study develops and demonstrates a cubic photoelastic model that enables accurate fringe-order estimation from three orthogonal views, providing a foundation for reconstructing full three-dimensional stress states. A transparent, low-elasticity epoxy cube, free of prestress, was fabricated and examined using combined transmission and reflection photoelastic imaging. Three mutually orthogonal isochromatic fringe fields were recorded simultaneously under a single applied load. Image analysis employed a peak-valley intensity method to extract sub-fringe orders and to resolve low-stress cases with minimal noise. The cubic block produced high-quality fringe patterns in all directions, enabling separation of tangential and normal stress components. Independent orthogonal views confirmed directional sensitivity and yielded consistent fringe-order estimates under low loading, with response times on the order of tens of microseconds. These results establish a practical approach for three-dimensional photoelastic stress measurement from orthogonal views and create a pathway toward full vector force reconstruction with strong potential for biomedical applications and studies of dynamic loading.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7acb\u65b9\u5149\u5f39\u6027\u6a21\u578b\u7684\u7acb\u65b9\u4f53\u5149\u5f39\u6027\u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e09\u4e2a\u6b63\u4ea4\u89c6\u56fe\u8fdb\u884c\u7cbe\u786e\u7684\u6761\u7eb9\u7ea7\u6b21\u4f30\u8ba1\uff0c\u4ece\u800c\u5b9e\u73b0\u4e09\u7ef4\u5e94\u529b\u72b6\u6001\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u5f39\u6027\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u4e8c\u7ef4\u5e94\u529b\u53ef\u89c6\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u751f\u7269\u529b\u5b66\u548c\u52a8\u6001\u529b\u5b66\u5206\u6790\u4e2d\u5bf9\u4f4e\u5e94\u529b\u6c34\u5e73\u4e0b\u4e09\u7ef4\u529b\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u7075\u654f\u5ea6\u6355\u83b7\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u7ed3\u5408\u900f\u5c04\u548c\u53cd\u5c04\u5149\u5f39\u6027\u6210\u50cf\u7684\u900f\u660e\u3001\u4f4e\u5f39\u6027\u7684\u65e0\u9884\u5e94\u529b\u73af\u6c27\u6811\u8102\u7acb\u65b9\u4f53\u6a21\u578b\uff0c\u540c\u65f6\u8bb0\u5f55\u4e09\u4e2a\u76f8\u4e92\u6b63\u4ea4\u7684\u7b49\u8272\u6761\u7eb9\u573a\u3002\u91c7\u7528\u5cf0\u8c37\u5f3a\u5ea6\u6cd5\u8fdb\u884c\u56fe\u50cf\u5206\u6790\uff0c\u63d0\u53d6\u4e9a\u6761\u7eb9\u7ea7\u6b21\uff0c\u4ee5\u6700\u5c0f\u7684\u566a\u58f0\u5206\u8fa8\u4f4e\u5e94\u529b\u60c5\u51b5\u3002", "result": "\u7acb\u65b9\u4f53\u5728\u6240\u6709\u65b9\u5411\u4e0a\u5747\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u6761\u7eb9\u56fe\u6837\uff0c\u53ef\u4ee5\u5206\u79bb\u5207\u5411\u548c\u6cd5\u5411\u5e94\u529b\u5206\u91cf\u3002\u72ec\u7acb\u7684\u6b63\u4ea4\u89c6\u56fe\u8bc1\u5b9e\u4e86\u65b9\u5411\u654f\u611f\u6027\uff0c\u5e76\u5728\u4f4e\u8d1f\u8f7d\u4e0b\u4ea7\u751f\u4e86\u7a33\u5b9a\u4e00\u81f4\u7684\u6761\u7eb9\u7ea7\u6b21\u4f30\u8ba1\uff0c\u54cd\u5e94\u65f6\u95f4\u7ea6\u4e3a\u51e0\u5341\u5fae\u79d2\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u7acb\u4e86\u4e00\u79cd\u4ece\u6b63\u4ea4\u89c6\u56fe\u8fdb\u884c\u4e09\u7ef4\u5149\u5f39\u6027\u5e94\u529b\u6d4b\u91cf\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u73b0\u5168\u77e2\u91cf\u529b\u91cd\u5efa\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u548c\u52a8\u6001\u8f7d\u8377\u7814\u7a76\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11951", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11951", "abs": "https://arxiv.org/abs/2511.11951", "authors": ["Nghia Thinh Nguyen", "Tri Nhu Do"], "title": "Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification", "comment": null, "summary": "In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684T-MDS-ViT\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u5fae\u591a\u666e\u52d2\u529f\u7387\u8c31\u56fe\uff0c\u4ee5\u5b9e\u73b0\u591a\u7c7b\u522b\u76ee\u6807\u5206\u7c7b\u3002\u8be5\u6a21\u578b\u5229\u7528Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u8f74\u6ce8\u610f\u529b\u673a\u5236\u663e\u5f0f\u5efa\u6a21MDS\u6570\u636e\u7684\u5e8f\u5217\u7279\u6027\uff0c\u5e76\u7ed3\u5408\u79fb\u52a8\u611f\u77e5\u7ea6\u675f\u5904\u7406\u76ee\u6807\u91cd\u53e0\u548c\u906e\u6321\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u53ef\u89e3\u91ca\u673a\u5236\u5206\u6790\u6a21\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4e0e\u73b0\u6709CNN\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86T-MDS-ViT\u5728\u5206\u7c7b\u7cbe\u5ea6\u3001\u6570\u636e\u6548\u7387\u548c\u5b9e\u65f6\u90e8\u7f72\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7528\u4e8e\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u5fae\u591a\u666e\u52d2\uff08MDS\uff09\u529f\u7387\u8c31\u56fe\u7684\u591a\u7c7b\u522b\u76ee\u6807\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u76ee\u6807\u91cd\u53e0\u548c\u90e8\u5206\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684T-MDS-ViT\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528Transformer\u67b6\u6784\u5904\u7406\u5806\u53e0\u7684RVA\uff08\u8ddd\u79bb-\u901f\u5ea6-\u89d2\u5ea6\uff09\u65f6\u7a7a\u5f20\u91cf\u3002\u901a\u8fc7\u5757\u5d4c\u5165\u548c\u4ea4\u53c9\u8f74\u6ce8\u610f\u529b\u673a\u5236\u6765\u663e\u5f0f\u5efa\u6a21MDS\u6570\u636e\u5728\u591a\u4e2a\u5e27\u4e2d\u7684\u5e8f\u5217\u7279\u6027\u3002\u5728\u6ce8\u610f\u529b\u5c42\u4e2d\u5229\u7528\u79fb\u52a8\u611f\u77e5\u7ea6\u675f\u6765\u4fdd\u6301\u76ee\u6807\u5206\u79bb\u6027\uff0c\u4ee5\u5e94\u5bf9\u76ee\u6807\u91cd\u53e0\u548c\u90e8\u5206\u906e\u6321\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u673a\u5236\u6765\u68c0\u67e5\u6ce8\u610f\u529b\u5c42\u5982\u4f55\u805a\u7126\u4e8eMDS\u8868\u793a\u7684\u9ad8\u80fd\u91cf\u533a\u57df\u53ca\u5176\u5bf9\u7279\u5b9a\u7c7b\u522b\u8fd0\u52a8\u5b66\u7279\u5f81\u7684\u5f71\u54cd\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cT-MDS-ViT\u5728\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u5728\u6570\u636e\u6548\u7387\u548c\u5b9e\u65f6\u90e8\u7f72\u65b9\u9762\u4e5f\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "T-MDS-ViT\u5728\u5904\u7406\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbeMDS\u529f\u7387\u8c31\u56fe\u8fdb\u884c\u591a\u7c7b\u522b\u76ee\u6807\u5206\u7c7b\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u76ee\u6807\u91cd\u53e0\u548c\u90e8\u5206\u906e\u6321\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u548c\u5b9e\u65f6\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2511.11598", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11598", "abs": "https://arxiv.org/abs/2511.11598", "authors": ["Van-Vi Vo", "Tien-Dung Nguyen", "Duc-Tai Le", "Hyunseung Choo"], "title": "Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks", "comment": null, "summary": "Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQ\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7269\u8054\u7f51\u4f20\u611f\u5668\u7f51\u7edc\u7684\u8def\u7531\u9009\u62e9\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u80fd\u8017\u7684\u901a\u4fe1\u3002", "motivation": "\u4f20\u7edf\u4e2d\u5fc3\u5316\u8def\u7531\u7b97\u6cd5\u5728\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5b58\u5728\u8ba1\u7b97\u5bc6\u96c6\u3001\u4e0d\u9002\u5e94\u52a8\u6001\u6027\u548c\u5206\u5e03\u5f0f\u7279\u6027\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7684Q\u5b66\u4e60\u6846\u67b6\uff0c\u4f20\u611f\u5668\u8282\u70b9\u5229\u7528\u5c40\u90e8\u4fe1\u606f\u72ec\u7acb\u5b66\u4e60\u6700\u4f18\u7684\u4e0b\u4e00\u8df3\u9009\u62e9\u3002\u72b6\u6001\u5b9a\u4e49\u5305\u62ec\u8282\u70b9\u4f4d\u7f6e\u548c\u8def\u7531\u5386\u53f2\uff0c\u5956\u52b1\u51fd\u6570\u7528\u4e8e\u6fc0\u52b1\u5411\u6c47\u805a\u8282\u70b9\u79fb\u52a8\u5e76\u60e9\u7f5a\u4f4e\u6548\u8def\u5f84\u3002\u5728\u4e00\u4e2a\u5305\u542b100\u5230500\u4e2a\u8282\u70b9\u7684\u6a21\u62df\u7f51\u7edc\u4e2d\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u7531\u51c6\u786e\u6027\uff08\u8d85\u8fc799%\u7684\u51c6\u786e\u6027\uff0c\u5bf9\u4e8e\u8d85\u8fc7300\u4e2a\u8282\u70b9\u7684\u7f51\u7edc\uff09\uff0c\u5e76\u4e14\u5728\u8f83\u5c0f\u7684\u7f51\u7edc\u4e2d\u53ea\u5e26\u67651-2\u4e2a\u989d\u5916\u8df3\u6570\u3002\u4e0e\u4e2d\u5fc3\u5316\u548c\u6cdb\u6d2a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u80fd\u591f\u9002\u5e94\u62d3\u6251\u53d8\u5316\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u548c\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "Q\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u5177\u6709\u81ea\u4e3b\u3001\u9c81\u68d2\u8def\u7531\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u4f20\u7edf\u534f\u8bae\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.11865", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.11865", "abs": "https://arxiv.org/abs/2511.11865", "authors": ["Jiong Tao", "Yong-Liang Yang", "Bailin Deng"], "title": "Learning Conjugate Direction Fields for Planar Quadrilateral Mesh Generation", "comment": "Accepted to AAAI 2026", "summary": "Planar quadrilateral (PQ) mesh generation is a key process in computer-aided design, particularly for architectural applications where the goal is to discretize a freeform surface using planar quad faces. The conjugate direction field (CDF) defined on the freeform surface plays a significant role in generating a PQ mesh, as it largely determines the PQ mesh layout. Conventionally, a CDF is obtained by solving a complex non-linear optimization problem that incorporates user preferences, i.e., aligning the CDF with user-specified strokes on the surface. This often requires a large number of iterations that are computationally expensive, preventing the interactive CDF design process for a desirable PQ mesh. To address this challenge, we propose a data-driven approach based on neural networks for controlled CDF generation. Our approach can effectively learn and fuse features from the freeform surface and the user strokes, and efficiently generate quality CDF respecting user guidance. To enable training and testing, we also present a dataset composed of 50000+ freeform surfaces with ground-truth CDFs, as well as a set of metrics for quantitative evaluation. The effectiveness and efficiency of our work are demonstrated by extensive experiments using testing data, architectural surfaces, and general 3D shapes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u81ea\u7531\u66f2\u9762\u548c\u7528\u6237\u7b14\u89e6\u4e2d\u5b66\u4e60\u5e76\u878d\u5408\u7279\u5f81\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u7b26\u5408\u7528\u6237\u6307\u5bfc\u7684\u5171\u8f6d\u65b9\u5411\u573a\uff08CDF\uff09\uff0c\u8fdb\u800c\u751f\u6210\u5e73\u9762\u56db\u8fb9\u5f62\uff08PQ\uff09\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u590d\u6742\u7684\u975e\u7ebf\u6027\u4f18\u5316\u6765\u751f\u6210\u5171\u8f6d\u65b9\u5411\u573a\uff08CDF\uff09\uff0c\u4ee5\u83b7\u5f97\u5e73\u9762\u56db\u8fb9\u5f62\uff08PQ\uff09\u7f51\u683c\uff0c\u4f46\u8be5\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b66\u4e60\u81ea\u7531\u66f2\u9762\u548c\u7528\u6237\u7b14\u89e6\u7684\u7279\u5f81\uff0c\u751f\u6210\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u7684CDF\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b50000\u591a\u4e2a\u81ea\u7531\u66f2\u9762\u53ca\u5176\u5bf9\u5e94CDF\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u91cf\u5316\u8bc4\u4f30\u6307\u6807\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5e76\u878d\u5408\u66f2\u9762\u548c\u7528\u6237\u7b14\u89e6\u7684\u7279\u5f81\uff0c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684CDF\uff0c\u4e3aPQ\u7f51\u683c\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12118", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12118", "abs": "https://arxiv.org/abs/2511.12118", "authors": ["Luxin Xu", "Changliang Ren"], "title": "Enhanced Nonreciprocal Quantum Battery Performance via Nonlinear Two-Photon Driving", "comment": null, "summary": "Quantum batteries have attracted significant attention as efficient quantum energy storage devices.In this work, we propose a nonlinear two-photon driving quantum battery model featuring nonreciprocal dynamics that enables a highly efficient unidirectional charging mechanism through environmental engineering. Using a Markovian master-equation approach, we derive analytical solutions for the system dynamics and identify the parameter regime required for dynamical equilibration. Our results reveal that increasing the driving strength enhances both energy conversion and storage efficiency, albeit at the cost of longer equilibration times. Compared with single-photon driving, the two-photon process exhibits a pronounced advantage in energy capacity and entropy regulation, which becomes more prominent under stronger driving. Under asymmetric dissipation, optimizing the system-bath coupling can further improve performance. The proposed model is experimentally feasible and can be implemented across multiple quantum platforms, including photonic systems, superconducting circuits, and magnonic devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u53cc\u5149\u5b50\u9a71\u52a8\u91cf\u5b50\u7535\u6c60\u6a21\u578b\uff0c\u901a\u8fc7\u73af\u5883\u5de5\u7a0b\u5b9e\u73b0\u9ad8\u6548\u5355\u5411\u5145\u7535\uff0c\u5e76\u5206\u6790\u4e86\u5176\u52a8\u529b\u5b66\u548c\u6548\u7387\u3002", "motivation": "\u91cf\u5b50\u7535\u6c60\u4f5c\u4e3a\u9ad8\u6548\u7684\u91cf\u5b50\u50a8\u80fd\u88c5\u7f6e\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u4e3b\u65b9\u7a0b\u65b9\u6cd5\u63a8\u5bfc\u51fa\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u786e\u5b9a\u4e86\u5b9e\u73b0\u52a8\u529b\u5b66\u5e73\u8861\u7684\u53c2\u6570\u8303\u56f4\uff0c\u7814\u7a76\u4e86\u53cc\u5149\u5b50\u9a71\u52a8\u4e0e\u5355\u5149\u5b50\u9a71\u52a8\u7684\u5bf9\u6bd4\u4ee5\u53ca\u7cfb\u7edf-\u6d74\u8026\u5408\u7684\u4f18\u5316\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u9ad8\u9a71\u52a8\u5f3a\u5ea6\u53ef\u4ee5\u63d0\u9ad8\u80fd\u91cf\u8f6c\u6362\u548c\u5b58\u50a8\u6548\u7387\uff0c\u4f46\u4f1a\u589e\u52a0\u8fbe\u5230\u5e73\u8861\u6240\u9700\u7684\u65f6\u95f4\u3002\u4e0e\u5355\u5149\u5b50\u9a71\u52a8\u76f8\u6bd4\uff0c\u53cc\u5149\u5b50\u8fc7\u7a0b\u5728\u80fd\u91cf\u5bb9\u91cf\u548c\u71b5\u8c03\u8282\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5728\u66f4\u5f3a\u7684\u9a71\u52a8\u4e0b\u8fd9\u79cd\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002\u901a\u8fc7\u4f18\u5316\u7cfb\u7edf-\u6d74\u8026\u5408\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u80fd\u91cf\u8f6c\u6362\u6548\u7387\u3001\u5b58\u50a8\u5bb9\u91cf\u548c\u71b5\u8c03\u8282\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u5177\u6709\u5b9e\u9a8c\u53ef\u884c\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u91cf\u5b50\u5e73\u53f0\u3002"}}
{"id": "2511.13573", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13573", "abs": "https://arxiv.org/abs/2511.13573", "authors": ["Joseph", "Naor", "Nitya Raju", "Abhishek Shetty", "Aravind Srinivasan", "Renata Valieva", "David Wajc"], "title": "Dimension-Free Correlated Sampling for the Hypersimplex", "comment": "ITCS 2026", "summary": "Sampling from multiple distributions so as to maximize overlap has been studied by statisticians since the 1950s. Since the 2000s, such correlated sampling from the probability simplex has been a powerful building block in disparate areas of theoretical computer science. We study a generalization of this problem to sampling sets from given vectors in the hypersimplex, i.e., outputting sets of size (at most) some $k$ in $[n]$, while maximizing the sampled sets' overlap. Specifically, the expected difference between two output sets should be at most $\u03b1$ times their input vectors' $\\ell_1$ distance. A value of $\u03b1=O(\\log n)$ is known to be achievable, due to Chen et al.~(ICALP'17). We improve this factor to $O(\\log k)$, independent of the ambient dimension~$n$. Our algorithm satisfies other desirable properties, including (up to a $\\log^* n$ factor) input-sparsity sampling time, logarithmic parallel depth and dynamic update time, as well as preservation of submodular objectives. Anticipating broader use of correlated sampling algorithms for the hypersimplex, we present applications of our algorithm to online paging, offline approximation of metric multi-labeling and swift multi-scenario submodular welfare approximating reallocation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7ed9\u5b9a\u5411\u91cf\u7684\u8d85\u5355\u7eaf\u5f62\u4e2d\u91c7\u6837\u96c6\u5408\u7684\u7b97\u6cd5\uff0c\u5728\u4fdd\u8bc1\u6837\u672c\u96c6\u91cd\u53e0\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u91cd\u53e0\u5ea6\u4e0e\u8f93\u5165\u5411\u91cf\u7684 $\\ell_1$ \u8ddd\u79bb\u4e4b\u6bd4\u7684\u56e0\u5b50\u4ece $O(\\log n)$ \u4f18\u5316\u81f3 $O(\\log k)$\uff0c\u5e76\u4e14\u8be5\u7b97\u6cd5\u8fd8\u5177\u6709\u8f93\u5165\u7a00\u758f\u91c7\u6837\u65f6\u95f4\u3001\u5bf9\u6570\u5e76\u884c\u6df1\u5ea6\u3001\u52a8\u6001\u66f4\u65b0\u65f6\u95f4\u548c\u4fdd\u6301\u5b50\u6a21\u76ee\u6807\u7b49\u4f18\u70b9\u3002", "motivation": "\u6700\u5927\u5316\u91cd\u53e0\u5ea6\u5730\u4ece\u591a\u4e2a\u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u7279\u522b\u662f\u4ece\u6982\u7387\u5355\u7eaf\u5f62\u4e2d\u8fdb\u884c\u76f8\u5173\u91c7\u6837\uff0c\u5df2\u6210\u4e3a\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u672c\u6587\u5c06\u8be5\u95ee\u9898\u63a8\u5e7f\u5230\u4ece\u8d85\u5355\u7eaf\u5f62\u4e2d\u7684\u7ed9\u5b9a\u5411\u91cf\u91c7\u6837\u96c6\u5408\uff0c\u65e8\u5728\u6700\u5927\u5316\u91c7\u6837\u96c6\u5408\u7684\u91cd\u53e0\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u7ed9\u5b9a\u5411\u91cf\u7684\u8d85\u5355\u7eaf\u5f62\u4e2d\u91c7\u6837\u96c6\u5408\uff0c\u5e76\u4fdd\u8bc1\u4e24\u4e2a\u8f93\u51fa\u96c6\u5408\u4e4b\u95f4\u7684\u671f\u671b\u5dee\u5f02\u6700\u591a\u662f\u5b83\u4eec\u8f93\u5165\u5411\u91cf $\\ell_1$ \u8ddd\u79bb\u7684 $O(\\log k)$ \u500d\u3002\u8be5\u7b97\u6cd5\u7684\u91c7\u6837\u65f6\u95f4\u4e0e\u8f93\u5165\u7a00\u758f\u5ea6\u6210\u6b63\u6bd4\uff0c\u5177\u6709\u5bf9\u6570\u5e76\u884c\u6df1\u5ea6\u548c\u52a8\u6001\u66f4\u65b0\u65f6\u95f4\uff0c\u5e76\u80fd\u4fdd\u6301\u5b50\u6a21\u76ee\u6807\u3002", "result": "\u672c\u6587\u7b97\u6cd5\u5c06\u91cd\u53e0\u5ea6\u4e0e\u8f93\u5165\u5411\u91cf $\\ell_1$ \u8ddd\u79bb\u4e4b\u6bd4\u7684\u56e0\u5b50\u4ece\u5df2\u77e5\u7684 $O(\\log n)$ \u4f18\u5316\u81f3 $O(\\log k)$\uff0c\u5b9e\u73b0\u4e86\u72ec\u7acb\u4e8e\u7ef4\u5ea6 $n$ \u7684\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u5728\u91c7\u6837\u65f6\u95f4\u3001\u5e76\u884c\u6df1\u5ea6\u3001\u52a8\u6001\u66f4\u65b0\u548c\u4fdd\u6301\u5b50\u6a21\u76ee\u6807\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4ece\u8d85\u5355\u7eaf\u5f62\u91c7\u6837\u96c6\u5408\u4ee5\u6700\u5927\u5316\u91cd\u53e0\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c06\u6027\u80fd\u56e0\u5b50\u4ece $O(\\log n)$ \u63d0\u5347\u81f3 $O(\\log k)$\uff0c\u5e76\u5177\u5907\u591a\u79cd\u4f18\u826f\u7684\u8ba1\u7b97\u7279\u6027\u3002\u8be5\u7b97\u6cd5\u6709\u671b\u5728\u5728\u7ebf\u5206\u9875\u3001\u5ea6\u91cf\u591a\u6807\u7b7e\u8fd1\u4f3c\u548c\u591a\u573a\u666f\u5b50\u6a21\u798f\u5229\u91cd\u65b0\u5206\u914d\u7b49\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.12018", "categories": ["cs.CV", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12018", "abs": "https://arxiv.org/abs/2511.12018", "authors": ["Shounak Ray Chaudhuri", "Arash Jahangiri", "Christopher Paolini"], "title": "Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis", "comment": "8 pages, 10 figures, Submitted to IEEE Intelligent Vehicles Symposium 2026", "summary": "Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6444\u50cf\u5934\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u65f6\u8bc4\u4f30\u4fe1\u53f7\u4ea4\u53c9\u53e3\u4ea4\u901a\u5b89\u5168\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u8d8a\u7ebf\u65f6\u95f4\uff08PET\uff09\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u78b0\u649e\u7684\u7814\u7a76\u5b58\u5728\u6570\u636e\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u5b9e\u65f6\u3001\u7cbe\u7ec6\u5316\u7684\u4ea4\u901a\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u56db\u4e2a\u540c\u6b65\u6444\u50cf\u5934\u6355\u6349\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7YOLOv11\u5206\u5272\u7b97\u6cd5\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\u3002\u5229\u7528\u5355\u5e94\u6027\u77e9\u9635\u5c06\u68c0\u6d4b\u5230\u7684\u8f66\u8f86\u591a\u8fb9\u5f62\u8f6c\u6362\u4e3a\u9e1f\u77b0\u56fe\uff0c\u5e76\u5e94\u7528\u65b0\u9896\u7684\u50cf\u7d20\u7ea7PET\u7b97\u6cd5\u6765\u7cbe\u786e\u8ba1\u7b97\u8f66\u8f86\u4f4d\u7f6e\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7684\u52a8\u6001\u70ed\u529b\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u4e9a\u79d2\u7ea7\u7cbe\u5ea6\u548c\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u541e\u5410\u91cf\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\uff0c\u751f\u6210\u7684800 x 800\u50cf\u7d20\u5bf9\u6570\u70ed\u529b\u56fe\u5e73\u5747\u5e27\u7387\u4e3a2.68 FPS\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u5206\u5e03\u5f0f\u89c6\u89c9PET\u5206\u6790\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u3001\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u4ea4\u53c9\u53e3\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u590d\u5236\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11829", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11829", "abs": "https://arxiv.org/abs/2511.11829", "authors": ["Mihir Gupte", "Ramesh S"], "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification", "comment": "To be submitted for publication", "summary": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u53ef\u7528\u4e8e\u9a8c\u8bc1LLM\u751f\u6210\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c06\u975e\u5f62\u5f0f\u5316\u8bed\u53e5\u8f6c\u6362\u4e3a\u5f62\u5f0f\u903b\u8f91\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9a8c\u8bc1\u8fd9\u4e9b\u8f93\u51fa\u51c6\u786e\u6027\u7684\u6b63\u5f0f\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\uff08NL\uff09\u8981\u6c42\u4e0eLLM\u751f\u6210\u7684\u8f93\u51fa\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8fdb\u884c\u4e00\u81f4\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u53ef\u4ee5\u8bc6\u522b\u51fa\u4e24\u4e2a\u4e0d\u540c\u63aa\u8f9e\u7684NL\u8981\u6c42\u5728\u903b\u8f91\u4e0a\u662f\u7b49\u4ef7\u7684\uff0c\u8fd8\u53ef\u4ee5\u8bc6\u522b\u51fa\u7ed9\u5b9a\u7684NL\u8981\u6c42\u548cLLM\u751f\u6210\u7684\u8f93\u51fa\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u81ea\u52a8\u5f62\u5f0f\u5316\u6709\u6f5c\u529b\u786e\u4fddLLM\u751f\u6210\u8f93\u51fa\u7684\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4e3a\u672a\u6765\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12987", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12987", "abs": "https://arxiv.org/abs/2511.12987", "authors": ["Daivik Patel", "Shrenik Patel"], "title": "Reuse, Don't Recompute: Efficient Large Reasoning Model Inference via Memory Orchestration", "comment": null, "summary": "Large reasoning models (LRMs) achieve strong accuracy through test-time scaling, generating longer chains of thought or sampling multiple solutions, but at steep costs in tokens and latency. We argue that memory is a core ingredient for efficient reasoning: when evidence already exists, models should think less by reusing structured memory instead of recomputing derivations. We present ENGRAM-R, an inference-time memory layer that integrates typed retrieval with compact fact card representations and explicit citation control. On the LoCoMo benchmark, ENGRAM-R reduces input tokens by 85% and reasoning tokens by 75% compared to full context while maintaining high accuracy. On a multi-hop slice of the LongMemEval benchmark, it achieves similar efficiency with substantial accuracy gains. These results show that memory is not only critical for long-horizon correctness but also a practical lever for efficient reasoning under tight compute, memory, and latency budgets.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684token\u6570\u91cf\u548c\u91c7\u6837\u591a\u4e2a\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u63d0\u51faENGRAM-R\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u7ed3\u5408\u7c7b\u578b\u5316\u68c0\u7d22\u3001\u7d27\u51d1\u7684\u4e8b\u5b9e\u5361\u7247\u8868\u793a\u548c\u663e\u5f0f\u5f15\u7528\u63a7\u5236\uff0c\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002ENGRAM-R\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u8f93\u5165token\u51cf\u5c11\u4e8685%\uff0c\u63a8\u7406token\u51cf\u5c11\u4e8675%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\u3002\u5728LongMemEval\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u8df3\u90e8\u5206\uff0cENGRAM-R\u5728\u76f8\u4f3c\u7684\u6548\u7387\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002\u8fd9\u8868\u660e\u8bb0\u5fc6\u4e0d\u4ec5\u5bf9\u957f\u8ddd\u79bb\u63a8\u7406\u7684\u6b63\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4e14\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u5ef6\u8fdf\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u662f\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u7684\u5b9e\u7528\u624b\u6bb5\u3002", "motivation": "\u76ee\u524d\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0token\u6570\u91cf\u6216\u91c7\u6837\u591a\u4e2a\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u8ba4\u4e3a\u8bb0\u5fc6\u662f\u9ad8\u6548\u63a8\u7406\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u5f53\u5df2\u6709\u8bc1\u636e\u65f6\uff0c\u6a21\u578b\u5e94\u901a\u8fc7\u91cd\u7528\u7ed3\u6784\u5316\u8bb0\u5fc6\u6765\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u63a8\u5bfc\u3002", "method": "\u63d0\u51faENGRAM-R\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u8bb0\u5fc6\u5c42\uff0c\u96c6\u6210\u4e86\u7c7b\u578b\u5316\u68c0\u7d22\u3001\u7d27\u51d1\u7684\u4e8b\u5b9e\u5361\u7247\u8868\u793a\u548c\u663e\u5f0f\u5f15\u7528\u63a7\u5236\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cENGRAM-R\u76f8\u6bd4\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u8f93\u5165token\u51cf\u5c11\u4e8685%\uff0c\u63a8\u7406token\u51cf\u5c11\u4e8675%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\u3002\u5728LongMemEval\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u8df3\u90e8\u5206\uff0cENGRAM-R\u5728\u76f8\u4f3c\u7684\u6548\u7387\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "\u8bb0\u5fc6\u4e0d\u4ec5\u5bf9\u957f\u8ddd\u79bb\u63a8\u7406\u7684\u6b63\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4e14\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u5ef6\u8fdf\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u662f\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u7684\u5b9e\u7528\u624b\u6bb5\u3002"}}
{"id": "2511.12307", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2511.12307", "abs": "https://arxiv.org/abs/2511.12307", "authors": ["Anton Kompatscher", "Morteza Shokrani", "Johanna Feurstein", "Martijn Kemerink"], "title": "Direct vs. Indirect Measurement of the Effective Electronic Temperature in Quantum Dot Solids", "comment": "-", "summary": "One of the characteristics of disordered semiconductors is the slow thermalization of charge carriers after excitation due to photoabsorption or high electric fields. An elegant way to capture the effects of the latter on the conductivity is through a field-dependent effective electronic temperature T_eff that can significantly exceed that of the lattice. Despite its elegance, its actual use has been limited, which, at least in part, can be attributed to the concept originating from computer simulations; experimental confirmations have largely been indirect (through scaling of conductivity) and did not establish that T_eff equals the real temperature of the electron distribution. Moreover, it has hardly been tested for important classes of disordered materials, including quantum dot solids. Here, we investigate whether the effective temperature concept is applicable to quantum dot solids, using zinc oxide as relevant model system. To verify that field-driven conductivity increases indeed reflect an actual increase of the electronic temperature, we combine direct and indirect measurements of T_eff: we convert conductivity changes at high fields to an effective temperature that we show to be consistent with a direct measurement of the electronic temperature using the Seebeck effect. These results not only confirm the relevance of the effective temperature concept to quantum dot solids but also confirm its general physical reality and open the way to systematic investigations into charge carrier (de)localization in disordered media.", "AI": {"tldr": "\u975e\u6676\u534a\u5bfc\u4f53\u4e2d\u8f7d\u6d41\u5b50\u5728\u5149\u5438\u6536\u6216\u9ad8\u7535\u573a\u4e0b\u7684\u70ed\u5316\u8fc7\u7a0b\u5f88\u6162\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5176\u7535\u5bfc\u7387\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u91cf\u5b50\u70b9\u56fa\u4f53\uff08\u4ee5\u6c27\u5316\u950c\u4e3a\u4f8b\uff09\u4e2d\uff0c\u573a\u4f9d\u8d56\u7684\u6709\u6548\u7535\u5b50\u6e29\u5ea6T_eff\u662f\u5426\u9002\u7528\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u9a8c\u8bc1\u6709\u6548\u7535\u5b50\u6e29\u5ea6T_eff\u6982\u5ff5\u5728\u91cf\u5b50\u70b9\u56fa\u4f53\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u7ed3\u5408\u76f4\u63a5\u548c\u95f4\u63a5\u6d4b\u91cf\u624b\u6bb5\u6765\u786e\u8ba4T_eff\u7684\u7269\u7406\u771f\u5b9e\u6027\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u6c27\u5316\u950c\u91cf\u5b50\u70b9\u56fa\u4f53\u7684\u7535\u5bfc\u7387\u53d8\u5316\uff0c\u8ba1\u7b97\u5f97\u5230T_eff\uff0c\u5e76\u5c06\u5176\u4e0e\u901a\u8fc7\u585e\u8d1d\u514b\u6548\u5e94\u76f4\u63a5\u6d4b\u91cf\u7684\u7535\u5b50\u6e29\u5ea6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6709\u6548\u7535\u5b50\u6e29\u5ea6T_eff\u6982\u5ff5\u5728\u91cf\u5b50\u70b9\u56fa\u4f53\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u8868\u660eT_eff\u7684\u6d4b\u91cf\u503c\u4e0e\u585e\u8d1d\u514b\u6548\u5e94\u7684\u6d4b\u91cf\u503c\u4e00\u81f4\u3002", "conclusion": "\u6709\u6548\u7535\u5b50\u6e29\u5ea6T_eff\u6982\u5ff5\u4e0d\u4ec5\u9002\u7528\u4e8e\u91cf\u5b50\u70b9\u56fa\u4f53\uff0c\u800c\u4e14\u5177\u6709\u666e\u904d\u7684\u7269\u7406\u73b0\u5b9e\u610f\u4e49\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u65e0\u5e8f\u4ecb\u8d28\u4e2d\u7684\u7535\u8377\u8f7d\u6d41\u5b50\uff08\u53bb\uff09\u5c40\u57df\u5316\u6253\u5f00\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.12167", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12167", "abs": "https://arxiv.org/abs/2511.12167", "authors": ["Quach Thi Thai Binh", "Thuan Phuoc", "Xuan Hai", "Thang Bach Phan", "Vu Thi Hanh Thu", "Nguyen Tuan Hung"], "title": "Rapid Machine Learning-Driven Detection of Pesticides and Dyes Using Raman Spectroscopy", "comment": "25 pages, 9 figures", "summary": "The extensive use of pesticides and synthetic dyes poses critical threats to food safety, human health, and environmental sustainability, necessitating rapid and reliable detection methods. Raman spectroscopy offers molecularly specific fingerprints but suffers from spectral noise, fluorescence background, and band overlap, limiting its real-world applicability. Here, we propose a deep learning framework based on ResNet-18 feature extraction, combined with advanced classifiers, including XGBoost, SVM, and their hybrid integration, to detect pesticides and dyes from Raman spectroscopy, called MLRaman. The MLRaman with the CNN-XGBoost model achieved a predictive accuracy of 97.4% and a perfect AUC of 1.0, while it with the CNN-SVM model provided competitive results with robust class-wise discrimination. Dimensionality reduction analyses (PCA, t-SNE, UMAP) confirmed the separability of Raman embeddings across 10 analytes, including 7 pesticides and 3 dyes. Finally, we developed a user-friendly Streamlit application for real-time prediction, which successfully identified unseen Raman spectra from our independent experiments and also literature sources, underscoring strong generalization capacity. This study establishes a scalable, practical MLRaman model for multi-residue contaminant monitoring, with significant potential for deployment in food safety and environmental surveillance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLRaman\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408ResNet-18\u7279\u5f81\u63d0\u53d6\u548cXGBoost/SVM\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u4ece\u62c9\u66fc\u5149\u8c31\u4e2d\u68c0\u6d4b\u519c\u836f\u548c\u67d3\u6599\uff0c\u5b9e\u73b0\u4e8697.4%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u548c1.0\u7684AUC\uff0c\u5e76\u5f00\u53d1\u4e86\u7528\u6237\u53cb\u597d\u7684Streamlit\u5e94\u7528\u7a0b\u5e8f\uff0c\u5728\u98df\u54c1\u5b89\u5168\u548c\u73af\u5883\u76d1\u6d4b\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u9274\u4e8e\u519c\u836f\u548c\u5408\u6210\u67d3\u6599\u5bf9\u98df\u54c1\u5b89\u5168\u3001\u4eba\u7c7b\u5065\u5eb7\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u6784\u6210\u7684\u5a01\u80c1\uff0c\u4ee5\u53ca\u73b0\u6709\u62c9\u66fc\u5149\u8c31\u6280\u672f\u7684\u5c40\u9650\u6027\uff08\u5149\u8c31\u566a\u58f0\u3001\u8367\u5149\u80cc\u666f\u548c\u6ce2\u6bb5\u91cd\u53e0\uff09\uff0c\u9700\u8981\u5feb\u901f\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eResNet-18\u7279\u5f81\u63d0\u53d6\u548cXGBoost\u3001SVM\u53ca\u5176\u6df7\u5408\u96c6\u6210\u7684\u9ad8\u7ea7\u5206\u7c7b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08MLRaman\uff09\uff0c\u7528\u4e8e\u4ece\u62c9\u66fc\u5149\u8c31\u4e2d\u68c0\u6d4b\u519c\u836f\u548c\u67d3\u6599\u3002", "result": "MLRaman\u7ed3\u5408CNN-XGBoost\u6a21\u578b\u5b9e\u73b0\u4e8697.4%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u548c1.0\u7684AUC\uff0c\u7ed3\u5408CNN-SVM\u6a21\u578b\u4e5f\u63d0\u4f9b\u4e86\u5177\u6709\u7a33\u5065\u7c7b\u533a\u5206\u80fd\u529b\u7684\u6709\u7ade\u4e89\u529b\u7ed3\u679c\u3002\u964d\u7ef4\u5206\u6790\uff08PCA\u3001t-SNE\u3001UMAP\uff09\u8bc1\u5b9e\u4e8610\u79cd\u5206\u6790\u7269\uff087\u79cd\u519c\u836f\u548c3\u79cd\u67d3\u6599\uff09\u7684\u62c9\u66fc\u5d4c\u5165\u5177\u6709\u53ef\u5206\u79bb\u6027\u3002\u6240\u5f00\u53d1\u7684Streamlit\u5e94\u7528\u7a0b\u5e8f\u80fd\u591f\u5b9e\u65f6\u9884\u6d4b\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u4e86\u72ec\u7acb\u5b9e\u9a8c\u548c\u6587\u732e\u6765\u6e90\u7684\u672a\u77e5\u62c9\u66fc\u5149\u8c31\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u7684MLRaman\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6b8b\u7559\u6c61\u67d3\u7269\u76d1\u6d4b\uff0c\u5728\u98df\u54c1\u5b89\u5168\u548c\u73af\u5883\u76d1\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.11708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11708", "abs": "https://arxiv.org/abs/2511.11708", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "LE-CapsNet: A Light and Enhanced Capsule Network", "comment": null, "summary": "Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).", "AI": {"tldr": "LE-CapsNet \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u589e\u5f3a\u578b\u4e14\u66f4\u51c6\u786e\u7684\u80f6\u56ca\u7f51\u7edc\u53d8\u4f53\uff0c\u5728 CIFAR-10 \u6570\u636e\u96c6\u4e0a\u4ee5 4 \u500d\u7684\u901f\u5ea6\u5b9e\u73b0\u4e86 76.73% \u7684\u51c6\u786e\u7387\uff0c\u5728 AffNIST \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 94.3% \u7684\u51c6\u786e\u7387\u3002", "motivation": "\u80f6\u56ca\u7f51\u7edc\uff08CapsNet\uff09\u867d\u7136\u5728\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u4f18\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u91cd\u53e0\u7c7b\u522b\u548c\u53d8\u6362\u56fe\u50cf\u65b9\u9762\uff0c\u4f46\u5176\u8ba1\u7b97\u901f\u5ea6\u6162\u3001\u53c2\u6570\u591a\u3001\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u5e76\u4e14\u5728\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LE-CapsNet \u7684\u8f7b\u91cf\u7ea7\u3001\u589e\u5f3a\u578b\u4e14\u66f4\u51c6\u786e\u7684\u80f6\u56ca\u7f51\u7edc\u53d8\u4f53\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728 CIFAR-10 \u548c AffNIST \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "LE-CapsNet \u5728 CIFAR-10 \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86 76.73% \u7684\u51c6\u786e\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4 CapsNet \u5feb 4 \u500d\u3002\u5728 AffNIST \u6570\u636e\u96c6\u4e0a\uff0cLE-CapsNet \u7684\u51c6\u786e\u7387\u4e3a 94.3%\uff0c\u4f18\u4e8e CapsNet \u7684 90.52%\u3002", "conclusion": "LE-CapsNet \u4f5c\u4e3a CapsNet \u7684\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u589e\u5f3a\u578b\u53d8\u4f53\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u901f\u5ea6\u548c\u5bf9\u4eff\u5c04\u53d8\u6362\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13050", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2511.13050", "abs": "https://arxiv.org/abs/2511.13050", "authors": ["Jiaqiang Jiang", "Wenfeng Xu", "Jing Fan", "Rui Yan"], "title": "DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks", "comment": "accepted by AAAI-26,The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.", "AI": {"tldr": "SNNs\u5728\u795e\u7ecf\u62df\u6001\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u534f\u540c\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u524d\u5411\u81ea\u9002\u5e94\u9608\u503c\u548c\u540e\u5411\u52a8\u6001SGs\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86SNNs\u7684\u6027\u80fd\u3002", "motivation": "SNNs\u7684\u76f4\u63a5\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u66ff\u4ee3\u68af\u5ea6\uff08SG\uff09\u5b66\u4e60\uff0c\u4f46\u795e\u7ecf\u5143\u819c\u7535\u4f4d\u5206\u5e03\u7684\u53d8\u5316\u548c\u56fa\u5b9a\u9608\u503c\u53ef\u80fd\u5bfc\u81f4\u8109\u51b2\u53d1\u653e\u4e0d\u5e73\u8861\u548c\u68af\u5ea6\u4fe1\u53f7\u51cf\u5f31\uff0c\u5f71\u54cdSNNs\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u534f\u540c\u5b66\u4e60\u7b97\u6cd5\uff1a\u524d\u5411\u4f20\u64ad\u4e2d\uff0c\u6839\u636e\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u819c\u7535\u4f4d\u52a8\u529b\u5b66\uff08MPD\uff09\u5206\u5e03\u81ea\u9002\u5e94\u5730\u8c03\u6574\u9608\u503c\uff0c\u4ee5\u4e30\u5bcc\u795e\u7ecf\u5143\u591a\u6837\u6027\u5e76\u5e73\u8861\u53d1\u653e\u7387\uff1b\u540e\u5411\u4f20\u64ad\u4e2d\uff0c\u52a8\u6001\u4f18\u5316SG\u4ee5\u589e\u5f3a\u68af\u5ea6\u4f30\u8ba1\uff0c\u51cf\u8f7b\u68af\u5ea6\u4fe1\u606f\u4e22\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f7f\u5f97\u795e\u7ecf\u5143\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u80fd\u7a33\u5b9a\u53d1\u653e\u4e00\u5b9a\u6bd4\u4f8b\u7684\u8109\u51b2\uff0c\u5e76\u589e\u52a0\u4e86\u5728\u66f4\u6df1\u5c42\u83b7\u5f97\u68af\u5ea6\u7684\u795e\u7ecf\u5143\u7684\u6bd4\u4f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u9636\u6bb5\u534f\u540c\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3SNNs\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u5176\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.11921", "categories": ["cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11921", "abs": "https://arxiv.org/abs/2511.11921", "authors": ["Liudong Xing", "Janet", "Lin"], "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability", "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature", "summary": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u6784\u5efa\u53ef\u9760\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff08\u7279\u522b\u662f\u4ee3\u7406AI\u7cfb\u7edf\uff09\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u8ba8\u8bba\u4e86\u4e0e\u51cf\u5c11\u7ea7\u8054\u6545\u969c\u98ce\u9669\u76f8\u5173\u7684\u5f00\u653e\u6027\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u5173\u6ce8\u4e86\u52a8\u6001\u73af\u5883\u3001\u4e0d\u4e00\u81f4\u7684\u4efb\u52a1\u6267\u884c\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u6d8c\u73b0\u884c\u4e3a\u4ee5\u53ca\u8d44\u6e90\u5bc6\u96c6\u578b\u53ef\u9760\u6027\u673a\u5236\u7b49\u65b9\u9762\u7684\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u51e0\u79cd\u6d4b\u8bd5\u548c\u8bc4\u4f30\u4ee3\u7406AI\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u4ee3\u7406AI\u7cfb\u7edf\uff09\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u91cd\u70b9\u5173\u6ce8\u51cf\u5c11\u7ea7\u8054\u6545\u969c\u98ce\u9669\u3001\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u3001\u4e0d\u4e00\u81f4\u7684\u4efb\u52a1\u6267\u884c\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u6d8c\u73b0\u884c\u4e3a\u4ee5\u53ca\u8d44\u6e90\u5bc6\u96c6\u578b\u53ef\u9760\u6027\u673a\u5236\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba8\u8bba\u4e0e\u51cf\u5c11\u7ea7\u8054\u6545\u969c\u98ce\u9669\u76f8\u5173\u7684\u5f00\u653e\u6027\u7814\u7a76\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u73af\u5883\u3001\u4e0d\u4e00\u81f4\u7684\u4efb\u52a1\u6267\u884c\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u6d8c\u73b0\u884c\u4e3a\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u53ef\u9760\u6027\u673a\u5236\u7b49\u65b9\u9762\u7684\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\uff0c\u5e76\u63a2\u8ba8\u6d4b\u8bd5\u548c\u8bc4\u4f30\u4ee3\u7406AI\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u672c\u7ae0\u63d0\u51fa\u4e86\u5728\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u4ee3\u7406AI\u7cfb\u7edf\uff09\u65b9\u9762\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u7684\u7814\u7a76\u95ee\u9898\u3001\u6311\u6218\u3001\u673a\u9047\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u9762\u4e34\u7740\u591a\u65b9\u9762\u7684\u6311\u6218\uff0c\u5305\u62ec\u51cf\u5c11\u7ea7\u8054\u6545\u969c\u98ce\u9669\u3001\u9002\u5e94\u52a8\u6001\u73af\u5883\u3001\u786e\u4fdd\u4e00\u81f4\u7684\u4efb\u52a1\u6267\u884c\u3001\u63a7\u5236\u4e0d\u53ef\u9884\u6d4b\u7684\u6d8c\u73b0\u884c\u4e3a\u4ee5\u53ca\u4f18\u5316\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u53ef\u9760\u6027\u673a\u5236\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u8fd9\u4e9b\u9886\u57df\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7406AI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.12879", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12879", "abs": "https://arxiv.org/abs/2511.12879", "authors": ["Ashish Kumar Perukari", "Polina Khoroshevskaya"], "title": "Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination", "comment": null, "summary": "Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u81ea\u4e3b\u8f66\u961f\u7ba1\u7406\u3002", "motivation": "\u5927\u89c4\u6a21\u81ea\u4e3b\u8f66\u961f\u5728\u8d44\u6e90\uff08\u5982\u80fd\u6e90\u3001\u71c3\u6599\u3001\u5145\u7535\u6869\u3001\u7ef4\u62a4\u3001\u65f6\u95f4\u7a97\u53e3\u3001\u901a\u4fe1\u5e26\u5bbd\uff09\u5206\u914d\u4e0a\u9762\u4e34\u901f\u5ea6\u3001\u5f39\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u5206\u5e03\u9884\u6d4b\u548c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u7684\u3001\u611f\u77e5\u4fa7\u4fe1\u606f\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u3002\u5229\u7528\u5c40\u90e8\u4fa7\u4fe1\u606f\u6784\u5efa\u6bcf\u4e2a\u4ee3\u7406\u7684\u98ce\u9669\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6545\u969c\u673a\u4f1a\u7ea6\u675f\u8fdb\u884c\u8026\u5408\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u5171\u8bc6-ADMM\u7b97\u6cd5\u5728\u7a00\u758f\u901a\u4fe1\u56fe\u4e0a\u534f\u8c03\u4ee3\u7406\uff0c\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u7684\u6027\u80fd\u5e76\u907f\u514d\u5355\u70b9\u6545\u969c\u3002", "result": "\u5728\u771f\u5b9e\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u548c\u536b\u661f\u661f\u5ea7\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u8d2a\u5a6a\u3001\u65e0\u4fa7\u4fe1\u606f\u548c\u6700\u4f18\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u6210\u672c\u4e0b\u5c06\u6545\u969c\u7387\u964d\u4f4e\u4e86 30-55%\uff0c\u5e76\u4e14\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u4fdd\u8bc1\u53ef\u884c\u6027\uff0c\u540c\u65f6\u8fd8\u80fd\u6269\u5c55\u5230\u6570\u5343\u4e2a\u4ee3\u7406\uff0c\u8fd0\u884c\u65f6\u957f\u63a5\u8fd1\u7ebf\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4fa7\u4fe1\u606f\u3001\u5206\u5e03\u9884\u6d4b\u548c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u81ea\u4e3b\u8f66\u961f\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6545\u969c\u7387\uff0c\u5e76\u4fdd\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13516", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13516", "abs": "https://arxiv.org/abs/2511.13516", "authors": ["Xiaojie Chen", "Bo Yang", "Naoki Shinohara", "Changjun Liu"], "title": "A High-Efficiency Microwave Power Combining System Based on Frequency-Tuning Injection-Locked Magnetrons", "comment": null, "summary": "To increase the power level and energy utilization rate of injection-locked magnetron sources, a dual way 1-kW S-band magnetron microwave power combining system with high combining efficiency was proposed and validated. A waveguide magic-Tee was used to achieve power combining and to provide a pathway for the reference signal. This system utilizes the power-dividing characteristic of a magic-Tee to lock two magnetrons. Frequency tuning is applied to adjust the phase difference between the two magnetrons' signals so as to achieve a high combining efficiency. Experimental results indicate that the microwave power combining efficiency of the proposed system reaches 94.5%. The attenuation of microwave power is caused only by the waveguides and magic-Tee. Our investigation provides a guideline for future high-power microwave combining systems with low losses.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd1\u5343\u74e6S\u6ce2\u6bb5\u78c1\u63a7\u7ba1\u5fae\u6ce2\u529f\u7387\u5408\u6210\u7cfb\u7edf\uff0c\u91c7\u7528\u6ce2\u5bfc\u9b54Tee\u5b9e\u73b0\u529f\u7387\u5408\u6210\u548c\u53c2\u8003\u4fe1\u53f7\u901a\u8def\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u8c03\u8c10\u5b9e\u73b0\u9ad8\u5408\u6210\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5408\u6210\u6548\u7387\u8fbe\u523094.5%\uff0c\u4e3a\u672a\u6765\u4f4e\u635f\u8017\u5927\u529f\u7387\u5fae\u6ce2\u5408\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6ce8\u5165\u9501\u5b9a\u78c1\u63a7\u7ba1\u7684\u529f\u7387\u6c34\u5e73\u548c\u80fd\u91cf\u5229\u7528\u7387\u3002", "method": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u53cc\u8def1\u5343\u74e6S\u6ce2\u6bb5\u78c1\u63a7\u7ba1\u5fae\u6ce2\u529f\u7387\u5408\u6210\u7cfb\u7edf\uff0c\u4f7f\u7528\u6ce2\u5bfc\u9b54Tee\u5b9e\u73b0\u529f\u7387\u5408\u6210\u548c\u53c2\u8003\u4fe1\u53f7\u901a\u8def\uff0c\u5e76\u5229\u7528\u9b54Tee\u7684\u529f\u7387\u5206\u914d\u7279\u6027\u9501\u5b9a\u4e24\u53f0\u78c1\u63a7\u7ba1\u3002\u901a\u8fc7\u9891\u7387\u8c03\u8c10\u6765\u8c03\u6574\u4e24\u53f0\u78c1\u63a7\u7ba1\u4fe1\u53f7\u7684\u76f8\u4f4d\u5dee\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u529f\u7387\u5408\u6210\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5fae\u6ce2\u529f\u7387\u5408\u6210\u7cfb\u7edf\u7684\u529f\u7387\u5408\u6210\u6548\u7387\u8fbe\u523094.5%\u3002\u5fae\u6ce2\u529f\u7387\u635f\u8017\u4ec5\u7531\u6ce2\u5bfc\u548c\u9b54Tee\u5f15\u8d77\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u4f4e\u635f\u8017\u7684\u5927\u529f\u7387\u5fae\u6ce2\u5408\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.11980", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11980", "abs": "https://arxiv.org/abs/2511.11980", "authors": ["Yuan Guo", "Wen Chen", "Xudong Bai", "Chong He", "Qiong Wu"], "title": "Resource Allocation for Transmissive RIS Transceiver Enabled SWIPT Systems", "comment": null, "summary": "A novel transmissive reconfigurable intelligent surface (TRIS) transceiver-empowered simultaneous wireless information and power transfer (SWIPT) framework is proposed. The sum-rate of the information decoding (ID) users is maximized by optimizing the TRIS transceiver's beamforming, subject to the energy harvesting (EH) users' quality-of-harvest and the per-antenna power constraints. To solve this non-convex problem, we develop an efficient optimization algorithm. First, the original problem is reformulated as a semi-definite programming (SDP) problem. The resulting SDP problem is then addressed using successive convex approximation (SCA) combined with a penalty-based method. Numerical results demonstrate the effectiveness of the algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b0\u578b\u900f\u5c04\u5f0f\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08TRIS\uff09\u6536\u53d1\u5668\u8d4b\u80fd\u7684\u8054\u5408\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\uff08SWIPT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316TRIS\u6536\u53d1\u5668\u7684\u6ce2\u675f\u6210\u5f62\u6765\u6700\u5927\u5316\u4fe1\u606f\u89e3\u7801\uff08ID\uff09\u7528\u6237\u7684\u548c\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\uff08EH\uff09\u7528\u6237\u7684\u80fd\u91cf\u6536\u96c6\u8d28\u91cf\u548c\u6bcf\u5929\u7ebf\u529f\u7387\u7ea6\u675f\u3002\u4e3a\u89e3\u51b3\u8be5\u975e\u51f8\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u9996\u5148\u5c06\u539f\u95ee\u9898\u91cd\u6784\u4e3a\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u95ee\u9898\uff0c\u7136\u540e\u7ed3\u5408\u7f5a\u51fd\u6570\u6cd5\u548c\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u6280\u672f\u6765\u6c42\u89e3\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8054\u5408\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\uff08SWIPT\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u6700\u5927\u5316\u4fe1\u606f\u89e3\u7801\uff08ID\uff09\u7528\u6237\u7684\u548c\u901f\u7387\uff0c\u5e76\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\uff08EH\uff09\u7528\u6237\u7684\u80fd\u91cf\u6536\u96c6\u8d28\u91cf\u548c\u529f\u7387\u7ea6\u675f\u3002", "method": "\u5c06TRIS\u6536\u53d1\u5668\u96c6\u6210\u4e86SWIPT\u6846\u67b6\u4e2d\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u6765\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u95ee\u9898\u30022. \u7ed3\u5408\u7f5a\u51fd\u6570\u6cd5\u548c\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u6280\u672f\u6765\u6c42\u89e3SDP\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u5728\u6700\u5927\u5316\u548c\u901f\u7387\u548c\u6ee1\u8db3\u7ea6\u675f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TRIS\u6536\u53d1\u5668\u8d4b\u80fd\u7684SWIPT\u6846\u67b6\u548c\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u8054\u5408\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2511.11601", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11601", "abs": "https://arxiv.org/abs/2511.11601", "authors": ["Elliott Wen", "Sean Ma", "Ewan Tempero", "Jens Dietrich", "Daniel Luo", "Jiaxing Shen", "Kaiqi Zhao", "Bruce Sham", "Yousong Song", "Jiayi Hua", "Jia Hong"], "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators", "comment": null, "summary": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.", "AI": {"tldr": "\u867d\u7136\u82f1\u4f1f\u8fbe\u5728\u4e91\u6570\u636e\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u52a0\u901f\u5668\u5e02\u573a\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46AMD\u3001Intel\u3001Mac\u548c\u534e\u4e3a\u7b49\u65b0\u5174\u4f9b\u5e94\u5546\u63d0\u4f9b\u4e86\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u58f0\u79f0\u5177\u6709\u517c\u5bb9\u6027\u548c\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5b9e\u8bc1\u7814\u7a76\uff0c\u7814\u7a76\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5f02\u6784\u4eba\u5de5\u667a\u80fd\u52a0\u901f\u5668\u4e2d\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5229\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5408\u6210\u4e86\u6e90\u81ea4000\u4e2a\u771f\u5b9e\u4e16\u754c\u6a21\u578b\u7684100,000\u591a\u4e2a\u53d8\u4f53\u6a21\u578b\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u4f01\u4e1a\u7ea7\u52a0\u901f\u5668\u4e0a\u6267\u884c\u4e86\u5b83\u4eec\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cMac\u548c\u534e\u4e3a\u8f83\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u5e73\u53f0\u652f\u6301\u7684\u7b97\u5b50\u6bd4\u82f1\u4f1f\u8fbe\u5c11\u81f3\u5c1117%\u3002\u8fd9\u4e9b\u5e73\u53f0\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8f93\u51fa\u5dee\u5f02\u7387\uff08\u8d85\u8fc75%\uff09\uff0c\u8fd9\u6e90\u4e8e\u7b97\u5b50\u5b9e\u73b0\u3001\u6570\u503c\u5f02\u5e38\u503c\u5904\u7406\u548c\u6307\u4ee4\u8c03\u5ea6\u7684\u5dee\u5f02\u3002\u5b83\u4eec\u5728\u57fa\u4e8e\u6a21\u578b\u7684\u7f16\u8bd1\u52a0\u901f\u8fc7\u7a0b\u4e2d\u4e5f\u66f4\u5bb9\u6613\u51fa\u73b0\u6545\u969c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7f16\u8bd1\u540e\u7684\u6a21\u578b\u4ea7\u751f\u7684\u8f93\u51fa\u4e0e\u4f7f\u7528\u6807\u51c6\u6267\u884c\u6a21\u5f0f\u751f\u6210\u7684\u8f93\u51fa\u660e\u663e\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0PyTorch\u67097\u4e2a\u5b9e\u73b0\u7f3a\u9677\uff0c\u4ee5\u53ca\u8de8\u4f9b\u5e94\u5546\u768440\u4e2a\u7279\u5b9a\u5e73\u53f0\u95ee\u9898\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u5728\u65e5\u76ca\u591a\u6837\u5316\u7684\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u884c\u4e3a\u4e00\u81f4\u6027\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5f02\u6784\u4eba\u5de5\u667a\u80fd\u52a0\u901f\u5668\u4e2d\u7684\u5dee\u5f02\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u591a\u6837\u5316\u7684\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5408\u6210\u6e90\u81ea4000\u4e2a\u771f\u5b9e\u4e16\u754c\u6a21\u578b\u7684100,000\u591a\u4e2a\u53d8\u4f53\u6a21\u578b\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u4f01\u4e1a\u7ea7\u52a0\u901f\u5668\u4e0a\u6267\u884c\u3002", "result": "Mac\u548c\u534e\u4e3a\u8f83\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u5e73\u53f0\u652f\u6301\u7684\u7b97\u5b50\u6bd4\u82f1\u4f1f\u8fbe\u5c11\u81f3\u5c1117%\u3002\u8fd9\u4e9b\u5e73\u53f0\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8f93\u51fa\u5dee\u5f02\u7387\uff08\u8d85\u8fc75%\uff09\uff0c\u8fd9\u6e90\u4e8e\u7b97\u5b50\u5b9e\u73b0\u3001\u6570\u503c\u5f02\u5e38\u503c\u5904\u7406\u548c\u6307\u4ee4\u8c03\u5ea6\u7684\u5dee\u5f02\u3002\u5b83\u4eec\u5728\u57fa\u4e8e\u6a21\u578b\u7684\u7f16\u8bd1\u52a0\u901f\u8fc7\u7a0b\u4e2d\u4e5f\u66f4\u5bb9\u6613\u51fa\u73b0\u6545\u969c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7f16\u8bd1\u540e\u7684\u6a21\u578b\u4ea7\u751f\u7684\u8f93\u51fa\u4e0e\u4f7f\u7528\u6807\u51c6\u6267\u884c\u6a21\u5f0f\u751f\u6210\u7684\u8f93\u51fa\u660e\u663e\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86PyTorch\u76847\u4e2a\u5b9e\u73b0\u7f3a\u9677\u548c\u8de8\u4f9b\u5e94\u5546\u768440\u4e2a\u7279\u5b9a\u5e73\u53f0\u95ee\u9898\u3002", "conclusion": "\u5728\u65e5\u76ca\u591a\u6837\u5316\u7684\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u884c\u4e3a\u4e00\u81f4\u6027\u6240\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2511.12251", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12251", "abs": "https://arxiv.org/abs/2511.12251", "authors": ["Xiaohui Li", "Xiaolong Liu", "Zhongchen Shi", "Wei Chen", "Liang Xie", "Meng Gai", "Jun Cao", "Suxia Zhang", "Erwei Yin"], "title": "Locomotion in CAVE: Enhancing Immersion through Full-Body Motion", "comment": null, "summary": "Cave Automatic Virtual Environment (CAVE) is one of the virtual reality (VR) immersive devices currently used to present virtual environments. However, the locomotion methods in the CAVE are limited by unnatural interaction methods, severely hindering the user experience and immersion in the CAVE. We proposed a locomotion framework for CAVE environments aimed at enhancing the immersive locomotion experience through optimized human motion recognition technology. Firstly, we construct a four-sided display CAVE system, then through the dynamic method based on Perspective-n-Point to calibrate the camera, using the obtained camera intrinsics and extrinsic parameters, and an action recognition architecture to get the action category. At last, transform the action category to a graphical workstation that renders display effects on the screen. We designed a user study to validate the effectiveness of our method. Compared to the traditional methods, our method has significant improvements in realness and self-presence in the virtual environment, effectively reducing motion sickness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684CAVE\u865a\u62df\u73b0\u5b9e\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u79fb\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4eba\u4f53\u8fd0\u52a8\u8bc6\u522b\u6280\u672f\u6765\u589e\u5f3a\u6c89\u6d78\u611f\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u6655\u52a8\u75c7\u3002", "motivation": "CAVE\u8bbe\u5907\u5728\u63d0\u4f9b\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5176\u79fb\u52a8\u65b9\u5f0f\u7684\u4ea4\u4e92\u4e0d\u81ea\u7136\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u6c89\u6d78\u611f\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u56db\u9762\u663e\u793aCAVE\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8ePerspective-n-Point\u7684\u52a8\u6001\u65b9\u6cd5\u6821\u51c6\u76f8\u673a\uff0c\u83b7\u53d6\u76f8\u673a\u5185\u5916\u53c2\u6570\uff0c\u5e76\u5229\u7528\u52a8\u4f5c\u8bc6\u522b\u67b6\u6784\u8bc6\u522b\u7528\u6237\u52a8\u4f5c\uff0c\u6700\u540e\u5c06\u52a8\u4f5c\u7c7b\u522b\u8f6c\u5316\u4e3a\u56fe\u5f62\u5de5\u4f5c\u7ad9\u7684\u663e\u793a\u6548\u679c\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u73af\u5883\u7684\u771f\u5b9e\u611f\u548c\u81ea\u6211\u4e34\u573a\u611f\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u6655\u52a8\u75c7\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u4eba\u4f53\u8fd0\u52a8\u8bc6\u522b\u7684\u79fb\u52a8\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347CAVE\u73af\u5883\u4e0b\u7684\u6c89\u6d78\u5f0f\u79fb\u52a8\u4f53\u9a8c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u4e2d\u5b58\u5728\u7684\u4ea4\u4e92\u4e0d\u81ea\u7136\u7684\u95ee\u9898\uff0c\u5e76\u5bf9\u51cf\u5c11\u7528\u6237\u6655\u52a8\u75c7\u6709\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2511.11895", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11895", "abs": "https://arxiv.org/abs/2511.11895", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing", "comment": "9 pages, 8 figures. this is the preprint version of the paper accepted for publication at ICCAD 2025", "summary": "This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u95ed\u73af\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6d4b\u8bd5\u9ad8\u5206\u8fa8\u7387SAR ADC\u7684\u7ebf\u6027\u5ea6\uff0c\u901a\u8fc7\u5b9e\u65f6\u66f4\u65b0\u7684EKF\u884c\u4e3a\u6a21\u578b\u548c\u81ea\u9002\u5e94\u6d4b\u91cf\u70b9\u9009\u62e9\uff0c\u51cf\u5c11\u6d4b\u8bd5\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709ADC\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u65b9\u6cd5\uff08\u5982\u76f4\u65b9\u56fe\u6cd5\u3001\u6b63\u5f26\u6ce2\u6d4b\u8bd5\u3001\u6a21\u578b\u9a71\u52a8\u91cd\u5efa\uff09\u5b58\u5728\u6d4b\u8bd5\u65f6\u95f4\u957f\u3001\u590d\u6742\u5ea6\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5927\u91cf\u6570\u636e\u91c7\u96c6\u548c\u79bb\u7ebf\u540e\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u5b9e\u65f6\u66f4\u65b0\u884c\u4e3a\u6a21\u578b\uff0c\u76f4\u63a5\u4f30\u8ba1\u51b3\u5b9aINL\u884c\u4e3a\u7684\u7535\u5bb9\u5931\u914d\u53c2\u6570\uff0c\u5e76\u6839\u636e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u9009\u62e9\u6d4b\u91cf\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u603b\u6d4b\u8bd5\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002", "conclusion": "\u63d0\u51fa\u7684\u95ed\u73af\u6d4b\u8bd5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u65f6\u884c\u4e3a\u6a21\u578b\u548c\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u9ad8\u6548\u5730\u8fdb\u884cSAR ADC\u7684\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u3002"}}
{"id": "2511.12138", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12138", "abs": "https://arxiv.org/abs/2511.12138", "authors": ["Dariya Salykina", "Daniil Shakhbaziants", "Igor Bilenko", "Farid Khalili"], "title": "Application of optical squeezing to microresonator based optical sensors", "comment": "9 pages, 2 figures", "summary": "High-Q optical microresonators combine low losses and high optical energy concentration in a small effective mode volume, making them an attractive platform for optical sensors. While light is confined in the microresonator by total internal reflection, a portion of the optical field, known as the evanescent field, extends outside. This makes the mode's resonant frequency sensitive to changes in the surrounding environment.\n  In this work, we explore the quantum sensitivity limits of this type of sensors. We demonstrate that by preparing the probe light in a squeezed quantum state, it is possible to surpass the shot-noise limit. The resulting sensitivity is constrained only by optical losses and the available degree of squeezing. The influence of the losses can be reduced using additional squeezing of the light inside the microresonator.", "AI": {"tldr": "\u9ad8Q\u503c\u5149\u5b66\u5fae\u8154\u53ef\u7528\u4e8e\u6784\u5efa\u8d85\u8d8a\u6807\u51c6\u91cf\u5b50\u6781\u9650\u7684\u8d85\u7075\u654f\u5ea6\u5149\u5b66\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u4f7f\u7528\u538b\u7f29\u6001\u5149\u548c\u8154\u5185\u538b\u7f29\u6280\u672f\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\u5e76\u964d\u4f4e\u635f\u8017\u7684\u5f71\u54cd\u3002", "motivation": "\u5149\u5b66\u5fae\u8154\u7684\u635f\u8017\u4f4e\u3001\u80fd\u91cf\u96c6\u4e2d\uff0c\u662f\u5149\u5b66\u4f20\u611f\u5668\u7684\u7406\u60f3\u5e73\u53f0\u3002\u7136\u800c\uff0c\u73b0\u6709\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\u53d7\u9650\u4e8e\u6807\u51c6\u91cf\u5b50\u6781\u9650\uff08\u6563\u7c92\u566a\u58f0\u6781\u9650\uff09\u3002", "method": "\u901a\u8fc7\u5236\u5907\u538b\u7f29\u6001\uff08squeezed state\uff09\u7684\u5149\u6765\u63a2\u6d4b\u5fae\u8154\uff0c\u5e76\u63a2\u7d22\u4e86\u8154\u5185\u538b\u7f29\u6280\u672f\u4ee5\u51cf\u5c0f\u635f\u8017\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528\u538b\u7f29\u6001\u5149\u53ef\u4ee5\u8d85\u8d8a\u6563\u7c92\u566a\u58f0\u6781\u9650\uff0c\u5b9e\u73b0\u7684\u7075\u654f\u5ea6\u4ec5\u53d7\u9650\u4e8e\u5149\u5b66\u635f\u8017\u548c\u53ef\u7528\u7684\u538b\u7f29\u5ea6\u3002\u8154\u5185\u538b\u7f29\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u635f\u8017\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u538b\u7f29\u6001\u5149\u548c\u8154\u5185\u538b\u7f29\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8d8a\u6807\u51c6\u91cf\u5b50\u6781\u9650\u7684\u9ad8\u7075\u654f\u5ea6\u5149\u5b66\u4f20\u611f\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4f20\u611f\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13582", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13582", "abs": "https://arxiv.org/abs/2511.13582", "authors": ["Alfonso Cevallos", "Robert Hambrock", "Alistair Stewart"], "title": "The Merkle Mountain Belt", "comment": "36 pages, 15 figures, 5 tables, 1 algorithm", "summary": "Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.\n  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.\n  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data.", "AI": {"tldr": "Merkle \u6811\u7ed3\u6784\u5728\u533a\u5757\u94fe\u5e94\u7528\u4e2d\u4f5c\u4e3a\u627f\u8bfa\u65b9\u6848\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 Merkle \u7ed3\u6784 MMB\uff0c\u5b9e\u73b0\u4e86\u7b80\u6d01\u3001\u589e\u91cf\u548c\u6700\u4f18\u53ef\u52a0\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u533a\u5757\u94fe\u5e94\u7528\u7684\u8f7b\u5ba2\u6237\u7aef\u534f\u8bae\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u52a0\u5feb\u7528\u6237\u4ece\u667a\u80fd\u624b\u673a\u9a8c\u8bc1\u4ea4\u6613\u7684\u901f\u5ea6\u3002", "method": "\u6bd4\u8f83\u4e86 Merkle \u6811\u3001Merkle \u94fe\u548c Merkle \u6811\u5c71 (MMR) \u7b49\u4e0d\u540c\u7684 Merkle \u7ed3\u6784\u4f5c\u4e3a\u627f\u8bfa\u65b9\u6848\u7684\u6027\u80fd\u3002\u5f15\u5165\u4e86\u65b0\u7684 Merkle \u7ed3\u6784\uff0c\u5982 Merkle \u6811\u5c71\u5e26 (MMB) \u53ca\u5176\u5f02\u6b65\u53d8\u4f53 UMMB\u3002", "result": "MMR \u7ed3\u6784\u5177\u6709\u7b80\u6d01\u6027\uff0c\u9002\u7528\u4e8e\u9996\u6b21\u540c\u6b65\u7684\u8f7b\u5ba2\u6237\u7aef\u3002Merkle \u94fe\u7ed3\u6784\u662f\u589e\u91cf\u7684\uff0c\u9002\u7528\u4e8e\u9891\u7e41\u540c\u6b65\u7684\u8f7b\u5ba2\u6237\u7aef\uff0c\u5e76\u4e14\u662f\u53ef\u52a0\u7684\u3002MMB \u7ed3\u6784\u9996\u6b21\u5b9e\u73b0\u4e86\u7b80\u6d01\u6027\u3001\u589e\u91cf\u6027\u548c\u6700\u4f18\u53ef\u52a0\u6027\u7684\u7ed3\u5408\u3002UMMB \u53d8\u4f53\u8fd8\u652f\u6301\u5f02\u6b65\u64cd\u4f5c\u3002\u65b0\u7ed3\u6784\u63d0\u4f9b\u7684\u6210\u5458\u8bc1\u660e\u957f\u5ea6\u4e0e\u9879\u76ee\u6dfb\u52a0\u65f6\u95f4\u76f8\u5173\uff0c\u6709\u5229\u4e8e\u67e5\u8be2\u6700\u8fd1\u751f\u6210\u7684\u6570\u636e\u3002", "conclusion": "MMB \u53ca\u5176\u53d8\u4f53 UMMB \u5728\u7b80\u6d01\u6027\u3001\u589e\u91cf\u6027\u548c\u53ef\u52a0\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709 Merkle \u7ed3\u6784\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f7b\u5ba2\u6237\u7aef\u534f\u8bae\u548c\u5904\u7406\u8fd1\u671f\u6570\u636e\u67e5\u8be2\u7684\u533a\u5757\u94fe\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.12784", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12784", "abs": "https://arxiv.org/abs/2511.12784", "authors": ["Hayden Moore", "Asfahan Shah"], "title": "Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.", "AI": {"tldr": "LLMs\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u7ecf\u8fc7\u91ca\u4e49\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u65f6\uff0c\u5176\u5f62\u5f0f\u5316\u8bc1\u660e\u7684\u751f\u6210\u4ecd\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u5373\u4f7f\u91ca\u4e49\u4fdd\u7559\u4e86\u5927\u90e8\u5206\u8bed\u4e49\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5728MiniF2F\u548cProofNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u4e24\u79cd\u73b0\u4ee3LLM\uff0c\u5e76\u8fdb\u884c\u8de8\u6a21\u578b\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u63a2\u7a76LLM\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u9886\u57df\u5bf9\u91ca\u4e49\u8f93\u5165\u7684\u654f\u611f\u6027\uff0c\u4ee5\u8bc4\u4f30\u5176\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528MiniF2F\u548cProofNet\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e24\u79cd\u73b0\u4ee3LLM\u751f\u6210\u91ca\u4e49\u540e\u7684\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\uff0c\u5e76\u8fdb\u884c\u8de8\u6a21\u578b\u8bc4\u4f30\uff0c\u8861\u91cf\u751f\u6210\u8bc1\u660e\u7684\u8bed\u4e49\u548c\u7f16\u8bd1\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u5904\u7406\u91ca\u4e49\u8f93\u5165\u7684\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7ec6\u5fae\u7684\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u53d8\u5316\u4f1a\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u9886\u57df\u5bf9\u91ca\u4e49\u8f93\u5165\u7684\u654f\u611f\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13233", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13233", "abs": "https://arxiv.org/abs/2511.13233", "authors": ["Jun Sashihara", "Yukihisa Fujita", "Kota Nakamura", "Masahiro Kuwahara", "Teruaki Hayashi"], "title": "LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces", "comment": "10 pages, 12 figures", "summary": "Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08LLM-MAS\uff09\u88ab\u63d0\u51fa\u7528\u4e8e\u6570\u636e\u5e02\u573a\uff0c\u4ee5\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u4ea4\u6613\u6a21\u5f0f\u548c\u5e02\u573a\u52a8\u6001\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5e02\u573a\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5e02\u573a\u53c2\u4e0e\u8005\u3001\u6570\u636e\u548c\u6cd5\u89c4\u4e4b\u95f4\u4ea4\u4e92\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7531LLM\u9a71\u52a8\u7684\u4e70\u5356\u53cc\u65b9\u4ee3\u7406\u7ec4\u6210\u7684\u7cfb\u7edf\uff08LLM-MAS\uff09\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u5730\u8fdb\u884c\u89c4\u5212\u3001\u641c\u7d22\u3001\u5b9a\u4ef7\u548c\u66f4\u65b0\u7b49\u7b56\u7565\u6027\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u9002\u5e94\u5e02\u573a\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\uff0c\u4f7f\u7528\u8d2d\u4e70\u6b21\u6570/\u6570\u636e\u96c6\u3001\u8d2d\u4e70\u6b21\u6570/\u4e70\u5bb6\u548c\u91cd\u590d\u8d2d\u4e70\u6b21\u6570\u4e09\u4e2a\u6307\u6807\u8bc4\u4f30\uff0cLLM-MAS\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u80fd\u51c6\u786e\u5730\u518d\u73b0\u771f\u5b9e\u6570\u636e\u5e02\u573a\u7684\u4ea4\u6613\u6a21\u5f0f\uff0c\u5e76\u6355\u6349\u5230\u5e02\u573a\u8d8b\u52bf\u7684\u51fa\u73b0\u548c\u6f14\u53d8\u3002", "conclusion": "LLM-MAS\u4e3a\u6570\u636e\u5e02\u573a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5f3a\u5927\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684\u6a21\u62df\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u5e02\u573a\u884c\u4e3a\u548c\u52a8\u6001\u3002"}}
{"id": "2511.12324", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12324", "abs": "https://arxiv.org/abs/2511.12324", "authors": ["Naomi Fokkens", "Fei Xue"], "title": "Deterministic Switching of Perpendicular Ferromagnets by Higher-order Spin-orbit Torque in Noncentrosymmetric Weyl Semimetals", "comment": null, "summary": "Field-free deterministic switching of perpendicular ferromagnets is a central challenge for spintronics applications, typically requiring explicit symmetry breaking. Here we show that deterministic switching can instead be achieved through higher-order (in magnetization angles) spin-orbit torques, even in systems that preserve in-plane mirror symmetry. Using a vector spherical harmonics expansion, we demonstrate that higher-order torque terms naturally give rise to additional out-of-equator fixed points, enabling reliable magnetization reversal when their magnitude is comparable to conventional lowest-order torques. We illustrate this mechanism with first-principles calculations on the noncentrosymmetric Weyl ferromagnet PrAlGe, where the combination of Weyl-node band topology and strong spin-orbit coupling produces sizable higher-order torque components. Because the Fermi surface is small, the conventional lowest-order torques are relatively weak, allowing the higher-order harmonics to compete on equal footing and strongly reshape the magnetization dynamics. The resulting spin dynamics confirm deterministic switching without additional symmetry breaking. Our results establish higher-order spin-orbit torque as a key ingredient for understanding and controlling magnetization dynamics in topological and spintronic materials.", "AI": {"tldr": "\u5728\u4e0d\u7834\u574f\u5bf9\u79f0\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u9ad8\u9636\u81ea\u65cb-\u8f68\u9053\u529b\u77e9\u786e\u5b9a\u6027\u5730\u5207\u6362\u5782\u76f4\u94c1\u78c1\u4f53\u3002", "motivation": "\u5b9e\u73b0\u65e0\u5916\u573a\u9a71\u52a8\u7684\u786e\u5b9a\u6027\u78c1\u5316\u5207\u6362\uff0c\u4ee5\u5e94\u5bf9\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u77e2\u91cf\u7403\u8c10\u51fd\u6570\u5c55\u5f00\uff0c\u8bc1\u660e\u9ad8\u9636\u529b\u77e9\u9879\u80fd\u4ea7\u751f\u989d\u5916\u7684\u8d64\u9053\u5916\u56fa\u5b9a\u70b9\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u9760\u7684\u78c1\u5316\u53cd\u8f6c\u3002\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u5728\u975e\u4e2d\u5fc3\u5bf9\u79f0\u7684Weyl\u94c1\u78c1\u4f53PrAlGe\u4e2d\u8bf4\u660e\u4e86\u8fd9\u4e00\u673a\u5236\u3002", "result": "\u5728PrAlGe\u4e2d\uff0c\u7531\u4e8eWeyl\u8282\u70b9\u5e26\u62d3\u6251\u548c\u5f3a\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff0c\u4ea7\u751f\u4e86\u663e\u8457\u7684\u9ad8\u9636\u81ea\u65cb-\u8f68\u9053\u529b\u77e9\u5206\u91cf\u3002\u8fd9\u4e9b\u9ad8\u9636\u529b\u77e9\u4e0e\u76f8\u5bf9\u8f83\u5f31\u7684\u4f4e\u9636\u529b\u77e9\u7ade\u4e89\uff0c\u91cd\u5851\u78c1\u5316\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u5207\u6362\u3002", "conclusion": "\u9ad8\u9636\u81ea\u65cb-\u8f68\u9053\u529b\u77e9\u662f\u7406\u89e3\u548c\u63a7\u5236\u62d3\u6251\u53ca\u81ea\u65cb\u7535\u5b50\u6750\u6599\u4e2d\u78c1\u5316\u52a8\u529b\u5b66\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2511.12243", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12243", "abs": "https://arxiv.org/abs/2511.12243", "authors": ["Ziduo Yang", "Yi-Ming Zhao", "Xian Wang", "Wei Zhuo", "Xiaoqing Liu", "Lei Shen"], "title": "Equivariant Atomic and Lattice Modeling Using Geometric Deep Learning for Crystal Structure Optimization", "comment": null, "summary": "Structure optimization, which yields the relaxed structure (minimum-energy state), is essential for reliable materials property calculations, yet traditional ab initio approaches such as density-functional theory (DFT) are computationally intensive. Machine learning (ML) has emerged to alleviate this bottleneck but suffers from two major limitations: (i) existing models operate mainly on atoms, leaving lattice vectors implicit despite their critical role in structural optimization; and (ii) they often rely on multi-stage, non-end-to-end workflows that are prone to error accumulation. Here, we present E3Relax, an end-to-end equivariant graph neural network that maps an unrelaxed crystal directly to its relaxed structure. E3Relax promotes both atoms and lattice vectors to graph nodes endowed with dual scalar-vector features, enabling unified and symmetry-preserving modeling of atomic displacements and lattice deformations. A layer-wise supervision strategy forces every network depth to make a physically meaningful refinement, mimicking the incremental convergence of DFT while preserving a fully end-to-end pipeline. We evaluate E3Relax on four benchmark datasets and demonstrate that it achieves remarkable accuracy and efficiency. Through DFT validations, we show that the structures predicted by E3Relax are energetically favorable, making them suitable as high-quality initial configurations to accelerate DFT calculations.", "AI": {"tldr": "E3Relax\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u76f4\u63a5\u5c06\u672a\u5f1b\u8c6b\u7684\u6676\u4f53\u6620\u5c04\u5230\u5176\u5f1b\u8c6b\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ece\u5934\u7b97\u65b9\u6cd5\u7684\u8ba1\u7b97\u5bc6\u96c6\u6027\u95ee\u9898\uff0c\u5e76\u514b\u670d\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u6676\u683c\u5411\u91cf\u548c\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u4ece\u5934\u7b97\u65b9\u6cd5\uff08\u5982DFT\uff09\u5728\u8fdb\u884c\u53ef\u9760\u7684\u6750\u6599\u6027\u8d28\u8ba1\u7b97\u65f6\uff0c\u5176\u7ed3\u6784\u4f18\u5316\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6709\u6240\u7f13\u89e3\uff0c\u4f46\u5b58\u5728\u53ea\u5173\u6ce8\u539f\u5b50\u800c\u5ffd\u7565\u6676\u683c\u5411\u91cf\u4ee5\u53ca\u5de5\u4f5c\u6d41\u7a0b\u975e\u7aef\u5230\u7aef\u6613\u7d2f\u79ef\u8bef\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aE3Relax\u7684\u7aef\u5230\u7aef\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u539f\u5b50\u548c\u6676\u683c\u5411\u91cf\u4f5c\u4e3a\u5177\u6709\u53cc\u91cd\u6807\u91cf-\u5411\u91cf\u7279\u5f81\u7684\u56fe\u8282\u70b9\uff0c\u5b9e\u73b0\u4e86\u539f\u5b50\u4f4d\u79fb\u548c\u6676\u683c\u53d8\u5f62\u7684\u7edf\u4e00\u548c\u5bf9\u79f0\u6027\u4fdd\u6301\u5efa\u6a21\u3002\u91c7\u7528\u5206\u5c42\u76d1\u7763\u7b56\u7565\uff0c\u4f7f\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u90fd\u80fd\u8fdb\u884c\u6709\u7269\u7406\u610f\u4e49\u7684\u7cbe\u70bc\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u7aef\u5230\u7aef\u6d41\u7a0b\u7684\u540c\u65f6\uff0c\u6a21\u62df\u4e86DFT\u7684\u589e\u91cf\u6536\u655b\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cE3Relax\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002\u901a\u8fc7DFT\u9a8c\u8bc1\uff0cE3Relax\u9884\u6d4b\u7684\u7ed3\u6784\u5177\u6709\u80fd\u91cf\u4f18\u52bf\uff0c\u53ef\u4f5c\u4e3a\u52a0\u901fDFT\u8ba1\u7b97\u7684\u9ad8\u8d28\u91cf\u521d\u59cb\u6784\u578b\u3002", "conclusion": "E3Relax\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed3\u6784\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5c40\u9650\u6027\u95ee\u9898\uff0c\u80fd\u591f\u76f4\u63a5\u9884\u6d4b\u6750\u6599\u7684\u5f1b\u8c6b\u7ed3\u6784\uff0c\u5e76\u80fd\u4f5c\u4e3a\u52a0\u901fDFT\u8ba1\u7b97\u7684\u6709\u6548\u521d\u59cb\u6784\u578b\u3002"}}
{"id": "2511.11710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11710", "abs": "https://arxiv.org/abs/2511.11710", "authors": ["Zhou Xu", "Qi Wang", "Yuxiao Yang", "Luyuan Zhang", "Zhang Liang", "Yang Li"], "title": "Target-Balanced Score Distillation", "comment": null, "summary": "Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.", "AI": {"tldr": "SDS\u5b58\u5728\u9971\u548c\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5f15\u5165\u8d1f\u9762\u63d0\u793a\u53ef\u7f13\u89e3\u4f46\u4f1a\u5f71\u54cd\u7eb9\u7406\u4f18\u5316\u6216\u5f62\u72b6\u4fdd\u771f\u5ea6\u3002\u672c\u6587\u63d0\u51faTBSD\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548c\u81ea\u9002\u5e94\u7b56\u7565\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7eb9\u7406\u548c\u51e0\u4f55\u5f62\u72b6\u3002", "motivation": "SDS\u65b9\u6cd5\u57283D\u8d44\u4ea7\u751f\u6210\u4e2d\u5b58\u5728\u8fc7\u9971\u548c\u548c\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u73b0\u6709\u8d1f\u9762\u63d0\u793a\u65b9\u6cd5\u5728\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u5e73\u8861\u5f97\u5206\u84b8\u998f\uff08TBSD\uff09\uff0c\u5c06\u751f\u6210\u89c6\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7b56\u7565\u6765\u89e3\u51b3\u7eb9\u7406\u548c\u5f62\u72b6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "result": "TBSD\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u751f\u6210\u76843D\u8d44\u4ea7\u5177\u6709\u9ad8\u4fdd\u771f\u7eb9\u7406\u548c\u51e0\u4f55\u5f62\u72b6\u3002", "conclusion": "TBSD\u80fd\u591f\u6709\u6548\u89e3\u51b3SDS\u65b9\u6cd5\u4e2d\u7684\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u8d44\u4ea7\u3002"}}
{"id": "2511.13053", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.13053", "abs": "https://arxiv.org/abs/2511.13053", "authors": ["Akira Tamamori"], "title": "Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks", "comment": "4 pages, 3 figures", "summary": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.", "AI": {"tldr": "Kernel-based Hopfield networks store more information due to a geometric energy landscape, with a novel ", "motivation": "The dynamical mechanism behind the storage capacity enhancement in kernel-based Hopfield networks is poorly understood.", "method": "Geometric analysis of the network's energy landscape, introducing a novel metric ", "result": "A rich phase diagram of attractor shapes was uncovered by varying kernel width and storage load. A key finding is the ", "conclusion": "The study reveals a sophisticated self-organization mechanism where kernel-based Hopfield networks adaptively use inter-pattern interactions as a cooperative feedback control system to create a robust energy landscape, enhancing the stability of high-capacity associative memories and offering design principles."}}
{"id": "2511.12544", "categories": ["cs.AR", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12544", "abs": "https://arxiv.org/abs/2511.12544", "authors": ["Mukul Lokhande", "Akash Sankhe", "S. V. Jaya Chand", "Santosh Kumar Vishvakarma"], "title": "FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration", "comment": null, "summary": "The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.", "AI": {"tldr": "FERMI-ML\u662f\u4e00\u79cd\u7528\u4e8eTinyML\u7684\u5185\u5b58\u7247\u4e0aSRAM\u5b8f\uff0c\u53ef\u5728AIoT\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u9ad8\u80fd\u6548\u7684\u63a8\u7406\u3002", "motivation": "AIoT\u8bbe\u5907\u5bf9\u4f4e\u529f\u8017\u3001\u5c0f\u9762\u79efTinyML\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u6700\u5c0f\u5316\u6570\u636e\u79fb\u52a8\u5e76\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u5185\u5b58\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e9T XNOR\u7684RX9T\u4f4d\u5355\u5143\uff0c\u96c6\u6210\u4e865T\u5b58\u50a8\u5355\u5143\u548c4T XNOR\u8ba1\u7b97\u5355\u5143\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u7247\u4e0a\uff08MIS\uff09SRAM\u5b8f\uff0c\u652f\u6301\u53ef\u53d8\u7cbe\u5ea6MAC\u548cCAM\u64cd\u4f5c\u3002\u91c7\u7528C22T\u538b\u7f29\u6811\u7d2f\u52a0\u5668\u5b9e\u73b0\u5bf9\u65701-64\u4f4dMAC\u8ba1\u7b97\u3002", "result": "\u8be54KB\u5b8f\u572865nm\u5de5\u827a\u4e0b\uff0c0.9V\u7535\u538b\u4e0b\u53ef\u8fbe350MHz\uff0c\u541e\u5410\u91cf\u4e3a1.93 TOPS\uff0c\u80fd\u6548\u4e3a364 TOPS/W\u3002\u5728InceptionV4\u548cResNet-18\u4e0a\u4fdd\u6301\u4e86\u8d85\u8fc797.5%\u7684\u6027\u80fd\u3002\u5e76\u4e14\u5b9e\u73b0\u4e86\u7247\u4e0a\u8ba1\u7b97\u548cCAM\u67e5\u627e\u7684\u53cc\u529f\u80fd\uff0c\u652f\u6301Posit-4\u6216FP-4\u7cbe\u5ea6\u3002", "conclusion": "FERMI-ML\u662f\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u91cd\u6784\u3001\u6ce8\u91cd\u80fd\u8017\u7684\u6570\u5b57\u7247\u4e0a\u5185\u5b58\u5b8f\uff0c\u80fd\u591f\u652f\u6301\u6df7\u5408\u7cbe\u5ea6TinyML\u5de5\u4f5c\u8d1f\u8f7d\u3002"}}
{"id": "2511.13056", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13056", "abs": "https://arxiv.org/abs/2511.13056", "authors": ["Xin Huang", "Shengwei Zhou"], "title": "An FPTAS for 7/9-Approximation to Maximin Share Allocations", "comment": "29 pages, 6 figures", "summary": "We present a new algorithm that achieves a $\\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\\frac{7}{9}-\\varepsilon$ approximation in $\\tfrac{1}{\\varepsilon} \\cdot \\mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5728\u5177\u6709\u52a0\u6027\u4f30\u503c\u7684\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u6700\u9ad8\u4fdd\u8bc1\u4efd\u989d\uff08MMS\uff09\u5206\u914d\u4e2d\u5b9e\u73b0\u4e86 7/9 \u7684\u8fd1\u4f3c\u6bd4\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u7684 10/13 \u7684\u8fd1\u4f3c\u6bd4\u3002", "motivation": "\u5728\u5177\u6709\u52a0\u6027\u4f30\u503c\u7684\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u6700\u9ad8\u4fdd\u8bc1\u4efd\u989d\uff08MMS\uff09\u5206\u914d\u4e2d\uff0c\u5c06\u8fd1\u4f3c\u6bd4\u4ece 10/13 \u63d0\u9ad8\u5230 7/9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u8fd8\u5f97\u5230\u4e00\u4e2a FPTAS\u3002", "result": "\u5b9e\u73b0\u4e86 7/9 \u7684\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u4e14\u5f97\u5230\u4e86\u4e00\u4e2a 7/9-\u03b5 \u7684 FPTAS\uff0c\u5176\u8fd0\u884c\u65f6\u95f4\u4e3a 1/\u03b5 * poly(n,m)\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u4f46\u8fd1\u4f3c\u6bd4\u66f4\u9ad8\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u8fd1\u4f3c\u65b9\u6848\u3002"}}
{"id": "2511.13543", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13543", "abs": "https://arxiv.org/abs/2511.13543", "authors": ["Yuning Zhang", "K. W. Wang"], "title": "In-memory phononic learning toward cognitive mechanical intelligence", "comment": "19 pages, 7 figures", "summary": "Modern autonomous systems are driving the critical need for next-generation adaptive materials and structures with embodied intelligence, i.e., the embodiment of memory, perception, learning, and decision-making within the mechanical domain. A fundamental challenge is the seamless and efficient integration of memory with information processing in a physically interpretable way that enables cognitive learning and decision-making under uncertainty. Prevailing paradigms, from intricate logic cascades to black-box morphological computing or physical neural networks, are seriously limited by trade-offs among efficiency, scalability, interpretability, transparency, and reliance on additional electronics. Here, we introduce in-memory phononic learning, a paradigm-shifting framework that unifies nonvolatile mechanical memory with wave-based perception within a phononic metastructure. Our system encodes spatial information into stable structural states as mechanical memory that directly programs its elastic wave-propagation landscape. This memory/wave-dynamics coupling enables effective sensory perception, decomposing complex patterns into informative geometric features through frequency-selective wave localization. Learning is created by optimizing input waveforms to selectively probe these features for memory-pattern classification, with decisions inferred directly from the output wave energy, thereby completing the entire information loop mechanically through an efficient and physically transparent mechanism without hidden architectures or electronics. This work transcends the paradigm of 'materials that compute' to cognitive matter capable of interpreting dynamic environments, paving the way for future intelligent structural-material systems with low power consumption, more direct interaction with surroundings, and enhanced cybersecurity and resilience in harsh conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c in-memory phononic learning\u201d\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u58f0\u5b50\u8d85\u7ed3\u6784\u4e2d\u96c6\u6210\u975e\u6613\u5931\u6027\u673a\u68b0\u8bb0\u5fc6\u548c\u57fa\u4e8e\u6ce2\u7684\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u673a\u68b0\u57df\u5185\u7684\u8bb0\u5fc6\u3001\u611f\u77e5\u3001\u5b66\u4e60\u548c\u51b3\u7b56\uff0c\u65e0\u9700\u989d\u5916\u7535\u5b50\u5143\u4ef6\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u73b0\u4ee3\u81ea\u4e3b\u7cfb\u7edf\u5bf9\u5177\u6709\u5185\u7f6e\u667a\u80fd\uff08\u8bb0\u5fc6\u3001\u611f\u77e5\u3001\u5b66\u4e60\u3001\u51b3\u7b56\uff09\u7684\u81ea\u9002\u5e94\u6750\u6599\u548c\u7ed3\u6784\u7684\u9700\u6c42\uff0c\u514b\u670d\u73b0\u6709\u6280\u672f\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u900f\u660e\u5ea6\u548c\u5bf9\u989d\u5916\u7535\u5b50\u5143\u4ef6\u7684\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201cin-memory phononic learning\u201d\u8303\u5f0f\uff0c\u5c06\u975e\u6613\u5931\u6027\u673a\u68b0\u8bb0\u5fc6\u4e0e\u58f0\u5b50\u8d85\u7ed3\u6784\u4e2d\u7684\u6ce2\u57fa\u611f\u77e5\u76f8\u7ed3\u5408\u3002\u8be5\u7cfb\u7edf\u5c06\u7a7a\u95f4\u4fe1\u606f\u7f16\u7801\u4e3a\u673a\u68b0\u8bb0\u5fc6\u7684\u7a33\u5b9a\u7ed3\u6784\u72b6\u6001\uff0c\u76f4\u63a5\u8c03\u6574\u5176\u5f39\u6027\u6ce2\u4f20\u64ad\u7279\u6027\u3002\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u6ce2\u5f62\u6765\u9009\u62e9\u6027\u5730\u63a2\u6d4b\u8fd9\u4e9b\u7279\u5f81\u4ee5\u8fdb\u884c\u8bb0\u5fc6\u6a21\u5f0f\u5206\u7c7b\uff0c\u5e76\u76f4\u63a5\u4ece\u8f93\u51fa\u6ce2\u80fd\u91cf\u63a8\u65ad\u51b3\u7b56\uff0c\u4ece\u800c\u5728\u673a\u68b0\u5c42\u9762\u5b8c\u6210\u6574\u4e2a\u4fe1\u606f\u5904\u7406\u5faa\u73af\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u9891\u7387\u9009\u62e9\u6027\u6ce2\u5c40\u57df\u5316\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u611f\u5b98\u611f\u77e5\uff0c\u5c06\u590d\u6742\u6a21\u5f0f\u5206\u89e3\u4e3a\u4fe1\u606f\u4e30\u5bcc\u7684\u51e0\u4f55\u7279\u5f81\u3002\u5b66\u4e60\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u6ce2\u5f62\u6765\u5b9e\u73b0\uff0c\u51b3\u7b56\u76f4\u63a5\u4ece\u8f93\u51fa\u6ce2\u80fd\u91cf\u4e2d\u63a8\u65ad\uff0c\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u9690\u85cf\u67b6\u6784\u6216\u7535\u5b50\u5143\u4ef6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7269\u7406\u4e0a\u900f\u660e\u7684\u673a\u5236\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u2018\u8ba1\u7b97\u6750\u6599\u2019\u7684\u8303\u5f0f\u63d0\u5347\u81f3\u80fd\u591f\u89e3\u91ca\u52a8\u6001\u73af\u5883\u7684\u8ba4\u77e5\u7269\u8d28\uff0c\u4e3a\u672a\u6765\u4f4e\u529f\u8017\u3001\u4e0e\u5468\u56f4\u73af\u5883\u66f4\u76f4\u63a5\u7684\u4ea4\u4e92\u3001\u4ee5\u53ca\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u589e\u5f3a\u7684\u7f51\u7edc\u5b89\u5168\u548c\u97e7\u6027\u7684\u667a\u80fd\u7ed3\u6784-\u6750\u6599\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.11985", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11985", "abs": "https://arxiv.org/abs/2511.11985", "authors": ["Yuan Guo", "Wen Chen", "Yanze Zhu", "Zhendong Li", "Qiong Wu", "Kunlun Wang"], "title": "Beamforming for Transmissive RIS Transmitter Enabled Simultaneous Wireless Information and Power Transfer Systems", "comment": null, "summary": "This paper investigates a novel transmissive reconfigurable intelligent surface (TRIS) transceiver-empowered simultaneous wireless information and power transfer (SWIPT) system with multiple information decoding (ID) and energy harvesting (EH) users. Under the considered system model, we formulate an optimization problem that maximizes the sum-rate of all ID users via the design of the TRIS transceiver's active beamforming. The design is constrained by per-antenna power limits at the TRIS transceiver and by the minimum harvested energy demand of all EH users. Due to the non-convexity of the objective function and the energy harvesting constraint, the sum-rate problem is difficult to tackle. To solve this challenging optimization problem, by leveraging the weighted minimum mean squared error (WMMSE) framework and the majorization-minimization (MM) method, we propose a second-order cone programming (SOCP)-based algorithm. Per-element power constraints introduce a large number of constraints, making the problem considerably more difficult. By applying the alternating direction method of multipliers (ADMM) method, we successfully develop an analytical, computationally efficient, and highly parallelizable algorithm to address this challenge. Numerical results are provided to validate the convergence and effectiveness of the proposed algorithms. Furthermore, the low-complexity algorithm significantly reduces computational complexity without performance degradation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e TRIS \u6536\u53d1\u5668\u548c SWIPT \u7684\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7 WMMSE \u548c MM \u65b9\u6cd5\u4f18\u5316\u548c\u89e3\u51b3\u4e86\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\uff0c\u6700\u7ec8\u901a\u8fc7 ADMM \u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u3001\u53ef\u5e76\u884c\u5316\u7684\u7b97\u6cd5\uff0c\u5728\u6570\u503c\u7ed3\u679c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6536\u655b\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u65e8\u5728\u901a\u8fc7\u8bbe\u8ba1 TRIS \u6536\u53d1\u5668\u7684\u6709\u6e90\u6ce2\u675f\u8d4b\u5f62\u6765\u6700\u5927\u5316\u6240\u6709 ID \u7528\u6237\u7684\u603b\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3 TRIS \u6536\u53d1\u5668\u7684\u5355\u5929\u7ebf\u529f\u7387\u9650\u5236\u4ee5\u53ca\u6240\u6709 EH \u7528\u6237\u7684\u6700\u5c0f\u80fd\u91cf\u6536\u96c6\u9700\u6c42\u3002\u7531\u4e8e\u76ee\u6807\u51fd\u6570\u548c\u80fd\u91cf\u6536\u96c6\u7ea6\u675f\u7684\u975e\u51f8\u6027\uff0c\u8be5\u95ee\u9898\u96be\u4ee5\u76f4\u63a5\u6c42\u89e3\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5c06\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08WMMSE\uff09\u6846\u67b6\u4e0e majorization-minimization\uff08MM\uff09\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6bcf\u6839\u5929\u7ebf\u529f\u7387\u7ea6\u675f\u5f15\u5165\u4e86\u5927\u91cf\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u7814\u7a76\u4eba\u5458\u91c7\u7528\u4e86\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u6765\u5f00\u53d1\u4e00\u79cd\u89e3\u6790\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u9ad8\u5ea6\u53ef\u5e76\u884c\u7684\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e TRIS \u6536\u53d1\u5668\u548c SWIPT \u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7 WMMSE\u3001MM \u548c ADMM \u7b49\u5148\u8fdb\u7b97\u6cd5\u89e3\u51b3\u4e86\u590d\u6742\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u5e76\u884c\u6027\u3002"}}
{"id": "2511.11603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11603", "abs": "https://arxiv.org/abs/2511.11603", "authors": ["Deep Bodra", "Sushil Khairnar"], "title": "Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review", "comment": null, "summary": "Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u6bd4\u8f83\u4e86\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u4e91\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u4e91\u8d44\u6e90\u5206\u914d\u5bf9\u591a\u76ee\u6807\u4f18\u5316\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8610\u79cd\u7b97\u6cd5\uff0c\u6db5\u76d6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u8bc4\u4f30\u7684\u7b97\u6cd5\u5728\u7f29\u77ed\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u4f18\u5316\u6210\u672c\u548c\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6df7\u5408\u67b6\u6784\u7684\u6027\u80fd\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u4e91\u8d44\u6e90\u5206\u914d\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\uff0c\u8fd9\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u8d44\u6e90\u5206\u914d\u7b56\u7565\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2511.13009", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13009", "abs": "https://arxiv.org/abs/2511.13009", "authors": ["Yong Liu", "Keyang Ye", "Tianjia Shao", "Kun Zhou"], "title": "TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting", "comment": "15 pages, 12 figures", "summary": "We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.", "AI": {"tldr": "TR-Gaussians\u662f\u4e00\u79cd\u65b0\u76843D\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u6e32\u67d3\u5ba4\u5185\u573a\u666f\u4e2d\u7684\u5e73\u9762\u900f\u5c04\u548c\u53cd\u5c04\u3002", "motivation": "\u5ba4\u5185\u573a\u666f\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5e73\u9762\u900f\u5c04\u548c\u53cd\u5c04\u7684\u6e32\u67d3\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u89c6\u70b9\u76f8\u5173\u7684\u53cd\u5c04\u5f3a\u5ea6\u548c\u590d\u6742\u5916\u89c2\u6548\u679c\u7684\u65b9\u6cd5\u3002", "method": "TR-Gaussians\u5c063D\u9ad8\u65af\u4e0e\u53ef\u5b66\u4e60\u7684\u53cd\u5c04\u5e73\u9762\u76f8\u7ed3\u5408\uff0c\u663e\u5f0f\u5730\u5efa\u6a21\u73bb\u7483\u5e73\u9762\u3002\u900f\u5c04\u548c\u53cd\u5c04\u5206\u91cf\u5206\u522b\u75313D\u9ad8\u65af\u548c\u955c\u50cf\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u6839\u636e\u57fa\u4e8e\u83f2\u6d85\u5c14\u6548\u5e94\u7684\u89c6\u70b9\u76f8\u5173\u52a0\u6743\u65b9\u6848\u8fdb\u884c\u6df7\u5408\u3002\u91c7\u7528\u591a\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u8fdb\u884c\u4f18\u5316\u3002", "result": "TR-Gaussians\u80fd\u591f\u5728\u5305\u542b\u5e73\u9762\u900f\u5c04\u548c\u53cd\u5c04\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u7684\u65b0\u89c6\u56fe\u5408\u6210\u3002", "conclusion": "TR-Gaussians\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5fe0\u5b9e\u5730\u5408\u6210\u590d\u6742\u7684\u5916\u89c2\u6548\u679c\uff0c\u5e76\u5728\u4e0d\u540c\u89c6\u70b9\u4e0b\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2511.11917", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11917", "abs": "https://arxiv.org/abs/2511.11917", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing", "comment": "6 pages, 5 figures, this is the preprint version of the paper accepted for publication at ATS 2025", "summary": "This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684UGLMS\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901fSAR ADC\u7684\u7ebf\u6027\u5ea6\u6d4b\u8bd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u5b9e\u65f6\u751f\u4ea7\u73af\u5883\u4e2d\u66f4\u6709\u6548\u5730\u6d4b\u8bd5SAR ADC\u7684\u7ebf\u6027\u5ea6\uff0c\u9700\u8981\u4e00\u79cd\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u7cbe\u786e\u7684\u6d4b\u8bd5\u7b56\u7565\u3002", "method": "\u8be5\u7814\u7a76\u5728UGLMS\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u5305\u62ec\uff1a1. \u4f7f\u7528\u79e9-1 EKF\u66f4\u65b0\u6765\u52a0\u901f\u8ba1\u7b97\u30022. \u5f15\u5165\u6d4b\u91cf\u5bf9\u9f50\u7684\u534f\u65b9\u5dee\u81a8\u80c0\u7b56\u7565\u6765\u52a0\u901f\u6536\u655b\u30023. \u6269\u5c55\u4e86\u9759\u6001\u5931\u914d\u6a21\u578b\uff0c\u52a0\u5165\u4e86\u4f4e\u9636\u591a\u9879\u5f0f\u4ee5\u6355\u6349\u975e\u7ebf\u6027\u30024. \u91c7\u7528\u57fa\u4e8e\u8ff9\u7684\u7ec8\u6b62\u6761\u4ef6\u6765\u4f18\u5316\u6d4b\u8bd5\u957f\u5ea6\u3002", "result": "\u589e\u5f3a\u7684UGLMS\u65b9\u6cd5\u572816\u4f4d\u548c18\u4f4dADC\u6d4b\u8bd5\u4e2d\u663e\u8457\u7f29\u77ed\u4e86\u6d4b\u8bd5\u65f6\u95f4\uff08\u5206\u522b\u4e3a36\u6beb\u79d2\u548c70\u6beb\u79d2\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002\u5bf9\u4e8e16\u4f4dADC\uff0c\u5176\u6d4b\u8bd5\u901f\u5ea6\u6bd4\u4ee5\u524d\u5feb8\u500d\u3002", "conclusion": "\u589e\u5f3a\u7684UGLMS\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e00\u7cfb\u5217\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86SAR ADC\u5b9e\u65f6\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u3002"}}
{"id": "2511.11850", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11850", "abs": "https://arxiv.org/abs/2511.11850", "authors": ["Ali Mashhadireza", "Ali Sadighi"], "title": "Neural Network-Augmented Iterative Learning Control for Friction Compensation of Motion Control Systems with Varying Disturbances", "comment": "6 pages, 10 figures, conference paper to be submitted to IEEE Conference on Control Systems", "summary": "This paper proposes a robust control strategy that integrates Iterative Learning Control (ILC) with a simple lateral neural network to enhance the trajectory tracking performance of a linear Lorentz force actuator under friction and model uncertainties. The ILC compensates for nonlinear friction effects, while the neural network estimates the nonlinear ILC effort for varying reference commands. By dynamically adjusting the ILC effort, the method adapts to time-varying friction, reduces errors at reference changes, and accelerates convergence. Compared to previous approaches using complex neural networks, this method simplifies online training and implementation, making it practical for real-time applications. Experimental results confirm its effectiveness in achieving precise tracking across multiple tasks with different reference trajectories.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u548c\u7b80\u5355\u6a2a\u5411\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8\u7ebf\u6027\u6d1b\u4f26\u5179\u529b\u81f4\u52a8\u5668\u5728\u6469\u64e6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5728\u5b58\u5728\u6469\u64e6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u7ebf\u6027\u6d1b\u4f26\u5179\u529b\u81f4\u52a8\u5668\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u5c06\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u4e0e\u7b80\u5355\u7684\u6a2a\u5411\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\uff0cILC\u8865\u507f\u975e\u7ebf\u6027\u6469\u64e6\u6548\u5e94\uff0c\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u53d8\u5316\u7684\u53c2\u8003\u6307\u4ee4\u7684\u975e\u7ebf\u6027ILC\u8865\u507f\u3002", "result": "\u901a\u8fc7\u52a8\u6001\u8c03\u6574ILC\u8865\u507f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u65f6\u53d8\u6469\u64e6\uff0c\u51cf\u5c11\u53c2\u8003\u6307\u4ee4\u53d8\u5316\u65f6\u7684\u8bef\u5dee\uff0c\u5e76\u52a0\u901f\u6536\u655b\u3002\u4e0e\u4f7f\u7528\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u5148\u524d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u5728\u7ebf\u8bad\u7ec3\u548c\u5b9e\u73b0\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u4e0d\u540c\u53c2\u8003\u8f68\u8ff9\u7684\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12168", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12168", "abs": "https://arxiv.org/abs/2511.12168", "authors": ["Sayantan Pramanik", "M Girish Chandra"], "title": "Stochastic Shadow Descent: Training Parametrized Quantum Circuits with Shadows of Gradients", "comment": "Under review at ICASSP 2026", "summary": "In this paper, we focus on the task of optimizing the parameters in Parametrized Quantum Circuits (PQCs). While popular algorithms, such as Simultaneous Perturbation Stochastic Approximation (SPSA), limit the number of circuit-execution to two per iteration, irrespective of the number of parameters in the circuit, they have their own challenges. These methods use central-differences to calculate biased estimates of directional derivatives. We show, both theoretically and numerically, that this may lead to instabilities in \\emph{training} the PQCs. To remedy this, we propose Stochastic Shadow Descent (\\texttt{SSD}), which uses random-projections (or \\emph{shadows}) of the gradient to update the parameters iteratively. We eliminate the bias in directional derivatives by employing the Parameter-Shift Rule, along with techniques from Quantum Signal Processing, to construct a quantum circuit that parsimoniously computes \\emph{unbiased estimates} of directional derivatives. Finally, we prove the convergence of the \\texttt{SSD} algorithm, provide worst-case bounds on the number of iterations, and numerically demonstrate its efficacy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08SSD\uff09\u7684\u65b0\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQC\uff09\u3002\u4e0eSPSA\u7b49\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0cSSD\u901a\u8fc7\u4f7f\u7528\u53c2\u6570\u504f\u79fb\u89c4\u5219\u548c\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8ba1\u7b97\u68af\u5ea6\uff0c\u907f\u514d\u4e86\u56e0\u4e2d\u5fc3\u5dee\u5206\u4f30\u8ba1\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709PQC\u4f18\u5316\u7b97\u6cd5\uff08\u5982SPSA\uff09\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4ec5\u6267\u884c\u4e24\u6b21\u7535\u8def\uff0c\u65e0\u8bba\u53c2\u6570\u6570\u91cf\u591a\u5c11\uff0c\u4e14\u4f7f\u7528\u4e2d\u5fc3\u5dee\u5206\u6cd5\u8ba1\u7b97\u6709\u504f\u68af\u5ea6\u4f30\u8ba1\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08SSD\uff09\u7684\u65b0\u7b97\u6cd5\u3002SSD\u5229\u7528\u53c2\u6570\u504f\u79fb\u89c4\u5219\u548c\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u6784\u5efa\u91cf\u5b50\u7535\u8def\u6765\u8ba1\u7b97\u68af\u5ea6\u65e0\u504f\u4f30\u8ba1\uff0c\u4ece\u800c\u8fed\u4ee3\u66f4\u65b0PQC\u53c2\u6570\u3002", "result": "\u7406\u8bba\u548c\u6570\u503c\u4e0a\u8bc1\u660e\u4e86SSD\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8fed\u4ee3\u6b21\u6570\u754c\u9650\uff0c\u540c\u65f6\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SSD\u7b97\u6cd5\u901a\u8fc7\u8ba1\u7b97\u68af\u5ea6\u65e0\u504f\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PQC\u4f18\u5316\u7b97\u6cd5\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u7406\u8bba\u4e0a\u7684\u6536\u655b\u6027\u548c\u5b9e\u9645\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13605", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13605", "abs": "https://arxiv.org/abs/2511.13605", "authors": ["Niv Buchbinder", "Joseph", "Naor", "David Wajc"], "title": "Chasing Submodular Objectives, and Submodular Maximization via Cutting Planes", "comment": null, "summary": "We introduce the \\emph{submodular objectives chasing problem}, which generalizes many natural and previously-studied problems: a sequence of constrained submodular maximization problems is revealed over time, with both the objective and available ground set changing at each step. The goal is to maintain solutions of high approximation and low total \\emph{recourse} (number of changes), compared with exact offline algorithms for the same input sequence. For the central cardinality constraint and partition matroid constraints we provide polynomial-time algorithms achieving both optimal $(1-1/e-\u03b5)$-approximation and optimal competitive recourse for \\emph{any} constant-approximation.\n  Key to our algorithm's polynomial time, and of possible independent interest, is a new meta-algorithm for $(1-1/e-\u03b5)$-approximately maximizing the multilinear extension under general constraints, which we call {\\em approximate-or-separate}. Our algorithm relies on an improvement of the round-and-separate method [Gupta-Levin SODA'20], inspired by an earlier proof by [Vondr\u00e1k, PhD~Thesis'07]. The algorithm, whose guarantees are similar to the influential {\\em continuous greedy} algorithm [Calinescu-Chekuri-P\u00e1l-Vondr\u00e1k SICOMP'11], can use any cutting plane method and separation oracle for the constraints. This allows us to introduce cutting plane methods, used for exact unconstrained submodular minimization since the '80s [Gr\u00f6tschel/Lov\u00e1sz/Schrijver Combinatorica'81], as a useful method for (optimal approximate) constrained submodular maximization. We show further applications of this approach to static algorithms with curvature-sensitive approximation, and to communication complexity protocols.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u201c\u5b50\u6a21\u76ee\u6807\u8ffd\u9010\u95ee\u9898\u201d\uff0c\u65e8\u5728\u52a8\u6001\u5e8f\u5217\u5b50\u6a21\u6700\u5927\u5316\u95ee\u9898\u4e2d\u5e73\u8861\u89e3\u7684\u8fd1\u4f3c\u5ea6\u548c\u4fee\u6539\u6210\u672c\uff0c\u5e76\u9488\u5bf9\u57fa\u6570\u7ea6\u675f\u548c\u5206\u5272\u62df\u9635\u7ea6\u675f\u63d0\u51fa\u4e86\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u5e8f\u5217\u5b50\u6a21\u6700\u5927\u5316\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u89e3\u7684\u8fd1\u4f3c\u5ea6\u540c\u65f6\u6700\u5c0f\u5316\u4fee\u6539\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5143\u7b97\u6cd5\u201c\u8fd1\u4f3c\u6216\u5206\u79bb\u201d\uff0c\u7528\u4e8e\u5728\u4e00\u822c\u7ea6\u675f\u4e0b\u8fd1\u4f3c\u6700\u5927\u5316\u591a\u7ebf\u6027\u6269\u5c55\uff0c\u8be5\u7b97\u6cd5\u6539\u8fdb\u4e86\u201c\u8f6e\u5206\u79bb\u201d\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u5207\u5272\u5e73\u9762\u6cd5\u548c\u5206\u79bb\u9884\u8a00\u673a\u3002", "result": "\u9488\u5bf9\u57fa\u6570\u7ea6\u675f\u548c\u5206\u5272\u62df\u9635\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u540c\u65f6\u8fbe\u5230\u6700\u4f18 $(1-1/e-\u03b5)$-\u8fd1\u4f3c\u5ea6\u548c\u6700\u4f18\u7ade\u4e89\u6027\u8ffd\u7d22\u6210\u672c\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u52a8\u6001\u5e8f\u5217\u5b50\u6a21\u6700\u5927\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5207\u5272\u5e73\u9762\u6cd5\u5728\u7ea6\u675f\u5b50\u6a21\u6700\u5927\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u4e3a\u9759\u6001\u7b97\u6cd5\u548c\u901a\u4fe1\u590d\u6742\u5ea6\u534f\u8bae\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7684\u5e94\u7528\u3002"}}
{"id": "2511.12808", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12808", "abs": "https://arxiv.org/abs/2511.12808", "authors": ["Omar Adalat", "Francesco Belardinelli"], "title": "Expressive Temporal Specifications for Reward Monitoring", "comment": null, "summary": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.", "AI": {"tldr": "\u4f7f\u7528 LTLf \u7efc\u5408\u91cf\u5316\u5956\u52b1\u76d1\u63a7\u5668\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6536\u655b\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8bbe\u8ba1\u4fe1\u606f\u4e30\u5bcc\u4e14\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u662f\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\u7684\u5173\u952e\u6311\u6218\u3002\u7a00\u758f\u5956\u52b1\u5728\u957f horizonte \u51b3\u7b56\u4e2d\u5c24\u5176\u666e\u904d\u3002", "method": "\u5229\u7528\u91cf\u5316\u7ebf\u6027\u65f6\u95f4\u903b\u8f91\uff08LTLf[F]\uff09\u6765\u7efc\u5408\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u8be5\u76d1\u63a7\u5668\u4e3a\u8fd0\u884c\u65f6\u53ef\u89c2\u5bdf\u7684\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u7684\u5956\u52b1\u6d41\u3002\u8be5\u6846\u67b6\u72ec\u7acb\u4e8e\u7b97\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u72b6\u6001\u6807\u6ce8\u51fd\u6570\uff0c\u5e76\u80fd\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u5c5e\u6027\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u5e03\u5c14\u76d1\u63a7\u5668\u76f8\u6bd4\uff0c\u91cf\u5316\u76d1\u63a7\u5668\u5728\u6700\u5927\u5316\u4efb\u52a1\u5b8c\u6210\u7684\u91cf\u5316\u5ea6\u91cf\u548c\u51cf\u5c11\u6536\u655b\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5728\u67d0\u4e9b\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u7ec6\u81f4\u7684\u53cd\u9988\uff0c\u8fd9\u4e9b\u91cf\u5316\u5956\u52b1\u76d1\u63a7\u5668\u80fd\u591f\u6709\u6548\u5730\u6307\u5bfc\u667a\u80fd\u4f53\u8d70\u5411\u6700\u4f18\u884c\u4e3a\uff0c\u5e76\u7f13\u89e3\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2511.13445", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13445", "abs": "https://arxiv.org/abs/2511.13445", "authors": ["Clemens Anzinger", "Jiehua Chen", "Christian Hatschka", "Manuel Sorge", "Alexander Temper"], "title": "How Hard is it to Explain Preferences Using Few Boolean Attributes?", "comment": "Accepted at AAAI 2026", "summary": "We study the computational complexity of explaining preference data through Boolean attribute models (BAMs), motivated by extensive research involving attribute models and their promise in understanding preference structure and enabling more efficient decision-making processes. In a BAM, each alternative has a subset of Boolean attributes, each voter cares about a subset of attributes, and voters prefer alternatives with more of their desired attributes. In the BAM problem, we are given a preference profile and a number k, and want to know whether there is a Boolean k-attribute model explaining the profile.\n  We establish a complexity dichotomy for the number of attributes k: BAM is linear-time solvable for $k \\le 2$ but NP-complete for $k \\ge 3$. The problem remains hard even when preference orders have length two. On the positive side, BAM becomes fixed-parameter tractable when parameterized by the number of alternatives m. For the special case of two voters, we provide a linear-time algorithm.\n  We also analyze variants where partial information is given: When voter preferences over attributes are known (BAM WITH CARES) or when alternative attributes are specified (BAM WITH HAS), we show that for most parameters BAM WITH CARES is more difficult whereas BAM WITH HAS is more tractable except for being NP-hard even for one voter.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5e03\u5c14\u5c5e\u6027\u6a21\u578b\uff08BAM\uff09\u89e3\u91ca\u504f\u597d\u6570\u636e\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u73b0\u6709\u7684\u5173\u4e8e\u5c5e\u6027\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u89e3\u504f\u597d\u7ed3\u6784\u548c\u652f\u6301\u66f4\u6709\u6548\u7684\u51b3\u7b56\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u5728BAM\u4e2d\uff0c\u6bcf\u4e2a\u5907\u9009\u9879\u90fd\u6709\u4e00\u7ec4\u5e03\u5c14\u5c5e\u6027\uff0c\u6bcf\u4e2a\u9009\u6c11\u90fd\u5173\u5fc3\u4e00\u7ec4\u5c5e\u6027\uff0c\u5e76\u4e14\u9009\u6c11\u504f\u597d\u5177\u6709\u66f4\u591a\u4ed6\u4eec\u6240\u9700\u5c5e\u6027\u7684\u5907\u9009\u9879\u3002BAM\u95ee\u9898\u7684\u76ee\u6807\u662f\uff0c\u5728\u7ed9\u5b9a\u504f\u597d\u5217\u8868\u548c\u6570\u5b57k\u7684\u60c5\u51b5\u4e0b\uff0c\u5224\u65ad\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u5e03\u5c14k-\u5c5e\u6027\u6a21\u578b\u53ef\u4ee5\u89e3\u91ca\u8be5\u504f\u597d\u5217\u8868\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5c5e\u6027\u6570\u91cfk\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4e8c\u5206\u6cd5\uff1a\u5f53k\u22642\u65f6\uff0cBAM\u95ee\u9898\u53ef\u4ee5\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u89e3\u51b3\uff1b\u5f53k\u22653\u65f6\uff0c\u8be5\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\u3002\u5373\u4f7f\u5728\u504f\u597d\u987a\u5e8f\u957f\u5ea6\u4e3a2\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u95ee\u9898\u4ecd\u7136\u662f\u56f0\u96be\u7684\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5f53\u53c2\u6570\u4e3a\u5907\u9009\u9879\u6570\u91cfm\u65f6\uff0cBAM\u95ee\u9898\u662f\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7684\u3002\u5bf9\u4e8e\u53ea\u6709\u4e24\u4e2a\u9009\u6c11\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u5305\u542b\u90e8\u5206\u4fe1\u606f\u7684\u53d8\u4f53\uff1a\u5728\u5df2\u77e5\u9009\u6c11\u504f\u597d\u5c5e\u6027\uff08BAM WITH CARES\uff09\u6216\u5df2\u6307\u5b9a\u5907\u9009\u9879\u5c5e\u6027\uff08BAM WITH HAS\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\uff0cBAM WITH CARES\u6bd4BAM WITH HAS\u66f4\u96be\u5904\u7406\uff0c\u800cBAM WITH HAS\u66f4\u6613\u4e8e\u5904\u7406\uff0c\u4f46\u5373\u4f7f\u53ea\u6709\u4e00\u4e2a\u9009\u6c11\uff0c\u8be5\u95ee\u9898\u4e5f\u662fNP\u96be\u7684\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u4e86\u5173\u4e8eBAM\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u7684\u660e\u786e\u7ed3\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c\u53c2\u6570\u548c\u53d8\u4f53\u7684\u6709\u6548\u7b97\u6cd5\u548c\u56f0\u96be\u6027\u7ed3\u679c\u3002"}}
{"id": "2511.12427", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.12427", "abs": "https://arxiv.org/abs/2511.12427", "authors": ["Jie Pan", "Huanhuan Wang", "Lin Zou", "Xiaoyu Wang", "Lihao Zhang", "Xueyan Dong", "Haibo Xie", "Yi Ding", "Yuze Zhang", "Takashi Taniguchi", "Kenji Watanabe", "Shuxi Wang", "Zhe Wang"], "title": "Topological Valley Transport in Bilayer Graphene Induced by Interlayer Sliding", "comment": "19 pages", "summary": "Interlayer sliding, together with twist angle, is a crucial parameter that defines the atomic registry and thus determines the properties of two-dimensional (2D) material homobilayers. Here, we theoretically demonstrate that controlled interlayer sliding in bilayer graphene induces Berry curvature reversals, leading to topological states confined within a one-dimensional moir\u00e9 channel. We experimentally realize interlayer sliding by bending the bilayer graphene geometry across a nanoridge. Systematic electronic transport measurements reveal topological valley transport when the Fermi energy resides within the band gap, consistent with theoretical predictions of eight topological channels. Our findings establish interlayer sliding as a powerful tool for tuning the electronic properties of bilayer graphene and underscore its potential for broad application across 2D material systems.", "AI": {"tldr": "Bilayer graphene's interlayer sliding, controlled by bending, creates topological states and valley transport, which can be used to tune electronic properties.", "motivation": "Interlayer sliding and twist angle are crucial for 2D material homobilayer properties. This paper explores the effect of interlayer sliding on bilayer graphene.", "method": "Theoretically demonstrated interlayer sliding-induced Berry curvature reversals and topological states. Experimentally realized sliding by bending bilayer graphene across a nanoridge and performed electronic transport measurements.", "result": "Observed topological valley transport when the Fermi energy is in the band gap, consistent with theoretical predictions of eight topological channels.", "conclusion": "Interlayer sliding is a powerful tool for tuning bilayer graphene's electronic properties and has potential for other 2D material systems."}}
{"id": "2511.12260", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12260", "abs": "https://arxiv.org/abs/2511.12260", "authors": ["Jonas Elsborg", "Arghya Bhowmik"], "title": "Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles", "comment": "15 pages, 7 figures, 1 table", "summary": "We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5bfb\u627e\u53cc\u91d1\u5c5e\u7eb3\u7c73\u9897\u7c92\uff08NPs\uff09\u4e2d\u6700\u4f73\u5143\u7d20\u6392\u5e8f\u7684\u95ee\u9898\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2aRL\u4ee3\u7406\uff0c\u5229\u7528NPs\u7684\u51e0\u4f55\u56fe\u8868\u793a\u6765\u5b66\u4e60\u6267\u884c\u8fd9\u79cd\u5168\u5c40\u4f18\u5316\u3002", "motivation": "\u5c06\u5bfb\u627e\u6700\u4f73\u5143\u7d20\u6392\u5e8f\u7684\u95ee\u9898\u6784\u5efa\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u53cc\u91d1\u5c5e\u7eb3\u7c73\u9897\u7c92\u7684\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u56fe\u8868\u793a\u548c\u57fa\u4e8e\u539f\u5b50\u4ea4\u6362\u52a8\u4f5c\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6765\u4f18\u5316\u7eb3\u7c73\u9897\u7c92\u7684\u5143\u7d20\u6392\u5e8f\u3002", "result": "\u8bad\u7ec3\u7684RL\u4ee3\u7406\u80fd\u591f\u53d1\u73b0\u5148\u524d\u5df2\u77e5\u7684\u91d1\u94f6\u7eb3\u7c73\u9897\u7c92\uff08AgXAu309-X\uff09\u57fa\u6001\u7ed3\u6784\uff0c\u5e76\u4e14\u5bf9\u4e0d\u540c\u7684\u521d\u59cb\u6784\u578b\u5177\u6709\u9c81\u68d2\u6027\u3002\u8be5\u7b56\u7565\u8fd8\u53ef\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u5c3a\u5bf8\u7684\u7eb3\u7c73\u9897\u7c92\uff0c\u4f46\u5728\u591a\u7ec4\u5206\u5408\u91d1\u4e2d\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u7b49\u53d8\u56fe\u7f16\u7801\u53ef\u4ee5\u6709\u6548\u5730\u5728\u7eb3\u7c73\u9897\u7c92\u5c3a\u5ea6\u4e0a\u5bfc\u822a\u7ec4\u5408\u6392\u5e8f\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u53ef\u8fc1\u79fb\u7684\u4f18\u5316\u7b56\u7565\uff0c\u6709\u671b\u5b9e\u73b0\u8de8\u7ec4\u5206\u7684\u6cdb\u5316\u5e76\u964d\u4f4e\u91cd\u590d\u641c\u7d22\u6210\u672c\u3002"}}
{"id": "2511.11716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11716", "abs": "https://arxiv.org/abs/2511.11716", "authors": ["Sudhakar Sah", "Nikhil Chabbra", "Matthieu Durnerin"], "title": "CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition", "comment": "11 pages, 6 figures", "summary": "Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.", "AI": {"tldr": "CNN\u6a21\u578b\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0cCompressNAS\u901a\u8fc7\u5168\u5c40\u641c\u7d22\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\u7684\u79e9\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u3002", "motivation": "\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u56e0\u5176\u65e5\u76ca\u589e\u957f\u7684\u5c3a\u5bf8\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u5728\u5fae\u63a7\u5236\u5668\uff08MCU\uff09\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5355\u5143\uff08NPU\uff09\u4e0a\u7684\u90e8\u7f72\u8d8a\u6765\u8d8a\u56f0\u96be\u3002", "method": "CompressNAS\u662f\u4e00\u4e2a\u53d7MicroNAS\u542f\u53d1\u7684\u6846\u67b6\uff0c\u5c06\u79e9\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u5168\u5c40\u641c\u7d22\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5feb\u901f\u51c6\u786e\u7387\u4f30\u8ba1\u5668\u6765\u8bc4\u4f30\u5019\u9009\u5206\u89e3\uff0c\u4ece\u800c\u5728\u5185\u5b58\u548c\u51c6\u786e\u7387\u7684\u7ea6\u675f\u4e0b\u8fdb\u884c\u9ad8\u6548\u7684\u79e9\u63a2\u7d22\u3002", "result": "\u5728ImageNet\u4e0a\uff0cCompressNAS\u5c06ResNet-18\u538b\u7f29\u4e868\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52304%\uff1b\u5728COCO\u4e0a\uff0c\u5c06YOLOv5s\u538b\u7f29\u4e862\u500d\uff0c\u51c6\u786e\u7387\u65e0\u4e0b\u964d\uff1b\u5c06YOLOv5n\u538b\u7f29\u4e862\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d2.5%\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u538b\u7f29\u6a21\u578b\u7cfb\u5217STResNet\u3002", "conclusion": "CompressNAS\u80fd\u591f\u6709\u6548\u5730\u538b\u7f29CNN\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u51c6\u786e\u7387\u635f\u5931\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002"}}
{"id": "2511.12664", "categories": ["quant-ph", "cs.ET", "cs.SC"], "pdf": "https://arxiv.org/pdf/2511.12664", "abs": "https://arxiv.org/abs/2511.12664", "authors": ["Fabio Cumbo", "Rui-Hao Li", "Bryan Raubenolt", "Jayadev Joshi", "Abu Kaisar Mohammad Masum", "Sercan Aygun", "Daniel Blankenberg"], "title": "Quantum Hyperdimensional Computing: a foundational paradigm for quantum neuromorphic architectures", "comment": "44 pages, 6 figures", "summary": "A significant challenge in quantum computing (QC) is developing learning models that truly align with quantum principles, as many current approaches are complex adaptations of classical frameworks. In this work, we introduce Quantum Hyperdimensional Computing (QHDC), a fundamentally new paradigm. We demonstrate that the core operations of its classical counterpart, Hyperdimensional Computing (HDC), a brain-inspired model, map with remarkable elegance and direct correspondence onto the native operations of a QC. This suggests HDC is exceptionally well-suited for a quantum-native implementation. We establish a direct, resource-efficient mapping: (i) hypervectors are mapped to quantum states, (ii) the bundling operation is implemented as a quantum-native averaging process using a Linear Combination of Unitaries (LCU) and Oblivious Amplitude Amplification (OAA), (iii) the binding operation is realized via quantum phase oracles, (iv) the permutation operation is implemented using the Quantum Fourier Transform (QFT), and (v) vector similarity is calculated using quantum state fidelity measurements based on the Hadamard Test. We present the first-ever implementation of this framework, validated through symbolic analogical reasoning and supervised classification tasks. The viability of QHDC is rigorously assessed via a comparative analysis of results from classical computation, ideal quantum simulation, and execution of a 156-qubit IBM Heron r3 quantum processor. Our results validate the proposed mappings and demonstrate the versatility of the framework, establishing QHDC as a physically realizable technology. This work lays the foundation for a new class of quantum neuromorphic algorithms and opens a promising avenue for tackling complex cognitive and biomedical problems intractable for classical systems.", "AI": {"tldr": "QHDC\u662f\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u8ba1\u7b97\u8303\u5f0f\uff0c\u5b83\u5c06HDC\u7684\u7c7b\u8111\u6a21\u578b\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u672c\u5730\u64cd\u4f5c\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u91cf\u5b50\u6001\u3001LCU\u3001OAA\u3001\u91cf\u5b50\u76f8\u4f4d\u9884\u8a00\u3001QFT\u548cHadamard\u6d4b\u8bd5\u7b49\u64cd\u4f5c\u5b9e\u73b0HDC\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u5e76\u5728\u7b26\u53f7\u7c7b\u6bd4\u63a8\u7406\u548c\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u8bb8\u591a\u73b0\u6709\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u662f\u7ecf\u5178\u6846\u67b6\u7684\u590d\u6742\u6539\u7f16\uff0c\u4e0e\u91cf\u5b50\u539f\u7406\u4e0d\u5b8c\u5168\u5951\u5408\u3002HDC\u6a21\u578b\u5177\u6709\u7b80\u6d01\u7684\u7c7b\u8111\u673a\u5236\uff0c\u9002\u5408\u91cf\u5b50\u672c\u5730\u5b9e\u73b0\u3002", "method": "\u5c06HDC\u4e2d\u7684\u8d85\u5411\u91cf\u6620\u5c04\u5230\u91cf\u5b50\u6001\uff1b\u5c06\u6346\u7ed1\u64cd\u4f5c\u5b9e\u73b0\u4e3a\u57fa\u4e8eLCU\u548cOAA\u7684\u91cf\u5b50\u5e73\u5747\u8fc7\u7a0b\uff1b\u5c06\u7ed1\u5b9a\u64cd\u4f5c\u901a\u8fc7\u91cf\u5b50\u76f8\u4f4d\u9884\u8a00\u5b9e\u73b0\uff1b\u5c06\u7f6e\u6362\u64cd\u4f5c\u901a\u8fc7QFT\u5b9e\u73b0\uff1b\u5c06\u5411\u91cf\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u901a\u8fc7\u57fa\u4e8eHadamard\u6d4b\u8bd5\u7684\u91cf\u5b50\u6001\u4fdd\u771f\u5ea6\u6d4b\u91cf\u5b9e\u73b0\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86QHDC\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u7c7b\u6bd4\u63a8\u7406\u548c\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u4e0e\u7ecf\u5178\u8ba1\u7b97\u548c\u7406\u60f3\u91cf\u5b50\u6a21\u62df\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u5728156\u4f4dIBM Heron r3\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9645\u6267\u884c\u3002", "conclusion": "QHDC\u662f\u4e00\u79cd\u7269\u7406\u4e0a\u53ef\u5b9e\u73b0\u7684\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u795e\u7ecf\u5f62\u6001\u7b97\u6cd5\u7c7b\u522b\uff0c\u5e76\u4e3a\u89e3\u51b3\u7ecf\u5178\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u7684\u590d\u6742\u8ba4\u77e5\u548c\u751f\u7269\u533b\u5b66\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.13080", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.13080", "abs": "https://arxiv.org/abs/2511.13080", "authors": ["Steven Landers", "Benjamin Marsh"], "title": "MEV in Multiple Concurrent Proposer Blockchains", "comment": null, "summary": "We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \\(M(\u03c4)\\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders.", "AI": {"tldr": "\u591a\u63d0\u8bae\u8005\u533a\u5757\u94fe\u4e2d\u7684MEV\u5206\u6790\uff0c\u8003\u8651\u4e86\u5e76\u53d1\u6027\u5e26\u6765\u7684\u65b0MEV\u6e20\u9053\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u5206\u6790\u5728\u591a\u4e2a\u533a\u5757\u5728\u6700\u7ec8\u6267\u884c\u987a\u5e8f\u786e\u5b9a\u4e4b\u524d\u53d8\u5f97\u6570\u636e\u53ef\u7528\u7684\u591a\u63d0\u8bae\u8005\u533a\u5757\u94fe\u4e2d\u7684\u6700\u5927\u53ef\u63d0\u53d6\u4ef7\u503c\uff08MEV\uff09\u3002", "method": "\u5f00\u53d1\u4e86\u5ef6\u8fdf\u548c\u5305\u542b\u7684\u98ce\u9669\u5f52\u4e00\u5316\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u5ef6\u8fdf\u5305\u7edc\u7ebfM(\u03c4)\uff0c\u5e76\u5bf9\u5ba1\u67e5\u3001\u91cd\u590d\u548c\u62cd\u5356\u535a\u5f08\u8fdb\u884c\u4e86\u5747\u8861\u5206\u6790\u3002", "result": "\u8868\u660e\u786e\u5b9a\u6027\u4f18\u5148DAG\u8c03\u5ea6\u548c\u8003\u8651\u91cd\u590d\u7684\u652f\u4ed8\u53ef\u4ee5\u6d88\u9664\u76f8\u540ctick\u7684MEV\uff0c\u540c\u65f6\u4fdd\u6301\u541e\u5410\u91cf\u3002", "conclusion": "\u786e\u5b9a\u4e86\u7b80\u5355\u7684\u534f\u8bae\u914d\u7f6e\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u4e2d\u5fc3\u5316\u6784\u5efa\u8005\u7684\u60c5\u51b5\u4e0b\u7f13\u89e3MCP\u7279\u5b9a\u7684\u63d0\u53d6\u3002"}}
{"id": "2511.13636", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13636", "abs": "https://arxiv.org/abs/2511.13636", "authors": ["Ond\u0159ej Wojewoda", "Miela J. Gross", "Jan Kl\u00edma", "Jaganandha Panda", "Jakub Kr\u010dma", "Jakub Holobr\u00e1dek", "Krist\u00fdna Dav\u00eddkov\u00e1", "Andrii V. Chumak", "Philipp Pirro", "Roman Verba", "Sebastian Wintz", "Qi Wang", "Caroline A. Ross", "Michal Urb\u00e1nek"], "title": "Ultrafast propagation of magnon-polaritons", "comment": null, "summary": "The manipulation of magnetization lies at the heart of spintronic and magnonic technologies, with the ultimate performance of such systems limited by the velocity at which magnetic excitations can propagate. Here, we demonstrate ultrafast propagation of magnon-polaritons-hybrid quasiparticles arising from the coupling between spin waves and electromagnetic fields in thin pure, bismuth-, and gallium substituted yttrium iron garnet (YIG, Bi:YIG and Ga:YIG) films. Using time- and phase-resolved Brillouin light scattering microscopy and time-resolved scanning transmission microscopy, we show that magnon-polaritons can propagate faster than 100 km/s, nearly three orders of magnitude more than conventional spin waves, and can be observed at distances exceeding 40 micrometers in 20 nm thick films. Analytical modeling based on retarded Maxwell equations and Polder tensor formalism confirms the hybridized nature of the excitations and captures the nontrivial dispersion and attenuation profiles. Notably, the magnon-polaritons maintain high initial magnetization amplitudes and long decay lengths, enabling ultrafast manipulation of the magnetization far away from the excitation source. We show, that they can move domain walls or stabilize nonlinear magnetization processes. The unprecedentedly high propagation velocities make magnon-polaritons promising candidates for high-speed information transfer in future spin-based computing architectures, potentially overcoming long-standing group delay bottlenecks in magnonic logic circuits.", "AI": {"tldr": "\u6587\u7ae0\u5c55\u793a\u4e86\u6df7\u5408\u51c6\u7c92\u5b50\u201c\u78c1\u632f\u6781\u5316\u6fc0\u5b50\u201d\u5728\u94c1\u78c1\u8584\u819c\u4e2d\u80fd\u591f\u4ee5\u8d85\u8fc7100\u516c\u91cc/\u79d2\u7684\u901f\u5ea6\u4f20\u64ad\uff0c\u8fdc\u8d85\u4f20\u7edf\u81ea\u65cb\u6ce2\uff0c\u5e76\u80fd\u5728\u957f\u8ddd\u79bb\u4e0a\u4f20\u64ad\uff0c\u6709\u6f5c\u529b\u7528\u4e8e\u672a\u6765\u81ea\u65cb\u7535\u5b50\u5b66\u3002", "motivation": "\u63a2\u7d22\u6bd4\u4f20\u7edf\u81ea\u65cb\u6ce2\u4f20\u64ad\u901f\u5ea6\u66f4\u5feb\u7684\u78c1\u6fc0\u53d1\u4f20\u64ad\u65b9\u5f0f\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u5b8f\u89c2\u5b50\u5b66\u6280\u672f\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u5229\u7528\u65f6\u95f4\u5206\u8fa8\u548c\u76f8\u4f4d\u5206\u8fa8\u5e03\u91cc\u6e0a\u5149\u6563\u5c04\u663e\u5fae\u955c\u4ee5\u53ca\u65f6\u95f4\u5206\u8fa8\u626b\u63cf\u900f\u5c04\u663e\u5fae\u955c\uff0c\u7814\u7a76\u4e86\u63ba\u6742\u548c\u672a\u63ba\u6742\u7684\u9487\u94c1\u77f3\u69b4\u77f3\u8584\u819c\u4e2d\u78c1\u632f\u6781\u5316\u6fc0\u5b50\u7684\u4f20\u64ad\u52a8\u529b\u5b66\u3002\u5e76\u901a\u8fc7\u57fa\u4e8e\u5ef6\u8fdf\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u548cPolder\u5f20\u91cf\u5f62\u5f0f\u7684\u89e3\u6790\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u9487\u94c1\u77f3\u69b4\u77f3\u8584\u819c\uff08YIG, Bi:YIG, Ga:YIG\uff09\u4e2d\u89c2\u5bdf\u5230\u78c1\u632f\u6781\u5316\u6fc0\u5b50\uff0c\u5176\u4f20\u64ad\u901f\u5ea6\u8d85\u8fc7100\u516c\u91cc/\u79d2\uff0c\u4f20\u64ad\u8ddd\u79bb\u8d85\u8fc740\u5fae\u7c73\u3002\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6df7\u5408\u6027\u8d28\u3001\u8272\u6563\u548c\u8870\u51cf\u7279\u6027\u3002\u78c1\u632f\u6781\u5316\u6fc0\u5b50\u80fd\u4fdd\u6301\u9ad8\u521d\u59cb\u78c1\u5316\u632f\u5e45\u548c\u957f\u8870\u51cf\u957f\u5ea6\uff0c\u80fd\u591f\u79fb\u52a8\u78c1\u7574\u58c1\u6216\u7a33\u5b9a\u975e\u7ebf\u6027\u78c1\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u78c1\u632f\u6781\u5316\u6fc0\u5b50\u5177\u6709\u524d\u6240\u672a\u6709\u9ad8\u4f20\u64ad\u901f\u5ea6\u548c\u957f\u4f20\u64ad\u8ddd\u79bb\uff0c\u662f\u672a\u6765\u81ea\u65cb\u8ba1\u7b97\u67b6\u6784\u4e2d\u9ad8\u901f\u4fe1\u606f\u4f20\u8f93\u7684\u6709\u5e0c\u671b\u7684\u5019\u9009\u8005\uff0c\u6709\u671b\u89e3\u51b3\u5b8f\u89c2\u5b50\u903b\u8f91\u7535\u8def\u4e2d\u957f\u671f\u5b58\u5728\u7684\u7fa4\u5ef6\u8fdf\u74f6\u9888\u3002"}}
{"id": "2511.12045", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12045", "abs": "https://arxiv.org/abs/2511.12045", "authors": ["Paloma Sette", "Maria Werneck", "William Barbosa", "Ana Loubacker"], "title": "MUSTEM: A Dual-Modality System for Vibrotactile and Visual Translation of Music as an Assistive Technology", "comment": null, "summary": "The emotional and structural experience of music remains a significant accessibility challenge for the deaf and hard of hearing community. This paper introduces MUSTEM (Multisensorial Emotional Translation), a novel system designed to translate music into a rich, coherent, and scientifically-grounded sensory experience. We present a dual-modality approach addressing this challenge through two interconnected components. First, a low-cost, portable hardware prototype that performs real-time audio analysis, mapping distinct frequency bands (sub-bass, bass, mid-range, treble) to a four-channel vibrotactile system, allowing users to feel the music's rhythmic and foundational structure. Second, to overcome the processing limitations of embedded hardware, we developed a high-fidelity software simulation that demonstrates the full potential of the visual translation. This assistive dashboard decodes musical components - such as rhythm, harmony, and frequency spectrum - into an intuitive and educational visual interface. MUSTEM offers a comprehensive framework for sensory substitution, presenting a viable and accessible pathway for the deaf community to experience music not just as vibration, but as a structured, substantiated and emotionally resonant visual and tactile language. Preliminary feedback from seven deaf users suggests the system's spatial vibrotactile mapping is perceptible and engaging. All source code and hardware designs are released as open-source. Video demonstrations and open-source code are available on the project's official channel.", "AI": {"tldr": "MUSTEM\u7cfb\u7edf\u901a\u8fc7\u5c06\u97f3\u4e50\u8f6c\u6362\u4e3a\u89e6\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u4e3a\u804b\u54d1\u4eba\u7fa4\u63d0\u4f9b\u4e86\u65b0\u7684\u97f3\u4e50\u4f53\u9a8c\u65b9\u5f0f\uff0c\u5e76\u5df2\u83b7\u5f97\u521d\u6b65\u79ef\u6781\u7684\u7528\u6237\u53cd\u9988\u3002", "motivation": "\u89e3\u51b3\u804b\u54d1\u4eba\u7fa4\u5728\u7406\u89e3\u548c\u4f53\u9a8c\u97f3\u4e50\u65f6\u9762\u4e34\u7684\u60c5\u611f\u548c\u7ed3\u6784\u969c\u788d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u786c\u4ef6\u539f\u578b\uff08\u5c06\u97f3\u9891\u9891\u7387\u6620\u5c04\u5230\u632f\u52a8\u89e6\u89c9\uff09\u548c\u8f6f\u4ef6\u6a21\u62df\uff08\u5c06\u97f3\u4e50\u6210\u5206\u89e3\u7801\u4e3a\u89c6\u89c9\u754c\u9762\uff09\u7684\u53cc\u6a21\u6001\u7cfb\u7edfMUSTEM\u3002", "result": "\u786c\u4ef6\u539f\u578b\u80fd\u591f\u5b9e\u65f6\u5206\u6790\u97f3\u9891\uff0c\u5e76\u5c06\u4e0d\u540c\u9891\u6bb5\u6620\u5c04\u5230\u632f\u52a8\u89e6\u89c9\u7cfb\u7edf\uff1b\u8f6f\u4ef6\u6a21\u62df\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u89c6\u89c9\u7ffb\u8bd1\u6f5c\u529b\uff0c\u5e76\u5c06\u5176\u89e3\u7801\u4e3a\u76f4\u89c2\u7684\u89c6\u89c9\u754c\u9762\u3002\u4e03\u540d\u804b\u54d1\u7528\u6237\u521d\u6b65\u53cd\u9988\u8ba4\u4e3a\u89e6\u89c9\u6620\u5c04\u662f\u53ef\u611f\u77e5\u548c\u5f15\u4eba\u5165\u80dc\u7684\u3002", "conclusion": "MUSTEM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u611f\u5b98\u66ff\u4ee3\u6846\u67b6\uff0c\u4e3a\u804b\u54d1\u4eba\u7fa4\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u53ef\u53ca\u7684\u9014\u5f84\uff0c\u8ba9\u4ed6\u4eec\u80fd\u591f\u5c06\u97f3\u4e50\u4f53\u9a8c\u4e3a\u7ed3\u6784\u5316\u3001\u6709\u6839\u636e\u4e14\u5bcc\u6709\u60c5\u611f\u5171\u9e23\u7684\u89c6\u89c9\u548c\u89e6\u89c9\u8bed\u8a00\u3002"}}
{"id": "2511.11605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11605", "abs": "https://arxiv.org/abs/2511.11605", "authors": ["David Balaban", "Adrian Micl\u0103u\u015f"], "title": "PACE Solver Description: twin_width_fmi", "comment": null, "summary": "In this paper we present \\texttt{twin\\_width\\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.\n  As a baseline, we implement \\texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.\n  Our best-performing component, which we ultimately submitted, is \\texttt{hedom5}. The design of \\texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \\texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7528\u4e8e PACE 2025 \u7ade\u8d5b\u7684\u65b0\u542f\u53d1\u5f0f\u7b97\u6cd5 hedom5 \u6765\u89e3\u51b3\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\u3002", "motivation": "\u5728 PACE 2025 \u7ade\u8d5b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8d5b\u9053\u4e0a\uff0c\u4e3a\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6c42\u89e3\u5668\u3002", "method": "\u8bba\u6587\u9996\u5148\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u7ebf\u8d2a\u5fc3\u7b97\u6cd5 greedy-ln\u3002\u7136\u540e\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6a21\u62df\u9000\u706b\u5c40\u90e8\u641c\u7d22\u3002\u6700\u7ec8\u63d0\u4ea4\u7684 hedom5 \u7b97\u6cd5\u7ed3\u5408\u4e86\u8fed\u4ee3\u8d2a\u5fc3\u3001\u56fe\u7684\u538b\u7f29\u8868\u793a (CSR)\u3001\u56fe\u7b80\u5316\uff08\u5904\u7406\u5b64\u7acb\u70b9\u548c\u53f6\u5b50\u90bb\u5c45\uff09\u3001\u57fa\u4e8e\u4f18\u5148\u961f\u5217\u7684\u5ef6\u8fdf\u589e\u76ca\u8d2a\u5fc3\u9636\u6bb5\u3001\u53cd\u5411\u5220\u9664\u526a\u679d\u4ee5\u53ca\u9884\u7b97\u4e3a1\u7684\u4ea4\u6362\u5c40\u90e8\u6539\u8fdb\uff0c\u6700\u540e\u8fdb\u884c\u5b89\u5168\u4fee\u8865\u4ee5\u786e\u4fdd\u5b8c\u5168\u652f\u914d\u3002", "result": "hedom5 \u5728 PACE 2025 \u7ade\u8d5b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8d5b\u9053\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "hedom5 \u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\uff08\u5982\u56fe\u7b80\u5316\u3001\u5ef6\u8fdf\u589e\u76ca\u8d2a\u5fc3\u3001\u53cd\u5411\u526a\u679d\u548c\u5c40\u90e8\u641c\u7d22\uff09\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\uff0c\u5e76\u5728\u7ade\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.13247", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.13247", "abs": "https://arxiv.org/abs/2511.13247", "authors": ["Zhuo Chen", "Zhongqun Zhang", "Yihua Cheng", "Ales Leonardis", "Hyung Jin Chang"], "title": "Force-Aware 3D Contact Modeling for Stable Grasp Generation", "comment": "AAAI'26 Camera Ready. Project page at https://chzh9311.github.io/force-aware-grasp-project/", "summary": "Contact-based grasp generation plays a crucial role in various applications. Recent methods typically focus on the geometric structure of objects, producing grasps with diverse hand poses and plausible contact points. However, these approaches often overlook the physical attributes of the grasp, specifically the contact force, leading to reduced stability of the grasp. In this paper, we focus on stable grasp generation using explicit contact force predictions. First, we define a force-aware contact representation by transforming the normal force value into discrete levels and encoding it using a one-hot vector. Next, we introduce force-aware stability constraints. We define the stability problem as an acceleration minimization task and explicitly relate stability with contact geometry by formulating the underlying physical constraints. Finally, we present a pose optimizer that systematically integrates our contact representation and stability constraints to enable stable grasp generation. We show that these constraints can help identify key contact points for stability which provide effective initialization and guidance for optimization towards a stable grasp. Experiments are carried out on two public benchmarks, showing that our method brings about 20% improvement in stability metrics and adapts well to novel objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u63a5\u89e6\u529b\u7684\u7a33\u5b9a\u6293\u53d6\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u9884\u6d4b\u63a5\u89e6\u529b\u6765\u63d0\u9ad8\u6293\u53d6\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u6293\u53d6\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u51e0\u4f55\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u63a5\u89e6\u529b\uff0c\u5bfc\u81f4\u6293\u53d6\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "1. \u5b9a\u4e49\u529b\u611f\u77e5\u63a5\u89e6\u8868\u793a\uff0c\u5c06\u6cd5\u5411\u529b\u503c\u79bb\u6563\u5316\u5e76\u8fdb\u884c\u72ec\u70ed\u7f16\u7801\u3002 2. \u5f15\u5165\u529b\u611f\u77e5\u7a33\u5b9a\u6027\u7ea6\u675f\uff0c\u5c06\u7a33\u5b9a\u6027\u95ee\u9898\u5b9a\u4e49\u4e3a\u52a0\u901f\u5ea6\u6700\u5c0f\u5316\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u5c06\u7a33\u5b9a\u6027\u4e0e\u63a5\u89e6\u51e0\u4f55\u8054\u7cfb\u8d77\u6765\u3002 3. \u63d0\u51fa\u4e00\u4e2a\u59ff\u6001\u4f18\u5316\u5668\uff0c\u6574\u5408\u63a5\u89e6\u8868\u793a\u548c\u7a33\u5b9a\u6027\u7ea6\u675f\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6293\u53d6\u7a33\u5b9a\u6027\u6307\u6807\u63d0\u9ad8\u4e86\u7ea620%\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u9002\u5e94\u65b0\u7269\u4f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u529b\u611f\u77e5\u63a5\u89e6\u8868\u793a\u548c\u7a33\u5b9a\u6027\u7ea6\u675f\u80fd\u591f\u8bc6\u522b\u51fa\u5bf9\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u7684\u63a5\u89e6\u70b9\uff0c\u5e76\u4e3a\u4f18\u5316\u63d0\u4f9b\u6709\u6548\u7684\u521d\u59cb\u5316\u548c\u6307\u5bfc\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\u3002"}}
{"id": "2511.12035", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12035", "abs": "https://arxiv.org/abs/2511.12035", "authors": ["Wenxuan Miao", "Yulin Sun", "Aiyue Chen", "Jing Lin", "Yiwu Yao", "Yiming Gan", "Jieru Zhao", "Jingwen Leng", "Mingyi Guo", "Yu Feng"], "title": "TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space", "comment": null, "summary": "The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.\n  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\\% loss on VBench).", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u6765\u52a0\u901f\u89c6\u9891\u6269\u6563 Transformer (vDiT) \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u7528\u90e8\u5206\u6ce8\u610f\u529b\u5206\u6570\u6765\u5b9e\u73b0\u663e\u8457\u7684\u8ba1\u7b97\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u4e3b\u8981\u57fa\u4e8e vDiT\uff09\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u5927\u7684\u95ee\u9898\uff0c\u800c\u4ee5\u5f80\u7684\u7814\u7a76\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u6d41\u56fa\u6709\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u7684\u91cd\u7528\u7b56\u7565\uff0c\u901a\u8fc7\u91cd\u7528\u7a7a\u95f4\u6216\u65f6\u95f4\u4e0a\u76f8\u5173\u7684\u4ee4\u724c\u5728\u5355\u4e2a\u901a\u9053\u4e0a\u7684\u90e8\u5206\u6ce8\u610f\u529b\u5206\u6570\u6765\u8fd1\u4f3c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u5728 4 \u4e2a vDiT \u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86 85% \u7684\u8ba1\u7b97\u8282\u7701\uff0c\u89c6\u9891\u8d28\u91cf\u635f\u5931\u5c0f\u4e8e 0.06%\uff08\u5728 VBench \u4e0a\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u4e86 vDiT \u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u8d28\u91cf\u3002"}}
{"id": "2511.11875", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11875", "abs": "https://arxiv.org/abs/2511.11875", "authors": ["Elena Petri", "Koen J. A. Scheres", "Erik Steur", "W. P. M. H.", "Heemels"], "title": "Emulation-based Neuromorphic Control for the Stabilization of LTI Systems", "comment": null, "summary": "Brain-inspired neuromorphic technologies can offer important advantages over classical digital clock-based technologies in various domains, including systems and control engineering. Indeed, neuromorphic engineering could provide low-latency, low-energy and adaptive control systems in the form of spiking neural networks (SNNs) exploiting spike-based control and communication. However, systematic methods for designing and analyzing neuron-inspired spiking controllers are currently lacking. This paper presents a new systematic approach for stabilizing linear time-invariant (LTI) systems using SNN-based controllers, designed as a network of integrate-and-fire neurons, whose input is the measured output from the plant and generating spiking control signals. The new approach consists of a two-step emulation-based design procedure. In the first step, we establish conditions on the neuron parameters to ensure that the spiking signal generated by a pair of neurons emulates any continuous-time signal input to the neurons with arbitrary accuracy in terms of a special metric for spiky signals. In the second step, we propose a novel stability notion, called integral spiking-input-to-state stability (iSISS) building on this special metric. We prove that an asymptotically stable LTI system has this iSISS property. By combining these steps, a certifiable practical stability property of the closed-loop system can be established. Generalizations are discussed and the effectiveness of the approach is illustrated in a numerical case study.", "AI": {"tldr": "Brain-inspired SNNs offer advantages for control systems, but design methods are lacking. This paper presents a systematic approach to stabilize LTI systems using SNN controllers by emulating continuous signals and introducing a new stability notion (iSISS).", "motivation": "Classical digital systems lack the low-latency, low-energy, and adaptive control capabilities offered by neuromorphic engineering and SNNs. However, systematic methods for designing and analyzing neuron-inspired spiking controllers are currently missing.", "method": "A two-step procedure: 1. Establish conditions for neuron parameters to emulate continuous-time signals accurately using a special metric. 2. Introduce and prove a new stability notion, integral spiking-input-to-state stability (iSISS), based on this metric. Demonstrate that asymptotically stable LTI systems possess this property.", "result": "A certifiable practical stability property of the closed-loop system can be established by combining the two design steps. The approach is demonstrated to be effective through a numerical case study.", "conclusion": "This paper introduces a systematic approach for designing SNN-based controllers to stabilize LTI systems, addressing the lack of such methods. The proposed technique ensures emulation of continuous signals and establishes a novel stability notion (iSISS), leading to certifiable practical stability for closed-loop systems."}}
{"id": "2511.11616", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11616", "abs": "https://arxiv.org/abs/2511.11616", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance", "comment": "Accepted and scheduled for conference presentation", "summary": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\u03b5\\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u7f3a\u4e4f\u62dc\u5360\u5ead\u5bb9\u9519\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u62dc\u5360\u5ead\u5bb9\u9519\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u5c42\u67b6\u6784\uff1a\u5305\u542b\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\uff08<10ms\u5ef6\u8fdf\uff09\u7684\u5c40\u90e8\u5c42\u3001\u57fa\u4e8e\u7a00\u758f\u6ce8\u610f\u529b\u548c\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff08O(nk)\u590d\u6742\u5ea6\uff09\u7684\u533a\u57df\u5c42\u3001\u4ee5\u53ca\u57fa\u4e8eHashgraph\u534f\u8bae\u7684\u5168\u5c40\u5c42\u3002\u91c7\u7528\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u673a\u5236\u52a8\u6001\u8c03\u6574\u566a\u58f0\u6c34\u5e73\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eDHT\u7684\u8f7b\u91cf\u7ea7\u5ba1\u8ba1\u65e5\u5fd7\u66ff\u4ee3\u533a\u5757\u94fe\u5171\u8bc6\u3002", "result": "\u5728500\u67b6\u65e0\u4eba\u673a\u7684\u4eff\u771f\u573a\u666f\u4e0b\uff0c\u5b9e\u73b0\u4e86<2.0%\u7684\u78b0\u649e\u7387\u548cf<n/3\u7684\u62dc\u5360\u5ead\u5bb9\u9519\u80fd\u529b\uff0c\u4e14\u4e2d\u4f4d\u6570\u6210\u672c\u572850ms\u5185\u83b7\u5f9795%\u7f6e\u4fe1\u533a\u95f4\u7684\u51b3\u7b56\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u548c\u62dc\u5360\u5ead\u5bb9\u9519\u80fd\u529b\u3002"}}
{"id": "2511.12176", "categories": ["quant-ph", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12176", "abs": "https://arxiv.org/abs/2511.12176", "authors": ["Xiaobin Song", "Siyuan Bai", "Da-Wei Wang", "Hanxiao Tao", "Xizhe Wang", "Rebing Wu", "Benben Jiang"], "title": "Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries", "comment": null, "summary": "Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.12857", "categories": ["quant-ph", "cs.DS", "cs.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12857", "abs": "https://arxiv.org/abs/2511.12857", "authors": ["Noah Siekierski", "Kausthubh Chandramouli", "Christian K\u00fcmmerle", "Bojko N. Bakalov", "Dror Baron"], "title": "Approximate Message Passing for Quantum State Tomography", "comment": null, "summary": "Quantum state tomography (QST) is an indispensable tool for characterizing many-body quantum systems. However, due to the exponential scaling cost of the protocol with system size, many approaches have been developed for quantum states with specific structure, such as low-rank states. In this paper, we show how approximate message passing (AMP), a compressed sensing technique, can be used to perform low-rank QST. AMP provides asymptotically optimal performance guarantees for large systems, which suggests its utility for QST. We discuss the design challenges that come with applying AMP to QST, and show that by properly designing the AMP algorithm, we can reduce the reconstruction infidelity by over an order of magnitude compared to existing approaches to low-rank QST. We also performed tomographic experiments on IBM Kingston and considered the effect of device noise on the reliability of the predicted fidelity of state preparation. Our work advances the state of low-rank QST and may be applicable to other quantum tomography protocols.", "AI": {"tldr": "AMP\u53ef\u7528\u4e8e\u4f4e\u79e9\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u91cd\u6784\u7cbe\u5ea6\u5e76\u8003\u8651\u566a\u58f0\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\uff08QST\uff09\u7684\u6210\u672c\u968f\u7cfb\u7edf\u89c4\u6a21\u5448\u6307\u6570\u589e\u957f\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u5177\u6709\u7279\u5b9a\u7ed3\u6784\uff08\u5982\u4f4e\u79e9\u6001\uff09\u7684\u91cf\u5b50\u6001\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u538b\u7f29\u611f\u77e5\u6280\u672f\u4e2d\u7684\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\uff08AMP\uff09\u7b97\u6cd5\u6765\u8fdb\u884c\u4f4e\u79e9\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u3002", "result": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684AMP\u7b97\u6cd5\uff0c\u4e0e\u73b0\u6709\u7684\u4f4e\u79e9QST\u65b9\u6cd5\u76f8\u6bd4\uff0c\u91cd\u6784\u4fdd\u771f\u5ea6\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728IBM Kingston\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u8003\u8651\u4e86\u5668\u4ef6\u566a\u58f0\u5bf9\u72b6\u6001\u5236\u5907\u4fdd\u771f\u5ea6\u9884\u6d4b\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "AMP\u4e3a\u4f4e\u79e9QST\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5177\u6709\u6e10\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u4e14\u53ef\u4ee5\u51cf\u5c11\u91cd\u6784\u5931\u771f\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6\u91cf\u5b50\u5c42\u6790\u6210\u50cf\u534f\u8bae\u3002"}}
{"id": "2511.13492", "categories": ["cs.MA", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13492", "abs": "https://arxiv.org/abs/2511.13492", "authors": ["Jesus Fernandez-Bes", "Roc\u00edo Arroyo-Valles", "Jes\u00fas Cid-Sueiro"], "title": "Asymptotic analysis of cooperative censoring policies in sensor networks", "comment": null, "summary": "The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u7535\u6c60\u4f9b\u7535\u7684\u591a\u8df3\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u534f\u4f5c\u6570\u636e\u5ba1\u67e5\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u627e\u5230\u4e86\u8fd1\u4f3c\u7684\u6700\u4f18\u5ba1\u67e5\u9608\u503c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8282\u7701\u80fd\u6e90\u5e76\u4f18\u4e8e\u975e\u534f\u4f5c\u65b9\u6848\u3002", "motivation": "\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u7535\u6c60\u4f9b\u7535\u7684\u591a\u8df3\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u534f\u4f5c\u6570\u636e\u5ba1\u67e5\u95ee\u9898\uff0c\u4ee5\u8282\u7701\u80fd\u6e90\uff0c\u786e\u4fdd\u8282\u70b9\u80fd\u591f\u8fdb\u884c\u66f4\u957f\u65f6\u95f4\u7684\u901a\u4fe1\u3002", "method": "\u672c\u7bc7\u8bba\u6587\u4f7f\u7528\u8054\u5408\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5bf9\u6574\u4e2a\u7f51\u7edc\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u627e\u5230\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6700\u4f18\u7684\u5ba1\u67e5\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u4ee5\u6700\u5927\u5316\u957f\u671f\u5956\u52b1\u3002\u5c3d\u7ba1\u6700\u4f18\u5ba1\u67e5\u89c4\u5219\u5728\u8ba1\u7b97\u4e0a\u662f\u96be\u4ee5\u5b9e\u73b0\u7684\uff0c\u4f46\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u8fd9\u4e9b\u9608\u503c\u7684\u4e2d\u5fc3\u5316\u7b97\u6cd5\uff0c\u5e76\u4e14\u53d1\u73b0\uff0c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u6700\u4f18\u89c4\u5219\u53ef\u4ee5\u7531\u6709\u9650\u7684\u5e38\u91cf\u9608\u503c\u89c4\u5219\u96c6\u5408\u6765\u8fd1\u4f3c\u3002", "result": "\u5b9e\u9a8c\u6a21\u62df\u8868\u660e\uff0c\u534f\u4f5c\u5ba1\u67e5\u7b56\u7565\u5177\u6709\u8282\u80fd\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u975e\u534f\u4f5c\u65b9\u6848\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u6c60\u4f9b\u7535\u7684\u591a\u8df3\u4f20\u611f\u5668\u7f51\u7edc\u7684\u534f\u4f5c\u6570\u636e\u5ba1\u67e5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5927\u5316\u957f\u671f\u5956\u52b1\u6765\u4f18\u5316\u80fd\u6e90\u4f7f\u7528\uff0c\u5e76\u627e\u5230\u4e86\u53ef\u884c\u7684\u8fd1\u4f3c\u6700\u4f18\u9608\u503c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12604", "categories": ["cond-mat.mes-hall", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12604", "abs": "https://arxiv.org/abs/2511.12604", "authors": ["Yizhe Liu", "Bo Sun"], "title": "Electron Tunneling Enhances Thermal Conductance through Metal-Insulator-Semiconductor Junctions", "comment": "4 figures", "summary": "The presence of interfaces in semiconductor devices substantially hinders thermal transport, contributing disproportionately to the overall thermal resistance. However, approaches to enhance interfacial thermal transport remain scarce without changing the interface structure, as the intrinsic electron and phonon properties of constituent materials set an upper limit. Here, we find a new thermal transport pathway, electronic heat tunneling, to enhance interfacial thermal conductance through metal-insulator-semiconductor junctions. By applying photoexcitation or bias voltage, we observe remarkable thermal conductance increases in operando, opening a new channel for efficient interfacial heat dissipation. The electron quantum tunneling pathway is parallel to conventional phonon-mediated interfacial thermal transport, and violates the Wiedemann-Franz law since this pathway deviates from the paradigm of diffusive transport. Moreover, we develop a tunneling mismatch model to describe the enhanced thermal conductance, originating from tunneling heat flux. Our Letter demonstrates a previously unexplored heat transport mechanism to enhance thermal conductance, bypassing the need for interface engineering. These findings emphasize the essential need to understand semiconductor thermal properties under realistic operating conditions.", "AI": {"tldr": "\u7535\u5b50\u70ed\u96a7\u7a7f\u6548\u5e94\u663e\u8457\u63d0\u5347\u754c\u9762\u70ed\u5bfc\uff0c\u4e14\u4e0d\u4f9d\u8d56\u754c\u9762\u7ed3\u6784\u3002", "motivation": "\u534a\u5bfc\u4f53\u5668\u4ef6\u4e2d\u7684\u754c\u9762\u663e\u8457\u963b\u788d\u70ed\u4f20\u8f93\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u5728\u4e0d\u6539\u53d8\u754c\u9762\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u754c\u9762\u70ed\u4f20\u8f93\u3002", "method": "\u901a\u8fc7\u5149\u6fc0\u53d1\u6216\u504f\u7f6e\u7535\u538b\uff0c\u5229\u7528\u7535\u5b50\u91cf\u5b50\u96a7\u7a7f\u6548\u5e94\u5f00\u8f9f\u65b0\u7684\u70ed\u4f20\u8f93\u901a\u9053\uff0c\u5e76\u5efa\u7acb\u96a7\u9053\u5931\u914d\u6a21\u578b\u63cf\u8ff0\u589e\u5f3a\u7684\u70ed\u5bfc\u3002", "result": "\u5728\u91d1\u5c5e-\u7edd\u7f18\u4f53-\u534a\u5bfc\u4f53\u7ed3\u4e2d\uff0c\u901a\u8fc7\u5149\u6fc0\u53d1\u6216\u504f\u7f6e\u7535\u538b\u89c2\u5bdf\u5230\u663e\u8457\u7684\u70ed\u5bfc\u589e\u52a0\uff0c\u8bc1\u660e\u4e86\u7535\u5b50\u70ed\u96a7\u7a7f\u6548\u5e94\u7684\u5b58\u5728\u3002", "conclusion": "\u7535\u5b50\u70ed\u96a7\u7a7f\u662f\u4e00\u79cd\u65b0\u7684\u754c\u9762\u70ed\u4f20\u8f93\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u754c\u9762\u70ed\u5bfc\uff0c\u4e14\u4e0d\u4f9d\u8d56\u754c\u9762\u7ed3\u6784\uff0c\u5e76\u5f3a\u8c03\u4e86\u7406\u89e3\u534a\u5bfc\u4f53\u5728\u5b9e\u9645\u5de5\u4f5c\u6761\u4ef6\u4e0b\u7684\u70ed\u7279\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.12338", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.12338", "abs": "https://arxiv.org/abs/2511.12338", "authors": ["Felix Eder", "Catherine Witteveen", "Enrico Giannini", "Fabian O. von Rohr"], "title": "Stoichiometry and Phase Control in K$_{1-x}$CrSe$_2$ via Self-Flux Synthesis", "comment": null, "summary": "Layered delafossite-type magnetic materials, such as KCrSe$_2$, are promising platforms for studying magnetic systems and potential frustration on triangular lattices. Synthesis, structure-type control, and off-stoichiometries remain major challenges in the investigation of these delafossite-type magnets. Starting from the same self-flux composition (K:Cr:Se = 8:1:8), we isolated three distinct K$_{1-x}$CrSe$_2$ phases with $x$ = 0, 0.13--0.17, and 0.32--0.35, each adopting a different structure type depending on the quenching temperature applied. The phase evolution indicates a sequence of transformations during synthesis between compounds with varying degrees of potassium deficiency. Building on these insights into phase stability and crystal growth, we successfully grew single crystals of full-stoichiometric KCrSe$_2$ -- enabling direction-dependent magnetization measurements. These measurements reveal a pronounced field dependence of the N\u00e9el temperature at low external fields, as well as a weak metamagnetic transition. Our findings demonstrate that even a simple parameter -- such as quenching temperature -- can be used to control stoichiometry, direct phase formation, and ultimately tune the magnetic properties of delafossite-type materials.", "AI": {"tldr": "KCrSe2\u5728\u4e0d\u540c\u6dec\u706b\u6e29\u5ea6\u4e0b\u4f1a\u5f62\u6210\u4e0d\u540c\u7684\u76f8\uff0c\u5176\u4e2d\u5168\u5316\u5b66\u8ba1\u91cf\u76f8KCrSe2\u7684\u78c1\u6027\u8868\u73b0\u51fa\u573a\u4f9d\u8d56\u6027\uff0c\u8868\u660e\u6dec\u706b\u6e29\u5ea6\u53ef\u4ee5\u8c03\u63a7\u6750\u6599\u7684\u5316\u5b66\u8ba1\u91cf\u6bd4\u548c\u78c1\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5408\u6210\u3001\u7ed3\u6784\u7c7b\u578b\u63a7\u5236\u548c\u975e\u5316\u5b66\u8ba1\u91cf\u5b66\u5728\u5c42\u72b6\u7c7b \u0938\u094d\u0925\u093f\u0930\u0924\u093e\u77f3\u78c1\u4f53\uff08\u5982KCrSe2\uff09\u7814\u7a76\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u901a\u8fc7\u8c03\u63a7\u5408\u6210\u53c2\u6570\u6765\u63a7\u5236\u6750\u6599\u7684\u78c1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u6dec\u706b\u6e29\u5ea6\uff0c\u4ece\u76f8\u540c\u7684K:Cr:Se = 8:1:8\u7684\u81ea\u7194\u5242\u6210\u5206\u4e2d\u5206\u79bb\u51fa\u4e09\u79cd\u4e0d\u540c\u7684K1-xCrSe2\u76f8\uff08x = 0, 0.13-0.17, 0.32-0.35\uff09\uff0c\u6bcf\u79cd\u76f8\u5177\u6709\u4e0d\u540c\u7684\u7ed3\u6784\u7c7b\u578b\u3002\u968f\u540e\uff0c\u6210\u529f\u751f\u957f\u51fa\u5168\u5316\u5b66\u8ba1\u91cf\u76f8KCrSe2\u7684\u5355\u6676\uff0c\u5e76\u8fdb\u884c\u4f9d\u8d56\u4e8e\u65b9\u5411\u7684\u78c1\u5316\u6d4b\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e09\u79cd\u4e0d\u540c\u7684K1-xCrSe2\u76f8\uff0c\u5176\u7ed3\u6784\u7c7b\u578b\u968f\u6dec\u706b\u6e29\u5ea6\u53d8\u5316\u3002\u5168\u5316\u5b66\u8ba1\u91cf\u76f8KCrSe2\u7684\u78c1\u5316\u6d4b\u91cf\u663e\u793a\u51fa\u4e0e\u4f4e\u5916\u78c1\u573a\u76f8\u5173\u7684\u660e\u663e N\u00e9el \u6e29\u5ea6\u4f9d\u8d56\u6027\u4ee5\u53ca\u8f83\u5f31\u7684\u4e9a\u78c1\u8f6c\u53d8\u3002", "conclusion": "\u5408\u6210\u8fc7\u7a0b\u4e2d\u7b80\u5355\u7684\u6dec\u706b\u6e29\u5ea6\u8c03\u63a7\u5373\u53ef\u63a7\u5236\u5316\u5b66\u8ba1\u91cf\u6bd4\u3001\u76f8\u5f62\u6210\uff0c\u5e76\u6700\u7ec8\u8c03\u8282\u5c42\u72b6\u7c7b \u0938\u094d\u0925\u093f\u0930\u0924\u093e\u77f3\u6750\u6599\u7684\u78c1\u6027\u80fd\u3002"}}
{"id": "2511.11720", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11720", "abs": "https://arxiv.org/abs/2511.11720", "authors": ["Jiao Chen", "Haoyi Wang", "Jianhua Tang", "Junyi Wang"], "title": "AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks", "comment": null, "summary": "Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.", "AI": {"tldr": "AdaptFly\u662f\u4e00\u4e2a\u7528\u4e8e\u4f4e\u7a7a\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u5373\u65f6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6765\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u7684\u9c81\u68d2\u6027\u3002\u5b83\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u8d44\u6e90\u4e30\u5bcc\u7684\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u81ea\u9002\u5e94\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u8de8\u65e0\u4eba\u673a\u77e5\u8bc6\u5e93\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u534f\u540c\u3002", "motivation": "\u4f4e\u7a7a\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u5929\u6c14\u3001\u5149\u7167\u548c\u89c6\u89d2\u53d8\u5316\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u65e0\u6cd5\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff0c\u800c\u8d44\u6e90\u4e30\u5bcc\u7684\u65e0\u4eba\u673a\u5219\u72ec\u7acb\u81ea\u9002\u5e94\uff0c\u6d6a\u8d39\u4e86\u5171\u4eab\u7ecf\u9a8c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "AdaptFly\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u3002\u5b83\u5305\u542b\u4e24\u79cd\u81ea\u9002\u5e94\u6a21\u5f0f\uff1a\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684token-prompt\u68c0\u7d22\uff1b\u5bf9\u4e8e\u8d44\u6e90\u4e30\u5bcc\u7684\u65e0\u4eba\u673a\uff0c\u91c7\u7528\u65e0\u68af\u5ea6\u7a00\u758f\u89c6\u89c9\u63d0\u793a\u4f18\u5316\uff08Covariance Matrix Adaptation Evolution Strategy\uff09\u3002\u901a\u8fc7\u6fc0\u6d3b\u7edf\u8ba1\u68c0\u6d4b\u5668\u89e6\u53d1\u81ea\u9002\u5e94\uff0c\u5e76\u5229\u7528\u8de8\u65e0\u4eba\u673a\u77e5\u8bc6\u6c60\u6574\u5408\u63d0\u793a\u77e5\u8bc6\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u96c6\u7fa4\u7684\u534f\u540c\uff0c\u540c\u65f6\u5c06\u5e26\u5bbd\u5f00\u9500\u964d\u81f3\u6700\u4f4e\u3002", "result": "\u5728UAVid\u548cVDD\u6570\u636e\u96c6\u4ee5\u53ca\u771f\u5b9e\u65e0\u4eba\u673a\u90e8\u7f72\u7684\u5b9e\u9a8c\u4e2d\uff0cAdaptFly\u5728\u5404\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u9759\u6001\u6a21\u578b\u548c\u73b0\u6709\u7684TTA\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AdaptFly\u4e3a\u65b0\u5174\u7684\u4f4e\u7a7a\u7ecf\u6d4e\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u611f\u77e5\u8def\u5f84\uff0c\u80fd\u591f\u5b9e\u73b0\u5f39\u6027\u3001\u53ef\u9760\u7684\u8bed\u4e49\u5206\u5272\uff0c\u5373\u4f7f\u5728\u4e25\u82db\u7684\u73af\u5883\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.13131", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.13131", "abs": "https://arxiv.org/abs/2511.13131", "authors": ["Gagan Raj Gupta", "Anshul Kumar", "Manish Rai", "Apu Chakraborty", "Ashutosh Modi", "Abdelaali Chaoub", "Soumajit Pramanik", "Moyank Giri", "Yashwanth Holla", "Sunny Kumar", "M. V. Kiran Sooraj"], "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.", "AI": {"tldr": "MM-Telco \u662f\u4e00\u4e2a\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u4efb\u52a1\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u65e8\u5728\u5e2e\u52a9\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u7535\u4fe1\u9886\u57df\u3002\u5b83\u8fd8\u5305\u62ec\u5bf9\u5f53\u524d\u591a\u6a21\u6001 LLM \u7684\u5206\u6790\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u3002", "motivation": "\u7535\u4fe1\u884c\u4e1a\u9762\u4e34\u7740\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7279\u5b9a\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9002\u5e94\u3002MM-Telco \u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u91cf\u8eab\u5b9a\u5236\u7684\u57fa\u51c6\u548c\u6a21\u578b\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "MM-Telco \u57fa\u51c6\u5957\u4ef6\u5305\u542b\u5404\u79cd\u6587\u672c\u548c\u56fe\u50cf\u4efb\u52a1\uff0c\u4ee5\u89e3\u51b3\u7535\u4fe1\u9886\u57df\u7684\u5b9e\u9645\u7528\u4f8b\uff0c\u4f8b\u5982\u7f51\u7edc\u8fd0\u8425\u3001\u7f51\u7edc\u7ba1\u7406\u3001\u6587\u6863\u8d28\u91cf\u6539\u8fdb\u548c\u76f8\u5173\u6587\u672c/\u56fe\u50cf\u68c0\u7d22\u3002\u6211\u4eec\u8fd8\u5bf9\u5404\u79cd LLM \u548c VLM \u8fdb\u884c\u4e86\u57fa\u7ebf\u5b9e\u9a8c\u3002", "result": "\u5728 MM-Telco \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002\u5b9e\u9a8c\u8fd8\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001 LLM \u7684\u8584\u5f31\u73af\u8282\u3002", "conclusion": "MM-Telco \u8bc1\u660e\u4e86\u9488\u5bf9\u7535\u4fe1\u9886\u57df\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001 LLM \u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.13678", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.13678", "abs": "https://arxiv.org/abs/2511.13678", "authors": ["Haichuan Wang", "Yifan Wu", "Haifeng Xu"], "title": "The Publication Choice Problem", "comment": "Accepted by AAAI 2025 as an oral presentation", "summary": "Researchers strategically choose where to submit their work in order to maximize its impact, and these publication decisions in turn determine venues' impact factors. To analyze how individual publication choices both respond to and shape venue impact, we introduce a game-theoretic framework, coined the Publication Choice Problem, that captures this two-way interplay. We show the existence of a pure-strategy equilibrium in the Publication Choice Problem and its uniqueness under binary researcher types. Our characterizations of the equilibrium properties offer insights about what publication behaviors better indicate a researcher's impact level. Through equilibrium analysis, we further investigate how labeling papers with ``spotlight'' affects the impact factor of venues in the research community. Our analysis shows that competitive venue labeling top papers with ``spotlight'' may decrease the overall impact of other venues in the community, while less competitive venues with ``spotlight'' labeling have the opposite impact.", "AI": {"tldr": "\u7814\u7a76\u8005\u5728\u9009\u62e9\u8bba\u6587\u53d1\u8868\u5730\u70b9\u65f6\u4f1a\u8003\u8651\u5982\u4f55\u6700\u5927\u5316\u8bba\u6587\u5f71\u54cd\u529b\uff0c\u800c\u8fd9\u4e00\u51b3\u7b56\u53c8\u4f1a\u53cd\u8fc7\u6765\u5f71\u54cd\u53d1\u8868\u671f\u520a\u7684\u5f71\u54cd\u56e0\u5b50\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u535a\u5f08\u8bba\u6a21\u578b\u6765\u5206\u6790\u8fd9\u79cd\u53cc\u5411\u4e92\u52a8\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5b58\u5728\u552f\u4e00\u5747\u8861\u89e3\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u671f\u520a\u7684\u201c\u91cd\u70b9\u63a8\u8350\u201d\u6807\u7b7e\u53ef\u80fd\u4f1a\u964d\u4f4e\u793e\u533a\u5185\u5176\u4ed6\u671f\u520a\u7684\u6574\u4f53\u5f71\u54cd\u529b\uff0c\u800c\u7ade\u4e89\u529b\u8f83\u5f31\u7684\u671f\u520a\u5219\u4f1a\u4ea7\u751f\u76f8\u53cd\u7684\u5f71\u54cd\u3002", "motivation": "\u5206\u6790\u7814\u7a76\u8005\u53d1\u8868\u9009\u62e9\u4e0e\u671f\u520a\u5f71\u54cd\u56e0\u5b50\u4e4b\u95f4\u7684\u53cc\u5411\u4e92\u52a8\u5173\u7cfb\uff0c\u4ee5\u53ca\u201c\u91cd\u70b9\u63a8\u8350\u201d\u6807\u7b7e\u5bf9\u671f\u520a\u793e\u533a\u6574\u4f53\u5f71\u54cd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u535a\u5f08\u8bba\u6a21\u578b\uff08Publication Choice Problem\uff09\uff0c\u5206\u6790\u4e86\u5176\u4e2d\u5b58\u5728\u7684\u7eaf\u7b56\u7565\u5747\u8861\u53ca\u5176\u552f\u4e00\u6027\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bf9\u5747\u8861\u6027\u8d28\u8fdb\u884c\u4e86\u523b\u753b\u3002", "result": "\u8bc1\u660e\u4e86Publication Choice Problem\u5b58\u5728\u7eaf\u7b56\u7565\u5747\u8861\uff0c\u5e76\u4e14\u5728\u4e8c\u5143\u7814\u7a76\u8005\u7c7b\u578b\u4e0b\u662f\u552f\u4e00\u7684\u3002\u7814\u7a76\u4e86\u5747\u8861\u6027\u8d28\u5982\u4f55\u63ed\u793a\u7814\u7a76\u8005\u7684\u5f71\u54cd\u529b\u6c34\u5e73\u3002\u5206\u6790\u4e86\u201c\u91cd\u70b9\u63a8\u8350\u201d\u6807\u7b7e\u5bf9\u671f\u520a\u793e\u533a\u5f71\u54cd\u56e0\u5b50\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7ade\u4e89\u6fc0\u70c8\u7684\u671f\u520a\u91c7\u7528\u6b64\u6807\u7b7e\u53ef\u80fd\u964d\u4f4e\u793e\u533a\u6574\u4f53\u5f71\u54cd\u529b\uff0c\u800c\u7ade\u4e89\u529b\u8f83\u5f31\u7684\u671f\u520a\u5219\u76f8\u53cd\u3002", "conclusion": "\u7814\u7a76\u8005\u7684\u53d1\u8868\u9009\u62e9\u4e0e\u671f\u520a\u5f71\u54cd\u529b\u76f8\u4e92\u5f71\u54cd\u3002\u671f\u520a\u7684\u201c\u91cd\u70b9\u63a8\u8350\u201d\u7b56\u7565\u5bf9\u793e\u533a\u6574\u4f53\u5f71\u54cd\u529b\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u671f\u520a\u672c\u8eab\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2511.12250", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12250", "abs": "https://arxiv.org/abs/2511.12250", "authors": ["Doru Sticlet", "Romulus Tetean", "Coriolan Tiusan"], "title": "Skyrmionic qubits stabilized by Dzyaloshinskii-Moriya interaction as platforms for qubits and quantum gates", "comment": "28 figures", "summary": "Quantum computation departs from the classical paradigm of deterministic, bit-based processing by exploiting inherently quantum phenomena such as superposition and entanglement. We propose a framework for qubit realization based on skyrmionic states stabilized by the Dzyaloshinskii-Moriya interaction (DMI) in two-dimensional spin lattices. The model incorporates competing exchange interactions, perpendicular magnetic anisotropy, and Zeeman coupling, solved via exact diagonalization under periodic (PBC) and open boundary conditions (OBC). A quantum skyrmionic phase emerges for PBC within a parameter space defined by DMI, exchange, field, and anisotropy, while OBC favor classical-like, topologically protected skyrmions. Quantum logic gates (Pauli X, Y, Z, Hadamard) are implemented on both skyrmion types. Energy density and entanglement entropy analyses reveal that quantum skyrmions suffer from DMI-driven decoherence and reduced gate fidelity, whereas classical-like skyrmions maintain stability. Exact simulations of qubit dynamics, including drive effects and Lindblad decoherence, demonstrate tunable anharmonic energy levels and coherent Bloch-sphere manipulation, making these skyrmionic states promising candidates for qubit implementation. Overall, the Dzyaloshinskii-Moriya interaction plays a dual role-stabilizing skyrmionic qubits while simultaneously inducing decoherence during gate operations.", "AI": {"tldr": "\u57fa\u4e8eDMI\u7684\u91cf\u5b50\u5929\u4f53\u5b50\u6a21\u578b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u7528\u4e8e\u91cf\u5b50\u6bd4\u7279\u5b9e\u73b0\uff0c\u4f46\u5b58\u5728\u9000\u76f8\u5e72\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u62d3\u6251\u975e\u5e73\u51e1\u78c1\u6001\uff08\u5982\u5929\u4f53\u5b50\uff09\u7684\u91cf\u5b50\u6bd4\u7279\u5b9e\u73b0\u65b9\u6848\uff0c\u7279\u522b\u662f\u5229\u7528DMI\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\uff0c\u5728\u5468\u671f\u6027\uff08PBC\uff09\u548c\u5f00\u8fb9\u754c\u6761\u4ef6\uff08OBC\uff09\u4e0b\uff0c\u5bf9\u5305\u542b\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u3001\u78c1\u5404\u5411\u5f02\u6027\u3001\u585e\u66fc\u8026\u5408\u548cDMI\u7684\u4e8c\u7ef4\u81ea\u65cb\u6676\u683c\u6a21\u578b\u8fdb\u884c\u6c42\u89e3\u3002", "result": "PBC\u6761\u4ef6\u4e0b\u51fa\u73b0\u91cf\u5b50\u5929\u4f53\u5b50\u76f8\uff0cOBC\u6761\u4ef6\u4e0b\u5219\u51fa\u73b0\u62d3\u6251\u4fdd\u62a4\u7684\u7ecf\u5178\u7c7b\u5929\u4f53\u5b50\u3002\u4e24\u79cd\u5929\u4f53\u5b50\u5747\u53ef\u5b9e\u73b0\u91cf\u5b50\u903b\u8f91\u95e8\u64cd\u4f5c\u3002\u80fd\u91cf\u5bc6\u5ea6\u548c\u7ea0\u7f20\u71b5\u5206\u6790\u8868\u660e\uff0c\u91cf\u5b50\u5929\u4f53\u5b50\u6613\u53d7DMI\u9a71\u52a8\u7684\u9000\u76f8\u5e72\u5f71\u54cd\uff0c\u4fdd\u771f\u5ea6\u8f83\u4f4e\uff0c\u800c\u7ecf\u5178\u7c7b\u5929\u4f53\u5b50\u66f4\u7a33\u5b9a\u3002\u91cf\u5b50\u6bd4\u7279\u52a8\u529b\u5b66\u6a21\u62df\u663e\u793a\u53ef\u8c03\u8c10\u7684\u975e\u8c10\u80fd\u91cf\u6c34\u5e73\u548c\u5e03\u6d1b\u8d6b\u7403\u9762\u4e0a\u7684\u76f8\u5e72\u64cd\u63a7\u3002", "conclusion": "DMI\u5728\u7a33\u5b9a\u5929\u4f53\u5b50\u91cf\u5b50\u6bd4\u7279\u7684\u540c\u65f6\uff0c\u4e5f\u4f1a\u5728\u95e8\u64cd\u4f5c\u671f\u95f4\u5f15\u5165\u9000\u76f8\u5e72\u3002\u7ecf\u5178\u7c7b\u5929\u4f53\u5b50\u6bd4\u91cf\u5b50\u5929\u4f53\u5b50\u66f4\u7a33\u5b9a\uff0c\u4f46\u4e24\u79cd\u72b6\u6001\u90fd\u663e\u793a\u51fa\u4f5c\u4e3a\u91cf\u5b50\u6bd4\u7279\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u76f8\u5e72\u64cd\u63a7\u3002"}}
{"id": "2511.12051", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12051", "abs": "https://arxiv.org/abs/2511.12051", "authors": ["Scott Staniewicz", "Sara Mirzaee", "Heresh Fattahi", "Talib Oliver-Cabrera", "Emre Havazli", "Geoffrey Gunter", "Se-Yeon Jeon", "Mary Grace Bato", "Jinwoo Kim", "Simran S. Sangha", "Bruce Chapman", "Alexander L. Handwerger", "Marin Govorcin", "Piyush Agram", "David Bekaert"], "title": "Near-Real-Time InSAR Phase Estimation for Large-Scale Surface Displacement Monitoring", "comment": "14 pages, 11 figures, plus supplementary material", "summary": "Operational near-real-time monitoring of Earth's surface deformation using Interferometric Synthetic Aperture Radar (InSAR) requires processing algorithms that efficiently incorporate new acquisitions without reprocessing historical archives. We present sequential phase linking approach using compressed single-look-complex images (SLCs) capable of producing surface displacement estimates within hours of the time of a new acquisition. Our key algorithmic contribution is a mini-stack reference scheme that maintains phase consistency across processing batches without adjusting or re-estimating previous time steps, enabling straightforward operational deployment. We introduce online methods for persistent and distributed scatterer identification that adapt to temporal changes in surface properties through incremental amplitude statistics updates. The processing chain incorporates multiple complementary metrics for pixel quality that are reliable for small SLC stack sizes, and an L1-norm network inversion to limit propagation of unwrapping errors across the time series. We use our algorithm to produce OPERA Surface Displacement from Sentinel-1 product, the first continental-scale surface displacement product over North America. Validation against GPS measurements and InSAR residual analysis demonstrates millimeter-level agreement in velocity estimates in varying environmental conditions. We demonstrate our algorithm's capabilities with a successful recovery of meter-scale co-eruptive displacement at Kilauea volcano during the 2018 eruption, as well as detection of subtle uplift at Three Sisters volcano, Oregon- a challenging environment for C-band InSAR due to dense vegetation and seasonal snow. We have made all software available as open source libraries, providing a significant advancement to the open scientific community's ability to process large InSAR data sets in a cloud environment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684InSAR\u5904\u7406\u7b97\u6cd5\uff0c\u80fd\u591f\u8fd1\u4e4e\u5b9e\u65f6\u5730\u76d1\u6d4b\u5730\u8868\u5f62\u53d8\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5317\u7f8e\u5927\u9646\u5c3a\u5ea6\u7684\u5730\u8868\u4f4d\u79fb\u4ea7\u54c1\uff08OPERA\uff09\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406\u65b0\u91c7\u96c6\u6570\u636e\u4e14\u65e0\u9700\u91cd\u65b0\u5904\u7406\u5386\u53f2\u5b58\u6863\u7684InSAR\u5904\u7406\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b9e\u65f6\u7684\u5730\u8868\u5f62\u53d8\u76d1\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u987a\u5e8f\u76f8\u4f4d\u8fde\u63a5\u65b9\u6cd5\uff0c\u5229\u7528\u538b\u7f29\u7684\u5355\u89c6\u590d\u6570\u5f71\u50cf\uff08SLCs\uff09\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5c0f\u5806\u6808\u53c2\u8003\u65b9\u6848\u3001\u5728\u7ebf\u6301\u4e45\u6563\u5c04\u4f53\u548c\u5206\u5e03\u5f0f\u6563\u5c04\u4f53\u8bc6\u522b\u65b9\u6cd5\u3001\u591a\u9879\u50cf\u7d20\u8d28\u91cf\u6307\u6807\u4ee5\u53caL1\u8303\u6570\u7f51\u7edc\u53cd\u6f14\u6765\u7ef4\u6301\u76f8\u4f4d\u4e00\u81f4\u6027\u5e76\u9650\u5236\u89e3\u5305\u88f9\u8bef\u5dee\u7684\u4f20\u64ad\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86OPERA\u5730\u8868\u5f62\u53d8\u4ea7\u54c1\uff0c\u5e76\u5728\u5317\u7f8e\u5927\u9646\u8303\u56f4\u5185\u8fdb\u884c\u4e86\u5e94\u7528\u3002\u901a\u8fc7\u4e0eGPS\u6d4b\u91cf\u548cInSAR\u6b8b\u5dee\u5206\u6790\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u901f\u5ea6\u4f30\u8ba1\u5177\u6709\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u3002\u7b97\u6cd5\u80fd\u591f\u6210\u529f\u6062\u590d\u57fa\u62c9\u97e6\u5384\u706b\u5c712018\u5e74\u55b7\u53d1\u671f\u95f4\u7c73\u7ea7\u7684\u540c\u9707\u5f62\u53d8\uff0c\u5e76\u68c0\u6d4b\u5230Three Sisters\u706b\u5c71\u7684\u5fae\u5c0f\u62ac\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684InSAR\u5904\u7406\u7b97\u6cd5\u80fd\u591f\u8fd1\u4e4e\u5b9e\u65f6\u5730\u76d1\u6d4b\u5730\u8868\u5f62\u53d8\uff0c\u5e76\u4e14\u7b97\u6cd5\u548c\u6240\u6709\u8f6f\u4ef6\u5df2\u5f00\u6e90\uff0c\u4e3a\u79d1\u5b66\u754c\u5904\u7406\u5927\u89c4\u6a21InSAR\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u652f\u6301\u3002"}}
{"id": "2511.11608", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11608", "abs": "https://arxiv.org/abs/2511.11608", "authors": ["Mingyu Sung", "Suhwan Im", "Daeho Bang", "Il-Min Kim", "Sangseok Yun", "Jae-Mo Kang"], "title": "Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression", "comment": null, "summary": "Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.", "AI": {"tldr": "SLICER\u901a\u8fc7\u538b\u7f29\u4e2d\u95f4\u7279\u5f81\u6765\u51cf\u5c11\u8fb9\u7f18-\u4e91\u6a21\u578b\u5206\u5272\u4e2d\u7684\u901a\u4fe1\u548c\u670d\u52a1\u5668\u8d1f\u8f7d\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8fb9\u7f18-\u4e91\u6a21\u578b\u5206\u5272\u65b9\u6848\uff08MP\uff09\u7531\u4e8e\u9759\u6001\u5206\u5272\u70b9\u5bfc\u81f4\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\uff0c\u5e76\u5c06\u5ef6\u8fdf\u548c\u80fd\u8017\u96c6\u4e2d\u5728\u670d\u52a1\u5668\u7aef\u3002\u8fd9\u5728\u81ea\u56de\u5f52\uff08AR\uff09\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u5c24\u4e3a\u4e25\u91cd\uff0c\u56e0\u4e3a\u9010\u4ee4\u724c\u7684\u524d\u5411\u4f20\u64ad\u4f1a\u53cd\u590d\u751f\u6210\u5927\u91cf\u7684\u4e2d\u95f4\u7279\u5f81\uff08IFs\uff09\u3002", "method": "SLICER\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u4e0e\u6a21\u578b\u7ed3\u6784\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u538b\u7f29IFs\uff1a(i) \u4e0d\u5bf9\u79f0Top-K\u8fc7\u6ee4\uff08ATKF\uff09\u7a00\u758f\u5316\u4f4e\u5e45\u5ea6\u6fc0\u6d3b\u503c\uff1b(ii) \u5e45\u5ea6\u5206\u88c2\uff08MS\uff09\u5c06\u5269\u4f59\u7684\u975e\u96f6\u503c\u5206\u7ec4\u4e3a\u7b49\u57fa\u6570\u5757\uff1b(iii) \u81ea\u9002\u5e94\u6bd4\u7279\u91cf\u5316\uff08ABQ\uff09\u5728\u5931\u771f\u9884\u7b97\u5185\u4e3a\u6bcf\u5757\u9009\u62e9\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "SLICER\u5728\u6807\u51c6\u7684\u89c6\u89c9\u548cLLM\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982ImageNet/COCO\uff1bHellaSwag\u3001PIQA\u3001ARC-E/C\u3001GSM8K\u3001HumanEval\uff09\u4e0a\uff0c\u5c06\u4e0a\u884c\u94fe\u8def\u4f20\u8f93\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe10\u500d\uff0c\u5c06\u670d\u52a1\u5668GPU\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe4.4\u500d\uff0c\u540c\u65f6\u5c06\u4efb\u52a1\u8d28\u91cf\u635f\u5931\u63a7\u5236\u5728\u57fa\u7ebf\u6c34\u5e73\u7ea60-3\u4e2a\u767e\u5206\u70b9\u4ee5\u5185\u3002\u5728\u591a\u8bbe\u5907\u8bbe\u7f6e\u548cAR LLM\u4e2d\uff0cSLICER\u901a\u8fc7\u5c06\u6709\u610f\u4e49\u7684\u8ba1\u7b97\u8f6c\u79fb\u5230\u8fb9\u7f18\uff0c\u5e76\u964d\u4f4e\u6bcf\u4ee4\u724c\u6bd4\u7279\u6570\u548c\u6bcf\u4ee4\u724c\u670d\u52a1\u5668\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u7a33\u5b9a\u4e86\u6bcf\u6b65\u901a\u4fe1\u91cf\u3002", "conclusion": "SLICER\u4f5c\u4e3a\u4e00\u4e2a\u7f16\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u96c6\u6210\u5230\u73b0\u6709\u7684\u6a21\u578b\u4e2d\uff0c\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11679", "categories": ["cs.LG", "cs.CV", "cs.GR", "math.CV", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.11679", "abs": "https://arxiv.org/abs/2511.11679", "authors": ["Zhehao Xu", "Lok Ming Lui"], "title": "A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications", "comment": null, "summary": "Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.", "AI": {"tldr": "\u4f20\u7edf\u7684\u81ea\u7531\u8fb9\u754c\u53ef\u5fae\u540c\u80da\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u5bf9\u6807\u79f0\u6761\u4ef6\u548c\u68af\u5ea6\u4f18\u5316\u4e0d\u517c\u5bb9\u7684\u9650\u5236\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u62c9\u666e\u62c9\u65af\u7f51\u7edc\uff08SBN\uff09\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86SBN\u4f18\u5316\u6846\u67b6\uff08SBN-Opt\uff09\uff0c\u4ee5\u89e3\u51b3\u81ea\u7531\u8fb9\u754c\u53ef\u5fae\u540c\u80da\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u663e\u5f0f\u63a7\u5236\u5c40\u90e8\u51e0\u4f55\u7578\u53d8\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSBN-Opt\u5728\u5bc6\u5ea6\u5747\u8861\u6620\u5c04\u548c\u4e0d\u4e00\u81f4\u8868\u9762\u914d\u51c6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "motivation": "\u81ea\u7531\u8fb9\u754c\u53ef\u5fae\u540c\u80da\u4f18\u5316\u5728\u8868\u9762\u6620\u5c04\u95ee\u9898\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8fb9\u754c\u7ea6\u675f\u7684\u7f3a\u5931\u548c\u9700\u8981\u4fdd\u6301\u5927\u53d8\u5f62\u4e0b\u7684\u5c40\u90e8\u53cc\u5c04\u6027\uff0c\u8be5\u95ee\u9898\u4e00\u76f4\u96be\u4ee5\u89e3\u51b3\u3002\u4f20\u7edf\u7684\u6570\u503c\u6700\u5c0f\u4e8c\u4e58\u62df\u5171\u5f62\uff08LSQC\uff09\u7406\u8bba\u867d\u7136\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u6807\u79f0\u6761\u4ef6\u4e14\u4e0d\u80fd\u7528\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8c31\u62c9\u666e\u62c9\u65af\u7f51\u7edc\uff08SBN\uff09\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06LSQC\u80fd\u91cf\u5d4c\u5165\u5230\u591a\u5c3a\u5ea6\u7f51\u683c-\u8c31\u67b6\u6784\u4e2d\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86SBN\u4f18\u5316\u6846\u67b6\uff08SBN-Opt\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u81ea\u7531\u8fb9\u754c\u53ef\u5fae\u540c\u80da\uff0c\u5e76\u663e\u5f0f\u63a7\u5236\u5c40\u90e8\u51e0\u4f55\u7578\u53d8\u3002", "result": "\u901a\u8fc7\u5728\u5bc6\u5ea6\u5747\u8861\u6620\u5c04\u548c\u4e0d\u4e00\u81f4\u8868\u9762\u914d\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86SBN-Opt\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u6570\u503c\u7b97\u6cd5\u3002", "conclusion": "SBN-Opt\u4e3a\u89e3\u51b3\u81ea\u7531\u8fb9\u754c\u53ef\u5fae\u540c\u80da\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u9700\u8981\u663e\u5f0f\u63a7\u5236\u5c40\u90e8\u51e0\u4f55\u7578\u53d8\u7684\u573a\u666f\u4e0b\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.12152", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12152", "abs": "https://arxiv.org/abs/2511.12152", "authors": ["Jianyi Yu", "Yuxuan Wang", "Xiang Fu", "Fei Qiao", "Ying Wang", "Rui Yuan", "Liyuan Liu", "Cong Shi"], "title": "A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation", "comment": null, "summary": "Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b57\u8ba1\u7b97\u5185\u5b58\uff08CIM\uff09\u5b8f\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97Transformer\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u6784\u8ba1\u7b97\u8fc7\u7a0b\u548c\u4f18\u5316\u7535\u8def\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u548c\u9ad8\u9762\u79ef\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709CPU\u3001GPU\u53ca\u5176\u4ed6Transformer-CIM\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u5728\u80fd\u91cf\u6548\u7387\u548c\u6027\u80fd\u4e0a\u6ee1\u8db3AI\u5904\u7406\u5668\u5bf9Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u9700\u6c42\uff0c\u5e76\u89e3\u51b3\u4f20\u7edfCIM\u5904\u7406\u52a8\u6001\u77e9\u9635\u4e58\u6cd5\u4e0d\u9002\u7528\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b57CIM\u5b8f\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\u91cd\u6784\u4e3a\u57fa\u4e8e\u7ec4\u5408QK\u6743\u91cd\u77e9\u9635\u7684\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u8f93\u5165\u76f4\u63a5\u9001\u5165CIM\u5355\u5143\u83b7\u53d6\u5206\u6570\u3002\u540c\u65f6\uff0c\u5c06\u4e8c\u9879\u5f0f\u77e9\u9635\u4e58\u6cd5\u5206\u89e3\u4e3a\u4f4d\u4e32\u884c\u79fb\u4f4d\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c\u5e76\u91c7\u7528\u96f6\u503c\u4f4d\u8df3\u8fc7\u3001\u6570\u636e\u9a71\u52a8\u5b57\u7ebf\u6fc0\u6d3b\u3001\u8bfb\u5199\u5206\u79bb\u76846T\u5355\u5143\u4ee5\u53ca\u4f4d\u4ea4\u66ff\u768414T/28T\u52a0\u6cd5\u5668\u7b49\u4f18\u5316\u6280\u672f\u3002", "result": "\u6240\u63d0\u51fa\u7684CIM\u5b8f\u572865nm\u5de5\u827a\u4e0b\uff0c\u9762\u79ef\u4e3a0.35 mm2\uff0c\u5cf0\u503c\u6027\u80fd\u4e3a42.27 GOPS\uff0c\u529f\u8017\u4e3a1.24 mW\uff081.0 V\u4f9b\u7535\uff0c100 MHz\u65f6\u949f\uff09\uff0c\u80fd\u6548\u6bd4\u4e3a34.1 TOPS/W\uff0c\u9762\u79ef\u6548\u7387\u4e3a120.77 GOPS/mm2\u3002\u4e0eCPU\u548cGPU\u76f8\u6bd4\uff0c\u80fd\u6548\u5206\u522b\u63d0\u9ad8\u4e8625\u500d\u548c13\u500d\u3002\u4e0e\u540c\u6280\u672f\u8282\u70b9\u4e0b\u7684\u5176\u4ed6Transformer-CIMs\u76f8\u6bd4\uff0c\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\u5206\u522b\u63d0\u9ad8\u4e86\u81f3\u5c117\u500d\u548c2\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CIM\u5b8f\u5728Transformer\u6ce8\u610f\u529b\u8ba1\u7b97\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\u4f18\u52bf\uff0c\u5728\u8fb9\u7f18AI\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11897", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11897", "abs": "https://arxiv.org/abs/2511.11897", "authors": ["Shuo Liu", "Wei Xiao", "Calin A. Belta"], "title": "Sampling-Aware Control Barrier Functions for Safety-Critical and Finite-Time Constrained Control", "comment": "8 pages, 4 figures", "summary": "In safety-critical control systems, ensuring both safety and feasibility under sampled-data implementations is crucial for practical deployment. Existing Control Barrier Function (CBF) frameworks, such as High-Order CBFs (HOCBFs), effectively guarantee safety in continuous time but may become unsafe when executed under zero-order-hold (ZOH) controllers due to inter-sampling effects. Moreover, they do not explicitly handle finite-time reach-and-remain requirements or multiple simultaneous constraints, which often lead to conflicts between safety and reach-and-remain objectives, resulting in feasibility issues during control synthesis. This paper introduces Sampling-Aware Control Barrier Functions (SACBFs), a unified framework that accounts for sampling effects and high relative-degree constraints by estimating and incorporating Taylor-based upper bounds on barrier evolution between sampling instants. The proposed method guarantees continuous-time forward invariance of safety and finite-time reach-and-remain sets under ZOH control. To further improve feasibility, a relaxed variant (r-SACBF) introduces slack variables for handling multiple constraints realized through time-varying CBFs. Simulation studies on a unicycle robot demonstrate that SACBFs achieve safe and feasible performance in scenarios where traditional HOCBF methods fail.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u91c7\u6837\u611f\u77e5\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08SACBF\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5b89\u5168\u5173\u952e\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u91c7\u6837\u6570\u636e\u5b9e\u73b0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u96f6\u9636\u4fdd\u6301\uff08ZOH\uff09\u63a7\u5236\u5668\u4e0b\uff0c\u7531\u4e8e\u91c7\u6837\u6548\u5e94\u548c\u591a\u7ea6\u675f\u51b2\u7a81\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6216\u4e0d\u53ef\u884c\u3002", "method": "SACBF\u6846\u67b6\u901a\u8fc7\u4f30\u8ba1\u548c\u5305\u542b\u57fa\u4e8e\u6cf0\u52d2\u5c55\u5f00\u7684\u969c\u788d\u51fd\u6570\u6f14\u5316\u4e0a\u754c\u6765\u8003\u8651\u91c7\u6837\u6548\u5e94\u548c\u9ad8\u76f8\u5bf9\u5ea6\u7ea6\u675f\uff0c\u786e\u4fdd\u4e86\u5728ZOH\u63a7\u5236\u5668\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6709\u9650\u65f6\u95f4\u53ef\u8fbe\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3ar-SACBF\u7684\u677e\u5f1b\u53d8\u4f53\uff0c\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u6765\u5904\u7406\u591a\u7ea6\u675f\u95ee\u9898\u3002", "result": "SACBF\u4fdd\u8bc1\u4e86\u5b89\u5168\u96c6\u548c\u6709\u9650\u65f6\u95f4\u53ef\u8fbe\u96c6\u7684\u8fde\u7eed\u65f6\u95f4\u524d\u5411\u4e0d\u53d8\u6027\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cSACBF\u5728\u4f20\u7edfHOCBF\u65b9\u6cd5\u5931\u6548\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u5b9e\u73b0\u5b89\u5168\u53ef\u884c\u7684\u6027\u80fd\u3002", "conclusion": "SACBF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u91c7\u6837\u6548\u5e94\u548c\u591a\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u5173\u952e\u63a7\u5236\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2511.11634", "categories": ["cs.RO", "cs.CV", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.11634", "abs": "https://arxiv.org/abs/2511.11634", "authors": ["Michikuni Eguchi", "Takekazu Kitagishi", "Yuichi Hiroi", "Takefumi Hiraki"], "title": "Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding", "comment": "3 pages, 2 figures, 1 table. Presented at SIGGRAPH Asia 2025 Posters (SA Posters '25), December 15-18, 2025, Hong Kong, Hong Kong", "summary": "The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u624b\u81c2\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u6536\u96c6\u670d\u88c5\u7684\u89e6\u89c9\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u8bc1\u660e\u4e86\u8fd0\u52a8\u76f8\u5173\u6807\u7b7e\u5bf9\u8868\u5f81\u670d\u88c5\u89e6\u89c9\u611f\u77e5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63ed\u793a\u4f7f\u670d\u88c5\u8212\u9002\u7684\u7269\u7406\u7279\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u5730\u6536\u96c6\u6ed1\u52a8\u8fc7\u7a0b\u4e2d\u7684\u89e6\u89c9\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u4eba\u624b\u81c2\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528\u6a21\u62df\u6307\u5c16\u8fdb\u884c\u626b\u63a0\u6d4b\u91cf\uff0c\u5e76\u7cbe\u786e\u63a7\u5236\u901f\u5ea6\u548c\u65b9\u5411\uff0c\u4ee5\u521b\u5efa\u5e26\u8fd0\u52a8\u6807\u7b7e\u7684\u591a\u6a21\u6001\u89e6\u89c9\u6570\u636e\u5e93\u3002", "result": "\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u663e\u793a\uff0c\u52a0\u5165\u8fd0\u52a8\u76f8\u5173\u53c2\u6570\u80fd\u591f\u63d0\u9ad8\u97f3\u9891\u548c\u52a0\u901f\u5ea6\u6570\u636e\u7684\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u8fd0\u52a8\u76f8\u5173\u6807\u7b7e\u5728\u8868\u5f81\u670d\u88c5\u89e6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u975e\u7834\u574f\u6027\u7684\u670d\u88c5\u89e6\u89c9\u6570\u636e\u6355\u83b7\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7ec7\u7269\u611f\u77e5\u548c\u518d\u73b0\u7684\u7814\u7a76\u3002"}}
{"id": "2511.12221", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12221", "abs": "https://arxiv.org/abs/2511.12221", "authors": ["Qin-Sheng Zhu", "Geng Chen", "Lian-Hui Yu", "Xiaodong Xing", "Xiao-Yu Li"], "title": "Channel-Constrained Markovian Quantum Diffusion Model from Open System Perspective", "comment": "38 pages, 10 figures", "summary": "We present a channel-constrained Markovian quantum diffusion (CCMQD) model that prepares quantum states by rigorously framing the generative process within the dynamics of open quantum systems. Our model interprets the forward diffusion process as natural decoherence using quantum master equations, whereas the reverse denoising is achieved by learning inverse quantum channels. Our core innovation is a comprehensive channel-constrained framework: we model the diffusion and denoising steps as quantum channels defined by Kraus operators, ensure their physical validity through optimization on the Stiefel manifold, and introduce tailored training strategies and loss functions that leverage this constrained structure for high-fidelity state reconstruction. Experimental validation on systems ranging from single qubits to entangled states $7$ -qubits demonstrates high-fidelity state generation, achieving fidelities exceeding $0.998$ under both random and depolarizing noise conditions. This work confirms that quantum diffusion can be characterized as a controlled Markov evolution, demonstrating that environmental interactions are not limited to being a source of decoherence but can also be utilized to achieve high-fidelity quantum state synthesis.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u9053\u7ea6\u675f\u7684\u9a6c\u5c14\u53ef\u592b\u91cf\u5b50\u6269\u6563\uff08CCMQD\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u4e25\u683c\u6846\u5b9a\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u4e2d\u6765\u5236\u5907\u91cf\u5b50\u6001\u3002", "motivation": "\u8be5\u6a21\u578b\u5c06\u524d\u5411\u6269\u6563\u8fc7\u7a0b\u89e3\u91ca\u4e3a\u4f7f\u7528\u91cf\u5b50\u4e3b\u65b9\u7a0b\u7684\u81ea\u7136\u9000\u76f8\u5e72\uff0c\u800c\u53cd\u5411\u53bb\u566a\u5219\u901a\u8fc7\u5b66\u4e60\u53cd\u5411\u91cf\u5b50\u901a\u9053\u6765\u5b9e\u73b0\u3002", "method": "\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u4e00\u4e2a\u5168\u9762\u7684\u901a\u9053\u7ea6\u675f\u6846\u67b6\uff1a\u5c06\u6269\u6563\u548c\u53bb\u566a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u7531Kraus\u7b97\u7b26\u5b9a\u4e49\u7684\u91cf\u5b50\u901a\u9053\uff0c\u901a\u8fc7\u5728Stiefel\u6d41\u5f62\u4e0a\u8fdb\u884c\u4f18\u5316\u6765\u786e\u4fdd\u5176\u7269\u7406\u6709\u6548\u6027\uff0c\u5e76\u5f15\u5165\u5b9a\u5236\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5229\u7528\u8fd9\u79cd\u7ea6\u675f\u7ed3\u6784\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u72b6\u6001\u91cd\u5efa\u3002", "result": "\u5728\u4ece\u5355\u91cf\u5b50\u6bd4\u7279\u52307\u91cf\u5b50\u6bd4\u7279\u7ea0\u7f20\u6001\u7684\u7cfb\u7edf\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5728\u9ad8\u4fdd\u771f\u5ea6\u7684\u72b6\u6001\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5728\u968f\u673a\u548c\u9000\u706b\u566a\u58f0\u6761\u4ef6\u4e0b\u4fdd\u771f\u5ea6\u5747\u8d85\u8fc70.998\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u5b9e\u4e86\u91cf\u5b50\u6269\u6563\u53ef\u4ee5\u8868\u5f81\u4e3a\u53d7\u63a7\u7684\u9a6c\u5c14\u53ef\u592b\u6f14\u5316\uff0c\u5e76\u8868\u660e\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u4e0d\u4ec5\u662f\u9000\u76f8\u5e72\u7684\u6765\u6e90\uff0c\u8fd8\u53ef\u4ee5\u7528\u4e8e\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cf\u5b50\u6001\u5408\u6210\u3002"}}
{"id": "2511.13614", "categories": ["cs.MA", "cs.CE", "q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2511.13614", "abs": "https://arxiv.org/abs/2511.13614", "authors": ["Jerick Shi", "Burton Hollifield"], "title": "Market-Dependent Communication in Multi-Agent Alpha Generation", "comment": null, "summary": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.", "AI": {"tldr": "\u901a\u4fe1\u80fd\u63d0\u9ad8\u5bf9\u51b2\u57fa\u91d1\u7684\u8868\u73b0\uff0c\u4f46\u6700\u4f73\u901a\u4fe1\u65b9\u5f0f\u53d6\u51b3\u4e8e\u5e02\u573a\u7279\u5f81\u3002\u7ade\u4e89\u6027\u5bf9\u8bdd\u9002\u7528\u4e8e\u6ce2\u52a8\u6027\u5927\u7684\u79d1\u6280\u80a1\uff0c\u534f\u4f5c\u6027\u5bf9\u8bdd\u9002\u7528\u4e8e\u7a33\u5b9a\u7684\u4e00\u822c\u80a1\u7968\uff0c\u800c\u91d1\u878d\u80a1\u5219\u5bf9\u6240\u6709\u901a\u4fe1\u5e72\u9884\u90fd\u4e0d\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u5bf9\u51b2\u57fa\u91d1\u4e2d\u4ea4\u6613\u7b56\u7565\u5206\u6790\u5e08\u4e4b\u95f4\u7684\u901a\u4fe1\u65b9\u5f0f\u9009\u62e9\uff0c\u4ee5\u53ca\u4e0d\u540c\u901a\u4fe1\u7ed3\u6784\u5bf9\u57fa\u91d1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u76845\u4e2a\u4ee3\u7406\u4ea4\u6613\u7cfb\u7edf\u8fdb\u884c450\u6b21\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4ece\u5b64\u7acb\u57fa\u7ebf\u5230\u534f\u4f5c\u548c\u7ade\u4e89\u6027\u5bf9\u8bdd\u7684\u4e94\u79cd\u7ec4\u7ec7\u7ed3\u6784\u3002", "result": "\u901a\u4fe1\u80fd\u63d0\u9ad8\u4e1a\u7ee9\uff0c\u4f46\u6700\u4f73\u901a\u4fe1\u8bbe\u8ba1\u53d6\u51b3\u4e8e\u5e02\u573a\u7279\u5f81\u3002\u7ade\u4e89\u6027\u5bf9\u8bdd\u5728\u6ce2\u52a8\u6027\u5927\u7684\u79d1\u6280\u80a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u534f\u4f5c\u6027\u5bf9\u8bdd\u5728\u7a33\u5b9a\u7684\u666e\u901a\u80a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u91d1\u878d\u80a1\u5bf9\u6240\u6709\u901a\u4fe1\u5e72\u9884\u90fd\u4e0d\u654f\u611f\u3002\u6240\u6709\u7ed3\u6784\uff08\u5305\u62ec\u5b64\u7acb\u4ee3\u7406\uff09\u90fd\u8d8b\u5411\u4e8e\u76f8\u4f3c\u7684\u7b56\u7565\u3002\u4e1a\u7ee9\u5dee\u5f02\u6e90\u4e8e\u884c\u4e3a\u673a\u5236\uff1a\u7ade\u4e89\u6027\u4ee3\u7406\u5173\u6ce8\u80a1\u7968\u5206\u914d\uff0c\u534f\u4f5c\u6027\u4ee3\u7406\u5f00\u53d1\u6280\u672f\u6846\u67b6\u3002\u5bf9\u8bdd\u8d28\u91cf\u4e0e\u56de\u62a5\u4e0d\u76f8\u5173\u3002", "conclusion": "\u6700\u4f18\u901a\u4fe1\u8bbe\u8ba1\u5fc5\u987b\u4e0e\u5e02\u573a\u6ce2\u52a8\u6027\u7279\u5f81\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u590d\u6742\u7684\u8ba8\u8bba\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u66f4\u597d\u7684\u8868\u73b0\u3002"}}
{"id": "2511.12647", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2511.12647", "abs": "https://arxiv.org/abs/2511.12647", "authors": ["Thales F. Macedo", "Juli\u00e1n Fa\u00fandez", "Ant\u00f4nio S. Coelho", "Caio Lewenkopf", "Mauro S. Ferreira", "Felipe A. Pinheiro", "Natanael C. Costa"], "title": "Inverse determination of light-matter coupling in disordered systems from transmittance spectra", "comment": "13 pages, 9 figures", "summary": "We investigate quantum inverse problems in one-dimensional (1D) electronic disordered systems strongly coupled to optical cavities. More specifically, we consider the Anderson and the Aubry-Andre-Harper models connected to electronic reservoirs and embedded in a single-mode optical cavity. The light-matter interaction enables photon-assisted hopping processes that significantly modify the transmittance spectrum. Within the nonequilibrium Green's function formalism, we implement an inversion-based approach capable of accurately extracting the electron-photon coupling strength directly from transmittance spectra. While cavity coupling acts as a minor perturbation within the Anderson model, yielding broad yet precise parameter estimates, its influence is markedly different in the Aubry-Andr\u00e9-Harper model. The latter exhibits a sharp metal-insulator transition in 1D, thus resulting in more pronounced cavity-induced spectral changes. This renders even more accurate inverse solutions, offering unparalleled precision in the characterization of low-dimensional disordered systems. Altogether, our results demonstrate that the quantum inverse problem provides a robust diagnostic tool for quantum materials, particularly effective for systems exhibiting metal-insulator transitions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u8026\u5408\u5230\u5149\u5b66\u8154\u7684\u4e00\u7ef4\u7535\u5b50\u65e0\u5e8f\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u9006\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u900f\u5c04\u5149\u8c31\u4e2d\u63d0\u53d6\u7535\u5b50-\u5149\u5b50\u8026\u5408\u5f3a\u5ea6\u7684\u9006\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5f3a\u8026\u5408\u5230\u5149\u5b66\u8154\u7684\u4e00\u7ef4\u7535\u5b50\u65e0\u5e8f\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u9006\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4ece\u900f\u5c04\u5149\u8c31\u4e2d\u63d0\u53d6\u7535\u5b50-\u5149\u5b50\u8026\u5408\u5f3a\u5ea6\u7684\u9006\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u975e\u5e73\u8861\u683c\u6797\u51fd\u6570\u5f62\u5f0f\u4e3b\u4e49\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u6f14\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u7535\u5b50-\u5149\u5b50\u8026\u5408\u5f3a\u5ea6\u3002", "result": "\u5b89\u5fb7\u68ee\u6a21\u578b\u4e2d\u7684\u8154\u8026\u5408\u4f5c\u7528\u8f83\u5c0f\uff0c\u53c2\u6570\u4f30\u8ba1\u5bbd\u6cdb\u4f46\u7cbe\u786e\uff1b\u800c\u5965\u5e03\u91cc-\u5b89\u5fb7\u70c8-\u54c8\u73c0\u6a21\u578b\u4e2d\uff0c\u8154\u8026\u5408\u5bfc\u81f4\u4e86\u66f4\u663e\u8457\u7684\u5149\u8c31\u53d8\u5316\uff0c\u4f7f\u5f97\u53cd\u6f14\u89e3\u66f4\u52a0\u7cbe\u786e\u3002", "conclusion": "\u91cf\u5b50\u9006\u95ee\u9898\u4e3a\u91cf\u5b50\u6750\u6599\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5bf9\u4e8e\u8868\u73b0\u51fa\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\u7684\u7cfb\u7edf\u5c24\u5176\u6709\u6548\u3002"}}
{"id": "2511.12420", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12420", "abs": "https://arxiv.org/abs/2511.12420", "authors": ["Fengyu Xie", "Ruoyu Wang", "Taoyuze Lv", "Yuxiang Gao", "Hongyu Wu", "Zhicheng Zhong"], "title": "Chemical-space completeness: a new strategy for crystalline materials exploration", "comment": "23 pages, 7 figures", "summary": "The emergence of deep learning has brought the long-standing goal of comprehensively understanding and exploring crystalline materials closer to reality. Yet, universal exploration across all elements remains hindered by the combinatorial explosion of possible chemical environments, making it difficult to balance accuracy and efficiency. Crucially, within any finite set of elements, the diversity of short-range bonding types and local geometric motifs is inherently limited. Guided by this chemical intuition, we propose a chemical-system-centric strategy for crystalline materials exploration. In this framework, generative models are coupled with machine-learned force fields as fast energy evaluators, and both are iteratively refined in a closed-loop cycle of generation, evaluation, and fine-tuning. Using the Li-P-S ternary system as a case study, we show that this approach captures the diversity of local environments with minimal additional first-principles data while maintaining structural creativity, achieving closed-loop convergence toward chemical completeness within a bounded chemical space. We further demonstrate downstream applications, including phase-diagram construction, ionic-diffusivity screening, and electronic-structure prediction. Together, this strategy provides a systematic and data-efficient framework for modeling both atomistic and electronic structures within defined chemical spaces, bridging accuracy and efficiency, and paving the way toward scalable, AI-driven discovery of crystalline materials with human-level creativity and first-principles fidelity.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u4ecd\u53d7\u9650\u4e8e\u7ec4\u5408\u7206\u70b8\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ee5\u5316\u5b66\u7cfb\u7edf\u4e3a\u4e2d\u5fc3\u3001\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u529b\u573a\u7684\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eLi-P-S\u4f53\u7cfb\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u9ad8\u6548\u7684\u95ed\u73af\u6536\u655b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u76f8\u56fe\u6784\u5efa\u3001\u79bb\u5b50\u6269\u6563\u7b5b\u9009\u548c\u7535\u5b50\u7ed3\u6784\u9884\u6d4b\u7b49\u65b9\u9762\u7684\u5e94\u7528\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u7406\u89e3\u548c\u63a2\u7d22\u6676\u4f53\u6750\u6599\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8de8\u5143\u7d20\u63a2\u7d22\u53d7\u9650\u4e8e\u7ec4\u5408\u7206\u70b8\uff0c\u96be\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5316\u5b66\u76f4\u89c9\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u5316\u5b66\u7cfb\u7edf\u4e3a\u4e2d\u5fc3\u7684\u7b56\u7565\uff0c\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u673a\u5668\u5b66\u4e60\u529b\u573a\u7ed3\u5408\uff0c\u901a\u8fc7\u751f\u6210\u3001\u8bc4\u4f30\u548c\u5fae\u8c03\u7684\u95ed\u73af\u5faa\u73af\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u4ee5\u5feb\u901f\u8bc4\u4f30\u80fd\u91cf\u3002", "result": "\u5728Li-P-S\u4e09\u5143\u4f53\u7cfb\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u5c0f\u5316\u7b2c\u4e00\u6027\u539f\u7406\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6355\u6349\u5230\u4e86\u5c40\u90e8\u73af\u5883\u7684\u591a\u6837\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u7ed3\u6784\u7684\u521b\u9020\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u6709\u754c\u5316\u5b66\u7a7a\u95f4\u5185\u5411\u5316\u5b66\u5b8c\u6574\u6027\u7684\u95ed\u73af\u6536\u655b\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u76f8\u56fe\u6784\u5efa\u3001\u79bb\u5b50\u6269\u6563\u7b5b\u9009\u548c\u7535\u5b50\u7ed3\u6784\u9884\u6d4b\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "\u8be5\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b9a\u4e49\u7684\u5316\u5b66\u7a7a\u95f4\u5185\u5bf9\u539f\u5b50\u548c\u7535\u5b50\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u3001\u5177\u6709\u4eba\u7c7b\u6c34\u5e73\u521b\u9020\u529b\u548c\u7b2c\u4e00\u6027\u539f\u7406\u4fdd\u771f\u5ea6\u7684\u6676\u4f53\u6750\u6599\u53d1\u73b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.11725", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11725", "abs": "https://arxiv.org/abs/2511.11725", "authors": ["Zekai Shi", "Zhixi Cai", "Kalin Stefanov"], "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video", "comment": null, "summary": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.", "AI": {"tldr": "\u513f\u7ae5\u901a\u8fc7\u5c06\u53e3\u8bed\u4e0e\u89c6\u89c9\u6307\u4ee3\u7269\u8054\u7cfb\u8d77\u6765\u5b66\u4e60\u8bcd\u8bed\uff0c\u4f46\u65b0\u8bcd\u7684\u7406\u89e3\u5b58\u5728\u6b67\u4e49\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u3001\u751f\u7269\u5b66\u53ef\u884c\u7684\u7b56\u7565\uff0c\u5229\u7528\u5177\u6709\u4eba\u7c7b\u89c6\u89c9\u76f2\u70b9\u77e5\u8bc6\u7684\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u6765\u5b66\u4e60\u89c6\u89c9\u8868\u5f81\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a9\u7801\u548c\u91cd\u5efa\u6765\u6a21\u4eff\u4eba\u8111\u7684\u89c6\u89c9\u4fe1\u606f\u586b\u5145\u673a\u5236\uff0c\u5e76\u5e94\u7528\u4e8e\u89c6\u9891-\u6587\u672c\u6a21\u578b\u4ee5\u5b66\u4e60\u8bcd\u8bed-\u6307\u4ee3\u7269\u6620\u5c04\u3002", "motivation": "\u89e3\u51b3\u513f\u7ae5\u5728\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u4ece\u6a21\u7cca\u7684\u8f93\u5165\u4e2d\u5b66\u4e60\u8bcd\u8bed\u4e0e\u89c6\u89c9\u6307\u4ee3\u7269\u4e4b\u95f4\u6620\u5c04\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u751f\u7269\u5b66\u4e0a\u53ef\u884c\u7684\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89c6\u89c9\u4e3b\u5e72\u7f51\u7edc\uff0c\u5176\u63a9\u7801\u7b56\u7565\u7ed3\u5408\u4e86\u4eba\u7c7b\u773c\u775b\u7684\u76f2\u70b9\u77e5\u8bc6\uff0c\u6a21\u4eff\u4eba\u8111\u7684\u89c6\u89c9\u4fe1\u606f\u586b\u5145\u673a\u5236\u3002\u7136\u540e\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u5e94\u7528\u4e8e\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u89c6\u9891-\u6587\u672c\u6a21\u578b\uff0c\u4ee5\u5b66\u4e60\u8bcd\u8bed-\u6307\u4ee3\u7269\u6620\u5c04\u3002", "result": "\u6240\u63d0\u51fa\u7684\u751f\u7269\u5b66\u63a9\u7801\u7b56\u7565\u5728\u5b66\u4e60\u8bcd\u8bed-\u6307\u4ee3\u7269\u6620\u5c04\u65b9\u9762\uff0c\u4e0e\u968f\u673a\u63a9\u7801\u7b56\u7565\u7684\u6548\u679c\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\uff08\u5982\u76f2\u70b9\u5904\u7406\uff09\u7684\u751f\u7269\u5b66\u7b56\u7565\uff0c\u662f\u5b66\u4e60\u5f3a\u5927\u89c6\u89c9\u8868\u5f81\u5e76\u6700\u7ec8\u5b9e\u73b0\u8bcd\u8bed-\u6307\u4ee3\u7269\u6620\u5c04\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.13330", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.13330", "abs": "https://arxiv.org/abs/2511.13330", "authors": ["Dhilan Nag", "Suhun Kim", "Cole Johnson", "Collin Sumrell"], "title": "HPC-Accelerated Simulation and Calibration for Silicon Quantum Dots", "comment": "35th IEEE International Conference on Collaborative Advances in Software and Computing (CASCON 2025)", "summary": "Quantum computers (QCs) have the potential to solve critical problems significantly faster than today's most advanced supercomputers. One major challenge in realizing this technology is designing robust electrostatic pulses to realize unitaries on qubits. Current practice when calibrating unitaries involves recursive experimentation to find the highest-fidelity pulses. To accelerate this process for experimentalists, we implement Qalibrate, a fast, JAX-enabled simulator that generates pulses given target unitaries. Specifically, we generate a propagator that models the time evolution of three-electron spin qubits and integrate our gradient-based optimizer to generate the pulses. The simulation involves solving the Lindblad master equation, which we parallelize by employing an approximation of the time evolution called the Magnus expansion. Qalibrate shows up to a 34x speedup compared to an existing ODE simulator, making progress towards generating robust pulses for n-qubit systems.", "AI": {"tldr": "Qalibrate\u662f\u4e00\u4e2aJAX\u52a0\u901f\u7684\u6a21\u62df\u5668\uff0c\u53ef\u4ee5\u4e3a\u91cf\u5b50\u6bd4\u7279\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u8109\u51b2\uff0c\u5176\u901f\u5ea6\u6bd4\u73b0\u6709\u6a21\u62df\u5668\u5feb34\u500d\u3002", "motivation": "\u8bbe\u8ba1\u7528\u4e8e\u5b9e\u73b0\u91cf\u5b50\u6bd4\u7279\u4e0a\u5e7a\u6b63\u53d8\u6362\u7684\u9c81\u68d2\u9759\u7535\u8109\u51b2\u662f\u5b9e\u73b0\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u5bf9\u4e8e\u5b9e\u9a8c\u8005\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528JAX\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aQalibrate\u7684\u5feb\u901f\u6a21\u62df\u5668\uff0c\u5e76\u751f\u6210\u4e86\u4e00\u4e2a\u6a21\u62df\u4e09\u7535\u5b50\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u65f6\u95f4\u6f14\u5316\u7684\u4f20\u64ad\u5668\u3002\u901a\u8fc7\u4f7f\u7528Magnus\u5c55\u5f00\u5f0f\u6765\u8fd1\u4f3c\u65f6\u95f4\u6f14\u5316\u5e76\u89e3\u51b3Lindblad\u4e3b\u65b9\u7a0b\uff0c\u53ef\u4ee5\u5e76\u884c\u5316\u6a21\u62df\u8fc7\u7a0b\u3002", "result": "Qalibrate\u5728\u751f\u6210\u8109\u51b2\u65b9\u9762\u6bd4\u73b0\u6709\u7684ODE\u6a21\u62df\u5668\u5feb\u4e8634\u500d\u3002", "conclusion": "Qalibrate\u7684\u5f00\u53d1\u548c\u5b9e\u73b0\u4e3a\u751f\u6210\u9c81\u68d2\u7684n\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u8109\u51b2\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u6709\u671b\u52a0\u901f\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.11602", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.11602", "abs": "https://arxiv.org/abs/2511.11602", "authors": ["Georgios C. Chasparis"], "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games", "comment": null, "summary": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u652f\u4ed8\u9a71\u52a8\u7684\u5b66\u4e60\u7b97\u6cd5APLA\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\u3002\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cAPLA\u8003\u8651\u4e86\u73a9\u5bb6\u7684\u671f\u671b\u6c34\u5e73\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8\u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u65f6\u7684\u6536\u655b\u6027\u3002\u8be5\u7814\u7a76\u8fd8\u901a\u8fc7\u5c06\u65e0\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0e\u6709\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e3a\u901a\u7528\u975e\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u65b9\u5f31\u5faa\u73af\u535a\u5f08\u4e2d\uff0c\u72ec\u7acb\u5b66\u4e60\u52a8\u6001\u53ef\u80fd\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7eaf\u7eb3\u4ec0\u5747\u8861\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u9650\u4e8e\u4e00\u5c0f\u7c7b\u535a\u5f08\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u652f\u4ed8\u9a71\u52a8\u7684\u5b66\u4e60\u65b9\u6848\uff0c\u79f0\u4e3a\u201c\u57fa\u4e8e\u671f\u671b\u7684\u53d7\u6270\u5b66\u4e60\u81ea\u52a8\u673a\u201d\uff08APLA\uff09\u3002\u5728\u8be5\u65b9\u6848\u4e2d\uff0c\u73a9\u5bb6\u9009\u62e9\u52a8\u4f5c\u7684\u6982\u7387\u5206\u5e03\u4e0d\u4ec5\u53d7\u5230\u91cd\u590d\u9009\u62e9\u7684\u5f3a\u5316\uff0c\u8fd8\u53d7\u5230\u6355\u83b7\u73a9\u5bb6\u6ee1\u610f\u5ea6\u6c34\u5e73\u7684\u671f\u671b\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u65b9\u6b63\u6548\u7528\u535a\u5f08\u4e2d\uff0c\u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9APLA\u8fdb\u884c\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002\u8be5\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u8bf1\u5bfc\u7684\u65e0\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0e\u6709\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b49\u4ef7\u6027\uff0c\u9996\u6b21\u8868\u5f81\u4e86\u901a\u7528\u975e\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u3002", "conclusion": "APLA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u652f\u4ed8\u9a71\u52a8\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u826f\u597d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u901a\u7528\u975e\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2511.12073", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12073", "abs": "https://arxiv.org/abs/2511.12073", "authors": ["Woojae Jeong", "Wenhui Cui", "Kleanthis Avramidis", "Takfarinas Medani", "Shrikanth Narayanan", "Richard Leahy"], "title": "Informed Bootstrap Augmentation Improves EEG Decoding", "comment": null, "summary": "Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at https://github.com/lyricists/NeuroBootstrap.", "AI": {"tldr": "\u52a0\u6743\u5f15\u5bfc\u6cd5\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u66f4\u53ef\u9760\u7684\u6837\u672c\u6765\u63d0\u9ad8EEG\u4fe1\u53f7\u7684\u89e3\u7801\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684EEG\u4fe1\u53f7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u5982\u5747\u5300\u5e73\u5747\uff09\u5ffd\u7565\u4e86\u4e0d\u540c\u8bd5\u9a8c\u4fe1\u606f\u91cf\u7684\u5dee\u5f02\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ee3\u8868\u6027\u8d28\u91cf\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u9ad8EEG\u4fe1\u53f7\u7684\u89e3\u7801\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u52a0\u6743\u5f15\u5bfc\uff08weighted bootstrapping\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bd5\u9a8c\u7684\u53ef\u9760\u6027\u5206\u914d\u6743\u91cd\uff0c\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u91cf\u66f4\u591a\u7684\u8bd5\u9a8c\u6765\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u589e\u5f3a\u6837\u672c\u3002\u5728\u53e5\u5b50\u8bc4\u4f30\u8303\u5f0f\u4e2d\uff0c\u8bd5\u9a8c\u6743\u91cd\u57fa\u4e8e\u76f8\u5bf9ERP\u5dee\u5f02\u8ba1\u7b97\uff0c\u5e76\u7528\u4e8e\u6982\u7387\u62bd\u6837\u548c\u5e73\u5747\u3002", "result": "\u4e0e\u672a\u52a0\u6743\u7684\u5f15\u5bfc\u6cd5\u76f8\u6bd4\uff0c\u52a0\u6743\u5f15\u5bfc\u6cd5\u5728\u53e5\u5b50\u8bc4\u4f30\u8303\u5f0f\u4e2d\u7684\u89e3\u7801\u7cbe\u5ea6\u6709\u6240\u63d0\u9ad8\uff08\u6700\u4f73\u60c5\u51b5\u4ece68.35%\u63d0\u9ad8\u523071.25%\uff09\uff0c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u589e\u5f3a\u4ee3\u8868\u6027\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u533a\u5206\u6027\u7684EEG\u4fe1\u53f7\u4ee3\u8868\u3002"}}
{"id": "2511.11612", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11612", "abs": "https://arxiv.org/abs/2511.11612", "authors": ["Aasish Kumar Sharma", "Julian Kunkel"], "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems", "comment": "14 pages, 4 figures, 2 tables. Evaluation study on LLM-based reasoning for HPC scheduling. Published in Research in Academic Engineering Journal (RAEJ), 2025", "summary": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.", "AI": {"tldr": "LLM\u5728\u89e3\u51b3\u8ba1\u7b97\u4f18\u5316\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u7cbe\u786e\u6027\u548c\u7ea6\u675f\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u5904\u7406\u57fa\u4e8e\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff08\u7279\u522b\u662fHPC\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u548c\u8c03\u5ea6\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5305\u542b\u7cfb\u7edf\u8282\u70b9\u3001\u4efb\u52a1\u9700\u6c42\u548c\u8c03\u5ea6\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u8bc4\u4f30\u4e8621\u4e2aLLM\u5728HPC\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u548c\u8c03\u5ea6\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u624b\u52a8\u8ba1\u7b97\u7684\u6700\u4f18\u89e3\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "3\u4e2aLLM\u5b8c\u7f8e\u590d\u73b0\u4e86\u6700\u4f18\u89e3\uff0c12\u4e2aLLM\u7684\u7ed3\u679c\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c6\u4e2aLLM\u5219\u51fa\u73b0\u4e86\u9519\u8bef\u3002\u6240\u6709\u6a21\u578b\u90fd\u751f\u6210\u4e86\u53ef\u884c\u7684\u4efb\u52a1\u6620\u5c04\uff0c\u4f46\u53ea\u6709\u7ea6\u4e00\u534a\u7684\u6a21\u578b\u4e25\u683c\u9075\u5b88\u4e86\u6240\u6709\u7ea6\u675f\u300219\u4e2a\u6a21\u578b\u751f\u6210\u4e86\u90e8\u5206\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u4ee3\u7801\uff0c18\u4e2a\u6a21\u578b\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "LLM\u5728\u7ec4\u5408\u4f18\u5316\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u91cd\u5efa\u6700\u4f18\u8c03\u5ea6\uff0c\u4f46\u5927\u591a\u6570\u6a21\u578b\u5728\u7cbe\u786e\u8ba1\u65f6\u3001\u6570\u636e\u4f20\u8f93\u8ba1\u7b97\u548c\u4f9d\u8d56\u5173\u7cfb\u5f3a\u5236\u6267\u884c\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002LLM\u66f4\u9002\u5408\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u800c\u975e\u72ec\u7acb\u7684\u4f18\u5316\u6c42\u89e3\u5668\u3002"}}
{"id": "2511.12474", "categories": ["cs.CV", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12474", "abs": "https://arxiv.org/abs/2511.12474", "authors": ["Chucheng Xiang", "Ruchao Bao", "Biyin Feng", "Wenzheng Wu", "Zhongyuan Liu", "Yirui Guan", "Ligang Liu"], "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout", "comment": null, "summary": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u57fa\u4e8e\u7f51\u683c\u7684\u6574\u6570\u89c4\u5212\u7684\u65b0\u578b\u81ea\u52a8\u5316\u5ba4\u5185\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u623f\u95f4\u5e03\u5c40\u548c\u5bb6\u5177\u6446\u653e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e24\u9636\u6bb5\u8bbe\u8ba1\u6d41\u7a0b\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "1. LLM\u9a71\u52a8\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u63d0\u53d6\u8bbe\u8ba1\u7ea6\u675f\u30022. \u91c7\u7528\u53d7\u201cModulor\u201d\u542f\u53d1\u7684\u7edf\u4e00\u7f51\u683c\u8868\u793a\u30023. \u8003\u8651\u4e86\u8d70\u5eca\u8fde\u901a\u6027\u3001\u623f\u95f4\u53ef\u8fbe\u6027\u3001\u7a7a\u95f4\u6392\u4ed6\u6027\u548c\u7528\u6237\u504f\u597d\u7b49\u8bbe\u8ba1\u8981\u6c42\u30024. \u91c7\u7528\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u4f18\u5316\u7b56\u7565\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u5404\u79cd\u573a\u666f\u4e0b\uff0c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7b56\u7565\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u5ba4\u5185\u8bbe\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7387\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.12286", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12286", "abs": "https://arxiv.org/abs/2511.12286", "authors": ["Khyati Kiyawat", "Zhenxing Fan", "Yasas Seneviratne", "Morteza Baradaran", "Akhil Shekar", "Zihan Xia", "Mingu Kang", "Kevin Skadron"], "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.", "AI": {"tldr": "LLMs \u5185\u5b58\u74f6\u9888\u901a\u8fc7\u57fa\u4e8e chiplet \u7684 PIM \u5185\u5b58\u6a21\u5757 Sangam \u89e3\u51b3\uff0c\u8be5\u6a21\u5757\u5728 LLaMA\u3001Mistral \u548c LLaMA 3 \u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "LLMs \u968f\u7740\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u5185\u5b58\u6d88\u8017\u6025\u5267\u589e\u957f\uff0c\u5bfc\u81f4\u5185\u5b58\u5e26\u5bbd\u6210\u4e3a\u63a8\u7406\u74f6\u9888\u3002\u73b0\u6709 PIM \u89e3\u51b3\u65b9\u6848\u56e0\u96c6\u6210\u6210\u672c\u548c\u6280\u672f\u9650\u5236\u800c\u53d7\u635f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e chiplet \u7684\u5185\u5b58\u6a21\u5757 Sangam\uff0c\u5c06\u903b\u8f91\u548c\u5185\u5b58\u5206\u79bb\u5230\u5f02\u6784\u6280\u672f\u8282\u70b9\u4e2d\uff0c\u5e76\u901a\u8fc7\u4e92\u8fde\u5668\u8fde\u63a5\u3002\u8be5\u6a21\u5757\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u5904\u7406\u7ec4\u4ef6\uff0c\u5982 systolic arrays \u548c\u57fa\u4e8e SRAM \u7684\u7f13\u51b2\u533a\uff0c\u4ee5\u52a0\u901f\u5185\u5b58\u5bc6\u96c6\u578b GEMM \u5185\u6838\u3002", "result": "Sangam \u5728 LLaMA 2-7B\u3001Mistral-7B \u548c LLaMA 3-70B \u4e0a\uff0c\u4e0e H100 GPU \u76f8\u6bd4\uff0c\u5728\u67e5\u8be2\u5ef6\u8fdf\u65b9\u9762\u5b9e\u73b0\u4e86 3.93x\u30014.22x \u548c 2.82x \u7684\u52a0\u901f\uff0c\u5728\u89e3\u7801\u541e\u5410\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86 10.3x\u30019.5x \u548c 6.36x \u7684\u52a0\u901f\uff0c\u5e76\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u8282\u80fd\u3002", "conclusion": "Sangam \u4f5c\u4e3a\u4e00\u4e2a CXL \u8fde\u63a5\u7684 PIM chiplet \u5185\u5b58\u6a21\u5757\uff0c\u901a\u8fc7\u89e3\u51b3\u73b0\u6709 PIM \u67b6\u6784\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 LLMs \u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u5e76\u53ef\u4f5c\u4e3a GPU \u7684\u66ff\u4ee3\u54c1\u6216\u4e0e\u5176\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2511.12029", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12029", "abs": "https://arxiv.org/abs/2511.12029", "authors": ["Nicholas Tetteh Ofoe", "Weilun Wang", "Lei Wu"], "title": "On The Detection of Minimum Forecast Horizon For Real-Time Scheduling of Energy Storage Systems in Smart Grid", "comment": null, "summary": "The increasing integration of energy storage systems (ESSs) into power grids has necessitated effective real-time control strategies under uncertain and volatile electricity prices. An important problem of model predictive control of ESSs is identifying the minimum forecast horizon needed to exactly simulate the globally optimal control trajectory. Existing methods in the literature provide only sufficient conditions and might ignore real-world inconsistencies in control actions. In this paper, we introduce a trajectory-alignment-based definition of the minimum forecast horizon and propose an algorithm that identifies the minimum planning horizon for which all rolling-horizon control decisions match those of the full-horizon global optimization. Using real price data from the bidding zone DK1 in Denmark of the Nord Pool day-ahead market and a realistic ESS model, we illustrate that $60$ hours of forecast horizon allows us to exactly simulate the global control sequence and economic outcomes. In addition, we illustrate that under other parameter configurations, no forecast horizon ensures full convergence, demonstrating the sensitivity of the existence of a forecast horizon to various parameters. Our findings provide an operationally significant framework for minimum forecast horizon detection in storage scheduling and pave the way for the analytical description of this important planning measure.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u5bf9\u9f50\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u5b9a\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\u6765\u8bc6\u522b\u6700\u5c0f\u89c4\u5212\u65f6\u57df\uff0c\u4f7f\u6240\u6709\u6eda\u52a8\u65f6\u57df\u63a7\u5236\u51b3\u7b56\u4e0e\u5168\u65f6\u57df\u5168\u5c40\u4f18\u5316\u76f8\u5339\u914d\u3002", "motivation": "\u968f\u7740\u50a8\u80fd\u7cfb\u7edf\uff08ESSs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u6574\u5408\u5230\u7535\u7f51\u4e2d\uff0c\u5728\u4e0d\u786e\u5b9a\u548c\u6ce2\u52a8\u7684\u7535\u529b\u4ef7\u683c\u4e0b\uff0c\u9700\u8981\u6709\u6548\u7684\u5b9e\u65f6\u63a7\u5236\u7b56\u7565\u3002\u50a8\u80fd\u7cfb\u7edf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u662f\u786e\u5b9a\u7cbe\u786e\u6a21\u62df\u5168\u5c40\u6700\u4f18\u63a7\u5236\u8f68\u8ff9\u6240\u9700\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u3002\u73b0\u6709\u6587\u732e\u4e2d\u7684\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u53ef\u80fd\u5ffd\u7565\u63a7\u5236\u52a8\u4f5c\u4e2d\u7684\u5b9e\u9645\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u5bf9\u9f50\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u5b9a\u4e49\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u6765\u8bc6\u522b\u6700\u5c0f\u89c4\u5212\u65f6\u57df\u3002", "result": "\u4f7f\u7528\u6765\u81eaNord Pool\u65e5\u5185\u5e02\u573a\u4e39\u9ea6DK1\u6295\u6807\u533a\u7684\u5b9e\u9645\u4ef7\u683c\u6570\u636e\u548c\u73b0\u5b9e\u7684ESS\u6a21\u578b\uff0c\u8bf4\u660e60\u5c0f\u65f6\u7684\u9884\u6d4b\u65f6\u57df\u53ef\u4ee5\u7cbe\u786e\u6a21\u62df\u5168\u5c40\u63a7\u5236\u5e8f\u5217\u548c\u7ecf\u6d4e\u7ed3\u679c\u3002\u5728\u5176\u4ed6\u53c2\u6570\u914d\u7f6e\u4e0b\uff0c\u6ca1\u6709\u9884\u6d4b\u65f6\u57df\u80fd\u786e\u4fdd\u5b8c\u5168\u6536\u655b\uff0c\u8fd9\u8868\u660e\u9884\u6d4b\u65f6\u57df\u7684\u5b58\u5728\u5bf9\u5404\u79cd\u53c2\u6570\u5f88\u654f\u611f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u50a8\u80fd\u8c03\u5ea6\u4e2d\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8fd0\u884c\u4e0a\u91cd\u8981\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u8fd9\u4e00\u91cd\u8981\u89c4\u5212\u63aa\u65bd\u7684\u5206\u6790\u63cf\u8ff0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.11639", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11639", "abs": "https://arxiv.org/abs/2511.11639", "authors": ["Jie Fan", "Francesco Visentin", "Barbara Mazzolai", "Emanuela Del Dottore"], "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature", "comment": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)", "summary": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u901a\u8fc73D\u5206\u6bb5\u95ed\u5408\u66f2\u7ebf\u6a21\u578b\u6765\u5206\u6790\u690d\u7269\u5377\u987b\u5728\u673a\u68b0\u523a\u6fc0\u4e0b\u7684\u5f62\u72b6\u53d8\u5316\u3002", "motivation": "\u5c3d\u7ba1\u6500\u7f18\u690d\u7269\u5df2\u88ab\u7814\u7a76\u4e86\u5f88\u957f\u65f6\u95f4\uff0c\u4f46\u8981\u63d0\u53d6\u6709\u5173\u65f6\u95f4\u5f62\u72b6\u53d8\u5316\u3001\u89e6\u53d1\u4e8b\u4ef6\u53ca\u5176\u63a5\u89e6\u4f4d\u7f6e\u4e4b\u95f4\u5173\u7cfb\u7684\u4fe1\u606f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u4f7f\u75283D\u5206\u6bb5\u95ed\u5408\u66f2\u7ebf\u6a21\u578b\u6765\u5206\u6790\u690d\u7269\u5377\u987b\u5728\u673a\u68b0\u523a\u6fc0\u4e0b\u7684\u5f62\u72b6\u53d8\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5177\u6709\u5f88\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u7cbe\u5ea6 R2 > 0.99\u3002\u4e0e\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6570\u636e\u9700\u6c42\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5f3a\u7b49\u4f18\u70b9\u3002\u5206\u6790\u663e\u793a\uff0c\u690d\u7269\u5377\u987b\u7684\u9876\u7aef\u90e8\u5206\u54cd\u5e94\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u83b7\u5f97\u5bf9\u690d\u7269\u751f\u7269\u529b\u5b66\u65b0\u89c1\u89e3\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u548c\u5f00\u53d1\u53d7\u6500\u7f18\u690d\u7269\u542f\u53d1\u7684\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12227", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12227", "abs": "https://arxiv.org/abs/2511.12227", "authors": ["Weibin Ni", "Zhijie Li", "Guanyu Qu", "Zhecheng Sun", "Jiale Dai", "Fazhan Shi", "Lei Sun"], "title": "Scalable quantum error mitigation with phase-cycled dynamical decoupling", "comment": "48 pages, 11 figures", "summary": "The realization of quantum technologies in the Noisy Intermediate-Scale Quantum era is severely constrained by qubit decoherence and control errors, presenting fundamental challenges to achieving quantum advantages. Dynamical decoupling is a widely used, powerful technique for decoherence error suppression. However, it is susceptible to control errors, making non-robust sequences like UDD impractical to implement and robust ones like CPMG to significantly overestimate decoherence times. This overestimation issue remains largely unexplored in the past few decades, leading to many reports of exceptionally long yet plausible decoherence times across various qubit platforms. Here, we construct Hadamard phase cycling as a non-Markovian quantum error mitigation method for dynamical decoupling. This method exploits group structure to design phase configurations of equivalent ensemble quantum circuits, effectively eliminates circuit outputs generated from erroneous dynamics, and scales linearly with circuit depth. Harnessing its error mitigation capability for ensemble solid-state electron spin qubits embedded in paramagnetic molecules and nitrogen-vacancy centers in diamond enables accurate acquisition of decoherence times. Applying Hadamard phase cycling on single trapped ion and superconducting transmon qubits effectively preserves their state fidelity during dynamical decoupling. The integration of scalable quantum error mitigation and suppression would facilitate the development of quantum technologies with noisy qubits and control hardware.", "AI": {"tldr": "\u91cf\u5b50\u65f6\u4ee3\uff0c\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u548c\u63a7\u5236\u9519\u8bef\u662f\u4e3b\u8981\u6311\u6218\uff0c\u52a8\u529b\u5b66\u89e3\u8026\u662f\u5e38\u7528\u6280\u672f\uff0c\u4f46\u6613\u53d7\u63a7\u5236\u9519\u8bef\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51faHadamard\u76f8\u4f4d\u76f8\u5dee\u5faa\u73af\u4f5c\u4e3a\u4e00\u79cd\u975e\u9a6c\u5c14\u53ef\u592b\u91cf\u5b50\u9519\u8bef\u7f13\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u529b\u5b66\u89e3\u8026\uff0c\u80fd\u591f\u7cbe\u786e\u6d4b\u91cf\u9000\u76f8\u5e72\u65f6\u95f4\uff0c\u5e76\u63d0\u5347\u91cf\u5b50\u6bd4\u7279\u4fdd\u771f\u5ea6\u3002", "motivation": "\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u548c\u63a7\u5236\u9519\u8bef\u662f\u5b9e\u73b0\u91cf\u5b50\u6280\u672f\u7684\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u7684\u52a8\u529b\u5b66\u89e3\u8026\u6280\u672f\u6613\u53d7\u63a7\u5236\u9519\u8bef\u5f71\u54cd\uff0c\u5bfc\u81f4\u5bf9\u9000\u76f8\u5e72\u65f6\u95f4\u7684\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u6784\u5efaHadamard\u76f8\u4f4d\u76f8\u5dee\u5faa\u73af\u4f5c\u4e3a\u4e00\u79cd\u975e\u9a6c\u5c14\u53ef\u592b\u91cf\u5b50\u9519\u8bef\u7f13\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u529b\u5b66\u89e3\u8026\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7fa4\u7ed3\u6784\u8bbe\u8ba1\u7b49\u6548\u7cfb\u7efc\u91cf\u5b50\u7535\u8def\u7684\u76f8\u4f4d\u914d\u7f6e\uff0c\u6709\u6548\u6d88\u9664\u7531\u9519\u8bef\u52a8\u529b\u5b66\u4ea7\u751f\u7684\u7535\u8def\u8f93\u51fa\uff0c\u5e76\u4e0e\u7535\u8def\u6df1\u5ea6\u5448\u7ebf\u6027\u6269\u5c55\u3002", "result": "\u5c06Hadamard\u76f8\u4f4d\u76f8\u5dee\u5faa\u73af\u5e94\u7528\u4e8e\u56fa\u6001\u7535\u5b50\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u548c\u91d1\u521a\u77f3\u4e2d\u7684\u6c2e-\u7a7a\u4f4d\uff08NV\uff09\u8272\u5fc3\uff0c\u5b9e\u73b0\u4e86\u9000\u76f8\u5e72\u65f6\u95f4\u7684\u7cbe\u786e\u83b7\u53d6\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u4e5f\u6709\u6548\u4fdd\u6301\u4e86\u5355\u9631\u79bb\u5b50\u548c\u8d85\u5bfctransmon\u91cf\u5b50\u6bd4\u7279\u5728\u52a8\u529b\u5b66\u89e3\u8026\u8fc7\u7a0b\u4e2d\u7684\u72b6\u6001\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u9519\u8bef\u7f13\u89e3\u548c\u6291\u5236\u6280\u672f\u7684\u96c6\u6210\u5c06\u4fc3\u8fdb\u5177\u6709\u566a\u58f0\u91cf\u5b50\u6bd4\u7279\u548c\u63a7\u5236\u786c\u4ef6\u7684\u91cf\u5b50\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.13061", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13061", "abs": "https://arxiv.org/abs/2511.13061", "authors": ["Vladim\u00edr Macko", "Vladim\u00edr Bo\u017ea"], "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity", "comment": "8 pages + 7 pages appendix, 11 figures, Code available at https://github.com/vlejd/macko_spmv", "summary": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.", "AI": {"tldr": "MACKO-SpMV\u662f\u4e00\u79cd\u65b0\u7684GPU\u4f18\u5316\u683c\u5f0f\u548c\u5185\u6838\uff0c\u53ef\u9ad8\u6548\u5904\u7406LLM\u4e2d\u5e38\u89c1\u7684\u4f4e\u3001\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u548c\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u77e9\u9635-\u5411\u91cf\u4e58\u79ef\uff08SpMV\uff09\u65b9\u6cd5\u5728\u4f4e\u4e14\u975e\u7ed3\u6784\u5316\u7684\u7a00\u758f\u6027\uff0830-90%\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u975e\u7ed3\u6784\u5316\u526a\u679d\u5728\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7a00\u758fLLM\u7684\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMACKO-SpMV\u7684GPU\u4f18\u5316\u683c\u5f0f\u548c\u5185\u6838\uff0c\u5b83\u5728\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eGPU\u6267\u884c\u6a21\u578b\u7684\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u6216\u9884\u8ba1\u7b97\u3002", "result": "\u572850%\u7a00\u758f\u5ea6\u4e0b\uff0cMACKO-SpMV\u5b9e\u73b0\u4e861.5\u500d\u7684\u5185\u5b58\u7f29\u51cf\u548c1.2-1.5\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u4e0e\u5176\u4ed6SpMV\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u5728cuSPARSE\u4e0a\u5b9e\u73b0\u4e862.8-13.0\u500d\u7684\u52a0\u901f\uff0c\u5728Sputnik\u4e0a\u5b9e\u73b0\u4e861.9-2.6\u500d\u7684\u52a0\u901f\uff0c\u5728DASP\u4e0a\u5b9e\u73b0\u4e862.2-2.5\u500d\u7684\u52a0\u901f\u3002\u5c06MACKO\u5e94\u7528\u4e8e\u526a\u679d\u540e\u7684Llama2-7B\u6a21\u578b\uff0c\u5728fp16\u7cbe\u5ea6\u4e0b\u5b9e\u73b0\u4e861.5\u500d\u7684\u5185\u5b58\u7f29\u51cf\u548c1.5\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "MACKO-SpMV\u7684\u63d0\u51fa\u4f7f\u5f97\u5728\u5b9e\u9645LLM\u5e94\u7528\u4e2d\uff0c50%\u7a00\u758f\u5ea6\u7684\u975e\u7ed3\u6784\u5316\u526a\u679d\u6210\u4e3a\u4e00\u79cd\u53ef\u884c\u4e14\u6709\u76ca\u7684\u4f18\u5316\u624b\u6bb5\u3002"}}
{"id": "2511.12719", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2511.12719", "abs": "https://arxiv.org/abs/2511.12719", "authors": ["M. Houzet", "T. Vakhtel", "J. S. Meyer"], "title": "Bloch diode", "comment": "8 pages, 5 figures", "summary": "In a SQUID tuned away from half-integer flux (in units of the superconducting flux quantum), the concurrence of multiple Josephson harmonics and an asymmetry between the junctions leads to the Josephson diode effect -- a nonreciprocal current-voltage characteristic manifested as an asymmetry of critical currents at opposite polarities. We predict a dual version of this effect in a gate-tunable Cooper pair transistor placed in series with a highly resistive environment. When tuned away from half-integer gate charge (in units of the Cooper pair charge) it shows an asymmetry of critical voltages at opposite polarities -- a dual diode effect we refer to as the Bloch diode effect. It arises from an asymmetry in the dispersion of the transistor's Bloch bands. A highly resistive environment can be realized with a Josephson junction array, suggesting that such a diode could be implemented using conventional superconducting quantum circuits.", "AI": {"tldr": "SQUID \u548c Cooper \u5bf9\u6676\u4f53\u7ba1\u4e2d\u5b58\u5728\u975e\u4e92\u6613\u6027\u6548\u5e94\uff0c\u8868\u73b0\u4e3a\u4e34\u754c\u7535\u6d41\u6216\u7535\u538b\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8 SQUID \u548c Cooper \u5bf9\u6676\u4f53\u7ba1\u4e2d\u7684 Josephson \u4e8c\u6781\u7ba1\u6548\u5e94\u53ca\u5176\u53d8\u4f53\uff0c\u5373 Bloch \u4e8c\u6781\u7ba1\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u5b9e\u73b0 Bloch \u4e8c\u6781\u7ba1\u6548\u5e94\u7684\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4e86 SQUID \u548c Cooper \u5bf9\u6676\u4f53\u7ba1\u7684\u7535\u6d41-\u7535\u538b\u7279\u6027\uff0c\u5206\u6790\u4e86\u591a\u91cd Josephson \u8c10\u6ce2\u3001\u7ed3\u4e0d\u5bf9\u79f0\u6027\u4ee5\u53ca Bloch \u8c31\u5e26\u4e0d\u5bf9\u79f0\u6027\u5728\u4ea7\u751f\u975e\u4e92\u6613\u6027\u6548\u5e94\u4e2d\u7684\u4f5c\u7528\u3002", "result": "1. SQUID \u5728\u504f\u79bb\u534a\u6574\u6570\u78c1\u901a\u91cf\u65f6\u8868\u73b0\u51fa Josephson \u4e8c\u6781\u7ba1\u6548\u5e94\uff0c\u8868\u73b0\u4e3a\u4e34\u754c\u7535\u6d41\u7684\u4e0d\u5bf9\u79f0\u6027\u3002 2. \u63d0\u51fa\u4e86\u4e00\u79cd Bloch \u4e8c\u6781\u7ba1\u6548\u5e94\uff0c\u5728 Cooper \u5bf9\u6676\u4f53\u7ba1\u504f\u79bb\u534a\u6574\u6570\u6805\u7535\u8377\u65f6\uff0c\u8868\u73b0\u4e3a\u4e34\u754c\u7535\u538b\u7684\u4e0d\u5bf9\u79f0\u6027\u3002 3. \u5efa\u8bae\u4f7f\u7528 Josephson \u7ed3\u9635\u5217\u6765\u5b9e\u73b0 Bloch \u4e8c\u6781\u7ba1\u6548\u5e94\u3002", "conclusion": "Josephson \u4e8c\u6781\u7ba1\u6548\u5e94\u548c Bloch \u4e8c\u6781\u7ba1\u6548\u5e94\u90fd\u662f\u7531\u975e\u4e92\u6613\u6027\u5f15\u8d77\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u8d85\u5bfc\u91cf\u5b50\u7535\u8def\u4e2d\u5b9e\u73b0\u3002"}}
{"id": "2511.12514", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12514", "abs": "https://arxiv.org/abs/2511.12514", "authors": ["Manish Sahoo", "Akash Deshmukh", "Yash Kokane", "Jayaprakash H M", "Raghavan Ranganathan"], "title": "An Active Learning Interatomic Potential For Defect-Engineered CoCrFeMnNi High-Entropy Alloy", "comment": "44 pages, 8 figures", "summary": "High-entropy alloys (HEAs) exhibit exceptional properties arising from a combination of thermodynamic, kinetic and structural factors and have found applications in numerous fields such as aerospace, energy, chemical industries, hydrogen storage, and ocean engineering. However, a large compositional space remains to be explored. Unlike conventional approaches, computational methods have shown accelerated discovery of novel alloys in a short time. However, the lack of interatomic potentials have posed a challenge in discovering new alloy compositions and property measurements. In the present work, we have developed a Moment Tensor Potential (MTP) trained by Machine Learning based approach using the BFGS unconstrained optimization algorithm for the CoCrFeMnNi High-entropy alloy. Our training set consists of various defects induced configurations such as vacancies, dislocations and stacking-faults. An active learning scheme to re-train the potential was undertaken to dynamically to add training data upon encountering extrapolative configurations during non-equilibrium simulations. A thorough investigation of the error metrics, equation of state, uniaxial tensile deformation, nano-indentation and solid-liquid interface stability for this alloy was carried out, and it is seen that the MTP potential outperforms the popular Modified Embedded Atom Method (MEAM) potential on physical properties prediction. The accuracy and high computational speed are discussed using scaling performance. The potential is prepared for public use by embedding it into the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) code.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u5229\u7528\u673a\u5668\u5b66\u4e60\u548cBFGS\u7b97\u6cd5\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8eCoCrFeMnNi\u9ad8\u71b5\u5408\u91d1\u7684\u529b\u77e9\u5f20\u91cf\u52bf(MTP)\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230LAMMPS\u4e2d\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u52a0\u901f\u4e86\u65b0\u578b\u5408\u91d1\u7684\u53d1\u73b0\uff0c\u4f46\u7f3a\u4e4f\u539f\u5b50\u95f4\u52bf\u662f\u53d1\u73b0\u65b0\u5408\u91d1\u6210\u5206\u548c\u6d4b\u91cf\u6027\u8d28\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u539f\u5b50\u95f4\u52bf\uff0c\u4ee5\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548cBFGS\u65e0\u7ea6\u675f\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u5305\u542b\u5404\u79cd\u7f3a\u9677\uff08\u7a7a\u4f4d\u3001\u4f4d\u9519\u3001\u5806\u79ef\u5c42\u9519\uff09\u7684\u8bad\u7ec3\u96c6\uff0c\u4e3aCoCrFeMnNi\u9ad8\u71b5\u5408\u91d1\u5f00\u53d1\u4e86\u529b\u77e9\u5f20\u91cf\u52bf(MTP)\u3002\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u65b9\u6848\u5bf9\u52bf\u8fdb\u884c\u518d\u8bad\u7ec3\uff0c\u4ee5\u5728\u975e\u5e73\u8861\u6a21\u62df\u4e2d\u52a8\u6001\u6dfb\u52a0\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5f00\u53d1\u51fa\u7684MTP\u52bf\u5728\u9884\u6d4b\u7269\u7406\u6027\u8d28\u65b9\u9762\u4f18\u4e8eMEAM\u52bf\uff0c\u5e76\u4e14\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684MTP\u52bf\u662f\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u539f\u5b50\u95f4\u52bf\uff0c\u53ef\u7528\u4e8eCoCrFeMnNi\u9ad8\u71b5\u5408\u91d1\u7684\u7814\u7a76\uff0c\u5e76\u5df2\u96c6\u6210\u5230LAMMPS\u4e2d\uff0c\u4f9b\u516c\u4f17\u4f7f\u7528\u3002"}}
{"id": "2511.11730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11730", "abs": "https://arxiv.org/abs/2511.11730", "authors": ["Yongjun Xiao", "Dian Meng", "Xinlei Huang", "Yanran Liu", "Shiwei Ruan", "Ziyue Qiao", "Xubin Zheng"], "title": "GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion", "comment": "8 pages, 3 figures, Accepted to AAAI 2026", "summary": "Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.", "AI": {"tldr": "GROVER\u6846\u67b6\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u878d\u5408\u591a\u7ec4\u5b66\u7a7a\u95f4\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u95f4\u5f02\u8d28\u6027\u548c\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7a7a\u95f4\u7ec4\u5b66\u6570\u636e\uff08\u8f6c\u5f55\u7ec4\u5b66\u3001\u86cb\u767d\u8d28\u7ec4\u5b66\u3001\u8868\u89c2\u57fa\u56e0\u7ec4\u5b66\uff09\u7f3a\u4e4f\u75c5\u7406\u5f62\u6001\u5b66\u80cc\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u6574\u5408\u4ee5\u8fdb\u884c\u5168\u9762\u7684\u75be\u75c5\u7ec4\u7ec7\u5206\u6790\u3002\u7136\u800c\uff0c\u7ec4\u5b66\u3001\u6210\u50cf\u548c\u7a7a\u95f4\u6a21\u6001\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u5f02\u8d28\u6027\uff0c\u4ee5\u53ca\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u548c\u6837\u672c\u5236\u5907\u8fc7\u7a0b\u4e2d\u7684\u751f\u7269\u6270\u52a8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faGROVER\uff08Graph-guided Representation of Omics and Vision with Expert Regulation\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u7f16\u7801\u5668\u6765\u6355\u6349\u5404\u6a21\u6001\u4e0e\u5176\u76f8\u5173\u7a7a\u95f4\u7ed3\u6784\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u6a21\u6001\u7279\u5b9a\u7684\u5d4c\u5165\u3002\u901a\u8fc7\u5f15\u5165\u70b9-\u7279\u5f81\u5bf9\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6765\u4f18\u5316\u8de8\u6a21\u6001\u5728\u6bcf\u4e2a\u70b9\u4e0a\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8bbe\u8ba1\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u70b9\u81ea\u9002\u5e94\u5730\u9009\u62e9\u4fe1\u606f\u6a21\u6001\uff0c\u540c\u65f6\u6291\u5236\u566a\u58f0\u6216\u4f4e\u8d28\u91cf\u8f93\u5165\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGROVER\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GROVER\u4e3a\u591a\u6a21\u6001\u7a7a\u95f4\u7ec4\u5b66\u6570\u636e\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11842", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11842", "abs": "https://arxiv.org/abs/2511.11842", "authors": ["Lucas Fenaux", "Christopher Srinivasa", "Florian Kerschbaum"], "title": "On the Trade-Off Between Transparency and Security in Adversarial Machine Learning", "comment": null, "summary": "Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.", "AI": {"tldr": "\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u5728AI\u5bf9\u6297\u6027\u653b\u51fb\u4e2d\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\uff0c\u653b\u51fb\u8005\u5728\u6a21\u4eff\u9632\u5fa1\u8005\u51b3\u7b56\u65f6\u66f4\u6613\u6210\u529f\uff0c\u8868\u660e\u6a21\u578b\u9690\u85cf\u6027\u5bf9\u9632\u5fa1\u8005\u6709\u5229\u3002\u901a\u8fc7\u535a\u5f08\u8bba\u5206\u6790\uff0c\u53d1\u73b0\u4ec5\u4e86\u89e3\u9632\u5fa1\u6a21\u578b\u662f\u5426\u88ab\u9632\u5fa1\u5c31\u53ef\u80fd\u635f\u5bb3\u5176\u5b89\u5168\u6027\uff0c\u63ed\u793a\u4e86AI\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u666e\u904d\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\uff0c\u900f\u660e\u5ea6\u5bf9AI\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u53ef\u8fc1\u79fb\u5bf9\u6297\u6837\u672c\u653b\u51fb\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff08\u4e5d\u79cd\u653b\u51fb\uff0c181\u4e2a\u6a21\u578b\uff09\u548c\u535a\u5f08\u8bba\uff08\u7eb3\u4ec0\u5747\u8861\u548c\u65af\u5854\u514b\u5c14\u4f2f\u683c\u535a\u5f08\uff09\u6765\u5206\u6790\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u6743\u8861\u3002", "result": "\u653b\u51fb\u8005\u5728\u6a21\u4eff\u9632\u5fa1\u8005\u51b3\u7b56\u65f6\u653b\u51fb\u66f4\u6210\u529f\uff0c\u6a21\u578b\u9690\u85cf\u6027\u5bf9\u9632\u5fa1\u8005\u6709\u5229\u3002\u4ec5\u4e86\u89e3\u9632\u5fa1\u6a21\u578b\u7684\u9632\u5fa1\u72b6\u6001\u5c31\u53ef\u80fd\u5bf9\u5176\u5b89\u5168\u6027\u9020\u6210\u635f\u5bb3\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u53ef\u80fd\u4e0e\u5176\u5b89\u5168\u6027\u76f8\u51b2\u7a81\uff0c\u5e76\u4e14\u535a\u5f08\u8bba\u53ef\u7528\u4e8e\u63ed\u793a\u8fd9\u79cd\u51b2\u7a81\u3002"}}
{"id": "2511.12674", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.12674", "abs": "https://arxiv.org/abs/2511.12674", "authors": ["Zhihua Zheng", "Xiaolong Yao", "Cailian Yu", "Menghao Gao", "Fangping Ouyang", "Shiwu Gao"], "title": "Distortion-Driven Carrier Decoupling in Doped LiMgPO4", "comment": null, "summary": "The interplay between lattice distortions and charge carriers governs the properties of many functional oxides. In alkali-doped LiMgPO4, a significant enhancement in dosimetric response is observed, but its microscopic origin is not understood. Using non-adiabatic molecular dynamics, we reveal a fundamental mechanism of carrier decoupling driven by a hierarchy of lattice distortions. We show that electrons localize into stable small polarons on an ultrafast timescale, trapped by the strong local potential induced by the dopant, while holes form more delocalized polarons that migrate efficiently through a lattice smoothed by global strain. The stark contrast between the dynamics of trapped electrons and mobile holes explains the suppressed recombination and enhanced energy storage. These results present a clear physical picture of how multiscale lattice distortions can independently control electron and hole transport, offering new insights into the physics of polarons in complex materials.", "AI": {"tldr": "\u5728\u78b1\u91d1\u5c5e\u63ba\u6742\u7684LiMgPO4\u4e2d\uff0c\u8f7d\u6d41\u5b50\u89e3\u8026\u673a\u5236\u7531\u6676\u683c\u7578\u53d8\u9a71\u52a8\uff0c\u7535\u5b50\u88ab\u6355\u83b7\u5f62\u6210\u5c0f\u6781\u5316\u5b50\uff0c\u800c\u7a7a\u7a74\u5f62\u6210\u66f4\u5177\u8fc1\u79fb\u6027\u7684\u6781\u5316\u5b50\uff0c\u4ece\u800c\u6291\u5236\u4e86\u590d\u5408\uff0c\u589e\u5f3a\u4e86\u50a8\u80fd\u3002", "motivation": "\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5728\u78b1\u91d1\u5c5e\u63ba\u6742\u7684LiMgPO4\u4e2d\uff0c\u5242\u91cf\u5b66\u54cd\u5e94\u4f1a\u663e\u8457\u589e\u5f3a\uff0c\u4f46\u5176\u5fae\u89c2\u8d77\u6e90\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528\u975e\u7edd\u70ed\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u63ed\u793a\u4e86\u7531\u6676\u683c\u7578\u53d8\u9a71\u52a8\u7684\u8f7d\u6d41\u5b50\u89e3\u8026\u673a\u5236\u3002", "result": "\u7535\u5b50\u5728\u8d85\u5feb\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5c40\u57df\u5316\u5f62\u6210\u7a33\u5b9a\u7684\u6781\u5c0f\u5316\u6781\u5316\u5b50\uff0c\u800c\u7a7a\u7a74\u5f62\u6210\u66f4\u5206\u6563\u7684\u6781\u5316\u5b50\uff0c\u6709\u6548\u5730\u901a\u8fc7\u88ab\u5168\u5c40\u5e94\u53d8\u5e73\u6ed1\u7684\u6676\u683c\u8fc1\u79fb\u3002\u8fd9\u79cd\u7535\u5b50\u548c\u7a7a\u7a74\u52a8\u529b\u5b66\u7684\u5de8\u5927\u5dee\u5f02\u89e3\u91ca\u4e86\u590d\u5408\u7684\u6291\u5236\u548c\u80fd\u91cf\u5b58\u50a8\u7684\u589e\u5f3a\u3002", "conclusion": "\u63ed\u793a\u4e86\u591a\u5c3a\u5ea6\u6676\u683c\u7578\u53d8\u5982\u4f55\u72ec\u7acb\u63a7\u5236\u7535\u5b50\u548c\u7a7a\u7a74\u4f20\u8f93\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u6750\u6599\u4e2d\u7684\u6781\u5316\u5b50\u7269\u7406\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.12102", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12102", "abs": "https://arxiv.org/abs/2511.12102", "authors": ["Abhisha Garg", "Akash Kumar", "Suraj Srivastava", "Nimish Yadav", "Aditya K. Jagannatham", "Lajos Hanzo"], "title": "Bayesian Learning Aided Simultaneous Sparse Estimation of Dual-Wideband THz Channels in Multi-User Hybrid MIMO Systems", "comment": null, "summary": "This work conceives the Bayesian Group-Sparse Regression (BGSR) for the estimation of a spatial and frequency wideband, i.e., a dual wideband channel in Multi-User (MU) THz hybrid MIMO scenarios. We develop a practical dual wideband THz channel model that incorporates absorption losses, reflection losses, diffused ray modeling and angles of arrival/departure (AoAs/AoDs) using a Gaussian Mixture Model (GMM). Furthermore, a low-resolution analog-to-digital converter (ADC) is employed at each RF chain, which is crucial for wideband THz massive MIMO systems to reduce power consumption and hardware complexity, given the high sampling rates and large number of antennas involved. The quantized MU THz MIMO model is linearized using the popular Bussgang decomposition followed by BGSR based channel learning framework that results in sparsity across different subcarriers, where each subcarrier has its unique dictionary matrix. Next, the Bayesian Cram\u00e9r Rao Bound (BCRB) is devised for bounding the normalized mean square error (NMSE) performance. Extensive simulations were performed to assess the performance improvements achieved by the proposed BGSR method compared to other sparse estimation techniques. The metrics considered for quantifying the performance improvements include the NMSE and bit error rate (BER).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u7ec4\u7a00\u758f\u56de\u5f52\uff08BGSR\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u7528\u6237\uff08MU\uff09\u592a\u8d6b\u5179\uff08THz\uff09\u6df7\u5408MIMO\u573a\u666f\u4e2d\u4f30\u8ba1\u53cc\u5bbd\u5e26\uff08\u7a7a\u95f4\u548c\u9891\u7387\uff09\u4fe1\u9053\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u592a\u8d6b\u5179\u901a\u4fe1\u9ad8\u91c7\u6837\u7387\u548c\u5927\u89c4\u6a21\u5929\u7ebf\u6570\u91cf\u5e26\u6765\u7684\u529f\u8017\u548c\u786c\u4ef6\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u91c7\u7528\u4e86\u4f4e\u5206\u8fa8\u7387ADC\uff0c\u5e76\u63d0\u51fa\u4e86BGSR\u65b9\u6cd5\u6765\u5b66\u4e60\u91cf\u5316\u540e\u7684\u4fe1\u9053\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5438\u6536\u635f\u8017\u3001\u53cd\u5c04\u635f\u8017\u3001\u6563\u5c04\u5c04\u7ebf\u5efa\u6a21\u4ee5\u53ca\u5230\u8fbe/\u79bb\u5f00\u89d2\u5ea6\uff08AoAs/AoDs\uff09\u7684\u5b9e\u7528\u53cc\u5bbd\u5e26\u592a\u8d6b\u5179\u4fe1\u9053\u6a21\u578b\u3002\u7136\u540e\uff0c\u5229\u7528Bussgang\u5206\u89e3\u5c06\u91cf\u5316\u540e\u7684MU\u592a\u8d6b\u5179MIMO\u6a21\u578b\u7ebf\u6027\u5316\uff0c\u5e76\u91c7\u7528BGSR\u4fe1\u9053\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u5b50\u8f7d\u6ce2\u7684\u7a00\u758f\u6027\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5b50\u8f7d\u6ce2\u90fd\u6709\u5176\u72ec\u7279\u7684\u5b57\u5178\u77e9\u9635\u3002\u6700\u540e\uff0c\u63a8\u5bfc\u4e86\u8d1d\u53f6\u65afCram\u00e9r Rao\u754c\uff08BCRB\uff09\u6765\u7ea6\u675f\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684BGSR\u65b9\u6cd5\u5728NMSE\u548c\u6bd4\u7279\u9519\u8bef\u7387\uff08BER\uff09\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u7a00\u758f\u4f30\u8ba1\u6280\u672f\u3002", "conclusion": "BGSR\u65b9\u6cd5\u4e3a\u5728\u4f4e\u5206\u8fa8\u7387ADC\u6761\u4ef6\u4e0b\u8fdb\u884c\u53cc\u5bbd\u5e26\u592a\u8d6b\u5179\u4fe1\u9053\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11614", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11614", "abs": "https://arxiv.org/abs/2511.11614", "authors": ["Arturo Ur\u00edas Jim\u00e9nez"], "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI", "comment": null, "summary": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.\n  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.", "AI": {"tldr": "FPGAs are a flexible and efficient alternative to GPUs for AI acceleration, offering lower latency, energy efficiency, and customization, especially for edge computing and specialized data center tasks.", "motivation": "The limitations of fixed GPU architectures for AI acceleration, particularly concerning latency, energy efficiency, and hardware control, necessitate alternative solutions.", "method": "FPGAs enable direct mapping of AI algorithms into device logic, allowing for parallel pipelines, deterministic timing, and reduced power consumption. They offer reconfigurability, integration with embedded processors, and support for partial reconfiguration and compilation flows from AI frameworks for hardware-algorithm co-design.", "result": "FPGAs provide a strategic option for AI workloads demanding predictable performance and deep customization. They reduce latency and bandwidth requirements, improve privacy by enabling near-sensor inference, and offload specialized tasks from data centers.", "conclusion": "FPGAs represent a powerful and adaptable platform for AI acceleration, overcoming the limitations of traditional hardware and paving the way for more efficient, customized, and high-performance AI deployments, especially at the edge."}}
{"id": "2511.12507", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12507", "abs": "https://arxiv.org/abs/2511.12507", "authors": ["Jingtian Ma", "Jingyuan Wang", "Leong Hou U"], "title": "Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning", "comment": null, "summary": "Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.", "AI": {"tldr": "HiFiNet\u662f\u4e00\u79cd\u65b0\u578b\u5206\u5c42\u9891\u7387\u5206\u89e3\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u7ea7\u865a\u62df\u8282\u70b9\u548c\u91c7\u7528\u5206\u89e3-\u66f4\u65b0-\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u56feTransformer\uff0c\u6709\u6548\u878d\u5408\u4e86\u7a7a\u95f4\u548c\u9891\u8c31\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9053\u8def\u7f51\u7edc\u5efa\u6a21\u4e2d\u7684\u7a7a\u95f4-\u9891\u8c31\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9053\u8def\u7f51\u7edc\u5efa\u6a21\u4e2d\u5b58\u5728\u7a7a\u95f4-\u9891\u8c31\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u9053\u8def\u7f51\u7edc\u590d\u6742\u4ea4\u4e92\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "HiFiNet\u6784\u5efa\u591a\u7ea7\u865a\u62df\u8282\u70b9\u4ee5\u5b9e\u73b0\u5c40\u90e8\u9891\u7387\u5206\u6790\uff0c\u5e76\u91c7\u7528\u5206\u89e3-\u66f4\u65b0-\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u56feTransformer\u6765\u5206\u522b\u5efa\u6a21\u548c\u878d\u5408\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u53f7\u3002", "result": "HiFiNet\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5728\u6355\u83b7\u6709\u6548\u7684\u9053\u8def\u7f51\u7edc\u8868\u793a\u65b9\u9762\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HiFiNet\u6210\u529f\u5730\u878d\u5408\u4e86\u7a7a\u95f4\u548c\u9891\u8c31\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9053\u8def\u7f51\u7edc\u5efa\u6a21\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5177\u6709\u51fa\u8272\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12349", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12349", "abs": "https://arxiv.org/abs/2511.12349", "authors": ["Divya Kiran Kadiyala", "Alexandros Daglis"], "title": "Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting", "comment": null, "summary": "The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.\n  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.", "AI": {"tldr": "\u670d\u52a1\u5668CPU\u6838\u5fc3\u6570\u91cf\u7684\u4e0d\u65ad\u589e\u52a0\u5bf9\u5185\u5b58\u7cfb\u7edf\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\uff0c\u800c\u5185\u5b58\u7cfb\u7edf\u53d7\u9650\u4e8e\u6709\u9650\u7684\u82af\u7247\u5f15\u811a\u548c\u6570\u636e\u4f20\u8f93\u901f\u7387\u7684\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u9ad8\u7aef\u5904\u7406\u5668\u901a\u5e38\u6bcf\u4e2a\u6838\u5fc3\u7684\u5185\u5b58\u5e26\u5bbd\u8f83\u4f4e\uff0c\u8fd9\u4e0d\u5229\u4e8e\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u3002\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u63d0\u9ad8CPU\u6709\u9650\u5f15\u811a\u7684\u5229\u7528\u7387\u6765\u7f13\u89e3\u8fd9\u4e00\u6311\u6218\u3002\u5728\u4e00\u4e2a\u5178\u578b\u7684CPU\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u7528\u7684\u5f15\u811a\u88ab\u5206\u914d\u7ed9\u5185\u5b58\u548cI/O\u6d41\u91cf\uff0c\u4e24\u8005\u5404\u5360\u603b\u82af\u7247\u5916\u5e26\u5bbd\u53ef\u7528\u6027\u7684\u4e00\u534a\u3002\u56e0\u6b64\uff0c\u9664\u975e\u5185\u5b58\u548cI/O\u540c\u65f6\u88ab\u9ad8\u5ea6\u5229\u7528\uff0c\u5426\u5219\u8fd9\u79cd\u788e\u7247\u5316\u4f1a\u5bfc\u81f4\u5b9d\u8d35\u7684\u82af\u7247\u5916\u5e26\u5bbd\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u3002\u7406\u60f3\u7684\u67b6\u6784\u5c06\u63d0\u4f9bI/O\u548c\u5185\u5b58\u5e26\u5bbd\u7684\u53ef\u66ff\u4ee3\u6027\uff0c\u5141\u8bb8\u6839\u636e\u6bcf\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9700\u8981\u6765\u4f7f\u7528\u603b\u7684\u82af\u7247\u5916\u5e26\u5bbd\u3002\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86SURGE\uff0c\u4e00\u79cd\u8f6f\u4ef6\u652f\u6301\u7684\u67b6\u6784\u6280\u672f\uff0c\u901a\u8fc7\u633d\u6551\u95f2\u7f6e\u7684I/O\u5e26\u5bbd\u8d44\u6e90\u6765\u63d0\u9ad8\u5185\u5b58\u5e26\u5bbd\u7684\u53ef\u7528\u6027\u3002SURGE\u5229\u7528\u4e86\u50cfCXL\u8fd9\u6837\u7684\u901a\u7528\u4e92\u8fde\u6280\u672f\u7684\u5f3a\u5927\u529f\u80fd\uff0c\u53ef\u4ee5\u5728\u540c\u4e00\u5904\u7406\u5668\u63a5\u53e3\u4e0a\u52a8\u6001\u5730\u591a\u8def\u590d\u7528\u5185\u5b58\u548cI/O\u6d41\u91cf\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u589e\u5f3aSURGE\u7684\u67b6\u6784\u53ef\u4ee5\u5c06\u5e26\u5bbd\u53d7\u9650\u670d\u52a1\u5668\u4e0a\u7684\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u52a0\u901f\u9ad8\u8fbe1.3\u500d\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u5668CPU\u6838\u5fc3\u6570\u91cf\u7684\u4e0d\u65ad\u589e\u52a0\uff0c\u5185\u5b58\u7cfb\u7edf\u9762\u4e34\u7740\u5e26\u5bbd\u74f6\u9888\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u6838\u5fc3\u7684\u5185\u5b58\u5e26\u5bbd\u4f1a\u964d\u4f4e\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0cCPU\u7684\u82af\u7247\u5916\u5f15\u811a\u6570\u91cf\u6709\u9650\uff0c\u5e76\u4e14\u8fd9\u4e9b\u5f15\u811a\u901a\u5e38\u88ab\u5e73\u5747\u5206\u914d\u7ed9\u5185\u5b58\u548cI/O\u6d41\u91cf\uff0c\u5bfc\u81f4\u5728\u4efb\u4e00\u6d41\u91cf\u4e0d\u6ee1\u8f7d\u65f6\u5e26\u5bbd\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63d0\u9ad8\u82af\u7247\u5916\u5e26\u5bbd\u7684\u5229\u7528\u7387\uff0c\u4f7f\u5176\u80fd\u591f\u6839\u636e\u9700\u8981\u7075\u6d3b\u5730\u7528\u4e8e\u5185\u5b58\u6216I/O\u6d41\u91cf\u3002", "method": "SURGE\u662f\u4e00\u79cd\u8f6f\u4ef6\u652f\u6301\u7684\u67b6\u6784\u6280\u672f\uff0c\u5b83\u5229\u7528CXL\u7b49\u901a\u7528\u4e92\u8fde\u6280\u672f\uff0c\u5141\u8bb8\u5728\u540c\u4e00\u5904\u7406\u5668\u63a5\u53e3\u4e0a\u52a8\u6001\u5730\u591a\u8def\u590d\u7528\u5185\u5b58\u548cI/O\u6d41\u91cf\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cSURGE\u53ef\u4ee5\u5c06\u95f2\u7f6e\u7684I/O\u5e26\u5bbd\u8d44\u6e90\u7528\u4e8e\u5185\u5b58\u8bbf\u95ee\uff0c\u4ece\u800c\u63d0\u9ad8\u5185\u5b58\u5e26\u5bbd\u7684\u53ef\u7528\u6027\u3002", "result": "SURGE\u6280\u672f\u53ef\u4ee5\u5c06\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5e26\u5bbd\u53d7\u9650\u670d\u52a1\u5668\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u9ad8\u8fbe1.3\u500d\u3002", "conclusion": "SURGE\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u590d\u7528\u5185\u5b58\u548cI/O\u6d41\u91cf\u6765\u63d0\u9ad8CPU\u82af\u7247\u5916\u5e26\u5bbd\u7684\u5229\u7528\u7387\uff0c\u4ece\u800c\u663e\u8457\u52a0\u901f\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12053", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12053", "abs": "https://arxiv.org/abs/2511.12053", "authors": ["Xubo Gu", "Xun Huan", "Yao Ren", "Wenqing Zhou", "Weiran Jiang", "Ziyou Song"], "title": "Real-Time Physics-Aware Battery Health Monitoring from Partial Charging Profiles via Physics-Informed Neural Networks", "comment": null, "summary": "Monitoring battery health is essential for ensuring safe and efficient operation. However, there is an inherent trade-off between assessment speed and diagnostic depth-specifically, between rapid overall health estimation and precise identification of internal degradation states. Capturing detailed internal battery information efficiently remains a major challenge, yet such insights are key to understanding the various degradation mechanisms. To address this, we develop a parameterized physics-informed neural network (P-PINNSPM) over the key aging-related parameter space for a single particle model. The model can accurately predict internal battery variables across the parameter space and identifies internal parameters in about 30 seconds-achieving a 47x speedup over the finite volume method-while maintaining high accuracy. These parameters improve the battery state-of-health (SOH) estimation accuracy by at least 60.61%, compared to models without parameter incorporation. Moreover, they enable extrapolation to unseen SOH levels and support robust estimation across diverse charging profiles and operating conditions. Our results demonstrate the strong potential of physics-informed machine learning to advance real-time, data-efficient, and physics-aware battery management systems.", "AI": {"tldr": "\u901a\u8fc7\u53c2\u6570\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08P-PINNSPM\uff09\u5728\u5355\u7c92\u5b50\u6a21\u578b\u5173\u952e\u8001\u5316\u76f8\u5173\u53c2\u6570\u7a7a\u95f4\u4e0a\uff0c\u5b9e\u73b0\u4e86\u7535\u6c60\u5185\u90e8\u72b6\u6001\u7684\u5feb\u901f\u3001\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u5065\u5eb7\u72b6\u6001\uff08SOH\uff09\u4f30\u8ba1\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7535\u6c60\u5065\u5eb7\u76d1\u6d4b\u65b9\u6cd5\u5728\u8bc4\u4f30\u901f\u5ea6\u548c\u8bca\u65ad\u6df1\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u5728\u5feb\u901f\u8bc4\u4f30\u7684\u540c\u65f6\u7cbe\u786e\u8bc6\u522b\u5185\u90e8\u9000\u5316\u72b6\u6001\uff0c\u800c\u6df1\u5165\u4e86\u89e3\u9000\u5316\u673a\u5236\u5bf9\u7535\u6c60\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08P-PINNSPM\uff09\uff0c\u5728\u5355\u7c92\u5b50\u6a21\u578b\uff08SPM\uff09\u7684\u5173\u952e\u8001\u5316\u76f8\u5173\u53c2\u6570\u7a7a\u95f4\u4e0a\u8fdb\u884c\u5efa\u6a21\uff0c\u80fd\u591f\u9884\u6d4b\u5185\u90e8\u53d8\u91cf\u5e76\u8bc6\u522b\u5185\u90e8\u53c2\u6570\u3002", "result": "P-PINNSPM\u6a21\u578b\u80fd\u5728\u7ea630\u79d2\u5185\u8bc6\u522b\u5185\u90e8\u53c2\u6570\uff0c\u901f\u5ea6\u6bd4\u6709\u9650\u4f53\u79ef\u6cd5\u5feb47\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u8be5\u6a21\u578b\u5c06\u7535\u6c60\u5065\u5eb7\u72b6\u6001\uff08SOH\uff09\u4f30\u8ba1\u7cbe\u5ea6\u81f3\u5c11\u63d0\u9ad8\u4e8660.61%\uff0c\u5e76\u80fd\u5916\u63a8\u81f3\u672a\u89c1\u7684SOH\u6c34\u5e73\uff0c\u652f\u6301\u5728\u4e0d\u540c\u5145\u7535\u548c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u8fdb\u884c\u9c81\u68d2\u4f30\u8ba1\u3002", "conclusion": "\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u5f00\u53d1\u5b9e\u65f6\u3001\u6570\u636e\u9ad8\u6548\u3001\u7269\u7406\u611f\u77e5\uff08physics-aware\uff09\u7684\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11740", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11740", "abs": "https://arxiv.org/abs/2511.11740", "authors": ["Haowen Jiang", "Xinyu Huang", "You Lu", "Dingji Wang", "Yuheng Cao", "Chaofeng Sha", "Bihuan Chen", "Keyu Chen", "Xin Peng"], "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts", "comment": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026", "summary": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.", "AI": {"tldr": "ExpertAD\u662f\u4e00\u4e2a\u5229\u7528MoE\u67b6\u6784\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7PA\u548cMoSE\u6a21\u5757\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u3001\u4efb\u52a1\u5e72\u6270\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u80fd\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u548c\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\uff0c\u9762\u4e34\u8bed\u4e49\u7406\u89e3\u4e0d\u51c6\u786e\u3001\u591a\u4efb\u52a1\u89c4\u5212\u5e72\u6270\u4ee5\u53ca\u63a8\u7406\u5ef6\u8fdf\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u51b3\u7b56\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faExpertAD\u6846\u67b6\uff0c\u91c7\u7528MoE\u67b6\u6784\u3002\u5f15\u5165\u611f\u77e5\u9002\u914d\u5668\uff08PA\uff09\u589e\u5f3a\u5173\u952e\u7279\u5f81\uff0c\u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08MoSE\uff09\u51cf\u5c11\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u3002", "result": "ExpertAD\u5c06\u5e73\u5747\u78b0\u649e\u7387\u964d\u4f4e\u9ad8\u8fbe20%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c1125%\u3002\u5728\u7f55\u89c1\u573a\u666f\uff08\u5982\u4e8b\u6545\u3001\u4e3a\u7d27\u6025\u8f66\u8f86\u8ba9\u884c\uff09\u548c\u672a\u89c1\u8fc7\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u591a\u6280\u80fd\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "ExpertAD\u901a\u8fc7MoE\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u611f\u77e5\u548c\u89c4\u5212\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u5e76\u5728\u590d\u6742\u548c\u7f55\u89c1\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12246", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2511.12246", "abs": "https://arxiv.org/abs/2511.12246", "authors": ["Fei Wang", "Guoying Liang", "Zecheng Zhao", "Lin-Yue Luo", "Da-Jian Zhang", "Bao-Ming Xu"], "title": "Survival of Hermitian Criticality in the Non-Hermitian Framework", "comment": "9 pages, 5 figures, 1 table", "summary": "In this work, we investigate many-body phase transitions in a one-dimensional anisotropic XY model subject to a complex-valued transverse field. Within the biorthogonal framework, we calculate the ground-state correlation functions and entanglement entropy, confirming that their scaling behavior remains identical to that in the Hermitian XY model. The preservation of Hermitian phase transition features in the non-Hermitian setting is rooted in the persistence and emergence of symmetries and their breaking. Specifically, the ferromagnetic (FM) phase arises from the breaking of a $Z_2$ symmetry, while the Luttinger liquid (LL) phase is enabled by the emergence of a $U(1)$ symmetry together with the degeneracy of the real part of the energy spectrum. The nontrivial topology of the LL phase are characterized by the winding number around the exceptional point (EP). Given that non-Hermitian systems are inherently open, this research opens a new avenue for exploring conventional quantum phase transitions that are typically vulnerable to decoherence and environmental disruption in open quantum systems.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u5728\u590d\u503c\u6a2a\u5411\u573a\u4f5c\u7528\u4e0b\uff0c\u7814\u7a76\u4e00\u7ef4\u5404\u5411\u5f02\u6027XY\u6a21\u578b\u7684\u591a\u4f53\u76f8\u53d8\uff0c\u5e76\u8868\u660e\u5176\u76f8\u53d8\u7279\u5f81\u5728\u975e\u5384\u7c73\u4f53\u7cfb\u4e2d\u5f97\u4ee5\u4fdd\u7559\u3002", "motivation": "\u63a2\u7d22\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\uff0c\u901a\u5e38\u6613\u53d7\u9000\u76f8\u5e72\u548c\u73af\u5883\u5e72\u6270\u5f71\u54cd\u7684\u91cf\u5b50\u76f8\u53d8\u3002", "method": "\u5728\u53cc\u6b63\u4ea4\u6846\u67b6\u4e0b\uff0c\u8ba1\u7b97\u57fa\u6001\u5173\u8054\u51fd\u6570\u548c\u7ea0\u7f20\u71b5\uff0c\u5e76\u901a\u8fc7\u5bf9\u79f0\u6027\uff08Z2\u548cU(1)\u5bf9\u79f0\u6027\u7684\u4fdd\u6301\u3001\u51fa\u73b0\u548c\u7834\u574f\uff09\u548c\u80fd\u8c31\u7b80\u5e76\u6027\u6765\u8868\u5f81\u94c1\u78c1\u76f8\u548cLuttinger\u6db2\u4f53\u76f8\uff0c\u5e76\u5229\u7528\u7ed5\u7740\u5947\u5f02\u70b9\uff08EP\uff09\u7684\u7f20\u7ed5\u6570\u6765\u8868\u5f81Luttinger\u6db2\u4f53\u76f8\u7684\u62d3\u6251\u6027\u8d28\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5384\u7c73XY\u6a21\u578b\u76f8\u6bd4\uff0c\u5176\u5173\u8054\u51fd\u6570\u548c\u7ea0\u7f20\u71b5\u7684\u6807\u5ea6\u884c\u4e3a\u4fdd\u6301\u4e0d\u53d8\u3002\u94c1\u78c1\u76f8\u6e90\u4e8eZ2\u5bf9\u79f0\u6027\u7684\u7834\u574f\uff0c\u800cLuttinger\u6db2\u4f53\u76f8\u5219\u6e90\u4e8eU(1)\u5bf9\u79f0\u6027\u7684\u51fa\u73b0\u548c\u80fd\u8c31\u5b9e\u90e8\u7684\u7b80\u5e76\u3002", "conclusion": "\u975e\u5384\u7c73\u4f53\u7cfb\u4e2d\u7684\u5bf9\u79f0\u6027\u4fdd\u62a4\u673a\u5236\u4f7f\u5f97\u8be5\u6a21\u578b\u80fd\u591f\u4fdd\u7559\u5384\u7c73\u4f53\u7cfb\u4e2d\u7684\u76f8\u53d8\u7279\u5f81\uff0c\u4e3a\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7814\u7a76\u91cf\u5b50\u76f8\u53d8\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.11611", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11611", "abs": "https://arxiv.org/abs/2511.11611", "authors": ["David H. Silver"], "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games", "comment": null, "summary": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6e38\u620f\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\u7684\u4e92\u8865\u63a7\u5236\u6e90\uff0c\u6765\u533a\u5206\u6280\u80fd\u548c\u8fd0\u6c14\u3002", "motivation": "\u5728\u6e38\u620f\u4e2d\u5206\u79bb\u6280\u80fd\u548c\u8fd0\u6c14\uff0c\u4ee5\u91cf\u5316\u73a9\u5bb6\u7684\u5f71\u54cd\u529b\u3001\u6e38\u620f\u5e73\u8861\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u6e38\u620f\u7ed3\u679c\u5206\u89e3\u4e3a\u6280\u80fd\u6760\u6746K\u548c\u8fd0\u6c14\u6760\u6746L\uff0c\u5b9a\u4e49\u6280\u80fd-\u8fd0\u6c14\u6307\u6570S(G)\u5728[-1, 1]\u4e4b\u95f4\u3002\u5f15\u5165\u6ce2\u52a8\u6027Sigma\u6765\u91cf\u5316\u8fde\u7eed\u8f6e\u6b21\u4e2d\u7684\u7ed3\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e30\u79cd\u6e38\u620f\uff0c\u63ed\u793a\u4e86\u4ece\u7eaf\u8fd0\u6c14\uff08S=-1\uff09\u5230\u6df7\u5408\u9886\u57df\uff08\u5982\u53cc\u9646\u68cb\uff0cS=0\uff09\u518d\u5230\u7eaf\u6280\u80fd\uff08\u5982\u56fd\u9645\u8c61\u68cb\uff0cS=+1\uff09\u7684\u8fde\u7eed\u8c31\u3002\u6251\u514b\u663e\u793a\u51fa\u4e2d\u7b49\u6280\u80fd\u4f18\u52bf\uff08S=0.33\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e00\u822c\u968f\u673a\u51b3\u7b56\u7cfb\u7edf\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u6e38\u620f\u8bbe\u8ba1\u3001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u548c\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2511.12933", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.12933", "abs": "https://arxiv.org/abs/2511.12933", "authors": ["D. B. Karki"], "title": "Tunneling in multi-site mesoscopic quantum Hall circuits", "comment": null, "summary": "Transport properties of the single- and two-site mesoscoipc quantum Hall (QH) circuits at high transparencies can be described in terms of the lowest-order backscattering perturbations, and mapping to the boundary sine-Gordon model can be exploited in full generality. While the higher-order backscattering processes are exactly marginal in the case of corresponding three-site circuits, they become crucial in a device with four or more sites. Here, we explore the transport properties of a multi-site QH circuit with special focus on that with four sites, and report their unique quantum critical behaviors that can be accessed via transport measurements. Tunneling phenomena in multichannel QH circuits based on multi-site geometry are also investigated, and a promising route to realizing different aspects of quantum critical phenomena is offered", "AI": {"tldr": "The study analyzes transport properties of multi-site quantum Hall (QH) circuits, finding unique quantum critical behaviors in four-site circuits and exploring tunneling phenomena in multichannel QH circuits.", "motivation": "The motivation is to explore the transport properties of multi-site QH circuits, especially those with four or more sites, and to investigate their unique quantum critical behaviors and tunneling phenomena.", "method": "The paper exploits the mapping to the boundary sine-Gordon model and analyzes backscattering perturbations in single- and two-site QH circuits. It then extends this analysis to multi-site circuits, with a special focus on four-site circuits, to explore their transport properties and quantum critical behaviors.", "result": "The results indicate that higher-order backscattering processes, which are exactly marginal in three-site circuits, become crucial in four or more site circuits, leading to unique quantum critical behaviors. Tunneling phenomena in multichannel QH circuits are also investigated.", "conclusion": "The paper offers a promising route to realizing different aspects of quantum critical phenomena through the study of multi-site QH circuits and their unique quantum critical behaviors and tunneling phenomena."}}
{"id": "2511.11732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11732", "abs": "https://arxiv.org/abs/2511.11732", "authors": ["Aditya Mehta", "Swarnim Chaudhary", "Pratik Narang", "Jagat Sesh Challa"], "title": "Exposing DeepFakes via Hyperspectral Domain Mapping", "comment": "Accepted at AAAI 2026 Student Abstract", "summary": "Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSI-Detect\u7684\u8d85\u5149\u8c31\u6210\u50cf\u6280\u672f\uff0c\u7528\u4e8e\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u56fe\u50cf\u3002\u8be5\u6280\u672f\u901a\u8fc7\u5c06RGB\u56fe\u50cf\u91cd\u5efa\u4e3a31\u901a\u9053\u7684\u8d85\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u5728\u8d85\u5149\u8c31\u57df\u4e2d\u8fdb\u884c\u68c0\u6d4b\uff0c\u80fd\u591f\u653e\u5927\u5e76\u68c0\u6d4b\u5230\u5728RGB\u57df\u4e2d\u96be\u4ee5\u5bdf\u89c9\u7684\u4f2a\u9020\u75d5\u8ff9\uff0c\u5e76\u5728FaceForensics++\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8eRGB\u65b9\u6cd5\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u903c\u771f\u56fe\u50cf\u80fd\u591f\u8bef\u5bfc\u4eba\u7c7b\u548c\u81ea\u52a8\u5316\u68c0\u6d4b\u7cfb\u7edf\uff0c\u800c\u5927\u591a\u6570\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u9650\u4e8eRGB\u7a7a\u95f4\uff08\u4e09\u4e2a\u5149\u8c31\u901a\u9053\uff09\u8fdb\u884c\u5206\u6790\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u5230\u7ec6\u5fae\u7684\u4f2a\u9020\u75d5\u8ff9\u3002", "method": "HSI-Detect\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u7ba1\u9053\u3002\u9996\u5148\uff0c\u5b83\u4ece\u6807\u51c6\u7684RGB\u8f93\u5165\u91cd\u5efa\u4e00\u4e2a31\u901a\u9053\u7684\u8d85\u5149\u8c31\u56fe\u50cf\u3002\u7136\u540e\uff0c\u5728\u8d85\u5149\u8c31\u57df\u4e2d\u8fdb\u884c\u68c0\u6d4b\u3002\u901a\u8fc7\u6269\u5c55\u8f93\u5165\u8868\u793a\u5230\u66f4\u5bc6\u96c6\u7684\u9891\u8c31\u5e26\uff0c\u53ef\u4ee5\u653e\u5927\u4f2a\u9020\u7684\u75d5\u8ff9\uff0c\u8fd9\u4e9b\u75d5\u8ff9\u5728RGB\u57df\u4e2d\u901a\u5e38\u5f88\u5f31\u6216\u4e0d\u53ef\u89c1\uff0c\u5c24\u5176\u662f\u5728\u7279\u5b9a\u7684\u9891\u7387\u5e26\u4e2d\u3002", "result": "\u5728FaceForensics++\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cHSI-Detect\u5728\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u56fe\u50cf\u65b9\u9762\uff0c\u76f8\u8f83\u4e8e\u4ec5\u4f7f\u7528RGB\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u5c06\u9891\u8c31\u57df\u6620\u5c04\u5e94\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5177\u6709\u6f5c\u529b\uff0cHSI-Detect\u901a\u8fc7\u5728\u8d85\u5149\u8c31\u57df\u4e2d\u8fdb\u884c\u5206\u6790\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u5230\u7ecf\u8fc7\u7be1\u6539\u7684\u56fe\u50cf\u3002"}}
{"id": "2511.12089", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12089", "abs": "https://arxiv.org/abs/2511.12089", "authors": ["Yanchang Fu", "Qiyue Yin", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything", "comment": null, "summary": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.", "AI": {"tldr": "KrwEmd\u662f\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u5fb7\u5dde\u6251\u514b\u7b49\u6e38\u620f\u4e2d\u8fc7\u5ea6\u62bd\u8c61\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165k-recall\u80dc\u7387\u7279\u5f81\u548c\u4f7f\u7528earth mover's distance\u8fdb\u884c\u805a\u7c7b\uff0c\u6709\u6548\u63d0\u5347\u4e86AI\u5728\u590d\u6742\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u5fb7\u5dde\u6251\u514b\u7b49\u6e38\u620f\u4e2d\uff0c\u8fc7\u5ea6\u62bd\u8c61\uff08\u5c24\u5176\u662f\u5b8c\u5168\u4e22\u5f03\u5386\u53f2\u4fe1\u606f\u7684\u65e0\u8bb0\u5fc6\u62bd\u8c61\uff09\u4f1a\u4e25\u91cd\u5f71\u54cdAI\u7684\u6027\u80fd\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fak-recall\u80dc\u7387\u7279\u5f81\uff0c\u8be5\u7279\u5f81\u5229\u7528\u672a\u6765\u548c\u5386\u53f2\u4fe1\u606f\u6765\u533a\u5206\u548c\u91cf\u5316\u89c2\u5bdf\u4fe1\u606f\u96c6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86KrwEmd\u7b97\u6cd5\uff0c\u4f7f\u7528earth mover's distance\u6765\u8861\u91cf\u7279\u5f81\u5dee\u5f02\u5e76\u5bf9\u89c2\u5bdf\u4fe1\u606f\u96c6\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKrwEmd\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86AI\u7684\u6e38\u620f\u6027\u80fd\u3002", "conclusion": "KrwEmd\u662f\u7b2c\u4e00\u4e2a\u89e3\u51b3\u5927\u578b\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6e38\u620f\u4e2d\u8fc7\u5ea6\u62bd\u8c61\u95ee\u9898\u7684\u5b9e\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u548c\u672a\u6765\u4fe1\u606f\u6765\u6539\u8fdbAI\u7684\u8868\u73b0\u3002"}}
{"id": "2511.12790", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.12790", "abs": "https://arxiv.org/abs/2511.12790", "authors": ["Yi Xia"], "title": "Lattice Thermal Transport Beyond the Quasiparticle Approximation: Nontrivial Spectral Competition between Three- and Four-Phonon Interactions", "comment": null, "summary": "The breakdown of the quasiparticle approximation (QPA) for phonons in strongly anharmonic materials necessitates advanced first-principles frameworks for accurate lattice dynamics and thermal transport predictions. We develop a comprehensive beyond-quasiparticle approximation (BQPA) approach incorporating both three- (3ph) and four-phonon (4ph) interactions and apply it to investigate lattice thermal conductivity ($\u03ba_{\\rm L}$) in MgO, PbTe, and AgCl -- materials that span a broad spectrum of anharmonicity, from weak to severe anharmonic regimes with overdamped phonons. We reveal that while BQPA consistently increases $\u03ba_{\\rm L}$ relative to QPA due to phonon softening when considering only 3ph interactions, the inclusion of additional 4ph interactions hardens the phonon spectrum and suppresses this enhancement, bringing BQPA and QPA predictions into close agreement via subtle spectral competition effects across all three compounds. These findings highlight that accurate modeling of $\u03ba_{\\rm L}$ in strongly anharmonic materials requires treating both full phonon spectral function and higher-order anharmonicity on equal footing. Our work establishes a systematic framework for modeling thermal transport in systems with overdamped phonons and provides critical insights for materials design beyond the limits of conventional phonon transport theory.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u51c6\u7c92\u5b50\u8fd1\u4f3c\uff08BQPA\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4e09\u58f0\u5b50\uff083ph\uff09\u548c\u56db\u58f0\u5b50\uff084ph\uff09\u76f8\u4e92\u4f5c\u7528\uff0c\u7528\u4e8e\u8ba1\u7b97\u5f3a\u975e\u8c10\u6750\u6599\u4e2d\u7684\u6676\u683c\u70ed\u5bfc\u7387\uff08\u03baL\uff09\u3002\u7814\u7a76\u5e94\u7528\u4e8eMgO\u3001PbTe\u548cAgCl\u4e09\u79cd\u6750\u6599\uff0c\u53d1\u73b0BQPA\u5728\u8003\u86513ph\u76f8\u4e92\u4f5c\u7528\u65f6\u4f1a\u589e\u52a0\u03baL\uff0c\u4f46\u52a0\u51654ph\u76f8\u4e92\u4f5c\u7528\u540e\u4f1a\u6291\u5236\u8fd9\u79cd\u589e\u52a0\uff0c\u4f7fBQPA\u548cQPA\u7684\u9884\u6d4b\u7ed3\u679c\u8d8b\u4e8e\u4e00\u81f4\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5f3a\u975e\u8c10\u6750\u6599\u4e2d\u51c6\u786e\u6a21\u62df\u03baL\u9700\u8981\u540c\u65f6\u8003\u8651\u5b8c\u6574\u7684\u58f0\u5b50\u8c31\u51fd\u6570\u548c\u9ad8\u9636\u975e\u8c10\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u7b2c\u4e00\u6027\u539f\u7406\u6846\u67b6\u5728\u5f3a\u975e\u8c10\u6750\u6599\u4e2d\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u6676\u683c\u52a8\u529b\u5b66\u548c\u70ed\u8f93\u8fd0\uff0c\u56e0\u4e3a\u5176\u57fa\u4e8e\u51c6\u7c92\u5b50\u8fd1\u4f3c\uff08QPA\uff09\u3002\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u6846\u67b6\u6765\u5904\u7406\u975e\u8c10\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8d85\u8d8a\u51c6\u7c92\u5b50\u8fd1\u4f3c\uff08BQPA\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u540c\u65f6\u8003\u8651\u4e86\u4e09\u58f0\u5b50\uff083ph\uff09\u548c\u56db\u58f0\u5b50\uff084ph\uff09\u76f8\u4e92\u4f5c\u7528\u3002\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eMgO\u3001PbTe\u548cAgCl\u4e09\u79cd\u6750\u6599\uff0c\u7814\u7a76\u5b83\u4eec\u7684\u6676\u683c\u70ed\u5bfc\u7387\uff08\u03baL\uff09\u3002", "result": "\u5728\u53ea\u8003\u86513ph\u76f8\u4e92\u4f5c\u7528\u65f6\uff0cBQPA\u6bd4QPA\u9884\u6d4b\u7684\u03baL\u6709\u6240\u589e\u52a0\uff0c\u8fd9\u662f\u7531\u4e8e\u58f0\u5b50\u8f6f\u5316\u3002\u7136\u800c\uff0c\u5f53\u540c\u65f6\u8003\u86514ph\u76f8\u4e92\u4f5c\u7528\u65f6\uff0c\u58f0\u5b50\u8c31\u53d8\u786c\uff0c\u6291\u5236\u4e86\u03baL\u7684\u589e\u5f3a\uff0c\u4f7f\u5f97BQPA\u548cQPA\u7684\u9884\u6d4b\u7ed3\u679c\u5728\u6240\u6709\u4e09\u79cd\u6750\u6599\u4e2d\u90fd\u975e\u5e38\u63a5\u8fd1\u3002\u8fd9\u8868\u660e\u5b58\u5728\u4e00\u79cd\u5fae\u5999\u7684\u8c31\u7ade\u4e89\u6548\u5e94\u3002", "conclusion": "\u51c6\u786e\u6a21\u62df\u5f3a\u975e\u8c10\u6750\u6599\u4e2d\u7684\u03baL\u9700\u8981\u540c\u65f6\u8003\u8651\u5b8c\u6574\u7684\u58f0\u5b50\u8c31\u51fd\u6570\u548c\u9ad8\u9636\u975e\u8c10\u6027\u3002\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u6709\u8fc7\u963b\u5c3c\u58f0\u5b50\u7684\u4f53\u7cfb\u4e2d\u7684\u70ed\u8f93\u8fd0\uff0c\u5e76\u4e3a\u8d85\u8d8a\u4f20\u7edf\u58f0\u5b50\u8f93\u8fd0\u7406\u8bba\u7684\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.12137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12137", "abs": "https://arxiv.org/abs/2511.12137", "authors": ["Zheng Wang", "Yifu Li", "Yuchao Mei", "Xinyu Sui", "Qingbin Li", "Xu Luo", "Rui Wang", "Dongxin Ni", "Jian Pang"], "title": "A 24-GHz CMOS Transformer-Based Three-Tline Series Doherty Power Amplifier Achieving 39% PAE", "comment": "This paper has been accepted by ICTA2025", "summary": "This paper presents a transformer-based three- transmission-line (Tline) series Doherty power amplifier (PA) implemented in 65-nm CMOS, targeting broadband K/Ka-band applications. By integrating an impedance-scaling network into the output matching structure, the design enables effective load modulation and reduced impedance transformation ratio (ITR) at power back-off when employing stacked cascode transistors. The PA demonstrates a -3-dB small-signal gain bandwidth from 22 to 32.5 GHz, a saturated output power (Psat) of 21.6 dBm, and a peak power-added efficiency (PAE) of 39%. At 6dB back-off, the PAE remains above 24%, validating its suitability for high- efficiency mm-wave phased-array transmitters in next-generation wireless systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u538b\u5668\u4e09\u4f20\u8f93\u7ebf\uff08Tline\uff09\u4e32\u8054Doherty\u529f\u7387\u653e\u5927\u5668\uff08PA\uff09\uff0c\u91c7\u752865nm CMOS\u6280\u672f\uff0c\u9762\u5411\u5bbd\u5e26K/Ka\u6ce2\u6bb5\u5e94\u7528\u3002", "motivation": "\u901a\u8fc7\u5728\u8f93\u51fa\u5339\u914d\u7ed3\u6784\u4e2d\u96c6\u6210\u963b\u6297\u53d8\u6362\u7f51\u7edc\uff0c\u8be5\u8bbe\u8ba1\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u8d1f\u8f7d\u8c03\u5236\uff0c\u5e76\u5728\u4f7f\u7528\u5806\u53e0\u5171\u6e90\u5171\u6805\u6676\u4f53\u7ba1\u65f6\u964d\u4f4e\u529f\u7387\u56de\u9000\u65f6\u7684\u963b\u6297\u53d8\u6362\u6bd4\uff08ITR\uff09\u3002", "method": "\u63d0\u51fa\u7684Doherty PA\u91c7\u7528\u4e86\u53d8\u538b\u5668\u4e09\u4f20\u8f93\u7ebf\uff08Tline\uff09\u4e32\u8054\u7ed3\u6784\uff0c\u5e76\u96c6\u6210\u4e86\u963b\u6297\u53d8\u6362\u7f51\u7edc\u3002", "result": "PA\u572822\u81f332.5 GHz\u9891\u5e26\u5185\u5b9e\u73b0\u4e86-3 dB\u7684\u5c0f\u4fe1\u53f7\u589e\u76ca\u5e26\u5bbd\uff0c\u9971\u548c\u8f93\u51fa\u529f\u7387\uff08Psat\uff09\u4e3a21.6 dBm\uff0c\u5cf0\u503c\u529f\u7387\u9644\u52a0\u6548\u7387\uff08PAE\uff09\u4e3a39%\u3002\u57286dB\u56de\u9000\u65f6\uff0cPAE\u4ecd\u4fdd\u6301\u572824%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5PA\u7684\u6027\u80fd\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u6548\u7387\u6beb\u7c73\u6ce2\u76f8\u63a7\u9635\u53d1\u5c04\u5668\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u3002"}}
{"id": "2511.11617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11617", "abs": "https://arxiv.org/abs/2511.11617", "authors": ["Wendong Xu", "Chujie Chen", "He Xiao", "Kuan Li", "Jing Xiong", "Chen Zhang", "Wenyong Zhou", "Chaofan Tao", "Yang Bai", "Bei Yu", "Ngai Wong"], "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism", "comment": "accpeted paper by Design, Automation and Test in Europe Conference (DATE'26). 8 pages in total with 6 figures and 2 tables", "summary": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.", "AI": {"tldr": "AnchorTP\u662f\u4e00\u4e2a\u5f39\u6027\u5f20\u91cf\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591aGPU\u73af\u5883\u4e2d\u4fdd\u6301\u6a21\u578b\u72b6\u6001\u5e76\u6700\u5c0f\u5316\u6570\u636e\u79fb\u52a8\u6765\u5b9e\u73b0LLM\u63a8\u7406\u670d\u52a1\u7684\u5feb\u901f\u6062\u590d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6545\u969c\u540e\u7684\u505c\u673a\u65f6\u95f4\u548c\u6062\u590d\u65f6\u95f4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u670d\u52a1\u9700\u8981\u9ad8\u53ef\u7528\u6027\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u591aGPU\u5f20\u91cf\u5e76\u884c\uff08TP\uff09\u5bb9\u6613\u56e0\u5355GPU\u6545\u969c\u800c\u4e2d\u65ad\u670d\u52a1\u3002", "method": "AnchorTP\u5b9e\u73b0\u4e86\u5f39\u6027\u5f20\u91cf\u5e76\u884c\uff08ETP\uff09\uff0c\u652f\u6301\u4e0d\u7b49\u5bbd\u5206\u533a\u3001\u4efb\u610f\u6570\u91cf\u7684GPU\u4ee5\u53ca\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u72ec\u7acb\u7684\u5b88\u62a4\u8fdb\u7a0b\u5c06\u6a21\u578b\u53c2\u6570\u548cKV\u7f13\u5b58\u4fdd\u7559\u5728GPU\u5185\u5b58\u4e2d\u3002\u4e3a\u4e86\u51cf\u5c11\u505c\u673a\u65f6\u95f4\uff0cAnchorTP\u91c7\u7528\u57fa\u4e8e\u8fde\u7eed\u6700\u5c0f\u8fc1\u79fb\uff08CMM\uff09\u7b97\u6cd5\u7684\u5e26\u5bbd\u611f\u77e5\u89c4\u5212\u5668\u6765\u6700\u5c0f\u5316\u91cd\u8f7d\u5b57\u8282\uff0c\u5e76\u7ed3\u5408\u6d41\u6c34\u7ebf\u5316\u70b9\u5bf9\u70b9\u4f20\u8f93\u4e0e\u91cd\u8f7d\u7684\u6267\u884c\u8c03\u5ea6\u5668\u3002", "result": "AnchorTP\u5728\u5178\u578b\u6545\u969c\u573a\u666f\u4e0b\uff0c\u4e0e\u91cd\u542f\u548c\u91cd\u65b0\u52a0\u8f7d\u76f8\u6bd4\uff0c\u5c06\u9996\u6b21\u6210\u529f\u65f6\u95f4\uff08TFS\uff09\u7f29\u77ed\u9ad8\u8fbe11\u500d\uff0c\u5cf0\u503c\u6062\u590d\u65f6\u95f4\uff08TTP\uff09\u7f29\u77ed\u9ad8\u8fbe59%\u3002", "conclusion": "AnchorTP\u901a\u8fc7\u5176\u5f39\u6027\u5f20\u91cf\u5e76\u884c\u3001\u72b6\u6001\u4fdd\u6301\u673a\u5236\u548c\u4f18\u5316\u7684\u6062\u590d\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591aGPU LLM\u63a8\u7406\u4e2d\u7684\u5355\u70b9\u6545\u969c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4f4e\u635f\u8017\u7684\u670d\u52a1\u6062\u590d\uff0c\u800c\u65e0\u9700\u6539\u53d8\u670d\u52a1\u63a5\u53e3\u3002"}}
{"id": "2511.12785", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12785", "abs": "https://arxiv.org/abs/2511.12785", "authors": ["Maria Larchenko", "Dmitry Guskov", "Alexander Lobashev", "Georgy Derevyanko"], "title": "Lightweight Optimal-Transport Harmonization on Edge Devices", "comment": "AAAI 2026, Oral", "summary": "Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.", "AI": {"tldr": "MKL-Harmonizer \u7b97\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u989c\u8272\u534f\u8c03\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e (AR) \u5e94\u7528\uff0c\u901a\u8fc7\u5229\u7528\u6700\u4f18\u8f93\u8fd0\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u9ad8\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d AR \u6d41\u7a0b\u4e2d\u7f3a\u4e4f\u5b9e\u65f6\u989c\u8272\u534f\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u963b\u788d\u4e86\u5176\u96c6\u6210\u3002", "method": "\u5229\u7528\u7ecf\u5178\u6700\u4f18\u8f93\u8fd0\u7406\u8bba\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u7f16\u7801\u5668\u6765\u9884\u6d4b Monge-Kantorovich \u8f93\u8fd0\u56fe\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u4e14\u652f\u6301\u8bbe\u5907\u7aef\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "result": "MKL-Harmonizer \u7b97\u6cd5\u5728\u771f\u5b9e\u5408\u6210 AR \u56fe\u50cf\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7efc\u5408\u5f97\u5206\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u989c\u8272\u534f\u8c03\u65b9\u6cd5\u80fd\u591f\u6ee1\u8db3 AR \u5e94\u7528\u7684\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\u5c06\u6709\u52a9\u4e8e\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2511.12163", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12163", "abs": "https://arxiv.org/abs/2511.12163", "authors": ["Vlad-Matei Anghelu\u0163\u0103", "Bogdan Gheorghe", "Daniel Ioan", "Ionela Prodan", "Florin Stoican"], "title": "Tight displacement-based formation control under bounded disturbances. A set-theoretic perspective", "comment": "submitted to ECC'26", "summary": "This paper investigates the synthesis of controllers for displacement-based formation control in the presence of bounded disturbances, specifically focusing on uncertainties originating from measurement noise. While the literature frequently addresses such problems using stochastic frameworks, this work proposes a deterministic methodology grounded in set-theoretic concepts. By leveraging the principles of set invariance, we adapt the theory of ultimate boundedness to the specific dynamics of displacement-based formations. This approach provides a rigorous method for analyzing the system's behavior under persistent disturbances. Furthermore, this set-theoretic framework allows for the optimized selection of the proposed control law parameters to guarantee pre-specified performance bounds. The efficacy of the synthesized controller is demonstrated in the challenging application of maintaining tight formations in a multi-obstacles environment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u5229\u7528\u96c6\u8bba\u6982\u5ff5\u548c\u4e0d\u53d8\u96c6\u539f\u7406\uff0c\u6765\u7efc\u5408\u8003\u8651\u6709\u754c\u6270\u52a8\uff08\u5982\u6d4b\u91cf\u566a\u58f0\uff09\u4e0b\u57fa\u4e8e\u4f4d\u79fb\u7684\u7f16\u961f\u63a7\u5236\u5668\u7684\u5408\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u4fdd\u8bc1\u9884\u5b9a\u6027\u80fd\u8fb9\u754c\u7684\u4f18\u5316\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u591a\u4f7f\u7528\u968f\u673a\u6846\u67b6\u5904\u7406\u6709\u754c\u6270\u52a8\u4e0b\u7684\u7f16\u961f\u63a7\u5236\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96c6\u8bba\u6982\u5ff5\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4e0d\u53d8\u96c6\u539f\u7406\uff0c\u5c06\u6700\u7ec8\u6709\u754c\u7406\u8bba\u5e94\u7528\u4e8e\u57fa\u4e8e\u4f4d\u79fb\u7684\u7f16\u961f\u52a8\u529b\u5b66\uff0c\u5e76\u4e3a\u63a7\u5236\u5f8b\u53c2\u6570\u63d0\u4f9b\u4e86\u4f18\u5316\u9009\u62e9\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e25\u683c\u5206\u6790\u7cfb\u7edf\u5728\u6301\u7eed\u6270\u52a8\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u591a\u969c\u788d\u73af\u5883\u4e2d\u4fdd\u6301\u7d27\u5bc6\u7f16\u961f\u7684\u6311\u6218\u6027\u573a\u666f\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u8bba\u7684\u65b9\u6cd5\u4e3a\u6709\u754c\u6270\u52a8\u4e0b\u57fa\u4e8e\u4f4d\u79fb\u7684\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e25\u8c28\u4e14\u53ef\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11777", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11777", "abs": "https://arxiv.org/abs/2511.11777", "authors": ["Vinit Mehta", "Charu Sharma", "Karthick Thiyagarajan"], "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review", "comment": "45 pages, 15 figures, MDPI Sensors Journal", "summary": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.", "AI": {"tldr": "LLM\u4e0e3D\u89c6\u89c9\u7684\u878d\u5408\u6b63\u5728\u6539\u53d8\u673a\u5668\u4eba\u4f20\u611f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u73af\u5883\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u673a\u5668\u4eba\u4f20\u611f\u6280\u672f\u6765\u611f\u77e5\u3001\u63a8\u7406\u548c\u4e0e\u590d\u6742\u73af\u5883\u4ea4\u4e92\u3002", "method": "\u7efc\u8ff0\u4e86LLM\u548c3D\u89c6\u89c9\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u5305\u62ec3D\u6570\u636e\u8868\u793a\u30013D\u611f\u77e5\u6280\u672f\u3001\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u3001\u5177\u8eab\u4ee3\u7406\u3001\u591a\u6a21\u6001\u878d\u5408\u7b49\uff0c\u5e76\u4ecb\u7ecd\u4e86\u76f8\u5173\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "LLM\u4e0e3D\u89c6\u89c9\u7684\u878d\u5408\u5728\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u548c\u5177\u8eab\u4ee3\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u7406\u89e3\u548c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "LLM\u4e0e3D\u89c6\u89c9\u7684\u878d\u5408\u4e3a\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u4f20\u611f\u6280\u672f\u6307\u660e\u4e86\u65b9\u5411\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u81ea\u9002\u5e94\u6a21\u578b\u67b6\u6784\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u7684\u673a\u5668\u4eba\u4f20\u611f\u7cfb\u7edf\u3002"}}
{"id": "2511.13699", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13699", "abs": "https://arxiv.org/abs/2511.13699", "authors": ["Parikshit Gopalan", "Konstantinos Stavropoulos", "Kunal Talwar", "Pranay Tankala"], "title": "Efficient Calibration for Decision Making", "comment": "50 pages, 3 figures", "summary": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.\n  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u9884\u6d4b\u5668\u6821\u51c6\u7a0b\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6821\u51c6\u51b3\u7b56\u635f\u5931\uff08CDL\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u5728\u7279\u5b9a\u51fd\u6570\u65cf K \u4e0b\u8ba1\u7b97 CDL \u7684\u53ef\u5904\u7406\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e9b\u91cd\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "Hu and Wu (FOCS'24) \u5b9a\u4e49\u4e86CDL\u6765\u8861\u91cf\u9884\u6d4b\u5668\u6821\u51c6\uff0c\u4f46\u5b83\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u8ba1\u7b97\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3CDL\u7684\u53ef\u8ba1\u7b97\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5728\u7279\u5b9a\u7ed3\u6784\u5316\u540e\u5904\u7406\u51fd\u6570\u65cf K \u4e0b\uff0cCDL\u7684\u53ef\u4fe1\u606f\u8bba\u548c\u8ba1\u7b97\u53ef\u5904\u7406\u6027\uff0c\u5e76\u4e3a\u81ea\u7136\u51fd\u6570\u7c7bK\u63a8\u5bfc\u4e86\u4e0a\u4e0b\u754c\u3002", "result": "\u5728\u7279\u5b9a\u51fd\u6570\u65cf K \u4e0b\uff0cCDL \u662f\u53ef\u5904\u7406\u7684\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e86\u76f8\u5e94\u7684\u4e0a\u4e0b\u754c\uff0c\u4e3a\u4e00\u4e9b\u5e38\u7528\u7684\u673a\u5668\u5b66\u4e60\u91cd\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u51b3\u7b56\u5236\u5b9a\u4e2d\u7684\u6821\u51c6\u7406\u8bba\u5f15\u5165\u4e86\u65b0\u7684\u5b9a\u4e49\u548c\u7b97\u6cd5\u6280\u672f\uff0c\u5e76\u4e3a\u4e00\u4e9b\u5e7f\u6cdb\u4f7f\u7528\u7684\u673a\u5668\u5b66\u4e60\u91cd\u6821\u51c6\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u4fdd\u8bc1\u3002"}}
{"id": "2511.13075", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13075", "abs": "https://arxiv.org/abs/2511.13075", "authors": ["Chengxin Jiang", "Hui Shan Wang", "Chen Chen", "Lingxiu Chen", "Xiujun Wang", "Yibo Wang", "Ziqiang Kong", "Yuhan Feng", "Yixin Liu", "Yu Feng", "Chenxi Liu", "Yu Zhang", "Zhipeng Wei", "Maosen Guo", "Aomei Tong", "Gang Mu", "Yumeng Yang", "Kenji Watanabe", "Takashi Taniguchi", "Wangzhou Shi", "Haomin Wang"], "title": "Signatures of magnetism in zigzag graphene nanoribbon embedded in h-BN lattice", "comment": "30 pages, 17 figures", "summary": "Zigzag edges of graphene have long been predicted to exhibit magnetic electronic state near the Fermi level, which can cause spin-related phenomena and offer unique potentials for graphene-based spintronics. However, the magnetic conduction channels along these edges have yet been reported experimentally. Here, we report the observation on signatures of magnetism in zigzag graphene nanoribbons (zGNRs) embedded in hexagonal boron nitride (h-BN). The in-plane bonding with BN can stabilize the edges of zGNRs, and thus enable a direct probing of the intrinsic magnetism. Firstly, the presence of magnetism of a zGNR was confirmed by scanning NV center microscopy. And then, zGNR was fabricated into a transistor with a width of ~9 nm wide and a channel length of sub-50 nm. By performing magneto-transport measurements, Fabry-P\u00e9rot interference patterns were observed in the transistor at 4 Kelvin, which indicates a coherent transport through the channel. A large magnetoresistance of ~175 \u03a9, corresponding to a ratio of ~1.3 %, was observed at the same temperature. More importantly, such magneto-transport signal is highly anisotropic on the magnetic field direction, and its appearance extends well above room temperature. All these evidences corroborate the existence of robust magnetic ordering in the edge state of zGNR. The findings on zGNR embedded in h-BN provide an effective platform for the future exploration of graphene-based spintronic devices.", "AI": {"tldr": "\u77f3\u58a8\u70ef\u952f\u9f7f\u8fb9\u7f18\u56e0\u8d39\u7c73\u80fd\u7ea7\u9644\u8fd1\u7684\u78c1\u6027\u7535\u5b50\u6001\u800c\u5177\u6709\u72ec\u7279\u7684\u81ea\u65cb\u7535\u5b50\u5b66\u6f5c\u529b\uff0c\u4f46\u6b64\u524d\u5c1a\u672a\u6709\u5b9e\u9a8c\u62a5\u9053\u3002\u672c\u7814\u7a76\u9996\u6b21\u5728\u6c2e\u5316\u787c\uff08h-BN\uff09\u57fa\u5e95\u7684\u952f\u9f7f\u5f62\u77f3\u58a8\u70ef\u7eb3\u7c73\u5e26\uff08zGNRs\uff09\u4e2d\u89c2\u6d4b\u5230\u78c1\u6027\u7279\u5f81\u3002", "motivation": "\u89c2\u6d4b\u77f3\u58a8\u70ef\u952f\u9f7f\u8fb9\u7f18\u7684\u78c1\u6027\u7535\u5b50\u6001\uff0c\u63a2\u7d22\u5176\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5229\u7528\u626b\u63cfNV\u8272\u5fc3\u663e\u5fae\u955c\u786e\u8ba4zGNRs\u7684\u78c1\u6027\uff1b\u5236\u5907\u5bbd\u7ea69 nm\uff0c\u6c9f\u9053\u957f\u5ea6\u5c0f\u4e8e50 nm\u7684zGNRs\u6676\u4f53\u7ba1\uff1b\u57284\u5f00\u5c14\u6587\u4e0b\u8fdb\u884c\u78c1\u8f93\u8fd0\u6d4b\u91cf\uff0c\u89c2\u5bdf\u6cd5\u5e03\u91cc-\u73c0\u7f57\u5e72\u6d89\u548c\u78c1\u963b\u6548\u5e94\uff1b\u7814\u7a76\u78c1\u963b\u6548\u5e94\u7684\u78c1\u573a\u65b9\u5411\u5404\u5411\u5f02\u6027\u53ca\u5176\u6e29\u5ea6\u4f9d\u8d56\u6027\u3002", "result": "\u6210\u529f\u89c2\u6d4b\u5230zGNRs\u7684\u78c1\u6027\uff1b\u5728\u4f4e\u6e29\u4e0b\u89c2\u5bdf\u5230\u6cd5\u5e03\u91cc-\u73c0\u7f57\u5e72\u6d89\uff0c\u8868\u660e\u6c9f\u9053\u5185\u7684\u76f8\u5e72\u8f93\u8fd0\uff1b\u89c2\u6d4b\u5230\u7ea6175\u6b27\u59c6\u7684\u78c1\u963b\u6548\u5e94\uff0c\u76f8\u5bf9\u53d8\u5316\u7ea6\u4e3a1.3%\uff1b\u53d1\u73b0\u78c1\u8f93\u8fd0\u4fe1\u53f7\u5177\u6709\u663e\u8457\u7684\u78c1\u573a\u65b9\u5411\u5404\u5411\u5f02\u6027\uff0c\u4e14\u8be5\u4fe1\u53f7\u5b58\u5728\u4e8e\u9ad8\u4e8e\u5ba4\u6e29\u7684\u6e29\u5ea6\u4e0b\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86zGNRs\u8fb9\u7f18\u6001\u5177\u6709\u9c81\u68d2\u7684\u78c1\u6709\u5e8f\u6027\uff0c\u5e76\u4e14\u5d4c\u5165h-BN\u7684zGNRs\u4e3a\u672a\u6765\u63a2\u7d22\u77f3\u58a8\u70ef\u57fa\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e73\u53f0\u3002"}}
{"id": "2511.12697", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12697", "abs": "https://arxiv.org/abs/2511.12697", "authors": ["Soumendra Kumar Das", "Prasanjit Samal", "Brahmananda Chakraborty", "Sridhar Sahu"], "title": "Unraveling the Surface Stability and Chemical Reactivity of Aza-Triphenylene Monolayer under O$_2$ and H$_2$O Exposure", "comment": null, "summary": "Environmental oxidation has a great impact in tuning the physical, chemical and electronic properties of two-dimensional (2D) monolayers which can affect their practical applications in nanoscale engineering devices under ambient conditions. aza-triphenylene is a recently synthesized 2D materials whose practcal applications have not been systematically studied yet. In this study, we report for the first time, the adsorption and dissociation of O$_2$ and H$_2$O molecules on the surface of 2D aza-triphenylene monolayer through first principles calculations in combination with climbing image nudged elastic band (CINEB) method. The results indicates that both the O$_2$ and H$_2$O molecules weakly interact over the monolayer surface with an adsorption energy -0.16 eV and -0.37 eV respectively. In contrast, both the molecules exhibit resistance for dissociation due to the formation of energy barriers. The transition path indicates that molecular oxygen experience two energy barriers (0.16 ev and 1.22 eV) before getting dissociated atomic oxygen. However, the dissociation of H$_2$O requires larger energy barrier (2.3 eV and 0.86 eV) due to breaking of covalent bonds and transfer of hydrogen. The strong chemical adsorption of atomic oxygen and H$^+$/OH$^-$ ions is due to the significant charge transfer from monolayer to the adsorbate as evidenced from the charge density difference and Bader charge analysis. Moreover, the dissociated configuration exhibit a larger band gap as compared to the pristine aza-triphenylene due to the strong hybridization between the p states of carbon and oxygen. our work predicts the robustness of azatriphylene monolayer against oxygen/water exposer thus ensuring their stability for device applications using these materials.", "AI": {"tldr": "\u73af\u5883\u6c27\u5316\u4f1a\u5f71\u54cd\u4e8c\u7ef4\u6750\u6599\u7684\u6027\u8d28\uff0c\u4f462D aza-triphenylene\u5355\u5c42\u819c\u5bf9\u6c27\u6c14\u548c\u6c34\u7684\u5438\u9644\u548c\u79bb\u89e3\u6709\u5f88\u5f3a\u7684\u62b5\u6297\u529b\uff0c\u8868\u660e\u5176\u5728\u5668\u4ef6\u5e94\u7528\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u73af\u5883\u6c27\u5316\uff08O2\u548cH2O\u5438\u9644\u548c\u79bb\u89e3\uff09\u5bf92D aza-triphenylene\u5355\u5c42\u819c\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u7eb3\u7c73\u5de5\u7a0b\u5668\u4ef6\u4e2d\u7684\u5e94\u7528\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548cCINEB\u65b9\u6cd5\u7814\u7a76O2\u548cH2O\u57282D aza-triphenylene\u5355\u5c42\u819c\u8868\u9762\u7684\u5438\u9644\u548c\u79bb\u89e3\u8fc7\u7a0b\u3002", "result": "O2\u548cH2O\u5206\u5b50\u4e0e\u5355\u5c42\u819c\u5f31\u76f8\u4e92\u4f5c\u7528\uff08\u5438\u9644\u80fd\u5206\u522b\u4e3a-0.16 eV\u548c-0.37 eV\uff09\uff0c\u4f46\u79bb\u89e3\u8fc7\u7a0b\u9700\u8981\u8f83\u9ad8\u7684\u80fd\u91cf\u52bf\u5792\uff08O2\u4e3a0.16 eV\u548c1.22 eV\uff0cH2O\u4e3a2.3 eV\u548c0.86 eV\uff09\u3002\u79bb\u89e3\u540e\u7684\u4ea7\u7269\uff08\u539f\u5b50\u6c27\u3001H+\u3001OH-\uff09\u4e0e\u5355\u5c42\u819c\u53d1\u751f\u5f3a\u5316\u5b66\u5438\u9644\uff0c\u5e76\u5bfc\u81f4\u5e26\u9699\u589e\u5927\u3002", "conclusion": "2D aza-triphenylene\u5355\u5c42\u819c\u5bf9\u6c27\u6c14\u548c\u6c34\u66b4\u9732\u5177\u6709\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5668\u4ef6\u5e94\u7528\u3002"}}
{"id": "2511.11735", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11735", "abs": "https://arxiv.org/abs/2511.11735", "authors": ["Yonatan Sverdlov", "Eitan Rosen", "Nadav Dym"], "title": "Toward bilipshiz geometric models", "comment": null, "summary": "Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.\n  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.12297", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12297", "abs": "https://arxiv.org/abs/2511.12297", "authors": ["Angqi Liu", "Filippo Moro", "Sebastian Billaudelle", "Melika Payvand"], "title": "A Linear Implementation of an Analog Resonate-and-Fire Neuron", "comment": null, "summary": "Oscillatory dynamics have recently proven highly effective in machine learning (ML), particularly through State-Space-Models (SSM) that leverage structured linear recurrences for long-range temporal processing. Resonate-and-Fire neurons capture such oscillatory behavior in a spiking framework, offering strong expressivity with sparse event-based communication. While early analog RAF circuits employed nonlinear coupling and suffered from process sensitivity, modern ML practice favors linear recurrence. In this work, we introduce a resonate-and-fire (RAF) neuron, built in 22nm Fully-Depleted Silicon-on-Insulator technology, that aligns with SSM principles while retaining the efficiency of spike-based communication. We analyze its dynamics, linearity, and resilience to Process, Voltage, and Temperature variations, and evaluate its power, performance, and area trade-offs. We map the characteristics of our circuit into a system-level simulation where our RAF neuron is utilized in a keyword-spotting task, showing that its non-idealities do not hinder performance. Our results establish RAF neurons as robust, energy-efficient computational primitives for neuromorphic hardware.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7b26\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u539f\u5219\u7684\u5171\u632f\u53d1\u653e\uff08RAF\uff09\u795e\u7ecf\u5143\u7535\u8def\uff0c\u8be5\u7535\u8def\u572822nm FD-SOI\u6280\u672f\u4e0b\u5b9e\u73b0\uff0c\u5e76\u4fdd\u7559\u4e86\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u7684\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5c06\u6709\u6548\u7684SSM\u7ebf\u6027\u9012\u63a8\u539f\u7406\u4e0e\u5177\u6709\u7a00\u758f\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u7684RAF\u795e\u7ecf\u5143\u7ed3\u5408\uff0c\u540c\u65f6\u514b\u670d\u65e9\u671fRAF\u7535\u8def\u7684\u975e\u7ebf\u6027\u8026\u5408\u548c\u5de5\u827a\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5236\u9020\u4e86\u4e00\u79cd\u57fa\u4e8e22nm FD-SOI\u6280\u672f\u7684RAF\u795e\u7ecf\u5143\u7535\u8def\uff0c\u5206\u6790\u4e86\u5176\u52a8\u6001\u7279\u6027\u3001\u7ebf\u6027\u5ea6\u4ee5\u53ca\u5bf9\u5de5\u827a\u3001\u7535\u538b\u548c\u6e29\u5ea6\uff08PVT\uff09\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u7cfb\u7edf\u7ea7\u6a21\u62df\u4e2d\u8bc4\u4f30\u4e86\u5176\u5728\u5173\u952e\u8bcd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684RAF\u795e\u7ecf\u5143\u7535\u8def\u5728\u7cfb\u7edf\u7ea7\u6a21\u62df\u4e2d\u88ab\u7528\u4e8e\u5173\u952e\u8bcd\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u4e14\u5176\u975e\u7406\u60f3\u7279\u6027\u5e76\u672a\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u540c\u65f6\u5728\u529f\u8017\u3001\u6027\u80fd\u548c\u9762\u79ef\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6743\u8861\u3002", "conclusion": "RAF\u795e\u7ecf\u5143\u662f\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u7a33\u5065\u4e14\u8282\u80fd\u7684\u8ba1\u7b97\u57fa\u5143\u3002"}}
{"id": "2511.11619", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11619", "abs": "https://arxiv.org/abs/2511.11619", "authors": ["Yuanjie Liu", "Wenpeng Xing", "Ye Zhou", "Gaowei Chang", "Changting Lin", "Meng Han"], "title": "DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack", "comment": null, "summary": "The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.\n  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.\n  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.\n  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.", "AI": {"tldr": "DIAP\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u4ee3\u7406\u7684\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u6846\u67b6\uff0c\u4f7f\u7528IPFS/IPNS\u548c\u96f6\u77e5\u8bc6\u8bc1\u660e\u5b9e\u73b0\u6301\u4e45\u3001\u53ef\u9a8c\u8bc1\u548c\u65e0\u9700\u4fe1\u4efb\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u5f53\u524d\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u534f\u8bae\u5728\u53ef\u9a8c\u8bc1\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4e2d\u5fc3\u5316\u4e2d\u4ecb\u6216\u7f3a\u4e4f\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u89e3\u6790\u673a\u5236\u3002", "method": "\u63d0\u51faDIAP\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u8eab\u4efd\u7ed1\u5b9a\u5230IPFS/IPNS\u6807\u8bc6\u7b26\uff0c\u5229\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u8fdb\u884c\u6240\u6709\u6743\u9a8c\u8bc1\u3002\u63d0\u4f9bRust SDK\uff0c\u96c6\u6210Noir\u3001DID-Key\u3001IPFS\u548cLibp2p GossipSub/Iroh\u3002\u91c7\u7528\u96f6\u4f9d\u8d56\u7684ZKP\u90e8\u7f72\u6a21\u578b\u3002", "result": "DIAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u94fe\u5373\u53ef\u5b9e\u73b0\u5373\u65f6\u3001\u53ef\u9a8c\u8bc1\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u8eab\u4efd\u8bc1\u660e\u7684\u65b9\u6cd5\u3002", "conclusion": "DIAP\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u548c\u4ee3\u7406\u95f4\u7ecf\u6d4e\u5960\u5b9a\u4e86\u5b9e\u9645\u3001\u9ad8\u6027\u80fd\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12935", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12935", "abs": "https://arxiv.org/abs/2511.12935", "authors": ["Dianbing Xi", "Guoyuan An", "Jingsen Zhu", "Zhijian Liu", "Yuan Liu", "Ruiyuan Zhang", "Jiayuan Lu", "Rui Wang", "Yuchi Huo"], "title": "PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos", "comment": "Accepted by AAAI 2026", "summary": "We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.", "AI": {"tldr": "PFAvatar\u662f\u4e00\u79cd\u4eceOOTD\u7167\u7247\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u5934\u50cf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a1. \u51e0\u5f20OOTD\u7167\u7247\u5fae\u8c03\u59ff\u52bf\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\uff1b2. \u84b8\u998f\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u8868\u793a\u76843D\u5934\u50cf\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u56fe\u50cf\u5206\u89e3\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u7ec6\u8282\uff0c\u5b9e\u73b0\u4e8648\u500d\u7684\u52a0\u901f\u3002\u4e0e\u7f51\u683c\u8868\u793a\u76f8\u6bd4\uff0cNeRF\u8868\u793a\u80fd\u66f4\u597d\u5730\u5904\u7406\u906e\u6321\u548c\u9ad8\u9891\u7eb9\u7406\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPFAvatar\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u865a\u62df\u8bd5\u7a7f\u3001\u52a8\u753b\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eceOOTD\u7167\u7247\u91cd\u5efa3D\u5934\u50cf\u65f6\uff0c\u901a\u5e38\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u5404\u4e2a\u7ec4\u4ef6\u8fdb\u884c3D\u7ec4\u88c5\uff0c\u5bb9\u6613\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u7f51\u683c\u8868\u793a\u5728\u5904\u7406\u906e\u6321\u548c\u9ad8\u9891\u7eb9\u7406\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5efa\u6a21\u5168\u8eab\u5916\u89c2\u3001\u5904\u7406\u906e\u6321\u548c\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\uff0c\u5e76\u5b9e\u73b0\u5feb\u901f\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\u3002", "method": "PFAvatar\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u59ff\u52bf\u611f\u77e5\u6269\u6563\u6a21\u578b\u5fae\u8c03\uff1a\u5229\u7528\u51e0\u5f20OOTD\u7167\u7247\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u7684ControlNet\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u5148\u9a8c\u4fdd\u6301\u635f\u5931\uff08CPPL\uff09\u6765\u4fc3\u8fdb\u7aef\u5230\u7aef\u5b66\u4e60\u548c\u51cf\u5c11\u8bed\u8a00\u6f02\u79fb\u30022. NeRF\u8868\u793a\u76843D\u5934\u50cf\u84b8\u998f\uff1a\u4f7f\u7528\u57fa\u4e8eNeRF\u7684\u5934\u50cf\u8868\u793a\uff0c\u901a\u8fc7\u6807\u51c6SMPL-X\u7a7a\u95f4\u91c7\u6837\u548c\u591a\u5206\u8fa8\u73873D-SDS\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u5e76\u6b63\u786e\u5904\u7406\u906e\u6321\u3002", "result": "PFAvatar\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u5bf9\u906e\u6321/\u622a\u65ad\u7684\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4e0e\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u76f8\u6bd4\uff0c\u5176\u8fde\u7eed\u8f90\u5c04\u573a\u80fd\u591f\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\uff08\u5982\u5934\u53d1\uff09\u5e76\u6b63\u786e\u5904\u7406\u906e\u6321\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e865\u5206\u949f\u7684\u4e2a\u6027\u5316\uff0c\u901f\u5ea6\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u5feb48\u500d\u3002", "conclusion": "PFAvatar\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5feb\u901f\u76843D\u5934\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u4eceOOTD\u7167\u7247\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u5934\u50cf\u3002\u5b83\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u5904\u7406\u906e\u6321\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u7684\u4e0b\u6e38\u5e94\u7528\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u96453D\u5934\u50cf\u751f\u6210\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2511.12616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12616", "abs": "https://arxiv.org/abs/2511.12616", "authors": ["Arya Parameshwara"], "title": "SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration", "comment": "10 pages, 7 figures, conference-style formatting", "summary": "This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.", "AI": {"tldr": "SynapticCore-X\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u795e\u7ecf\u5904\u7406\u67b6\u6784\uff0c\u9488\u5bf9\u4f4e\u6210\u672cFPGA\u5e73\u53f0\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u7684\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u548c\u786c\u4ef6\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u4f4e\u6210\u672cFPGA\u5e73\u53f0\u63d0\u4f9b\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u8d44\u6e90\u9ad8\u6548\u4e14\u53ef\u5b8c\u5168\u5f00\u6e90\u7684\u795e\u7ecf\u5904\u7406\u67b6\u6784\uff0c\u4ee5\u964d\u4f4e\u795e\u7ecf\u5fae\u67b6\u6784\u7814\u7a76\u7684\u95e8\u69db\u3002", "method": "\u96c6\u6210\u8f7b\u91cf\u7ea7RISC-V\u63a7\u5236\u6838\u5fc3\u548c\u53ef\u914d\u7f6e\u7684\u795e\u7ecf\u8ba1\u7b97\u74e6\u7247\uff0c\u652f\u6301\u77e9\u9635\u3001\u6fc0\u6d3b\u548c\u6570\u636e\u79fb\u52a8\u64cd\u4f5c\u7684\u878d\u5408\u3002\u63d0\u4f9b\u5b8c\u5168\u5f00\u6e90\u7684SystemVerilog\u5fae\u67b6\u6784\uff0c\u5141\u8bb8\u8c03\u6574\u5e76\u884c\u5ea6\u3001\u7247\u4e0a\u5185\u5b58\u6df1\u5ea6\u548cDMA\u7a81\u53d1\u884c\u4e3a\u3002", "result": "\u5728Zynq-7020\u4e0a\u5b9e\u73b0\u4e86100 MHz\u7684\u65f6\u5e8f\u6536\u655b\uff0c\u8d44\u6e90\u5360\u7528\u7387\u4f4e\uff086.1% LUTs, 32.5% DSPs, 21.4% BRAMs\uff09\u3002\u786c\u4ef6\u9a8c\u8bc1\u786e\u8ba4\u4e86\u5bc4\u5b58\u5668\u7ea7\u6267\u884c\u3001\u786e\u5b9a\u6027\u63a7\u5236\u6d41\u548c\u5faa\u73af\u7cbe\u786e\u7684\u6027\u80fd\u3002", "conclusion": "SynapticCore-X\u8bc1\u660e\u4e86\u80fd\u5728\u666e\u901a\u7684\u6559\u80b2FPGA\u4e0a\u539f\u578b\u5316\u5f00\u53d1\u9ad8\u80fd\u6548\u7684\u7c7bNPU\u52a0\u901f\u5668\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5b66\u672f\u754c\u548c\u5f00\u653e\u786c\u4ef6\u5728\u795e\u7ecf\u5fae\u67b6\u6784\u7814\u7a76\u9886\u57df\u7684\u5165\u95e8\u95e8\u69db\u3002"}}
{"id": "2511.12175", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12175", "abs": "https://arxiv.org/abs/2511.12175", "authors": ["Koushik Ahmed Kushal", "Florimond Gueniat"], "title": "AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach", "comment": "12 pages, 6 figures, includes simulation and evaluation results", "summary": "This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u7269\u8054\u7f51\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u5b9e\u73b0\u667a\u80fd\u5fae\u7535\u7f51\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u53ef\u8d1f\u62c5\u6027\u4f18\u5316\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5206\u5e03\u5f0f\u5fae\u7535\u7f51\u73af\u5883\u7684\u53ef\u9760\u6027\u548c\u80fd\u6e90\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u3001\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6545\u969c\u9884\u6d4b\u548c\u6210\u672c\u611f\u77e5\u8fd0\u8425\u5206\u6790\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u7269\u8054\u7f51\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u540c\u6b65\u7269\u7406\u5fae\u7535\u7f51\u7ec4\u4ef6\u548c\u865a\u62df\u6570\u5b57\u5b6a\u751f\uff0c\u4ee5\u5b9e\u73b0\u7ec4\u4ef6\u9000\u5316\u3001\u52a8\u6001\u8d1f\u8f7d\u7ba1\u7406\u548c\u4f18\u5316\u7ef4\u62a4\u8ba1\u5212\u7684\u65e9\u671f\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\uff0c\u4e0e\u57fa\u7ebf\u5fae\u7535\u7f51\u7ba1\u7406\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u5f97\u5230\u63d0\u9ad8\uff0c\u8fd0\u884c\u505c\u673a\u65f6\u95f4\u51cf\u5c11\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u53ef\u8861\u91cf\u7684\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6570\u5b57\u5b6a\u751f\u9a71\u52a8\u7684\u7269\u8054\u7f51\u67b6\u6784\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u548c\u53ef\u8d1f\u62c5\u80fd\u6e90\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11840", "abs": "https://arxiv.org/abs/2511.11840", "authors": ["Shuangyu Xie", "Kaiyuan Chen", "Wenjing Chen", "Chengyuan Qian", "Christian Juette", "Liu Ren", "Dezhen Song", "Ken Goldberg"], "title": "LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles", "comment": null, "summary": "When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.", "AI": {"tldr": "\u5f53\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\uff0c\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u53ef\u4ee5\u5bfb\u6c42\u8fdc\u7a0b\u4eba\u5de5\u64cd\u4f5c\u5458\u7684\u5e2e\u52a9\uff0c\u4ee5\u83b7\u5f97\u9ad8\u5c42\u6307\u5bfc\u3002\u4e3a\u89e3\u51b3\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u5de5\u54cd\u5e94\u65f6\u95f4\u5e26\u6765\u7684\u5173\u952e\u51b3\u7b56\u65f6\u673a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LAVQA\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\u7684\u5ef6\u8fdf\u611f\u77e5\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\u3002LAVQA \u901a\u8fc7\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u56fe\uff08LICOM\uff09\u589e\u5f3a\u4e86\u89c6\u89c9\u67e5\u8be2\uff0cLICOM \u662f\u4e00\u4e2a\u52a8\u6001\u6f14\u53d8\u7684\u5730\u56fe\uff0c\u540c\u65f6\u8868\u793a\u65f6\u95f4\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u4f7f\u5f97\u8fdc\u7a0b\u64cd\u4f5c\u5458\u80fd\u591f\u89c2\u5bdf\u5230\u5728\u52a8\u6001\u969c\u788d\u7269\u548c\u5ef6\u8fdf\u54cd\u5e94\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u8f66\u8f86\u5b89\u5168\u533a\u57df\u968f\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u53d8\u5316\u3002\u5728 CARLA \u4e2d\u8fdb\u884c\u7684\u95ed\u73af\u6a21\u62df\u8868\u660e\uff0c\u4e0e\u5ffd\u7565\u5ef6\u8fdf\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cLAVQA \u53ef\u5c06\u78b0\u649e\u7387\u964d\u4f4e 8 \u500d\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u9762\u4e34\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u5b83\u4eec\u53ef\u80fd\u9700\u8981\u6682\u505c\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u5e76\u80fd\u591f\u4ece\u8fdc\u7a0b\u4eba\u5de5\u64cd\u4f5c\u5458\u90a3\u91cc\u83b7\u5f97\u9ad8\u5c42\u6307\u5bfc\u3002\u8fd9\u79cd\u79f0\u4e3a\u201c\u5171\u4eab\u81ea\u4e3b\u201d\u7684\u6a21\u5f0f\u5141\u8bb8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u8fdc\u7a0b\u64cd\u4f5c\u5458\u5171\u540c\u5236\u5b9a\u9002\u5f53\u7684\u54cd\u5e94\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u5de5\u54cd\u5e94\u65f6\u95f4\u53ef\u53d8\uff0c\u5173\u952e\u51b3\u7b56\u7684\u65f6\u673a\u6210\u4e3a\u4e86\u4e00\u4e2a\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LAVQA \u7684\u5ef6\u8fdf\u611f\u77e5\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\u3002LAVQA \u7684\u6838\u5fc3\u662f\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u56fe\uff08LICOM\uff09\uff0c\u5b83\u662f\u4e00\u4e2a\u52a8\u6001\u6f14\u53d8\u7684\u5730\u56fe\uff0c\u80fd\u591f\u540c\u65f6\u8868\u793a\u65f6\u95f4\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7 LICOM\uff0cLAVQA \u589e\u5f3a\u4e86\u5bf9\u8f66\u8f86\u5468\u56f4\u73af\u5883\u7684\u89c6\u89c9\u67e5\u8be2\uff0c\u4f7f\u8fdc\u7a0b\u64cd\u4f5c\u5458\u80fd\u591f\u89c2\u5bdf\u5230\u8f66\u8f86\u5b89\u5168\u533a\u57df\u5982\u4f55\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u52a8\u6001\u969c\u788d\u7269\u548c\u901a\u4fe1\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u3002", "result": "\u5728 CARLA\uff08\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u6a21\u62df\u7684\u5b9e\u9645\u6807\u51c6\uff09\u4e2d\u8fdb\u884c\u7684\u95ed\u73af\u6a21\u62df\u8868\u660e\uff0cLAVQA \u6846\u67b6\u80fd\u591f\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u3002\u4e0e\u5ffd\u7565\u5ef6\u8fdf\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cLAVQA \u53ef\u4ee5\u5c06\u78b0\u649e\u7387\u964d\u4f4e 8 \u500d\u4ee5\u4e0a\u3002", "conclusion": "LAVQA \u6846\u67b6\u901a\u8fc7\u96c6\u6210 VQA \u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\uff0c\u5e76\u5f15\u5165\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u56fe\uff08LICOM\uff09\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u7f51\u7edc\u5ef6\u8fdf\u4e0b\u7684\u5171\u4eab\u81ea\u4e3b\u95ee\u9898\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u964d\u4f4e\u78b0\u649e\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u652f\u6301\u3002"}}
{"id": "2511.12258", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2511.12258", "abs": "https://arxiv.org/abs/2511.12258", "authors": ["Ju Gao", "Fang Shen"], "title": "Transitional Bell Correlation from Dirac Wavepackets", "comment": "4 pages, 1 figure", "summary": "We derive a closed-form expression for the Bell--CHSH correlation of entangled, counter-propagating electrons using realistic Dirac wavepackets and localized detection. In contrast to the conventional distance-independent result, the Bell parameter evolves continuously from the quantum bound $2\\sqrt{2}$ to the classical limit $2$ as the spatial overlap of the two waves decreases. The quantum enhancement arises entirely from transverse overlap, showing that the Bell violation reflects the local overlap of propagating Dirac waves rather than any action at a distance.", "AI": {"tldr": "\u8d1d\u5c14\u53c2\u6570\u968f\u7a7a\u95f4\u91cd\u53e0\u5ea6\u53d8\u5316\uff0c\u91cf\u5b50\u589e\u5f3a\u6e90\u4e8e\u6a2a\u5411\u91cd\u53e0\u3002", "motivation": "\u63a2\u7d22\u7ea0\u7f20\u7c92\u5b50\u5728\u4e0d\u540c\u7a7a\u95f4\u91cd\u53e0\u5ea6\u4e0b\u7684\u8d1d\u5c14-CHSH\u5173\u8054\uff0c\u5e76\u4e0e\u4f20\u7edf\u72ec\u7acb\u4e8e\u8ddd\u79bb\u7684\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u7684\u72c4\u62c9\u514b\u6ce2\u5305\u548c\u5c40\u90e8\u63a2\u6d4b\uff0c\u63a8\u5bfc\u51fa\u7ea0\u7f20\u7684\u3001\u53cd\u5411\u4f20\u64ad\u7684\u7535\u5b50\u7684\u8d1d\u5c14-CHSH\u5173\u8054\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u4e0e\u4f20\u7edf\u7ed3\u679c\u4e0d\u540c\uff0c\u8d1d\u5c14\u53c2\u6570\u4ece\u91cf\u5b50\u8fb9\u754c $2\nvarchar{2}$ \u8fde\u7eed\u6f14\u53d8\u4e3a\u7ecf\u5178\u6781\u9650 $2$\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u4e24\u4e2a\u6ce2\u7684\u91cd\u53e0\u7a0b\u5ea6\u3002\u91cf\u5b50\u589e\u5f3a\u5b8c\u5168\u6765\u81ea\u6a2a\u5411\u91cd\u53e0\u3002", "conclusion": "\u8d1d\u5c14\u8fdd\u80cc\u53cd\u6620\u4e86\u4f20\u64ad\u7684\u72c4\u62c9\u514b\u6ce2\u7684\u5c40\u90e8\u91cd\u53e0\uff0c\u800c\u4e0d\u662f\u4efb\u4f55\u8d85\u8ddd\u4f5c\u7528\u3002"}}
{"id": "2511.11654", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11654", "abs": "https://arxiv.org/abs/2511.11654", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Convergence of Multiagent Learning Systems for Traffic control", "comment": "14 pages 2 figures", "summary": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.", "AI": {"tldr": "\u672c\u6587\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u7528\u4e8e\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u7814\u7a76\u4e2d\u5b9e\u8bc1\u6709\u6548\u6027\u4e0e\u7406\u8bba\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": " Bangalore\u7b49\u57ce\u5e02\u5feb\u901f\u7684\u57ce\u5e02\u5316\u8fdb\u7a0b\u5bfc\u81f4\u4e86\u4e25\u91cd\u7684\u4ea4\u901a\u62e5\u5835\uff0c\u4f7f\u5f97\u9ad8\u6548\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08TSC\uff09\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5df2\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u51cf\u5c11\u5e73\u5747\u901a\u52e4\u5ef6\u8bef\uff0c\u4f46\u5bf9\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u5229\u7528\u968f\u673a\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5bf9\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7b97\u6cd5\u7684\u5b66\u4e60\u52a8\u6001\u8fdb\u884c\u4e86\u6b63\u5f0f\u5206\u6790\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6536\u655b\uff0c\u4ece\u800c\u5c06\u5355\u667a\u80fd\u4f53\u5f02\u6b65\u503c\u8fed\u4ee3\u7684\u6536\u655b\u6027\u8bc1\u660e\u6269\u5c55\u5230\u4e86\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u4f7f\u7528\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e2d\u5177\u6709\u6536\u655b\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\u3002"}}
{"id": "2511.13177", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.13177", "abs": "https://arxiv.org/abs/2511.13177", "authors": ["Lefan Dolg", "Moritz Scharfst\u00e4dt", "Andrea Bergschneider", "Dante M. Kennes", "Silvia Viola-Kusminskiy"], "title": "Numerical investigation of electrostatically confined excitons in monolayer $\\text{MoSe}_2$", "comment": "11 pages, 9 figures", "summary": "We investigate exciton confinement to a quantum wire in monolayer $\\text{MoSe}_2$ where the confinement is achieved by a p-i-n junction. We employ an effective-mass exciton model and solve the problem numerically, reflecting device geometries found in experimental state-of-the-art set up. Our method allows us to investigate the entire spectrum of confined states. We show the emergence of quantum confinement and study the dependence of the confined states as a function of electrical gate voltages, which are experimentally tunable parameters. We find that the confined states can be divided into bright and dark states with the dark states having small but finite oscillator strengths. Their oscillator strengths are low enough that they have not yet been detected in experiments, whereas the spectrum of the bright exciton states reproduces recent experimental measurements. Our results provide insight into the theoretical background of confined exciton states beyond the ground state and pave the way for the development of new confinement schemes as well as avenues to access the previously not detected dark states.", "AI": {"tldr": "\u901a\u8fc7p-i-n\u7ed3\u7684\u91cf\u5b50\u9650\u5236\u6765\u5b9e\u73b0\u5355\u5206\u5b50\u5c42MoSe2\u4e2d\u7684\u6fc0\u5b50\u9650\u5236\uff0c\u5e76\u5bf9\u9650\u5236\u6001\u7684\u5149\u8c31\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5355\u5206\u5b50\u5c42MoSe2\u4e2d\u6fc0\u5b50\u5728p-i-n\u7ed3\u91cf\u5b50\u9650\u5236\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u7406\u89e3\u548c\u5229\u7528\u53d7\u9650\u6fc0\u5b50\u6001\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u91c7\u7528\u6709\u6548\u8d28\u91cf\u6fc0\u5b50\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u8003\u8651\u4e86\u5b9e\u9a8c\u88c5\u7f6e\u51e0\u4f55\u5f62\u72b6\u7684\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\uff0c\u4ee5\u7814\u7a76\u6574\u4e2a\u53d7\u9650\u6001\u5149\u8c31\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u660e\u4eae\u6001\u548c\u6697\u6de1\u6001\u6fc0\u5b50\uff0c\u5176\u4e2d\u6697\u6de1\u6001\u5177\u6709\u5fae\u5c0f\u4f46\u6709\u9650\u7684\u632f\u52a8\u5b50\u5f3a\u5ea6\u3002\u8be5\u6a21\u578b\u80fd\u591f\u91cd\u73b0\u8fd1\u671f\u5b9e\u9a8c\u4e2d\u6d4b\u91cf\u7684\u4eae\u6001\u6fc0\u5b50\u5149\u8c31\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u53d7\u9650\u6fc0\u5b50\u6001\uff08\u5305\u62ec\u57fa\u6001\u4ee5\u4e0a\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u4e3a\u5f00\u53d1\u65b0\u7684\u9650\u5236\u65b9\u6848\u548c\u63a2\u6d4b\u672a\u88ab\u53d1\u73b0\u7684\u6697\u6001\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2511.11751", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11751", "abs": "https://arxiv.org/abs/2511.11751", "authors": ["Sanchit Sinha", "Guangzhi Xiong", "Zhenghao He", "Aidong Zhang"], "title": "Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models", "comment": "AAAI 2026 (oral)", "summary": "Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.", "AI": {"tldr": "Concept-RuleNet\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ece\u56fe\u50cf\u6570\u636e\u4e2d\u6316\u6398\u89c6\u89c9\u6982\u5ff5\u6765\u589e\u5f3a\u7b26\u53f7\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u548c\u89c6\u89c9\u57fa\u7840\uff0c\u4ece\u800c\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u91ca\u5176\u51b3\u7b56\u539f\u56e0\u65b9\u9762\u5374\u5f88\u8584\u5f31\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5206\u5e03\u5916\u6570\u636e\u65f6\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u867d\u7136\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5176\u7b26\u53f7\u63d0\u53d6\u4ec5\u4f9d\u8d56\u4e8e\u4efb\u52a1\u6807\u7b7e\uff0c\u5bfc\u81f4\u4e0e\u5e95\u5c42\u89c6\u89c9\u6570\u636e\u7684\u5173\u8054\u8f83\u5f31\u3002", "method": "Concept-RuleNet\u9996\u5148\u5229\u7528\u4e00\u4e2a\u591a\u6a21\u6001\u6982\u5ff5\u751f\u6210\u5668\u4ece\u8bad\u7ec3\u56fe\u50cf\u4e2d\u6316\u6398\u5177\u6709\u533a\u5206\u6027\u7684\u89c6\u89c9\u6982\u5ff5\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u89c6\u89c9\u6982\u5ff5\u6765\u6307\u5bfc\u7b26\u53f7\u7684\u53d1\u73b0\uff0c\u4f7f\u4e4b\u4e0e\u771f\u5b9e\u7684\u56fe\u50cf\u7edf\u8ba1\u6570\u636e\u76f8\u5173\u8054\uff0c\u5e76\u51cf\u8f7b\u6807\u7b7e\u504f\u89c1\u3002\u63a5\u7740\uff0c\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4ee3\u7406\u5c06\u7b26\u53f7\u7ec4\u5408\u6210\u53ef\u6267\u884c\u7684\u4e00\u9636\u89c4\u5219\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u4e2a\u89c6\u89c9\u9a8c\u8bc1\u4ee3\u7406\u91cf\u5316\u6bcf\u4e2a\u7b26\u53f7\u5b58\u5728\u7684\u7a0b\u5ea6\uff0c\u5e76\u4e0e\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u6765\u534f\u540c\u89e6\u53d1\u89c4\u5219\u7684\u6267\u884c\uff0c\u4ece\u800c\u63d0\u4f9b\u5177\u6709\u660e\u786e\u63a8\u7406\u8def\u5f84\u7684\u9884\u6d4b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u4e24\u4e2a\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u548c\u4e09\u4e2a\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConcept-RuleNet\u5c06\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7b26\u53f7\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e865%\uff0c\u540c\u65f6\u5c06\u89c4\u5219\u4e2d\u5e7b\u89c9\u7b26\u53f7\u7684\u51fa\u73b0\u9891\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe50%\u3002", "conclusion": "Concept-RuleNet\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u57fa\u7840\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u73b0\u6709VLM\u7684\u5c40\u9650\u6027\uff0c\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u51c6\u786e\u4fe1\u606f\u7684\u4ea7\u751f\u3002"}}
{"id": "2511.13412", "categories": ["eess.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13412", "abs": "https://arxiv.org/abs/2511.13412", "authors": ["Liyang Jin", "Zichen Xi", "Joseph G. Thomas", "Jun Ji", "Yuanzhi Zhang", "Nuo Chen", "Yizheng Zhu", "Linbo Shao", "Liyan Zhu"], "title": "Microwave-acoustic-driven power electronics", "comment": null, "summary": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 \u00b0C) to 544 K (271 \u00b0C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u94cc\u9178\u9502\u7684\u5fae\u6ce2\u8868\u9762\u58f0\u6ce2(SAW)\u5668\u4ef6\u5b9e\u73b0\u4e86\u9ad8\u538b\u3001\u4f4e\u5bb9\u503c\u7684\u673a\u68b0\u9694\u79bb\u6805\u9a71\u52a8\u5668\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728GaN\u5668\u4ef6\u548c\u964d\u538b\u8f6c\u6362\u5668\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u5177\u6709\u8d85\u5bbd\u6e29\u5ea6\u8303\u56f4\u548c\u7535\u78c1\u5e72\u6270\uff08EMI\uff09\u514d\u75ab\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u901a\u8fc7\u7edf\u4e00\u901a\u9053\u540c\u65f6\u4f20\u8f93\u529f\u7387\u548c\u4fe1\u53f7\uff0c\u540c\u65f6\u5b9e\u73b0\u7535\u6c14\u9694\u79bb\u3002", "method": "\u57fa\u4e8e\u94cc\u9178\u9502\u7684\u5fae\u6ce2\u8868\u9762\u58f0\u6ce2\uff08SAW\uff09\u5668\u4ef6\uff0c\u5229\u75281.25\u6beb\u7c73\u7684\u673a\u68b0\u4f20\u64ad\u957f\u5ea6\u5b9e\u73b0\u4e862.75\u5343\u4f0f\u7684\u9694\u79bb\u7535\u538b\u548c0.032\u76ae\u6cd5\u7684\u9694\u79bb\u7535\u5bb9\u3002", "result": "\u5b9e\u73b0\u4e8613.4\u4f0f\u7684\u5f00\u8def\u7535\u538b\u548c44.4\u6beb\u5b89\u7684\u77ed\u8def\u7535\u6d41\uff0c\u9a71\u52a8\u4e86\u6c2e\u5316\u9553\uff08GaN\uff09\u9ad8\u8fc1\u79fb\u7387\u6676\u4f53\u7ba1\uff0c\u5f00\u5173\u65f6\u95f4\u4e3a108.8\u7eb3\u79d2\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u964d\u538b\u8f6c\u6362\u5668\u3002\u5668\u4ef6\u57280.5 K\u81f3544 K\u7684\u8d85\u5bbd\u6e29\u5ea6\u8303\u56f4\u5185\u5de5\u4f5c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5fae\u6ce2SAW\u5668\u4ef6\u4e3a\u5148\u8fdb\u7535\u529b\u7535\u5b50\u9886\u57df\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u9ad8\u6027\u80fd\u7684\u9694\u79bb\u7535\u6e90\u548c\u4fe1\u53f7\u4f20\u8f93\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u56fa\u6709\u7535\u78c1\u5e72\u6270\uff08EMI\uff09\u514d\u75ab\u80fd\u529b\u548c\u5f02\u6784\u96c6\u6210\u6f5c\u529b\u3002"}}
{"id": "2511.12308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12308", "abs": "https://arxiv.org/abs/2511.12308", "authors": ["Jiajun Zhu", "Yanqun Tang", "Cong Yi", "Haoran Yin", "Yuanhan Ni", "Fan Liu", "Zhiqiang Wei", "Huseyin Arslan"], "title": "ISAC with Affine Frequency Division Multiplexing: An FMCW-Based Signal Processing Perspective", "comment": "Submitted to IEEE for possible publication", "summary": "This paper investigates the sensing potential of affine frequency division multiplexing (AFDM) in high-mobility integrated sensing and communication (ISAC) from the perspective of radar waveforms. We introduce an innovative parameter selection criterion that establishes a precise mathematical equivalence between AFDM subcarriers and Nyquist-sampled frequency-modulated continuous-wave (FMCW). This connection not only provides a clear physical insight into AFDM's sensing mechanism but also enables a direct mapping from the DAFT index to delay-Doppler (DD) parameters of wireless channels. Building on this, we develop a novel input-output model in a DD-parameterized DAFT (DD-DAFT) domain for AFDM, which explicitly reveals the inherent DD coupling effect arising from the chirp-channel interaction. Subsequently, we design two matched-filtering sensing algorithms. The first is performed in the time-frequency domain with low complexity, while the second is operated in the DD-DAFT domain to precisely resolve the DD coupling. Simulations show that our algorithms achieve effective pilot-free sensing and demonstrate a fundamental trade-off between sensing performance, communication overhead, and computational complexity. The proposed AFDM outperforms classical AFDM and other variants in most scenarios.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u9ad8\u79fb\u52a8\u6027\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u4e2d\uff0c\u4ece\u96f7\u8fbe\u6ce2\u5f62\u89d2\u5ea6\u51fa\u53d1\u7684\u4eff\u5c04\u9891\u5206\u590d\u7528\uff08AFDM\uff09\u4f20\u611f\u6f5c\u529b\u3002", "motivation": "\u4ece\u96f7\u8fbe\u6ce2\u5f62\u89d2\u5ea6\u7814\u7a76\u9ad8\u79fb\u52a8\u6027ISAC\u4e2dAFDM\u7684\u4f20\u611f\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u9009\u62e9\u6807\u51c6\uff0c\u5efa\u7acb\u4e86AFDM\u5b50\u8f7d\u6ce2\u4e0e\u5948\u594e\u65af\u7279\u91c7\u6837\u8c03\u9891\u8fde\u7eed\u6ce2\uff08FMCW\uff09\u4e4b\u95f4\u7684\u7cbe\u786e\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u5e76\u6784\u5efa\u4e86DD-DAFT\u57df\u4e2d\u7684\u8f93\u5165\u8f93\u51fa\u6a21\u578b\uff0c\u6700\u540e\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5339\u914d\u6ee4\u6ce2\u4f20\u611f\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u65e0\u5bfc\u9891\u4f20\u611f\uff0c\u5e76\u5c55\u793a\u4e86\u4f20\u611f\u6027\u80fd\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002AFDM\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u7ecf\u5178\u7684AFDM\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "AFDM\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u5177\u6709\u4f18\u8d8a\u7684\u4f20\u611f\u6f5c\u529b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3DD\u8026\u5408\u95ee\u9898\u5e76\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u4f20\u611f\u3002"}}
{"id": "2511.11621", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.11621", "abs": "https://arxiv.org/abs/2511.11621", "authors": ["Pedro Antunes", "Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Lu\u00eds Fraz\u00e3o", "Nuno Costa", "Ant\u00f3nio Pereira"], "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs", "comment": null, "summary": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.", "AI": {"tldr": "AIvailable\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9ad8\u53ef\u7528\u6027\u7684LLMaaS\u5e73\u53f0\uff0c\u901a\u8fc7\u8f6f\u4ef6\u5b9a\u4e49\u7684\u65b9\u6cd5\uff0c\u5728\u5f02\u6784\u548c\u9057\u7559\u7684GPU\u8282\u70b9\uff08\u5305\u62ecNVIDIA\u548cAMD\uff09\u4e0a\u8fd0\u884cLLM\uff0c\u6700\u5927\u9650\u5ea6\u5730\u5229\u7528\u6bcf\u4e2a\u8282\u70b9\u7684\u663e\u5b58\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6846\u67b6\u901a\u5e38\u5047\u8bbe\u786c\u4ef6\u8d44\u6e90\u4e30\u5bcc\u4e14\u540c\u8d28\uff0c\u8fd9\u5728\u5b66\u672f\u754c\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5e76\u4e0d\u73b0\u5b9e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "AIvailable\u91c7\u7528\u8f6f\u4ef6\u5b9a\u4e49\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784GPU\u8282\u70b9\uff08NVIDIA\u548cAMD\uff09\u7684LLM\u63a8\u7406\u3002\u5b83\u5305\u62ec\u5ba2\u6237\u7aef\u63a5\u53e3\u3001\u670d\u52a1\u524d\u7aef\u3001SDAI\u63a7\u5236\u5668\u548c\u670d\u52a1\u540e\u7aef\u56db\u4e2a\u7ec4\u4ef6\uff0c\u80fd\u591f\u8fdb\u884c\u52a8\u6001\u7684\u3001\u663e\u5b58\u611f\u77e5\u7684\u6a21\u578b\u5206\u914d\u548c\u91cd\u65b0\u5206\u914d\uff0c\u5b9e\u73b0\u5168GPU\u52a0\u901f\u63a8\u7406\uff0c\u65e0CPU\u56de\u9000\u3002", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u5b8c\u5168\u5229\u7528\u8282\u70b9\u7684\u663e\u5b58\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5ba2\u6237\u7aef\u63a5\u53e3\u5b9e\u73b0\u5bf9\u6240\u6709\u90e8\u7f72LLM\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002\u5b83\u901a\u8fc7\u62bd\u8c61GPU\u7ec6\u8282\u548c\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528\u4ee5\u53ca\u5bf9\u6545\u969c\u6216\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\u7684\u5f39\u6027\u3002", "conclusion": "AIvailable\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u9057\u7559GPU\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u5f00\u6e90LLM\uff0c\u65e8\u5728\u4e3a\u5b66\u672f\u5b9e\u9a8c\u5ba4\u3001\u79c1\u8425\u516c\u53f8\u548c\u8d44\u6e90\u53d7\u9650\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u53ef\u7528\u7684LLMaaS\u670d\u52a1\uff0c\u4ece\u800c\u63a8\u52a8\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\u3002"}}
{"id": "2511.13264", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.13264", "abs": "https://arxiv.org/abs/2511.13264", "authors": ["Keshav Gupta", "Akshat Sanghvi", "Shreyas Reddy Palley", "Astitva Srivastava", "Charu Sharma", "Avinash Sharma"], "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression", "comment": "Project Page: https://symgs.github.io/", "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u5185\u5b58\u5360\u7528\u968f\u573a\u666f\u590d\u6742\u5ea6\u7684\u589e\u52a0\u800c\u6025\u5267\u589e\u52a0\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSymGS\u7684\u65b0\u578b\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u955c\u50cf\u6765\u6d88\u9664\u5197\u4f59\u76843D\u9ad8\u65af\u6cfc\u6e85\u56fe\u5143\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe108\u500d\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6cd5\u5185\u5b58\u5360\u7528\u4f9d\u7136\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u538b\u7f29\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSymGS\u7684\u65b0\u578b\u538b\u7f29\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u955c\u50cf\u6765\u8bc6\u522b\u548c\u6d88\u9664\u573a\u666f\u4e2d\u7684\u53cd\u5c04\u5197\u4f59\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf93D\u9ad8\u65af\u6cfc\u6e85\u56fe\u5143\u7684\u538b\u7f29\u3002SymGS\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u5148\u8fdb\u538b\u7f29\u65b9\u6cd5\u7684\u5373\u63d2\u5373\u7528\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u4e0e\u73b0\u6709\u7684HAC\u538b\u7f29\u65b9\u6cd5\u76f8\u6bd4\uff0cSymGS\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e861.66\u500d\u7684\u538b\u7f29\u7387\uff08\u5728\u5927\u578b\u573a\u666f\u4e2d\u9ad8\u8fbe3\u500d\uff09\u3002\u5e73\u5747\u800c\u8a00\uff0cSymGS\u80fd\u591f\u5b9e\u73b0108\u500d\u76843D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "SymGS\u901a\u8fc7\u5229\u7528\u5bf9\u79f0\u6027\uff0c\u7279\u522b\u662f\u955c\u50cf\u5bf9\u79f0\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u538b\u7f29\u6548\u7387\uff0c\u4e3a\u5904\u7406\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12860", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12860", "abs": "https://arxiv.org/abs/2511.12860", "authors": ["Yongjoo Jang", "Sangwoo Hwang", "Hojin Lee", "Sangwoo Jung", "Donghun Lee", "Wonbo Shim", "Jaeha Kung"], "title": "Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs", "comment": "This paper is accepted in the 43rd IEEE International Conference on Computer Design (ICCD), 2025", "summary": "The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5355\u6279\u6b21token\u751f\u6210\u4efb\u52a1\u5378\u8f7d\u52303D NAND\u95ea\u5b58\u5904\u7406\u5185\u5b58\uff08PIM\uff09\u8bbe\u5907\u4e0a\uff0c\u4ee5\u514b\u670dDRAM\u5bb9\u91cf\u9650\u5236\u548cGPU\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u5bf9\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6781\u9ad8\uff0c\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u90e8\u7f72\u9762\u4e34DRAM\u5bb9\u91cf\u4e0d\u8db3\u548cGPU\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5c06LLM\u7684\u5355\u6279\u6b21token\u751f\u6210\u4efb\u52a1\u5378\u8f7d\u52303D NAND\u95ea\u5b58PIM\u8bbe\u5907\uff0c\u5229\u7528\u5176\u9ad8\u5b58\u50a8\u5bc6\u5ea6\u89e3\u51b3DRAM\u5bb9\u91cf\u74f6\u9888\u3002\u63a2\u7d22\u4e863D NAND\u95ea\u5b58\u914d\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PIM\u9635\u5217\u7ed3\u6784\uff08H\u5f62\u6811\u7f51\u7edc\uff09\u4ee5\u4f18\u5316\u5ef6\u8fdf\u548c\u5bc6\u5ea6\u3002\u5f00\u53d1\u4e86LLM\u5c42\u7ea7\u7684\u64cd\u4f5c\u5206\u5757\u548c\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u6240\u63d0\u51fa\u76843D NAND\u95ea\u5b58PIM\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f7f\u75284\u5757RTX4090\uff08\u7ed3\u5408vLLM\uff09\u5feb2.4\u500d\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e4\u5757A100\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5ef6\u8fdf\u5f00\u9500\u4ec5\u4e3a4.9%\u3002\u8be6\u7ec6\u7684\u9762\u79ef\u5206\u6790\u8868\u660e\uff0c\u8be5PIM\u67b6\u6784\u53ef\u4ee5\u57284.98mm2\u7684\u82af\u7247\u9762\u79ef\u5185\u96c6\u6210\uff0c\u4e14\u65e0\u989d\u5916\u9762\u79ef\u5f00\u9500\u3002", "conclusion": "\u6240\u63d0\u51fa\u76843D NAND\u95ea\u5b58PIM\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u5bb9\u91cf\u548c\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u9762\u79ef\u6548\u7387\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.12277", "categories": ["eess.SY", "cs.DL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12277", "abs": "https://arxiv.org/abs/2511.12277", "authors": ["Dmytro Valiaiev"], "title": "DataOps-driven CI/CD for analytics repositories", "comment": null, "summary": "The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.", "AI": {"tldr": "SQL\u5f00\u53d1\u7f3a\u4e4f\u89c4\u8303\u5bfc\u81f4\u6570\u636e\u6cbb\u7406\u56f0\u96be\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDataOps\u7684\u6570\u636e\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5341\u4e8c\u9879\u63a7\u5236\u548cCI/CD\u6d41\u6c34\u7ebf\u6765\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "SQL\u5728\u6570\u636e\u5904\u7406\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u7f3a\u4e4f\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e25\u8c28\u6027\uff0c\u5bfc\u81f4\u4e86\u5404\u81ea\u4e3a\u653f\u3001\u903b\u8f91\u91cd\u590d\u548c\u98ce\u9669\u589e\u52a0\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u6570\u636e\u6cbb\u7406\u548c\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aDataOps\u9a71\u52a8\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u6e90\u81ea\u591a\u65b9\u6587\u732e\u7efc\u8ff0\u7684DataOps\u63a7\u5236\u8bc4\u5206\u5361\uff08\u5305\u542b\u5341\u4e8c\u9879\u53ef\u6d4b\u8bd5\u7684\u63a7\u5236\uff09\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u3001\u53ef\u6269\u5c55\u7684CI/CD\u6d41\u6c34\u7ebf\u6846\u67b6\uff08\u5305\u542bLint, Optimize, Parse, Validate, Observe\u4e94\u4e2a\u9636\u6bb5\uff09\uff0c\u6700\u540e\u901a\u8fc7\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\u77e9\u9635\uff08RTM\uff09\u6765\u786e\u4fdd\u9ad8\u5c42\u63a7\u5236\u7684\u5177\u4f53\u6267\u884c\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u5341\u4e8c\u9879\u63a7\u5236\u548c\u4e94\u9636\u6bb5CI/CD\u6d41\u6c34\u7ebf\u7684DataOps\u6570\u636e\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e76\u7528RTM\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u6570\u636e\u8d28\u91cf\u3001\u6cbb\u7406\u548c\u534f\u4f5c\uff0c\u4f7f\u5f97\u56e2\u961f\u80fd\u591f\u900f\u660e\u4e14\u53ef\u63a7\u5730\u6269\u5c55\u5206\u6790\u5f00\u53d1\u3002"}}
{"id": "2511.11845", "categories": ["cs.RO", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11845", "abs": "https://arxiv.org/abs/2511.11845", "authors": ["K. A. I. N Jayarathne", "R. M. N. M. Rathnayaka", "D. P. S. S. Peiris"], "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture", "comment": "6 pages, 2 figures", "summary": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86SLAM\u548c\u57fa\u4e8eSoar\u7684\u8ba4\u77e5\u67b6\u6784\u7684\u81ea\u4e3b\u6c34\u4e0b\u8ba4\u77e5\u7cfb\u7edf(AUCS)\uff0c\u4ee5\u5e94\u5bf9\u6df1\u6d77\u63a2\u7d22\u4e2d\u7684\u5bfc\u822a\u6311\u6218\u3002", "motivation": "\u6df1\u6d77\u63a2\u7d22\u9762\u4e34\u5931\u8054\u3001\u8ff7\u822a\u3001\u5bfc\u822a\u5931\u8d25\u7b49\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "AUCS\u878d\u5408\u4e86\u58f0\u7eb3\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u591a\u666e\u52d2\u8ba1\u7a0b\u4eea\u7b49\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e86\u611f\u77e5\u3001\u6ce8\u610f\u3001\u89c4\u5212\u548c\u5b66\u4e60\u7684\u8ba4\u77e5\u63a8\u7406\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u7406\u89e3\u3001\u81ea\u9002\u5e94\u4f20\u611f\u5668\u7ba1\u7406\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u5b66\u4e60\u3002", "result": "AUCS\u80fd\u591f\u533a\u5206\u52a8\u6001\u548c\u9759\u6001\u7269\u4f53\uff0c\u51cf\u5c11\u9519\u8bef\u7684\u95ed\u73af\uff0c\u63d0\u9ad8\u5730\u56fe\u957f\u671f\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u5b8c\u6574\u7684\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8-\u5b66\u4e60\u5faa\u73af\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4e0b\u4e00\u4ee3\u8ba4\u77e5\u6f5c\u6c34\u5668\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u6df1\u6d77\u63a2\u7d22\u7684\u5b89\u5168\u6027\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2511.12313", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12313", "abs": "https://arxiv.org/abs/2511.12313", "authors": ["Nitin Jha", "Abhishek Parakh", "Mahadevan Subramaniam"], "title": "An Improved Quantum Anonymous Notification Protocol for Quantum-Augmented Networks", "comment": null, "summary": "The scalability of current quantum networks is limited due to noisy quantum components and high implementation costs, thereby limiting the security advantages that quantum networks provide over their classical counterparts. Quantum Augmented Networks (QuANets) address this by integrating quantum components in classical network infrastructure to improve robustness and end-to-end security. To enable such integration, Quantum Anonymous Notification (QAN) is a method to anonymously inform a receiver of an incoming quantum communication. Therefore, several quantum primitives will serve as core tools, namely, quantum voting, quantum anonymous protocols, quantum secret sharing, etc. However, all current quantum protocols can be compromised in the presence of several common channel noises. In this work, we propose an improved quantum anonymous notification (QAN) protocol that utilizes rotation operations on shared GHZ states to produce an anonymous notification in an n-user quantum-augmented network. We study the behavior of this modified QAN protocol under the dephasing noise model and observe stronger resilience to false notifications than earlier QAN approaches. The QAN framework is also proposed to be integrated with a machine-learning classifier, enhanced quantum-augmented network. Finally, we discuss how this notification layer integrates with QuANets so that receivers can allow switch-bypass handling of quantum payloads, reducing header-based information leakage and vulnerability to targeted interference at compromised switches.", "AI": {"tldr": "\u5f53\u524d\u91cf\u5b50\u7f51\u7edc\u7684\u6269\u5c55\u6027\u53d7\u9650\u4e8e\u6709\u566a\u58f0\u7684\u91cf\u5b50\u7ec4\u4ef6\u548c\u9ad8\u6602\u7684\u5b9e\u65bd\u6210\u672c\u3002\u91cf\u5b50\u589e\u5f3a\u7f51\u7edc\uff08QuANets\uff09\u901a\u8fc7\u5c06\u91cf\u5b50\u7ec4\u4ef6\u96c6\u6210\u5230\u7ecf\u5178\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u4e2d\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u7aef\u5230\u7aef\u5b89\u5168\u6027\u3002\u91cf\u5b50\u533f\u540d\u901a\u77e5\uff08QAN\uff09\u662f\u4e00\u79cd\u533f\u540d\u901a\u77e5\u63a5\u6536\u8005\u6709\u5173\u4f20\u5165\u91cf\u5b50\u901a\u4fe1\u7684\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684QAN\u534f\u8bae\uff0c\u5229\u7528\u5171\u4eabGHZ\u72b6\u6001\u4e0a\u7684\u65cb\u8f6c\u64cd\u4f5c\u5728n\u7528\u6237\u91cf\u5b50\u589e\u5f3a\u7f51\u7edc\u4e2d\u4ea7\u751f\u533f\u540d\u901a\u77e5\uff0c\u5e76\u89c2\u5bdf\u5230\u5176\u5728\u9000\u76f8\u5e72\u566a\u58f0\u6a21\u578b\u4e0b\u6bd4\u65e9\u671fQAN\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u6297\u865a\u5047\u901a\u77e5\u80fd\u529b\u3002\u6700\u540e\uff0c\u5c06QAN\u6846\u67b6\u4e0e\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u96c6\u6210\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u901a\u77e5\u5c42\u5982\u4f55\u4e0eQuANets\u96c6\u6210\u4ee5\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u91cf\u5b50\u901a\u4fe1\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7f51\u7edc\u7684\u6269\u5c55\u6027\u53d7\u566a\u58f0\u548c\u9ad8\u6210\u672c\u7684\u9650\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b89\u5168\u4f18\u52bf\u3002\u73b0\u6709\u7684\u91cf\u5b50\u533f\u540d\u901a\u77e5\uff08QAN\uff09\u534f\u8bae\u5728\u5e38\u89c1\u7684\u4fe1\u9053\u566a\u58f0\u4e0b\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684QAN\u534f\u8bae\uff0c\u5229\u7528\u5171\u4eabGHZ\u72b6\u6001\u4e0a\u7684\u65cb\u8f6c\u64cd\u4f5c\u5728n\u7528\u6237\u91cf\u5b50\u589e\u5f3a\u7f51\u7edc\u4e2d\u4ea7\u751f\u533f\u540d\u901a\u77e5\uff0c\u5e76\u7814\u7a76\u4e86\u8be5\u534f\u8bae\u5728\u9000\u76f8\u5e72\u566a\u58f0\u6a21\u578b\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u6539\u8fdb\u7684QAN\u534f\u8bae\u5728\u9000\u76f8\u5e72\u566a\u58f0\u6a21\u578b\u4e0b\u8868\u73b0\u51fa\u6bd4\u65e9\u671fQAN\u65b9\u6cd5\u66f4\u5f3a\u7684\u6297\u865a\u5047\u901a\u77e5\u80fd\u529b\u3002\u5c06QAN\u6846\u67b6\u4e0e\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u96c6\u6210\uff0c\u5f62\u6210\u589e\u5f3a\u7684\u91cf\u5b50\u589e\u5f3a\u7f51\u7edc\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6539\u8fdbQAN\u534f\u8bae\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u91cf\u5b50\u566a\u58f0\uff0c\u5e76\u4e14\u901a\u8fc7\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u91cf\u5b50\u589e\u5f3a\u7f51\u7edc\uff08QuANets\uff09\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002\u8fd9\u79cd\u901a\u77e5\u5c42\u7684\u96c6\u6210\u53ef\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\u548c\u5bf9\u4ea4\u6362\u673a\u7684\u9488\u5bf9\u6027\u5e72\u6270\u3002"}}
{"id": "2511.13349", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13349", "abs": "https://arxiv.org/abs/2511.13349", "authors": ["Amrita Mukherjee", "Rahul Verma", "Pritesh Srivastava", "Bahadur Singh"], "title": "Nontrivial flat bands and quantum Hall crossovers in square-octagon lattice materials", "comment": "9 pages, 6 figures", "summary": "Coexistence of nontrivial topology and flat electronic bands in low-energy lattices provides a fertile platform for correlated quantum states. The square-octagon lattice hosts Dirac nodes and flat bands at half-filling, yet the influence of intrinsic spin-orbit coupling (SOC) and staggered magnetic flux on its topological and flat-band properties remains largely unexplored. Here, we examine this lattice using tight-binding models that include SOC and magnetic flux, uncovering a quantum spin Hall phase with spin Chern number $C_s=1$, crossovers to quantum anomalous Hall phases with $C=1$ and $C=2$, and higher-order topological insulator phases carrying quantized quadrupolar corner charges. The initially dispersionless flat bands evolve into quasi-flat, topologically nontrivial bands with uniform quantum geometry and large flatness ratios, conducive to fractional Chern insulator states. We further identify realistic material candidates, including octagraphene, transition-metal dichalcogenides, synthetic $\\mathrm{MoSi_2N_4}$, and magnetic $\u03b1$-MnO$_2$, as potential candidates for realizing tunable topological phases intertwined with flat-band physics, opening new opportunities for correlated topological matter.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5177\u6709\u975e\u5e73\u51e1\u62d3\u6251\u548c\u7535\u5b50\u5e73\u5766\u5e26\u7684\u65b9\u683c-\u516b\u8fb9\u5f62\u6676\u683c\uff0c\u5e76\u8003\u8651\u4e86\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u78c1\u901a\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65b9\u683c-\u516b\u8fb9\u5f62\u6676\u683c\u4e2d\u56fa\u6709\u7684\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u548c the staggered magnetic flux \u5bf9\u5176\u62d3\u6251\u548c\u80fd\u5e26\u7279\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5305\u542b SOC \u548c\u78c1\u901a\u91cf\u7684\u7d27\u675f\u7f1a\u6a21\u578b\u5bf9\u8be5\u6676\u683c\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4e86\u91cf\u5b50\u81ea\u65cb\u970d\u5c14\u76f8\uff08Cs=1\uff09\u3001\u91cf\u5b50\u53cd\u5e38\u970d\u5c14\u76f8\uff08C=1 \u548c C=2\uff09\u4ee5\u53ca\u5177\u6709\u91cf\u5316\u56db\u6781\u89d2\u7535\u8377\u7684\u9ad8\u9636\u62d3\u6251\u7edd\u7f18\u4f53\u76f8\u3002\u6700\u521d\u7684\u5e73\u5e26\u6f14\u53d8\u6210\u5177\u6709\u5747\u5300\u91cf\u5b50\u51e0\u4f55\u548c\u9ad8\u5e73\u5766\u5ea6\u6bd4\u7684\u51c6\u5e73\u5e26\uff0c\u6709\u5229\u4e8e\u5206\u6570\u9648\u7edd\u7f18\u4f53\u6001\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5177\u6709\u6f5c\u5728\u5b9e\u73b0\u53ef\u8c03\u8c10\u62d3\u6251\u76f8\u548c\u80fd\u5e26\u7269\u7406\u7684\u6750\u6599\u5019\u9009\u7269\uff0c\u5982\u8f9b\u70ef\u3001\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u5316\u7269\u3001\u5408\u6210 MoSi2N4 \u548c\u78c1\u6027 \u03b1-MnO2\uff0c\u4e3a\u91cf\u5b50\u5173\u8054\u62d3\u6251\u7269\u8d28\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.12989", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12989", "abs": "https://arxiv.org/abs/2511.12989", "authors": ["Ayush Subedi", "Anthony Torres", "Jeff Ganley"], "title": "XPS Analysis of Surface Chemical Transformations in ZBLAN Glass under Thermal and Vibrational Stimuli", "comment": "11 Pages 6 Figures", "summary": "ZBLAN glass is highly sensitive to thermal and mechanical stimuli, yet the associated surface chemical changes remain poorly understood. X-Ray Photoelectron Spectroscopy (XPS) measurements were performed on multiple ZBLAN samples representing distinct structural states: fully amorphous, incipiently crystalline, and highly crystalline, produced through thermal treatments at 250C and 350C and vibration-assisted processing at 400C under low (L2) and high (H5) vibration levels. High-resolution F 1s, Zr 3d, Hf 4f, Ba 3d, La 3d, and Na 1s spectra show progressive peak sharpening and intensity enhancement with increasing temperature and vibration, indicating reduced surface disorder and greater local structural ordering. The most pronounced changes occur under high vibration at 400C. No binding-energy shifts were detected, confirming that all elements retain their expected oxidation states and that the observed evolution reflects structural rather than chemical changes. These results provide direct evidence that thermomechanical input enhances surface ordering in ZBLAN and clarify its role in crystallization behavior relevant to infrared optical applications.", "AI": {"tldr": "ZBLAN\u73bb\u7483\u5bf9\u70ed\u548c\u673a\u68b0\u523a\u6fc0\u654f\u611f\uff0c\u4f46\u8868\u9762\u5316\u5b66\u53d8\u5316\u4ecd\u4e0d\u6e05\u695a\u3002XPS\u6d4b\u91cf\u663e\u793a\uff0c\u70ed\u5904\u7406\u548c\u632f\u52a8\u8f85\u52a9\u5904\u7406\u4f1a\u63d0\u9ad8ZBLAN\u7684\u8868\u9762\u7ed3\u6784\u6709\u5e8f\u5ea6\uff0c\u4f46\u4e0d\u4f1a\u6539\u53d8\u5176\u5316\u5b66\u72b6\u6001\u3002", "motivation": "ZBLAN\u73bb\u7483\u5bf9\u70ed\u548c\u673a\u68b0\u523a\u6fc0\u7684\u654f\u611f\u6027\u4e0e\u5176\u8868\u9762\u5316\u5b66\u53d8\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528X\u5c04\u7ebf\u5149\u7535\u5b50\u80fd\u8c31\uff08XPS\uff09\u6d4b\u91cf\u4e86\u4e0d\u540c\u7ed3\u6784\u72b6\u6001\uff08\u5b8c\u5168\u975e\u6676\u6001\u3001\u521d\u59cb\u7ed3\u6676\u6001\u3001\u9ad8\u5ea6\u7ed3\u6676\u6001\uff09\u7684ZBLAN\u6837\u54c1\u3002\u8fd9\u4e9b\u6837\u54c1\u901a\u8fc7\u5728250\u00b0C\u548c350\u00b0C\u8fdb\u884c\u70ed\u5904\u7406\uff0c\u4ee5\u53ca\u5728400\u00b0C\u8fdb\u884c\u4f4e\uff08L2\uff09\u548c\u9ad8\uff08H5\uff09\u632f\u52a8\u6c34\u5e73\u4e0b\u7684\u632f\u52a8\u8f85\u52a9\u5904\u7406\u6765\u5236\u5907\u3002", "result": "\u9ad8\u5206\u8fa8\u7387\u7684F 1s\u3001Zr 3d\u3001Hf 4f\u3001Ba 3d\u3001La 3d\u548cNa 1s\u5149\u8c31\u663e\u793a\uff0c\u968f\u7740\u6e29\u5ea6\u548c\u632f\u52a8\u7684\u589e\u52a0\uff0c\u5cf0\u53d8\u5c16\u9510\uff0c\u5f3a\u5ea6\u589e\u5f3a\uff0c\u8868\u660e\u8868\u9762\u65e0\u5e8f\u6027\u964d\u4f4e\uff0c\u5c40\u90e8\u7ed3\u6784\u6709\u5e8f\u6027\u589e\u5f3a\u3002\u5728\u9ad8\u632f\u52a8\uff08400\u00b0C\uff09\u4e0b\u53d8\u5316\u6700\u660e\u663e\u3002\u672a\u68c0\u6d4b\u5230\u7ed3\u5408\u80fd\u79fb\u52a8\uff0c\u8bc1\u5b9e\u6240\u6709\u5143\u7d20\u7684\u6c27\u5316\u6001\u4e0d\u53d8\uff0c\u53d8\u5316\u4ec5\u53cd\u6620\u7ed3\u6784\u800c\u975e\u5316\u5b66\u53d8\u5316\u3002", "conclusion": "\u70ed\u673a\u68b0\u8f93\u5165\u53ef\u589e\u5f3aZBLAN\u7684\u8868\u9762\u6709\u5e8f\u6027\uff0c\u8fd9\u4e0e\u5176\u5728\u7ea2\u5916\u5149\u5b66\u5e94\u7528\u4e2d\u7684\u7ed3\u6676\u884c\u4e3a\u76f8\u5173\u3002"}}
{"id": "2511.11754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11754", "abs": "https://arxiv.org/abs/2511.11754", "authors": ["Stanislav Selitskiy"], "title": "Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition", "comment": null, "summary": "A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike \"traditional\" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the \"important\" dimensions (primary components) is implemented. In such a way, the \"important\" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u5f0f\u7a00\u758f\u98ce\u683cTransformer\u53d8\u4f53\u67b6\u6784\uff0c\u79f0\u4e3aBatch Transformers\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u5904\u7406\u5e8f\u5217\u6216\u6279\u6b21\u5b9e\u4f53\u65f6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u53ea\u5173\u6ce8\u201c\u91cd\u8981\u201d\u7ef4\u5ea6\uff08\u4e3b\u8981\u6210\u5206\uff09\u6765\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "Batch Transformers \u67b6\u6784\u53ea\u5bf9 \"\u91cd\u8981\" \u7ef4\u5ea6\uff08\u7279\u5f81\u9009\u62e9\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u4ece\u800c\u51cf\u5c0f\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u7684\u74f6\u9888\u5c3a\u5bf8\u3002", "result": "\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\uff08\u9488\u5bf9\u5316\u5986\u548c\u906e\u6321\u6570\u636e\u96c6\uff09\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u80fd\u591f\u589e\u52a0\u6709\u9650\u7684\u539f\u59cb\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002", "conclusion": "Batch Transformers \u67b6\u6784\u901a\u8fc7\u5173\u6ce8\u91cd\u8981\u7ef4\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2511.13652", "categories": ["cond-mat.mes-hall", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13652", "abs": "https://arxiv.org/abs/2511.13652", "authors": ["Hadi Mohammed Soufy", "Colin Benjamin"], "title": "A High-Efficiency Three-Stroke Quantum Isochoric Heat Engine: From Infinite Potential Wells to Magic Angle Twisted Bilayer Graphene", "comment": "15 pages, 12 figures, 3 tables", "summary": "We introduce a three-stroke quantum isochoric cycle that functions as a heat engine operating between two thermal reservoirs. Implemented for a particle confined in a one-dimensional infinite potential well, the cycle's performance is benchmarked against the classical three-stroke triangular and isochoric engines. We find that the quantum isochoric cycle achieves a higher efficiency than both classical counterparts and also surpasses the efficiency of the recently proposed three-stroke quantum isoenergetic cycle. Owing to its reduced number of strokes, the design substantially lowers control complexity in nanoscale thermodynamic devices, offering a more feasible route to experimental realization compared to conventional four-stroke architectures. We further evaluate the cycle in graphene-based systems under an external magnetic field, including monolayer graphene (MLG), AB-stacked bilayer graphene (BLG), and twisted bilayer graphene (TBG) at both magic and non-magic twist angles. Among these platforms, magic-angle twisted bilayer graphene (MATBG) attains the highest efficiency at fixed work output, highlighting its promise for quantum thermodynamic applications.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u51b2\u7a0b\u91cf\u5b50\u7b49\u5bb9\u5faa\u73af\uff0c\u53ef\u4f5c\u4e3a\u5728\u4e24\u4e2a\u70ed\u5e93\u4e4b\u95f4\u8fd0\u884c\u7684\u70ed\u673a\u3002\u5b83\u5728\u91cf\u5b50\u4f53\u7cfb\u4e2d\u4f18\u4e8e\u7ecf\u5178\u548c\u4e4b\u524d\u7684\u91cf\u5b50\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u9b54\u89d2\u626d\u66f2\u53cc\u5c42\u77f3\u58a8\u70ef\uff08MATBG\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u91cf\u5b50\u70ed\u529b\u5b66\u5668\u4ef6\u7684\u63a7\u5236\u590d\u6742\u6027\u9ad8\uff0c\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u70ed\u673a\u5faa\u73af\uff0c\u5e76\u63a2\u7d22\u5176\u5b9e\u9a8c\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e09\u51b2\u7a0b\u91cf\u5b50\u7b49\u5bb9\u5faa\u73af\uff0c\u5e76\u5728\u91cf\u5b50\u4f53\u7cfb\uff08\u4e00\u7ef4\u65e0\u9650\u6df1\u52bf\u9631\u3001\u77f3\u58a8\u70ef\u4f53\u7cfb\uff09\u4e2d\u8fdb\u884c\u4e86\u5206\u6790\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u4e0e\u7ecf\u5178\u6a21\u578b\u548c\u4e4b\u524d\u7684\u91cf\u5b50\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u91cf\u5b50\u7b49\u5bb9\u5faa\u73af\u7684\u6548\u7387\u4f18\u4e8e\u7ecf\u5178\u4e09\u51b2\u7a0b\u4e09\u89d2\u548c\u7b49\u5bb9\u53d1\u52a8\u673a\uff0c\u4e5f\u4f18\u4e8e\u4e09\u51b2\u7a0b\u91cf\u5b50\u7b49\u80fd\u5faa\u73af\u3002\u5728\u77f3\u58a8\u70ef\u4f53\u7cfb\u4e2d\uff0c\u9b54\u89d2\u626d\u66f2\u53cc\u5c42\u77f3\u58a8\u70ef\uff08MATBG\uff09\u5728\u56fa\u5b9a\u529f\u8f93\u51fa\u4e0b\u6548\u7387\u6700\u9ad8\u3002", "conclusion": "\u4e09\u51b2\u7a0b\u91cf\u5b50\u7b49\u5bb9\u5faa\u73af\u56e0\u5176\u7ed3\u6784\u7b80\u5355\uff0c\u63a7\u5236\u590d\u6742\u5ea6\u4f4e\uff0c\u5728\u91cf\u5b50\u70ed\u529b\u5b66\u5668\u4ef6\u7684\u5b9e\u9a8c\u5b9e\u73b0\u65b9\u9762\u6bd4\u4f20\u7edf\u56db\u51b2\u7a0b\u7ed3\u6784\u66f4\u5177\u4f18\u52bf\u3002\u9b54\u89d2\u626d\u66f2\u53cc\u5c42\u77f3\u58a8\u70ef\uff08MATBG\uff09\u662f\u91cf\u5b50\u70ed\u529b\u5b66\u5e94\u7528\u7684\u6709\u524d\u666f\u5e73\u53f0\u3002"}}
{"id": "2511.12348", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12348", "abs": "https://arxiv.org/abs/2511.12348", "authors": ["Mostafa Nozari", "Israel Leyva-Mayorga", "Fabio Saggese", "Gilberto Berardinelli"], "title": "Toward ISAC-empowered subnetworks: Cooperative localization and iterative node selection", "comment": null, "summary": "This paper tackles the sensing-communication trade-off in integrated sensing and communication (ISAC)-empowered subnetworks for mono-static target localization. We propose a low-complexity iterative node selection algorithm that exploits the spatial diversity of subnetwork deployments and dynamically refines the set of sensing subnetworks to maximize localization accuracy under tight resource constraints. Simulation results show that our method achieves sub-7 cm accuracy in additive white Gaussian noise (AWGN) channels within only three iterations, yielding over 97% improvement compared to the best-performing benchmark under the same sensing budget. We further demonstrate that increasing spatial diversity through additional antennas and subnetworks enhances sensing robustness, especially in fading channels. Finally, we quantify the sensing-communication trade-off, showing that reducing sensing iterations and the number of sensing subnetworks improves throughput at the cost of reduced localization precision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u8282\u70b9\u9009\u62e9\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u6700\u5927\u5316ISAC\u5b50\u7f51\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u541e\u5410\u91cf\u7684\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u8d4b\u80fd\u7684\u5b50\u7f51\u4e2d\uff0c\u5355\u7ad9\u76ee\u6807\u5b9a\u4f4d\u7684\u4f20\u611f-\u901a\u4fe1\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u8282\u70b9\u9009\u62e9\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u5b50\u7f51\u90e8\u7f72\u7684\u7a7a\u95f4\u5206\u96c6\uff0c\u5e76\u52a8\u6001\u4f18\u5316\u4f20\u611f\u5b50\u96c6\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u6700\u5927\u5316\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5728\u52a0\u6027\u767d\u9ad8\u65af\u566a\u58f0\uff08AWGN\uff09\u4fe1\u9053\u4e2d\uff0c\u7b97\u6cd5\u5728\u4ec5\u4e09\u6b21\u8fed\u4ee3\u5185\u5b9e\u73b0\u4e86\u4f4e\u4e8e7\u5398\u7c73\u7684\u7cbe\u5ea6\uff0c\u4e0e\u6700\u4f18\u57fa\u51c6\u76f8\u6bd4\u63d0\u9ad8\u4e8697%\u4ee5\u4e0a\u3002\u589e\u52a0\u7a7a\u95f4\u5206\u96c6\uff08\u901a\u8fc7\u66f4\u591a\u5929\u7ebf\u548c\u5b50\u7f51\uff09\u53ef\u63d0\u9ad8\u4f20\u611f\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u8870\u843d\u4fe1\u9053\u4e2d\u3002\u91cf\u5316\u4e86\u4f20\u611f-\u901a\u4fe1\u6743\u8861\uff0c\u8868\u660e\u51cf\u5c11\u4f20\u611f\u8fed\u4ee3\u6b21\u6570\u548c\u4f20\u611f\u5b50\u7f51\u6570\u91cf\u53ef\u4ee5\u5728\u727a\u7272\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u8282\u70b9\u9009\u62e9\u7b97\u6cd5\u5728ISAC\u5b50\u7f51\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u611f-\u901a\u4fe1\u6743\u8861\u95ee\u9898\uff0c\u80fd\u591f\u5728\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u541e\u5410\u91cf\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2511.11624", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11624", "abs": "https://arxiv.org/abs/2511.11624", "authors": ["Md Romyull Islam", "Bobin Deng", "Nobel Dhar", "Tu N. Nguyen", "Selena He", "Yong Shi", "Kun Suo"], "title": "Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges", "comment": "Submitted version; 9 pages, 5 figures; presented at IEEE MASS 2025 (online publication pending)", "summary": "Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86 Llama 3.2\u3001Phi-3 Mini\u3001TinyLlama \u548c Gemma 2 \u7b49\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u5728 Raspberry Pi 5\u3001Jetson Nano \u548c Jetson Orin Nano \u4e0a\u7684\u80fd\u6548\u3002\u7ed3\u679c\u8868\u660e\uff0cJetson Orin Nano \u7684 GPU \u52a0\u901f\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002Llama 3.2 \u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u800c TinyLlama \u5219\u9002\u5408\u4f4e\u529f\u8017\u73af\u5883\uff0c\u4f46\u51c6\u786e\u6027\u6709\u6240\u964d\u4f4e\u3002Phi-3 Mini \u5c3d\u7ba1\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u80fd\u8017\u6700\u9ad8\u3002GPU \u52a0\u901f\u3001\u5185\u5b58\u5e26\u5bbd\u548c\u6a21\u578b\u67b6\u6784\u662f\u5f71\u54cd\u63a8\u7406\u80fd\u6548\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u7f51\u7edc\u8fde\u63a5\u72ec\u7acb\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u80fd\u8017\u9884\u7b97\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c SLM \u5728\u4e0d\u540c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u80fd\u6548\u8868\u73b0\uff0c\u4e3a\u5728\u80fd\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u6743\u8861\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u5728 Raspberry Pi 5\u3001Jetson Nano \u548c Jetson Orin Nano\uff08CPU \u548c GPU \u914d\u7f6e\uff09\u4e0a\u8bc4\u4f30\u4e86\u4e94\u79cd\u4ee3\u8868\u6027 SLM\uff08Llama 3.2\u3001Phi-3 Mini\u3001TinyLlama \u548c Gemma 2\uff09\u7684\u80fd\u6548\u3002", "result": "Jetson Orin Nano \u642d\u914d GPU \u52a0\u901f\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u80fd\u6548\u6bd4\u3002Llama 3.2 \u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002TinyLlama \u9002\u5408\u4f4e\u529f\u8017\u573a\u666f\uff0c\u4f46\u51c6\u786e\u6027\u8f83\u4f4e\u3002Phi-3 Mini \u80fd\u8017\u6700\u9ad8\uff0c\u4f46\u51c6\u786e\u6027\u4e5f\u9ad8\u3002GPU \u52a0\u901f\u3001\u5185\u5b58\u5e26\u5bbd\u548c\u6a21\u578b\u67b6\u6784\u662f\u5f71\u54cd\u63a8\u7406\u80fd\u6548\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "GPU \u52a0\u901f\u3001\u5185\u5b58\u5e26\u5bbd\u548c\u6a21\u578b\u67b6\u6784\u5bf9 SLM \u7684\u63a8\u7406\u80fd\u6548\u81f3\u5173\u91cd\u8981\u3002Llama 3.2 \u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6298\u8877\uff0c\u800c TinyLlama \u5219\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u573a\u666f\u3002\u672c\u7814\u7a76\u4e3a\u5728\u80fd\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c AI\u3001\u667a\u80fd\u7cfb\u7edf\u548c\u79fb\u52a8 ad-hoc \u5e73\u53f0\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\uff0c\u6307\u5bfc\u4e86\u4f18\u5316\u63a8\u7406\u80fd\u6548\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.12930", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12930", "abs": "https://arxiv.org/abs/2511.12930", "authors": ["Changhun Oh", "Seongryong Oh", "Jinwoo Hwang", "Yoonsung Kim", "Hardik Sharma", "Jongse Park"], "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration", "comment": null, "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.", "AI": {"tldr": "Neo\u901a\u8fc7\u5f15\u5165\u590d\u7528\u548c\u66f4\u65b0\u6392\u5e8f\u7b97\u6cd5\u53ca\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6e85\u5c04\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u5728\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u65f6\u5e27\u7387\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u5b9e\u73b03D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u5b9e\u65f6\u6e32\u67d3\u5bf9\u4e8e\u63d0\u4f9b\u6c89\u6d78\u5f0fAR/VR\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u65f6\u96be\u4ee5\u8fbe\u5230\u9ad8\u5e27\u7387\u3002", "method": "Neo\u5f15\u5165\u4e86\u4e00\u79cd\u590d\u7528\u548c\u66f4\u65b0\u6392\u5e8f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u4e86\u8fde\u7eed\u5e27\u4e4b\u95f4\u9ad8\u65af\u6392\u5e8f\u7684\u65f6\u95f4\u5197\u4f59\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9488\u5bf9\u8be5\u7b97\u6cd5\u4f18\u5316\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002\u901a\u8fc7\u6709\u6548\u5730\u8ddf\u8e2a\u548c\u66f4\u65b0\u9ad8\u65af\u6df1\u5ea6\u6392\u5e8f\uff0c\u800c\u4e0d\u662f\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u6392\u5e8f\uff0cNeo\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\u548c\u5185\u5b58\u5e26\u5bbd\u538b\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNeo\u7684\u541e\u5410\u91cf\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8fb9\u7f18GPU\u548cASIC\u89e3\u51b3\u65b9\u6848\u5206\u522b\u63d0\u9ad8\u4e8610.0\u500d\u548c5.6\u500d\uff0c\u540c\u65f6DRAM\u6d41\u91cf\u5206\u522b\u51cf\u5c11\u4e8694.5%\u548c81.3%\u3002", "conclusion": "Neo\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u5185\u5b58\u5e26\u5bbd\u538b\u529b\uff0c\u4f7f\u5f97\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u8bbe\u5907\u7aef3D\u6e32\u67d3\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2511.12329", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12329", "abs": "https://arxiv.org/abs/2511.12329", "authors": ["Arman Pourghorban", "Dipankar Maity"], "title": "Target Defense against Sequentially Arriving Intruders: Algorithm for Agents with Dubins Dynamics", "comment": null, "summary": "We consider a variant of the target defense problem where a single defender is tasked to capture a sequence of incoming intruders. Both the defender and the intruders have non-holonomic dynamics. The intruders' objective is to breach the target perimeter without being captured by the defender, while the defender's goal is to capture as many intruders as possible. After one intruder breaches or is captured, the next appears randomly on a fixed circle surrounding the target. Therefore, the defender's final position in one game becomes its starting position for the next. We divide an intruder-defender engagement into two phases, partial information and full information, depending on the information available to the players. We address the capturability of an intruder by the defender using the notions of Dubins path and guarding arc. We quantify the percentage of capture for both finite and infinite sequences of incoming intruders. Finally, the theoretical results are verified through numerical examples using Monte-Carlo-type random trials of experiments.", "AI": {"tldr": "\u5728\u6b64\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u76ee\u6807\u9632\u5fa1\u95ee\u9898\u53d8\u4f53\uff0c\u5176\u4e2d\u4e00\u540d\u9632\u5fa1\u8005\u9700\u8981\u62e6\u622a\u4e00\u7cfb\u5217\u5165\u4fb5\u8005\u3002\u9632\u5fa1\u8005\u548c\u5165\u4fb5\u8005\u90fd\u5177\u6709\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u3002\u5165\u4fb5\u8005\u7684\u76ee\u6807\u662f\u5728\u4e0d\u88ab\u9632\u5fa1\u8005\u6355\u83b7\u7684\u60c5\u51b5\u4e0b\u7a81\u7834\u76ee\u6807\u533a\u57df\uff0c\u800c\u9632\u5fa1\u8005\u7684\u76ee\u6807\u662f\u5c3d\u53ef\u80fd\u591a\u5730\u6355\u83b7\u5165\u4fb5\u8005\u3002\u5728\u4e00\u4e2a\u5165\u4fb5\u8005\u88ab\u6355\u83b7\u6216\u7a81\u7834\u540e\uff0c\u4e0b\u4e00\u4e2a\u5165\u4fb5\u8005\u4f1a\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684\u5706\u5708\u4e0a\u968f\u673a\u51fa\u73b0\u3002\u6211\u4eec\u6839\u636e\u73a9\u5bb6\u83b7\u5f97\u7684\u4fe1\u606f\u5c06\u5165\u4fb5\u8005-\u9632\u5fa1\u8005\u4ea4\u4e92\u5206\u4e3a\u90e8\u5206\u4fe1\u606f\u548c\u5b8c\u5168\u4fe1\u606f\u4e24\u4e2a\u9636\u6bb5\u3002\u6211\u4eec\u4f7f\u7528 Dubins \u8def\u5f84\u548c\u5b88\u62a4\u5f27\u7684\u6982\u5ff5\u6765\u89e3\u51b3\u5165\u4fb5\u8005\u7684\u53ef\u6355\u83b7\u6027\u95ee\u9898\u3002\u6211\u4eec\u91cf\u5316\u4e86\u6709\u9650\u548c\u65e0\u9650\u5e8f\u5217\u5165\u4fb5\u8005\u7684\u6355\u83b7\u7387\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u7c7b\u578b\u7684\u968f\u673a\u5b9e\u9a8c\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u7684\u9632\u5fa1\u8005\u548c\u5165\u4fb5\u8005\u4e4b\u95f4\u7684\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u9632\u5fa1\u8005\u7684\u6355\u83b7\u80fd\u529b\u3002", "method": "\u5c06\u4ea4\u4e92\u5206\u4e3a\u90e8\u5206\u4fe1\u606f\u548c\u5b8c\u5168\u4fe1\u606f\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u5229\u7528 Dubins \u8def\u5f84\u548c\u5b88\u62a4\u5f27\u7684\u6982\u5ff5\u6765\u5206\u6790\u53ef\u6355\u83b7\u6027\u3002", "result": "\u91cf\u5316\u4e86\u6709\u9650\u548c\u65e0\u9650\u5e8f\u5217\u5165\u4fb5\u8005\u7684\u6355\u83b7\u7387\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u7406\u8bba\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u9632\u5fa1\u8005\u7684\u6355\u83b7\u80fd\u529b\u3002"}}
{"id": "2511.11931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11931", "abs": "https://arxiv.org/abs/2511.11931", "authors": ["Saida Liu", "Nikolay Atanasov", "Shumon Koga"], "title": "MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy", "comment": "14 pages, 3 figures. Submitted to L4DC 2026", "summary": "This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.", "AI": {"tldr": "MATT-Diff\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u7684\u6269\u6563\u7b56\u7565\uff0c\u80fd\u591f\u6355\u6349\u63a2\u7d22\u3001\u4e13\u6ce8\u8ddf\u8e2a\u548c\u91cd\u65b0\u83b7\u53d6\u76ee\u6807\u7b49\u591a\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u65e0\u9700\u9884\u5148\u4e86\u89e3\u76ee\u6807\u6570\u91cf\u3001\u72b6\u6001\u6216\u52a8\u6001\u7684\u667a\u80fd\u4f53\u63a7\u5236\u3002", "motivation": "\u6709\u6548\u7684\u76ee\u6807\u8ddf\u8e2a\u9700\u8981\u5e73\u8861\u5bf9\u672a\u63a2\u6d4b\u5230\u6216\u4e22\u5931\u76ee\u6807\u7684\u63a2\u7d22\u4e0e\u5bf9\u5df2\u63a2\u6d4b\u5230\u4f46\u4e0d\u786e\u5b9a\u76ee\u6807\u7684\u8ddf\u8e2a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u53d8\u6362\u5668\u8fdb\u884c\u81ea\u6211\u4e2d\u5fc3\u5730\u56fe\u6807\u8bb0\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u6574\u5408\u9ad8\u65af\u5bc6\u5ea6\u8868\u793a\u7684\u53ef\u53d8\u76ee\u6807\u4f30\u8ba1\u7684\u63a7\u5236\u7b56\u7565\u3002\u8be5\u7b56\u7565\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u5b66\u4e60\u751f\u6210\u591a\u6a21\u6001\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cMATT-Diff\u5728\u591a\u79cd\u76ee\u6807\u8fd0\u52a8\u60c5\u51b5\u4e0b\uff0c\u5176\u8ddf\u8e2a\u6027\u80fd\u4f18\u4e8e\u4e13\u5bb6\u548c\u884c\u4e3a\u514b\u9686\u57fa\u7ebf\u3002", "conclusion": "MATT-Diff\u5728\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.12318", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12318", "abs": "https://arxiv.org/abs/2511.12318", "authors": ["Ilias Cherkaoui", "Indrakshi Dey"], "title": "QMA Complete Quantum-Enhanced Kyber: Provable Security Through CHSH Nonlocality", "comment": null, "summary": "Post-quantum cryptography (PQC) must secure large-scale communication systems against quantum adversaries where classical hardness alone is insufficient and purely quantum schemes remain impractical. Lattice-based key encapsulation mechanisms (KEMs) such as CRYSTALS-Kyber provide efficient quantum-resistant primitives but rely solely on computational hardness assumptions that are susceptible to hybrid classical-quantum attacks. To overcome this limitation, we introduce the first Clauser-Horne-Shimony-Holt (CHSH)-certified Kyber protocol, which embeds quantum non-locality verification directly within the key exchange phase. The proposed design integrates CHSH entanglement tests using Einstein-Podolsky-Rosen (EPR) pairs to yield measurable quantum advantage values exceeding classical correlation limits, thereby coupling information--theoretic quantum guarantees with lattice-based computational security. Formal reductions demonstrate that any polynomial-time adversary breaking the proposed KEM must either solve the Module Learning With Errors (Module-LWE) problem or a Quantum Merlin-Arthur (QMA)-complete instance of the 2-local Hamiltonian problem, under the standard complexity assumption QMA $\\subset$ NP. The construction remains fully compatible with the Fujisaki-Okamoto (FO) transform, preserving chosen-ciphertext attack (CCA) security and Kyber's efficiency profile. The resulting CHSH-augmented Kyber scheme therefore establishes a mathematically rigorous, hybrid post-quantum framework that unifies lattice cryptography and quantum non-locality to achieve verifiable, composable, and forward-secure key agreement.", "AI": {"tldr": "To address the limitations of purely computational post-quantum cryptography, this paper introduces a hybrid scheme that combines lattice-based cryptography with quantum non-locality verification, offering enhanced security.", "motivation": "Current post-quantum cryptography (PQC) schemes like CRYSTALS-Kyber, while efficient, rely on computational hardness assumptions vulnerable to hybrid classical-quantum attacks. There is a need for PQC that incorporates information-theoretic security guarantees.", "method": "The paper proposes the first Clauser-Horne-Shimony-Holt (CHSH)-certified Kyber protocol. This scheme embeds quantum non-locality verification using Einstein-Podolsky-Rosen (EPR) pairs directly into the key exchange phase. It integrates CHSH entanglement tests to achieve quantum advantage values exceeding classical correlation limits, thus combining information-theoretic quantum guarantees with lattice-based computational security. The construction is compatible with the Fujisaki-Okamoto (FO) transform for chosen-ciphertext attack (CCA) security.", "result": "The proposed CHSH-augmented Kyber scheme provides a hybrid post-quantum framework. Formal reductions show that breaking this KEM requires solving either the Module Learning With Errors (Module-LWE) problem or a Quantum Merlin-Arthur (QMA)-complete instance of the 2-local Hamiltonian problem, assuming QMA $\\subset$ NP. The scheme maintains Kyber's efficiency and CCA security.", "conclusion": "The CHSH-augmented Kyber protocol establishes a mathematically rigorous, hybrid post-quantum security framework by unifying lattice cryptography and quantum non-locality. This achieves verifiable, composable, and forward-secure key agreement, offering a more robust security solution against quantum adversaries."}}
{"id": "2511.13468", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2511.13468", "abs": "https://arxiv.org/abs/2511.13468", "authors": ["Jean Rodriguez", "Holger Grisk", "Alberto Anad\u00f3n", "Harjinder Singh", "Gregory Malinowski", "Michel Hehn", "Javier Curiale", "Jon Gorchon"], "title": "Spin Accumulation based deep MOKE Microscopy", "comment": null, "summary": "Magnetic imaging techniques are widespread critical tools used in fields such as magnetism, spintronics or even superconductivity. Among them, one of the most versatile methods is the magneto-optical Kerr effect. However, as soon as light is blocked from interacting with the magnetic layer, such as in deeply buried layers, optical techniques become ineffective. In this work, we present a spin-accumulation based magneto-optical Kerr effect (SA-MOKE) microscopy technique that enables imaging of a magnetic thin-films covered by thick and opaque metallic layers. The technique is based on the generation and detection of transient spin-accumulations that propagate through the thick metallic layer. These spin-accumulation signals are directly triggered and detected optically on the same side, lifting any substrate transparency requirements. The spin-accumulation signals detected on a Cu layer decay with a characteristic length of 60 nm, much longer than the 12 nm optical penetration depth, allowing for detection of magnetic contrast with Cu capping layers up to hundreds of nm. This method should enable magnetic imaging in a wide-range of experiments where the surface of interest is covered by electrodes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u65cb\u79ef\u7d2f\u7684\u78c1\u5149\u514b\u5c14\u6548\u5e94\u663e\u5fae\u955c\u6280\u672f\uff0c\u53ef\u7528\u4e8e\u6210\u50cf\u88ab\u539a\u800c\u4e0d\u900f\u660e\u91d1\u5c5e\u5c42\u8986\u76d6\u7684\u78c1\u6027\u8584\u819c\u3002", "motivation": "\u5149\u5b66\u6280\u672f\u5728\u6210\u50cf\u6df1\u57cb\u5c42\u65f6\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6210\u50cf\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5149\u6fc0\u53d1\u548c\u63a2\u6d4b\u77ac\u6001\u81ea\u65cb\u79ef\u7d2f\u6765\u6210\u50cf\u78c1\u6027\u8584\u819c\u3002", "result": "\u6240\u63d0\u51fa\u7684SA-MOKE\u6280\u672f\u80fd\u591f\u6210\u50cf\u9ad8\u8fbe\u6570\u767e\u7eb3\u7c73\u7684\u94dc\u8986\u76d6\u5c42\u4e0b\u7684\u78c1\u6027\u8584\u819c\uff0c\u8fdc\u8d85\u5149\u5b66\u7a7f\u900f\u6df1\u5ea6\u3002", "conclusion": "\u8be5SA-MOKE\u6280\u672f\u80fd\u591f\u514b\u670d\u5149\u5b66\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5305\u542b\u7535\u6781\u7684\u8868\u9762\u6750\u6599\u7684\u78c1\u6027\u6210\u50cf\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12995", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12995", "abs": "https://arxiv.org/abs/2511.12995", "authors": ["Enze Hou", "Xiaoyang Wang", "Han Wang"], "title": "Revealing the dynamic responses of Pb under shock loading based on DFT-accuracy machine learning potential", "comment": null, "summary": "Lead (Pb) is a typical low-melting-point ductile metal and serves as an important model material in the study of dynamic responses. Under shock-wave loading, its dynamic mechanical behavior comprises two key phenomena: plastic deformation and shock induced phase transitions. The underlying mechanisms of these processes are still poorly understood. Revealing these mechanisms remains challenging for experimental approaches. Non-equilibrium molecular dynamics (NEMD) simulations are an alternative theoretical tool for studying dynamic responses, as they capture atomic-scale mechanisms such as defect evolution and deformation pathways. However, due to the limited accuracy of empirical interatomic potentials, the reliability of previous NEMD studies is questioned. Using our newly developed machine learning potential for Pb-Sn alloys, we revisited the microstructure evolution in response to shock loading under various shock orientations. The results reveal that shock loading along the [001] orientation of Pb exhibits a fast, reversible, and massive phase transition and stacking fault evolution. The behavior of Pb differs from previous studies by the absence of twinning during plastic deformation. Loading along the [011] orientation leads to slow, irreversible plastic deformation, and a localized FCC-BCC phase transition in the Pitsch orientation relationship. This study provides crucial theoretical insights into the dynamic mechanical response of Pb, offering a theoretical input for understanding the microstructure-performance relationship under extreme conditions.", "AI": {"tldr": "Lead (Pb)\u5728\u51b2\u51fb\u52a0\u8f7d\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5851\u6027\u53d8\u5f62\u548c\u76f8\u53d8\uff0c\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u4f7f\u7528\u65b0\u5f00\u53d1\u7684\u673a\u5668\u5b66\u4e60\u52bf\u80fd\uff0c\u901a\u8fc7\u975e\u5e73\u8861\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff08NEMD\uff09\u7814\u7a76\u4e86\u4e0d\u540c\u51b2\u51fb\u65b9\u5411\u4e0bPb\u7684\u5fae\u89c2\u7ed3\u6784\u6f14\u5316\u3002\u7ed3\u679c\u663e\u793a\uff0c[001]\u65b9\u5411\u7684\u51b2\u51fb\u5f15\u8d77\u4e86\u5feb\u901f\u3001\u53ef\u9006\u7684\u76f8\u53d8\u548c\u5c42\u9519\u6f14\u5316\uff0c\u4e14\u65e0\u5b6a\u6676\u73b0\u8c61\uff1b[011]\u65b9\u5411\u7684\u51b2\u51fb\u5219\u5bfc\u81f4\u7f13\u6162\u3001\u4e0d\u53ef\u9006\u7684\u5851\u6027\u53d8\u5f62\u4ee5\u53ca\u5c40\u90e8\u5316\u7684FCC-BCC\u76f8\u53d8\u3002", "motivation": "\u9610\u660e\u94c5\uff08Pb\uff09\u5728\u51b2\u51fb\u52a0\u8f7d\u4e0b\u7684\u52a8\u6001\u529b\u5b66\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5851\u6027\u53d8\u5f62\u548c\u51b2\u51fb\u8bf1\u5bfc\u76f8\u53d8\u7684\u57fa\u672c\u673a\u5236\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u673a\u5236\u7684\u7406\u89e3\u4ecd\u7136\u4e0d\u8db3\uff0c\u4e14\u5b9e\u9a8c\u65b9\u6cd5\u96be\u4ee5\u63ed\u793a\u3002", "method": "\u5229\u7528\u65b0\u5f00\u53d1\u7684\u7528\u4e8ePb-Sn\u5408\u91d1\u7684\u673a\u5668\u5b66\u4e60\u52bf\u80fd\uff0c\u91c7\u7528\u975e\u5e73\u8861\u5206\u5b50\u52a8\u529b\u5b66\uff08NEMD\uff09\u6a21\u62df\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u51b2\u51fb\u65b9\u5411\uff08[001]\u548c[011]\uff09\u4e0bPb\u7684\u5fae\u89c2\u7ed3\u6784\u6f14\u5316\u3002", "result": "\u5728[001]\u65b9\u5411\u7684\u51b2\u51fb\u4e0b\uff0c\u89c2\u5bdf\u5230Pb\u53d1\u751f\u4e86\u5feb\u901f\u3001\u53ef\u9006\u7684\u5927\u89c4\u6a21\u76f8\u53d8\u548c\u5c42\u9519\u6f14\u5316\uff0c\u4e14\u5851\u6027\u53d8\u5f62\u8fc7\u7a0b\u4e2d\u672a\u51fa\u73b0\u5b6a\u6676\u3002\u800c\u5728[011]\u65b9\u5411\u7684\u51b2\u51fb\u4e0b\uff0cPb\u8868\u73b0\u51fa\u7f13\u6162\u3001\u4e0d\u53ef\u9006\u7684\u5851\u6027\u53d8\u5f62\uff0c\u5e76\u5728Pitsch\u53d6\u5411\u5173\u7cfb\u4e0b\u53d1\u751f\u4e86\u5c40\u90e8\u5316\u7684FCC-BCC\u76f8\u53d8\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5148\u8fdb\u7684\u8ba1\u7b97\u6a21\u62df\uff0c\u4e3a\u7406\u89e3\u94c5\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u52a8\u6001\u529b\u5b66\u54cd\u5e94\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u89c1\u89e3\uff0c\u7279\u522b\u662f\u63ed\u793a\u4e86\u4e0d\u540c\u51b2\u51fb\u65b9\u5411\u5bf9\u76f8\u53d8\u548c\u5851\u6027\u53d8\u5f62\u673a\u5236\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u7406\u89e3\u5fae\u89c2\u7ed3\u6784\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.11780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11780", "abs": "https://arxiv.org/abs/2511.11780", "authors": ["Hossein Mohebbi", "Mohammed Abdulrahman", "Yanting Miao", "Pascal Poupart", "Suraj Kothawade"], "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing", "comment": null, "summary": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.", "AI": {"tldr": "Image-POSER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u548c\u8c03\u5ea6\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u6a21\u578b\u6765\u5904\u7406\u957f\u800c\u590d\u6742\u7684\u6307\u4ee4\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002", "motivation": "\u5f53\u524d\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u957f\u800c\u590d\u6742\u7684\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u521b\u610f\u5de5\u4f5c\u6d41\u7684\u9700\u6c42\u3002", "method": "Image-POSER\u91c7\u7528\u53cd\u5c04\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u89c6\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002\u5b83\u52a8\u6001\u5730\u5c06\u957f\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8fdb\u884c\u76d1\u7763\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u7ec4\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u3002", "result": "Image-POSER\u5728\u884c\u4e1a\u6807\u51c6\u548c\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u56fe\u50cf\u7684\u5bf9\u9f50\u5ea6\u3001\u4fdd\u771f\u5ea6\u548c\u7f8e\u5b66\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4e5f\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u4f7f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5177\u5907\u81ea\u4e3b\u5206\u89e3\u3001\u91cd\u7ec4\u548c\u7ec4\u5408\u89c6\u89c9\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4ece\u800c\u671d\u7740\u901a\u7528\u89c6\u89c9\u52a9\u624b\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2511.12470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12470", "abs": "https://arxiv.org/abs/2511.12470", "authors": ["Zijun Wang", "Anjali Omer", "Jacob Chakareski", "Nicholas Mastronarde", "Rui Zhang"], "title": "Cross-Layer Design for Near-Field mmWave Beam Management and Scheduling under Delay-Sensitive Traffic", "comment": "Workshop paper submitted to the AI4NextG Workshop at NeurIPS 2025", "summary": "Next-generation wireless networks will rely on mmWave/sub-THz spectrum and extremely large antenna arrays (ELAAs). This will push their operation into the near field where far-field beam management degrades and beam training becomes more costly and must be done more frequently. Because ELAA training and data transmission consume energy and training trades off with service time, we pose a cross-layer control problem that couples PHY-layer beam management with MAC-layer service under delay-sensitive traffic. The controller decides when to retrain and how aggressively to train (pilot count and sparsity) while allocating transmit power, explicitly balancing pilot overhead, data-phase rate, and energy to reduce the queueing delay of MAC-layer frames/packets to be transmitted. We model the problem as a partially observable Markov decision process and solve it with deep reinforcement learning. In simulations with a realistic near-field channel and varying mobility and traffic load, the learned policy outperforms strong 5G-NR--style baselines at a comparable energy: it achieves 85.5% higher throughput than DFT sweeping and reduces the overflow rate by 78%. These results indicate a practical path to overhead-aware, traffic-adaptive near-field beam management with implications for emerging low-latency, high-rate next-generation applications such as digital twin, spatial computing, and immersive communication.", "AI": {"tldr": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u5c06\u4f9d\u8d56\u6beb\u7c73\u6ce2/\u4e9a\u592a\u8d6b\u5179\u9891\u8c31\u548c\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff08ELAA\uff09\u3002\u8fd9\u5c06\u63a8\u52a8\u5176\u8fd0\u884c\u8fdb\u5165\u8fd1\u573a\uff0c\u5bfc\u81f4\u8fdc\u573a\u6ce2\u675f\u7ba1\u7406\u6027\u80fd\u4e0b\u964d\uff0c\u6ce2\u675f\u8bad\u7ec3\u6210\u672c\u66f4\u9ad8\u4e14\u9700\u8981\u66f4\u9891\u7e41\u5730\u8fdb\u884c\u3002\u7531\u4e8e ELAA \u8bad\u7ec3\u548c\u6570\u636e\u4f20\u8f93\u4f1a\u6d88\u8017\u80fd\u91cf\uff0c\u800c\u8bad\u7ec3\u4f1a\u5f71\u54cd\u670d\u52a1\u65f6\u95f4\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u5c42\u63a7\u5236\u95ee\u9898\uff0c\u5c06\u7269\u7406\u5c42\u6ce2\u675f\u7ba1\u7406\u4e0e MAC \u5c42\u670d\u52a1\u76f8\u7ed3\u5408\uff0c\u4ee5\u5904\u7406\u5ef6\u8fdf\u654f\u611f\u6d41\u91cf\u3002\u63a7\u5236\u5668\u5728\u5206\u914d\u4f20\u8f93\u529f\u7387\u7684\u540c\u65f6\uff0c\u51b3\u5b9a\u4f55\u65f6\u91cd\u65b0\u8bad\u7ec3\u4ee5\u53ca\u8bad\u7ec3\u7684\u5f3a\u5ea6\uff08\u5bfc\u9891\u6570\u91cf\u548c\u7a00\u758f\u5ea6\uff09\uff0c\u660e\u786e\u5730\u5e73\u8861\u5bfc\u9891\u5f00\u9500\u3001\u6570\u636e\u9636\u6bb5\u901f\u7387\u548c\u80fd\u91cf\uff0c\u4ee5\u51cf\u5c11\u5f85\u4f20\u8f93\u7684 MAC \u5c42\u5e27/\u6570\u636e\u5305\u7684\u6392\u961f\u5ef6\u8fdf\u3002\u6211\u4eec\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6a21\u578b\u3002\u5728\u5177\u6709\u73b0\u5b9e\u8fd1\u573a\u4fe1\u9053\u3001\u4e0d\u540c\u79fb\u52a8\u6027\u548c\u6d41\u91cf\u8d1f\u8f7d\u7684\u6a21\u62df\u4e2d\uff0c\u6240\u5b66\u7b56\u7565\u5728\u53ef\u6bd4\u7684\u80fd\u91cf\u6d88\u8017\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5f3a\u5927\u7684 5G-NR \u98ce\u683c\u57fa\u7ebf\uff1a\u541e\u5410\u91cf\u6bd4 DFT \u626b\u63cf\u9ad8 85.5%\uff0c\u6ea2\u51fa\u7387\u964d\u4f4e 78%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4e00\u79cd\u8003\u8651\u5f00\u9500\u548c\u9002\u5e94\u6d41\u91cf\u7684\u8fd1\u573a\u6ce2\u675f\u7ba1\u7406\u5b9e\u7528\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e3a\u6570\u5b57\u5b6a\u751f\u3001\u7a7a\u95f4\u8ba1\u7b97\u548c\u6c89\u6d78\u5f0f\u901a\u4fe1\u7b49\u65b0\u5174\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u4e0b\u4e00\u4ee3\u5e94\u7528\u5e26\u6765\u542f\u793a\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u5c06\u4f9d\u8d56\u6beb\u7c73\u6ce2/\u4e9a\u592a\u8d6b\u5179\u9891\u8c31\u548c\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff08ELAA\uff09\uff0c\u8fd9\u5c06\u5bfc\u81f4\u5176\u8fd0\u884c\u8fdb\u5165\u8fd1\u573a\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u7684\u8fdc\u573a\u6ce2\u675f\u7ba1\u7406\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u5e76\u4e14\u6ce2\u675f\u8bad\u7ec3\u7684\u6210\u672c\u4f1a\u589e\u52a0\uff0c\u9700\u8981\u66f4\u9891\u7e41\u5730\u8fdb\u884c\u3002\u7531\u4e8e ELAA \u7684\u8bad\u7ec3\u548c\u6570\u636e\u4f20\u8f93\u4f1a\u6d88\u8017\u80fd\u91cf\uff0c\u5e76\u4e14\u8bad\u7ec3\u4f1a\u5f71\u54cd\u670d\u52a1\u65f6\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8de8\u5c42\u63a7\u5236\u95ee\u9898\uff0c\u5c06\u7269\u7406\u5c42\u6ce2\u675f\u7ba1\u7406\u4e0e MAC \u5c42\u670d\u52a1\u76f8\u7ed3\u5408\uff0c\u4ee5\u5904\u7406\u5ef6\u8fdf\u654f\u611f\u6d41\u91cf\u3002\u63a7\u5236\u5668\u5728\u5206\u914d\u4f20\u8f93\u529f\u7387\u7684\u540c\u65f6\uff0c\u51b3\u5b9a\u4f55\u65f6\u91cd\u65b0\u8bad\u7ec3\u4ee5\u53ca\u8bad\u7ec3\u7684\u5f3a\u5ea6\uff08\u5bfc\u9891\u6570\u91cf\u548c\u7a00\u758f\u5ea6\uff09\uff0c\u660e\u786e\u5730\u5e73\u8861\u5bfc\u9891\u5f00\u9500\u3001\u6570\u636e\u9636\u6bb5\u901f\u7387\u548c\u80fd\u91cf\uff0c\u4ee5\u51cf\u5c11\u5f85\u4f20\u8f93\u7684 MAC \u5c42\u5e27/\u6570\u636e\u5305\u7684\u6392\u961f\u5ef6\u8fdf\u3002\u8be5\u95ee\u9898\u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u5728\u5177\u6709\u73b0\u5b9e\u8fd1\u573a\u4fe1\u9053\u3001\u4e0d\u540c\u79fb\u52a8\u6027\u548c\u6d41\u91cf\u8d1f\u8f7d\u7684\u6a21\u62df\u4e2d\uff0c\u6240\u5b66\u7b56\u7565\u5728\u53ef\u6bd4\u7684\u80fd\u91cf\u6d88\u8017\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5f3a\u5927\u7684 5G-NR \u98ce\u683c\u57fa\u7ebf\uff1a\u541e\u5410\u91cf\u6bd4 DFT \u626b\u63cf\u9ad8 85.5%\uff0c\u6ea2\u51fa\u7387\u964d\u4f4e 78%\u3002", "conclusion": "\u6240\u5b66\u7b56\u7565\u8868\u660e\uff0c\u5b58\u5728\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u8003\u8651\u5f00\u9500\u548c\u9002\u5e94\u6d41\u91cf\u7684\u8fd1\u573a\u6ce2\u675f\u7ba1\u7406\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u6570\u5b57\u5b6a\u751f\u3001\u7a7a\u95f4\u8ba1\u7b97\u548c\u6c89\u6d78\u5f0f\u901a\u4fe1\u7b49\u65b0\u5174\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u4e0b\u4e00\u4ee3\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.11628", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11628", "abs": "https://arxiv.org/abs/2511.11628", "authors": ["Xinbo Wang", "Shian Jia", "Ziyang Huang", "Jing Cao", "Mingli Song"], "title": "Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies", "comment": null, "summary": "Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This \"one-policy-fits-all\" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.\n  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable \"expert\" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.\n  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.", "AI": {"tldr": "\u64cd\u4f5c\u7cfb\u7edf\u8c03\u5ea6\u5668\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f73\u7b56\u7565\u7ec4\u5408\u6765\u9002\u5e94\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u64cd\u4f5c\u7cfb\u7edf\u8c03\u5ea6\u5668\u91c7\u7528\u5355\u4e00\u3001\u9759\u6001\u7684\u7b56\u7565\uff0c\u96be\u4ee5\u5728\u591a\u6837\u5316\u548c\u52a8\u6001\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u3001\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u65b9\u9762\u51fa\u73b0\u59a5\u534f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8c03\u5ea6\u4ee3\u7406\uff08ASA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u52a0\u6743\u6982\u7387\u6295\u7968\u548c\u786c\u4ef6\u7279\u5b9a\u7684\u6620\u5c04\u8868\uff0c\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u4ee5\u7528\u6237\u4f53\u9a8c\u6307\u6807\u4e3a\u91cd\u70b9\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cASA\u572886.4%\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u9ed8\u8ba4\u7684Linux\u8c03\u5ea6\u5668\uff08EEVDF\uff09\uff0c\u5e76\u4e14\u572878.6%\u7684\u573a\u666f\u4e2d\u9009\u62e9\u7684\u8c03\u5ea6\u5668\u6392\u540d\u524d\u4e09\u3002", "conclusion": "ASA\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u548c\u54cd\u5e94\u8fc5\u901f\u7684\u64cd\u4f5c\u7cfb\u7edf\u8c03\u5ea6\u5668\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2511.13139", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13139", "abs": "https://arxiv.org/abs/2511.13139", "authors": ["Zhiteng Chao", "Yonghao Wang", "Xinyu Zhang", "Jiaxin Zhou", "Tenghui Hua", "Husheng Han", "Tianmeng Yang", "Jianan Mu", "Bei Yu", "Rui Zhang", "Jing Ye", "Huawei Li"], "title": "Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT", "comment": "6 pages, 5 figures", "summary": "Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.", "AI": {"tldr": "VeriBToT\u662f\u4e00\u79cd\u65b0\u7684LLM\u63a8\u7406\u8303\u5f0f\uff0c\u7528\u4e8e\u81ea\u52a8\u5316Verilog\u751f\u6210\uff0c\u901a\u8fc7\u96c6\u6210\u81ea\u9876\u5411\u4e0b\u548cDFV\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e2d\u95f4\u6b65\u9aa4\u7684\u81ea\u89e3\u8026\u548c\u81ea\u9a8c\u8bc1\uff0c\u5e76\u6784\u5efa\u5177\u6709\u5f62\u5f0f\u5316\u7b97\u5b50\u7684\u601d\u60f3\u56de\u6eaf\u6811\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684CoT\u8303\u5f0f\uff0c\u5b83\u63d0\u9ad8\u4e86Verilog\u751f\u6210\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u7075\u6d3b\u7684\u6a21\u5757\u5316\u3001\u5c42\u6b21\u5316\u548c\u53ef\u91cd\u7528\u6027\u4f18\u5316\u4e86\u4ee3\u5e01\u6210\u672c\u3002", "motivation": "\u5728\u81ea\u52a8\u5316\u96c6\u6210\u7535\u8def\uff08IC\uff09\u5de5\u7a0b\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08HDL\uff09\u5982Verilog\u65f6\uff0c\u9762\u4e34\u7740\u751f\u6210\u8d28\u91cf\u4e0d\u9ad8\uff0c\u590d\u6742\u8bbe\u8ba1\u96be\u4ee5\u4e00\u6b21\u6027\u751f\u6210\uff0c\u4ee5\u53ca\u89e3\u8026\u5b50\u4efb\u52a1\u96be\u4ee5\u9a8c\u8bc1\u7b49\u6311\u6218\u3002\u4f20\u7edf\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u65b9\u6cd5\u5728IC\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e0\u6cd5\u63a7\u5236CoT\u63a8\u7406\u7684\u65b9\u5411\u548c\u6b65\u9aa4\u7c92\u5ea6\uff0c\u8fd9\u4e0eRTL\u8bbe\u8ba1\u4e13\u5bb6\u7684\u77e5\u8bc6\u4e0d\u5339\u914d\u3002", "method": "VeriBToT\u901a\u8fc7\u96c6\u6210\u81ea\u9876\u5411\u4e0b\uff08Top-down\uff09\u548c\u9762\u5411\u9a8c\u8bc1\u7684\u8bbe\u8ba1\uff08DFV\uff09\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e2d\u95f4\u6b65\u9aa4\u7684\u81ea\u89e3\u8026\u548c\u81ea\u9a8c\u8bc1\uff0c\u5e76\u6784\u5efa\u4e86\u5177\u6709\u5f62\u5f0f\u5316\u7b97\u5b50\u7684\u601d\u60f3\u56de\u6eaf\u6811\uff08Backtrack Tree of Thought\uff09\u3002", "result": "\u4e0e\u4f20\u7edf\u7684CoT\u8303\u5f0f\u76f8\u6bd4\uff0cVeriBToT\u5728Verilog\u751f\u6210\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u901a\u8fc7\u7075\u6d3b\u7684\u6a21\u5757\u5316\u3001\u5c42\u6b21\u5316\u548c\u53ef\u91cd\u7528\u6027\u4f18\u5316\u4e86\u4ee3\u5e01\u6210\u672c\u3002", "conclusion": "VeriBToT\u901a\u8fc7\u5176\u72ec\u7279\u7684\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u5316Verilog\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3aLLM\u5728IC\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12384", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12384", "abs": "https://arxiv.org/abs/2511.12384", "authors": ["Weiqi Meng", "Hongyi Li", "Bai Cui"], "title": "DER Day-Ahead Offering: A Neural Network Column-and-Constraint Generation Approach", "comment": "5 pages, 1 figure. Submitted to IEEE PES General Meeting 2026", "summary": "In the day-ahead energy market, the offering strategy of distributed energy resource (DER) aggregators must be submitted before the uncertainty realization in the form of price-quantity pairs. This work addresses the day-ahead offering problem through a two-stage robust adaptive stochastic optimization model, wherein the first-stage price-quantity pairs and second-stage operational commitment decisions are made before and after DER uncertainty is realized, respectively. Uncertainty in day-ahead price is addressed using a stochastic programming, while uncertainty of DER generation is handled through robust optimization. To address the max-min structure of the second-stage problem, a neural network-accelerated column-and-constraint generation method is developed. A dedicated neural network is trained to approximate the value function, while optimality is maintained by the design of the network architecture. Numerical studies indicate that the proposed method yields high-quality solutions and is up to 100 times faster than Gurobi and 33 times faster than classical column-and-constraint generation on the same 1028-node synthetic distribution network.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u9c81\u68d2\u81ea\u9002\u5e94\u968f\u673a\u4f18\u5316\u6a21\u578b\u6765\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u65e5\u524d\u80fd\u91cf\u5e02\u573a\u7684\u62a5\u4ef7\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u65e5\u524d\u4ef7\u683c\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u53d1\u7535\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u7684\u5217\u548c\u7ea6\u675f\u751f\u6210\u65b9\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6570\u503c\u7814\u7a76\u4e2d\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u5728\u65e5\u524d\u80fd\u91cf\u5e02\u573a\u4e2d\uff0c\u9700\u8981\u5728\u4ef7\u683c\u548c\u6570\u91cf\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u4e4b\u524d\u63d0\u4ea4\u62a5\u4ef7\uff0c\u8fd9\u5bf9\u4ed6\u4eec\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u9c81\u68d2\u81ea\u9002\u5e94\u968f\u673a\u4f18\u5316\u6a21\u578b\u3002\u7b2c\u4e00\u9636\u6bb5\u786e\u5b9a\u62a5\u4ef7\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u60c5\u51b5\u505a\u51fa\u8fd0\u884c\u627f\u8bfa\u51b3\u7b56\u3002\u4f7f\u7528\u968f\u673a\u89c4\u5212\u5904\u7406\u65e5\u524d\u4ef7\u683c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u9c81\u68d2\u4f18\u5316\u5904\u7406\u5206\u5e03\u5f0f\u80fd\u6e90\u53d1\u7535\u4e0d\u786e\u5b9a\u6027\u3002\u4e3a\u89e3\u51b3\u7b2c\u4e8c\u9636\u6bb5\u95ee\u9898\u7684\u6700\u5927\u6700\u5c0f\u7ed3\u6784\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u7684\u5217\u548c\u7ea6\u675f\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u8fd1\u4f3c\u4ef7\u503c\u51fd\u6570\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57281028\u8282\u70b9\u5408\u6210\u914d\u7535\u7f51\u7edc\u4e0a\uff0c\u4e0eGurobi\u548c\u5176\u4ed6\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u7387\u63d0\u9ad8\u4e86100\u500d\u548c33\u500d\uff0c\u5e76\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4f18\u5316\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65e5\u524d\u80fd\u91cf\u5e02\u573a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2511.11958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11958", "abs": "https://arxiv.org/abs/2511.11958", "authors": ["Derek Chen", "Zoe Samuels", "Lizzie Peiros", "Sujaan Mukherjee", "Michael C. Yip"], "title": "Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media", "comment": null, "summary": "Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e0d\u540c\u87ba\u65cb\u6868\u914d\u7f6e\u5728\u5e72\u6c99\u3001\u6e7f\u6c99\u3001\u9971\u548c\u6c99\u548c\u6c34\u7b49\u4ecb\u8d28\u4e2d\u7684\u79fb\u52a8\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f20\u70ed\u5668\u8bbe\u8ba1\u7684\u4f18\u5316\u87ba\u65cb\u6868\u58f3\u4f53\u548c\u81ea\u9002\u5e94\u79fb\u52a8\u7b56\u7565\u3002", "motivation": "\u9700\u8981\u4f18\u5316\u4e24\u6816\u79fb\u52a8\u7684\u63a8\u8fdb\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u5728\u6c34\u3001\u6c99\u5730\u7b49\u591a\u79cd\u73af\u5883\u4e0b\u7684\u79fb\u52a8\u6311\u6218\u3002", "method": "\u901a\u8fc7\u539f\u7406\u4f18\u5148\u7684\u65b9\u6cd5\u5206\u6790\u87ba\u65cb\u6868\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u501f\u9274\u4f18\u5316\u6563\u70ed\u5668\u8bbe\u8ba1\u7684\u53c2\u6570\u6765\u8868\u5f81\u87ba\u65cb\u6868\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u786e\u5b9a\u4e86\u5f71\u54cd\u87ba\u65cb\u6868\u6027\u80fd\u7684\u5173\u952e\u53c2\u6570\uff0c\u5e76\u6839\u636e\u4ecb\u8d28\u4e0d\u540c\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u501f\u9274\u6563\u70ed\u5668\u8bbe\u8ba1\u7684\u53c2\u6570\u6765\u5bf9\u6027\u80fd\u8fdb\u884c\u5206\u7c7b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u87ba\u65cb\u6868\u5916\u58f3\u8bbe\u8ba1\u548c\u5f00\u53d1\u81ea\u9002\u5e94\u79fb\u52a8\u7b56\u7565\u4ee5\u589e\u5f3a\u87ba\u65cb\u6868\u63a8\u8fdb\u7cfb\u7edf\u5728\u4e24\u6816\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.12339", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12339", "abs": "https://arxiv.org/abs/2511.12339", "authors": ["Mattheus Burkhard", "Malte Kroj", "K\u00e9vin Falque", "Alberto Bramati", "Iacopo Carusotto", "Maxime J Jacquet"], "title": "Stimulated Hawking effect and quasinormal mode resonance in a polariton simulator of field theory on curved spacetime", "comment": null, "summary": "The Hawking effect amplifies fluctuations in the vicinity of horizons, both in black holes and in analogue platforms.\n  Here, we consider a polariton simulator and numerically examine the \\emph{stimulated} Hawking effect using a coherent probe incident on the horizon from the exterior.\n  We implement an experimentally realistic effective spacetime that supports a quasinormal mode (QNM) in the vicinity of the horizon.\n  We find that the stimulated Hawking effect manifests as transmission into a negative-energy Bogoliubov channel inside the horizon, consistent with pseudo-unitary Bogoliubov scattering.\n  Moreover, transmission across the horizon peaks at the QNM frequency.\n  The computed spectral signatures provide a practical guide for future experimental investigations of the Hawking effect and its interplay with QNMs, an open question in quantum field theory in curved spacetime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6781\u5316\u5b50\u6a21\u62df\u5668\u4e2d\uff0c\u901a\u8fc7\u76f8\u5e72\u63a2\u6d4b\u5668\u8bf1\u5bfc\u7684\u970d\u91d1\u6548\u5e94\uff0c\u5e76\u53d1\u73b0\u5176\u8868\u73b0\u4e3a\u8d1f\u80fd\u91cf\u7684\u6709\u6548\u4f20\u64ad\uff0c\u4e14\u4f20\u64ad\u5cf0\u503c\u51fa\u73b0\u5728\u89c6\u754c\u9644\u8fd1\u7684\u51c6\u6b63\u89c4\u6a21\u5f0f\u9891\u7387\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u7814\u7a76\u970d\u91d1\u6548\u5e94\uff08\u5728\u9ed1\u6d1e\u548c\u6a21\u62df\u5e73\u53f0\u4e2d\u90fd\u5b58\u5728\uff09\u5982\u4f55\u88ab\u8bf1\u5bfc\uff0c\u5e76\u5229\u7528\u6781\u5316\u5b50\u6a21\u62df\u5668\u8fdb\u884c\u6570\u503c\u68c0\u9a8c\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u4e0a\u53ef\u884c\u7684\u6709\u6548\u65f6\u7a7a\u6a21\u578b\uff0c\u5e76\u5229\u7528\u76f8\u5e72\u63a2\u6d4b\u5668\u4ece\u5916\u90e8\u5165\u5c04\u5230\u89c6\u754c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bf1\u5bfc\u970d\u91d1\u6548\u5e94\u8868\u73b0\u4e3a\u8fdb\u5165\u89c6\u754c\u5185\u90e8\u7684\u8d1f\u80fd\u91cf\u7684\u6709\u6548\u4f20\u64ad\uff0c\u8fd9\u4e0e\u4f2a\u5e7a\u6b63\u7684\u6709\u6548\u6563\u5c04\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u8de8\u8d8a\u89c6\u754c\u7684\u4f20\u64ad\u5cf0\u503c\u51fa\u73b0\u5728\u51c6\u6b63\u89c4\u6a21\u5f0f\u9891\u7387\u3002", "conclusion": "\u8ba1\u7b97\u51fa\u7684\u5149\u8c31\u7279\u5f81\u4e3a\u672a\u6765\u5b9e\u9a8c\u7814\u7a76\u970d\u91d1\u6548\u5e94\u53ca\u5176\u4e0e\u51c6\u6b63\u89c4\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u91cf\u5b50\u573a\u8bba\u9886\u57df\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12237", "categories": ["cs.RO", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12237", "abs": "https://arxiv.org/abs/2511.12237", "authors": ["Alysson Ribeiro da Silva", "Luiz Chaimowicz"], "title": "Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration", "comment": "9 pages, 9 figures, International Conference on Advanced Robotics", "summary": "Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u901a\u4fe1\u53d7\u9650\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u4e0b\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\uff08MRE-CCIC\uff09\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6765\u751f\u6210\u6c47\u5408\u8ba1\u5212\uff0c\u4ee5\u53ca\u57fa\u4e8e\u201c\u672a\u77e5\u573a\u666f\u4e0b\u7684\u6c47\u5408\u8ddf\u8e2a\u201d\uff08RTUS\uff09\u673a\u5236\u7684\u7b56\u7565\u6765\u6267\u884c\u8ba1\u5212\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u8ba1\u5212\u751f\u6210\u4e0d\u6700\u4f18\u548c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u5e76\u5728 Gazebo \u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u901a\u4fe1\u53d7\u9650\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\uff08MRE\uff09\u7cfb\u7edf\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u9884\u5148\u4e86\u89e3\u73af\u5883\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5148\u524d\u5de5\u4f5c\u4e2d\u8ba1\u5212\u751f\u6210\u4e0d\u6700\u4f18\u548c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u8f68\u8ff9\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa MRE-CCIC \u95ee\u9898\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u8fdb\u884c\u6c47\u5408\u8ba1\u5212\u751f\u6210\uff0c\u4ee5\u53ca\u57fa\u4e8e\u201c\u672a\u77e5\u573a\u666f\u4e0b\u7684\u6c47\u5408\u8ddf\u8e2a\u201d\uff08RTUS\uff09\u7684\u7b56\u7565\u6765\u6267\u884c\u8ba1\u5212\uff0cRTUS \u662f\u4e00\u79cd\u5141\u8bb8\u673a\u5668\u4eba\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u9075\u5faa\u65e2\u5b9a\u8ba1\u5212\u7684\u7b80\u5355\u89c4\u5219\u3002", "result": "\u5728 Gazebo \u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u53ca\u65f6\u5730\u9075\u5faa\u8ba1\u5212\u5e76\u6709\u6548\u5730\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408 MILP \u8ba1\u5212\u751f\u6210\u548c RTUS \u7b56\u7565\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u901a\u4fe1\u53d7\u9650\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u4e0b\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\u95ee\u9898\uff0c\u5e76\u80fd\u53ca\u65f6\u9075\u5faa\u8ba1\u5212\u4ee5\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002"}}
{"id": "2511.13493", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2511.13493", "abs": "https://arxiv.org/abs/2511.13493", "authors": ["Kento Takemura", "Mikio Eto", "Tomohiro Yokoyama"], "title": "Tuning of Weyl point emergence in multi-terminal Josephson junctions using quantum point contacts", "comment": "6 pages, 5 figures", "summary": "Multi-terminal Josephson junction with three or more superconductors is an attractive quantum system to emerge and tune exotic electronic states. In four terminal Josephson junctions, the Weyl physics, namely topologically protected zero energy state, emerges without assuming any exotic materials. In this study, we consider the four-terminal Josephson junction with the quantum point contact structures between the mesoscopic normal region and four superconducting terminals. The quantum point contacts can tune electrically the number of conduction channels. We theoretically investigate an effect of the increase of channels on the emergence of Weyl points. The increase of channels causes the increase of Andreev bound states in the system, which increase the emergence probability of Weyl points. When all terminals have two channels, the emergence probability is up to 17\\%, which is about four times larger than that for all single channel junctions. We consider the balance of the number of conduction channels in the four terminals. When the number of channels is unbalanced, the increase of emergence probability is suppressed.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u91cf\u5b50\u70b9\u63a5\u89e6\u7ed3\u6784\u7684\u591a\u7ec8\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\uff0c\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u589e\u52a0\u901a\u9053\u6570\u5bf9\u97e6\u5c14\u70b9\u51fa\u73b0\u6982\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u589e\u52a0\u901a\u9053\u6570\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u97e6\u5c14\u70b9\u7684\u51fa\u73b0\u6982\u7387\uff0c\u4f46\u901a\u9053\u6570\u4e0d\u5e73\u8861\u4f1a\u6291\u5236\u8fd9\u79cd\u63d0\u9ad8\u3002", "motivation": "\u591a\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\u662f\u8c03\u63a7\u5947\u5f02\u7535\u5b50\u6001\u7684\u6709\u5438\u5f15\u529b\u7684\u91cf\u5b50\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u56db\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\u53ef\u4ee5\u5728\u65e0\u9700\u5916\u5ef6\u6750\u6599\u7684\u60c5\u51b5\u4e0b\u51fa\u73b0\u62d3\u6251\u4fdd\u62a4\u7684\u96f6\u80fd\u6001\uff08\u97e6\u5c14\u7269\u7406\uff09\u3002", "method": "\u672c\u6587\u7406\u8bba\u7814\u7a76\u4e86\u5728\u56db\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\u4e2d\uff0c\u901a\u8fc7\u91cf\u5b50\u70b9\u63a5\u89e6\u7ed3\u6784\u8c03\u8282\u8d85\u5bfc\u7aef\u4e0e\u4ecb\u89c2\u6b63\u5e38\u533a\u4e4b\u95f4\u7684\u5bfc\u7535\u901a\u9053\u6570\u91cf\uff0c\u5e76\u5206\u6790\u901a\u9053\u6570\u53d8\u5316\u5bf9\u97e6\u5c14\u70b9\u51fa\u73b0\u6982\u7387\u7684\u5f71\u54cd\u3002", "result": "\u589e\u52a0\u5bfc\u7535\u901a\u9053\u6570\u91cf\u4f1a\u589e\u52a0\u7cfb\u7edf\u4e2d\u7684\u5b89\u5fb7\u70c8\u7ed3\u675f\u6001\u6570\u91cf\uff0c\u8fdb\u800c\u63d0\u9ad8\u97e6\u5c14\u70b9\u51fa\u73b0\u7684\u6982\u7387\u3002\u5f53\u6240\u6709\u7aef\u70b9\u90fd\u5177\u6709\u4e24\u4e2a\u901a\u9053\u65f6\uff0c\u51fa\u73b0\u6982\u7387\u53ef\u8fbe17%\uff0c\u662f\u5355\u901a\u9053\u7ed3\u7684\u56db\u500d\u3002\u7136\u800c\uff0c\u5f53\u901a\u9053\u6570\u91cf\u4e0d\u5e73\u8861\u65f6\uff0c\u51fa\u73b0\u6982\u7387\u7684\u589e\u52a0\u4f1a\u88ab\u6291\u5236\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5b50\u70b9\u63a5\u89e6\u7ed3\u6784\u8c03\u63a7\u591a\u7aef\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u5bfc\u7535\u901a\u9053\u6570\u91cf\uff0c\u53ef\u4ee5\u6709\u6548\u8c03\u63a7\u97e6\u5c14\u70b9\u7684\u51fa\u73b0\u6982\u7387\uff0c\u4e3a\u5b9e\u73b0\u548c\u5229\u7528\u97e6\u5c14\u7269\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.13202", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13202", "abs": "https://arxiv.org/abs/2511.13202", "authors": ["Piyush Paliwal", "Aftab Alam"], "title": "Accelerated Prediction of Temperature-Dependent Lattice Thermal Conductivity via Ensembled Machine Learning Models", "comment": null, "summary": "Lattice thermal conductivity ($\u03ba_L$) is a key physical property governing heat transport in solids, with direct relevance to thermoelectrics, thermal barrier coatings, and heat management applications. However, while experimental determination of $\u03ba_L$ is challenging, its theoretical calculation via ab initio methods particularly using density functional theory (DFT) is computationally intensive, often more demanding than electronic transport calculations by an order of magnitude. In this work, we present a machine learning (ML) approach to predict $\u03ba_L$ with DFT-level accuracy over a wide temperature range (100-1000 K). Among various models trained on DFT-calculated data \\textcolor{black}{obtained} from literature, the Extra Trees Regressor (ETR) yielded the best performance on log-scaled $\u03ba_L$, achieving an average $R^2$ of 0.9994 and a root mean square error (RMSE) of 0.0466 $W\\,m^{-1}\\,K^{-1}$. The ETR model also generalized well to twelve previously unseen (randomly chosen) low and high $\u03ba_L$ compounds with diverse space group symmetries, reaching an $R^2$ of 0.961 against DFT benchmarks. Notably, the model excels in predicting $\u03ba_L$ for both low- and high-symmetry compounds, enabling efficient high-throughput screening. We also demonstrate this capability by screening ultralow and ultrahigh $\u03ba_L$ candidates among 960 half-Heusler \\textcolor{black}{compounds} and 60,000 ICSD compounds from the AFLOW database. This result shows reliability of model developed for screening of potential thermoelectric materials. At the end, we have tested model's prediction ability on systems that have experimental $\u03ba_L$ available that shows model's ability to search material that has desirable experimental $\u03ba_L$ for thermoelectric applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u6676\u683c\u70ed\u5bfc\u7387\uff0c\u52a0\u901f\u70ed\u7535\u6750\u6599\u7684\u53d1\u73b0\u3002", "motivation": "\u8ba1\u7b97\u6750\u6599\u7684\u6676\u683c\u70ed\u5bfc\u7387\uff08\u03baL\uff09\u901a\u5e38\u6bd4\u8ba1\u7b97\u7535\u5b50\u8f93\u8fd0\u66f4\u8017\u65f6\uff0c\u9650\u5236\u4e86\u6750\u6599\u7684\u53d1\u73b0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u03baL\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fExtra Trees Regressor\uff0cETR\uff09\u6765\u9884\u6d4b\u03baL\u3002\u8be5\u6a21\u578b\u5728\u6765\u81ea\u6587\u732e\u7684\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5404\u79cd\u6e29\u5ea6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "ETR\u6a21\u578b\u5728\u9884\u6d4b\u03baL\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u51c6\u786e\u6027\uff0cR\u00b2\u4e3a0.9994\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.0466 W m\u207b\u00b9 K\u207b\u00b9\u3002\u8be5\u6a21\u578b\u8fd8\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u548c\u5177\u6709\u4e0d\u540c\u5bf9\u79f0\u6027\u7684\u5316\u5408\u7269\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u8be5\u6a21\u578b\u7b5b\u9009\u4e86\u6f5c\u5728\u7684\u70ed\u7535\u6750\u6599\uff0c\u5e76\u5728\u5177\u6709\u53ef\u7528\u5b9e\u9a8c\u03baL\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u03baL\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u7b5b\u9009\u5177\u6709\u6240\u9700\u70ed\u7535\u7279\u6027\u7684\u6750\u6599\u3002"}}
{"id": "2511.11824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11824", "abs": "https://arxiv.org/abs/2511.11824", "authors": ["Zhongping Dong", "Pengyang Yu", "Shuangjian Li", "Liming Chen", "Mohand Tahar Kechadi"], "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction", "comment": null, "summary": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.", "AI": {"tldr": "SOTFormer\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684Transformer\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u5b9e\u65f6\u611f\u77e5\u4e2d\u7edf\u4e00\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u77ed\u671f\u8fd0\u52a8\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u65f6\u95f4\u6f02\u79fb\u7b49\u6311\u6218\u3002", "motivation": "\u5728\u906e\u6321\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u65f6\u95f4\u6f02\u79fb\u7b49\u6311\u6218\u4e0b\uff0c\u51c6\u786e\u7684\u5355\u76ee\u6807\u8ddf\u8e2a\u548c\u77ed\u671f\u8fd0\u52a8\u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u4f1a\u7834\u574f\u5b9e\u65f6\u611f\u77e5\u6240\u9700\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "SOTFormer\u662f\u4e00\u4e2a\u6700\u5c0f\u7684\u3001\u5177\u6709\u6052\u5b9a\u5185\u5b58\u7684Transformer\u6a21\u578b\uff0c\u5b83\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\u4e2d\u7edf\u4e00\u4e86\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u77ed\u671f\u8f68\u8ff9\u9884\u6d4b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4e00\u4e2a\u771f\u5b9e\u76ee\u6807\u9a71\u52a8\u7684\u5185\u5b58\u548c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7a33\u5b9a\u521d\u59cb\u5316\u7684burn-in anchor loss\u6765\u5b9e\u73b0\u7a33\u5b9a\u7684\u8eab\u4efd\u4f20\u64ad\u3002\u5355\u4e2a\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u8de8\u5e27\u4f18\u5316\u5d4c\u5165\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u548c\u56fa\u5b9a\u7684GPU\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5728Mini-LaSOT\uff0820%\uff09\u57fa\u51c6\u4e0a\uff0cSOTFormer\u8fbe\u5230\u4e8676.3 AUC\u548c53.7 FPS\uff08AMP\uff0c4.3 GB VRAM\uff09\uff0c\u5728\u5feb\u901f\u8fd0\u52a8\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u906e\u6321\u65b9\u9762\u4f18\u4e8eTrackFormer\u548cMOTRv2\u7b49Transformer\u57fa\u7ebf\u3002", "conclusion": "SOTFormer\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5185\u5b58\u548c\u635f\u5931\u673a\u5236\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5355\u76ee\u6807\u8ddf\u8e2a\u548c\u77ed\u671f\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.12478", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12478", "abs": "https://arxiv.org/abs/2511.12478", "authors": ["Mahdi Pirayesh Shirazi Nejad", "David Hicks", "Matt Valentine", "Ki H. Chon"], "title": "Lightweight Deep Autoencoder for ECG Denoising with Morphology Preservation and Near Real-Time Hardware Deployment", "comment": null, "summary": "Electrocardiogram (ECG) signals are often degraded by various noise sources such as baseline wander, motion artifacts, and electromyographic interference, posing a major challenge in clinical settings. This paper presents a lightweight deep learning-based denoising framework, forming a compact autoencoder architecture. The model was trained under severe noise conditions (-5 dB signal-to-noise ratio (SNR)) using a rigorously partitioned dataset to ensure no data leakage and robust generalization. Extensive evaluations were conducted across seven noise configurations and three SNR levels (-5 dB, 0 dB, and +5 dB), showing consistent denoising performance with minimal morphological distortion, critical for maintaining diagnostic integrity. In particular, tests on clinically vital rhythms such as ventricular tachycardia (VT) and ventricular fibrillation (VF) confirm that the proposed model effectively suppresses noise without altering arrhythmic features essential for diagnosis. Visual and quantitative assessments, including SNR improvement, RMSE, and correlation metrics, validate the model's efficacy in preserving waveform fidelity. To demonstrate real-world applicability, the model was deployed on a Raspberry Pi 4 using TensorFlow Lite with float16 precision. Inference latency was measured at just 1.41 seconds per 14-second ECG segment, indicating feasibility for near-real-time use in edge devices. Overall, this study introduces a lightweight, hardware-validated, and morphologically reliable ECG denoising solution suitable for integration into portable or wearable healthcare systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60ECG\u53bb\u566a\u6846\u67b6\uff0c\u80fd\u5728\u6076\u52a3\u566a\u58f0\u6761\u4ef6\u4e0b\u6709\u6548\u53bb\u566a\uff0c\u540c\u65f6\u4fdd\u6301\u5fc3\u5f8b\u7279\u5f81\uff0c\u5e76\u5df2\u5728\u6811\u8393\u6d3e\u4e0a\u9a8c\u8bc1\u5176\u5b9e\u65f6\u6027\u3002", "motivation": "ECG\u4fe1\u53f7\u5e38\u88ab\u566a\u58f0\u5e72\u6270\uff0c\u5f71\u54cd\u4e34\u5e8a\u8bca\u65ad\uff0c\u9700\u8981\u6709\u6548\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u81ea\u7f16\u7801\u5668\u67b6\u6784\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u6846\u67b6\uff0c\u5728\u4e25\u82db\u566a\u58f0\u6761\u4ef6\u4e0b\uff08\u4fe1\u566a\u6bd4-5 dB\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u566a\u58f0\u914d\u7f6e\u548c\u4fe1\u566a\u6bd4\u6c34\u5e73\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u566a\u58f0\u548c\u4fe1\u566a\u6bd4\u4e0b\u8868\u73b0\u51fa\u7a33\u5b9a\u53bb\u566a\u6027\u80fd\uff0c\u6700\u5c0f\u5316\u5f62\u6001\u5931\u771f\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u540c\u65f6\u4fdd\u7559VT\u548cVF\u7b49\u5fc3\u5f8b\u5931\u5e38\u7279\u5f81\u3002\u5728\u6811\u8393\u6d3e4\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u4e3a1.41\u79d2/14\u79d2ECG\u7247\u6bb5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u8fd1\u5b9e\u65f6\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u786c\u4ef6\u9a8c\u8bc1\u3001\u5f62\u6001\u53ef\u9760\u7684ECG\u53bb\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4fbf\u643a\u5f0f\u6216\u53ef\u7a7f\u6234\u533b\u7597\u4fdd\u5065\u7cfb\u7edf\u3002"}}
{"id": "2511.11640", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11640", "abs": "https://arxiv.org/abs/2511.11640", "authors": ["Sed Centeno", "Christopher Sprague", "Arnab A Purkayastha", "Ray Simar", "Neeraj Magotra"], "title": "Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications", "comment": "5 pages", "summary": "Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.", "AI": {"tldr": "Speculative backpropagation accelerates neural network training by overlapping forward and backward passes using speculative weight updates. Implemented on MNIST with OpenMP, it achieves up to 24% speedup on CPU and shows potential for FPGA hardware acceleration.", "motivation": "To accelerate neural network training by overlapping forward and backward passes using speculative backpropagation.", "method": "Implemented speculative backpropagation on MNIST using OpenMP for multi-threading, enabling simultaneous forward and backward steps. Evaluated on CPU and planned for FPGA synthesis.", "result": "Achieved a maximum speedup of 24% in execution time with a threshold of 0.25, while maintaining accuracy within 3-4% of the baseline. Individual step execution showed a maximum speedup of 35%.", "conclusion": "Speculative backpropagation effectively speeds up training by overlapping passes, with significant execution time improvements demonstrated on CPU and promising potential for hardware acceleration on FPGAs."}}
{"id": "2511.13343", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13343", "abs": "https://arxiv.org/abs/2511.13343", "authors": ["A Cormier", "David Roqui", "Fabrice Surma", "Martin Labour\u00e9", "Jean-Marc Vallet", "Odile Guillon", "N Grozavu", "Ann Bourg\u00e8s"], "title": "Coliseum project: Correlating climate change data with the behavior of heritage materials", "comment": null, "summary": "Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.", "AI": {"tldr": "\u8be5COLISEUM\u9879\u76ee\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6cd5\u56fd\u4e09\u4e2a\u9057\u5740\u6536\u96c6\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8ba1\u7b97\u673a\u6a21\u578b\u9884\u6d4b\u6587\u7269\u6750\u6599\u7684\u884c\u4e3a\uff0c\u4ee5\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5bf9\u6587\u7269\u7684\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u6b63\u5728\u5f71\u54cd\u6587\u7269\u6750\u6599\uff0c\u6c14\u5019\u53d8\u5316\u589e\u52a0\u4e86\u77f3\u5316\u7a0b\u5ea6\uff0c\u7f29\u77ed\u4e86\u7eaa\u5ff5\u7891\u7684\u4f7f\u7528\u5bff\u547d\u3002\u7531\u4e8e\u98ce\u5316\u53d6\u51b3\u4e8e\u8bb8\u591a\u56e0\u7d20\uff0c\u56e0\u6b64\u5f88\u96be\u5c06\u5176\u8fdb\u5c55\u4e0e\u6c14\u5019\u53d8\u5316\u8054\u7cfb\u8d77\u6765\u3002\u4e3a\u4e86\u9884\u6d4b\u98ce\u5316\uff0c\u5fc5\u987b\u6536\u96c6\u6c14\u5019\u6570\u636e\uff0c\u540c\u65f6\u76d1\u6d4b\u98ce\u5316\u8fdb\u5c55\u3002\u6240\u6536\u96c6\u6570\u636e\u7684\u591a\u6a21\u6001\u6027\u8d28\uff08\u56fe\u50cf\u3001\u6587\u672c\u7b49\uff09\u4f7f\u5f97\u76f8\u5173\u6027\u96be\u4ee5\u786e\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u3002", "method": "COLISEUM\u9879\u76ee\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6cd5\u56fd\u4e09\u4e2a\u9057\u5740\u6536\u96c6\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8ba1\u7b97\u673a\u6a21\u578b\u9884\u6d4b\u6587\u7269\u6750\u6599\u7684\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u5728\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u3001\u6bd4\u5e03\u62c9\u514b\u7279\u8003\u53e4\u9057\u5740\u548c\u5723\u76ae\u57c3\u5c14\u6559\u5802\u8fdb\u884c\u73b0\u573a\u76d1\u6d4b\uff0c\u5e76\u6536\u96c6\u6c14\u5019\u6570\u636e\u548c\u75c5\u5bb3\u6570\u636e\u3002\u901a\u8fc7\u8ba1\u7b97\u98ce\u5316\u6307\u6570\uff0c\u5c06\u6570\u636e\u6536\u96c6\u5230\u98ce\u5316\u77e9\u9635\u4e2d\uff0c\u4ee5\u5b9e\u65bd\u98ce\u5316\u6a21\u578b\u3002", "result": "\u8be5\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4eea\u5668\u65b9\u6cd5\u3001\u521d\u6b65\u8bca\u65ad\u4ee5\u53ca\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u73b0\u573a\u7684\u7b2c\u4e00\u4e2a\u793a\u4f8b\u7ed3\u679c\u3002\u5728\u6bcf\u4e2a\u5730\u70b9\uff0c\u5fae\u6c14\u5019\u4f20\u611f\u5668\u4f1a\u6301\u7eed\u8bb0\u5f55\u968f\u65f6\u95f4\u53d8\u5316\u7684\u53c2\u6570\u3002\u901a\u8fc7\u5316\u5b66\u5206\u6790\u3001\u7ed8\u56fe\u6d4b\u91cf\u548c\u79d1\u5b66\u6210\u50cf\u6d3b\u52a8\uff0c\u5b9a\u671f\u76d1\u6d4b\u98ce\u5316\u72b6\u6001\u3002", "conclusion": "\u8be5COLISEUM\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8ba1\u7b97\u673a\u6a21\u578b\u9884\u6d4b\u6587\u7269\u6750\u6599\u7684\u884c\u4e3a\uff0c\u6765\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5bf9\u6587\u7269\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u6765\u81ea\u4e0d\u540c\u6c14\u5019\u53d8\u5316\u60c5\u666f\u7684\u5df2\u77e5\u6570\u636e\uff0c\u9884\u6d4b\u6a21\u578b\u5c06\u80fd\u591f\u9884\u6d4b\u6750\u6599\u7684\u672a\u6765\u884c\u4e3a\u3002"}}
{"id": "2511.12431", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12431", "abs": "https://arxiv.org/abs/2511.12431", "authors": ["Zhuoyuan Wang", "Xiyu Deng", "Hikaru Hoshino", "Yorie Nakahira"], "title": "Online Adaptive Probabilistic Safety Certificate with Language Guidance", "comment": null, "summary": "Achieving long-term safety in uncertain or extreme environments while accounting for human preferences remains a fundamental challenge for autonomous systems. Existing methods often trade off long-term guarantees for fast real-time control and cannot adapt to variability in human preferences or risk tolerance. To address these limitations, we propose a language-guided adaptive probabilistic safety certificate (PSC) framework that guarantees long-term safety for stochastic systems under environmental uncertainty while accommodating diverse human preferences. The proposed framework integrates natural-language inputs from users and Bayesian estimators of the environment into adaptive safety certificates that explicitly account for user preferences, system dynamics, and quantified uncertainties. Our key technical innovation leverages probabilistic invariance--a generalization of forward invariance to a probability space--to obtain myopic safety conditions with long-term safety guarantees that integrate language guidance, model information, and quantified uncertainty. We validate the framework through numerical simulations of autonomous lane-keeping with human-in-the-loop guidance under uncertain and extreme road conditions, demonstrating enhanced safety-performance trade-offs, adaptability to changing environments, and personalization to different user preferences.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6982\u7387\u5b89\u5168\u8bc1\u4e66\uff08PSC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6216\u6781\u7aef\u73af\u5883\u4e0b\u5b9e\u73b0\u5bf9\u968f\u673a\u7cfb\u7edf\u7684\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u80fd\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u548c\u98ce\u9669\u5bb9\u5fcd\u5ea6\u7684\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\u4e0e\u5feb\u901f\u5b9e\u65f6\u63a7\u5236\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u65e0\u6cd5\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u6216\u98ce\u9669\u5bb9\u5fcd\u5ea6\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6982\u7387\u5b89\u5168\u8bc1\u4e66\uff08PSC\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u6765\u81ea\u7528\u6237\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u548c\u5bf9\u73af\u5883\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u4ee5\u751f\u6210\u81ea\u9002\u5e94\u5b89\u5168\u8bc1\u4e66\u3002\u8be5\u8bc1\u4e66\u660e\u786e\u8003\u8651\u4e86\u7528\u6237\u504f\u597d\u3001\u7cfb\u7edf\u52a8\u6001\u548c\u91cf\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5173\u952e\u6280\u672f\u521b\u65b0\u5728\u4e8e\u5229\u7528\u6982\u7387\u4e0d\u53d8\u6027\uff08probabilistic invariance\uff09\u2014\u2014\u524d\u5411\u4e0d\u53d8\u6027\uff08forward invariance\uff09\u5728\u6982\u7387\u7a7a\u95f4\u4e0a\u7684\u63a8\u5e7f\u2014\u2014\u6765\u83b7\u5f97\u5177\u6709\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\u7684\u8fd1\u89c6\u5b89\u5168\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u786e\u5b9a\u7684\u6781\u7aef\u9053\u8def\u6761\u4ef6\u4e0b\u8fdb\u884c\u5e26\u6709\u201c\u4eba\u673a\u534f\u540c\u201d\u6307\u5bfc\u7684\u81ea\u4e3b\u8f66\u9053\u4fdd\u6301\u7684\u6570\u503c\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u5b89\u5168\u6027-\u6027\u80fd\u7684\u6743\u8861\uff0c\u9002\u5e94\u53d8\u5316\u7684\u73af\u5883\uff0c\u5e76\u5b9e\u73b0\u4e2a\u6027\u5316\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u504f\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684PSC\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\u3001\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u53d8\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u590d\u6742\u73af\u5883\u4e0b\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2511.11967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11967", "abs": "https://arxiv.org/abs/2511.11967", "authors": ["Mani Amani", "Behrad Beheshti", "Reza Akhavian"], "title": "Bootstrapped LLM Semantics for Context-Aware Path Planning", "comment": null, "summary": "Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM \"danger\" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f6c\u53d8\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba9\u673a\u5668\u4eba\u7406\u89e3\u201c\u505a\u4ec0\u4e48\u201d\u800c\u975e\u201c\u5982\u4f55\u5b89\u5168\u9ad8\u6548\u5730\u505a\u201d\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u3001\u5bcc\u542b\u8bed\u4e49\u7684\u7a7a\u95f4\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06LLM\u4f5c\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u5176\u8f93\u51fa\u53ef\u4ee5\u8c03\u8282\u7ecf\u5178\u89c4\u5212\u5668\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3001\u8bed\u4e49\u5730\u56fe\uff0c\u901a\u8fc7\u591a\u6b21LLM\u201c\u5371\u9669\u201d\u5224\u65ad\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65afbootstrap\u6765\u8fd1\u4f3c\u98ce\u9669\u7684\u540e\u9a8c\u5206\u5e03\u3002\u57fa\u4e8e\u540e\u9a8c\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u52bf\u80fd\u6210\u672c\u51fd\u6570\uff0c\u7528\u4e8e\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u57fa\u4e8eBIM\u7684\u6570\u5b57\u5b6a\u751f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u660e\u786e\u7684\u63d0\u793a\u548c\u9690\u542b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u673a\u5668\u4eba\u7684\u79fb\u52a8\u65b9\u5f0f\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u6765\u652f\u6301\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5229\u7528LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u5168\u3001\u9ad8\u6548\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002"}}
{"id": "2511.12352", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2511.12352", "abs": "https://arxiv.org/abs/2511.12352", "authors": ["Adnan Naimy", "Abdallah Slaoui", "Abderrahim Lakhfif", "Rachid Ahl Laamara"], "title": "Optimal Multiparameter Quantum Estimation of Magnonic Couplings in a Magnomechanical Cavity", "comment": null, "summary": "In this work, we introduce an experimentally viable scheme to enhance the simultaneous estimation precision of the couplings $G_{mc}$ and $G_{mb}$, with a particular focus on the performance of heterodyne detection. By comparing simultaneous and individual estimation strategies, we demonstrate that the simultaneous approach offers a notable advantage in our system. To support this, we compute the quantum Fisher information matrices (QFIMs) based on the symmetric logarithmic derivative (SLD) and the right logarithmic derivative (RLD). Our results show that the quantum Cram\u00e9r Rao bound (QCRB) associated with the RLD is consistently lower than that of the SLD, indicating superior estimation precision. From a physical standpoint, this improvement reflects the system's enhanced capacity to encode, transfer, and extract quantum information while allowing optimal control of fundamental interactions. We show that increasing the Rabi frequency, cavity loss rate, and the average number of photons and phonons, combined with reduced mechanical damping and temperature, enhances the system's sensitivity to the coupling parameters. These mechanisms act on the available quantum resources, such as entanglement, squeezing, and state purity, leading to more precise estimations. Furthermore, our analysis reveals that under certain conditions, heterodyne detection can closely approach the ultimate precision set by the QFIM. This suggests that a measurement strategy based on heterodyne detection can offer an efficient and practical route for estimating the couplings $G_{mc}$ and $G_{mb}$, paving the way for high precision hybrid quantum sensors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u9a8c\u4e0a\u53ef\u884c\u7684\u65b9\u6848\uff0c\u5229\u7528\u5916\u5dee\u63a2\u6d4b\u6765\u63d0\u9ad8 $G_{mc}$ \u548c $G_{mb}$ \u8026\u5408\u53c2\u6570\u7684\u540c\u6b65\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u4e0e\u5355\u72ec\u4f30\u8ba1\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63d0\u9ad8\u91cf\u5b50\u7cfb\u7edf\u4e2d\u8026\u5408\u53c2\u6570 $G_{mc}$ \u548c $G_{mb}$ \u7684\u540c\u6b65\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u63a2\u7d22\u5916\u5dee\u63a2\u6d4b\u5728\u8be5\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u5bf9\u79f0\u5bf9\u6570\u5bfc\u6570\uff08SLD\uff09\u548c\u53f3\u5bf9\u6570\u5bfc\u6570\uff08RLD\uff09\u7684\u91cf\u5b50\u8d39\u96ea\u4fe1\u606f\u77e9\u9635\uff08QFIM\uff09\uff0c\u6bd4\u8f83\u4e86\u540c\u6b65\u4f30\u8ba1\u548c\u5355\u72ec\u4f30\u8ba1\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u62c9\u6bd4\u9891\u7387\u3001\u8154\u635f\u8017\u7387\u3001\u5149\u5b50\u548c\u58f0\u5b50\u6570\u91cf\u3001\u673a\u68b0\u963b\u5c3c\u548c\u6e29\u5ea6\u7b49\u53c2\u6570\u5bf9\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u540c\u6b65\u4f30\u8ba1\u7b56\u7565\u76f8\u6bd4\u5355\u72ec\u4f30\u8ba1\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002RLD \u5bf9\u5e94\u7684\u91cf\u5b50\u514b\u62c9\u7f8e-\u7f57\u754c\uff08QCRB\uff09\u59cb\u7ec8\u4f4e\u4e8e SLD \u5bf9\u5e94\u7684 QCRB\uff0c\u8868\u660e\u4f30\u8ba1\u7cbe\u5ea6\u66f4\u9ad8\u3002\u589e\u52a0\u62c9\u6bd4\u9891\u7387\u3001\u8154\u635f\u8017\u7387\u3001\u5149\u5b50\u548c\u58f0\u5b50\u6570\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u673a\u68b0\u963b\u5c3c\u548c\u6e29\u5ea6\uff0c\u53ef\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u5bf9\u8026\u5408\u53c2\u6570\u7684\u7075\u654f\u5ea6\u3002\u5916\u5dee\u63a2\u6d4b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u63a5\u8fd1 QFIM \u8bbe\u5b9a\u7684\u6781\u9650\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63d0\u9ad8\u8026\u5408\u53c2\u6570\u4f30\u8ba1\u7cbe\u5ea6\u7684\u65b9\u6848\uff0c\u5e76\u8868\u660e\u5916\u5dee\u63a2\u6d4b\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u6d4b\u91cf\u7b56\u7565\uff0c\u6709\u671b\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u6df7\u5408\u91cf\u5b50\u4f20\u611f\u5668\u7684\u5f00\u53d1\u3002"}}
{"id": "2511.12439", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12439", "abs": "https://arxiv.org/abs/2511.12439", "authors": ["Yujia Liu", "Sophia Yu", "Hongyue Jin", "Jessica Wen", "Alexander Qian", "Terrence Lee", "Mattheus Ramsis", "Gi Won Choi", "Lianhui Qin", "Xin Liu", "Edward J. Wang"], "title": "Multi-agent Self-triage System with Medical Flowcharts", "comment": null, "summary": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684AI\u5206\u8bca\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86100\u4e2aAMA\u6d41\u7a0b\u56fe\uff0c\u8fbe\u5230\u4e8695%\u4ee5\u4e0a\u7684\u5206\u8bca\u51c6\u786e\u7387\uff0c\u65e8\u5728\u63d0\u9ad8\u533b\u7597\u51b3\u7b56\u652f\u6301\u7684\u900f\u660e\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5728\u7ebf\u5065\u5eb7\u8d44\u6e90\u548cLLM\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u5b58\u5728\u51c6\u786e\u6027\u4f4e\u3001\u900f\u660e\u5ea6\u5dee\u3001\u4fe1\u606f\u672a\u7ecf\u6838\u5b9e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u68c0\u7d22\u4ee3\u7406\u3001\u51b3\u7b56\u4ee3\u7406\u548c\u804a\u5929\u4ee3\u7406\uff0c\u5229\u7528100\u4e2aAMA\u6d41\u7a0b\u56fe\u6765\u6307\u5bfcLLM\u8fdb\u884c\u60a3\u8005\u81ea\u6211\u5206\u8bca\u3002", "result": "\u5728\u6a21\u62df\u5bf9\u8bdd\u4e2d\uff0c\u6d41\u7a0b\u56fe\u68c0\u7d22\u7684top-3\u51c6\u786e\u7387\u4e3a95.29%\uff08N=2,000\uff09\uff0c\u6d41\u7a0b\u56fe\u5bfc\u822a\u7684\u51c6\u786e\u7387\u4e3a99.10%\uff08N=37,200\uff09\u3002", "conclusion": "\u7ed3\u5408LLM\u7684\u7075\u6d3b\u6027\u548c\u6807\u51c6\u5316\u4e34\u5e8a\u534f\u8bae\u7684\u4e25\u8c28\u6027\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u900f\u660e\u3001\u51c6\u786e\u3001\u53ef\u6cdb\u5316\u7684AI\u8f85\u52a9\u81ea\u6211\u5206\u8bca\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u652f\u6301\u60a3\u8005\u7684\u660e\u667a\u51b3\u7b56\u5e76\u63d0\u9ad8\u533b\u7597\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2511.13210", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13210", "abs": "https://arxiv.org/abs/2511.13210", "authors": ["Tushar Jogi"], "title": "Variationally Consistent Framework for Finite-Strain Microelasticity", "comment": null, "summary": "Modeling microstructural evolution at large strains requires mechanical formulations that remain thermodynamically consistent while capturing significant lattice rotations and transformation-induced stresses. However, most existing finite-strain microelasticity and phase-field approaches apply macroscopic boundary conditions heuristically, preventing proper stress relaxation and violating the Hill-Mandel work equivalence required for homogenization. These limitations can misrepresent stress states and transformation pathways under finite strains. Here a variationally consistent finite-strain microelasticity framework is presented that couples microscopic and macroscopic mechanical equilibrium through a single energy functional. The resulting Euler-Lagrange conditions, periodic micro-equilibrium and macroscopic stress balance, are solved using a staggered FFT-Newton algorithm that combines a spectral fixed-point update for local fields with a Newton step for the homogenized deformation gradient. The formulation accommodates general hyperelastic constitutive laws and arbitrary transformation gradients. Benchmarks demonstrate accurate recovery of small-strain Eshelby solutions and systematic nonlinear deviations at large dilatations. Applied to deformation twinning in magnesium, the framework reproduces lenticular morphology, stress redistribution, and faster lateral growth consistent with experiments. This approach establishes a rigorous and scalable foundation for finite-strain phase-field simulations of coherent transformations under general stress or mixed boundary conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u4e00\u81f4\u7684\u6709\u9650\u5e94\u53d8\u5fae\u5f39\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5927\u5e94\u53d8\u4e0b\u7684\u663e\u5fae\u7ed3\u6784\u6f14\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b8f\u89c2\u8fb9\u754c\u6761\u4ef6\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7FFT-Newton\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u6c42\u89e3\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u6a21\u62df\u9541\u7684\u5f62\u53d8\u5b6a\u6676\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u5e94\u53d8\u663e\u5fae\u5f39\u6027\u65b9\u6cd5\u5728\u5904\u7406\u5b8f\u89c2\u8fb9\u754c\u6761\u4ef6\u65f6\u5b58\u5728\u542f\u53d1\u5f0f\u5e94\u7528\u7684\u95ee\u9898\uff0c\u8fd9\u5bfc\u81f4\u5e94\u529b\u677e\u5f1b\u4e0d\u5f53\u548c\u5b8f\u89c2\u5747\u5300\u5316\u4e2d\u7684Hill-Mandel\u529f\u5f53\u91cf\u5173\u7cfb\u5931\u6548\uff0c\u4ece\u800c\u53ef\u80fd\u9519\u8bef\u5730\u8868\u793a\u6709\u9650\u5e94\u53d8\u4e0b\u7684\u5e94\u529b\u72b6\u6001\u548c\u76f8\u53d8\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u4e00\u81f4\u7684\u6709\u9650\u5e94\u53d8\u5fae\u5f39\u6027\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5355\u4e00\u80fd\u91cf\u6cdb\u51fd\u5c06\u5fae\u89c2\u548c\u5b8f\u89c2\u529b\u5b66\u5e73\u8861\u8026\u5408\u8d77\u6765\u3002\u5229\u7528\u4ea4\u9519\u5f0fFFT-Newton\u7b97\u6cd5\u6c42\u89e3\u5f97\u5230\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u6761\u4ef6\uff08\u5468\u671f\u6027\u5fae\u5e73\u8861\u548c\u5b8f\u89c2\u5e94\u529b\u5e73\u8861\uff09\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u7528\u4e8e\u5c40\u90e8\u573a\u7684\u8c31\u4e0d\u52a8\u70b9\u66f4\u65b0\u548c\u7528\u4e8e\u5747\u5300\u5316\u53d8\u5f62\u68af\u5ea6\u7684\u725b\u987f\u6b65\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4e00\u822c\u7684\u8d85\u5f39\u6027\u672c\u6784\u5f8b\u548c\u4efb\u610f\u7684\u76f8\u53d8\u68af\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u5730\u6062\u590d\u5c0f\u5e94\u53d8\u4e0b\u7684Eshelby\u89e3\uff0c\u5e76\u5728\u5927\u53d8\u5f62\u65f6\u663e\u793a\u51fa\u7cfb\u7edf\u6027\u7684\u975e\u7ebf\u6027\u504f\u5dee\u3002\u5e94\u7528\u4e8e\u9541\u7684\u5f62\u53d8\u5b6a\u6676\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u91cd\u73b0\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684\u900f\u955c\u72b6\u5f62\u8c8c\u3001\u5e94\u529b\u518d\u5206\u914d\u4ee5\u53ca\u66f4\u5feb\u7684\u4fa7\u5411\u751f\u957f\u901f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e25\u683c\u4e14\u53ef\u6269\u5c55\u7684\u6709\u9650\u5e94\u53d8\u76f8\u573a\u6a21\u62df\u57fa\u7840\uff0c\u80fd\u591f\u5904\u7406\u4e00\u822c\u5e94\u529b\u6216\u6df7\u5408\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u76f8\u5e72\u76f8\u53d8\u95ee\u9898\u3002"}}
{"id": "2511.11837", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11837", "abs": "https://arxiv.org/abs/2511.11837", "authors": ["Fatemeh Elhambakhsh", "Gaurav Ameta", "Aditi Roy", "Hyunwoong Ko"], "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning", "comment": null, "summary": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MP-GFormer\uff0c\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u4e09\u7ef4\u51e0\u4f55\u4fe1\u606f\u548c\u52a8\u6001\u56fe\u5b66\u4e60\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u68b0\u52a0\u5de5\u8fc7\u7a0b\u89c4\u5212\u4e2d\u7684\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u9884\u6d4b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8624%\u7684\u4e3b\u64cd\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u548c36%\u7684\u5b50\u64cd\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u673a\u68b0\u52a0\u5de5\u8fc7\u7a0b\u89c4\u5212\uff08MP\uff09\u4e2d\u7684\u52a8\u6001\u4f9d\u8d56\u6027\u65f6\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u96f6\u4ef6\u7684\u4e09\u7ef4\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u9886\u57df\u8ba4\u77e5\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMP-GFormer\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4e09\u7ef4\u51e0\u4f55\u611f\u77e5\u52a8\u6001\u56feTransformer\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6f14\u53d8\u7684\u4e09\u7ef4\u51e0\u4f55\u8868\u793a\u6574\u5408\u5230\u52a8\u6001\u56fe\u5b66\u4e60\u4e2d\uff0c\u7528\u4e8e\u9884\u6d4b\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u7acb\u4f53\u5149\u523b\u66f2\u9762\u7f51\u683c\u6765\u8868\u793a\u6bcf\u6b21\u52a0\u5de5\u64cd\u4f5c\u540e\u7684\u96f6\u4ef6\u4e09\u7ef4\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u91c7\u7528\u8fb9\u754c\u8868\u793a\u6cd5\u5904\u7406\u521d\u59cb\u4e09\u7ef4\u8bbe\u8ba1\u3002", "result": "MP-GFormer\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e3b\u64cd\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e8624%\uff0c\u5b50\u64cd\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e8636%\u3002", "conclusion": "MP-GFormer\u901a\u8fc7\u6574\u5408\u4e09\u7ef4\u51e0\u4f55\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001\u56fe\u5b66\u4e60\u5728\u673a\u68b0\u52a0\u5de5\u8fc7\u7a0b\u89c4\u5212\u4e2d\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12508", "abs": "https://arxiv.org/abs/2511.12508", "authors": ["Yanhao Wang", "Lei Wang", "Jie Wang", "Yimin Liu"], "title": "Robust Radar HRRP Recognition under Non-uniform Jamming Based on Complex-valued Frequency Attention Network", "comment": null, "summary": "Complex electromagnetic environments, often containing multiple jammers with different jamming patterns, produce non-uniform jamming power across the frequency spectrum. This spectral non-uniformity directly induces severe distortion in the target's HRRP, consequently compromising the performance and reliability of conventional HRRP-based target recognition methods. This paper proposes a novel, end-to-end trained network for robust radar target recognition. The core of our model is a CFA module that operates directly on the complex spectrum of the received echo. The CFA module learns to generate an adaptive frequency-domain filter, assigning lower weights to bands corrupted by strong jamming while preserving critical target information in cleaner bands. The filtered spectrum is then fed into a classifier backbone for recognition. Experimental results on simulated HRRP data with various jamming combinations demonstrate our method's superiority. Notably, under severe jamming conditions, our model achieves a recognition accuracy nearly 9% higher than traditional model-based approaches, all while introducing negligible computational overhead. This highlights its exceptional performance and robustness in challenging jamming environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u96f7\u8fbe\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7CFA\u6a21\u5757\u81ea\u9002\u5e94\u5730\u6ee4\u9664\u5e72\u6270\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684\u8bc6\u522b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u590d\u6742\u7684\u7535\u78c1\u73af\u5883\u548c\u591a\u5e72\u6270\u6e90\u5bfc\u81f4\u63a5\u6536\u4fe1\u53f7\u7684\u9891\u7387\u5149\u8c31\u975e\u5747\u5300\uff0c\u4e25\u91cd\u626d\u66f2\u76ee\u6807\u7684\u9ad8\u5206\u8fa8\u8ddd\u79bb\u591a\u666e\u52d2\uff08HRRP\uff09\u7279\u5f81\uff0c\u635f\u5bb3\u4f20\u7edf\u57fa\u4e8eHRRP\u7684\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u7f51\u7edc\uff0c\u5176\u6838\u5fc3\u662fCFA\u6a21\u5757\u3002\u8be5\u6a21\u5757\u76f4\u63a5\u5904\u7406\u63a5\u6536\u56de\u6ce2\u7684\u590d\u9891\u8c31\uff0c\u5b66\u4e60\u751f\u6210\u81ea\u9002\u5e94\u7684\u9891\u57df\u6ee4\u6ce2\u5668\uff0c\u4e3a\u53d7\u5f3a\u5e72\u6270\u7684\u9891\u6bb5\u5206\u914d\u8f83\u4f4e\u6743\u91cd\uff0c\u540c\u65f6\u4fdd\u7559\u5e72\u51c0\u9891\u6bb5\u7684\u5173\u952e\u76ee\u6807\u4fe1\u606f\u3002\u8fc7\u6ee4\u540e\u7684\u9891\u8c31\u968f\u540e\u88ab\u8f93\u5165\u5206\u7c7b\u5668\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u8bc6\u522b\u3002", "result": "\u5728\u6a21\u62df\u7684HRRP\u6570\u636e\u548c\u5404\u79cd\u5e72\u6270\u7ec4\u5408\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u5c24\u5176\u662f\u5728\u4e25\u91cd\u5e72\u6270\u6761\u4ef6\u4e0b\uff0c\u8be5\u6a21\u578b\u7684\u8bc6\u522b\u7cbe\u5ea6\u6bd4\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u9ad8\u51fa\u8fd19%\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e25\u5cfb\u7684\u5e72\u6270\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684\u96f7\u8fbe\u76ee\u6807\u8bc6\u522b\u6311\u6218\u3002"}}
{"id": "2511.11660", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11660", "abs": "https://arxiv.org/abs/2511.11660", "authors": ["Zizheng Guo", "Haichuan Liu", "Xizhe Shi", "Shenglu Hua", "Zuodong Zhang", "Chunyuan Zhao", "Runsheng Wang", "Yibo Lin"], "title": "HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support", "comment": "7 pages, 3 figures, to be published in ASP-DAC 2026", "summary": "We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.", "AI": {"tldr": "HeteroSTA\u662f\u4e00\u4e2aCPU-GPU\u5f02\u6784\u65f6\u5e8f\u5206\u6790\u5f15\u64ce\uff0c\u652f\u6301\u591a\u79cd\u7cbe\u5ea6-\u901f\u5ea6\u9009\u62e9\u7684\u5ef6\u8fdf\u8ba1\u7b97\u6a21\u578b\u3001\u884c\u4e1a\u6807\u51c6\u683c\u5f0f\uff08\u5305\u62ec.sdc\u7ea6\u675f\uff09\u4ee5\u53ca\u7aef\u5230\u7aef\u7684\u56fe\u548c\u8def\u5f84\u65f6\u5e8f\u67e5\u8be2GPU\u52a0\u901f\uff0c\u63d0\u4f9b\u96f6\u5f00\u9500\u7684API\u3002", "motivation": "\u4ecb\u7ecdHeteroSTA\uff0c\u9996\u4e2aCPU-GPU\u5f02\u6784\u65f6\u5e8f\u5206\u6790\u5f15\u64ce\u3002", "method": "HeteroSTA\u652f\u6301\u4e00\u5957\u5ef6\u8fdf\u8ba1\u7b97\u6a21\u578b\uff0c\u63d0\u4f9b\u7cbe\u5ea6-\u901f\u5ea6\u9009\u62e9\uff1b\u652f\u6301.sdc\u7ea6\u675f\uff1b\u5bf9\u56fe\u548c\u8def\u5f84\u65f6\u5e8f\u67e5\u8be2\u8fdb\u884c\u7aef\u5230\u7aefGPU\u52a0\u901f\uff0c\u901a\u8fc7\u96f6\u5f00\u9500\u7684API\u66b4\u9732\u3002", "result": "HeteroSTA\u5728\u4f5c\u4e3a\u72ec\u7acb\u5de5\u5177\u3001DREAMPlace 4.0\u96c6\u6210\u548c\u5168\u5c40\u8def\u7531\u96c6\u6210\u7b49\u7528\u4f8b\u4e2d\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u8fd0\u884c\u65f6\u52a0\u901f\u548c\u53ef\u6bd4\u7684\u8d28\u91cf\u3002", "conclusion": "HeteroSTA\u662f\u4e00\u4e2a\u9ad8\u6548\u7684CPU-GPU\u5f02\u6784\u65f6\u5e8f\u5206\u6790\u5f15\u64ce\uff0c\u5177\u6709\u591a\u79cd\u529f\u80fd\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.13676", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13676", "abs": "https://arxiv.org/abs/2511.13676", "authors": ["Hyunwoo Oh", "KyungIn Nam", "Rajat Bhattacharjya", "Hanning Chen", "Tamoghno Das", "Sanggeon Yun", "Suyeon Jang", "Andrew Ding", "Nikil Dutt", "Mohsen Imani"], "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization", "comment": "Accepted to DATE 2026", "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.", "AI": {"tldr": "T-SAR\u662f\u4e00\u4e2a\u5728CPU\u4e0a\u8fdb\u884c\u53ef\u6269\u5c55\u4e09\u5143LLM\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5bc4\u5b58\u5668\u6587\u4ef6\u4e2d\u52a8\u6001\u751f\u6210\u67e5\u627e\u8868\u6765\u514b\u670d\u5185\u5b58\u74f6\u9888\uff0c\u5b9e\u73b0\u9ad8\u6548\u7387\u548c\u4f4e\u529f\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\uff08\u4e3b\u8981\u4f7f\u7528CPU\uff09\u5728\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u80fd\u529b\u7684\u6311\u6218\u3002\u867d\u7136\u4e09\u5143\u91cf\u5316\u80fd\u8282\u7701\u8d44\u6e90\uff0c\u4f46\u73b0\u6709\u7684CPU\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u5185\u5b58\u67e5\u627e\u8868\uff08LUT\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u800cFPGA\u6216GPU\u5219\u4e0d\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "method": "T-SAR\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u5229\u7528SIMD\u5bc4\u5b58\u5668\u6587\u4ef6\u6765\u52a8\u6001\u751f\u6210\u7247\u5185LUT\uff0c\u65e0\u9700\u8fdb\u884c\u5927\u89c4\u6a21\u786c\u4ef6\u4fee\u6539\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u4e09\u5143LLM\u63a8\u7406\u3002", "result": "T-SAR\u6d88\u9664\u4e86\u5185\u5b58\u74f6\u9888\u5e76\u6700\u5927\u5316\u4e86\u6570\u636e\u7ea7\u5e76\u884c\u6027\uff0c\u5728GEMM\u5ef6\u8fdf\u65b9\u9762\u5b9e\u73b0\u4e865.6-24.5\u500d\u7684\u63d0\u5347\uff0c\u5728GEMV\u541e\u5410\u91cf\u65b9\u9762\u5b9e\u73b0\u4e861.1-86.2\u500d\u7684\u63d0\u5347\uff0c\u540c\u65f6\u5728SIMD\u5355\u5143\u4e0a\u4ec5\u589e\u52a0\u4e863.2%\u7684\u529f\u8017\u548c1.4%\u7684\u9762\u79ef\u5f00\u9500\u3002\u4e0eNVIDIA Jetson AGX Orin\u76f8\u6bd4\uff0cT-SAR\u7684\u80fd\u6548\u6700\u9ad8\u53ef\u63d0\u9ad82.5-4.9\u500d\u3002", "conclusion": "T-SAR\u4e3a\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728CPU\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u4e09\u5143LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u529f\u8017\u3002"}}
{"id": "2511.12484", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12484", "abs": "https://arxiv.org/abs/2511.12484", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Qi Wang", "Yue Yang", "Wenchuan Wu"], "title": "One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing", "comment": null, "summary": "With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADN-Agent\u7684\u67b6\u6784\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u534f\u8c03\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08DSM\uff09\uff0c\u4ee5\u89e3\u51b3\u4e3b\u52a8\u914d\u7535\u7f51\uff08ADN\uff09\u590d\u6742\u7684\u591a\u573a\u666f\u3001\u591a\u76ee\u6807\u8fd0\u884c\u95ee\u9898\u3002\u8be5\u67b6\u6784\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u610f\u56fe\u3001\u5206\u89e3\u4efb\u52a1\u548c\u8c03\u7528DSM\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u901a\u4fe1\u673a\u5236\u7edf\u4e00\u4e0d\u540cDSM\u7684\u63a5\u53e3\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u8bad\u7ec3\u6d41\u7a0b\u6765\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u89e3\u51b3\u8bed\u8a00\u5bc6\u96c6\u578b\u5b50\u4efb\u52a1\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660eADN-Agent\u4f18\u4e8e\u73b0\u6709\u7684LLM\u5e94\u7528\u8303\u5f0f\u3002", "motivation": "\u4e3b\u52a8\u914d\u7535\u7f51\uff08ADN\uff09\u7684\u8fd0\u884c\u65e5\u76ca\u590d\u6742\uff0c\u6d89\u53ca\u6d77\u91cf\u5206\u5e03\u5f0f\u80fd\u6e90\u548c\u65b0\u578b\u5e02\u573a\u5b9e\u4f53\uff0c\u5bfc\u81f4\u5176\u8fd0\u884c\u6210\u4e3a\u4e00\u4e2a\u591a\u573a\u666f\u3001\u591a\u76ee\u6807\u95ee\u9898\u3002\u5c3d\u7ba1\u5b58\u5728\u8bb8\u591a\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08DSM\uff09\u6765\u89e3\u51b3\u5177\u4f53\u6280\u672f\u95ee\u9898\uff0c\u4f46\u5bf9\u8fd9\u4e9b\u5f02\u6784DSM\u7684\u638c\u63e1\u3001\u96c6\u6210\u548c\u534f\u8c03\u7ed9ADN\u8fd0\u8425\u5546\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8d1f\u62c5\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00DSM\u5e76\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\u7684\u667a\u80fd\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADN-Agent\u7684\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u901a\u7528\u7684LLM\u6765\u534f\u8c03\u591a\u4e2aDSM\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u610f\u56fe\u8bc6\u522b\u3001\u4efb\u52a1\u5206\u89e3\u548cDSM\u8c03\u7528\u3002\u5728ADN-Agent\u5185\u90e8\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u4fe1\u673a\u5236\uff0c\u4e3a\u5404\u79cd\u5f02\u6784DSM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u63a5\u53e3\u3002\u6700\u540e\uff0c\u9488\u5bf9\u4e00\u4e9b\u8bed\u8a00\u5bc6\u96c6\u578b\u7684\u5b50\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u8bad\u7ec3\u6d41\u7a0b\u6765\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u7684\u6574\u4f53\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u6bd4\u8f83\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660eADN-Agent\u67b6\u6784\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684LLM\u5e94\u7528\u8303\u5f0f\u3002", "conclusion": "ADN-Agent\u67b6\u6784\u901a\u8fc7\u5229\u7528LLM\u534f\u8c03\u5f02\u6784DSM\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9ADN\u590d\u6742\u7684\u591a\u573a\u666f\u3001\u591a\u76ee\u6807\u8fd0\u884c\u6311\u6218\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.11970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11970", "abs": "https://arxiv.org/abs/2511.11970", "authors": ["Sara Wickenhiser", "Lizzie Peiros", "Calvin Joyce", "Peter Gavrilrov", "Sujaan Mukherjee", "Syler Sylvester", "Junrong Zhou", "Mandy Cheung", "Jason Lim", "Florian Richter", "Michael C. Yip"], "title": "ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot", "comment": "8 pages, 9 figures, ICRA", "summary": "Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.", "AI": {"tldr": "ARCSnake V2\u662f\u4e00\u79cd\u4e24\u6816\u3001\u87ba\u65cb\u63a8\u8fdb\u7684\u86c7\u5f62\u673a\u5668\u4eba\uff0c\u9002\u7528\u4e8e\u9646\u5730\u3001\u6c99\u5730\u548c\u6c34\u4e0b\u73af\u5883\u7684\u63a2\u7d22\uff0c\u5177\u6709\u9ad8\u673a\u52a8\u6027\u548c\u5730\u5f62\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8f6e\u5f0f\u6216\u817f\u5f0f\u673a\u5668\u4eba\u96be\u4ee5\u5728\u6d1e\u7a74\u3001\u6d77\u6d0b\u548c\u884c\u661f\u8868\u9762\u7b49\u6781\u7aef\u73af\u5883\u7684\u591a\u53d8\u5730\u5f62\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u79fb\u52a8\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u673a\u5668\u4eba\u3002", "method": "\u63d0\u51faARCSnake V2\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u8d85\u5197\u4f59\u86c7\u5f62\u673a\u5668\u4eba\u9ad8\u673a\u52a8\u6027\u548c\u963f\u57fa\u7c73\u5fb7\u87ba\u65cb\u63a8\u8fdb\u6280\u672f\u7684\u5730\u5f62\u9002\u5e94\u6027\u673a\u5668\u4eba\u3002\u5176\u8bbe\u8ba1\u5305\u62ec\u6c34\u5bc6\u673a\u68b0\u7ed3\u6784\u3001\u4e32\u8054\u8fde\u63a5\u7684\u87ba\u65cb\u548c\u5173\u8282\u9a71\u52a8\u3001\u96c6\u6210\u6d6e\u529b\u63a7\u5236\u7cfb\u7edf\uff0c\u5e76\u53ef\u901a\u8fc7\u624b\u6301\u63a7\u5236\u5668\u8fdb\u884c\u9065\u64cd\u4f5c\u3002\u673a\u5668\u4eba\u652f\u6301\u87ba\u65cb\u3001\u8f6e\u5f0f\u548c\u4fa7\u6eda\u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u53ef\u5728\u6a21\u5f0f\u95f4\u5e73\u7a33\u5207\u6362\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ARCSnake V2\u5728\u6c34\u4e0b\u673a\u52a8\u6027\u3001\u901a\u4fe1\u9c81\u68d2\u6027\u548c\u529b\u53cd\u9988\u63a7\u5236\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "ARCSnake V2\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u591a\u57df\u73af\u5883\u4e0b\u7684\u63a2\u7d22\u3001\u641c\u7d22\u6551\u63f4\u548c\u73af\u5883\u76d1\u6d4b\u3002"}}
{"id": "2511.12379", "categories": ["quant-ph", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12379", "abs": "https://arxiv.org/abs/2511.12379", "authors": ["Jonas Stein", "Maximilian Zorn", "Leo S\u00fcnkel", "Thomas Gabor"], "title": "Quantum Optimization Algorithms", "comment": "Preprint submitted to appear in a Springer Nature Book on Combinatorial Optimization using Quantum Computing", "summary": "Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.", "AI": {"tldr": "QAOA\u662f\u4e00\u79cd\u91cf\u5b50\u4f18\u5316\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u95e8\u63a7\u91cf\u5b50\u8ba1\u7b97\u673a\uff0c\u53ef\u89e3\u51b3\u5de5\u4e1a\u754c\u95ee\u9898\u3002\u672c\u6587\u8ba8\u8bba\u4e86QAOA\u7684\u5b9e\u73b0\u3001\u53c2\u6570\u8bad\u7ec3\u548c\u7ea6\u675f\u5904\u7406\uff0c\u5e76\u6982\u8ff0\u4e86VQE\u4f5c\u4e3a\u5176\u63a8\u5e7f\u3002", "motivation": "\u91cf\u5b50\u4f18\u5316\u5728\u89e3\u51b3\u67d0\u4e9b\u5de5\u4e1a\u95ee\u9898\u4e0a\u80fd\u63d0\u4f9b\u6307\u6570\u7ea7\u7684\u91cf\u5b50\u52a0\u901f\uff0cQAOA\u662f\u5176\u4e2d\u7684\u5173\u952e\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86QAOA\u7684\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\uff0c\u5305\u62ec\u9ad8\u9636\u4f0a\u8f9b\u6a21\u578b\u7684\u54c8\u5bc6\u987f\u6a21\u62df\u548c\u53c2\u6570\u8fc1\u79fb\u89c4\u5219\uff0c\u5e76\u901a\u8fc7Pennylane\u7684\u6e90\u4ee3\u7801\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6700\u5927\u5272\u95ee\u9898\u7684\u793a\u4f8b\u3002", "result": "\u901a\u8fc7Pennylane\u5b9e\u73b0\u6700\u5927\u5272\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528Grover\u6df7\u5408\u5668\u5c06\u7ea6\u675f\u6761\u4ef6\u7eb3\u5165QAOA\uff0c\u4ee5\u9650\u5236\u641c\u7d22\u7a7a\u95f4\u3002", "conclusion": "QAOA\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u91cf\u5b50\u4f18\u5316\u7b97\u6cd5\uff0cVQE\u662f\u5176\u63a8\u5e7f\uff0c\u4e24\u8005\u5728NISQ\u65f6\u4ee3\u90fd\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u8bad\u7ec3\u548c\u7ebf\u8def\u8bbe\u8ba1\u7b49\u6311\u6218\u3002"}}
{"id": "2511.12754", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12754", "abs": "https://arxiv.org/abs/2511.12754", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies", "comment": "Accepted to NeurIPS 2025", "summary": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6761\u4ef6\u5408\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u5b66\u4e60\u3001\u5206\u7c7b\u548c\u9002\u5e94\u5f02\u6784\u56e2\u961f\u6210\u5458\u7684\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\uff0c\u7279\u522b\u662f\u9762\u4e34\u65f6\u95f4\u538b\u529b\u548c\u590d\u6742\u7b56\u7565\u7a7a\u95f4\u7684\u4efb\u52a1\u65f6\uff0c\u8ba9AI\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u4eba\u7c7b\u4f19\u4f34\u72ec\u7279\u7684\u3001\u52a8\u6001\u53d8\u5316\u7684\u504f\u597d\u548c\u7b56\u7565\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6765\u5b66\u4e60\u6f5c\u5728\u7b56\u7565\u7a7a\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u7684\u7b56\u7565\u7c7b\u578b\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u6761\u4ef6\u5408\u4f5c\u8005\u3002\u5bf9\u4e8e\u5728\u7ebf\u9002\u5e94\uff0c\u91c7\u7528\u4e86\u56fa\u5b9a\u5171\u4eab\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u6765\u52a8\u6001\u63a8\u65ad\u548c\u8c03\u6574\u4f19\u4f34\u7b56\u7565\u4f30\u8ba1\u3002", "result": "\u5728\u4fee\u6539\u540e\u7684Overcooked\u57df\uff08\u4e00\u4e2a\u590d\u6742\u7684\u591a\u4eba\u534f\u4f5c\u70f9\u996a\u73af\u5883\uff09\u4e2d\u7684\u5b9e\u9a8c\u4ee5\u53ca\u5728\u7ebf\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0e\u65b0\u9896\u7684\u4eba\u7c7b\u6216AI\u961f\u53cb\u914d\u5bf9\u65f6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6761\u4ef6\u5408\u4f5c\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u3001\u5206\u7c7b\u548c\u9002\u5e94\u6f5c\u5728\u7684\u4f19\u4f34\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u4e2d\u4eba\u673a\u4ea4\u4e92\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0e\u65b0\u961f\u53cb\u7684\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.13506", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13506", "abs": "https://arxiv.org/abs/2511.13506", "authors": ["Ida C. Skogvoll", "Benjamin A. D. Williamson", "Sverre M. Selbach"], "title": "Local indirect magnetoelectric coupling at twin walls in CaMnO$_3$", "comment": null, "summary": "Ferroelastic twin walls in centrosymmetric perovskites can host emergent polar and magnetic properties forbidden in the bulk. We use density functional theory calculations to study the geometry and magnetic properties of ferroelastic domain walls in orthorhombic CaMnO$_3$, which belongs to the most common perovskite space group, $Pnma$. At the wall, the inherent inversion symmetry-breaking induces local polar distortions dependent on the wall geometry, which couple to the magnetic order through the octahedral distortions. Noncollinear calculations reveal enhanced out-of-plane magnetic moments on the Mn atoms and a local, finite magnetization confined to the wall. Strain fields across twin walls thus give rise to coexistence of polarization and magnetization as well as magnetoelectric response that is absent and symmetry-forbidden in bulk CaMnO$_3$. We propose that magnetoelectric coupling and coexisting polarization and magnetization can emerge at twin walls in bulk centrosymmetric antiferromagnets.", "AI": {"tldr": "Ferroelastic twin walls in centrosymmetric perovskites like CaMnO3 can exhibit polar and magnetic properties not found in the bulk material, driven by symmetry breaking and octahedral distortions at the walls, leading to emergent magnetoelectric coupling.", "motivation": "The motivation is to investigate the emergence of polar and magnetic properties at ferroelastic twin walls in centrosymmetric perovskites, specifically CaMnO3, which are symmetry-forbidden in the bulk.", "method": "Density functional theory (DFT) calculations were used to study the geometry and magnetic properties of ferroelastic domain walls in orthorhombic CaMnO3. Noncollinear calculations were employed to reveal magnetic moments.", "result": "The calculations showed that at the twin walls, inversion symmetry breaking induces local polar distortions coupled to magnetic order through octahedral distortions. Enhanced out-of-plane magnetic moments and a local, finite magnetization confined to the wall were observed. This leads to the coexistence of polarization and magnetization and a magnetoelectric response absent in bulk CaMnO3.", "conclusion": "Strain fields across twin walls in centrosymmetric antiferromagnets can lead to emergent magnetoelectric coupling and the coexistence of polarization and magnetization, even when these phenomena are symmetry-forbidden in the bulk material."}}
{"id": "2511.11851", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11851", "abs": "https://arxiv.org/abs/2511.11851", "authors": ["Wei-Jia Chen", "Min-Yen Tsai", "Cheng-Yi Lee", "Chia-Mu Yu"], "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection", "comment": "10 pages, under review", "summary": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.", "AI": {"tldr": "\u6a21\u578b\u5408\u5e76\u5b58\u5728\u77e5\u8bc6\u4ea7\u6743\u548c\u6240\u6709\u6743\u98ce\u9669\uff0cMergeGuard\u6846\u67b6\u901a\u8fc7\u7834\u574f\u5408\u5e76\u517c\u5bb9\u6027\u6765\u4fdd\u62a4\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u5b9e\u8df5\u5b58\u5728\u672a\u7ecf\u6388\u6743\u7684\u5408\u5e76\u95ee\u9898\uff0c\u4fb5\u72af\u77e5\u8bc6\u4ea7\u6743\uff0c\u635f\u5bb3\u6a21\u578b\u6240\u6709\u6743\u548c\u95ee\u8d23\u5236\u3002", "method": "MergeGuard\u662f\u4e00\u4e2a\u53cc\u9636\u6bb5\u4fdd\u62a4\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7L2\u6b63\u5219\u5316\u4f18\u5316\u91cd\u65b0\u5206\u914d\u5c42\u95f4\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u5206\u6563\u68af\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6ce8\u5165\u7ed3\u6784\u5316\u6270\u52a8\uff0c\u9519\u5f00\u4efb\u52a1\u5b50\u7a7a\u95f4\uff0c\u7834\u574f\u635f\u5931\u666f\u89c2\u7684\u66f2\u7387\u517c\u5bb9\u6027\u3002", "result": "MergeGuard\u6210\u529f\u5c06\u5408\u5e76\u6a21\u578b\u7684\u51c6\u786e\u7387\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u4fdd\u62a4\u540e\u7684\u6a21\u578b\u6027\u80fd\u635f\u5931\u5c0f\u4e8e1.5%\u3002", "conclusion": "MergeGuard\u80fd\u591f\u6709\u6548\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5bf9\u539f\u59cb\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2511.12540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12540", "abs": "https://arxiv.org/abs/2511.12540", "authors": ["Dimitris Antoniadis", "Timothy G. Constandinou"], "title": "A mixed-signal analogue front-end for brain-implantable neural interfaces using a digital fixed-point IIR filter and bulk offset cancellation", "comment": "4 pages plus 1 references IEEE conference style", "summary": "Advances in miniaturised implantable neural electronics have paved the way for therapeutic brain-computer interfaces with clinical potential for movement disorders, epilepsy, and broader neurological applications. This paper presents a mixed-signal analogue front end (AFE) designed to record both extracellular action potentials (EAPs) and local field potentials (LFPs). The feedforward path integrates a low-noise amplifier (LNA) and a successive-approximation-register (SAR) analogue-to-digital converter (ADC), while the feedback path employs a fixed-point infinite-impulse-response (IIR) Chebyshev Type II low-pass filter to suppress sub-mHz components via bulk-voltage control of the LNA input differential pair using two R-2R pseudo-resistor digital-to-analogue converters (DACs). The proposed AFE achieves up to 41.42dB gain, consumes 2.178uA per channel, occupies 0.198mm2 per channel, and supports neural signal monitoring from 0.1Hz to 10kHz with 3.59uVrms input-referred integrated noise.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u795e\u7ecf\u63a5\u53e3\u7684\u6df7\u5408\u4fe1\u53f7\u524d\u7aef\uff08AFE\uff09\uff0c\u80fd\u591f\u540c\u65f6\u8bb0\u5f55\u80de\u5916\u52a8\u4f5c\u7535\u4f4d\uff08EAPs\uff09\u548c\u5c40\u90e8\u573a\u7535\u4f4d\uff08LFPs\uff09\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f4e\u529f\u8017\u3001\u5c0f\u9762\u79ef\u548c\u5bbd\u9891\u5e26\u7684\u795e\u7ecf\u4fe1\u53f7\u76d1\u6d4b\u3002", "motivation": "\u63a8\u52a8\u7528\u4e8e\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u6cbb\u7597\u7684\u8111\u673a\u63a5\u53e3\u6280\u672f\u53d1\u5c55\uff0c\u5b9e\u73b0\u5bf9\u795e\u7ecf\u4fe1\u53f7\u7684\u7cbe\u786e\u8bb0\u5f55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6df7\u5408\u4fe1\u53f7\u6a21\u62df\u524d\u7aef\uff08AFE\uff09\uff0c\u5305\u62ec\u4f4e\u566a\u58f0\u653e\u5927\u5668\uff08LNA\uff09\u548c\u9010\u6b21\u903c\u8fd1\u5bc4\u5b58\u5668\uff08SAR\uff09\u6a21\u6570\u8f6c\u6362\u5668\uff08ADC\uff09\uff0c\u5e76\u7ed3\u5408\u65e0\u9650\u8109\u51b2\u54cd\u5e94\uff08IIR\uff09\u5207\u6bd4\u96ea\u592bII\u578b\u4f4e\u901a\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u4f4e\u9891\u6210\u5206\u3002", "result": "AFE\u5b9e\u73b0\u4e8641.42dB\u7684\u589e\u76ca\uff0c\u6bcf\u901a\u9053\u529f\u8017\u4e3a2.178uA\uff0c\u9762\u79ef\u4e3a0.198mm2\uff0c\u652f\u63010.1Hz\u523010kHz\u7684\u795e\u7ecf\u4fe1\u53f7\u76d1\u6d4b\uff0c\u8f93\u5165\u53c2\u8003\u566a\u58f0\u4e3a3.59uVrms\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AFE\u5728\u529f\u8017\u3001\u9762\u79ef\u548c\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5e73\u8861\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u4e34\u5e8a\u8111\u673a\u63a5\u53e3\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11664", "categories": ["cs.DC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11664", "abs": "https://arxiv.org/abs/2511.11664", "authors": ["Mingyu Sung", "Suhwan Im", "Vikas Palakonda", "Jae-Mo Kang"], "title": "Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks", "comment": null, "summary": "Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8erANS\u7f16\u7801\u3001\u975e\u5bf9\u79f0\u6574\u6570\u91cf\u5316\u548c\u7a00\u758f\u5f20\u91cf\u8868\u793a\u7684\u8f7b\u91cf\u7ea7\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5272\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u538b\u7f29\u7387\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u5206\u5272\u8ba1\u7b97\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u4e91\u670d\u52a1\u5668\u4e4b\u95f4\u5206\u914d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u4f46\u4e2d\u95f4\u7279\u5f81\u4f20\u8f93\u9762\u4e34\u901a\u4fe1\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u975e\u5bf9\u79f0\u6574\u6570\u91cf\u5316\u548c\u7a00\u758f\u5f20\u91cf\u8868\u793a\u7684\u8f7b\u91cf\u7ea7\u538b\u7f29\u6846\u67b6\uff0c\u5229\u7528rANS\u7f16\u7801\u6280\u672f\u3002", "result": "\u8be5\u6846\u67b6\u5728ResNet\u3001VGG16\u7b49\u591a\u79cd\u6a21\u578b\u548cCIFAR100\u3001ImageNet\u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u57fa\u7ebf\u7684\u51c6\u786e\u7387\u3002\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08Llama2 7B/13B\uff09\u4e0a\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u4f4e\u4e8e1\u6beb\u79d2\u7684\u7f16\u7801/\u89e3\u7801\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5272\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u5f00\u9500\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.13679", "categories": ["cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13679", "abs": "https://arxiv.org/abs/2511.13679", "authors": ["Hyunwoo Oh", "Hanning Chen", "Sanggeon Yun", "Yang Ni", "Wenjun Huang", "Tamoghno Das", "Suyeon Jang", "Mohsen Imani"], "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention", "comment": "Accepted to DATE 2026", "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.", "AI": {"tldr": "Deformable transformers are powerful for detection but inefficient on hardware. QUILL is a new accelerator that makes deformable attention more hardware-friendly by optimizing memory access and computation, achieving significant speedups and energy efficiency improvements.", "motivation": "Deformable transformers achieve state-of-the-art results in object detection but suffer from poor hardware performance due to irregular memory access and low arithmetic intensity.", "method": "The paper introduces QUILL, a schedule-aware accelerator that optimizes deformable attention for hardware. It uses a core technique called Distance-based Out-of-Order Querying (DOOQ) to order queries by spatial proximity and prefetch data into an alternate buffer. This creates a prefetch loop that overlaps memory access and computation. A fused MSDeformAttn engine performs interpolation, Softmax, aggregation, and projection in a single pass, keeping small tensors on-chip and running dense layers on integrated GEMMs.", "result": "Quill achieves up to 7.29x higher throughput and 47.3x better energy efficiency compared to an RTX 4090. It also outperforms prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy remains within 0.9 AP of FP32 performance across different Deformable and Sparse DETR variants.", "conclusion": "Quill effectively converts sparsity into locality, and locality into utilization, resulting in consistent end-to-end speedups for deformable transformers. The accelerator significantly improves hardware efficiency for these models."}}
{"id": "2511.12492", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12492", "abs": "https://arxiv.org/abs/2511.12492", "authors": ["Sungjun Seo", "Kooktae Lee"], "title": "Density-Driven Multi-Agent Coordination for Efficient Farm Coverage and Management in Smart Agriculture", "comment": "Author Accepted Manuscript (AAM) of a paper accepted for publication in the IEEE Transactions on Control Systems Technology (TCST)", "summary": "The growing scale of modern farms has increased the need for efficient and adaptive multi-agent coverage strategies for pest, weed, and disease management. Traditional methods such as manual inspection and blanket pesticide spraying often lead to excessive chemical use, resource waste, and environmental impact. While unmanned aerial vehicles (UAVs) offer a promising platform for precision agriculture through targeted spraying and improved operational efficiency, existing UAV-based approaches remain limited by battery life, payload capacity, and scalability, especially in large fields where single-UAV or uniformly distributed spraying is insufficient. Although multi-UAV coordination has been explored, many current frameworks still assume uniform spraying and do not account for infestation severity, UAV dynamics, non-uniform resource allocation, or energy-efficient coordination.\n  To address these limitations, this paper proposes a Density-Driven Optimal Control (D2OC) framework that integrates Optimal Transport (OT) theory with multi-UAV coverage control for large-scale agricultural spraying. The method supports non-uniform, priority-aware resource allocation based on infestation intensity, reducing unnecessary chemical application. UAVs are modeled as a linear time-varying (LTV) system to capture variations in mass and inertia during spraying missions. The D2OC control law, derived using Lagrangian mechanics, enables efficient coordination, balanced workload distribution, and improved mission duration. Simulation results demonstrate that the proposed approach outperforms uniform spraying and Spectral Multiscale Coverage (SMC) in coverage efficiency, chemical reduction, and operational sustainability, providing a scalable solution for smart agriculture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5bc6\u5ea6\u9a71\u52a8\u6700\u4f18\u63a7\u5236\uff08D2OC\uff09\u6846\u67b6\uff0c\u5229\u7528\u6700\u4f18\u8f93\u8fd0\uff08OT\uff09\u7406\u8bba\u548c\u591a\u65e0\u4eba\u673a\u534f\u540c\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u519c\u4e1a\u55b7\u6d12\u7684\u667a\u80fd\u5316\u3001\u81ea\u9002\u5e94\u548c\u8282\u80fd\u3002", "motivation": "\u4f20\u7edf\u519c\u4e1a\u75c5\u866b\u8349\u5bb3\u7ba1\u7406\u65b9\u6cd5\uff08\u5982\u624b\u52a8\u68c0\u67e5\u548c\u519c\u836f blanket \u55b7\u6d12\uff09\u6548\u7387\u4f4e\u4e0b\u3001\u8d44\u6e90\u6d6a\u8d39\u4e14\u5bf9\u73af\u5883\u9020\u6210\u5f71\u54cd\u3002\u73b0\u6709\u65e0\u4eba\u673a\uff08UAV\uff09\u7cbe\u51c6\u519c\u4e1a\u65b9\u6cd5\u53d7\u7535\u6c60\u5bff\u547d\u3001\u8f7d\u8377\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5728\u5904\u7406\u5927\u89c4\u6a21\u519c\u7530\u548c\u8003\u8651\u5b9e\u9645\u4f5c\u4e1a\u56e0\u7d20\uff08\u5982\u4fb5\u67d3\u4e25\u91cd\u7a0b\u5ea6\u3001UAV \u52a8\u529b\u5b66\u3001\u8d44\u6e90\u5206\u914d\u548c\u8282\u80fd\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u4f18\u8f93\u8fd0\uff08OT\uff09\u7406\u8bba\u548c\u591a\u65e0\u4eba\u673a\u534f\u540c\u8986\u76d6\u63a7\u5236\u7684\u5bc6\u5ea6\u9a71\u52a8\u6700\u4f18\u63a7\u5236\uff08D2OC\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6839\u636e\u75c5\u866b\u5bb3\u5f3a\u5ea6\u8fdb\u884c\u975e\u5747\u5300\u3001\u6709\u4f18\u5148\u7ea7\u7684\u8d44\u6e90\u5206\u914d\uff0c\u5e76\u8003\u8651\u65e0\u4eba\u673a\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff08\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edfLTV\uff09\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u529b\u5b66\u63a8\u5bfc\u51fa D2OC \u63a7\u5236\u5f8b\uff0c\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u4efb\u52a1\u65f6\u957f\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5747\u5300\u55b7\u6d12\u548c\u5149\u8c31\u591a\u5c3a\u5ea6\u8986\u76d6\uff08SMC\uff09\u65b9\u6cd5\u76f8\u6bd4\uff0cD2OC \u6846\u67b6\u5728\u8986\u76d6\u6548\u7387\u3001\u5316\u5b66\u54c1\u51cf\u91cf\u548c\u4f5c\u4e1a\u53ef\u6301\u7eed\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "D2OC \u6846\u67b6\u4e3a\u667a\u80fd\u519c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5927\u89c4\u6a21\u519c\u7530\u7684\u75c5\u866b\u8349\u5bb3\u7ba1\u7406\u6311\u6218\u3002"}}
{"id": "2511.12022", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12022", "abs": "https://arxiv.org/abs/2511.12022", "authors": ["Anh-Quan Pham", "Kabir Ram Puri", "Shreyas Raorane"], "title": "SBAMP: Sampling Based Adaptive Motion Planning", "comment": "8 pages, 13 figures", "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.", "AI": {"tldr": "SBAMP \u7ed3\u5408 RRT* \u548c SEDS\uff0c\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u5b9e\u65f6\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u5168\u5c40\u6700\u4f18\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SBAMP \u7684\u65b0\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86 RRT* \u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u80fd\u529b\u548c SEDS \u7684\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u8fde\u7eed\u81ea\u9002\u5e94\u8f68\u8ff9\u8c03\u6574\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7 Lyaponuv \u4fdd\u8bc1\u7ef4\u6301\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c RoboRacer \u786c\u4ef6\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0cSBAMP \u5728\u52a8\u6001\u969c\u788d\u7269\u573a\u666f\u3001\u5feb\u901f\u6062\u590d\u548c\u6025\u8f6c\u5f2f\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "SBAMP \u6846\u67b6\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5168\u5c40\u8def\u5f84\u7684\u4f18\u5316\u6027\uff0c\u4e3a\u975e\u7ed3\u6784\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12403", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12403", "abs": "https://arxiv.org/abs/2511.12403", "authors": ["Mordecai Waegell", "Kelvin J. McQueen"], "title": "Nonlocal action in Everettian Quantum Mechanics", "comment": "19 pages, 1 figure", "summary": "According to a common view, Everettian quantum mechanics (EQM) is a local theory because it avoids nonlocal action at a distance, and this is an important point in EQM's favor. Unlike collapse theories, EQM does not allow an action on one system to change the reduced density matrix (RDM) of a remote entangled system - a clear case of nonlocal action. However, EQM does allow an action on one system to change the global state of the system and its remote entangled partners. We argue that such changes should also count as nonlocal actions, meaning EQM is not local after all. First, we consider an argument to the contrary, which deems such global changes to be mere extrinsic changes, whereas nonlocal action requires intrinsic changes to the remote system. We respond that the intrinsic-extrinsic distinction is problematic and cannot hold the weight of this argument. We then try to clarify when actions that change global states count as nonlocal actions. We argue that it is when the global states are essential explanatory mechanisms of the theory. In EQM, the global state is needed to explain why, in an anti-correlated Bell state, Alice's measuring spin-up ensures that she encounters only the branch where Bob measures spin-down.", "AI": {"tldr": "Everettian quantum mechanics (EQM) is often considered local because it doesn't involve nonlocal action at a distance like collapse theories. However, this paper argues that EQM *is* nonlocal because actions on one system can change the global state of entangled systems, which should count as a nonlocal action. The paper refutes the counterargument that such changes are merely extrinsic, arguing that the intrinsic-extrinsic distinction is flawed and that global states, when essential for explanations, signify nonlocal actions. In EQM, the global state is crucial for explaining how Alice's measurement outcome influences Bob's, demonstrating nonlocality.", "motivation": "The common view holds that Everettian quantum mechanics (EQM) is local, which is seen as an advantage. This paper challenges this view by arguing that EQM is, in fact, nonlocal.", "method": "The paper analyzes the concept of nonlocal action in EQM by examining how actions on one system can affect the global state of entangled systems. It refutes the intrinsic-extrinsic distinction as a means to deny nonlocality in EQM and proposes that global state changes constitute nonlocal actions when these states are essential explanatory mechanisms within the theory.", "result": "The paper argues that EQM is not local. It demonstrates that actions on one part of an entangled system can change the global state, which should be considered a nonlocal action. This is exemplified by how the global state in EQM explains the correlation between Alice's and Bob's measurement outcomes in an anti-correlated Bell state.", "conclusion": "Everettian quantum mechanics (EQM) should be considered a nonlocal theory because changes to the global state of entangled systems, even if not affecting individual remote systems intrinsically, constitute nonlocal actions when these global states are essential for the theory's explanations. The distinction between intrinsic and extrinsic changes is problematic and cannot be used to preserve the locality of EQM."}}
{"id": "2511.12869", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12869", "abs": "https://arxiv.org/abs/2511.12869", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zeeshan Memon", "Muhammad Ibtsaam Qadir", "Sagnik Bhattacharya", "Hassan Rizwan", "Abhiram R. Gorle", "Maahe Zehra Kazmi", "Ayesha Mohsin", "Muhammad Usman Rafique", "Zihao He", "Pulkit Mehta", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "On the Fundamental Limits of LLMs at Scale", "comment": "Submitted to TMLR 2025", "summary": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.", "AI": {"tldr": "LLMs\u56e0\u89c4\u6a21\u6269\u5927\u800c\u53d7\u76ca\u532a\u6d45\uff0c\u4f46\u9762\u4e34\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u63a8\u7406\u4e0b\u964d\u3001\u68c0\u7d22\u8106\u5f31\u548c\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\u7b49\u57fa\u672c\u9650\u5236\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u8bc1\u660e\u7684\u6846\u67b6\uff0c\u5c06\u8fd9\u4e9b\u9650\u5236\u4e0e\u8ba1\u7b97\u3001\u4fe1\u606f\u548c\u5b66\u4e60\u7684\u57fa\u7840\u7406\u8bba\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u7684\u8c03\u67e5\u4e3b\u8981\u57fa\u4e8e\u7ecf\u9a8c\uff0c\u7f3a\u4e4f\u5bf9LLM\u89c4\u6a21\u5316\u6536\u76ca\u57fa\u672c\u9650\u5236\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u8bc1\u660e\u7684\u6846\u67b6\uff0c\u5c06LLM\u89c4\u6a21\u5316\u7684\u7406\u8bba\u4e0a\u9650\u5f62\u5f0f\u5316\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u8ba1\u7b97\u7406\u8bba\u3001\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u5b66\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002\u5b83\u8fd0\u7528\u4e86\u5bf9\u89d2\u7ebf\u5316\u3001\u53ef\u8ba1\u7b97\u6027\u548c\u4e0d\u53ef\u8ba1\u7b97\u6027\u3001\u4fe1\u606f\u8bba\u7ea6\u675f\u3001\u51e0\u4f55\u6548\u5e94\u548c\u8ba1\u7b97\u6548\u5e94\u6765\u5206\u6790LLM\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u7406\u8bba\u548c\u7ecf\u9a8c\u8bc1\u636e\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u6982\u8ff0\u4e86\u89c4\u6a21\u5316 LLM \u7684\u8fdb\u5c55\u3001\u9971\u548c\u70b9\u548c\u74f6\u9888\u3002", "result": "\u672c\u7814\u7a76\u63ed\u793a\u4e86LLM\u89c4\u6a21\u5316\u9762\u4e34\u7684\u4e94\u5927\u57fa\u672c\u9650\u5236\uff1a\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u63a8\u7406\u4e0b\u964d\u3001\u68c0\u7d22\u8106\u5f31\u548c\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u89d2\u7ebf\u5316\u4fdd\u8bc1\u4e86\u603b\u4f1a\u5b58\u5728\u67d0\u4e9b\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u7684\u8f93\u5165\uff0c\u4e0d\u53ef\u5224\u5b9a\u67e5\u8be2\u4f1a\u5bfc\u81f4\u6240\u6709\u53ef\u8ba1\u7b97\u9884\u6d4b\u5668\u51fa\u73b0\u65e0\u9650\u7684\u5931\u8d25\u96c6\u3002\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u7ea6\u675f\u9650\u5236\u4e86\u53ef\u5224\u5b9a\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u6709\u9650\u7684\u63cf\u8ff0\u957f\u5ea6\u5bfc\u81f4\u538b\u7f29\u9519\u8bef\uff0c\u957f\u5c3e\u4e8b\u5b9e\u77e5\u8bc6\u9700\u8981\u6781\u9ad8\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u51e0\u4f55\u548c\u8ba1\u7b97\u6548\u5e94\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u7684\u538b\u7f29\uff0c\u57fa\u4e8e\u4f3c\u7136\u7684\u8bad\u7ec3\u504f\u5411\u4e8e\u6a21\u5f0f\u5b8c\u6210\u800c\u975e\u63a8\u7406\uff0c\u68c0\u7d22\u5728\u6709\u9650\u7684token\u4e0b\u4f1a\u53d7\u5230\u8bed\u4e49\u6f02\u79fb\u548c\u8026\u5408\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u591a\u6a21\u6001\u89c4\u6a21\u5316\u7ee7\u627f\u4e86\u6d45\u5c42\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u7406\u8bba\u5206\u6790\u548c\u7ecf\u9a8c\u8bc1\u636e\u90fd\u8868\u660e\u4e86\u89c4\u6a21\u5316\u4f5c\u7528\u7684\u8fb9\u754c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u8bc1\u660e\u7684\u6846\u67b6\uff0c\u4e3aLLM\u89c4\u6a21\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u5982\u8fb9\u754c\u9884\u8a00\u68c0\u7d22\u3001\u4f4d\u7f6e\u8bfe\u7a0b\u548c\u7a00\u758f\u6216\u5206\u5c42\u6ce8\u610f\u529b\u3002"}}
{"id": "2511.13556", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13556", "abs": "https://arxiv.org/abs/2511.13556", "authors": ["Soumendra Kumar Das", "Smruti Ranjan Parida", "Prasanjit Samal", "Brahmananda Chakraborty", "Sridhar Sahu"], "title": "First Principles study of Photocatalytic Water Splitting in BO Monolayer: Effect of Strain and Surface Functionalization", "comment": null, "summary": "Light element based two dimensional (2D) materials are promising photocatalysts for hydrogen production via water splitting. Boron oxide (BO) is a recently synthesized 2D monolayer which has yet to be thoroughly explored for its potential applications. In this article, using first principles calculations, we report, for the first time, the visible-light photocatalytic activity of a BO monolayer for water splitting under mechanical strain and surface modification with single- and double-atom decorations (C, N, Si, Ge, P, As). The pristine BO monolayer exhibits an indirect band gap of 3.8 eV with band edges spanning the water redox potentials, but its optical absorption lies in the UV region (~ 4.5 eV). Strain engineering tunes the band gap and band alignment with a minimal shifting in the optical absorption (~0.5 eV). Single atom decoration produces a metallic state for elements like N, P, As, and an insulating state for single C, Si, Ge with a partial shifting in optical absorption. In contrast, double atom decoration produces substantial band gap reduction, improved band alignment, a pronounced red-shift in optical absorption into the visible range (1.6 to 3.2 eV) thus satisfying the criteria for water splitting. The stability of all the adsorbed configurations was confirmed by negative formation energy and ab-initio molecular dynamics simulations. These findings suggest BO monolayer functionalization can improve photocatalytic efficiency, providing hydrogen generation insights.", "AI": {"tldr": "BO\u5355\u5c42\u6750\u6599\u662f\u5149\u50ac\u5316\u5206\u89e3\u6c34\u7684\u6709\u524d\u9014\u7684\u6750\u6599\uff0c\u4f46\u5176\u5438\u6536\u5728\u7d2b\u5916\u533a\u57df\u3002\u901a\u8fc7\u5e94\u53d8\u5de5\u7a0b\u548c\u5355/\u53cc\u539f\u5b50\u4fee\u9970\uff0c\u53ef\u4ee5\u5c06\u5176\u5149\u5438\u6536\u7ea2\u79fb\u5230\u53ef\u89c1\u5149\u533a\u57df\uff0c\u4ece\u800c\u63d0\u9ad8\u5149\u50ac\u5316\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u65b0\u578b\u4e8c\u7ef4\u6750\u6599BO\u5355\u5c42\u5728\u5149\u50ac\u5316\u5206\u89e3\u6c34\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7814\u7a76\u4e86\u5e94\u53d8\u548c\u8868\u9762\u4fee\u9970\uff08\u5355\u539f\u5b50\u548c\u53cc\u539f\u5b50C, N, Si, Ge, P, As\uff09\u5bf9BO\u5355\u5c42\u5149\u50ac\u5316\u6d3b\u6027\u7684\u5f71\u54cd\u3002", "result": "\u539f\u59cbBO\u5355\u5c42\u5177\u67093.8 eV\u7684\u95f4\u63a5\u5e26\u9699\uff0c\u4f46\u5149\u5438\u6536\u5728\u7d2b\u5916\u533a\u57df\uff08~4.5 eV\uff09\u3002\u5e94\u53d8\u5de5\u7a0b\u53ef\u8c03\u8282\u5e26\u9699\u548c\u5e26\u5bf9\u9f50\uff0c\u4f46\u5bf9\u5149\u5438\u6536\u5f71\u54cd\u5f88\u5c0f\u3002\u5355\u539f\u5b50\u4fee\u9970\u53ef\u80fd\u5bfc\u81f4\u91d1\u5c5e\u6027\u6216\u7edd\u7f18\u6027\uff0c\u5e76\u90e8\u5206\u6539\u53d8\u5149\u5438\u6536\u3002\u53cc\u539f\u5b50\u4fee\u9970\u53ef\u663e\u8457\u964d\u4f4e\u5e26\u9699\uff0c\u6539\u5584\u5e26\u5bf9\u9f50\uff0c\u5e76\u5c06\u5149\u5438\u6536\u7ea2\u79fb\u5230\u53ef\u89c1\u5149\u533a\u57df\uff081.6\u81f33.2 eV\uff09\u3002\u6240\u6709\u4fee\u9970\u914d\u7f6e\u5747\u5177\u6709\u8d1f\u5f62\u6210\u80fd\u548c\u826f\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "BO\u5355\u5c42\u7684\u8868\u9762\u529f\u80fd\u5316\uff08\u7279\u522b\u662f\u53cc\u539f\u5b50\u4fee\u9970\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u5149\u50ac\u5316\u6d3b\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5229\u7528\u53ef\u89c1\u5149\u8fdb\u884c\u6c34\u5206\u89e3\u4ea7\u6c22\u3002"}}
{"id": "2511.11864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11864", "abs": "https://arxiv.org/abs/2511.11864", "authors": ["Muzammal Shafique", "Nasir Rahim", "Jamil Ahmad", "Mohammad Siadat", "Khalid Malik", "Ghaus Malik"], "title": "FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision", "comment": null, "summary": "Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.", "AI": {"tldr": "FocusSDF\u662f\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5173\u6ce8\u8fb9\u754c\u533a\u57df\u6765\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u660e\u786e\u7f16\u7801\u8fb9\u754c\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8fb9\u754c\u4fdd\u7559\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u51c6\u786e\u7684\u5206\u5272\u5bf9\u4e8e\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570FocusSDF\u3002\u8be5\u51fd\u6570\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u4e3a\u9760\u8fd1\u75c5\u53d8\u6216\u5668\u5b98\u8fb9\u754c\u7684\u50cf\u7d20\u5206\u914d\u66f4\u9ad8\u7684\u6743\u91cd\uff0c\u5f15\u5bfc\u7f51\u7edc\u5173\u6ce8\u8fb9\u754c\u533a\u57df\uff0c\u4ece\u800c\u5b9e\u73b0\u8fb9\u754c\u611f\u77e5\u3002", "result": "\u5728\u5305\u62ecMedSAM\u5728\u5185\u7684\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002\u4f7f\u7528\u56db\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u6db5\u76d6\u4e86\u8111\u52a8\u8109\u7624\u3001\u4e2d\u98ce\u3001\u809d\u810f\u548c\u4e73\u817a\u80bf\u7624\u5206\u5272\u7b49\u591a\u79cd\u4efb\u52a1\u548c\u6210\u50cf\u6a21\u6001\u3002\u7ed3\u679c\u4e00\u81f4\u8868\u660e\uff0cFocusSDF\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8ddd\u79bb\u53d8\u6362\u7684\u635f\u5931\u51fd\u6570\u3002", "conclusion": "FocusSDF\u901a\u8fc7\u5c06\u7126\u70b9\u653e\u5728\u8fb9\u754c\u533a\u57df\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u4fdd\u6301\u95ee\u9898\uff0c\u5e76\u5728\u5e7f\u6cdb\u7684\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12733", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12733", "abs": "https://arxiv.org/abs/2511.12733", "authors": ["Ahmed Hussain", "Ahmed Sultan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Near Field Tapering with Slepian Window: Balancing the Range Angle Sidelobe Trade off", "comment": null, "summary": "Near-field beamforming enables target discrimination in both range (axial) and angle (lateral) dimensions. Elevated sidelobes along either dimension, however, increase susceptibility to interference and degrade detection performance. Conventional amplitude tapering techniques, designed for far-field scenarios, cannot simultaneously suppress axial and lateral sidelobes in near-field. In this letter, we propose a Slepian-based amplitude tapering approach that maximizes mainlobe energy concentration, achieving significant sidelobe reduction in both dimensions. Numerical results show that the proposed taper improves peak sidelobe suppression by approximately 24 dB in the lateral domain and 10 dB in the axial domain compared to a conventional uniform window.", "AI": {"tldr": "Slepian-based amplitude tapering effectively reduces near-field sidelobes in both range and angle, outperforming conventional methods.", "motivation": "Conventional amplitude tapering is insufficient for simultaneous near-field axial and lateral sidelobe suppression, leading to interference susceptibility and degraded detection. This paper aims to address this limitation.", "method": "A Slepian-based amplitude tapering approach is proposed to maximize mainlobe energy concentration, thereby reducing sidelobes in both axial and lateral dimensions.", "result": "The proposed taper achieves approximately 24 dB of peak sidelobe suppression in the lateral domain and 10 dB in the axial domain compared to a uniform window.", "conclusion": "The Slepian-based amplitude tapering approach is an effective method for simultaneously suppressing near-field sidelobes in both range and angle, significantly improving detection performance over conventional techniques."}}
{"id": "2511.11672", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11672", "abs": "https://arxiv.org/abs/2511.11672", "authors": ["Zengyi Qin", "Jinyuan Chen", "Yunze Man", "Shengcao Cao", "Ziqi Pang", "Zhuoyuan Wang", "Xin Sun", "Gen Lin", "Han Fang", "Ling Zhu", "Zixin Xie", "Zibu Wei", "Tianshu Ran", "Haoran Geng", "Xander Wu", "Zachary Bright", "Qizhen Sun", "Rui Wang", "Yuyang Cai", "Song Wang", "Jiace Zhao", "Han Cao", "Yeyang Zhou", "Tianrui Liu", "Ray Pan", "Chongye Yang", "Xiang Ren", "Bo Zhang", "Yutong Ban", "Jitendra Malik", "Brian Anthony", "Pieter Abbeel"], "title": "OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents", "comment": null, "summary": "We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.", "AI": {"tldr": "OSGym\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5f15\u64ce\uff0c\u7528\u4e8e\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u76f8\u5173\u4efb\u52a1\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u3002\u5b83\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u6269\u5c55\u5230\u8d85\u8fc7\u4e00\u5343\u4e2a\u64cd\u4f5c\u7cfb\u7edf\u526f\u672c\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4efb\u52a1\u548c\u8bad\u7ec3\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u8bad\u7ec3\u667a\u80fd\u4f53\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u591a\u6837\u5316\u7684\u73af\u5883\uff0c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u5728\u53ef\u6269\u5c55\u6027\u3001\u901a\u7528\u6027\u548c\u6210\u672c\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "OSGym\u901a\u8fc7\u5e76\u884c\u5316\u8fd0\u884c\u5927\u91cf\u64cd\u4f5c\u7cfb\u7edf\u526f\u672c\uff08\u8d85\u8fc7\u4e00\u5343\u4e2a\uff09\u6765\u63d0\u4f9b\u52a8\u6001\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u4efb\u52a1\uff08\u5982\u5de5\u5177\u4f7f\u7528\u3001\u8f6f\u4ef6\u5de5\u7a0b\u7b49\uff09\u548c\u7075\u6d3b\u7684\u6a21\u578b\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u7ecf\u6d4e\u5b9e\u60e0\u7684\u6309\u9700\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "OSGym\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u591a\u8f6e\u5bf9\u8bdd\u8f68\u8ff9\uff0c\u652f\u6301\u5168\u9762\u7684\u6570\u636e\u6536\u96c6\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\u3002\u4f7f\u7528OSGym\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "OSGym\u5728\u53ef\u6269\u5c55\u6027\u3001\u901a\u7528\u6027\u548c\u7ecf\u6d4e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6709\u671b\u63a8\u52a8\u672a\u6765\u667a\u80fd\u4f53\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.12567", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12567", "abs": "https://arxiv.org/abs/2511.12567", "authors": ["Moussa Labbadi", "Denis Efimov"], "title": "On hyperexponential stabilization of a chain of integrators in continuous and discrete time subject to unmatched perturbations", "comment": "15 pages, 2 figures", "summary": "A recursive time-varying state feedback is presented for a chain of integrators with unmatched perturbations in continuous and discrete time. In continuous time, it is shown that hyperexponential convergence is achieved for the first state variable \\(x_1\\), while the second state \\(x_2\\) remains bounded. For the other states, we establish ISS {\\cb property} by saturating the growing {\\cb control} gain. In discrete time, we use implicit Euler discretization to {\\cb preserve} hyperexponential convergence. The main results are demonstrated through several examples of the proposed control laws, illustrating the conditions established for both continuous and discrete-time systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\u7684\u9012\u5f52\u65f6\u53d8\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u5747\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\u7684\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u72b6\u6001\u53d8\u91cf\u7684\u5feb\u901f\u6536\u655b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9012\u5f52\u65f6\u53d8\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u9971\u548c\u63a7\u5236\u589e\u76ca\u6765\u4fdd\u8bc1\u72b6\u6001\u7684\u6709\u754c\u6027\u3002\u5728\u79bb\u6563\u65f6\u95f4\u4e0b\uff0c\u91c7\u7528\u9690\u5f0f\u6b27\u62c9\u79bb\u6563\u5316\u6765\u4fdd\u6301\u6536\u655b\u6027\u3002", "result": "\u5728\u8fde\u7eed\u65f6\u95f4\u4e0b\uff0c\u7b2c\u4e00\u4e2a\u72b6\u6001\u53d8\u91cf\u5b9e\u73b0\u4e86\u8d85\u6307\u6570\u7ea7\u6536\u655b\uff0c\u7b2c\u4e8c\u4e2a\u72b6\u6001\u53d8\u91cf\u4fdd\u6301\u6709\u754c\uff0c\u5176\u4ed6\u72b6\u6001\u53d8\u91cf\u901a\u8fc7\u9971\u548c\u63a7\u5236\u589e\u76ca\u5b9e\u73b0\u4e86ISS\uff08\u8f93\u5165\u72b6\u6001\u7a33\u5b9a\u6027\uff09\u6027\u8d28\u3002\u5728\u79bb\u6563\u65f6\u95f4\u4e0b\uff0c\u901a\u8fc7\u9690\u5f0f\u6b27\u62c9\u79bb\u6563\u5316\u4fdd\u6301\u4e86\u8d85\u6307\u6570\u7ea7\u6536\u655b\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\uff0c\u5e76\u5728\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u5747\u5b9e\u73b0\u4e86\u671f\u671b\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2511.12101", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12101", "abs": "https://arxiv.org/abs/2511.12101", "authors": ["Jian Zhou", "Sihao Lin", "Shuai Fu", "Qi WU"], "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers", "comment": null, "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.", "AI": {"tldr": "\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\uff0c\u6269\u6563\u7b56\u7565\uff08DP\uff09\u53ca\u5176\u4e24\u79cd\u53d8\u4f53DP-CNN\uff08DP-C\uff09\u548cDP-Transformer\uff08DP-T\uff09\u662f\u6700\u6709\u6548\u548c\u5e7f\u6cdb\u91c7\u7528\u7684\u6a21\u578b\u3002\u7136\u800c\uff0cDP\u548cBC\u65b9\u6cd5\u90fd\u53d7\u5230\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u5e76\u4e14DP\u7684\u5185\u90e8\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u4e2d\uff0c\u7279\u522b\u662f\u6269\u6563\u7b56\u7565\uff08DP\uff09\u6a21\u578b\uff0c\u9762\u4e34\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3001\u5185\u90e8\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u7f3a\u4e4f\u539f\u5219\u6027\u8bbe\u8ba1\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u63a5\u8fd1\u514d\u8d39\u7684\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u4f5c\u4e3a\u65e0\u89c2\u5bdf\u6570\u636e\u6765\u9884\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u7684\u52a8\u4f5c\u5934\uff08\u52a8\u4f5c\u751f\u6210\u5668\uff09\uff0c\u7136\u540e\u51bb\u7ed3\u8be5\u9884\u8bad\u7ec3\u7684\u52a8\u4f5c\u5934\u5e76\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86DP-MLP\uff0c\u7528\u7b80\u5355\u7684MLP\u5757\u66ff\u6362DP-C\u7684U-Net\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\u3002\u89e3\u8026\u8bad\u7ec3\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0cDP-C\u7684\u8bad\u7ec3\u901f\u5ea6\u6700\u591a\u63d0\u9ad8\u4e8641%\u3002DP-MLP\u5728\u6b63\u5e38\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u4e8683.9%\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\uff0c\u5728\u89e3\u8026\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u4e8689.1%\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u4e14\u52a8\u4f5c\u751f\u6210\u9aa8\u5e72\u7f51\u7edc\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u4f5c\u7528\u6709\u9650\u3002DP-MLP\u6a21\u578b\u901a\u8fc7\u4f7f\u7528\u66f4\u7b80\u5355\u7684MLP\u5757\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002"}}
{"id": "2511.12416", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12416", "abs": "https://arxiv.org/abs/2511.12416", "authors": ["Zack Hassman", "Oliver Reardon-Smith", "Gokul Subramanian Ravi", "Frederic T. Chong", "Kevin J. Sung"], "title": "Enhancing Chemistry on Quantum Computers with Fermionic Linear Optical Simulation", "comment": "14 pages, 10 figures", "summary": "We present and open source a simulator for circuits composed of passive fermionic linear optical elements and controlled-phase gates. Given such a circuit, our simulator can compute Born-rule probabilities for samples drawn from it. Our simulator supports both exact and approximate probability calculation, allowing users to trade accuracy for efficiency as needed. For approximate Born-rule probability calculation, our simulator's runtime is exponential only in the magnitudes of the angles of the circuit's controlled-phase gates. This makes our simulator useful for simulating certain systems that are beyond the reach of conventional state vector methods. We demonstrate our simulator's utility by simulating the local unitary cluster Jastrow (LUCJ) ansatz and integrating it with sample-based quantum diagonalization (SQD) to improve the accuracy of molecular ground-state energy estimates. Applied to a 52-qubit $N_2$ system, we observe accuracy improvements of up to $46\\%$ over the baseline SQD implementation with negligible computational overhead. As an efficient and flexible tool for simulating fermionic circuits, our simulator enables new opportunities for enhancing near-term quantum algorithms in chemistry and related domains.", "AI": {"tldr": "\u63d0\u4f9b\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u5305\u542b\u65e0\u6e90\u8d39\u7c73\u5b50\u7ebf\u6027\u5149\u5b66\u5143\u4ef6\u548c\u53d7\u63a7-\u76f8\u79fb\u95e8\u7535\u8def\u7684\u5f00\u6e90\u6a21\u62df\u5668\uff0c\u652f\u6301\u7cbe\u786e\u548c\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u540e\u8005\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5206\u5b50\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6a21\u62df\u7279\u5b9a\u8d39\u7c73\u5b50\u7ebf\u6027\u5149\u5b66\u7535\u8def\u7684\u6a21\u62df\u5668\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4f20\u7edf\u72b6\u6001\u5411\u91cf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u7cfb\u7edf\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u63d0\u9ad8\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u5f00\u6e90\u4e00\u4e2a\u6a21\u62df\u5668\uff0c\u8be5\u6a21\u62df\u5668\u53ef\u4ee5\u8ba1\u7b97\u7ed9\u5b9a\u7535\u8def\u7684\u73bb\u8272\u89c4\u5219\u6982\u7387\u3002\u5b83\u652f\u6301\u7cbe\u786e\u548c\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5176\u4e2d\u8fd1\u4f3c\u8ba1\u7b97\u7684\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u4ec5\u53d6\u51b3\u4e8e\u53d7\u63a7-\u76f8\u79fb\u95e8\u7684\u89d2\u5ea6\u5e45\u5ea6\uff0c\u800c\u4e0d\u662f\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u3002\u901a\u8fc7\u5c06\u6b64\u6a21\u62df\u5668\u4e0e\u672c\u5730\u9149\u7c07Jastrow (LUCJ)\u6ce2\u51fd\u6570\u548c\u6837\u672c\u4e3a\u57fa\u7840\u7684\u91cf\u5b50\u5bf9\u89d2\u5316 (SQD) \u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u5206\u5b50\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "result": "\u8be5\u6a21\u62df\u5668\u5728\u6a21\u62dfLUCJ\u6ce2\u51fd\u6570\u548cSQD\u7ed3\u5408\u65b9\u9762\u663e\u793a\u51fa\u5b9e\u7528\u6027\u3002\u5728\u5e94\u7528\u4e8e52\u91cf\u5b50\u6bd4\u7279\u7684N2\u5206\u5b50\u7cfb\u7edf\u65f6\uff0c\u4e0e\u57fa\u7ebfSQD\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u4e8646%\uff0c\u800c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u8d39\u7c73\u5b50\u7535\u8def\u6a21\u62df\u5de5\u5177\uff0c\u4e3a\u5728\u5316\u5b66\u548c\u76f8\u5173\u9886\u57df\u4e2d\u6539\u8fdb\u8fd1\u671f\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2511.13103", "categories": ["cs.LG", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13103", "abs": "https://arxiv.org/abs/2511.13103", "authors": ["Vidur Sinha", "Muhammed Ustaomeroglu", "Guannan Qu"], "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions", "comment": "8 pages, 7 figures, submitted for review", "summary": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.", "AI": {"tldr": "STACCA\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684MARL\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7edc\u63a7\u5236\uff0c\u80fd\u591f\u5904\u7406\u957f\u671f\u4f9d\u8d56\u5e76\u8de8\u7f51\u7edc\u62d3\u6251\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u5728\u5904\u7406\u7f51\u7edc\u63a7\u5236\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\uff08\u5982\u7ea7\u8054\u6545\u969c\u3001\u75ab\u60c5\u7206\u53d1\uff09\u548c\u8de8\u7f51\u7edc\u62d3\u6251\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "STACCA\u91c7\u7528\u5171\u4eabTransformer Actor-Critic\uff08STACCA\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u96c6\u4e2d\u7684\u56feTransformer Critic\u6765\u5efa\u6a21\u957f\u671f\u4f9d\u8d56\u5e76\u63d0\u4f9b\u7cfb\u7edf\u7ea7\u53cd\u9988\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5171\u4eab\u7684\u56feTransformer Actor\u6765\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7b56\u7565\u3002\u6b64\u5916\uff0cSTACCA\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u53cd\u4e8b\u5b9e\u4f18\u52bf\u4f30\u8ba1\u5668\u6765\u6539\u8fdb\u8bad\u7ec3\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u3002", "result": "\u5728\u75ab\u60c5\u63a7\u5236\u548c\u8c23\u8a00\u4f20\u64ad\u7f51\u7edc\u63a7\u5236\u4efb\u52a1\u4e0a\uff0cSTACCA\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3001\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684MARL\u67b6\u6784\u6709\u6f5c\u529b\u5b9e\u73b0\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u63a7\u5236\u3002"}}
{"id": "2406.05348", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2406.05348", "abs": "https://arxiv.org/abs/2406.05348", "authors": ["Satanu Ghosh", "Neal R. Brodnik", "Carolina Frey", "Collin Holgate", "Tresa M. Pollock", "Samantha Daly", "Samuel Carton"], "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets", "comment": "LLM for information extraction. Update on 12/11/2024: We added some relevant literature that we missed in the previous version of the paper. Update on 05/25/2025: We changed the metadata", "summary": "We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature. We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted. We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.", "AI": {"tldr": "GPT-4\u5728\u79d1\u5b66\u6587\u732e\u7684\u7279\u5b9a\u6a21\u5f0f\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u6216\u4e0d\u89c4\u8303\u7684\u6750\u6599\u79d1\u5b66\u6570\u636e\u65f6\u3002", "motivation": "\u8bc4\u4f30GPT-4\u5728\u6839\u636e\u7279\u5b9a\u6a21\u5f0f\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u73b0\u6709\u624b\u52a8\u63d0\u53d6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8bc6\u522b\u5176\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u63d0\u793a\u65b9\u6cd5\uff0c\u8ba9GPT-4\u5c1d\u8bd5\u91cd\u73b0\u4e24\u4e2a\u5df2\u6709\u7684\u6750\u6599\u79d1\u5b66\u6570\u636e\u96c6\uff0c\u5e76\u7531\u6750\u6599\u79d1\u5b66\u5bb6\u8fdb\u884c\u8be6\u7ec6\u7684\u624b\u52a8\u9519\u8bef\u5206\u6790\uff0c\u4ee5\u786e\u5b9a\u6a21\u578b\u5728\u4fe1\u606f\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u95ee\u9898\u3002", "result": "GPT-4\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e00\u5b9a\u7a0b\u5ea6\u7684\u6210\u529f\uff0c\u80fd\u591f\u90e8\u5206\u91cd\u73b0\u73b0\u6709\u6570\u636e\u96c6\uff0c\u4f46\u4ecd\u5b58\u5728\u9519\u8bef\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u89c4\u8303\u6216\u590d\u6742\u7684\u6570\u636e\u65f6\u3002", "conclusion": "GPT-4\u5728\u79d1\u5b66\u6587\u732e\u4fe1\u606f\u63d0\u53d6\u65b9\u9762\u5c55\u73b0\u4e86\u524d\u666f\uff0c\u4f46\u4e3a\u4e86\u5b9e\u73b0\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u6765\u89e3\u51b3\u5176\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u3002"}}
{"id": "2511.11882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11882", "abs": "https://arxiv.org/abs/2511.11882", "authors": ["Simon Durand", "Samuel Foucher", "Alexandre Delplanque", "Jo\u00eblle Taillon", "J\u00e9r\u00f4me Th\u00e9au"], "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)", "comment": "34 pages, 10 figures, submitted to Remote Sensing in Ecology and Conservation", "summary": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.", "AI": {"tldr": "\u5229\u7528\u5408\u6210\u56fe\u50cf\uff08SI\uff09\u6280\u672f\uff0c\u5373\u4f7f\u5728\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u6709\u6548\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08ODMs\uff09\uff0c\u4ee5\u63d0\u5347\u5bf9\u9e9d\u725b\u7b49\u7a00\u758f\u7269\u79cd\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u7ba1\u7406\u9700\u8981\u51c6\u786e\u7684\u4eba\u53e3\u4f30\u8ba1\uff0c\u4f46\u4f20\u7edf\u7684\u8c03\u67e5\u65b9\u6cd5\uff08\u5982\u822a\u7a7a\u8ba1\u6570\u548cGNSS\u9065\u6d4b\uff09\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u540e\u52e4\u56f0\u96be\u3002\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08ODMs\uff09\u5728\u5904\u7406\u5c0f\u6570\u636e\u96c6\u65f6\u6548\u679c\u6709\u9650\uff0c\u800c\u9e9d\u725b\u7b49\u7269\u79cd\u7684\u8bad\u7ec3\u6570\u636e\u5c24\u4e3a\u7a00\u5c11\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u5408\u6210\u56fe\u50cf\uff08SI\uff09\u6765\u8865\u5145\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u5728\u96f6\u6837\u672c\uff08ZS\uff09\u548c\u5c11\u6837\u672c\uff08FS\uff09\u573a\u666f\u4e0b\u9e9d\u725b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e00\u4e2a\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4ee5\u53ca\u4e94\u4e2a\u5728\u8bad\u7ec3\u96c6\u4e2d\u9010\u6b65\u589e\u52a0\u5408\u6210\u56fe\u50cf\uff08SI\uff09\u7684\u96f6\u6837\u672c\uff08ZS\uff09\u6a21\u578b\u548c\u4e94\u4e2a\u5c11\u6837\u672c\uff08FS\uff09\u6a21\u578b\u3002\u5728ZS\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u96c6\u4e0d\u5305\u542b\u4efb\u4f55\u771f\u5b9e\u56fe\u50cf\uff1b\u5728FS\u6a21\u578b\u4e2d\uff0c\u5219\u7ed3\u5408\u4e86\u771f\u5b9e\u56fe\u50cf\u548cSI\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u4e0d\u540c\u6bd4\u4f8bSI\u5bf9\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u7684\u5f71\u54cd\u3002", "result": "\u5728\u96f6\u6837\u672c\uff08ZS\uff09\u6a21\u578b\u4e2d\uff0c\u6dfb\u52a0SI\u80fd\u591f\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u968f\u7740SI\u7684\u589e\u52a0\uff0c\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5f53SI\u8d85\u8fc7\u57fa\u7ebf\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u7684100%\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u9010\u6e10\u8d8b\u4e8e\u5e73\u7f13\u3002\u5728\u5c11\u6837\u672c\uff08FS\uff09\u6a21\u578b\u4e2d\uff0c\u7ed3\u5408\u771f\u5b9e\u56fe\u50cf\u548cSI\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\uff0c\u80fd\u591f\u63d0\u9ad8\u53ec\u56de\u7387\u5e76\u7565\u5fae\u63d0\u5347\u6574\u4f53\u51c6\u786e\u6027\uff0c\u4f46\u8fd9\u79cd\u6539\u8fdb\u5728\u7edf\u8ba1\u5b66\u4e0a\u4e0d\u663e\u8457\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\uff08SI\uff09\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5730\u8bad\u7ec3\u51fa\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08ODMs\uff09\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u601d\u8def\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u76d1\u6d4b\u7a00\u6709\u6216\u96be\u4ee5\u5230\u8fbe\u7684\u7269\u79cd\uff0c\u8fd8\u80fd\u63d0\u9ad8\u76d1\u6d4b\u9891\u7387\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u5728\u6ca1\u6709\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u542f\u52a8ODMs\uff0c\u5e76\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u548c\u771f\u5b9e\u56fe\u50cf\u7684\u83b7\u53d6\u800c\u4e0d\u65ad\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2511.12750", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12750", "abs": "https://arxiv.org/abs/2511.12750", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Uniform Circular Arrays in Near-Field: Omnidirectional Coverage with Limited Capacity", "comment": null, "summary": "Recent studies suggest that uniform circular arrays (UCAs) can extend the angular coverage of the radiative near field region. This work investigates whether such enhanced angular coverage translates into improved spatial multiplexing performance when compared to uniform linear arrays (ULAs). To more accurately delineate the effective near field region, we introduce the effective beamfocusing Rayleigh distance (EBRD), an angle dependent metric that bounds the spatial region where beamfocusing remains effective. Closed form expressions for both beamdepth and EBRD are derived for UCAs. Our analysis shows that, under a fixed antenna element count, ULAs achieve narrower beamdepth and a longer EBRD than UCAs. Conversely, under a fixed aperture length, UCAs provide slightly narrower beamdepth and a marginally longer EBRD. Simulation results further confirm that ULAs achieve higher sum rate under the fixed element constraint, while UCAs offer marginal performance gain under the fixed aperture constraint.", "AI": {"tldr": "\u5747\u5300\u5706\u9635\uff08UCA\uff09\u4e0e\u5747\u5300\u76f4\u7ebf\u9635\uff08ULA\uff09\u5728\u8f90\u5c04\u8fd1\u573a\u533a\u57df\u7684\u89d2\u5411\u8986\u76d6\u8303\u56f4\u548c\u7a7a\u95f4\u590d\u7528\u6027\u80fd\u65b9\u9762\u7684\u6bd4\u8f83\u3002ULA\u5728\u56fa\u5b9a\u5929\u7ebf\u5355\u5143\u6570\u4e0b\u5177\u6709\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u800cUCA\u5728\u56fa\u5b9a\u5b54\u5f84\u957f\u5ea6\u4e0b\u5177\u6709\u8f7b\u5fae\u4f18\u52bf\u3002", "motivation": "\u63a2\u7a76\u5747\u5300\u5706\u9635\uff08UCA\uff09\u662f\u5426\u80fd\u901a\u8fc7\u589e\u5f3a\u7684\u89d2\u5411\u8986\u76d6\u8303\u56f4\u6765\u63d0\u5347\u7a7a\u95f4\u590d\u7528\u6027\u80fd\uff0c\u5e76\u4e0e\u5747\u5300\u76f4\u7ebf\u9635\uff08ULA\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5f15\u5165\u89d2\u5ea6\u76f8\u5173\u7684\u5ea6\u91cf\u6807\u51c6\u2014\u2014\u6709\u6548\u6ce2\u675f\u805a\u7126\u745e\u5229\u8ddd\u79bb\uff08EBRD\uff09\uff0c\u4ee5\u754c\u5b9a\u6ce2\u675f\u805a\u7126\u6709\u6548\u7684\u7a7a\u95f4\u533a\u57df\u3002\u63a8\u5bfc\u4e86UCA\u7684\u6ce2\u675f\u6df1\u5ea6\u548cEBRD\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u3002", "result": "\u5728\u56fa\u5b9a\u5929\u7ebf\u5355\u5143\u6570\u4e0b\uff0cULA\u5b9e\u73b0\u4e86\u66f4\u7a84\u7684\u6ce2\u675f\u6df1\u5ea6\u548c\u66f4\u957f\u7684EBRD\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u603b\u548c\u901f\u7387\u3002\u5728\u56fa\u5b9a\u5b54\u5f84\u957f\u5ea6\u4e0b\uff0cUCA\u7684\u6ce2\u675f\u6df1\u5ea6\u7565\u7a84\uff0cEBRD\u7565\u957f\uff0c\u6027\u80fd\u589e\u76ca\u4e0d\u660e\u663e\u3002", "conclusion": "\u56fa\u5b9a\u5929\u7ebf\u5355\u5143\u6570\u65f6\uff0cULA\u5728\u7a7a\u95f4\u590d\u7528\u6027\u80fd\u4e0a\u4f18\u4e8eUCA\u3002\u56fa\u5b9a\u5b54\u5f84\u957f\u5ea6\u65f6\uff0cUCA\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002"}}
{"id": "2511.11678", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11678", "abs": "https://arxiv.org/abs/2511.11678", "authors": ["Yuze Liu", "Yunhan Wang", "Tiehua Zhang", "Zhishu Shen", "Cheng Peng", "Libing Wu", "Feng Xia", "Jiong Jin"], "title": "A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems", "comment": null, "summary": "The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.", "AI": {"tldr": "\u4e91\u8fb9\u534f\u540c\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6846\u67b6Co-PLMs\u89e3\u51b3\u4e86\u4e91\u670d\u52a1\u5668\u5e26\u5bbd\u9650\u5236\u548c\u7528\u6237\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u5f02\u6784\u6a21\u578b\u534f\u540c\u5b66\u4e60\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4e91\u8fb9\u534f\u540c\u7684LLM\u5e94\u7528\u9762\u4e34\u5e26\u5bbd\u9650\u5236\u3001\u9690\u79c1\u6cc4\u9732\u548c\u6a21\u578b\u5f02\u6784\u6027\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u6311\u6218\u3002", "method": "\u63d0\u51faCo-PLMs\u6846\u67b6\uff0c\u91c7\u7528\u84b8\u998f\u4ee3\u7406\u6a21\u578b\uff08DPMs\uff09\u4f5c\u4e3a\u6865\u6881\uff0c\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u7ed3\u6784\u65e0\u5173\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u548c\u534f\u540c\u8bad\u7ec3\u3002", "result": "Co-PLMs\u6846\u67b6\u5728Rouge-L\u548cEM\u6307\u6807\u4e0a\u5206\u522b\u53d6\u5f97\u4e86\u5e73\u57475.38%\u548c4.88%\u7684\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "Co-PLMs\u6846\u67b6\u901a\u8fc7\u534f\u540c\u8bad\u7ec3\u548c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e91\u8fb9\u534f\u540cLLM\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6548\u679c\u3002"}}
{"id": "2511.12632", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12632", "abs": "https://arxiv.org/abs/2511.12632", "authors": ["Gal Barkai", "Leonid Mirkin", "Daniel Zelazo"], "title": "On two-degrees-of-freedom agreement protocols", "comment": "7 page, 9 figures, this work has been submitted to ECC 2026 for possible publication", "summary": "We propose a distributed two-degrees-of-freedom (2DOF) architecture for driving autonomous, possibly heterogeneous, agents to agreement. The scheme mirrors classical servo structures, separating local feedback from network filtering. This separation enables independent network-filter design for prescribed noise attenuation and allows controller heterogeneity to reject local disturbances, including disturbances exciting unstable agreement poles -- which is known to be impossible via standard diffusive couplings. The potential of the framework is illustrated via two numerical examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u4e24\u81ea\u7531\u5ea6\uff082DOF\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u9a71\u52a8\u81ea\u4e3b\u5f02\u6784\u667a\u80fd\u4f53\u8fbe\u6210\u4e00\u81f4\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u6269\u6563\u8026\u5408\u65e0\u6cd5\u89e3\u51b3\u7684\u3001\u9a71\u52a8\u667a\u80fd\u4f53\u8fbe\u6210\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u5b58\u5728\u5e72\u6270\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u4e00\u81f4\u6781\u70b9\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u4e24\u81ea\u7531\u5ea6\uff082DOF\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6a21\u4eff\u4e86\u7ecf\u5178\u7684\u4f3a\u670d\u7ed3\u6784\uff0c\u5c06\u5c40\u90e8\u53cd\u9988\u4e0e\u7f51\u7edc\u6ee4\u6ce2\u5206\u79bb\u5f00\u6765\u3002\u8fd9\u79cd\u5206\u79bb\u5141\u8bb8\u72ec\u7acb\u8fdb\u884c\u7f51\u7edc\u6ee4\u6ce2\u8bbe\u8ba1\u4ee5\u5b9e\u73b0\u9884\u671f\u7684\u566a\u58f0\u8870\u51cf\uff0c\u5e76\u5141\u8bb8\u63a7\u5236\u5668\u5f02\u6784\u6027\u6765\u6291\u5236\u5c40\u90e8\u5e72\u6270\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6570\u503c\u793a\u4f8b\u8bf4\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5206\u5e03\u5f0f2DOF\u67b6\u6784\u80fd\u591f\u6709\u6548\u5730\u9a71\u52a8\u81ea\u4e3b\u5f02\u6784\u667a\u80fd\u4f53\u8fbe\u6210\u4e00\u81f4\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5e72\u6270\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u4e00\u81f4\u6781\u70b9\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u566a\u58f0\u8870\u51cf\u548c\u5c40\u90e8\u5e72\u6270\u6291\u5236\u3002"}}
{"id": "2511.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12148", "abs": "https://arxiv.org/abs/2511.12148", "authors": ["Advik Sinha", "Akshay Arjun", "Abhijit Das", "Joyjit Mukherjee"], "title": "Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies", "comment": "9 pages, 6 figures", "summary": "This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot", "AI": {"tldr": "\u4f7f\u7528NEAT\u7b97\u6cd5\u8fdb\u5316\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u5b9e\u73b0\u5e73\u9762\u86c7\u5f62\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e0b\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u5bf9\u5e73\u9762\u86c7\u5f62\u673a\u5668\u4eba\u8fdb\u884c\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u3002", "method": "\u91c7\u7528Neuro-Evolution of Augmenting Topologies (NEAT)\u7b97\u6cd5\u751f\u6210\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u86c7\u5f62\u6b65\u6001\u51fd\u6570\uff0c\u901a\u8fc7\u63a7\u5236\u5173\u8282\u89d2\u5ea6\u6765\u63a7\u5236\u673a\u5668\u4eba\u6cbf\u671f\u671b\u7684\u52a8\u6001\u8def\u5f84\u79fb\u52a8\u3002\u8f93\u5165\u5c42\u5305\u62ec\u5173\u8282\u89d2\u5ea6\u3001\u8fde\u6746\u4f4d\u7f6e\u3001\u5934\u90e8\u8fde\u6746\u4f4d\u7f6e\u4ee5\u53ca\u9644\u8fd1\u969c\u788d\u7269\u4f4d\u7f6e\uff1b\u8f93\u51fa\u5c42\u5305\u62ec\u63a7\u5236\u673a\u5668\u4eba\u901f\u5ea6\u548c\u822a\u5411\u7684\u9891\u7387\u548c\u504f\u79fb\u89d2\u5ea6\u3002\u5229\u7528LiDAR\u3001\u673a\u5668\u4eba\u4f20\u611f\u5668\u6570\u636e\u3001\u76ee\u6807\u4f4d\u7f6e\u548c\u65f6\u95f4\u53c2\u6570\u5316\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4f20\u64ad\u4f18\u8d8a\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6700\u5927\u5316\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u662f\u9ad8\u6548\u7684\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u8bb8\u591a\u969c\u788d\u7269\u7684\u5927\u578b\u73af\u5883\u4e2d\u3002\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u66f4\u4f18\u8d8a\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u4e0e\u6700\u8fd1\u7684CBRL\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "NEAT\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u5e73\u9762\u86c7\u5f62\u673a\u5668\u4eba\u7684\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u5b9e\u73b0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\uff0c\u5e76\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.12443", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12443", "abs": "https://arxiv.org/abs/2511.12443", "authors": ["Changchun Feng", "Xinyu Qiu", "Laifa Tao", "Lin Chen"], "title": "Machine Learning Framework for Efficient Prediction of Quantum Wasserstein Distance", "comment": null, "summary": "The quantum Wasserstein distance (W-distance) is a fundamental metric for quantifying the distinguishability of quantum operations, with critical applications in quantum error correction. However, computing the W-distance remains computationally challenging for multiqubit systems due to exponential scaling. We present a machine learning framework that efficiently predicts the quantum W-distance by extracting physically meaningful features from quantum state pairs, including Pauli measurements, statistical moments, quantum fidelity, and entanglement measures. Our approach employs both classical neural networks and traditional machine learning models. On three-qubit systems, the best-performing Random Forest model achieves near-perfect accuracy ($R^2 = 0.9999$) with mean absolute errors on the order of $10^{-5}$. We further validate the framework's practical utility by successfully verifying two fundamental theoretical propositions in quantum information theory: the bound on measurement probability differences between unitary operations and the $W_1$ gate error rate bound. The results establish machine learning as a viable and scalable alternative to traditional numerical methods for W-distance computation, with particular promise for real-time quantum circuit assessment and error correction protocol design in NISQ devices.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u53ef\u6709\u6548\u9884\u6d4b\u91cf\u5b50 Wasserstein \u8ddd\u79bb\uff0c\u4e3a\u91cf\u5b50\u7ea0\u9519\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "motivation": "\u8ba1\u7b97\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u91cf\u5b50 Wasserstein \u8ddd\u79bb\u5728\u91cf\u5b50\u7ea0\u9519\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6 Pauli \u6d4b\u91cf\u3001\u7edf\u8ba1\u77e9\u3001\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ea0\u7f20\u5ea6\u7b49\u7269\u7406\u7279\u5f81\u6765\u9884\u6d4b\u91cf\u5b50 W-\u8ddd\u79bb\uff0c\u5e76\u91c7\u7528\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u4e09\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff08R^2 = 0.9999\uff09\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u6570\u91cf\u7ea7\u4e3a 10^-5\uff0c\u5e76\u6210\u529f\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u547d\u9898\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e3a\u91cf\u5b50 W-\u8ddd\u79bb\u7684\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u7528\u4e8e NISQ \u8bbe\u5907\u4e2d\u7684\u5b9e\u65f6\u91cf\u5b50\u7535\u8def\u8bc4\u4f30\u548c\u7ea0\u9519\u534f\u8bae\u8bbe\u8ba1\u3002"}}
{"id": "2511.13274", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13274", "abs": "https://arxiv.org/abs/2511.13274", "authors": ["Taras Sereda", "Tom St. John", "Burak Bartan", "Natalie Serrino", "Sachin Katti", "Zain Asgar"], "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators", "comment": "Under review at MLSys 2026", "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.", "AI": {"tldr": "KForge\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u534f\u540c\u5de5\u4f5c\u7684LLM\u9a71\u52a8\u7684\u4ee3\u7406\u6765\u4f18\u5316GPU\u5185\u6838\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u548c\u8de8\u5e73\u53f0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4e3aNVIDIA CUDA\u548cApple Metal\u7b49\u4e0d\u540c\u7684\u5e76\u884c\u8ba1\u7b97\u5e73\u53f0\u751f\u6210\u548c\u4f18\u5316\u7a0b\u5e8f\u3002", "motivation": "GPU\u5185\u6838\u5bf9ML\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e0d\u540c\u52a0\u901f\u5668\u4e0a\u8fdb\u884c\u4f18\u5316\u975e\u5e38\u56f0\u96be\u3002", "method": "KForge\u4f7f\u7528\u4e00\u4e2a\u751f\u6210\u4ee3\u7406\uff08\u901a\u8fc7\u7f16\u8bd1\u548c\u6b63\u786e\u6027\u53cd\u9988\u8fed\u4ee3\u6539\u8fdb\u7a0b\u5e8f\uff09\u548c\u4e00\u4e2a\u6027\u80fd\u5206\u6790\u4ee3\u7406\uff08\u89e3\u91ca\u5206\u6790\u6570\u636e\u4ee5\u6307\u5bfc\u4f18\u5316\uff09\uff0c\u5b83\u4eec\u534f\u540c\u5de5\u4f5c\uff0c\u4ec5\u9700\u4e00\u6b21\u6027\u793a\u4f8b\u5373\u53ef\u9488\u5bf9\u65b0\u5e73\u53f0\u8fdb\u884c\u4f18\u5316\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u8fed\u4ee3\u6539\u8fdb\u7cfb\u7edf\uff0c\u4e24\u4e2a\u4ee3\u7406\u901a\u8fc7\u529f\u80fd\u548c\u4f18\u5316\u901a\u9053\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u91ca\u5404\u79cd\u5206\u6790\u6570\u636e\u4ee5\u751f\u6210\u6307\u5bfc\u7a0b\u5e8f\u7efc\u5408\u7684\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u751f\u6210\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u8de8\u5e73\u53f0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5e76\u4e14KForge\u5728NVIDIA CUDA\u548cApple Metal\u5e73\u53f0\u4e0a\u5747\u80fd\u6709\u6548\u8fdb\u884c\u7a0b\u5e8f\u7efc\u5408\uff0c\u9a8c\u8bc1\u4e86\u5176\u5e73\u53f0\u65e0\u5173\u6027\u3002", "conclusion": "KForge\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4ee3\u7406\u67b6\u6784\u548c\u8de8\u5e73\u53f0\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\uff0c\u4e3a\u5728\u4e0d\u540c\u52a0\u901f\u5668\u4e0a\u4f18\u5316GPU\u5185\u6838\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11630", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.11630", "abs": "https://arxiv.org/abs/2511.11630", "authors": ["Eliane Younes", "Elie Hachem", "Marc Bernacki"], "title": "Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models", "comment": null, "summary": "Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662fLSTM\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u6676\u7c92\u751f\u957f\u8fc7\u7a0b\u4e2d\u7684\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\uff0c\u6bd4\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u6676\u7c92\u751f\u957f\u5bf9\u6750\u6599\u529b\u5b66\u884c\u4e3a\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9884\u6d4b\u5176\u5728\u5fae\u89c2\u7ed3\u6784\u5de5\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528RNN\u3001LSTM\u3001TCN\u548cTransformer\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u9ad8\u4fdd\u771f\u6a21\u62df\u63d0\u53d6\u7684\u5747\u503c\u573a\u7edf\u8ba1\u63cf\u8ff0\u7b26\uff0c\u4ece\u5386\u53f2\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\u9884\u6d4b\u672a\u6765\u7684\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\u3002", "result": "LSTM\u6a21\u578b\u8fbe\u5230\u4e8690%\u4ee5\u4e0a\u7684\u9ad8\u51c6\u786e\u7387\u548c\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u8fdb\u884c\u957f\u671f\u7684\u7269\u7406\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u5e76\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u6bcf\u4e2a\u5e8f\u5217\u7ea620\u5206\u949f\u7f29\u77ed\u5230\u51e0\u79d2\u949f\u3002\u5176\u4ed6\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u65f6\u8d8b\u4e8e\u53d1\u6563\u3002", "conclusion": "\u4f4e\u7ef4\u63cf\u8ff0\u7b26\u548c\u57fa\u4e8eLSTM\u7684\u9884\u6d4b\u65b9\u6cd5\u5728\u9ad8\u6548\u51c6\u786e\u7684\u5fae\u89c2\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u5de5\u827a\u4f18\u5316\u3002"}}
{"id": "2511.11890", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11890", "abs": "https://arxiv.org/abs/2511.11890", "authors": ["Camila Machado de Araujo", "Egon P. B. S. Borges", "Ricardo Marcelo Canteiro Grangeiro", "Allan Pinto"], "title": "Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation", "comment": null, "summary": "High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.", "AI": {"tldr": "Harpia\u662f\u4e00\u4e2a\u65b0\u7684CUDA\u5e93\uff0c\u7528\u4e8e\u5927\u89c4\u6a213D\u533b\u5b66\u56fe\u50cf\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5185\u5b58\u63a7\u5236\u3001\u5206\u5757\u6267\u884c\u548cGPU\u52a0\u901f\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u9002\u7528\u4e8eHPC\u548c\u8fdc\u7a0b\u534f\u4f5c\u73af\u5883\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u4f53\u79ef\u6210\u50cf\u6280\u672f\u4ea7\u751f\u7684\u6570\u636e\u96c6\u8d8a\u6765\u8d8a\u5927\uff0c\u5bf9\u73b0\u6709\u5de5\u5177\u7684\u5904\u7406\u3001\u5206\u5272\u548c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u80fd\u529b\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51faHarpia\uff0c\u4e00\u4e2a\u57fa\u4e8eCUDA\u7684\u5e93\uff0c\u652f\u6301\u5927\u89c4\u6a21\u3001\u4ea4\u4e92\u5f0f\u76843D\u6570\u636e\u96c6\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u5185\u5b58\u63a7\u5236\u3001\u5206\u5757\u6267\u884c\u3001GPU\u52a0\u901f\u8fc7\u6ee4\u3001\u6ce8\u91ca\u548c\u91cf\u5316\u5de5\u5177\u3002", "result": "\u4e0eNVIDIA cuCIM\u548cscikit-image\u7b49\u6846\u67b6\u76f8\u6bd4\uff0cHarpia\u5728\u5904\u7406\u901f\u5ea6\u3001\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "Harpia\u7684\u4ea4\u4e92\u5f0f\u3001\u4eba\u673a\u534f\u540c\u754c\u9762\u548c\u9ad8\u6548\u7684GPU\u8d44\u6e90\u7ba1\u7406\uff0c\u4f7f\u5176\u7279\u522b\u9002\u5408\u5728\u5171\u4eabHPC\u57fa\u7840\u8bbe\u65bd\u4e2d\u8fdb\u884c\u534f\u4f5c\u79d1\u5b66\u6210\u50cf\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2511.13104", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13104", "abs": "https://arxiv.org/abs/2511.13104", "authors": ["Reiner Thom\u00e4", "Michael D\u00f6bereiner", "Reza Faramarzahangari", "Jonas Gedschold. Marc Francisco Colaco Miranda", "Saw James Myint", "Steffen Schieler", "Christian Schneider", "Sebastian Semper", "Carsten Smeenk", "Gerd Sommerkorn", "Zhixiang Zhao"], "title": "Distributed Multisensor ISAC", "comment": null, "summary": "Integrated Sensing and Communications (ISAC) will become a service in future mobile communication networks. It enables the detection and recognition of passive objects and environments using radar-like sensing. The ultimate advantage is the reuse of the mobile network and radio access resources for scene illumination, sensing, data transportation, computation, and fusion. It enables building a distributed, ubiquitous sensing network that can be adapted for a variety of radio sensing tasks and services.\n  In this article, we develop the principles of multi-sensor ISAC (MS-ISAC). MS-ISAC corresponds to multi-user MIMO communication, which in radar terminology is known as distributed MIMO radar. \\ First, we develop basic architectural principles for MS-ISAC and link them to example use cases. We then propose a generic MS-ISAC architecture. After a brief reference to multipath propagation and multistatic target reflectivity issues, we outline multilink access, coordination, precoding and link adaptation schemes for MS-ISAC. Moreover, we review model-based estimation and tracking of delay~/~Doppler from sparse OFDMA~/~TDMA frames. We emphasize Cooperative Passive Coherent Location (CPCL) for bistatic correlation and synchronization. Finally, issues of multisensor node synchronization and distributed data fusion are addressed.", "AI": {"tldr": "ISAC\u53ef\u4ee5\u5229\u7528\u672a\u6765\u7684\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u8fdb\u884c\u96f7\u8fbe\u7c7b\u4f20\u611f\uff0c\u4ece\u800c\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f20\u611f\u7f51\u7edc\u3002", "motivation": "ISAC\u7684\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u91cd\u590d\u5229\u7528\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u548c\u65e0\u7ebf\u63a5\u5165\u8d44\u6e90\uff0c\u7528\u4e8e\u7167\u660e\u3001\u4f20\u611f\u3001\u6570\u636e\u4f20\u8f93\u3001\u8ba1\u7b97\u548c\u878d\u5408\uff0c\u4ece\u800c\u6784\u5efa\u53ef\u9002\u5e94\u591a\u79cd\u65e0\u7ebf\u4f20\u611f\u4efb\u52a1\u548c\u670d\u52a1\u7684\u5206\u5e03\u5f0f\u3001\u6cdb\u5728\u4f20\u611f\u7f51\u7edc\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u4f20\u611f\u5668ISAC\uff08MS-ISAC\uff09\u7684\u539f\u7406\uff0c\u5c06\u5176\u4e0e\u5206\u5e03\u5f0fMIMO\u96f7\u8fbe\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684MS-ISAC\u67b6\u6784\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8ba8\u8bba\u4e86\u591a\u5f84\u4f20\u64ad\u3001\u591a\u7ad9\u76ee\u6807\u53cd\u5c04\u7387\u3001\u591a\u94fe\u8def\u63a5\u5165\u3001\u534f\u8c03\u3001\u9884\u7f16\u7801\u3001\u94fe\u8def\u81ea\u9002\u5e94\u65b9\u6848\u3001\u7a00\u758fOFDMA/TDMA\u5e27\u7684\u5ef6\u8fdf/\u591a\u666e\u52d2\u4f30\u8ba1\u548c\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u5408\u4f5c\u65e0\u6e90\u76f8\u5e72\u5b9a\u4f4d\uff08CPCL\uff09\u3001\u591a\u4f20\u611f\u5668\u8282\u70b9\u540c\u6b65\u548c\u5206\u5e03\u5f0f\u6570\u636e\u878d\u5408\u7b49\u95ee\u9898\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86MS-ISAC\u7684\u57fa\u672c\u67b6\u6784\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u6280\u672f\u65b9\u6848\u548c\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "MS-ISAC\u662f\u672a\u6765\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.11719", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11719", "abs": "https://arxiv.org/abs/2511.11719", "authors": ["Mohammad Mahdi Kamani", "Zhongwei Cheng", "Lin Chen"], "title": "ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation", "comment": null, "summary": "The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.", "AI": {"tldr": "Eccentric\u6846\u67b6\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8fb9\u7f18-\u4e91\u7aef\u63a8\u7406\u7cfb\u7edf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u6743\u8861\u7684\u6a21\u578b\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18AI\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e91\u7aef\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u968f\u7740\u8fb9\u7f18\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\u800c\u6025\u5267\u4e0a\u5347\uff0c\u5b58\u5728\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEccentric\u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u6743\u8861\u7684\u6a21\u578b\u6765\u9002\u5e94\u77e5\u8bc6\u4ece\u8fb9\u7f18\u6a21\u578b\u5230\u4e91\u7aef\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "\u5728\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "Eccentric\u6846\u67b6\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u65b0\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18-\u4e91\u7aef\u63a8\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002"}}
{"id": "2511.12788", "categories": ["cs.LG", "cs.AR", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12788", "abs": "https://arxiv.org/abs/2511.12788", "authors": ["Rub\u00e9n Dar\u00edo Guerrero"], "title": "Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data", "comment": "32 pages, 21 figures, 10 tables", "summary": "The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\\boldsymbol\u03b8 = \\{\u03b8_d, \u03b8_a, \u03b8_b, \u03b8_p, \u03b8_c\\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u7ea6\u675f\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3EUV\u5149\u523b\u4f18\u5316\u4e2d\u7684\u8ba1\u7b97\u5371\u673a\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u81ea\u52a8\u6821\u51c6\u7535\u78c1\u8fd1\u4f3c\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8fb9\u7f18\u653e\u7f6e\u8bef\u5dee\uff08EPE\uff09\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7eb3\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u5149\u523b\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u534a\u5bfc\u4f53\u884c\u4e1a\u5bf9EUV\u5149\u523b\u4e9a\u7eb3\u7c73\u7ea7\u7cbe\u5ea6\u7684\u9700\u6c42\uff0c\u5b58\u5728\u5b66\u672f\u754c\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e0e\u5de5\u4e1a\u754c\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7269\u7406\u7ea6\u675f\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u53ef\u5fae\u7684\u83f2\u6d85\u5c14\u884d\u5c04\u3001\u6750\u6599\u5438\u6536\u3001\u70b9\u6269\u6563\u51fd\u6570\u6a21\u7cca\u3001\u76f8\u79fb\u548c\u5bf9\u6bd4\u5ea6\u8c03\u5236\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u76f4\u63a5\u51e0\u4f55\u56fe\u6848\u5339\u914d\u76ee\u6807\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570 $\\boldsymbol\u03b8 = \\{\u03b8_d, \u03b8_a, \u03b8_b, \u03b8_p, \u03b8_c\\}$ \u81ea\u52a8\u6821\u51c6\u7535\u78c1\u8fd1\u4f3c\uff0c\u4ee5\u6700\u5c0f\u5316\u6a21\u62df\u56fe\u50cf\u548c\u76ee\u6807\u5149\u63a9\u6a21\u4e4b\u95f4\u7684EPE\u3002", "result": "\u572815\u4e2a\u4ee3\u8868\u6027\u56fe\u6848\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e860.664-2.536 nm\u8303\u56f4\u5185\u7684EPE\uff0c\u5e73\u5747\u6bd4\u65e0\u7269\u7406\u7ea6\u675f\u7684CNN\u57fa\u7ebf\u63d0\u9ad8\u4e8669.9%\uff0c\u4e14\u6240\u9700\u7684\u8bad\u7ec3\u6837\u672c\u6570\u91cf\u51cf\u5c11\u4e8690%\u3002", "conclusion": "\u7269\u7406\u7ea6\u675f\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3EUV\u5149\u523b\u4f18\u5316\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u901a\u8fc7\u8054\u5408\u7269\u7406\u6821\u51c6\u548c\u5236\u9020\u7cbe\u5ea6\u76ee\u6807\uff0c\u4e3a\u534a\u5bfc\u4f53\u5236\u9020\u4e1a\u7684\u5b9e\u65f6\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12678", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12678", "abs": "https://arxiv.org/abs/2511.12678", "authors": ["Yingzhuo Sun", "Yulan Gao", "Ming Xiao", "Zhu Han", "Octavia A. Dobre"], "title": "Visibility-aware Satellite Selection and Resource Allocation in Multi-Orbit LEO Networks", "comment": null, "summary": "Multi orbit low earth orbit (LEO) satellites communication is envisioned as a key infrastructure to deliver global coverage, enabling future services from space air ground integrated networks.However, the optimized design of LEO which jointly addresses satellite selection, association control, and resource scheduling while accounting for dynamic visibility in multi orbit constellations still remains open. Satellites moving along distinct orbital planes yield phase shifted ground tracks and heterogeneous, time varying coverage patterns that significantly complicate the optimization.To bridge the gap, we propose a dynamic visibility aware multi orbit satellite selection framework which can determine the optimal serving satellites across orbital layers. The framework is built upon Markov approximation and matching game theory. Specifically, we formulate a combinatorial optimization problem that maximizes the sum rate under per satellite power budgets. The problem is NP hard , combining discrete user association (UA) decisions with continuous power allocation, and an inherently non convex sum rate maximization objective. We address it through a problem specific Markov approximation. Moreover, we alternately solve UA or bandwidth allocation via a matching game and power allocation via a Lagrangian dual program, which together form a block coordinate descent method tailored to this problem. Simulation results show that the proposed algorithm converges to a suboptimal solution across all scenarios. Extensive experiments against four state of the art baselines further demonstrate that our algorithm achieves, on average, approximately 7.85% higher sum rate than the best performing baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u52a8\u6001\u53ef\u89c1\u6027\u7684\u591a\u8f68\u9053\u536b\u661f\u9009\u62e9\u6846\u67b6\uff0c\u4ee5\u4f18\u5316\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u536b\u661f\u9009\u62e9\u3001\u5173\u8054\u63a7\u5236\u548c\u8d44\u6e90\u8c03\u5ea6\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u548c\u5339\u914d\u535a\u5f08\u8bba\u6765\u89e3\u51b3NP\u96be\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7528\u6237\u5173\u8054\u3001\u5e26\u5bbd\u5206\u914d\u548c\u529f\u7387\u5206\u914d\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cfb\u7edf\u548c\u901f\u7387\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u65e8\u5728\u63d0\u4f9b\u5168\u7403\u8986\u76d6\uff0c\u4f46\u73b0\u6709\u7684\u4f18\u5316\u8bbe\u8ba1\u672a\u80fd\u6709\u6548\u89e3\u51b3\u591a\u8f68\u9053\u661f\u5ea7\u4e2d\u536b\u661f\u9009\u62e9\u3001\u5173\u8054\u63a7\u5236\u548c\u8d44\u6e90\u8c03\u5ea6\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u52a8\u6001\u53ef\u89c1\u6027\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u89c1\u6027\u611f\u77e5\u591a\u8f68\u9053\u536b\u661f\u9009\u62e9\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u548c\u5339\u914d\u535a\u5f08\u8bba\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u4ea4\u66ff\u6c42\u89e3\u7528\u6237\u5173\u8054\u3001\u5e26\u5bbd\u5206\u914d\u548c\u529f\u7387\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u5747\u6536\u655b\u81f3\u6b21\u4f18\u89e3\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u5b9e\u73b0\u4e86\u7ea67.85%\u7684\u66f4\u9ad8\u548c\u901f\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u53ef\u89c1\u6027\u611f\u77e5\u591a\u8f68\u9053\u536b\u661f\u9009\u62e9\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u4f18\u5316LEO\u536b\u661f\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u548c\u901f\u7387\u3002"}}
{"id": "2511.12160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12160", "abs": "https://arxiv.org/abs/2511.12160", "authors": ["Wenbin Mai", "Minghui Liwang", "Xinlei Yi", "Xiaoyu Xia", "Seyyedali Hosseinalipour", "Xianbin Wang"], "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)", "comment": "12 pages, 9 figures", "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u5b89\u5168\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u8fd0\u52a8\u89c4\u5212\u7684\u6846\u67b6\uff0c\u540d\u4e3aRE-DPG\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u9762\u4e34\u5b89\u5168\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u8fd0\u52a8\u89c4\u5212\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u590d\u6742\u7684\u4ea4\u4e92\u3001\u968f\u673a\u5e72\u6270\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8fbe\u6027\u589e\u5f3a\u7684\u52a8\u6001\u52bf\u535a\u5f08\uff08RE-DPG\uff09\u6846\u67b6\uff0c\u5c06\u535a\u5f08\u8bba\u534f\u8c03\u4e0e\u53ef\u8fbe\u6027\u5206\u6790\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u90bb\u57df\u4e3b\u5bfc\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94\uff08ND-iBR\uff09\u65b9\u6848\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\uff0c\u8be5\u65b9\u6848\u57fa\u4e8e\u4fdd\u8bc1\u6536\u655b\u5230\u03b5-NE\u7684\u8fed\u4ee3\u03b5-BR\uff08i\u03b5-BR\uff09\u8fc7\u7a0b\u3002\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u524d\u5411\u53ef\u8fbe\u96c6\uff08MA-FRS\uff09\u673a\u5236\u96c6\u6210\u5230\u6210\u672c\u51fd\u6570\u4e2d\u6765\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u660e\u786e\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5e76\u5f3a\u5236\u6267\u884c\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u57282D\u548c3D\u73af\u5883\u4e2d\u8fdb\u884c\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86RE-DPG\u5728\u5404\u79cd\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "RE-DPG\u6846\u67b6\u80fd\u591f\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u63d0\u4f9b\u5b89\u5168\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2511.12482", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12482", "abs": "https://arxiv.org/abs/2511.12482", "authors": ["Yue Yin", "Tailong Xiao", "Xiaoyang Deng", "Ming He", "Jianping Fan", "Guihua Zeng"], "title": "Discovering autonomous quantum error correction via deep reinforcement learning", "comment": null, "summary": "Quantum error correction is essential for fault-tolerant quantum computing. However, standard methods relying on active measurements may introduce additional errors. Autonomous quantum error correction (AQEC) circumvents this by utilizing engineered dissipation and drives in bosonic systems, but identifying practical encoding remains challenging due to stringent Knill-Laflamme conditions. In this work, we utilize curriculum learning enabled deep reinforcement learning to discover Bosonic codes under approximate AQEC framework to resist both single-photon and double-photon losses. We present an analytical solution of solving the master equation under approximation conditions, which can significantly accelerate the training process of reinforcement learning. The agent first identifies an encoded subspace surpassing the breakeven point through rapid exploration within a constrained evolutionary time-frame, then strategically fine-tunes its policy to sustain this performance advantage over extended temporal horizons. We find that the two-phase trained agent can discover the optimal set of codewords, i.e., the Fock states $\\ket{4}$ and $\\ket{7}$ considering the effect of both single-photon and double-photon loss. We identify that the discovered code surpasses the breakeven threshold over a longer evolution time and achieve the state-of-art performance. We also analyze the robustness of the code against the phase damping and amplitude damping noise. Our work highlights the potential of curriculum learning enabled deep reinforcement learning in discovering the optimal quantum error correct code especially in early fault-tolerant quantum systems.", "AI": {"tldr": "\u5229\u7528\u8bfe\u7a0b\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u53d1\u73b0\u65b0\u7684\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u4ee5\u62b5\u6297\u5355\u5149\u5b50\u548c\u53cc\u5149\u5b50\u635f\u8017\u3002", "motivation": "\u6807\u51c6\u7684\u91cf\u5b50\u7ea0\u9519\u65b9\u6cd5\u53ef\u80fd\u5f15\u5165\u989d\u5916\u9519\u8bef\uff0c\u800c\u81ea\u4e3b\u91cf\u5b50\u7ea0\u9519\uff08AQEC\uff09\u867d\u7136\u80fd\u907f\u514d\uff0c\u4f46\u5bfb\u627e\u5b9e\u9645\u7f16\u7801\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u53d1\u73b0\u65b0\u7684AQEC\u7f16\u7801\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u53d1\u73b0\u6ee1\u8db3\u8fd1\u4f3cAQEC\u6846\u67b6\u7684\u91cf\u5b50\u7801\uff0c\u4ee5\u62b5\u6297\u5355\u5149\u5b50\u548c\u53cc\u5149\u5b50\u635f\u8017\u3002\u9996\u5148\u5206\u6790\u4e86\u4e3b\u65b9\u7a0b\u5728\u8fd1\u4f3c\u6761\u4ef6\u4e0b\u7684\u89e3\u6790\u89e3\u4ee5\u52a0\u901f\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5feb\u901f\u63a2\u7d22\u4ee5\u8bc6\u522b\u7a81\u7834\u76c8\u4e8f\u5e73\u8861\u70b9\u7684\u7f16\u7801\u5b50\u7a7a\u95f4\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7cbe\u7ec6\u8c03\u6574\u7b56\u7565\u4ee5\u7ef4\u6301\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u53d1\u73b0\u4e86\u57fa\u4e8e Fock \u6001 $\\ket{4}$ \u548c $\\ket{7}$ \u7684\u6700\u4f18\u7f16\u7801\uff0c\u80fd\u591f\u62b5\u6297\u5355\u5149\u5b50\u548c\u53cc\u5149\u5b50\u635f\u8017\u3002\u8be5\u7f16\u7801\u5728\u66f4\u957f\u7684\u6f14\u5316\u65f6\u95f4\u5185\u8d85\u8d8a\u4e86\u76c8\u4e8f\u5e73\u8861\u9608\u503c\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u8be5\u7f16\u7801\u5bf9\u76f8\u4f4d\u963b\u5c3c\u548c\u5e45\u5ea6\u963b\u5c3c\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bfe\u7a0b\u5b66\u4e60\u9a71\u52a8\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u53d1\u73b0\u6700\u4f18\u91cf\u5b50\u7ea0\u9519\u7801\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2511.13361", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13361", "abs": "https://arxiv.org/abs/2511.13361", "authors": ["Jiyang Zheng", "Islam Nassar", "Thanh Vu", "Xu Zhong", "Yang Lin", "Tongliang Liu", "Long Duong", "Yuan-Fang Li"], "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding", "comment": null, "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.", "AI": {"tldr": "MedDCR\u662f\u4e00\u4e2a\u95ed\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u6765\u8bbe\u8ba1\u533b\u7597\u7f16\u7801\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u533b\u7597\u7f16\u7801\u662f\u5c06\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u8bca\u65ad\u548c\u7a0b\u5e8f\u4ee3\u7801\u7684\u8fc7\u7a0b\uff0c\u8fd9\u5bf9\u4e8e\u8ba1\u8d39\u3001\u533b\u9662\u8fd0\u8425\u548c\u533b\u5b66\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u4e0e\u666e\u901a\u7684\u6587\u672c\u5206\u7c7b\u4e0d\u540c\uff0c\u5b83\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\uff0c\u5e76\u4e14\u9700\u8981\u89e3\u51b3\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7684\u7ec6\u5fae\u5dee\u522b\u548c\u53ef\u53d8\u6027\u7684\u95ee\u9898\u3002", "method": "MedDCR\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u8bbe\u8ba1\u8005\uff08\u63d0\u51fa\u5de5\u4f5c\u6d41\uff09\u3001\u4e00\u4e2a\u7f16\u7801\u8005\uff08\u6267\u884c\u5de5\u4f5c\u6d41\uff09\u548c\u4e00\u4e2a\u53cd\u601d\u8005\uff08\u8bc4\u4f30\u9884\u6d4b\u5e76\u63d0\u4f9b\u53cd\u9988\uff09\uff0c\u5e76\u5229\u7528\u8bb0\u5fc6\u5e93\u6765\u4fdd\u5b58\u5148\u524d\u8bbe\u8ba1\u4ee5\u4f9b\u91cd\u7528\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "MedDCR\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5e76\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u66f4\u80fd\u53cd\u6620\u5b9e\u9645\u7f16\u7801\u5b9e\u8df5\u7684\u5de5\u4f5c\u6d41\u3002", "conclusion": "MedDCR\u901a\u8fc7\u5c06\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u89c6\u4e3a\u4e00\u4e2a\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u533b\u7597\u7f16\u7801\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.11697", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11697", "abs": "https://arxiv.org/abs/2511.11697", "authors": ["Liqin Tan", "Pin Chen", "Menghan Liu", "Xiean Wang", "Jianhuan Cen", "Qingsong Zou"], "title": "Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification", "comment": "12 pages, 1 figure, 5 tables", "summary": "We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.", "AI": {"tldr": "MatUQ\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u5728\u5916\u90e8\u5206\u5e03(OOD)\u6750\u6599\u6027\u8d28\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316(UQ)\u7684\u57fa\u51c6\u6846\u67b6\u3002", "motivation": "\u63d0\u51faMatUQ\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30GNNs\u5728\u5916\u90e8\u5206\u5e03\u6750\u6599\u6027\u8d28\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528SOAP-LOCO\u7b56\u7565\u6784\u5efa1,375\u4e2aOOD\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u7ed3\u5408\u8499\u7279\u5361\u6d1bDropout\u548c\u6df1\u5ea6\u8bc1\u636e\u56de\u5f52(DER)\u7684\u7edf\u4e00\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\uff0c\u4ee5\u53ca\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6307\u6807D-EviU\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff08\u5e73\u5747\u51cf\u5c1170.6%\u7684\u8bef\u5dee\uff09\uff0c\u4e14\u4e0d\u540cGNN\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u80fd\u5b8c\u5168\u80dc\u51fa\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u5bf9\u63d0\u9ad8OOD\u573a\u666f\u4e0b\u7684GNN\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u9700\u8981\u6839\u636e\u5177\u4f53\u6750\u6599\u6027\u8d28\u548c\u5206\u5e03\u53d8\u5316\u6765\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u3002"}}
{"id": "2511.11898", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11898", "abs": "https://arxiv.org/abs/2511.11898", "authors": ["Arnav Singhvi", "Vasiliki Bikia", "Asad Aali", "Akshay Chaudhari", "Roxana Daneshjou"], "title": "Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks", "comment": null, "summary": "Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06DSPy\u6846\u67b6\u5e94\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLMs\u5728\u533b\u5b66\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u96be\u4ee5\u63a8\u5e7f\u4e14\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u533b\u7597\u673a\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u5373\u53ef\u5229\u7528\u6a21\u578b\u5185\u7f6e\u77e5\u8bc6\u5e76\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4e0e\u6743\u91cd\u65e0\u5173\u7684\u6027\u80fd\u63d0\u5347\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5c06DSPy\u6846\u67b6\u5e94\u7528\u4e8e\u7ed3\u6784\u5316\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff0c\u9488\u5bf9\u653e\u5c04\u5b66\u3001\u80c3\u80a0\u75c5\u5b66\u548c\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\u4e94\u4e2a\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8610\u4e2a\u5f00\u6e90VLMs\u548c4\u79cd\u63d0\u793a\u4f18\u5316\u6280\u672f\u3002", "result": "\u4e0e\u96f6\u6837\u672c\u63d0\u793a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4f18\u5316\u540e\u7684\u63d0\u793a\u6d41\u6c34\u7ebf\u5728\u4e2d\u4f4d\u503c\u4e0a\u5b9e\u73b0\u4e8653%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5728\u96f6\u6837\u672c\u6027\u80fd\u8f83\u4f4e\u7684\u4efb\u52a1\u4e0a\uff0c\u63d0\u5347\u5e45\u5ea6\u9ad8\u8fbe300%\u81f33,400%\u3002", "conclusion": "\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u5728\u533b\u5b66AI\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u4e8e\u89c6\u89c9\u7684\u5e94\u7528\u7684\u6027\u80fd\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u4e13\u6ce8\u4e8e\u60a3\u8005\u62a4\u7406\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u5e76\u80fd\u63d0\u5347\u5f00\u6e90VLMs\u7684\u6027\u80fd\u3002\u7814\u7a76\u4eba\u5458\u516c\u5f00\u4e86\u8bc4\u4f30\u6d41\u6c34\u7ebf\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u7684\u7814\u7a76\u3002"}}
{"id": "2511.13171", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13171", "abs": "https://arxiv.org/abs/2511.13171", "authors": ["Niccol\u00f2 Paglierani", "Francesco Linsalata", "Vineeth Teeda", "Davide Scazzoli", "Maurizio Magarini"], "title": "Autonomous Sensing UAV for Accurate Multi-User Identification and Localization in Cellular Networks", "comment": null, "summary": "This paper presents an autonomous sensing frame- work for identifying and localizing multiple users in Fifth Generation (5G) networks using an Unmanned Aerial Vehicle (UAV) that is not part of the serving access network. Unlike conventional aerial serving nodes, the proposed UAV operates passively and is dedicated solely to sensing. It captures Uplink (UL) Sounding Reference Signals (SRS), and requires virtually no coordination with the network infrastructure. A complete signal processing chain is proposed and developed, encompassing synchronization, user identification, and localization, all executed onboard UAV during flight. The system autonomously plans and adapts its mission workflow to estimate multiple user positions within a single deployment, integrating flight control with real-time sensing. Extensive simulations and a full-scale low- altitude experimental campaign validate the approach, showing localization errors below 3 m in rural field tests and below 8 m in urban simulation scenarios, while reliably identifying each user. The results confirm the feasibility of infrastructure-independent sensing UAVs as a core element of the emerging Low Altitude Economy (LAE), supporting situational awareness and rapid deployment in emergency or connectivity-limited environments.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u4e3b\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u975e\u63a5\u5165\u7f51\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u57285G\u7f51\u7edc\u4e2d\u8bc6\u522b\u548c\u5b9a\u4f4d\u591a\u7528\u6237\u3002", "motivation": "\u4f20\u7edf\u7684\u7a7a\u4e2d\u8282\u70b9\u9700\u8981\u4e0e\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u534f\u8c03\uff0c\u800c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u5219\u88ab\u8bbe\u8ba1\u4e3a\u88ab\u52a8\u4f20\u611f\uff0c\u65e0\u9700\u7f51\u7edc\u534f\u8c03\uff0c\u4e13\u6ce8\u4e8e\u6355\u83b7\u4e0a\u884c\u94fe\u8def\uff08UL\uff09\u63a2\u6d4b\u53c2\u8003\u4fe1\u53f7\uff08SRS\uff09\uff0c\u4ee5\u5b9e\u73b0\u7528\u6237\u8bc6\u522b\u548c\u5b9a\u4f4d\u3002", "method": "\u8be5\u6846\u67b6\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u4fe1\u53f7\u5904\u7406\u94fe\uff0c\u5305\u62ec\u540c\u6b65\u3001\u7528\u6237\u8bc6\u522b\u548c\u5b9a\u4f4d\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5728\u65e0\u4eba\u673a\u98de\u884c\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u3002\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u548c\u8c03\u6574\u4efb\u52a1\u6d41\u7a0b\uff0c\u5728\u5355\u6b21\u90e8\u7f72\u4e2d\u4f30\u8ba1\u591a\u4e2a\u7528\u6237\u7684\u4f4d\u7f6e\uff0c\u5e76\u5c06\u98de\u884c\u63a7\u5236\u4e0e\u5b9e\u65f6\u4f20\u611f\u76f8\u7ed3\u5408\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u4f4e\u7a7a\u5b9e\u9a8c\uff0c\u5728\u519c\u6751\u5730\u533a\u6d4b\u8bd5\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e3\u7c73\uff0c\u5728\u57ce\u5e02\u6a21\u62df\u573a\u666f\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e8\u7c73\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u8bc6\u522b\u7528\u6237\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u72ec\u7acb\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u4f20\u611f\u65e0\u4eba\u673a\u5728\u65b0\u5174\u4f4e\u7a7a\u7ecf\u6d4e\uff08LAE\uff09\u4e2d\u4f5c\u4e3a\u6838\u5fc3\u5143\u7d20\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u591f\u652f\u6301\u5728\u7d27\u6025\u6216\u8fde\u63a5\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6001\u52bf\u611f\u77e5\u548c\u5feb\u901f\u90e8\u7f72\u3002"}}
{"id": "2511.11721", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11721", "abs": "https://arxiv.org/abs/2511.11721", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems", "comment": null, "summary": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e0d\u4f7f\u8282\u70b9\u8fc7\u8f7d\u7684\u60c5\u51b5\u4e0b\u5206\u914d\u4e91\u7cfb\u7edf\u670d\u52a1\u3001\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u5e76\u6700\u5c0f\u5316\u6210\u672c\u7684\u7b56\u7565\u3002", "motivation": "\u5728\u4e91\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5728\u4e0d\u4f7f\u8282\u70b9\u8fc7\u8f7d\u7684\u60c5\u51b5\u4e0b\u5206\u914d\u670d\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u5e76\u6700\u5c0f\u5316\u6210\u672c\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e91\u8d44\u6e90\u5229\u7528\u7684\u62bd\u8c61\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u62ec\u591a\u79cd\u7c7b\u578b\u7684\u8d44\u6e90\uff0c\u5e76\u8003\u8651\u4e86\u670d\u52a1\u8fc1\u79fb\u6210\u672c\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u578b\u5143\u542f\u53d1\u5f0f\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u5728\u4e0d\u4f7f\u8282\u70b9\u8fc7\u8f7d\u7684\u60c5\u51b5\u4e0b\u5206\u914d\u4e91\u7cfb\u7edf\u670d\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u5e76\u6700\u5c0f\u5316\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u5143\u542f\u53d1\u5f0f\u8d1f\u8f7d\u5747\u8861\u5668\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e91\u7cfb\u7edf\u4e2d\u7684\u670d\u52a1\u5206\u914d\u95ee\u9898\u3002"}}
{"id": "2511.12756", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12756", "abs": "https://arxiv.org/abs/2511.12756", "authors": ["Sungjun Seo", "Kooktae Lee"], "title": "Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport", "comment": "Author Accepted Manuscript (AAM) of a paper accepted for publication in IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.\n  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.\n  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a D2OC\uff08Density-Driven Optimal Control\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u95ee\u9898\uff0c\u5e76\u8003\u8651\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5404\u79cd\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u5747\u5300\u8986\u76d6\u7b56\u7565\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u540c\u533a\u57df\u9700\u8981\u4e0d\u540c\u5173\u6ce8\u5ea6\u7684\u9700\u6c42\uff0c\u800c\u73b0\u6709\u7684\u975e\u5747\u5300\u65b9\u6cd5\u7f3a\u4e4f\u6700\u4f18\u6027\u4fdd\u8bc1\u6216\u672a\u80fd\u8003\u8651\u667a\u80fd\u4f53\u52a8\u529b\u5b66\u3001\u6709\u9650\u64cd\u4f5c\u65f6\u95f4\u3001\u667a\u80fd\u4f53\u6570\u91cf\u548c\u5206\u5e03\u5f0f\u6267\u884c\u7b49\u73b0\u5b9e\u7ea6\u675f\u3002", "method": "D2OC \u6846\u67b6\u7ed3\u5408\u4e86\u6700\u4f18\u8f93\u8fd0\u7406\u8bba\u548c\u591a\u667a\u80fd\u4f53\u8986\u76d6\u63a7\u5236\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u4efb\u52a1\u7279\u5b9a\u7684\u53c2\u8003\u5bc6\u5ea6\u56fe\u8c03\u6574\u5176\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u51b3\u4e00\u4e2a\u5305\u542b\u7269\u7406\u548c\u64cd\u4f5c\u7ea6\u675f\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6765\u786e\u7acb\u6700\u4f18\u6027\uff0c\u5e76\u4ece\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u4e2d\u63a8\u5bfc\u51fa\u63a7\u5236\u8f93\u5165\uff0c\u4ece\u800c\u5f97\u5230\u7ebf\u6027\u7cfb\u7edf\u7684\u95ed\u5f0f\u89e3\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u901a\u7528\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u6570\u636e\u5171\u4eab\u673a\u5236\u6765\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cD2OC \u5728\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u5b9e\u65bd\u7684\u53ef\u884c\u6027\u3002", "conclusion": "D2OC \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u95ee\u9898\uff0c\u5e76\u5728\u8003\u8651\u5404\u79cd\u5b9e\u9645\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u3002"}}
{"id": "2511.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12184", "abs": "https://arxiv.org/abs/2511.12184", "authors": ["Jun Huo", "Kehan Xu", "Chengyao Li", "Yu Cao", "Jie Zuo", "Xinxing Chen", "Jian Huang"], "title": "Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance", "comment": null, "summary": "In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u6d6e\u52a8\u57fa\u5e95\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u548c\u5e72\u6270\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u5177\u6709\u6d6e\u52a8\u57fa\u5e95\u7684\u8d85\u989d\u673a\u5668\u4eba\u817f\uff08SRL\uff09\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u53d8\u963b\u6297\u63a7\u5236\uff08VIC\uff09\u65b9\u6cd5\u6765\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\uff0c\u7279\u522b\u662f\u5728\u5916\u90e8\u529b\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u72b6\u6001\u4e0b\u672a\u77e5\u7684\u73af\u5883\u5e72\u6270\u3002\u4e3aSRL\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u7a33\u5b9a\u4fdd\u8bc1\u7684\u963b\u6297\u53c2\u6570\u751f\u6210\u7f51\u7edc\uff0c\u4ee5\u5b9e\u73b0\u51cf\u9707\u548c\u9ad8\u521a\u5ea6\u652f\u6491\u3002", "motivation": "\u5728\u4eba\u673a\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u5728\u5b58\u5728\u5185\u90e8\u548c\u5916\u90e8\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u529b\u63a7\u5236\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u4e3a\u4e00\u79cd\u5178\u578b\u7684\u677e\u6563\u8026\u5408\u6d6e\u52a8\u57fa\u5e95\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u8d85\u989d\u673a\u5668\u4eba\u817f\uff08SRL\uff09\u7cfb\u7edf\u7279\u522b\u5bb9\u6613\u53d7\u5230\u5185\u90e8\u5e72\u6270\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u677e\u6563\u8026\u5408SRL\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u6765\u9002\u5e94\u52a8\u6001\u626d\u77e9\u8f93\u5165\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u53d8\u963b\u6297\u63a7\u5236\uff08VIC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\u6765\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u72b6\u6001\u4e0b\u672a\u77e5\u7684\u73af\u5883\u5e72\u6270\u3002\u4e3aSRL\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u7a33\u5b9a\u4fdd\u8bc1\u7684\u963b\u6297\u53c2\u6570\u751f\u6210\u7f51\u7edc\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u67d4\u6027\u72b6\u6001\u4e0b\u4fdd\u6301\u5e73\u7a33\u4fe1\u53f7\u8fc7\u6e21\uff0c\u5e76\u5728\u521a\u6027\u72b6\u6001\u4e0b\u63d0\u4f9b\u5f3a\u5927\u652f\u6491\u529b\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9002\u5e94\u4ea4\u4e92\u4e2d\u7684\u4e2a\u4f53\u6b65\u6001\u53d8\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12524", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12524", "abs": "https://arxiv.org/abs/2511.12524", "authors": ["Sanghyo Park", "Seuk Lee", "Keunyoung Lee", "Minhyeok Kim", "Donggyu Kim"], "title": "Autonomously Designed Pulses for Precise, Site-Selective Control of Atomic Qubits", "comment": null, "summary": "Quantum computers based on cold-atom arrays offer long-lived qubits with programmable connectivity, yet their progress toward fault-tolerant operation is limited by the relatively low fidelity of site-selective local control. We introduce an artificial-intelligence (AI) framework that overcomes this limitation. Trained on atom-laser dynamics, a deep neural network autonomously designs composite pulses that improve local control fidelities tenfold while remaining compatible with existing control hardware. We further demonstrate the robustness of these pulses against optical aberrations and beam misalignment. This approach establishes AI-trained pulse compilation for high-fidelity qubit control and can be readily extended to other atom-like platforms, such as trapped ions and solid-state color centers.", "AI": {"tldr": "AI\u6846\u67b6\u901a\u8fc7\u8bbe\u8ba1\u590d\u5408\u8109\u51b2\uff0c\u5c06\u51b7\u539f\u5b50\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u5c40\u90e8\u63a7\u5236\u4fdd\u771f\u5ea6\u63d0\u9ad8\u4e86\u5341\u500d\uff0c\u514b\u670d\u4e86\u73b0\u6709\u786c\u4ef6\u7684\u9650\u5236\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u5149\u5b66\u50cf\u5dee\u548c\u5149\u675f\u672a\u5bf9\u51c6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u51b7\u539f\u5b50\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u4fdd\u771f\u5ea6\u53d7\u9650\u4e8e\u4f4e\u4fdd\u771f\u5ea6\u7684\u4f4d\u70b9\u9009\u62e9\u6027\u5c40\u57df\u63a7\u5236\uff0c\u800c\u957f\u671f\u53d1\u5c55\u7684\u5bb9\u9519\u64cd\u4f5c\u53d7\u5230\u6b64\u74f6\u9888\u7684\u5236\u7ea6\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u539f\u5b50-\u6fc0\u5149\u52a8\u529b\u5b66\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u81ea\u4e3b\u8bbe\u8ba1\u590d\u5408\u8109\u51b2\uff0c\u4ee5\u63d0\u9ad8\u5c40\u57df\u63a7\u5236\u4fdd\u771f\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684AI\u6846\u67b6\u5c06\u5c40\u90e8\u63a7\u5236\u4fdd\u771f\u5ea6\u63d0\u9ad8\u4e86\u5341\u500d\uff0c\u5e76\u5c55\u793a\u4e86\u8109\u51b2\u5bf9\u5149\u5b66\u50cf\u5dee\u548c\u5149\u675f\u672a\u5bf9\u51c6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AI\u8bad\u7ec3\u7684\u8109\u51b2\u7f16\u8bd1\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u7684\u4e00\u79cd\u53ef\u884c\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u7c7b\u539f\u5b50\u91cf\u5b50\u5e73\u53f0\u3002"}}
{"id": "2511.11739", "categories": ["cs.DC", "cond-mat.mtrl-sci", "cs.LG", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.11739", "abs": "https://arxiv.org/abs/2511.11739", "authors": ["Christina Schenk", "Miguel Hern\u00e1ndez-del-Valle", "Luis Calero-Lumbreras", "Marcus Noack", "Maciej Haranczyk"], "title": "Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows", "comment": "17 pages, 4 figures, 2 tables", "summary": "Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.", "AI": {"tldr": "\u8bbe\u5907\u95f4\u7684\u5b9e\u9a8c\u566a\u58f0\u5dee\u5f02\u4f1a\u4e25\u91cd\u5f71\u54cd\u53ef\u91cd\u590d\u6027\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u5316\u3001\u9ad8\u901a\u91cf\u7cfb\u7edf\u4e2d\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u566a\u58f0\u611f\u77e5\u51b3\u7b56\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u91cf\u5316\u548c\u5efa\u6a21\u8bbe\u5907\u7279\u5b9a\u7684\u566a\u58f0\u5206\u5e03\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u7ba1\u7406\u53d8\u5f02\u6027\uff0c\u901a\u8fc7\u4f7f\u7528\u5206\u5e03\u5206\u6790\u548c\u805a\u7c7b\u6765\u9009\u62e9\u5355\u8bbe\u5907\u6216\u591a\u8bbe\u5907\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3001\u53ef\u91cd\u590d\u6027\u548c\u6548\u7387\u3002", "motivation": "\u8bbe\u5907\u95f4\u7684\u5b9e\u9a8c\u566a\u58f0\u5dee\u5f02\u4f1a\u4e25\u91cd\u5f71\u54cd\u53ef\u91cd\u590d\u6027\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u5316\u3001\u9ad8\u901a\u91cf\u7cfb\u7edf\u4e2d\uff0c\u5728\u5efa\u7b513D\u6253\u5370\u7b49\u89c4\u6a21\u5316\u5e94\u7528\u4e2d\uff0c\u566a\u58f0\u53ef\u80fd\u5bfc\u81f4\u7ed3\u6784\u6216\u7ecf\u6d4e\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5206\u6790\u548c\u6210\u5bf9\u53d1\u6563\u5ea6\u91cf\u4e0e\u805a\u7c7b\u6765\u9009\u62e9\u5355\u8bbe\u5907\u548c\u9c81\u68d2\u7684\u591a\u8bbe\u5907\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5bf9\u4e09\u4e2a\u76f8\u540c\u76843D\u6253\u5370\u673a\u8fdb\u884c\u5b9e\u9a8c\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u51cf\u5c11\u5197\u4f59\u3001\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5b9e\u9a8c\u5e73\u53f0\u4e2d\u7684\u7cbe\u5ea6\u548c\u8d44\u6e90\u611f\u77e5\u4f18\u5316\u5efa\u7acb\u4e86\u8303\u4f8b\u3002"}}
{"id": "2511.11908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11908", "abs": "https://arxiv.org/abs/2511.11908", "authors": ["Afifa Khaled", "Ebrahim Hamid Sumiea"], "title": "PI-NAIM: Path-Integrated Neural Adaptive Imputation Model", "comment": null, "summary": "Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model", "AI": {"tldr": "PI-NAIM\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u6210\u50cf\u548c\u591a\u6a21\u6001\u4e34\u5e8a\u8bbe\u7f6e\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u5230\u4f18\u5316\u7684\u63d2\u8865\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63d2\u8865\u65b9\u6cd5\u5728\u8868\u793a\u80fd\u529b\u6216\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u533b\u5b66\u6210\u50cf\u548c\u591a\u6a21\u6001\u4e34\u5e8a\u8bbe\u7f6e\u4e2d\u5e38\u89c1\u7684\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u3002", "method": "PI-NAIM\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u8def\u5f84\u67b6\u6784\uff0c\u6839\u636e\u7f3a\u5931\u7684\u590d\u6742\u6027\u52a8\u6001\u5730\u5c06\u6837\u672c\u8def\u7531\u5230\u4f18\u5316\u7684\u63d2\u8865\u65b9\u6cd5\u3002\u5b83\u5305\u62ec\uff1a1\uff09\u667a\u80fd\u8def\u5f84\u8def\u7531\uff0c\u5c06\u4f4e\u7f3a\u5931\u6837\u672c\u8def\u7531\u5230MICE\uff08\u4e00\u79cd\u7edf\u8ba1\u63d2\u8865\u65b9\u6cd5\uff09\uff0c\u5c06\u590d\u6742\u6a21\u5f0f\u8def\u7531\u5230GAIN\uff08\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5206\u6790\u7684\u795e\u7ecf\u7f51\u7edc\uff09\uff1b2\uff09\u8de8\u8def\u5f84\u6ce8\u610f\u529b\u878d\u5408\uff0c\u5229\u7528\u611f\u77e5\u7f3a\u5931\u7684\u5d4c\u5165\u6765\u667a\u80fd\u5730\u7ec4\u5408\u4e24\u4e2a\u5206\u652f\uff1b3\uff09\u8054\u5408\u4f18\u5316\u63d2\u8865\u7cbe\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5728MIMIC-III\u548c\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPI-NAIM\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cRMSE\u4e3a0.108\uff08\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7ebf\u65b9\u6cd5\u7684RMSE\u4e3a0.119-0.152\uff09\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u6b7b\u4ea1\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684AUROC\u4e3a0.812\u3002", "conclusion": "PI-NAIM\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u5904\u7406\u4e0d\u5b8c\u6574\u4f20\u611f\u5668\u6d4b\u91cf\u3001\u7f3a\u5931\u6a21\u6001\u6216\u635f\u574f\u8f93\u5165\u7684\u89c6\u89c9\u7ba1\u9053\u4e2d\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13272", "abs": "https://arxiv.org/abs/2511.13272", "authors": ["Zeyang Sun", "Xidong Mu", "Shuai Han", "Sai Xu", "Michail Matthaiou"], "title": "Pinching-Antenna-Enabled Cognitive Radio Networks", "comment": "13 pages, 7 figures", "summary": "This paper investigates a pinching-antenna (PA)-enabled cognitive radio network, where both the primary transmitter (PT) and secondary transmitter (ST) are equipped with a single waveguide and multiple PAs to facilitate simultaneous spectrum sharing. Under a general Ricean fading channel model, a closed-form analytical expression for the average spectral efficiency (SE) achieved by PAs is first derived. Based on this, a sum-SE maximization problem is formulated to jointly optimize the primary and secondary pinching beamforming, subject to system constraints on the transmission power budgets, minimum antenna separation requirements, and feasible PA deployment regions. To address this non-convex problem, a three-stage optimization algorithm is developed to sequentially optimize both the PT and ST pinching beamforming, and the ST power control. For the PT and ST pinching beamforming optimization, the coarse positions of PA are first determined at the waveguide-level. Then, wavelength-level refinements achieve constructive signal combination at the intended user and destructive superposition at the unintended user. For the ST power control, a closed-form solution is derived. Simulation results demonstrate that i) PAs can achieve significant SE improvements over conventional fixed-position antennas; ii) the proposed pinching beamforming design achieves effective interference suppression and superior performance for both even and odd numbers of PAs; and iii) the developed three-stage optimization algorithm enables nearly orthogonal transmission between the primary and secondary networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u634f\u7f29\u5929\u7ebf\uff08PA\uff09\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u548c\u5e72\u6270\u6291\u5236\u3002", "motivation": "\u5728\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u4e2d\uff0c\u9700\u8981\u89e3\u51b3\u9891\u8c31\u5171\u4eab\u5e26\u6765\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u9891\u8c31\u5229\u7528\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\uff0c\u5206\u522b\u4f18\u5316PT\u548cST\u7684\u634f\u7f29\u6ce2\u675f\u6210\u5f62\u548cST\u529f\u7387\u63a7\u5236\u3002\u5728\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u4e2d\uff0c\u9996\u5148\u5728\u6ce2\u5bfc\u5c42\u9762\u786e\u5b9aPA\u7684\u7c97\u7565\u4f4d\u7f6e\uff0c\u7136\u540e\u901a\u8fc7\u6ce2\u957f\u5c42\u9762\u7684\u4f18\u5316\u5b9e\u73b0\u4fe1\u53f7\u7684\u6709\u6548\u53e0\u52a0\u548c\u5e72\u6270\u6291\u5236\u3002\u5bf9\u4e8eST\u529f\u7387\u63a7\u5236\uff0c\u63a8\u5bfc\u4e86\u95ed\u5f0f\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cPA\u76f8\u6bd4\u4f20\u7edf\u5929\u7ebf\u80fd\u663e\u8457\u63d0\u9ad8\u9891\u8c31\u6548\u7387\uff0c\u6240\u63d0\u51fa\u7684\u634f\u7f29\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u80fd\u6709\u6548\u6291\u5236\u5e72\u6270\uff0c\u5e76\u4e14\u4e09\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u8fd1\u4e4e\u6b63\u4ea4\u7684\u4f20\u8f93\u3002", "conclusion": "PA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5929\u7ebf\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u7684\u9891\u8c31\u6548\u7387\u548c\u6027\u80fd\u3002\u6240\u63d0\u51fa\u7684\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u9891\u8c31\u5171\u4eab\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.11729", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11729", "abs": "https://arxiv.org/abs/2511.11729", "authors": ["Ao Xu", "Han Zhao", "Weihao Cui", "Quan Chen", "Yukang Chen", "Shulai Zhang", "Shuang Chen", "Jiemin Jiang", "Zhibin Yu", "Minyi Guo"], "title": "Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.\n  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.", "AI": {"tldr": "Harli\u901a\u8fc7\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4efb\u52a1\u4e0eLLM\u89e3\u7801\u5b9e\u4f8b\u5171\u7f6e\uff0c\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u548c\u5fae\u8c03\u541e\u5410\u91cf\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u4e25\u683c\u7684QoS\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u7684LLM\u670d\u52a1\u7cfb\u7edf\u5728 disaggregating \u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u65f6\uff0c\u7531\u4e8e\u89e3\u7801\u5b9e\u4f8b\u7684\u5185\u5b58\u74f6\u9888\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6279\u5904\u7406\u4e0d\u8db3\uff0c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "Harli\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u7edf\u4e00\u7684\u5185\u5b58\u5206\u914d\u5668\u3001\u4e00\u4e2a\u4e24\u9636\u6bb5\u5ef6\u8fdf\u9884\u6d4b\u5668\u548c\u4e00\u4e2aQoS\u4fdd\u8bc1\u7684\u541e\u5410\u91cf\u6700\u5927\u5316\u8c03\u5ea6\u5668\uff0c\u5c06PEFT\u4efb\u52a1\u4e0eLLM\u89e3\u7801\u5b9e\u4f8b\u5171\u7f6e\uff0c\u4ee5\u5e94\u5bf9\u5185\u5b58\u9650\u5236\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHarli\u5c06\u5fae\u8c03\u541e\u5410\u91cf\u5e73\u5747\u63d0\u9ad8\u4e8646.2%\uff08\u6700\u9ad8\u53ef\u8fbe92.0%\uff09\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u63a8\u7406\u89e3\u7801\u7684\u4e25\u683cQoS\u3002", "conclusion": "Harli\u6210\u529f\u5730\u63d0\u9ad8\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u7684GPU\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684PEFT\u4efb\u52a1\u4e0e\u5185\u5b58\u5bc6\u96c6\u578b\u7684LLM\u89e3\u7801\u5b9e\u4f8b\u5171\u7f6e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12758", "categories": ["eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.12758", "abs": "https://arxiv.org/abs/2511.12758", "authors": ["Shih-Chi Liao", "Maziar S. Hemati", "Peter Seiler"], "title": "On Boundedness of Quadratic Dynamics with Energy-Preserving Nonlinearity", "comment": "9 pages, 1 figures", "summary": "Boundedness is an important property of many physical systems. This includes incompressible fluid flows, which are often modeled by quadratic dynamics with an energy-preserving nonlinearity. For such systems, Schlegel and Noack proposed a sufficient condition for boundedness utilizing quadratic Lyapunov functions. They also propose a necessary condition for boundedness aiming to provide a more complete characterization of boundedness in this class of models. The sufficient condition is based on Lyapunov theory and is true. Our paper focuses on this necessary condition. We use an independent proof to show that the condition is true for two dimensional systems. However, we provide a three dimensional counterexample to illustrate that the necessary condition fails to hold in higher dimensions. Our results highlight a theoretical gap in boundedness analysis and suggest future directions to address the conservatism.", "AI": {"tldr": "\u867d\u7136\u5145\u5206\u6761\u4ef6\u6210\u7acb\uff0c\u4f46\u5fc5\u8981\u6761\u4ef6\u5728\u4e09\u7ef4\u7cfb\u7edf\u4e2d\u5931\u6548\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5f25\u5408\u7406\u8bba\u5dee\u8ddd\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5177\u6709\u80fd\u91cf\u5b88\u6052\u975e\u7ebf\u6027\u7684\u4e8c\u6b21\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u6709\u754c\u6027\u3002", "method": "\u5229\u7528\u72ec\u7acb\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u8be5\u5fc5\u8981\u6761\u4ef6\u5bf9\u4e8e\u4e8c\u7ef4\u7cfb\u7edf\u6210\u7acb\u3002\u4f46\u901a\u8fc7\u6784\u9020\u4e00\u4e2a\u4e09\u7ef4\u53cd\u4f8b\uff0c\u8bc1\u660e\u4e86\u8be5\u6761\u4ef6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4e0d\u6210\u7acb\u3002", "result": "\u4e8c\u7ef4\u7cfb\u7edf\u6ee1\u8db3\u8be5\u5fc5\u8981\u6761\u4ef6\uff0c\u4f46\u4e09\u7ef4\u7cfb\u7edf\u5b58\u5728\u53cd\u4f8b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6709\u754c\u6027\u5206\u6790\u4e2d\u7684\u7406\u8bba\u7f3a\u53e3\uff0c\u5e76\u4e3a\u89e3\u51b3\u4fdd\u5b88\u6027\u95ee\u9898\u6307\u660e\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.12186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12186", "abs": "https://arxiv.org/abs/2511.12186", "authors": ["Jun Huo", "Jian Huang", "Jie Zuo", "Bo Yang", "Zhongzheng Fu", "Xi Li", "Samer Mohammed"], "title": "Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization", "comment": null, "summary": "Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u8bbe\u8ba1\u7406\u8bba\uff0c\u7528\u4e8e\u8bbe\u8ba1\u901a\u7528\u578b\u8d85\u989d\u673a\u5668\u4eba\u80a2\u4f53\uff08SRLs\uff09\uff0c\u8be5\u7406\u8bba\u6574\u5408\u4e86\u6293\u63e1\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u884c\u8d70\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u7528\u4e8e\u5750\u5230\u7ad9\uff08STS\uff09\u8fd0\u52a8\u7684\u652f\u6491\u529b\u4ee5\u53ca\u6574\u4f53\u8d28\u91cf\u548c\u60ef\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u706b\u8424\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6293\u63e1\u6210\u529f\u7387\u3001\u884c\u8d70\u548cSTS\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u5747\u5f97\u5230\u6539\u5584\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u578bSRL\u8bbe\u5907\uff0c\u6ee1\u8db3\u4e0a\u4e0b\u80a2\u7684\u591a\u6837\u5316\u529f\u80fd\u9700\u6c42\uff0c\u5e76\u5bfb\u6c42\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u8bbe\u8ba1\u7406\u8bba\uff0c\u6574\u5408\u6293\u63e1\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u884c\u8d70\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u5750\u5230\u7ad9\uff08STS\uff09\u652f\u6491\u529b\u3001\u8d28\u91cf\u548c\u60ef\u6027\u3002\u5f00\u53d1\u4e86\u4f7f\u7528\u692d\u7403\u4f53\u8868\u793a\u5de5\u4f5c\u7a7a\u95f4\u7684\u51e0\u4f55\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u3002\u91c7\u7528\u591a\u4e9a\u7fa4\u4fee\u6b63\u706b\u8424\u7b97\u6cd5\u5904\u7406MOO\u4efb\u52a1\u3002", "result": "\u5728\u516d\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u548c\u4e24\u540d\u762b\u75ea\u60a3\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u4f18\u5316\u524d\u76f8\u6bd4\uff0c\u5e73\u5747\u6293\u63e1\u6210\u529f\u7387\u63d0\u9ad8\u4e867.2%\uff0c\u884c\u8d70\u548cSTS\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u5206\u522b\u5e73\u5747\u964d\u4f4e\u4e8612.7%\u548c25.1%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u7406\u8bba\u4e3a\u591a\u529f\u80fdSRL\u673a\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12537", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12537", "abs": "https://arxiv.org/abs/2511.12537", "authors": ["You-Cai Lv", "Yu-Jia Zhu", "Zong-Quan Zhou", "Chuan-Feng Li", "Guang-Can Guo"], "title": "Minute-Scale Photonic Quantum Memory", "comment": null, "summary": "Long-lived storage of single photons is a fundamental requirement for enabling quantum communication and foundational tests of quantum physics over extended distances. While the implementation of a global-scale quantum network requires quantum storage times on the order of seconds to minutes, existing photonic quantum memories have so far been limited to subsecond lifetimes. Although $^{151}$Eu$^{3+}$:Y$_2$SiO$_5$ crystals exhibit substantially extended spin coherence times at the `magic' magnetic field, the concomitant weak optical absorption has until now prevented single-photon storage. Here, we overcome this challenge by integrating a noiseless photon echo protocol -- which makes full use of the crystal's natural absorption for photonic storage -- with a universally robust dynamical decoupling sequence incorporating adiabatic pulses to protect nuclear spin coherence, enabling long-lived quantum storage at the `magic' magnetic field. At a storage time of 5.6 s, we achieve a time-bin qubit storage fidelity of 88.0 $\\pm$ 2.1%, surpassing the maximum fidelity attainable via classical strategies. Our device reaches a $1/e$ storage lifetime of 27.6 $\\pm$ 0.6 s, enabling single-photon-level storage for 42 s with a signal-to-noise ratio greater than unity. This work establishes photonic quantum memory in the minute-scale regime, laying a solid foundation for global-scale quantum network and deep-space quantum experiments.", "AI": {"tldr": "\u5728151Eu3+:Y2SiO5\u6676\u4f53\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u5149\u5b50\u56de\u58f0\u534f\u8bae\u548c\u7edd\u70ed\u8109\u51b2\u52a8\u529b\u5b66\u89e3\u8026\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u5206\u949f\u7ea7\u957f\u5bff\u547d\u5355\u5149\u5b50\u5b58\u50a8\uff0c\u4fdd\u771f\u5ea6\u8fbe\u523088.0\u00b12.1%\uff0c\u5b58\u50a8\u5bff\u547d\u8fbe\u523027.6\u00b10.6\u79d2\uff0c\u4e3a\u6784\u5efa\u5168\u7403\u91cf\u5b50\u7f51\u7edc\u548c\u6df1\u7a7a\u91cf\u5b50\u5b9e\u9a8c\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5b9e\u73b0\u957f\u8ddd\u79bb\u91cf\u5b50\u901a\u4fe1\u548c\u91cf\u5b50\u7269\u7406\u57fa\u7840\u68c0\u9a8c\uff0c\u9700\u8981\u79d2\u81f3\u5206\u949f\u7ea7\u7684\u91cf\u5b50\u5b58\u50a8\uff0c\u4f46\u73b0\u6709\u5149\u5b50\u91cf\u5b50\u5b58\u50a8\u5bff\u547d\u4e0d\u8db3\u3002", "method": "\u5728\u2018\u9b54\u5e7b\u2019\u78c1\u573a\u4e0b\uff0c\u5229\u7528151Eu3+:Y2SiO5\u6676\u4f53\uff0c\u7ed3\u5408\u5229\u7528\u6676\u4f53\u81ea\u7136\u5438\u6536\u8fdb\u884c\u5149\u5b50\u5b58\u50a8\u7684\u5149\u5b50\u56de\u58f0\u534f\u8bae\uff0c\u4ee5\u53ca\u5305\u542b\u7edd\u70ed\u8109\u51b2\u4ee5\u4fdd\u62a4\u6838\u81ea\u65cb\u76f8\u5e72\u6027\u7684\u52a8\u529b\u5b66\u89e3\u8026\u5e8f\u5217\u3002", "result": "\u57285.6\u79d2\u7684\u5b58\u50a8\u65f6\u95f4\u4e0b\uff0c\u5b9e\u73b0\u4e8688.0\u00b12.1%\u7684\u65f6\u95f4-\u4e8c\u80fd\u7ea7\u91cf\u5b50\u6bd4\u7279\u5b58\u50a8\u4fdd\u771f\u5ea6\uff0c\u8d85\u8d8a\u4e86\u7ecf\u5178\u7b56\u7565\u7684\u6700\u5927\u4fdd\u771f\u5ea6\u3002\u5b58\u50a8\u5bff\u547d\u76841/e\u8870\u51cf\u65f6\u95f4\u4e3a27.6\u00b10.6\u79d2\uff0c\u5b9e\u73b0\u4e86\u5927\u4e8e1\u7684\u4fe1\u566a\u6bd4\u4e0b\u957f\u8fbe42\u79d2\u7684\u5355\u5149\u5b50\u5b58\u50a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u5206\u949f\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u5149\u5b50\u91cf\u5b50\u5b58\u50a8\uff0c\u4e3a\u5168\u7403\u91cf\u5b50\u7f51\u7edc\u548c\u6df1\u7a7a\u91cf\u5b50\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11910", "abs": "https://arxiv.org/abs/2511.11910", "authors": ["Siyou Li", "Huanan Wu", "Juexi Shao", "Yinghao Ma", "Yujian Gan", "Yihao Luo", "Yuwei Wang", "Dong Nie", "Lu Wang", "Wengqing Wu", "Le Zhang", "Massimo Poesio", "Juntao Yu"], "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models", "comment": null, "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.\n  We will make all code, data, and trained models' weights publicly available.", "AI": {"tldr": "QTSplus\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e0e\u6587\u672c\u67e5\u8be2\u6700\u76f8\u5173\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u662f\u6311\u6218\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u968f\u89c6\u9891\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u6210\u672c\u3001\u5185\u5b58\u548c\u5ef6\u8fdf\u6025\u5267\u589e\u52a0\u3002", "method": "\u63d0\u51faQTSplus\u6a21\u5757\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u89c6\u89c9\u4ee4\u724c\u6253\u5206\uff0c\u6839\u636e\u67e5\u8be2\u590d\u6742\u6027\u9884\u6d4b\u4ee4\u724c\u4fdd\u7559\u9884\u7b97\uff0c\u5e76\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u76f4\u901a\u4f30\u8ba1\u5668\uff08\u8bad\u7ec3\u65f6\uff09\u548c\u786c\u95e8\u63a7\uff08\u63a8\u7406\u65f6\uff09\u9009\u62e9Top-n\u4ee4\u724c\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5c0f\u578b\u518d\u7f16\u7801\u5668\u7ed3\u5408\u7edd\u5bf9\u65f6\u95f4\u4fe1\u606f\u6765\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\uff0c\u5b9e\u73b0\u79d2\u7ea7\u5b9a\u4f4d\u548c\u5168\u5c40\u8986\u76d6\u3002", "result": "QTSplus\u5c06\u89c6\u89c9\u6d41\u538b\u7f29\u9ad8\u8fbe89%\uff0c\u5c06\u957f\u89c6\u9891\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e28%\u3002\u5728\u516b\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u6027\u4e0e\u539f\u59cbQwen\u6a21\u578b\u63a5\u8fd1\uff0c\u5728TempCompass\u7684\u65b9\u5411\u548c\u987a\u5e8f\u51c6\u786e\u6027\u4e0a\u5206\u522b\u63d0\u5347\u4e86+20.5\u548c+5.6\u3002\u5728\u957f\u89c6\u9891\u573a\u666f\u4e0b\uff0cQTSplus\u6709\u6548\u4e14\u901a\u7528\uff0c\u80fd\u5728\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u8bc1\u636e\u7684\u540c\u65f6\u6269\u5c55MLLMs\u3002", "conclusion": "QTSplus\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u8bc1\u636e\u7684\u540c\u65f6\uff0c\u5c06MLLMs\u6269\u5c55\u5230\u5b9e\u9645\u7684\u957f\u89c6\u9891\u573a\u666f\u3002"}}
{"id": "2511.13336", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13336", "abs": "https://arxiv.org/abs/2511.13336", "authors": ["Maolin Li", "Feng Shu", "Minghao Chen", "Cunhua Pan", "Fuhui Zhou", "Yongpeng Wu", "Liang Yang"], "title": "Sensing-enabled Secure Rotatable Array System Enhanced by Multi-Layer Transmitting RIS", "comment": null, "summary": "Programmable metasurfaces and adjustable antennas are promising technologies. The security of a rotatable array system is investigated in this paper. A dual-base-station (BS) architecture is adopted, in which the BSs collaboratively perform integrated sensing of the eavesdropper (the target) and communication tasks. To address the security challenge when the sensing target is located on the main communication link, the problem of maximizing the secrecy rate (SR) under sensing signal-to-interference-plus-noise ratio requirements and discrete constraints is formulated. This problem involves the joint optimization of the array pose, the antenna distribution on the array surface, the multi-layer transmitting RIS phase matrices, and the beamforming matrices, which is non-convex. To solve this challenge, an two-stage online algorithm based on the generalized Rayleigh quotient and an offline algorithm based on the Multi-Agent Deep Deterministic Policy Gradient are proposed. Simulation results validate the effectiveness of the proposed algorithms. Compared to conventional schemes without array pose adjustment, the proposed approach achieves approximately 22\\% improvement in SR. Furthermore, array rotation provides higher performance gains than position changes.", "AI": {"tldr": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9635\u5217\u59ff\u6001\u3001\u5929\u7ebf\u5206\u5e03\u3001RIS\u76f8\u4f4d\u77e9\u9635\u548c\u6ce2\u675f\u6210\u5f62\u77e9\u9635\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5728\u7ebf\u7b97\u6cd5\u548c\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7684\u79bb\u7ebf\u7b97\u6cd5\uff0c\u4ee5\u5728\u53cc\u57fa\u7ad9\u534f\u4f5c\u611f\u77e5\u548c\u901a\u4fe1\u7684\u573a\u666f\u4e0b\uff0c\u5728\u6ee1\u8db3\u611f\u77e5\u4fe1\u566a\u6bd4\u8981\u6c42\u548c\u79bb\u6563\u7ea6\u675f\u7684\u6761\u4ef6\u4e0b\u6700\u5927\u5316\u4fdd\u5bc6\u7387\uff0c\u5e76\u53d6\u5f97\u4e86\u7ea622%\u7684\u4fdd\u5bc6\u7387\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u65cb\u8f6c\u9635\u5217\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f20\u611f\u5668\u4f4d\u4e8e\u4e3b\u8981\u901a\u4fe1\u94fe\u8def\u4e0a\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u89e3\u51b3\u5b89\u5168\u901a\u4fe1\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5728\u7ebf\u7b97\u6cd5\uff08\u57fa\u4e8e\u5e7f\u4e49\u745e\u5229\u5546\uff09\u548c\u4e00\u79cd\u79bb\u7ebf\u7b97\u6cd5\uff08\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff09\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u9635\u5217\u59ff\u6001\u3001\u5929\u7ebf\u5206\u5e03\u3001\u591a\u5c42RIS\u76f8\u4f4d\u77e9\u9635\u548c\u6ce2\u675f\u6210\u5f62\u77e9\u9635\uff0c\u4ee5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e0d\u8fdb\u884c\u9635\u5217\u59ff\u6001\u8c03\u6574\u7684\u4f20\u7edf\u65b9\u6848\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u4fdd\u5bc6\u7387\u63d0\u9ad8\u4e86\u7ea622%\uff0c\u5e76\u4e14\u9635\u5217\u65cb\u8f6c\u6bd4\u4f4d\u7f6e\u53d8\u5316\u80fd\u5e26\u6765\u66f4\u9ad8\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u53cc\u57fa\u7ad9\u534f\u4f5c\u611f\u77e5\u548c\u901a\u4fe1\u7cfb\u7edf\u7684\u4fdd\u5bc6\u7387\uff0c\u5e76\u4e14\u9635\u5217\u65cb\u8f6c\u662f\u4e00\u79cd\u6709\u6548\u7684\u63d0\u5347\u6027\u80fd\u7684\u624b\u6bb5\u3002"}}
{"id": "2511.11733", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11733", "abs": "https://arxiv.org/abs/2511.11733", "authors": ["Jingwei Song", "Wanyi Chen", "Xinyuan Song", "Max", "Chris Tong", "Gufeng Chen", "Tianyi Zhao", "Eric Yang", "Bill Shi", "Lynn Ai"], "title": "Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput", "comment": "6 pages, 2 figures, 2 tables. Uses ICML 2025 style", "summary": "Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.", "AI": {"tldr": "Speculative decoding is enhanced for decentralized LLM inference through a framework called DSD, which parallelizes token verification across nodes and uses an adaptive strategy to improve speed by 15-20% without retraining, achieving up to 2.59x speedup on benchmarks.", "motivation": "The behavior of speculative decoding in decentralized systems, where network latency is a major bottleneck, was under-characterized. This paper aims to address this by adapting speculative decoding for decentralized inference.", "method": "The paper introduces Decentralized Speculative Decoding (DSD), a plug-and-play framework that enables parallel verification of multiple candidate tokens across distributed nodes. It also incorporates an adaptive speculative verification strategy that adjusts acceptance thresholds based on token-level semantic importance.", "result": "DSD theoretically reduces cross-node communication cost and practically achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, outperforming the Eagle3 baseline while maintaining accuracy. The adaptive strategy provides an additional 15% to 20% end-to-end speedup without retraining.", "conclusion": "Adapting speculative decoding for decentralized execution is a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference without requiring model retraining or architectural changes."}}
{"id": "2511.12826", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12826", "abs": "https://arxiv.org/abs/2511.12826", "authors": ["Sahel Vahedi Noori", "Bin Hu", "Geir Dullerud", "Peter Seiler"], "title": "Discrete-Time Stability Analysis of ReLU Feedback Systems via Integral Quadratic Constraints", "comment": null, "summary": "This paper analyzes internal stability of a discrete-time feedback system with a ReLU nonlinearity. This feedback system is motivated by recurrent neural networks. We first review existing static quadratic constraints (QCs) for slope-restricted nonlinearities. Next, we derive hard integral quadratic constraints (IQCs) for scalar ReLU by using finite impulse filters and structured matrices. These IQCs are combined with a dissipation inequality leading to an LMI condition that certifies internal stability. We show that our new dynamic IQCs for ReLU are a superset of the well-known Zames-Falb IQCs specified for slope-restricted nonlinearities. Numerical results show that the proposed hard IQCs give less conservative stability margins than Zames-Falb multipliers and prior static QC methods, sometimes dramatically so.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u5177\u6709ReLU\u975e\u7ebf\u6027\u7684\u79bb\u6563\u65f6\u95f4\u53cd\u9988\u7cfb\u7edf\uff0c\u5229\u7528\u65b0\u7684\u52a8\u6001\u79ef\u5206\u4e8c\u6b21\u7ea6\u675f\uff08IQCs\uff09\u548c\u8017\u6563\u4e0d\u7b49\u5f0f\uff0c\u63a8\u5bfc\u51faLMI\u6761\u4ef6\u6765\u8bc1\u660e\u5185\u90e8\u7a33\u5b9a\u6027\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982Zames-Falb IQCs\u548c\u9759\u6001\u4e8c\u6b21\u7ea6\u675f\uff09\u66f4\u4e0d\u4fdd\u5b88\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u6e90\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u7684\u5185\u90e8\u7a33\u5b9a\u6027\u5206\u6790\u3002", "method": "\u672c\u6587\u9996\u5148\u56de\u987e\u4e86\u73b0\u6709\u7684\u7528\u4e8e\u659c\u7387\u9650\u5236\u975e\u7ebf\u6027\u7684\u9759\u6001\u4e8c\u6b21\u7ea6\u675f\uff08QCs\uff09\u3002\u7136\u540e\uff0c\u5229\u7528\u6709\u9650\u8109\u51b2\u54cd\u5e94\u6ee4\u6ce2\u5668\u548c\u7ed3\u6784\u5316\u77e9\u9635\uff0c\u4e3aReLU\u63a8\u5bfc\u51fa\u4e86\u786c\u79ef\u5206\u4e8c\u6b21\u7ea6\u675f\uff08IQCs\uff09\u3002\u8fd9\u4e9bIQCs\u4e0e\u8017\u6563\u4e0d\u7b49\u5f0f\u7ed3\u5408\uff0c\u5f97\u5230\u4e86\u4e00\u4e2a\u8bc1\u660e\u5185\u90e8\u7a33\u5b9a\u6027\u7684LMI\u6761\u4ef6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u786cIQCs\u6bd4Zames-Falb\u4e58\u6570\u548c\u5148\u524d\u7684\u9759\u6001QC\u65b9\u6cd5\u5177\u6709\u66f4\u5c0f\u7684\u4fdd\u5b88\u6027\u7a33\u5b9a\u6027\u88d5\u5ea6\uff0c\u6709\u65f6\u751a\u81f3\u975e\u5e38\u663e\u8457\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u52a8\u6001IQCs\u4e3aReLU\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u7cbe\u786e\u7684\u5185\u90e8\u7a33\u5b9a\u6027\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u83b7\u5f97\u66f4\u4f18\u7684\u7a33\u5b9a\u6027\u88d5\u5ea6\u3002"}}
{"id": "2511.12203", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12203", "abs": "https://arxiv.org/abs/2511.12203", "authors": ["Antony Thomas", "Fulvio Mastrogiovanni", "Marco Baglietto"], "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps", "comment": "Robotics and Autonomous Systems", "summary": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u4f4d\u79fb\u7ea6\u675f\u6216\u969c\u788d\u7269\u6765\u5b9e\u73b0\u53ef\u884c\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u8ba1\u7b97\u7a7f\u8fc7\u969c\u788d\u7269\u7684\u8f68\u8ff9\u5e76\u6700\u5c0f\u5316\u76ee\u6807\u51fd\u6570\uff0c\u7136\u540e\u4f4d\u79fb\u969c\u788d\u7269\u4ee5\u4f7f\u8f68\u8ff9\u65e0\u78b0\u649e\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u4e24\u7c7b\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u4e0a\u6210\u529f\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u884c\u8def\u5f84\u7684\u5bfb\u627e\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1. \u8ba1\u7b97\u7a7f\u8fc7\u969c\u788d\u7269\u7684\u8f68\u8ff9\u5e76\u6700\u5c0f\u5316\u76ee\u6807\u51fd\u6570\u30022. \u4f4d\u79fb\u969c\u788d\u7269\uff0c\u4f7f\u8f68\u8ff9\u65e0\u78b0\u649e\u3002", "result": "\u5728\u4e24\u7c7b\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u4e0a\u6210\u529f\u6f14\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12546", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.12546", "abs": "https://arxiv.org/abs/2511.12546", "authors": ["Stav Lotan", "Hugo Defienne", "Ronen Talmon", "Guy Bartal"], "title": "Sparsity-Driven Entanglement Detection in High-Dimensional Quantum States", "comment": "8 pages, 10 figures", "summary": "The characterization of high-dimensional quantum entanglement is crucial for advanced quantum computing and quantum information algorithms. Traditional methods require extensive data acquisition and suffer from limited visibility due to experimental noise. Here, we introduce a sparsity-driven framework to enhance the detection and certification of high-dimensional entanglement in spatially entangled photon pairs. By applying $\\ell_1$-regularized reconstruction to sample covariance matrices obtained from measurements on photons produced via spontaneous parametric down-conversion (SPDC) measurements, we enhance the visibility of the correlation signal while suppressing noise. We demonstrate, using a position-momentum Einstein-Podolsky-Rosen (EPR) entanglement criterion, that this approach enables certification of an entanglement dimensionality that cannot be achieved without regularization. Our method is scalable, simple to use and compatible with existing quantum-optics platforms, thus paves the way for efficient, real-time analysis of high-dimensional quantum states.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u6027\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u9ad8\u7ef4\u91cf\u5b50\u7ea0\u7f20\u7684\u68c0\u6d4b\u548c\u8ba4\u8bc1\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7a7a\u95f4\u7ea0\u7f20\u5149\u5b50\u5bf9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635\u8fdb\u884c$\\|1$-\u6b63\u5219\u5316\u91cd\u5efa\uff0c\u80fd\u5728\u6291\u5236\u566a\u58f0\u7684\u540c\u65f6\u63d0\u5347\u76f8\u5173\u6027\u4fe1\u53f7\u7684\u53ef\u89c1\u5ea6\uff0c\u5e76\u6210\u529f\u901a\u8fc7EPR\u7ea0\u7f20\u5224\u636e\u8ba4\u8bc1\u4e86\u66f4\u9ad8\u7684\u7ea0\u7f20\u7ef4\u5ea6\uff0c\u4e14\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u6613\u7528\u6027\uff0c\u5e76\u517c\u5bb9\u73b0\u6709\u91cf\u5b50\u5149\u5b66\u5e73\u53f0\u3002", "motivation": "\u9ad8\u7ef4\u91cf\u5b50\u7ea0\u7f20\u7684\u8868\u5f81\u5bf9\u4e8e\u9ad8\u7ea7\u91cf\u5b50\u8ba1\u7b97\u548c\u91cf\u5b50\u4fe1\u606f\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u91c7\u96c6\u548c\u5b9e\u9a8c\u566a\u58f0\u3002", "method": "\u5229\u7528$\\|1$-\u6b63\u5219\u5316\u91cd\u5efa\u5bf9\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\u6d4b\u91cf\u5f97\u5230\u7684\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u4f4d\u7f6e-\u52a8\u91cfEPR\u7ea0\u7f20\u5224\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u7a00\u758f\u6027\u6846\u67b6\u80fd\u591f\u63d0\u5347\u76f8\u5173\u6027\u4fe1\u53f7\u7684\u53ef\u89c1\u5ea6\u5e76\u6291\u5236\u566a\u58f0\uff0c\u6210\u529f\u8ba4\u8bc1\u4e86\u5728\u65e0\u6b63\u5219\u5316\u60c5\u51b5\u4e0b\u65e0\u6cd5\u8fbe\u5230\u7684\u7ea0\u7f20\u7ef4\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u3001\u6613\u7528\u4e14\u517c\u5bb9\u73b0\u6709\u5e73\u53f0\uff0c\u4e3a\u9ad8\u6548\u3001\u5b9e\u65f6\u7684-\u9ad8\u7ef4\u91cf\u5b50\u6001\u5206\u6790\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2511.11944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11944", "abs": "https://arxiv.org/abs/2511.11944", "authors": ["Ling Wang", "Yunfan Lu", "Wenzong Ma", "Huizai Yao", "Pengteng Li", "Hui Xiong"], "title": "From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing", "comment": "11 pages, 8 figures. Completed in April 2025", "summary": "Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \\textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \\textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.", "AI": {"tldr": "\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u6269\u6563\u6a21\u578b\u8fdb\u884c\u96fe\u5929\u53bb\u96fe\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRGB\u5e27\u7684\u53bb\u96fe\u65b9\u6cd5\u7531\u4e8e\u52a8\u6001\u8303\u56f4\u7684\u9650\u5236\uff0c\u5bb9\u6613\u4e22\u5931\u7ed3\u6784\u548c\u5149\u7167\u7ec6\u8282\uff0c\u5bfc\u81f4\u53bb\u96fe\u95ee\u9898\u4e0d\u9002\u5b9a\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u66f4\u9002\u5408\u96fe\u5929\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u606f\u6765\u6307\u5bfc\u56fe\u50cf\u6062\u590d\u3002\u5177\u4f53\u5730\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u5f15\u5bfc\u6a21\u5757\uff0c\u5c06\u7a00\u758f\u7684\u4e8b\u4ef6\u7279\u5f81\uff08\u5982\u8fb9\u7f18\u3001\u89d2\u70b9\uff09\u6620\u5c04\u5230\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u7cbe\u786e\u7684\u7ed3\u6784\u6307\u5bfc\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6536\u96c6\u7684\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53bb\u96fe\u6548\u679c\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4e3a\u89e3\u51b3\u96fe\u5929\u53bb\u96fe\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u56fe\u50cf\u7684\u7ed3\u6784\u548c\u5149\u7167\u7ec6\u8282\u3002"}}
{"id": "2511.11940", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11940", "abs": "https://arxiv.org/abs/2511.11940", "authors": ["Christopher Sandino", "Sayeri Lala", "Geeling Chau", "Melika Ayoughi", "Behrooz Mahasseni", "Ellen Zippi", "Ali Moin", "Erdrin Azemi", "Hanlin Goh"], "title": "Learning the relative composition of EEG signals using pairwise relative shift pretraining", "comment": "Foundation Models for the Brain and Body NeurIPS 2025 Workshop", "summary": "Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.", "AI": {"tldr": "PARS\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u524d\u7f6e\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u91c7\u6837\u8111\u7535\u56fe\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\u6765\u5b66\u4e60\u8111\u7535\u56fe\u8868\u793a\uff0c\u5728\u6807\u7b7e\u6548\u7387\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u524d\u7f6e\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u8111\u7535\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u63a9\u7801\u91cd\u5efa\u7b56\u7565\uff0c\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u4f4d\u7f6e\u9884\u6d4b\u524d\u7f6e\u4efb\u52a1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPARS\uff08\u914d\u5bf9\u76f8\u5bf9\u79fb\u4f4d\uff09\u7684\u65b0\u578b\u524d\u7f6e\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u91c7\u6837\u8111\u7535\u56fe\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\u6765\u5b66\u4e60\u8111\u7535\u56fe\u8868\u793a\u3002", "result": "\u5728\u5404\u79cd\u8111\u7535\u56fe\u89e3\u7801\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u7ecf\u8fc7PARS\u9884\u8bad\u7ec3\u7684Transformer\u5728\u6807\u7b7e\u6548\u7387\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "PARS\u4e3a\u81ea\u76d1\u7763\u8111\u7535\u56fe\u8868\u793a\u5b66\u4e60\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u795e\u7ecf\u4fe1\u53f7\u4e2d\u7684\u76f8\u5bf9\u65f6\u95f4\u7ec4\u6210\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2511.12892", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12892", "abs": "https://arxiv.org/abs/2511.12892", "authors": ["Liangshun Wu", "Wen Chen", "Shunqing Zhang", "Yajun Wang", "Kunlun Wang"], "title": "Green Emergency Communications in RIS- and MA-Assisted Multi-UAV SAGINs: A Partially Observable Reinforcement Learning Approach", "comment": null, "summary": "In post-disaster space-air-ground integrated networks (SAGINs), terrestrial infrastructure is often impaired, and unmanned aerial vehicles (UAVs) must rapidly restore connectivity for mission-critical ground terminals in cluttered non-line-of-sight (NLoS) urban environments. To enhance coverage, UAVs employ movable antennas (MAs), while reconfigurable intelligent surfaces (RISs) on surviving high-rises redirect signals. The key challenge is communication-limited partial observability, leaving each UAV with a narrow, fast-changing neighborhood view that destabilizes value estimation. Existing multi-agent reinforcement learning (MARL) approaches are inadequate--non-communication methods rely on unavailable global critics, heuristic sharing is brittle and redundant, and learnable protocols (e.g., CommNet, DIAL) lose per-neighbor structure and aggravate non-stationarity under tight bandwidth. To address partial observability, we propose a spatiotemporal A2C where each UAV transmits prior-decision messages with local state, a compact policy fingerprint, and a recurrent belief, encoded per neighbor and concatenated. A spatial discount shapes value targets to emphasize local interactions, while analysis under one-hop-per-slot latency explains stable training with delayed views. Experimental results show our policy outperforms IA2C, ConseNet, FPrint, DIAL, and CommNet--achieving faster convergence, higher asymptotic reward, reduced Temporal-Difference(TD)/advantage errors, and a better communication throughput-energy trade-off.", "AI": {"tldr": "\u5728\u707e\u540e\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a A2C \u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u901a\u4fe1\u53d7\u9650\u7684\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u707e\u540e\u7a7a\u95f4-\u5929-\u5730\u4e00\u4f53\u5316\u7f51\u7edc\uff08SAGINs\uff09\u4e2d\uff0c\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u5f80\u5f80\u53d7\u635f\uff0c\u65e0\u4eba\u673a\uff08UAVs\uff09\u9700\u8981\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5feb\u901f\u6062\u590d\u901a\u4fe1\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u53d7\u9650\u548c\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65f6\u7a7a A2C \u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u4f20\u8f93\u5305\u542b\u5c40\u90e8\u72b6\u6001\u3001\u7b56\u7565\u6307\u7eb9\u548c\u5faa\u73af\u4fe1\u5ff5\u7684\u5148\u9a8c\u51b3\u7b56\u6d88\u606f\uff0c\u5e76\u7ed3\u5408\u7a7a\u95f4\u6298\u6263\u6765\u5851\u9020\u4ef7\u503c\u76ee\u6807\uff0c\u4ee5\u89e3\u51b3\u901a\u4fe1\u53d7\u9650\u7684\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e IA2C\u3001ConseNet\u3001FPrint\u3001DIAL \u548c CommNet \u7b49\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6e10\u8fd1\u5956\u52b1\u3001\u66f4\u4f4e\u7684 TD/advantage \u8bef\u5dee\uff0c\u5e76\u6539\u5584\u4e86\u901a\u4fe1\u541e\u5410\u91cf-\u80fd\u91cf\u7684\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u7a7a A2C \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u707e\u540e SAGINs \u4e2d\u7684\u901a\u4fe1\u53d7\u9650\u548c\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u65e0\u4eba\u673a\u7684\u901a\u4fe1\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.12232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12232", "abs": "https://arxiv.org/abs/2511.12232", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Xiaoshuai Hao", "Haoxiang Fu", "Zeying Gong", "Long Chen", "Xiaojun Liang", "Renjing Xu", "Hangjun Ye", "Wenbo Ding"], "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation", "comment": null, "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.", "AI": {"tldr": "SocialNav-Map\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u7684\u793e\u4f1a\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u52a8\u6001\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u548c\u5360\u7528\u6620\u5c04\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u73af\u5883\u7684\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u793e\u4ea4\u5bfc\u822a\u65b9\u6cd5\u9700\u8981\u957f\u65f6\u95f4\u7684\u8bad\u7ec3\uff082000+\u5c0f\u65f6\uff09\uff0c\u5e76\u4e14\u5728\u4e0d\u719f\u6089\u7684\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u7684\u65b0\u65b9\u6cd5\u3002", "method": "SocialNav-Map\u6846\u67b6\u9996\u5148\u5c06\u4efb\u52a1\u76ee\u6807\u4f4d\u7f6e\u8f6c\u6362\u4e3a\u6784\u5efa\u7684\u5730\u56fe\u5750\u6807\u7cfb\u3002\u7136\u540e\uff0c\u5b83\u521b\u5efa\u4e00\u4e2a\u52a8\u6001\u5360\u7528\u5730\u56fe\uff0c\u5c06\u9884\u6d4b\u7684\u52a8\u6001\u4eba\u7c7b\u8fd0\u52a8\u7eb3\u5165\u5176\u4e2d\u4f5c\u4e3a\u52a8\u6001\u969c\u788d\u7269\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff1a\u5386\u53f2\u9884\u6d4b\u548c\u65b9\u5411\u9884\u6d4b\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u9884\u6d4b\u7684\u8f68\u8ff9\u6574\u5408\u5230\u5360\u7528\u5730\u56fe\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u4e3b\u52a8\u907f\u514d\u4e0e\u4eba\u7c7b\u6f5c\u5728\u7684\u78b0\u649e\uff0c\u540c\u65f6\u9ad8\u6548\u5730\u5bfc\u822a\u5230\u76ee\u7684\u5730\u3002", "result": "\u5728Social-HM3D\u548cSocial-MP3D\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSocialNav-Map\u5728\u907f\u514d\u78b0\u649e\u65b9\u9762\u6bd4\u9700\u89812396\u5c0f\u65f6GPU\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5728\u65b0\u7684\u73af\u5883\u4e2d\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u3002", "conclusion": "SocialNav-Map\u901a\u8fc7\u6d88\u9664\u5bf9\u7279\u5b9a\u73af\u5883\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u80fd\u591f\u5b89\u5168\u9ad8\u6548\u5730\u5728\u5177\u6709\u591a\u6837\u5316\u4eba\u7c7b\u884c\u4e3a\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\u3002"}}
{"id": "2511.12591", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12591", "abs": "https://arxiv.org/abs/2511.12591", "authors": ["Darya Meniailava", "Michael Petrov", "Josef Soucek", "Milos Nesladek"], "title": "Charge-state stability of single NV centers in HPHT-type IIa diamond", "comment": "Draft version. 13 pages, 4 figures. A more complete version will follow", "summary": "This is a preliminary version. Improvements and additional analysis will be included in a revised manuscript. We investigate the charge-state stability of individual nitrogen-vacancy (NV) centers in weakly doped HPHT IIa diamond containing sub-ppm concentrations of boron and nitrogen. Using Ti/Al coplanar electrodes on an oxygen-terminated surface, we study how applied electric fields and optical excitation jointly govern NV charge conversion. By combining voltage-dependent photoluminescence, real-time charge-state monitoring, laser-power saturation with spectral decomposition, and time-resolved measurements, we reveal that electric fields several micrometers from the contacts significantly increase the NV- population and enhance spin readout. At low excitation powers, the NV- population evolves on minute timescales following compressed-exponential kinetics, consistent with slow space-charge rearrangement in ultra-insulating diamond. Under pulsed excitation, we observe hundreds-of-nanoseconds NV-/NV0 conversion driven by hole capture, which is strongly suppressed by applied bias. Our results demonstrate that residual boron acceptors play a key role in determining charge-state stability and show how electrical bias can reliably stabilize NV- in weakly doped bulk diamond.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u5f31\u63ba\u6742\u7684HPHT IIa\u578b\u91d1\u521a\u77f3\u4e2d\uff0c\u4f7f\u7528Ti/Al\u5171\u9762\u7535\u6781\u548c\u6c27\u7ec8\u6b62\u8868\u9762\uff0c\u7814\u7a76\u4e86\u5916\u52a0\u7535\u573a\u548c\u5149\u5b66\u6fc0\u53d1\u5982\u4f55\u5171\u540c\u63a7\u5236\u6c2e-\u7a7a\u4f4d(NV)\u4e2d\u5fc3\u7684\u7535\u8377\u8f6c\u6362\uff0c\u5e76\u63ed\u793a\u4e86\u7535\u573a\u53ef\u4ee5\u63d0\u9ad8NV-\u7684\u6bd4\u4f8b\u5e76\u589e\u5f3a\u81ea\u65cb\u8bfb\u6570\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u7535\u8377\u8f6c\u6362\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u8bc1\u660e\u4e86\u6b8b\u4f59\u787c\u53d7\u4e3b\u5728\u51b3\u5b9a\u7535\u8377\u6001\u7a33\u5b9a\u6027\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5f31\u63ba\u6742\u91d1\u521a\u77f3\u4e2dNV\u4e2d\u5fc3\u7684\u7535\u8377\u6001\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u7535\u573a\u548c\u5149\u5b66\u6fc0\u53d1\u5bf9\u5176\u7535\u8377\u8f6c\u6362\u7684\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u4e86\u7535\u538b\u4f9d\u8d56\u5149\u81f4\u53d1\u5149\u3001\u5b9e\u65f6\u7535\u8377\u6001\u76d1\u6d4b\u3001\u6fc0\u5149\u529f\u7387\u9971\u548c\u4e0e\u5149\u8c31\u5206\u89e3\u4ee5\u53ca\u65f6\u95f4\u5206\u8fa8\u6d4b\u91cf\uff0c\u5e76\u4f7f\u7528\u4e86Ti/Al\u5171\u9762\u7535\u6781\u3002", "result": "\u5916\u52a0\u7535\u573a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8NV-\u6bd4\u4f8b\u5e76\u589e\u5f3a\u81ea\u65cb\u8bfb\u6570\uff1b\u5728\u4f4e\u6fc0\u53d1\u529f\u7387\u4e0b\uff0cNV-\u6bd4\u4f8b\u7684\u6f14\u5316\u9075\u5faa\u538b\u7f29\u6307\u6570\u52a8\u529b\u5b66\uff0c\u4e0e\u6162\u901f\u7a7a\u95f4\u7535\u8377\u91cd\u6392\u4e00\u81f4\uff1b\u5728\u8109\u51b2\u6fc0\u53d1\u4e0b\uff0c\u89c2\u5bdf\u5230\u7531\u7a7a\u7a74\u6355\u83b7\u9a71\u52a8\u7684\u6570\u767e\u7eb3\u79d2\u7684NV-/NV0\u8f6c\u6362\uff0c\u5e76\u88ab\u504f\u538b\u5f3a\u70c8\u6291\u5236\u3002", "conclusion": "\u6b8b\u4f59\u787c\u53d7\u4e3b\u5728\u51b3\u5b9a\u7535\u8377\u6001\u7a33\u5b9a\u6027\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e14\u7535\u573a\u53ef\u4ee5\u53ef\u9760\u5730\u7a33\u5b9a\u5f31\u63ba\u6742\u5757\u72b6\u91d1\u521a\u77f3\u4e2d\u7684NV-\u3002"}}
{"id": "2511.11959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11959", "abs": "https://arxiv.org/abs/2511.11959", "authors": ["Leonardi Melo", "Lu\u00eds Gustavo", "Dimmy Magalh\u00e3es", "Lucciani Vieira", "Mauro Ara\u00fajo"], "title": "Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs", "comment": "14 pages, 8 figures. Preprint submitted to arXiv", "summary": "This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Po\u00e7o da Bebidinha Archaeological Complex, Piau\u00ed, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8eU-Net\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u5df4\u897f\u5ca9\u753b\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\uff0c\u5176\u4e2dAttention-Residual BEGL-UNet\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u5ca9\u753b\u6570\u5b57\u4fdd\u62a4\u7684\u51c6\u786e\u6027\uff0c\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u51e0\u79cd\u5148\u8fdb\u7684U-Net\u67b6\u6784\u5728\u5ca9\u753b\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u4e09\u79cdU-Net\u53d8\u4f53\uff08BEGL-UNet\u3001Attention-Residual BEGL-UNet\u3001Spatial Channel Attention BEGL-UNet\uff09\uff0c\u5e76\u7ed3\u5408\u4e86BEGL\u635f\u5931\u51fd\u6570\uff0c\u5728\u5df4\u897f\u8003\u53e4\u9057\u5740\u7684\u5ca9\u753b\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e865\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u3002", "result": "Attention-Residual BEGL-UNet\u53d6\u5f97\u4e86\u6700\u597d\u7684\u603b\u4f53\u6027\u80fd\uff08Dice Score: 0.710\uff09\uff0cSpatial Channel Attention BEGL-UNet\u8868\u73b0\u76f8\u5f53\uff08DSC: 0.707\uff09\uff0c\u5747\u4f18\u4e8e\u57fa\u7ebfBEGL-UNet\uff08DSC: 0.690\uff09\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u5ca9\u753b\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u8003\u53e4\u9057\u4ea7\u7684\u6570\u5b57\u5316\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2511.11749", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11749", "abs": "https://arxiv.org/abs/2511.11749", "authors": ["Almond Kiruthu Murimi"], "title": "How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems", "comment": "9 pages, 4 tables, seminar paper", "summary": "This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u6570\u636e\u590d\u5236\u7b56\u7565\u53ef\u4ee5\u63d0\u9ad8\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u901a\u8fc7\u9884\u6d4b\u6545\u969c\u548c\u5b9e\u65f6\u4f18\u5316\u6570\u636e\u653e\u7f6e\u6765\u5e94\u5bf9\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u610f\u5916\u6545\u969c\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u590d\u5236\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u914d\u7f6e\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u610f\u5916\u6545\u969c\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u548c\u505c\u673a\u65f6\u95f4\u5ef6\u957f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u9884\u6d4b\u5206\u6790\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u590d\u5236\u673a\u5236\uff0c\u80fd\u591f\u9884\u6d4b\u7cfb\u7edf\u6545\u969c\u5e76\u5b9e\u65f6\u4f18\u5316\u6570\u636e\u653e\u7f6e\u3002", "result": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u5b9a\u6027\u5206\u6790\u548c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6bd4\u8f83\u8bc4\u4f30\uff0c\u8be5\u7814\u7a76\u786e\u5b9a\u4e86\u73b0\u6709\u590d\u5236\u7b56\u7565\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u673a\u5668\u5b66\u4e60\u5728\u521b\u5efa\u66f4\u5177\u5f39\u6027\u3001\u81ea\u4f18\u5316\u7cfb\u7edf\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u5b9e\u65bd\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u7684\u5e0c\u671b\u548c\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u4ee5\u53ca\u5728\u4e91\u548c\u4f01\u4e1a\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2511.12911", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12911", "abs": "https://arxiv.org/abs/2511.12911", "authors": ["Muhammad Nadeem", "MirSaleh Bahavarnia", "Ahmad F. Taha"], "title": "Wide-Area Feedback Control for Renewables-Heavy Power Systems: A Comparative Study of Reinforcement Learning and Lyapunov-Based Design", "comment": null, "summary": "As renewable energy sources become more prevalent, accurately modeling power grid dynamics is becoming increasingly more complex. Concurrently, data acquisition and realtime system state monitoring are becoming more available for control centers. This motivates shifting from \\textit{model- and Lyapunov-based} feedback controller designs toward \\textit{model-free} ones. Reinforcement learning (RL) has emerged as a key tool for designing model-free controllers. Various studies have been carried out to study voltage/frequency control strategies via RL. However, usually a simplified system model is used neglecting detailed dynamics of solar, wind, and composite loads -- and damping system-wide oscillations and modeling power flows are all usually ignored. To that end, we pose an optimal feedback control problem for a detailed renewables-heavy power system, defined by a set of nonlinear differential algebraic equations (NDAE). The control problem is solved using a completely model-free design via RL as well as using a model-based approach built upon the Lyapunov stability theory with guarantees. The paper in its essence seeks to explore whether data-driven feedback control should be used in power grids over its model-driven counterpart. Theoretical developments and thorough case studies are presented with an eye on this exploration. Finally, a detailed analysis is provided to delineate the strengths and weaknesses of both approaches for renewables-heavy grids.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u5e26\u6765\u7684\u7535\u7f51\u52a8\u529b\u5b66\u5efa\u6a21\u590d\u6742\u6027\u589e\u52a0\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5b9e\u65f6\u76d1\u6d4b\u6570\u636e\u65e5\u76ca\u53ef\u7528\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u57fa\u4e8e\u6a21\u578b\u548cLyapunov\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u8f6c\u5411\u65e0\u6a21\u578b\u63a7\u5236\u5668\u8bbe\u8ba1\u7684\u8d8b\u52bf\u3002\u6587\u7ae0\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u5173\u952e\u7684\u65e0\u6a21\u578b\u63a7\u5236\u5668\u8bbe\u8ba1\u5de5\u5177\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u542b\u5927\u91cf\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7535\u529b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7531\u4e00\u7ec4\u975e\u7ebf\u6027\u5fae\u5206\u4ee3\u6570\u65b9\u7a0b\uff08NDAE\uff09\u5b9a\u4e49\u3002\u901a\u8fc7RL\u548c\u57fa\u4e8eLyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u7684\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u6765\u89e3\u51b3\u6700\u4f18\u53cd\u9988\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u5bf9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u4ee5\u63a2\u7d22\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5728\u7535\u7f51\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u666e\u53ca\uff0c\u7535\u529b\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u65e5\u8d8b\u590d\u6742\uff0c\u800c\u5b9e\u65f6\u76d1\u6d4b\u6570\u636e\u65e5\u76ca\u53ef\u7528\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u4ece\u4f20\u7edf\u7684\u57fa\u4e8e\u6a21\u578b\u548cLyapunov\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\u8f6c\u5411\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u8bbe\u8ba1\u63a7\u5236\u5668\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u5b8c\u5168\u65e0\u6a21\u578b\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u4e0e\u57fa\u4e8eLyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u7684\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u6765\u89e3\u51b3\u542b\u5927\u91cf\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7535\u529b\u7cfb\u7edf\uff08\u7531\u4e00\u7ec4\u975e\u7ebf\u6027\u5fae\u5206\u4ee3\u6570\u65b9\u7a0b\uff08NDAE\uff09\u5b9a\u4e49\uff09\u7684\u6700\u4f18\u53cd\u9988\u63a7\u5236\u95ee\u9898\u3002", "result": "\u6587\u7ae0\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u8be6\u5c3d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5bf9RL\uff08\u6570\u636e\u9a71\u52a8\uff09\u548cLyapunov\u7406\u8bba\uff08\u6a21\u578b\u9a71\u52a8\uff09\u4e24\u79cd\u65b9\u6cd5\u5728\u542b\u5927\u91cf\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7535\u7f51\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u63a2\u7d22\u548c\u6bd4\u8f83\u3002", "conclusion": "\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u6570\u636e\u9a71\u52a8\u7684\u53cd\u9988\u63a7\u5236\uff08\u5982RL\uff09\u662f\u5426\u5e94\u5728\u7535\u7f51\u4e2d\u4f18\u5148\u4e8e\u5176\u6a21\u578b\u9a71\u52a8\u7684\u5bf9\u5e94\u7269\uff0c\u5e76\u5bf9\u4e24\u79cd\u65b9\u6cd5\u5728\u53ef\u518d\u751f\u80fd\u6e90\u5360\u6bd4\u9ad8\u7684\u7535\u7f51\u4e2d\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002"}}
{"id": "2511.12613", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12613", "abs": "https://arxiv.org/abs/2511.12613", "authors": ["Pietro Zanotta", "Ljubomir Budinski", "Caglar Aytekin", "Valtteri Lahtinen"], "title": "Quantum Orthogonal Separable Physics-Informed Neural Networks", "comment": null, "summary": "This paper introduces Quantum Orthogonal Separable Physics-Informed Neural Networks (QO-SPINNs), a novel architecture for solving Partial Differential Equations, integrating quantum computing principles to address the computational bottlenecks of classical methods. We leverage a quantum algorithm for accelerating matrix multiplication within each layer, achieving a $\\mathcal O(d\\log d/\u03b5^2)$ complexity, a significant improvement over the classical $\\mathcal O(d^2)$ complexity, where $d$ is the dimension of the matrix, $\u03b5$ the accuracy level. This is accomplished by using a Hamming weight-preserving quantum circuit and a unary basis for data encoding, with a comprehensive theoretical analysis of the overall architecture provided. We demonstrate the practical utility of our model by applying it to solve both forward and inverse PDE problems. Furthermore, we exploit the inherent orthogonality of our quantum circuits (which guarantees a spectral norm of 1) to develop a novel uncertainty quantification method. Our approach adapts the Spectral Normalized Gaussian Process for SPINNs, eliminating the need for the computationally expensive spectral normalization step. By using a Quantum Orthogonal SPINN architecture based on stacking, we provide a robust and efficient framework for uncertainty quantification (UQ) which, to our knowledge, is the first UQ method specifically designed for Separable PINNs. Numerical results based on classical simulation of the quantum circuits, are presented to validate the theoretical claims and demonstrate the efficacy of the proposed method.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u91cf\u5b50\u6b63\u4ea4\u53ef\u5206\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08QO-SPINNs\uff09\u7684\u6458\u8981\uff0c\u8be5\u7f51\u7edc\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u52a0\u901f\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u6c42\u89e3\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u3002", "motivation": "\u5f53\u524d\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7ecf\u5178\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQO-SPINNs\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u91cf\u5b50\u5c42\u9762\u52a0\u901f\u77e9\u9635\u4e58\u6cd5\uff08\u590d\u6742\u5ea6\u4e3a O(d log d/\u03b5^2)\uff09\u6765\u89e3\u51b3\u8ba1\u7b97\u74f6\u9888\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4fdd\u6301\u6c49\u660e\u91cd\u91cf\u7684\u91cf\u5b50\u7535\u8def\u548c\u4e00\u5143\u57fa\u8fdb\u884c\u6570\u636e\u7f16\u7801\u3002\u6b64\u5916\uff0c\u5229\u7528\u91cf\u5b50\u7535\u8def\u7684\u56fa\u6709\u6b63\u4ea4\u6027\uff08\u4fdd\u8bc1\u8c31\u8303\u6570\u4e3a1\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6539\u7f16\u81eaSPINNs\u7684\u8c31\u5f52\u4e00\u5316\u9ad8\u65af\u8fc7\u7a0b\uff0c\u65e0\u9700\u8fdb\u884c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u8c31\u5f52\u4e00\u5316\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u5728\u7ecf\u5178\u6a21\u62df\u5668\u4e0a\u8fd0\u884c\u91cf\u5b50\u7535\u8def\uff0c\u5bf9QO-SPINNs\u6c42\u89e3\u6b63\u5411\u548c\u9006\u5411PDE\u95ee\u9898\u4ee5\u53ca\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u6570\u503c\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u90fd\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "QO-SPINNs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u548c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u53ef\u5206\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2511.12745", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12745", "abs": "https://arxiv.org/abs/2511.12745", "authors": ["Vivek Chawla", "Boris Slautin", "Utkarsh Pratiush", "Dayakar Penumadu", "Sergei Kalinin"], "title": "DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes", "comment": "33 pages, 10 main figures, 7 additional in SI", "summary": "Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.", "AI": {"tldr": "DIVIDE\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7279\u5b9a\u673a\u5236\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\uff0c\u5728\u8054\u5408\u6f5c\u7a7a\u95f4\u4e2d\u5206\u79bb\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u591a\u4e2a\u72ec\u7acb\u673a\u5236\uff08\u5982\u7a7a\u95f4\u3001\u7c7b\u522b\u6216\u7ed3\u6784\u6548\u5e94\uff09\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u3001\u9762\u5411\u673a\u5236\u7684\u9884\u6d4b\u548c\u4e3b\u52a8\u5b66\u4e60\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u901a\u5e38\u7531\u7a7a\u95f4\u3001\u7c7b\u522b\u6216\u7ed3\u6784\u6548\u5e94\u7b49\u591a\u79cd\u72ec\u7acb\u673a\u5236\u4ea7\u751f\uff0c\u8fd9\u4e9b\u673a\u5236\u7684\u7ec4\u5408\u5f71\u54cd\u4f1a\u63a9\u76d6\u5b83\u4eec\u5404\u81ea\u7684\u8d21\u732e\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5206\u79bb\u8fd9\u4e9b\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aDIVIDE\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u8054\u5408\u6f5c\u7a7a\u95f4\u4e2d\u96c6\u6210\u7279\u5b9a\u673a\u5236\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\u6765\u5b9e\u73b0\u89e3\u7f20\u3002\u8be5\u6846\u67b6\u5206\u79bb\u4e0d\u540c\u7684\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u6355\u6349\u5b83\u4eec\u5728\u5177\u6709\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u7684\u7ec4\u5408\u6548\u5e94\u3002\u8be5\u67b6\u6784\u652f\u6301\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u80fd\u591f\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u3001\u9762\u5411\u673a\u5236\u7684\u9884\u6d4b\u548c\u9ad8\u6548\u7684\u4e3b\u52a8\u5b66\u4e60\u3002", "result": "DIVIDE\u5728\u7ed3\u5408\u4e86\u7c7b\u522b\u56fe\u50cf\u5757\u548c\u975e\u7ebf\u6027\u7a7a\u95f4\u573a\u7684\u5408\u6210\u6570\u636e\u96c6\u3001\u94c1\u7535\u6a21\u5f0f\u7684FerroSIM\u81ea\u65cb\u6676\u683c\u6a21\u62df\u4ee5\u53caPbTiO3\u8584\u819c\u7684\u5b9e\u9a8cPFM\u6ede\u540e\u56de\u7ebf\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u3002\u7ed3\u679c\u8868\u660e\uff0cDIVIDE\u80fd\u591f\u5206\u79bb\u673a\u5236\u3001\u91cd\u73b0\u52a0\u6027\u548c\u7f29\u653e\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u5728\u566a\u58f0\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "DIVIDE\u6846\u67b6\u6210\u529f\u5730\u5c06\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u72ec\u7acb\u751f\u6210\u56e0\u7d20\u5206\u79bb\u5f00\u6765\uff0c\u5e76\u80fd\u81ea\u7136\u5730\u6269\u5c55\u5230\u5305\u542b\u591a\u79cd\u54cd\u5e94\uff08\u5982\u673a\u68b0\u3001\u7535\u78c1\u6216\u5149\u5b66\uff09\u7684\u591a\u529f\u80fd\u6570\u636e\u96c6\u3002"}}
{"id": "2511.11984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11984", "abs": "https://arxiv.org/abs/2511.11984", "authors": ["Zhenhao Guo", "Rachit Saluja", "Tianyuan Yao", "Quan Liu", "Junchao Zhu", "Haibo Wang", "Daniel Reisenb\u00fcchler", "Yuankai Huo", "Benjamin Liechty", "David J. Pisapia", "Kenji Ikemura", "Steven Salvatoree", "Surya Seshane", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology", "comment": null, "summary": "Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u7c7b\u5efa\u6a21\u4e3a\u4e34\u5e8a\u4e0a\u73b0\u5b9e\u7684\u5c11\u6837\u672c\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u4e13\u4e1a\u548c\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6570\u636e\u7ea6\u675f\u4e0b\u7684\u8868\u73b0\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u75c5\u7406\u5b66\u4e13\u4e1aVLMs\u5728\u5fae\u8c03\u540e\u662f\u6700\u6709\u6548\u7684\u8d77\u70b9\uff0c\u5373\u4f7f\u5728\u6bcf\u4e2a\u4e9a\u578b\u53ea\u67094-8\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u533a\u5206\u5ea6\u548c\u6821\u51c6\u5ea6\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u6b63\u8d1f\u6837\u672c\u533a\u5206\u4e0e\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u540c\u7b49\u91cd\u8981\uff0c\u5e76\u4e3a\u6a21\u578b\u9009\u62e9\u3001\u9002\u5e94\u7b56\u7565\u548c\u6807\u6ce8\u6295\u5165\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u75c5\u7406\u5b66\u65b9\u6cd5\u503e\u5411\u4e8e\u5728\u5168\u76d1\u7763\u4e0b\u8bc4\u4f30\u7c97\u7c92\u5ea6\u75be\u75c5\u5206\u7c7b\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6570\u636e\u7ea6\u675f\u4e0b\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u7c7b\u7684\u4e34\u5e8a\u4ef7\u503c\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982\u4f55\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u4e9a\u578b\u5206\u7c7b\u3002", "method": "\u5c06\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u7c7b\u5efa\u6a21\u4e3a\u4e34\u5e8a\u73b0\u5b9e\u7684\u5c11\u6837\u672c\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u8bbe\u7f6e\u4e0b\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u75c5\u7406\u5b66\u4e13\u4e1a\u548c\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u5206\u7c7b\u6027\u80fd\uff08\u51c6\u786e\u7387\u3001AUC\u3001F1\uff09\uff0c\u5e76\u68c0\u67e5\u4e86\u5b66\u4e60\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u7279\u5f81\u5bf9\u9f50\u4ee5\u53ca\u80be\u5c0f\u7403\u4e9a\u578b\u7684\u53ef\u5206\u79bb\u6027\u3002\u901a\u8fc7\u8054\u5408\u5206\u6790\u6837\u672c\u6570\u91cf\u3001\u6a21\u578b\u67b6\u6784\u548c\u9886\u57df\u77e5\u8bc6\u4ee5\u53ca\u9002\u5e94\u7b56\u7565\uff0c\u4e3a\u4e34\u5e8a\u6570\u636e\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u548c\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u75c5\u7406\u5b66\u4e13\u4e1a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u7b80\u5355\u5fae\u8c03\u540e\u662f\u6700\u6709\u6548\u7684\u8d77\u70b9\u3002\u5373\u4f7f\u6bcf\u4e2a\u80be\u5c0f\u7403\u4e9a\u578b\u53ea\u67094-8\u4e2a\u6807\u6ce8\u6837\u672c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e5f\u80fd\u5f00\u59cb\u6355\u6349\u533a\u5206\u7279\u5f81\uff0c\u5e76\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u5ea6\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u7684\u6536\u76ca\uff0c\u5c3d\u7ba1\u989d\u5916\u7684\u76d1\u7763\u53ef\u4ee5\u5e26\u6765\u6e10\u8fdb\u7684\u6539\u8fdb\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6b63\u8d1f\u6837\u672c\u4e4b\u95f4\u7684\u533a\u5206\u4e0e\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u540c\u7b49\u91cd\u8981\u3002", "conclusion": "\u76d1\u7763\u6c34\u5e73\u548c\u9002\u5e94\u7b56\u7565\u5171\u540c\u5f71\u54cd\u8bca\u65ad\u6027\u80fd\u548c\u591a\u6a21\u6001\u7ed3\u6784\u3002\u8be5\u7814\u7a76\u4e3a\u6a21\u578b\u9009\u62e9\u3001\u9002\u5e94\u7b56\u7565\u548c\u6807\u6ce8\u6295\u5165\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ee5\u5e94\u5bf9\u4e34\u5e8a\u6570\u636e\u7ea6\u675f\u4e0b\u7684\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u7c7b\u6311\u6218\u3002"}}
{"id": "2511.12222", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12222", "abs": "https://arxiv.org/abs/2511.12222", "authors": ["Hangshuo Tian"], "title": "Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling", "comment": null, "summary": "Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.\n  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.\n  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \\emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7c92\u5b50\u6ee4\u6ce2\u5668\uff08PF\uff09\u4e0e\u9e21\u7fa4\u4f18\u5316\uff08CSO\uff09\u7b97\u6cd5\u7ed3\u5408\u7528\u4e8e\u7c92\u5b50 \u092a\u0941\u0928\u0930\u094d\u091c\u0928\u094d\u092e\uff0c\u4ee5\u53ca Kullback-Leibler \u6563\u5ea6\uff08KLD\uff09\u91c7\u6837\u7528\u4e8e\u81ea\u9002\u5e94\u8c03\u6574\u7c92\u5b50\u96c6\u5927\u5c0f\u4e4b\u95f4\u7684\u7406\u8bba\u4e92\u52a8\u3002", "motivation": "\u73b0\u6709\u7684\u7c92\u5b50\u6ee4\u6ce2\u4e0e\u667a\u80fd\u4f18\u5316\u7b97\u6cd5\uff08\u5982CSO\uff09\u7684\u7ed3\u5408\uff0c\u4ee5\u53caKLD\u91c7\u6837\u5728\u81ea\u9002\u5e94\u8c03\u6574\u7c92\u5b50\u96c6\u5927\u5c0f\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5176\u7406\u8bba\u76f8\u4e92\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7b80\u5316\u7684\u6a21\u578b\u5206\u6790CSO \u092a\u0941\u0928\u0930\u094d\u091c\u0928\u094d\u092e\u6b65\u9aa4\u5bf9\u7c92\u5b50\u96c6\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u63d0\u51faCSO\u7684\u9002\u5e94\u6027\u66f4\u65b0\u53ef\u8fd1\u4f3c\u89c6\u4e3a\u5747\u65b9\u6536\u7f29\uff0c\u5bfc\u81f4\u7c92\u5b50\u5206\u5e03\u6bd4\u57fa\u7ebfPF\u66f4\u96c6\u4e2d\u3002\u5229\u7528Karamata\u4e0d\u7b49\u5f0f\u5206\u6790\uff0c\u8868\u660eCSO\u589e\u5f3a\u7684PF\uff08CPF\uff09\u5728\u6ee1\u8db3\u76f8\u540c\u7edf\u8ba1\u8bef\u5dee\u8fb9\u754c\u65f6\uff0c\u6240\u9700\u7684\u9884\u671f\u7c92\u5b50\u6570\u4f4e\u4e8e\u6807\u51c6PF\u3002", "result": "\u5206\u6790\u8868\u660e\uff0cCSO\u7684\u9002\u5e94\u6027\u66f4\u65b0\u8fd1\u4f3c\u4e8e\u5747\u65b9\u6536\u7f29\uff0c\u4f7f\u5f97\u7c92\u5b50\u5206\u5e03\u6bd4\u57fa\u7ebfPF\u66f4\u96c6\u4e2d\u3002\u7814\u7a76\u63a8\u65ad\uff0c\u5728CSO-PF\uff08CPF\uff09\u4e2d\uff0c\u6ee1\u8db3\u76f8\u540c\u7684\u7edf\u8ba1\u8bef\u5dee\u754c\u9650\u6240\u9700\u7684\u9884\u671f\u7c92\u5b50\u6570\u91cf\u53ef\u80fd\u5c11\u4e8e\u6807\u51c6PF\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u7ed3\u5408CSO\u548cKLD\u91c7\u6837\u6280\u672f\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\u63d0\u4f9b\u4e86\u8d77\u70b9\u3002"}}
{"id": "2511.11843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11843", "abs": "https://arxiv.org/abs/2511.11843", "authors": ["Yiwei Zhao", "Qiushi Lin", "Hongbo Kang", "Guy E. Blelloch", "Laxman Dhulipala", "Charles McGuffey", "Phillip B. Gibbons"], "title": "TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing", "comment": null, "summary": "In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.", "AI": {"tldr": "TD-Orch\u662f\u4e00\u4e2a\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u4efb\u52a1-\u6570\u636e\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u79cd\u63a8\u62c9\u6280\u672f\u6765\u5e73\u8861\u8d1f\u8f7d\u548c\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\uff0c\u53ef\u4ee5\u4e3a\u5206\u5e03\u5f0f\u5e94\u7528\u7a0b\u5e8f\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002TDO-GP\u662f\u57fa\u4e8eTD-Orch\u7684\u5206\u5e03\u5f0f\u56fe\u5904\u7406\u7cfb\u7edf\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u7cfb\u7edf\u6709\u66f4\u5feb\u7684\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u56fe\u5904\u7406\u548c\u952e\u503c\u5b58\u50a8\u7b49\u591a\u79cd\u5206\u5e03\u5f0f\u5e94\u7528\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u52a1\u548c\u6570\u636e\u5206\u5e03\u5728\u591a\u53f0\u673a\u5668\u4e0a\u7684\u4efb\u52a1-\u6570\u636e\u7f16\u6392\u62bd\u8c61\u3002", "method": "TD-Orch\u91c7\u7528\u5206\u5e03\u5f0f\u63a8\u62c9\u6280\u672f\uff0c\u53cc\u5411\u79fb\u52a8\u4efb\u52a1\u548c\u6570\u636e\u4ee5\u5b9e\u73b0\u8de8\u673a\u5668\u7684\u8d1f\u8f7d\u5747\u8861\uff0c\u5373\u4f7f\u5728\u6570\u636e\u70ed\u70b9\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u8bc1\u6700\u5c0f\u7684\u901a\u4fe1\u5f00\u9500\u3002TDO-GP\u5728\u6b64\u6846\u67b6\u4e0a\u6784\u5efa\uff0c\u5e76\u91c7\u7528\u4e86\u4e09\u79cd\u5b9e\u73b0\u6280\u672f\u6765\u5229\u7528TD-Orch\u7684\u6267\u884c\u6d41\u3002", "result": "TD-Orch\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5176\u901f\u5ea6\u6bd4\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u8c03\u5ea6\u57fa\u7ebf\u5feb2.7\u500d\u3002TDO-GP\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5176\u5728\u901a\u7528\u56fe\u5904\u7406\u65b9\u9762\u7684\u5e73\u5747\u901f\u5ea6\u6bd4\u73b0\u6709\u7684\u5f00\u6e90\u5206\u5e03\u5f0f\u56fe\u5904\u7406\u7cfb\u7edf\u5feb4.1\u500d\u3002", "conclusion": "TD-Orch\u6846\u67b6\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u80fd\u591f\u4e3a\u5206\u5e03\u5f0f\u5e94\u7528\u7a0b\u5e8f\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002TDO-GP\u5728TD-Orch\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u4e3a\u901a\u7528\u56fe\u5904\u7406\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u7cfb\u7edf\u66f4\u5feb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13006", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13006", "abs": "https://arxiv.org/abs/2511.13006", "authors": ["Fangzhi Li", "Zhichu Ren", "Cunhua Pan", "Hong Ren", "Jing Jin", "Qixing Wang", "Jiangzhou Wang"], "title": "Cooperative ISAC for LAE: Joint Trajectory Planning, Power allocation, and Dynamic Time Division", "comment": null, "summary": "To enhance the performance of aerial-ground networks, this paper proposes an integrated sensing and communication (ISAC) framework for multi-UAV systems. In our model, ground base stations (BSs) cooperatively serve multiple unmanned aerial vehicles (UAVs), and employ a time-division strategy in which beam scanning for sensing comes before data communication in each time slot. To maximize the sum communication rate while satisfying the total sensing mutual information (MI) requirement, we jointly optimize the UAV trajectories, communication and sensing power allocation, and the dynamic time-division ratio. The resulting non-convex optimization problem is efficiently solved using an alternating optimization (AO) framework. Simulation results demonstrate that our proposed joint design significantly outperforms benchmark schemes with static or partially optimized resources. The findings also reveal the critical importance of dynamic trajectory and resource management for effectively navigating the sensing-communication trade-off, especially under stringent power or sensing constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u5730\u7f51\u7edc\u7684\u6027\u80fd\u3002\u901a\u8fc7\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u901a\u4fe1\u4e0e\u4f20\u611f\u529f\u7387\u5206\u914d\u4ee5\u53ca\u52a8\u6001\u65f6\u5206\u6bd4\u7387\uff0c\u5728\u6ee1\u8db3\u603b\u4f20\u611f\u4e92\u4fe1\u606f\u8981\u6c42\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u901a\u4fe1\u901f\u7387\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u7a7a\u5730\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u901a\u4fe1\u4e0e\u4f20\u611f\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6846\u67b6\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u901a\u4fe1\u4e0e\u4f20\u611f\u529f\u7387\u5206\u914d\u4ee5\u53ca\u52a8\u6001\u65f6\u5206\u6bd4\u7387\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u663e\u8457\u4f18\u4e8e\u5177\u6709\u9759\u6001\u6216\u90e8\u5206\u4f18\u5316\u8d44\u6e90\u7684\u57fa\u51c6\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u52a8\u6001\u8f68\u8ff9\u548c\u8d44\u6e90\u7ba1\u7406\u5728\u5e94\u5bf9\u4f20\u611f-\u901a\u4fe1\u6743\u8861\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u52a8\u6001\u8f68\u8ff9\u548c\u8d44\u6e90\u7ba1\u7406\u5bf9\u4e8e\u6709\u6548\u5e94\u5bf9\u4f20\u611f-\u901a\u4fe1\u6743\u8861\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4e25\u683c\u7684\u529f\u7387\u6216\u4f20\u611f\u7ea6\u675f\u4e0b\u3002"}}
{"id": "2511.12361", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12361", "abs": "https://arxiv.org/abs/2511.12361", "authors": ["Leroy D'Souza", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty", "comment": null, "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.", "AI": {"tldr": "SAC-MoE\u662f\u4e00\u79cd\u7ed3\u5408\u4e86Soft Actor-Critic\uff08SAC\uff09\u548c\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u6f5c\u5728\u548c\u4e0d\u53ef\u89c2\u5bdf\u4e8b\u4ef6\u7684\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u3002\u901a\u8fc7\u5b66\u4e60\u8def\u7531\u5668\u81ea\u9002\u5e94\u5730\u9009\u62e9\u4e13\u5bb6\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u8bfe\u7a0b\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8be5\u6a21\u578b\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u6a21\u5f0f\u548c\u5207\u6362\u4f4d\u7f6e\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\u548c\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u6f5c\u5728\u53c2\u6570\u548c\u4e0d\u53ef\u89c2\u5bdf\u4e8b\u4ef6\u7684\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u7279\u522b\u662f\u5728 legged robots, vehicles and aircrafts \u7b49\u7cfb\u7edf\u4e2d\u3002SAC-MoE\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAC-MoE\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06SAC\u7684Actor\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u8def\u7531\u5668\uff0c\u8be5\u8def\u7531\u5668\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9009\u62e9\u4e0d\u540c\u7684\u4e13\u5bb6\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\u4f18\u5148\u6536\u96c6\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6df7\u5408\u52a8\u529b\u81ea\u52a8\u8d5b\u8f66\u548c\u81ea\u4e3b\u8fd0\u52a8\u4efb\u52a1\u7684\u6a21\u62df\u7814\u7a76\u4e2d\uff0cSAC-MoE\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u73af\u5883\u65b9\u9762\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u9ad8\u8fbe6\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002\u6240\u63d0\u51fa\u7684\u8bfe\u7a0b\u7b56\u7565\u5728\u6240\u6709\u8bc4\u4f30\u7684\u7b56\u7565\u4e2d\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4f53\u73b0\u5728\u8def\u7531\u5668\u80fd\u591f\u4e3a\u4e0d\u540c\u7684\u6f5c\u5728\u6a21\u5f0f\u6fc0\u6d3b\u4e0d\u540c\u7684\u4e13\u5bb6\u3002", "conclusion": "SAC-MoE\u5728\u5904\u7406\u5177\u6709\u6f5c\u5728\u53c2\u6570\u548c\u4e0d\u53ef\u89c2\u5bdf\u4e8b\u4ef6\u7684\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u65b9\u9762\uff0c\u901a\u8fc7MoE\u67b6\u6784\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12649", "categories": ["quant-ph", "math-ph", "nlin.PS", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2511.12649", "abs": "https://arxiv.org/abs/2511.12649", "authors": ["Georgy L. Alfimov", "Pavel A. Korchagin", "Dmitry E. Pelinovsky"], "title": "Stability of intrinsic localized modes on the lattice with competing power nonlinearities", "comment": "30 pages, 4 figures, 4 tables", "summary": "We study the discrete nonlinear Schrodinger equation with competing powers (p,q) satisfying 2 <= p < q. The physically relevant cases are given by (p,q) = (2,3), (p,q) = (3,4), and (p,q) = (3,5). In the anticontinuum limit, all intrinsic localized modes are compact and can be classified by their codes, which record one of two nonzero (smaller and larger) states and their sign alternations. By using the spectral stability analysis, we prove that the codes for larger states of the same sign are spectrally and nonlinearly (orbitally) stable, whereas the codes for smaller states of the alternating signs are spectrally stable but have eigenvalues of negative Krein signature. We also identify numerically the spectrally stable codes which consist of stacked combinations of the sign-definite larger states and the sign-alternating smaller states.", "AI": {"tldr": "We analyze the discrete nonlinear Schrodinger equation with competing powers (p,q) where 2 <= p < q. We found that intrinsic localized modes are compact and can be classified by codes. Larger states of the same sign are stable, while smaller states with alternating signs are spectrally stable but have negative Krein signature. We also numerically identified stable stacked combinations of these states.", "motivation": "The paper studies the discrete nonlinear Schrodinger equation with competing powers (p,q) to understand the behavior of intrinsic localized modes.", "method": "The study uses spectral stability analysis to prove stability properties of the localized modes and numerical identification for stable stacked combinations.", "result": "We proved that codes for larger states of the same sign are spectrally and nonlinearly stable. Codes for smaller states of alternating signs are spectrally stable but have eigenvalues of negative Krein signature. We numerically identified spectrally stable codes consisting of stacked combinations of larger and smaller states.", "conclusion": "The discrete nonlinear Schrodinger equation with competing powers exhibits complex localized modes with different stability properties based on their configuration and signs. Larger states of the same sign are stable, while smaller alternating sign states are spectrally stable but have negative Krein signature. Stable stacked combinations of these states were also identified numerically."}}
{"id": "2511.11989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11989", "abs": "https://arxiv.org/abs/2511.11989", "authors": ["Songsong Zhang", "Chuanqi Tang", "Hongguang Zhang", "Guijian Tang", "Minglong Li", "Xueqiong Li", "Shaowu Yang", "Yuanxi Peng", "Wenjing Yang", "Jing Zhao"], "title": "BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups", "comment": "9 pages, 10 figures", "summary": "Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8eab\u4efd\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\uff08IPPG\uff09\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u8fc7\u5ea6\u4f9d\u8d56\u9762\u90e8\u7279\u5199\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u573a\u666f\u8bed\u4e49\u521b\u5efa\u7684\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709IPPG\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9762\u90e8\u533a\u57df\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u4ee5\u9762\u90e8\u7279\u5199\u4e3a\u4e3b\uff0c\u89c6\u89c9\u53d9\u4e8b\u6027\u5dee\uff0c\u4e14\u5728\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u8bed\u4e49\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8eab\u4efd\uff08ID\uff09\u7279\u5f81\u5d4c\u5165\u524a\u5f31\u4e86\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ebf\u63a8\u7406\uff08DLI\uff09\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd-\u8bed\u4e49\u5206\u79bb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u7ebf\u67b6\u6784\u4e2dID\u548c\u8bed\u4e49\u7684\u8868\u793a\u51b2\u7a81\u3002\u8bbe\u8ba1\u4e86\u8eab\u4efd\u81ea\u9002\u5e94\u878d\u5408\uff08IdAF\uff09\u7b56\u7565\uff0c\u5c06ID-\u8bed\u4e49\u878d\u5408\u63a8\u8fdf\u5230\u566a\u58f0\u9884\u6d4b\u9636\u6bb5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u878d\u5408\u548c\u566a\u58f0\u51b3\u7b56\u63a9\u853d\uff0c\u907f\u514d\u4e86ID\u5d4c\u5165\u5bf9\u8bed\u4e49\u7684\u5e72\u6270\u3002\u5f15\u5165\u4e86\u8eab\u4efd\u805a\u5408\u9884\u5904\u7406\uff08IdAP\uff09\u6a21\u5757\uff0c\u805a\u5408ID\u4fe1\u606f\u5e76\u66ff\u4ee3\u968f\u673a\u521d\u59cb\u5316\uff0c\u589e\u5f3a\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728IPPG\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u6709\u6548\uff0c\u80fd\u591f\u751f\u6210\u8d85\u8d8a\u9762\u90e8\u7279\u5199\u7684\u56fe\u50cf\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u63a9\u853d\u6216\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u751f\u6210\u3002", "conclusion": "\u8be5IPPG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u9762\u90e8\u7ec6\u8282\uff0c\u5e76\u80fd\u5728\u73b0\u6709IPPG\u6846\u67b6\u4e2d\u5373\u63d2\u5373\u7528\uff0c\u4e3a\u5f71\u89c6\u7ea7\u89d2\u8272-\u573a\u666f\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12461", "categories": ["cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12461", "abs": "https://arxiv.org/abs/2511.12461", "authors": ["Fangqiang Du", "Sixuan Chong", "Zixuan Huang", "Rui Qin", "Fengnan Mi", "Caibao Hu", "Jiangang Chen"], "title": "Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA", "comment": null, "summary": "Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DSB Jacobi \u7684\u6570\u636e\u6d41 SVD \u5904\u7406\u7b97\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6 SVD \u8ba1\u7b97\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u5904\u7406\u573a\u666f\u4e0b\u3002", "motivation": "\u4f20\u7edf SVD \u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u77e9\u9635\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u5185\u5b58\u6d88\u8017\u5927\u4ee5\u53ca\u5ffd\u7565\u6570\u636e\u4f20\u8f93\u6311\u6218\u7b49\u95ee\u9898\uff0c\u4e0d\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u5b9e\u65f6\u6570\u636e\u6d41\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u6d41 SVD \u5904\u7406\u7b97\u6cd5\uff08DSB Jacobi\uff09\uff0c\u65e8\u5728\u51cf\u5c11\u7247\u4e0a BRAM \u6d88\u8017\u5e76\u63d0\u9ad8\u8ba1\u7b97\u901f\u5ea6\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cDSB Jacobi \u7b97\u6cd5\u5c06\u7247\u4e0a RAM \u6d88\u8017\u964d\u4f4e\u4e86 41.5%\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u4e86 23 \u500d\u3002", "conclusion": "DSB Jacobi \u7b97\u6cd5\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u6d41\u7684\u5b9e\u65f6 SVD \u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7247\u4e0a\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002"}}
{"id": "2511.11885", "categories": ["cs.DC", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11885", "abs": "https://arxiv.org/abs/2511.11885", "authors": ["Kausar Patherya", "Ashutosh Dhekne", "Francisco Romero"], "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs", "comment": "12 pages, 5 figures. Under review", "summary": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.\n  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.", "AI": {"tldr": "Flash-Fusion\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u8fb9\u7f18\u8ba1\u7b97\u548c\u4e91\u8ba1\u7b97\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u7269\u8054\u7f51\uff08IoT\uff09\u6570\u636e\u5206\u6790\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002\u5b83\u901a\u8fc7\u5728\u8fb9\u7f18\u7aef\u5bf9\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\u6458\u8981\uff08\u51cf\u5c1173.5%\u7684\u6570\u636e\u91cf\uff09\u548c\u5728\u4e91\u7aef\u8fdb\u884c\u67e5\u8be2\u89c4\u5212\uff08\u805a\u7c7b\u884c\u4e3a\u6570\u636e\u5e76\u6784\u5efa\u4e0a\u4e0b\u6587\u63d0\u793a\uff09\u6765\u89e3\u51b3\u6570\u636e\u91cf\u5927\u548c\u5206\u6790\u7f13\u6162\u7684\u95ee\u9898\u3002\u5728\u5b9e\u9645\u7684\u5927\u5b66\u516c\u4ea4\u8f66\u961f\u90e8\u7f72\u4e2d\uff0cFlash-Fusion\u5b9e\u73b0\u4e8695%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c98%\u7684\u4ee3\u5e01\u4f7f\u7528\u91cf\u53ca\u6210\u672c\u8282\u7ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u4f7f\u4e0d\u540c\u9886\u57df\u7684\u7528\u6237\u80fd\u9ad8\u6548\u5730\u67e5\u8be2IoT\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u667a\u80fd\u57ce\u5e02\u548c\u7269\u8054\u7f51\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u6d77\u91cf\u3001\u4f4e\u7ea7\u4f20\u611f\u5668\u6570\u636e\u65f6\u9762\u4e34\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u3001\u6570\u636e\u8fc7\u4e8e\u7cbe\u7ec6\u4ee5\u53ca\u5206\u6790\u7f13\u6162\u3001\u9700\u8981\u6280\u672f\u4e13\u957f\u7b49\u6311\u6218\u3002\u76f4\u63a5\u5c06\u6240\u6709\u9065\u6d4b\u6570\u636e\u8f93\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3001\u9ad8\u6602\u7684\u4ee3\u5e01\u6210\u672c\u548c\u5ef6\u8fdf\u800c\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u89e3\u6790\u7528\u6237\u67e5\u8be2\u3001\u9009\u62e9\u76f8\u5173\u6570\u636e\u5207\u7247\u3001\u9009\u62e9\u6b63\u786e\u8868\u793a\u5f62\u5f0f\u5e76\u6700\u7ec8\u8c03\u7528LLM\u7684\u7cfb\u7edf\u3002", "method": "Flash-Fusion\u7cfb\u7edf\u91c7\u7528\u4e24\u4e2a\u6838\u5fc3\u539f\u5219\uff1a1. \u57fa\u4e8e\u8fb9\u7f18\u7684\u7edf\u8ba1\u6458\u8981\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u91cf\u5927\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e8673.5%\u7684\u6570\u636e\u7f29\u51cf\u30022. \u57fa\u4e8e\u4e91\u7684\u67e5\u8be2\u89c4\u5212\uff0c\u901a\u8fc7\u5bf9\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u805a\u7c7b\u5e76\u7ec4\u88c5\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u63d0\u793a\uff0c\u6765\u89e3\u51b3\u6570\u636e\u89e3\u91ca\u7684\u95ee\u9898\u3002", "result": "\u5728\u5927\u5b66\u516c\u4ea4\u8f66\u961f\u7684\u90e8\u7f72\u548c\u8bc4\u4f30\u4e2d\uff0cFlash-Fusion\u76f8\u8f83\u4e8e\u76f4\u63a5\u5c06\u539f\u59cb\u6570\u636e\u8f93\u5165\u5148\u8fdbLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8695%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c98%\u7684\u4ee3\u5e01\u4f7f\u7528\u91cf\u53ca\u6210\u672c\u8282\u7ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\u3002", "conclusion": "Flash-Fusion\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8fb9\u7f18-\u4e91\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u8fb9\u7f18\u7aef\u7684\u7edf\u8ba1\u6458\u8981\u548c\u4e91\u7aef\u7684\u67e5\u8be2\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7269\u8054\u7f51\u6570\u636e\u7684\u91c7\u96c6\u548c\u5206\u6790\u8d1f\u62c5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u91cf\u548c\u5206\u6790\u6548\u7387\u7684\u6311\u6218\uff0c\u5e76\u4f7f\u7528\u6237\u80fd\u591f\u9ad8\u6548\u5730\u67e5\u8be2\u7269\u8054\u7f51\u6570\u636e\uff0c\u800c\u65e0\u9700\u624b\u52a8\u8fdb\u884c\u67e5\u8be2\u7f16\u5199\u6216\u9884\u5904\u7406\u3002"}}
{"id": "2511.13034", "categories": ["eess.SY", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.13034", "abs": "https://arxiv.org/abs/2511.13034", "authors": ["Rahul Misra", "Manuela L. Bujorianu", "Rafa\u0142 Wisniewski"], "title": "An Online Multiobjective Policy Gradient for Long-run Average-reward Markov Decision Process", "comment": null, "summary": "We propose a reinforcement learning (RL) framework for multi-objective decision-making, where the agent seeks to optimize a vector of rewards rather than a single scalar value. The objective is to ensure that the time-averaged reward vector converges asymptotically to a predefined target set. Since standard RL algorithms operate on scalar rewards, we introduce a dynamic scalarization mechanism guided by Blackwell's Approachability Theorem. This theorem enables adaptive updates of the scalarization vector to guarantee convergence toward the target set. Assuming ergodicity, the Markov chain induced by the learned policies admits a stationary distribution, ensuring all states recur with finite return times. Our algorithm exploits this property by defining an inner loop that applies a policy gradient method (with baseline) between successive visits to a designated recurrent state, enforcing Blackwell's condition at each iteration. An outer loop then updates the scalarization vector after each recurrence. We establish theoretical convergence of the long-run average reward vector to the target set and validate the approach through a numerical example.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u591a\u76ee\u6807\u51b3\u7b56\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u5956\u52b1\u5411\u91cf\u800c\u975e\u5355\u4e00\u6807\u91cf\u503c\uff0c\u5e76\u786e\u4fdd\u65f6\u95f4\u5e73\u5747\u5956\u52b1\u5411\u91cf\u6e10\u8fd1\u6536\u655b\u5230\u9884\u5b9a\u76ee\u6807\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u9488\u5bf9\u6807\u91cf\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u591a\u76ee\u6807\u51b3\u7b56\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5956\u52b1\u5411\u91cf\u7684\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6807\u91cf\u5316\u673a\u5236\uff0c\u8be5\u673a\u5236\u4ee5Blackwell\u7684\u65b9\u6cd5\u53ef\u8fbe\u6027\u5b9a\u7406\u4e3a\u6307\u5bfc\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u66f4\u65b0\u6807\u91cf\u5316\u5411\u91cf\uff0c\u4ee5\u4fdd\u8bc1\u6536\u655b\u5230\u76ee\u6807\u96c6\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u5e73\u7a33\u5206\u5e03\u7684\u6027\u8d28\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u548c\u6807\u91cf\u5316\u5411\u91cf\u7684\u8fed\u4ee3\u66f4\u65b0\uff0c\u786e\u4fdd\u957f\u671f\u5e73\u5747\u5956\u52b1\u5411\u91cf\u6536\u655b\u5230\u76ee\u6807\u96c6\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u957f\u671f\u5e73\u5747\u5956\u52b1\u5411\u91cf\u5411\u76ee\u6807\u96c6\u7684\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7b97\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u591a\u76ee\u6807\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6807\u91cf\u5316\u673a\u5236\u548c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5956\u52b1\u5411\u91cf\u5411\u76ee\u6807\u96c6\u7684\u6e10\u8fd1\u6536\u655b\u3002"}}
{"id": "2511.12380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12380", "abs": "https://arxiv.org/abs/2511.12380", "authors": ["Nicholas Gunter", "Heiko Kabutz", "Kaushik Jayaram"], "title": "Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots", "comment": null, "summary": "Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.", "AI": {"tldr": "\u5229\u7528\u538b\u7535\u805a\u504f\u4e8c\u6c1f\u4e59\u70ef\uff08PVDF\uff09\u7684\u591a\u5c42\u9a71\u52a8\u5668\u6709\u671b\u63d0\u9ad8\u8f6f\u5fae\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u8868\u5f81\u4e86\u5177\u6709\u5e73\u884c\u7535\u538b\u5206\u5e03\u7684\u591a\u5c42PVDF\u9a71\u52a8\u5668\uff0c\u586b\u8865\u4e86\u8106\u6027\u9ad8\u529bPZT\u5806\u53e0\u548c\u67d4\u6027\u4f46\u4f4e\u5e26\u5bbd\u7684\u8f6f\u805a\u5408\u7269\u9a71\u52a8\u5668\u4e4b\u95f4\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u5f00\u53d1\u548c\u8868\u5f81\u5177\u6709\u5e73\u884c\u7535\u538b\u5206\u5e03\u7684\u591a\u5c42PVDF\u9a71\u52a8\u5668\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u9a71\u52a8\u5668\u6280\u672f\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u5c42\u539a\u548c\u5c42\u6570\u6765\u7814\u7a76\u9a71\u52a8\u5668\u6027\u80fd\uff0c\u5e76\u4e0e\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u73b0\u4e86>3\u6beb\u7c73\u7684\u81ea\u7531\u504f\u8f6c\u3001>20\u6beb\u725b\u7684\u963b\u529b\u529b\u548c>=500\u8d6b\u5179\u7684\u9891\u7387\u54cd\u5e94\uff0c\u540c\u65f6\u5de5\u4f5c\u7535\u538b\u4f4e\u81f3150\u4f0f\u3002", "conclusion": "\u591a\u5c42PVDF\u9a71\u52a8\u5668\u5728\u6027\u80fd\uff08\u504f\u8f6c\u3001\u529b\u548c\u9891\u7387\uff09\u548c\u5de5\u4f5c\u7535\u538b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5df2\u6210\u529f\u96c6\u6210\u5230\u5171\u632f\u9a71\u52a8\u7684\u5fae\u578b\u673a\u5668\u4eba\u4e2d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u96c6\u6210\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11993", "abs": "https://arxiv.org/abs/2511.11993", "authors": ["Jiaming Liang", "Chi-Man Pun"], "title": "Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks", "comment": null, "summary": "Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6613\u53d7\u653b\u51fb\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u53d8\u6362\u7684\u653b\u51fb\u3002\u73b0\u6709\u653b\u51fb\u5728\u53c2\u6570\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u6f0f\u6d1e\u5f15\u8d77\u4e86\u793e\u4f1a\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u53d8\u6362\u7684\u653b\u51fb\u5728\u8fc1\u79fb\u653b\u51fb\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b58\u5728\u53c2\u6570\u4f18\u5316\u65b9\u9762\u7684\u76f2\u70b9\uff0c\u9650\u5236\u4e86\u5176\u5168\u90e8\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u540c\u5fc3\u8870\u51cf\u6a21\u578b\uff08CDM\uff09\u6765\u89e3\u91ca\u52a8\u6001\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5347\u540e\u964d\u6a21\u5f0f\u7684\u9ad8\u6548\u52a8\u6001\u53c2\u6570\u4f18\u5316\uff08DPO\uff09\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(mn)\u964d\u4f4e\u5230O(nlogm)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDPO\u80fd\u663e\u8457\u63d0\u9ad8\u73b0\u6709\u57fa\u4e8e\u53d8\u6362\u7684\u653b\u51fb\u5728\u4e0d\u540c\u6a21\u578b\u3001\u8fed\u4ee3\u6b21\u6570\u548c\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u63d0\u51fa\u7684CDM\u4e0eDPO\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u53d8\u6362\u7684\u653b\u51fb\u5728\u53c2\u6570\u4f18\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2511.13122", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13122", "abs": "https://arxiv.org/abs/2511.13122", "authors": ["Harsh Abhinandan", "Aditya Dhanraj", "Aryan Katoch", "R. Raja Singh"], "title": "A Comprehensive Review of Advancements in Powering and Charging Systems for Unmanned Aerial Vehicles", "comment": "This paper has been accepted for presentation at the 10th International Conference on Information and Communication Technology for Competitive Strategies (ICTCS-2025) and will be published in the conference proceedings under the Springer Lecture Notes in Networks and Systems (LNNS) series, ISSN: 2367-3370", "summary": "Unmanned Aerial Vehicles (UAVs) or drones have witnessed a spectacular surge in applications for military, commercial, and civilian purposes. However, their potential for flight is always limited by the finite power budget of their onboard power supplies. The limited flight time problem has led to intensive research into new sources of power and innovative charging strategies to enable protracted, autonomous flight. This paper gives a comparative summary of the current state-of-the-art in UAV power and refuelling technology. The paper begins with an analysis of the variety of energy sources, from classical batteries to fuel cells and hybrid systems, based on their relative advantages and disadvantages in energy density, weight, and safety. Subsequently, the review explores a spectrum of replenishment options, from simple manual battery swapping to sophisticated high-tech automatic docking stations and smart contact-based charging pads. Most of the review is dedicated to the newer technology of wireless power transfer, which involves near-field (inductive, capacitive) and far-field (laser, microwave) technology. The article also delves into the most important power electronic converter topologies, battery management systems, and control approaches that form the core of these charging systems. Finally, it recapitulates the most significant challenges in technical, economic, and social aspects for promising avenues of future research. The comprehensive review is a valuable guide for researchers, engineers, and policymakers striving to enhance UAV operational performance.", "AI": {"tldr": "\u65e0\u4eba\u673a\u5e94\u7528\u5e7f\u6cdb\u4f46\u53d7\u9650\u4e8e\u7eed\u822a\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a\u7684\u80fd\u6e90\u548c\u5145\u7535\u6280\u672f\u3002", "motivation": "\u65e0\u4eba\u673a\u56e0\u5176\u6709\u9650\u7684\u7eed\u822a\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u80fd\u6e90\u548c\u5145\u7535\u7b56\u7565\u6765\u5b9e\u73b0\u66f4\u957f\u7684\u81ea\u4e3b\u98de\u884c\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a\u7684\u80fd\u6e90\uff08\u7535\u6c60\u3001\u71c3\u6599\u7535\u6c60\u3001\u6df7\u5408\u7cfb\u7edf\uff09\u548c\u5145\u7535\u6280\u672f\uff08\u624b\u52a8\u66f4\u6362\u7535\u6c60\u3001\u81ea\u52a8\u5bf9\u63a5\u7ad9\u3001\u65e0\u7ebf\u5145\u7535\uff08\u8fd1\u573a\u548c\u8fdc\u573a\uff09\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u7684\u7535\u529b\u7535\u5b50\u8f6c\u6362\u5668\u62d3\u6251\u3001\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u6587\u7ae0\u5206\u6790\u4e86\u4e0d\u540c\u80fd\u6e90\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u79cd\u5145\u7535\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u65e0\u7ebf\u5145\u7535\u6280\u672f\uff0c\u540c\u65f6\u6d89\u53ca\u4e86\u76f8\u5173\u7684\u7535\u529b\u7535\u5b50\u6280\u672f\u548c\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u65e0\u4eba\u673a\u80fd\u6e90\u548c\u5145\u7535\u6280\u672f\u9762\u4e34\u6280\u672f\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u65b9\u9762\u7684\u6311\u6218\uff0c\u4f46\u4ecd\u6709\u5e7f\u9614\u7684\u7814\u7a76\u524d\u666f\uff0c\u672c\u6587\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8005\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2511.11907", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11907", "abs": "https://arxiv.org/abs/2511.11907", "authors": ["Huawei Zhang", "Chunwei Xia", "Zheng Wang"], "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference", "comment": null, "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.", "AI": {"tldr": "KV cache on disk for long-context LM inference on mobile devices.", "motivation": "On-device AI applications using LMs require processing long contexts, but the KV cache causes memory issues. KVSwap aims to solve this by offloading the KV cache to disk.", "method": "KVSwap stores the full KV cache on disk, uses in-memory metadata to predict critical entries, overlaps computation with disk access, and optimizes read patterns for storage devices.", "result": "KVSwap improves throughput on memory-constrained devices while preserving generation quality, outperforming existing methods.", "conclusion": "KVSwap effectively breaks the memory wall for long-context LM inference on mobile devices by offloading the KV cache to disk."}}
{"id": "2511.13117", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13117", "abs": "https://arxiv.org/abs/2511.13117", "authors": ["Anchita Dey", "Soutrik Bandyopadhyay", "Shubhendu Bhasin"], "title": "Initial Excitation-based Adaptive Observers for Discrete-Time LTI Systems", "comment": null, "summary": "In practical applications, the efficacy of a control algorithm relies critically on the accurate knowledge of the parameters and states of the underlying system. However, obtaining these quantities in practice is often challenging. Adaptive observers address this issue by performing simultaneous state and parameter estimation using only input-output measurements. While many adaptive observer designs exist for continuous-time systems, their discrete-time counterparts remain relatively unexplored. This paper proposes an initial excitation (IE)-based adaptive observer for discrete-time linear time-invariant systems. In contrast to conventional designs that rely on the persistence of excitation condition, which requires continuous excitation and infinite control effort, the proposed method does not require excitation for infinite time, thus making it more practical for stabilization tasks. We employ a two-layer filtering structure and a normalized gradient descent-based update law for learning the unknown parameters. We also propose modifying the regressors to enhance information extraction, leading to faster convergence. Rigorous theoretical analysis guarantees bounded and exponentially converging estimates of both states and parameters under the IE condition, and simulation results validate the efficacy of the proposed design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u521d\u59cb\u6fc0\u52b1\u7684\u79bb\u6563\u65f6\u95f4\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u63a7\u5236\u7b97\u6cd5\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u5bf9\u7cfb\u7edf\u53c2\u6570\u548c\u72b6\u6001\u7684\u51c6\u786e\u4e86\u89e3\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u96be\u4ee5\u83b7\u5f97\u3002\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u901a\u8fc7\u4ec5\u4f7f\u7528\u8f93\u5165\u8f93\u51fa\u6d4b\u91cf\u6765\u8fdb\u884c\u540c\u6b65\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u521d\u59cb\u6fc0\u52b1\uff08IE\uff09\u7684\u79bb\u6563\u65f6\u95f4\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\uff0c\u91c7\u7528\u4e24\u5c42\u6ee4\u6ce2\u7ed3\u6784\u548c\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u4e0b\u964d\u7684\u5b66\u4e60\u5f8b\u6765\u66f4\u65b0\u672a\u77e5\u53c2\u6570\uff0c\u5e76\u4fee\u6539\u56de\u5f52\u91cf\u4ee5\u52a0\u5feb\u6536\u655b\u901f\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u4e86\u5728IE\u6761\u4ef6\u4e0b\uff0c\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u7684\u6709\u754c\u6027\u548c\u6307\u6570\u6536\u655b\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e0e\u4f9d\u8d56\u6301\u7eed\u6fc0\u52b1\u548c\u65e0\u9650\u63a7\u5236\u8f93\u5165\u7684\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u65e0\u9650\u65f6\u95f4\u7684\u6fc0\u52b1\uff0c\u56e0\u6b64\u5728\u7a33\u5b9a\u5316\u4efb\u52a1\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12383", "abs": "https://arxiv.org/abs/2511.12383", "authors": ["Sanjar Atamuradov"], "title": "Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks", "comment": "7 pages, 5 figures", "summary": "Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.", "AI": {"tldr": "MAML-TRPO\u5728Meta-World ML10\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5c11\u6837\u672c\u9002\u5e94\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6cdb\u5316\u5dee\u8ddd\u548c\u4efb\u52a1\u9002\u5e94\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u6700\u5c11\u5316\u6570\u636e\u9700\u6c42\uff0c\u672c\u6587\u8bc4\u4f30\u4e86\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60\uff08MAML\uff09\u4e0e\u4fe1\u4efb\u57df\u7b56\u7565\u4f18\u5316\uff08TRPO\uff09\u7684\u7ed3\u5408\u3002", "method": "\u5728Meta-World ML10\u57fa\u51c6\u4e0a\uff0c\u5305\u542b\u63a8\u3001\u6293\u53d6\u548c\u62bd\u5c49\u64cd\u4f5c\u7b49\u5341\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u5e76\u5206\u6790\u4e86MAML-TRPO\u5b66\u4e60\u901a\u7528\u521d\u59cb\u5316\u7684\u80fd\u529b\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u8bed\u4e49\u64cd\u4f5c\u884c\u4e3a\u7684\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "MAML\u5728\u4e00\u6b21\u68af\u5ea6\u66f4\u65b0\u540e\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u4e00\u6b21\u6027\u9002\u5e94\uff0c\u8bad\u7ec3\u4efb\u52a1\u6210\u529f\u7387\u4e3a21.0%\uff0c\u6d4b\u8bd5\u4efb\u52a1\u6210\u529f\u7387\u4e3a13.2%\u3002\u7136\u800c\uff0c\u5728\u5143\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4e86\u6cdb\u5316\u5dee\u8ddd\uff0c\u6d4b\u8bd5\u4efb\u52a1\u6027\u80fd\u8d8b\u4e8e\u5e73\u7a33\uff0c\u800c\u8bad\u7ec3\u4efb\u52a1\u6027\u80fd\u4ecd\u5728\u63d0\u9ad8\u3002\u4e0d\u540c\u64cd\u4f5c\u4efb\u52a1\u7684\u9002\u5e94\u6027\u6548\u679c\u5dee\u5f02\u5f88\u5927\uff0c\u6210\u529f\u7387\u57280%\u523080%\u4e4b\u95f4\u3002", "conclusion": "\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u591a\u6837\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u6cdb\u5316\u5dee\u8ddd\u548c\u4efb\u52a1\u9002\u5e94\u6027\u65b9\u5dee\u5927\u7684\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u4e13\u6ce8\u4e8e\u4efb\u52a1\u611f\u77e5\u9002\u5e94\u548c\u7ed3\u6784\u5316\u7b56\u7565\u67b6\u6784\u3002"}}
{"id": "2511.12666", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2511.12666", "abs": "https://arxiv.org/abs/2511.12666", "authors": ["Disha Verma", "Indrajith VS", "R. Sankaranarayanan"], "title": "Dissipative Dynamics of Charged Graphene Quantum Batteries", "comment": null, "summary": "We investigate dissipative dynamics in a graphene-based quantum battery modeled as a four level spin valley system. The battery is charged via a Gaussian pulse and subsequently evolves under amplitude damping, dephasing, and both Markovian and non Markovian reservoirs. We find that amplitude damping, while inducing energy loss, can stabilize non passive steady states with finite ergotropy, whereas pure dephasing suppresses coherence and eliminates work extraction. On the other hand, non-Markovian memory slows ergotropy loss and enables partial recovery through information backflow. These results identify coherence and reservoir memory as essential resources for enhancing the long-time performance of graphene quantum batteries.", "AI": {"tldr": "\u77f3\u58a8\u70ef\u91cf\u5b50\u7535\u6c60\u7684\u8017\u6563\u52a8\u529b\u5b66\uff1a\u76f8\u5e72\u6027\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u662f\u5173\u952e\u8d44\u6e90\u3002", "motivation": "\u7814\u7a76\u77f3\u58a8\u70ef\u57fa\u91cf\u5b50\u7535\u6c60\u5728\u8017\u6563\u73af\u5883\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u91cd\u70b9\u5173\u6ce8\u76f8\u5e72\u6027\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u5bf9\u7535\u6c60\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u77f3\u58a8\u70ef\u91cf\u5b50\u7535\u6c60\u5efa\u6a21\u4e3a\u56db\u80fd\u7ea7\u81ea\u65cb\u8c37\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u65af\u8109\u51b2\u5145\u7535\uff0c\u5e76\u5728\u632f\u5e45\u963b\u5c3c\u3001\u9000\u76f8\u5e72\u4ee5\u53ca\u9a6c\u5c14\u53ef\u592b\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u4e24\u79cd\u7c7b\u578b\u7684\u8017\u6563\u73af\u5883\u4e0b\u8fdb\u884c\u6f14\u5316\uff0c\u5206\u6790\u80fd\u91cf\u635f\u8017\u548c\u529f\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u632f\u5e45\u963b\u5c3c\u5bfc\u81f4\u80fd\u91cf\u635f\u5931\u4f46\u80fd\u7a33\u5b9a\u975e\u88ab\u52a8\u7a33\u6001\u5e76\u4ea7\u751f\u6709\u9650\u7684\u6709\u6548\u80fd\uff1b\u7eaf\u9000\u76f8\u5e72\u6291\u5236\u76f8\u5e72\u6027\u5e76\u6d88\u9664\u529f\u63d0\u53d6\uff1b\u975e\u9a6c\u5c14\u53ef\u592b\u8bb0\u5fc6\u6548\u5e94\u80fd\u51cf\u7f13\u6709\u6548\u80fd\u7684\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u56de\u6d41\u5b9e\u73b0\u90e8\u5206\u6062\u590d\u3002", "conclusion": "\u76f8\u5e72\u6027\u548c\u8017\u6563\u73af\u5883\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u8bb0\u5fc6\u662f\u63d0\u9ad8\u77f3\u58a8\u70ef\u91cf\u5b50\u7535\u6c60\u957f\u671f\u6027\u80fd\u7684\u5173\u952e\u8d44\u6e90\u3002"}}
{"id": "2511.12005", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12005", "abs": "https://arxiv.org/abs/2511.12005", "authors": ["Xinyu He", "Botong Zhao", "Bingbing Li", "Shujing Lyu", "Jiwei Shen", "Yue Lu"], "title": "LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation", "comment": null, "summary": "Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.", "AI": {"tldr": "LithoSeg\u662f\u4e00\u4e2a\u7528\u4e8e\u5149\u523b\u626b\u63cf\u7535\u955c\u56fe\u50cf\u5206\u5272\u7684\u7c97\u5230\u7cbe\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u548c1D\u56de\u5f52\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u76d1\u7763\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u523b\u5206\u5272\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faLithoSeg\uff0c\u4e00\u4e2a\u7c97\u5230\u7cbe\u7684\u7f51\u7edc\u3002\u7c97\u7565\u9636\u6bb5\uff1a\u4f7f\u7528\u4eba\u673a\u4ea4\u4e92\u5f15\u5bfc\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u4ee5\u5b9e\u73b0\u6700\u5c0f\u76d1\u7763\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7cbe\u7ec6\u9636\u6bb5\uff1a\u5c062D\u5206\u5272\u91cd\u5851\u4e3a1D\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u91c7\u6837\u6c9f\u69fd\u6cd5\u7ebf\u8f6e\u5ed3\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7MLP\u8fdb\u884c\u9010\u70b9\u4f18\u5316\u3002", "result": "LithoSeg\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6d4b\u91cf\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u76d1\u7763\u3002", "conclusion": "LithoSeg\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5206\u5272\u7cbe\u5ea6\u3001\u6d4b\u91cf\u7cbe\u5ea6\u548c\u76d1\u7763\u9700\u6c42\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6539\u8fdb\u3002"}}
{"id": "2511.13680", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13680", "abs": "https://arxiv.org/abs/2511.13680", "authors": ["Leopoldo Agorio", "Juan Cervi\u00f1o", "Miguel Calvo-Fullana", "Alejandro Ribeiro", "Juan Andr\u00e9s Bazerque"], "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization", "comment": "13 pages, 11 figures", "summary": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.", "AI": {"tldr": "\u901a\u8fc7\u591a\u4efb\u52a1\u8054\u5408\u5b66\u4e60\u6765\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u5f53\u6570\u636e\u6709\u9650\u65f6\uff0c\u5b66\u4e60\u5230\u7684\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u60c5\u51b5\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u4efb\u52a1\u7684\u4ea4\u53c9\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8054\u5408\u53c2\u6570\u4f30\u8ba1\u8868\u8ff0\u4e3a\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u7ea6\u675f\u6761\u4ef6\u786e\u4fdd\u4e86\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u4fe1\u606f\u6574\u5408\u3002", "result": "\u5728\u5177\u6709\u9ad8\u65af\u6570\u636e\u7684\u53d7\u63a7\u6846\u67b6\u4e0b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u4f20\u67d3\u75c5\u4f20\u64ad\u7b49\u771f\u5b9e\u6570\u636e\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u4ea4\u53c9\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u5728\u6570\u636e\u5145\u8db3\u7684\u4efb\u52a1\u4e2d\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u6570\u636e\u7a00\u758f\u7684\u4efb\u52a1\u4e2d\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u4e3a\u4ece\u6709\u9650\u6570\u636e\u4e2d\u8fdb\u884c\u53c2\u6570\u63a8\u7406\u7684\u5173\u952e\u573a\u666f\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12009", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12009", "abs": "https://arxiv.org/abs/2511.12009", "authors": ["Guangchao Yao", "Yali Li"], "title": "High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts", "comment": null, "summary": "The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528NVIDIA GPU\u5e73\u53f0\u4e0a\u7684\u8fed\u4ee3\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u7b97\u6cd5\uff0c\u6210\u529f\u572828.4\u5929\u5185\u9a8c\u8bc1\u4e8627\u8def\u6613\u65af\u95ee\u9898\uff0c\u5e76\u9884\u6d4b11\u4e2a\u6708\u5185\u53ef\u89e3\u51b328\u8def\u6613\u65af\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709GPU\u65b9\u6cd5\u768410\u500d\u523026\u500d\u7684\u52a0\u901f\u3002 ", "motivation": "N\u8def\u6613\u65af\u95ee\u9898\u662f\u4e00\u4e2a\u7ecf\u5178\u7684NP\u5b8c\u5168\u95ee\u9898\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\uff0c\u76ee\u524d\u4ec5\u80fd\u9a8c\u8bc1\u5230N<=26\u7684\u89e3\u3002\u4e4b\u524d\u7684\u7814\u7a76\uff08\u59822016\u5e74PreuBer\u56e2\u961f\u4f7f\u7528FPGA\uff09\u867d\u7136\u89e3\u51b3\u4e8627\u8def\u6613\u65af\u95ee\u9898\uff0c\u4f46\u8017\u65f6\u4e00\u5e74\u4e14\u672a\u7ecf\u72ec\u7acb\u9a8c\u8bc1\u3002\u8fd1\u671fGPU\u5e76\u884c\u8ba1\u7b97\u7684\u7814\u7a76\u8868\u660e\uff0c\u9a8c\u8bc127\u8def\u6613\u65af\u95ee\u9898\u4ecd\u970017\u4e2a\u6708\uff0c\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684NVIDIA GPU\u5e73\u53f0\u5e76\u884c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1. \u8fed\u4ee3\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u7b97\u6cd5\uff1b2. \u5c06\u5806\u6808\u7ed3\u6784\u5b8c\u5168\u6620\u5c04\u5230GPU\u5171\u4eab\u5185\u5b58\uff1b3. \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u6709\u6548\u907f\u514d\u94f6\u884c\u51b2\u7a81\uff1b4. \u91c7\u7528\u591a\u79cd\u4f18\u5316\u6280\u672f\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "result": "\u5728\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u4e0b\uff0c\u4f7f\u75288\u5757RTX 5090 GPU\uff0c\u572828.4\u5929\u5185\u6210\u529f\u9a8c\u8bc1\u4e8627\u8def\u6613\u65af\u95ee\u9898\uff0c\u786e\u8ba4\u4e86PreuBer\u8ba1\u7b97\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002\u540c\u65f6\uff0c\u5c0628\u8def\u6613\u65af\u95ee\u9898\u7684\u9884\u8ba1\u6c42\u89e3\u65f6\u95f4\u7f29\u77ed\u81f3\u7ea611\u4e2a\u6708\uff0c\u4f7f\u5176\u5728\u8ba1\u7b97\u4e0a\u53d8\u5f97\u53ef\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728NVIDIA GPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u5bf9N\u8def\u6613\u65af\u95ee\u9898\u7684\u663e\u8457\u52a0\u901f\uff0c\u9a8c\u8bc1\u4e8627\u8def\u6613\u65af\u95ee\u9898\u7684\u89e3\uff0c\u5e76\u4f7f28\u8def\u6613\u65af\u95ee\u9898\u7684\u89e3\u51b3\u6210\u4e3a\u53ef\u80fd\u3002\u4e0e\u73b0\u6709GPU\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u786c\u4ef6\u914d\u7f6e\uff088\u5757A100\uff09\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc710\u500d\u7684\u52a0\u901f\uff0c\u5728\u4f7f\u75288\u5757RTX 5090 GPU\u65f6\u66f4\u662f\u5b9e\u73b0\u4e86\u8d85\u8fc726\u500d\u7684\u52a0\u901f\uff0c\u4e3a\u8be5\u957f\u671f\u505c\u6ede\u7684\u95ee\u9898\u5e26\u6765\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13119", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13119", "abs": "https://arxiv.org/abs/2511.13119", "authors": ["Xuxin Yang", "Xue Yuan", "Donghan Feng", "Siru Chen", "Yuanhao Feng"], "title": "Carbon Reduction Potential and Sensitivity Analysis of Rural Integrated Energy System with Carbon Trading and Coordinated Electric-Thermal Demand Response", "comment": null, "summary": "Constructing clean and low-carbon rural integrated energy system (RIES) is a fundamental requirement for supporting China's rural modernization and new-type urbanization. Existing research on RIES decarbonization primarily focuses on the optimal low-carbon operation of system-level energy devices at the macro level, while the synergistic carbon-reduction effects of demand-side flexible loads and external carbon trading mechanisms have not been fully explored. Meanwhile, at the micro level, the carbon sensitivity of device parameters and their potential contribution to emission reduction remain insufficiently investigated. To address these gaps, this study integrates macro- and micro-level analyses. At the macro level, a multi-energy-coupled low-carbon optimal operation framework is developed, incorporating coordinated electric-thermal demand response (DR) and carbon trading. At the micro level, a carbon emission model for RIES components is established, and sensitivity analysis is conducted on 28 carbon-related parameters to identify highly sensitive determinants of emission reduction. Case studies based on typical operation data from a rural region in northern China demonstrate that coordinated electric-thermal DR and carbon trading can achieve maximum carbon-reduction potential. Furthermore, the identified high-sensitivity parameters provide essential theoretical guidance for enhancing the decarbonization potential of RIES.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b8f\u89c2\u548c\u5fae\u89c2\u5206\u6790\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u519c\u6751\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf\uff08RIES\uff09\u7684\u4f4e\u78b3\u8fd0\u884c\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u5ffd\u89c6\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\u548c\u5916\u90e8\u78b3\u4ea4\u6613\u673a\u5236\u534f\u540c\u51cf\u6392\u6548\u5e94\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u519c\u6751\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf\uff08RIES\uff09\u8131\u78b3\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5b8f\u89c2\u5c42\u9762\u7684\u7cfb\u7edf\u8bbe\u5907\u4f18\u5316\u8fd0\u884c\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u9700\u6c42\u4fa7\u67d4\u6027\u8d1f\u8377\u548c\u5916\u90e8\u78b3\u4ea4\u6613\u673a\u5236\u7684\u534f\u540c\u51cf\u6392\u6548\u5e94\uff0c\u5e76\u4e14\u5bf9\u5fae\u89c2\u5c42\u9762\u8bbe\u5907\u53c2\u6570\u7684\u78b3\u654f\u611f\u6027\u53ca\u5176\u51cf\u6392\u6f5c\u529b\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u672c\u7814\u7a76\u878d\u5408\u4e86\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u7684\u5206\u6790\u3002\u5728\u5b8f\u89c2\u5c42\u9762\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u80fd\u6e90\u8026\u5408\u7684\u4f4e\u78b3\u6700\u4f18\u8fd0\u884c\u6846\u67b6\uff0c\u6574\u5408\u4e86\u534f\u8c03\u7684\u7535\u70ed\u9700\u6c42\u54cd\u5e94\uff08DR\uff09\u548c\u78b3\u4ea4\u6613\u3002\u5728\u5fae\u89c2\u5c42\u9762\uff0c\u5efa\u7acb\u4e86RIES\u7ec4\u4ef6\u7684\u78b3\u6392\u653e\u6a21\u578b\uff0c\u5e76\u5bf928\u4e2a\u78b3\u76f8\u5173\u53c2\u6570\u8fdb\u884c\u4e86\u654f\u611f\u6027\u5206\u6790\uff0c\u4ee5\u8bc6\u522b\u51cf\u6392\u6548\u679c\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u3002", "result": "\u57fa\u4e8e\u4e2d\u56fd\u5317\u65b9\u67d0\u519c\u6751\u5730\u533a\u7684\u5178\u578b\u8fd0\u884c\u6570\u636e\u8fdb\u884c\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u534f\u8c03\u7684\u7535\u70ed\u9700\u6c42\u54cd\u5e94\uff08DR\uff09\u548c\u78b3\u4ea4\u6613\u53ef\u4ee5\u5b9e\u73b0\u6700\u5927\u7684\u51cf\u6392\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u8bc6\u522b\u51fa\u7684\u9ad8\u654f\u611f\u6027\u53c2\u6570\u4e3a\u589e\u5f3aRIES\u7684\u8131\u78b3\u6f5c\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u6307\u5bfc\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u5b8f\u89c2\uff08\u9700\u6c42\u54cd\u5e94\u548c\u78b3\u4ea4\u6613\uff09\u548c\u5fae\u89c2\uff08\u78b3\u6392\u653e\u6a21\u578b\u548c\u654f\u611f\u6027\u5206\u6790\uff09\u5c42\u9762\u7684\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u4e3a\u5b9e\u73b0\u519c\u6751\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf\u7684\u4f4e\u78b3\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2511.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12390", "abs": "https://arxiv.org/abs/2511.12390", "authors": ["Sanjar Atamuradov"], "title": "Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control", "comment": "9 pages, 5 figures", "summary": "Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u795e\u7ecf\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236\u4eba\u5f62\u673a\u5668\u4eba\u8fdb\u884c\u590d\u6742\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\u4f9d\u8d56\u9006\u8fd0\u52a8\u5b66\uff08IK\uff09\u6c42\u89e3\u5668\u548c\u624b\u52a8\u8c03\u6574\u7684PD\u63a7\u5236\u5668\uff0c\u96be\u4ee5\u5904\u7406\u5916\u90e8\u529b\u3001\u9002\u5e94\u4e0d\u540c\u7528\u6237\u4ee5\u53ca\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u4ea7\u751f\u81ea\u7136\u7684\u8fd0\u52a8\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7b56\u7565\u6765\u76f4\u63a5\u6620\u5c04VR\u63a7\u5236\u5668\u8f93\u5165\u5230\u673a\u5668\u4eba\u5173\u8282\u6307\u4ee4\uff0c\u4ece\u800c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684IK+PD\u6d41\u7a0b\u3002\u7b56\u7565\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u4e86\u529b\u968f\u673a\u5316\u548c\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u4e0eIK\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u7b56\u7565\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e8634%\u7684\u66f4\u4f4e\u8ddf\u8e2a\u8bef\u5dee\u300145%\u7684\u66f4\u5e73\u6ed1\u8fd0\u52a8\u4ee5\u53ca\u66f4\u4f18\u8d8a\u7684\u529b\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\uff0850Hz\u63a7\u5236\u9891\u7387\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4eba\u5f62\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u81ea\u7136\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5df2\u5728\u7269\u4f53\u6293\u53d6\u3001\u5f00\u95e8\u548c\u53cc\u81c2\u534f\u8c03\u7b49\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2511.12700", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2511.12700", "abs": "https://arxiv.org/abs/2511.12700", "authors": ["Matthew Duschenes", "Diego Garc\u00eda-Mart\u00edn", "Zo\u00eb Holmes", "M. Cerezo"], "title": "Moments of quantum channel ensembles", "comment": "11+26 pages, 5+5 figures", "summary": "Moments of ensembles of unitaries play a central role in quantum information theory as they capture the statistical properties of dynamics of systems with some form of randomness. Indeed, concepts such as approximate $t$-designs arise when comparing how close an associated moment operator of a given unitary ensemble is to that of another, reference ensemble. Despite the importance of moment operators, their properties have not been as explored for quantum channels. In this work we develop a theoretical framework to compute moment operators for ensembles of quantum channels, for all moment orders $t$, with a special focus on determining ensembles that can be used as points of reference. By deriving hierarchies between ensembles, via inequalities of their moment operator norms, we give them operational meaning, and define useful concepts such as that of channel $t$-designs. Finally, we perform theoretical and numerical studies which show that different types of noise can decrease the norm of the moment operators (e.g., depolarizing noise), as well as increase it (e.g., amplitude damping), and generalize noise-induced concentration phenomena to channel-design-induced phenomena. Along the way, we find a block-orthogonal basis for permutations, which greatly simplifies our analyses, and may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u91cf\u5b50\u4fe1\u9053\u7cfb\u7efc\u77e9\u7b97\u5b50\u7684\u7406\u8bba\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u786e\u5b9a\u53c2\u8003\u7cfb\u7efc\uff0c\u5e76\u5b9a\u4e49\u4e86\u4fe1\u9053t-\u8bbe\u8ba1\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u566a\u58f0\u4f1a\u5f71\u54cd\u77e9\u7b97\u5b50\u7684\u8303\u6570\uff0c\u5e76\u63a8\u5e7f\u4e86\u566a\u58f0\u8bf1\u5bfc\u7684\u6d53\u5ea6\u73b0\u8c61\u3002", "motivation": "\u91cf\u5b50\u4fe1\u9053\u7cfb\u7efc\u7684\u77e9\u7b97\u5b50\u5728\u91cf\u5b50\u4fe1\u606f\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5bf9\u5176\u6027\u8d28\u7684\u7814\u7a76\u4e0d\u5982\u91cf\u5b50\u4f4d\u7cfb\u7efc\u5145\u5206\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u8ba1\u7b97\u6240\u6709\u9636\u6570t\u7684\u91cf\u5b50\u4fe1\u9053\u7cfb\u7efc\u7684\u77e9\u7b97\u5b50\uff0c\u5e76\u901a\u8fc7\u4e0d\u7b49\u5f0f\u63a8\u5bfc\u4e86\u7cfb\u7efc\u4e4b\u95f4\u7684\u5c42\u7ea7\u5173\u7cfb\uff0c\u4ece\u800c\u5b9a\u4e49\u4e86\u4fe1\u9053t-\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u4e86\u4e0d\u540c\u7c7b\u578b\u566a\u58f0\uff08\u5982\u9000\u5316\u566a\u58f0\u548c\u5e45\u5ea6\u8870\u51cf\u566a\u58f0\uff09\u5bf9\u77e9\u7b97\u5b50\u8303\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5e7f\u4e86\u566a\u58f0\u8bf1\u5bfc\u7684\u6d53\u5ea6\u73b0\u8c61\u3002\u53d1\u73b0\u4e86\u4e00\u4e2a\u5757\u6b63\u4ea4\u6392\u5217\u57fa\uff0c\u7b80\u5316\u4e86\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u91cf\u5b50\u4fe1\u9053\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u89c1\u89e3\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u968f\u673a\u52a8\u529b\u5b66\u548c\u566a\u58f0\u65b9\u9762\u3002"}}
{"id": "2511.12006", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12006", "abs": "https://arxiv.org/abs/2511.12006", "authors": ["Kai-Wen K. Yang", "Andrew Bai", "Alexandra Bermudez", "Yunqi Hong", "Zoe Latham", "Iris Sloan", "Michael Liu", "Vishrut Goyal", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy", "comment": null, "summary": "Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.", "AI": {"tldr": "\u901a\u8fc7\u4ec5\u8c03\u6574\u65e9\u671f\u5377\u79ef\u5c42\u5e76\u51bb\u7ed3\u66f4\u6df1\u5c42\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u663e\u5fae\u955c\u56fe\u50cf\u57df\u81ea\u9002\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSIT-ADDA-Auto\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u81ea\u52a8\u9009\u62e9\u9002\u5e94\u6df1\u5ea6\uff0c\u65e0\u9700\u76ee\u6807\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u6027\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff08\u5982ADDA\uff09\u5728\u5e94\u7528\u4e8e\u65b0\u4eea\u5668\u6216\u91c7\u96c6\u8bbe\u7f6e\u7684\u663e\u5fae\u955c\u56fe\u50cf\u65f6\uff0c\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc\uff0c\u8fd9\u53ef\u80fd\u4f1a\u7834\u574f\u5df2\u5b66\u4e60\u7684\u8bed\u4e49\u8868\u793a\u3002\u7136\u800c\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u4ec5\u9002\u5e94\u6700\u65e9\u7684\u5377\u79ef\u5c42\u5e76\u51bb\u7ed3\u66f4\u6df1\u5c42\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8fc1\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b50\u7f51\u7edc\u56fe\u50cf\u8f6c\u6362\u5bf9\u6297\u6027\u57df\u81ea\u9002\u5e94\uff08SIT-ADDA-Auto\uff09\u7684\u81ea\u914d\u7f6e\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u6d45\u5c42\u5bf9\u6297\u6027\u5bf9\u9f50\u4e0e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u76ee\u6807\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u81ea\u52a8\u9009\u62e9\u9002\u5e94\u6df1\u5ea6\u3002\u901a\u8fc7\u591a\u6307\u6807\u8bc4\u4f30\u3001\u76f2\u6cd5\u4e13\u5bb6\u8bc4\u4f30\u548c\u4e0d\u786e\u5b9a\u6027-\u6df1\u5ea6\u6d88\u878d\u7814\u7a76\u6765\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u3002", "result": "SIT-ADDA\u5728\u66dd\u5149\u548c\u7167\u660e\u53d8\u5316\u3001\u8de8\u4eea\u5668\u8fc1\u79fb\u4ee5\u53ca\u591a\u79cd\u67d3\u8272\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u5b8c\u6574\u7684\u7f16\u7801\u5668\u9002\u5e94\u548c\u975e\u5bf9\u6297\u6027\u57fa\u7ebf\uff0c\u5728\u91cd\u5efa\u548c\u4e0b\u6e38\u5206\u5272\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u51cf\u5c11\u8bed\u4e49\u7279\u5f81\u7684\u6f02\u79fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u663e\u5fae\u955c\u56fe\u50cf\u7684\u65e0\u6807\u7b7e\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.12025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12025", "abs": "https://arxiv.org/abs/2511.12025", "authors": ["Ivan Cao", "Jaromir J. Saloni", "David A. G. Harrison"], "title": "A Quick and Exact Method for Distributed Quantile Computation", "comment": "10 pages, 2 figures. Draft version for testing and feedback", "summary": "Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.", "AI": {"tldr": "Spark \u7684 GK Select \u7b97\u6cd5\u80fd\u5728\u4fdd\u8bc1\u7cbe\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u907f\u514d\u5168\u5c40\u6392\u5e8f\u548c\u4ec5\u5728\u5c40\u90e8\u6570\u636e\u4e0a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e0e GK Sketch \u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5728 Spark \u4e2d\uff0c\u7cbe\u786e\u8ba1\u7b97\u5206\u4f4d\u6570\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u5168\u5c40\u6392\u5e8f\uff0c\u800c\u73b0\u6709\u7684\u8fd1\u4f3c\u65b9\u6cd5 GK Sketch \u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u7cbe\u786e\u7ed3\u679c\u7684\u573a\u666f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u7cbe\u786e\u6027\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "GK Select \u7b97\u6cd5\u9996\u5148\u5229\u7528 GK Sketch \u786e\u5b9a\u4e00\u4e2a\u8fd1\u4f3c\u76ee\u6807\u67a2\u7ebd\u503c\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u7ebf\u6027\u65f6\u95f4\u63d0\u53d6\u8bef\u5dee\u8303\u56f4\u5185\u7684\u5019\u9009\u503c\uff0c\u6700\u540e\u901a\u8fc7\u6811\u5f52\u7ea6\uff08tree-reduce\uff09\u5408\u5e76\u8fd9\u4e9b\u5019\u9009\u96c6\u4ee5\u83b7\u5f97\u7cbe\u786e\u5206\u4f4d\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u5168\u6570\u636e\u6df7\u6d17\uff08shuffle\uff09\uff0c\u5e76\u4e14\u4ec5\u9700\u5e38\u6570\u6b21\u64cd\u4f5c\u3002", "result": "GK Select \u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u5b9e\u73b0\u4e86\u4e0e GK Sketch \u76f8\u540c\u7684\u6267\u884c\u5668\u4fa7\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4f46\u80fd\u8fd4\u56de\u7cbe\u786e\u5206\u4f4d\u6570\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u5904\u7406 10^9 \u4e2a\u6570\u636e\u70b9\u3001\u8de8\u8d8a 120 \u4e2a\u5206\u533a\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e GK Sketch \u76f8\u5f53\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u6bd4 Spark \u7684\u5168\u5c40\u6392\u5e8f\u65b9\u6cd5\u5feb\u7ea6 10.5 \u500d\u3002", "conclusion": "GK Select \u7b97\u6cd5\u6210\u529f\u5730\u89e3\u51b3\u4e86 Spark \u4e2d\u7cbe\u786e\u8ba1\u7b97\u5206\u4f4d\u6570\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u5177\u7cbe\u786e\u6027\u548c\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.12436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12436", "abs": "https://arxiv.org/abs/2511.12436", "authors": ["Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Yanbiao Ma", "Yunfeng Diao", "Ziyu Jia", "Wenbo Ding", "Hangjun Ye", "Long Chen"], "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation", "comment": null, "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.", "AI": {"tldr": "RoboAfford++\u662f\u4e00\u4e2a\u5305\u542b86.9\u4e07\u5f20\u56fe\u50cf\u548c200\u4e07\u4e2a\u95ee\u7b54\u6ce8\u91ca\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u7684\u901a\u7528\u5b66\u4e60\u3002RoboAfford-Eval\u662f\u4e00\u4e2a\u5305\u542b338\u4e2a\u6ce8\u91ca\u6837\u672c\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u901a\u7528\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u63a8\u65ad\u7269\u7406\u4ea4\u4e92\u7684\u53ef\u884c\u4f4d\u7f6e\uff08\u5982\u6293\u63e1\u70b9\u548c\u653e\u7f6e\u533a\u57df\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u7269\u4f53\u548c\u7a7a\u95f4\u901a\u7528\u6027\u6ce8\u91ca\u3002", "method": "\u63d0\u51faRoboAfford++\u6570\u636e\u96c6\uff08\u5305\u542b869,987\u5f20\u56fe\u50cf\u548c200\u4e07\u4e2a\u95ee\u7b54\u6ce8\u91ca\uff09\u548cRoboAfford-Eval\u57fa\u51c6\uff08\u5305\u542b338\u4e2a\u6ce8\u91ca\u6837\u672c\uff09\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u901a\u7528\u6027\u5b66\u4e60\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u73b0\u6709\u7684VLMs\u5728\u901a\u7528\u6027\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u4f7f\u7528RoboAfford++\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5b83\u4eec\u5bf9\u7269\u4f53\u548c\u7a7a\u95f4\u901a\u7528\u6027\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "RoboAfford++\u6570\u636e\u96c6\u548cRoboAfford-Eval\u57fa\u51c6\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5b8c\u6210\u64cd\u4f5c\u548c\u5bfc\u822a\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12721", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12721", "abs": "https://arxiv.org/abs/2511.12721", "authors": ["Miguel Castillo-Celeita", "Matteo Schiavon"], "title": "The role of averages in CV-QKD over fast fading channels", "comment": null, "summary": "This work presents a study of continuous-variable quantum key distribution (CV-QKD) protocols over fast-fading channels, typically found in free-space communication links. Two eavesdropping models are considered to evaluate their security under collective attacks: \\textit{Holevo bound average} (HBA) and \\textit{covariance matrix average} (CMA). In the HBA approach, the Holevo bound is averaged over the channel transmittance. In contrast, the CMA method calculates the Holevo bound from the average covariance matrix. Analytical expressions are developed for both strategies. The two methods also differ in how they calculate the mutual information between the legitimate parties. The results demonstrate that the SKR is significantly influenced by how you treat channel fluctuations, highlighting the importance of choosing the model that better describes the actual implementation of the protocol.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u81ea\u7531\u7a7a\u95f4\u901a\u4fe1\u94fe\u8def\u4e2d\u5e38\u89c1\u7684\u5feb\u901f\u8870\u843d\u4fe1\u9053\u4e0a\u7684\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08CV-QKD\uff09\u534f\u8bae\u3002", "motivation": "\u8bc4\u4f30CV-QKD\u534f\u8bae\u5728\u5feb\u901f\u8870\u843d\u4fe1\u9053\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u5e76\u8003\u8651\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u7a83\u542c\u6a21\u578b\uff1aHolevo\u754c\u5e73\u5747\uff08HBA\uff09\u548c\u534f\u65b9\u5dee\u77e9\u9635\u5e73\u5747\uff08CMA\uff09\u3002", "method": "\u4e3aHBA\u548cCMA\u4e24\u79cd\u7b56\u7565\u5f00\u53d1\u4e86\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u8ba1\u7b97\u5408\u6cd5\u65b9\u4e4b\u95f4\u4e92\u4fe1\u606f\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5b89\u5168\u5bc6\u94a5\u7387\uff08SKR\uff09\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u4fe1\u9053\u8d77\u4f0f\u5904\u7406\u65b9\u5f0f\u7684\u5f71\u54cd\u3002", "conclusion": "\u9009\u62e9\u6700\u80fd\u63cf\u8ff0\u534f\u8bae\u5b9e\u9645\u5b9e\u73b0\u7684\u6a21\u578b\u5bf9\u4e8eCV-QKD\u534f\u8bae\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.12031", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12031", "abs": "https://arxiv.org/abs/2511.12031", "authors": ["Arun Ramachandran", "Ramaswamy Govindarajan", "Murali Annavaram", "Prakash Raghavendra", "Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang"], "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding", "comment": null, "summary": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.", "AI": {"tldr": "KV cache\u66f4\u65b0\u7684\u5f00\u9500\u5f88\u5927\uff0cBMC\u901a\u8fc7r\u6b21\u8fed\u4ee3\u5206\u914d\u4e00\u6b21KV\u5f20\u91cf\u6765\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5229\u7528\u591a\u4f59\u7684\u884c\u8fdb\u884c\u6295\u673a\u89e3\u7801\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740GPU\u6210\u672c\u7684\u98d9\u5347\uff0c\u4f7f\u7528CPU\u8fdb\u884cLLM\u63a8\u7406\u53d8\u5f97\u975e\u5e38\u5fc5\u8981\u3002\u7136\u800c\uff0cKV cache\u66f4\u65b0\u7684\u5f00\u9500\u5f88\u5927\uff0c\u5c24\u5176\u662f\u5728\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u65f6\u3002", "method": "BMC\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684KV cache\u5206\u914d\u673a\u5236\uff0c\u8be5\u673a\u5236\u6bcfr\u6b21\u8fed\u4ee3\u5206\u914d\u4e00\u6b21\u5177\u6709r\u4e2a\u5197\u4f59\u884c\u7684KV\u5f20\u91cf\uff0c\u5141\u8bb8\u5728\u8fd9\u4e9b\u8fed\u4ee3\u4e2d\u8fdb\u884c\u539f\u5730\u66f4\u65b0\uff0c\u800c\u6ca1\u6709\u590d\u5236\u5f00\u9500\uff0c\u5e76\u5229\u7528\u591a\u4f59\u7684\u884c\u8fdb\u884c\u6295\u673a\u89e3\u7801\u3002", "result": "BMC\u7684\u5e73\u5747\u541e\u5410\u91cf\u52a0\u901f\u6700\u9ad8\u53ef\u8fbe3.2\u500d\uff08\u76f8\u6bd4\u57fa\u7ebfHuggingFace\uff09\uff0c\u4e0e\u6295\u673a\u89e3\u7801\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u989d\u5916\u7684\u52a0\u901f\u53ef\u8fbe1.39\u500d\u3002\u4e0evLLM\u548cDeepSpeed\u76f8\u6bd4\uff0cBMC\u7684\u52a0\u901f\u5206\u522b\u53ef\u8fbe1.36\u500d\u548c2.29\u500d\u3002BMC\u5728CPU\u548cGPU\u4e0a\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "BMC\u901a\u8fc7\u5e73\u8861\u5185\u5b58\u548c\u8ba1\u7b97\uff0c\u6709\u6548\u89e3\u51b3\u4e86KV cache\u66f4\u65b0\u7684\u5f00\u9500\u95ee\u9898\uff0c\u5e76\u80fd\u4e0e\u6295\u673a\u89e3\u7801\u7ed3\u5408\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\u3002"}}
{"id": "2511.13162", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13162", "abs": "https://arxiv.org/abs/2511.13162", "authors": ["Yifan Wang", "Yiyao Yu", "Yang Xia", "Yan Xu"], "title": "Cyber-Resilient Fault Diagnosis Methodology in Inverter-Based Resource-Dominated Microgrids with Single-Point Measurement", "comment": "5 pages, 5 figures", "summary": "Cyber-attacks jeopardize the safe operation of inverter-based resource-dominated microgrids (IBR-dominated microgrids). At the same time, existing diagnostic methods either depend on expensive multi-point instrumentation or stringent modeling assumptions that are untenable under single-point measurement constraints. This paper proposes a Fractional-Order Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves timely fault localization and cyber-resilient fault diagnosis using only one VPQ (voltage, active power, reactive power) measurement point. FO-MADS first constructs a dual fractional-order feature library by jointly applying Caputo and Gr\u00fcnwald-Letnikov derivatives, thereby amplifying micro-perturbations and slow drifts in the VPQ signal. A two-stage hierarchical classifier then pinpoints the affected inverter and isolates the faulty IGBT switch, effectively alleviating class imbalance. Robustness is further strengthened through Progressive Memory-Replay Adversarial Training (PMR-AT), whose attack-aware loss is dynamically re-weighted via Online Hard Example Mining (OHEM) to prioritize the most challenging samples. Experiments on a four-inverter IBR-dominated microgrid testbed comprising 1 normal and 24 fault classes under four attack scenarios demonstrate diagnostic accuracies of 96.6% (bias), 94.0% (noise), 92.8% (data replacement), and 95.7% (replay), while sustaining 96.7% under attack-free conditions. These results establish FO-MADS as a cost-effective and readily deployable solution that markedly enhances the cyber-physical resilience of IBR-dominated microgrids.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFO-MADS\u7684\u65b0\u578b\u5206\u6570\u9636\u8bb0\u5fc6\u589e\u5f3a\u653b\u51fb\u8bca\u65ad\u65b9\u6848\uff0c\u4ec5\u9700\u4e00\u4e2a\u7535\u538b\u3001\u6709\u529f\u529f\u7387\u548c\u65e0\u529f\u529f\u7387\uff08VPQ\uff09\u6d4b\u91cf\u70b9\uff0c\u5373\u53ef\u5b9e\u73b0\u5bf9\u9006\u53d8\u5668\u4e3b\u5bfc\u7684\u5fae\u7535\u7f51\uff08IBR-dominated microgrids\uff09\u7684\u7f51\u7edc\u653b\u51fb\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u548c\u8bca\u65ad\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u7f51\u7edc\u7269\u7406\u5f39\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u7535\u7f51\u7f51\u7edc\u653b\u51fb\u8bca\u65ad\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u4eea\u5668\u8bbe\u5907\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u4e25\u683c\u4f46\u4e0d\u53ef\u884c\u7684\u6a21\u578b\u5047\u8bbe\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u4ec5\u9700\u5355\u70b9\u6d4b\u91cf\u5373\u53ef\u6709\u6548\u8bca\u65ad\u7f51\u7edc\u653b\u51fb\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6570\u9636\u8bb0\u5fc6\u589e\u5f3a\u653b\u51fb\u8bca\u65ad\u65b9\u6848\uff08FO-MADS\uff09\uff0c\u8be5\u65b9\u6848\u9996\u5148\u5229\u7528Caputo\u548cGr\u00fcnwald-Letnikov\u5bfc\u6570\u6784\u5efa\u53cc\u5206\u6570\u9636\u7279\u5f81\u5e93\uff0c\u653e\u5927VPQ\u4fe1\u53f7\u7684\u5fae\u5c0f\u6270\u52a8\u548c\u7f13\u6162\u6f02\u79fb\u3002\u7136\u540e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5c42\u5206\u7c7b\u5668\u6765\u5b9a\u4f4d\u53d7\u5f71\u54cd\u7684\u9006\u53d8\u5668\u5e76\u9694\u79bb\u6545\u969c\u7684IGBT\u5f00\u5173\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bb0\u5fc6\u56de\u653e\u5bf9\u6297\u8bad\u7ec3\uff08PMR-AT\uff09\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5229\u7528\u5728\u7ebf\u786c\u6837\u672c\u6316\u6398\uff08OHEM\uff09\u52a8\u6001\u91cd\u52a0\u6743\u653b\u51fb\u611f\u77e5\u635f\u5931\uff0c\u4f18\u5148\u5904\u7406\u6700\u56f0\u96be\u7684\u6837\u672c\u3002", "result": "\u5728\u5305\u542b1\u4e2a\u6b63\u5e38\u548c24\u4e2a\u6545\u969c\u7c7b\u522b\u3001\u56db\u79cd\u653b\u51fb\u573a\u666f\u7684\u56db\u9006\u53d8\u5668IBR\u4e3b\u5bfc\u5fae\u7535\u7f51\u6d4b\u8bd5\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u504f\u7f6e\u3001\u566a\u58f0\u3001\u6570\u636e\u66ff\u6362\u548c\u91cd\u653e\u653b\u51fb\u4e0b\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u4e3a96.6%\u300194.0%\u300192.8%\u548c95.7%\uff0c\u5728\u65e0\u653b\u51fb\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u7387\u4e3a96.7%\u3002", "conclusion": "FO-MADS\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u663e\u8457\u589e\u5f3aIBR\u4e3b\u5bfc\u5fae\u7535\u7f51\u7684\u7f51\u7edc\u7269\u7406\u5f39\u6027\u3002"}}
{"id": "2511.12479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12479", "abs": "https://arxiv.org/abs/2511.12479", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Bijo Sebastian"], "title": "ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps", "comment": null, "summary": "Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.12777", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12777", "abs": "https://arxiv.org/abs/2511.12777", "authors": ["Adeeb Kabir", "Steven Nguyen", "Sohan Ghosh", "Tijil Kiran", "Isaac H. Kim", "Yipeng Huang"], "title": "Sdim: A Qudit Stabilizer Simulator", "comment": null, "summary": "Quantum computers have steadily improved over the last decade, but developing fault-tolerant quantum computing (FTQC) techniques, required for useful, universal computation remains an ongoing effort. Key elements of FTQC such as error-correcting codes and decoding are supported by a rich bed of stabilizer simulation software such as Stim and CHP, which are essential for numerically characterizing these protocols at realistic scales. Recently, experimental groups have built nascent high-dimensional quantum hardware, known as qudits, which have a myriad of attractive properties for algorithms and FTQC. Despite this, there are no widely available qudit stabilizer simulators. We introduce the first open-source realization of such a simulator for all dimensions. We demonstrate its correctness against existing state vector simulations and benchmark its performance in evaluating and sampling quantum circuits. This simulator is the essential computational infrastructure to explore novel qudit error correction as earlier stabilizer simulators have been for qubits.", "AI": {"tldr": "\u6709\u4e86\u4e00\u4e2a\u65b0\u7684\u5f00\u6e90\u6a21\u62df\u5668\uff0c\u53ef\u4ee5\u6a21\u62df\u6240\u6709\u7ef4\u5ea6\u4e0a\u7684 qudit \u91cf\u5b50\u8ba1\u7b97\uff0c\u4ece\u800c\u4e3a qudit \u91cf\u5b50\u7ea0\u9519\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u9274\u4e8e\u76ee\u524d\u7f3a\u4e4f\u5e7f\u6cdb\u4f7f\u7528\u7684 qudit \u7a33\u5b9a\u5668\u6a21\u62df\u5668\uff0c\u800c qudit \u5bf9\u4e8e\u91cf\u5b50\u7b97\u6cd5\u548c\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97 (FTQC) \u5177\u6709\u5438\u5f15\u529b\u3002", "method": "\u4ecb\u7ecd\u5e76\u5f00\u6e90\u4e86\u7b2c\u4e00\u4e2a\u9002\u7528\u4e8e\u6240\u6709\u7ef4\u5ea6\u7684 qudit \u7a33\u5b9a\u5668\u6a21\u62df\u5668\u3002\u901a\u8fc7\u4e0e\u73b0\u6709\u7684\u72b6\u6001\u5411\u91cf\u6a21\u62df\u5668\u8fdb\u884c\u6b63\u786e\u6027\u9a8c\u8bc1\uff0c\u5e76\u5bf9\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u8bc4\u4f30\u548c\u91c7\u6837\u6765\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "result": "\u8be5\u6a21\u62df\u5668\u5df2\u88ab\u8bc1\u660e\u662f\u6b63\u786e\u7684\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a qudit \u91cf\u5b50\u7ea0\u9519\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u57fa\u7840\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u662f\u63a2\u7d22\u65b0\u9896 qudit \u91cf\u5b50\u7ea0\u9519\u7684\u5173\u952e\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u7c7b\u4f3c\u4e8e\u65e9\u671f\u7a33\u5b9a\u5668\u6a21\u62df\u5668\u5bf9\u91cf\u5b50\u6bd4\u7279\u7684\u4f5c\u7528\u3002"}}
{"id": "2511.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12020", "abs": "https://arxiv.org/abs/2511.12020", "authors": ["Xianglong Shi", "Silin Cheng", "Sirui Zhao", "Yunhan Jiang", "Enhong Chen", "Yang Liu", "Sebastien Ourselin"], "title": "LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension", "comment": null, "summary": "Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\\%. The code is available at https://anonymous.4open.science/r/LIHE.", "AI": {"tldr": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08WREC\uff09\u65b9\u6cd5\u5b58\u5728\u4e00\u5bf9\u4e00\u6620\u5c04\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u73b0\u5b9e\u573a\u666f\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08WGREC\uff09\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86LIHE\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002LIHE\u901a\u8fc7\u201cReferential Decoupling\u201d\u9884\u6d4b\u76ee\u6807\u6570\u91cf\u5e76\u5c06\u590d\u6742\u8868\u8fbe\u5206\u89e3\u4e3a\u5b50\u8868\u8fbe\uff0c\u518d\u901a\u8fc7\u201cReferent Grounding\u201d\u5229\u7528HEMix\u6df7\u5408\u76f8\u4f3c\u6027\u6a21\u5757\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\uff0c\u8be5\u6a21\u5757\u7ed3\u5408\u4e86\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u53cc\u66f2\u51e0\u4f55\u7684\u4f18\u52bf\uff0c\u6709\u6548\u907f\u514d\u4e86\u8bed\u4e49\u584c\u9677\u3002LIHE\u5728gRefCOCO\u548cRef-ZOM\u4e0a\u5efa\u7acb\u4e86\u9996\u4e2a\u6709\u6548\u7684\u5f31\u76d1\u7763WGREC\u57fa\u7ebf\uff0cHEMix\u5728\u6807\u51c6REC\u57fa\u51c6\u4e0a\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08WREC\uff09\u65b9\u6cd5\u53d7\u4e00\u5bf9\u4e00\u6620\u5c04\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5904\u7406\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u73b0\u5b9e\u573a\u666f\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55WREC\u5230\u66f4\u5b9e\u9645\u7684WGREC\u4efb\u52a1\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLIHE\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\u201cReferential Decoupling\u201d\u9884\u6d4b\u76ee\u6807\u6570\u91cf\u5e76\u5206\u89e3\u8868\u8fbe\u3002\u7b2c\u4e8c\u9636\u6bb5\u201cReferent Grounding\u201d\u4f7f\u7528HEMix\u6df7\u5408\u76f8\u4f3c\u6027\u6a21\u5757\uff08\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u53cc\u66f2\u51e0\u4f55\uff09\u8fdb\u884c\u5b9a\u4f4d\u3002", "result": "LIHE\u5728gRefCOCO\u548cRef-ZOM\u4e0a\u5efa\u7acb\u4e86\u9996\u4e2a\u6709\u6548\u7684\u5f31\u76d1\u7763WGREC\u57fa\u7ebf\u3002HEMix\u5728\u6807\u51c6REC\u57fa\u51c6\u4e0a\u5c06IoU@0.5\u63d0\u9ad8\u4e862.5%\u3002", "conclusion": "LIHE\u6846\u67b6\u53ca\u5176HEMix\u6a21\u5757\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12185", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12185", "abs": "https://arxiv.org/abs/2511.12185", "authors": ["Mills Staylor", "Arup Kumar Sarker", "Gregor von Laszewski", "Geoffrey Fox", "Yue Cheng", "Judy Fox"], "title": "Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications", "comment": "12 pages, 9 figures, 3 tables", "summary": "Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a Cylon \u7684\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u6570\u636e\u6846\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u670d\u52a1\u5668\u51fd\u6570\u5728\u5904\u7406\u5927\u6570\u636e\u96c6\u65f6\u7684\u901a\u4fe1\u548c\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5de5\u7a0b\u3001\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u5de5\u4f5c\u8d1f\u8f7d\u5728\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\uff0c\u6210\u672c\u9ad8\u6602\u3002\u867d\u7136\u516c\u6709\u4e91\u548c\u65e0\u670d\u52a1\u5668\u51fd\u6570\uff08\u5982 AWS Lambda\uff09\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4fbf\u5229\u6027\uff0c\u4f46\u5728\u5904\u7406\u5927\u6570\u636e\u96c6\u65f6\uff0c\u5176\u5916\u90e8\u5b58\u50a8\u7684\u8bbf\u95ee\u901f\u5ea6\u8fdc\u6162\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u7684\u76f4\u63a5\u901a\u4fe1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7 FMI \u5e93\u542f\u53d1\u7684\u65e0\u670d\u52a1\u5668\u901a\u4fe1\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7 NAT Traversal TCP Hole Punching \u6280\u672f\u5b9e\u73b0\u76f4\u63a5\u901a\u4fe1\uff0c\u4ee5\u89e3\u51b3\u65e0\u670d\u52a1\u5668\u51fd\u6570\u5728\u5904\u7406\u5927\u6570\u636e\u96c6\u65f6\u7684\u901a\u4fe1\u548c\u6027\u80fd\u74f6\u9888\u3002", "result": "\u901a\u8fc7 Cylon \u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u5f3a\u6269\u5c55\u6027\u5b9e\u9a8c\u4e2d\uff0cAWS Lambda \u7684\u6027\u80fd\u53ef\u4ee5\u8fbe\u5230\u4e0e\u670d\u52a1\u5668 AWS (EC2) \u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u76f8\u5ab2\u7f8e\u7684\u6c34\u5e73\uff0c\u5176\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u5230 1% \u4ee5\u5185\u3002", "conclusion": "Cylon \u4f5c\u4e3a\u4e00\u4e2a\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u6570\u636e\u6846\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65e0\u670d\u52a1\u5668\u901a\u4fe1\u5668\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u670d\u52a1\u5668\u51fd\u6570\u5728\u5927\u6570\u636e\u5904\u7406\u4e2d\u7684\u6027\u80fd\u548c\u901a\u4fe1\u6311\u6218\uff0c\u4e3a\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5229\u7528\u65e0\u670d\u52a1\u5668\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.13206", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13206", "abs": "https://arxiv.org/abs/2511.13206", "authors": ["Yihuai Zhang", "Huan Yu"], "title": "Event-Triggered Regulation of Mixed-Autonomy Traffic Under Varying Traffic Conditions", "comment": "15 pages, Accepted by IEEE TITS", "summary": "Modeling and congestion mitigation of mixed-autonomy traffic systems consisting of human-driven vehicles (HVs) and autonomous vehicles (AVs) have become increasingly critical with the rapid development of autonomous driving technology. This paper develops an event-triggered control (ETC) framework for mitigating congestion in such systems, which are modeled using an extended Aw-Rascle-Zhang (ARZ) formulation consisting of coupled 4 x 4 hyperbolic partial differential equations (PDEs). Ramp metering is employed as the boundary actuation mechanism. To reduce computational and communication burdens while avoiding excessive ramp signal changes, we design the ETC strategy based on the backstepping method, together with an observer-based ETC formulation for practical implementation under limited sensing. Rigorous Lyapunov analysis ensures exponential convergence and avoidance of Zeno behavior. Extensive simulations validate the proposed approach under diverse traffic scenarios, including varying AV penetration rates, different spacing policies, multiple demand levels, and non-recurrent congestion patterns. Results show that ETC not only stabilizes mixed traffic flows but also significantly reduces control updates, improving driver comfort, and roadway safety. Higher AV penetration rates lead to longer release time and fewer triggering events, indicating the positive impact of AVs in mitigating traffic congestion while reducing computational resource usage. Compared to continuous backstepping controllers, the proposed ETC achieves near-equivalent stabilization performance with far fewer controller updates, resulting in longer signal release time that reduces driver distraction, which demonstrates great potential for ETC applications in traffic management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u89e6\u53d1\u63a7\u5236\uff08ETC\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7f13\u89e3\u7531\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HVs\uff09\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u7ec4\u6210\u7684\u6df7\u5408\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4ea4\u901a\u62e5\u5835\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u62e5\u5835\u7f13\u89e3\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u6269\u5c55\u7684Aw-Rascle-Zhang\uff08ARZ\uff09\u6a21\u578b\uff08\u8026\u5408\u76844x4\u53cc\u66f2\u504f\u5fae\u5206\u65b9\u7a0b\uff09\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u531d\u9053\u8ba1\u91cf\u4f5c\u4e3a\u8fb9\u754c\u9a71\u52a8\u673a\u5236\u3002\u57fa\u4e8e\u53cd\u6b65\u6cd5\u8bbe\u8ba1\u4e86ETC\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c2\u6d4b\u5668\u7684ETC\u6765\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u8d1f\u62c5\uff0c\u5e76\u907f\u514d\u8fc7\u5ea6\u7684\u531d\u9053\u4fe1\u53f7\u53d8\u5316\u3002", "result": "\u901a\u8fc7Lyapunov\u5206\u6790\u786e\u4fdd\u4e86\u6307\u6570\u6536\u655b\u548c\u907f\u514d\u4e86\u829d\u8bfa\u884c\u4e3a\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cETC\u80fd\u591f\u7a33\u5b9a\u6df7\u5408\u4ea4\u901a\u6d41\uff0c\u663e\u8457\u51cf\u5c11\u63a7\u5236\u66f4\u65b0\u6b21\u6570\uff0c\u63d0\u9ad8\u9a7e\u9a76\u5458\u8212\u9002\u5ea6\u548c\u9053\u8def\u5b89\u5168\u3002AVs\u7684\u6e17\u900f\u7387\u8d8a\u9ad8\uff0c\u91ca\u653e\u65f6\u95f4\u548c\u89e6\u53d1\u4e8b\u4ef6\u8d8a\u5c11\uff0c\u8868\u660eAVs\u5728\u7f13\u89e3\u4ea4\u901a\u62e5\u5835\u548c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u65b9\u9762\u7684\u79ef\u6781\u4f5c\u7528\u3002\u4e0e\u8fde\u7eed\u53cd\u6b65\u63a7\u5236\u5668\u76f8\u6bd4\uff0cETC\u5728\u63a5\u8fd1\u7684\u7a33\u5b9a\u6027\u80fd\u4e0b\u5b9e\u73b0\u4e86\u66f4\u5c11\u7684\u63a7\u5236\u5668\u66f4\u65b0\u6b21\u6570\u548c\u66f4\u957f\u7684\u4fe1\u53f7\u91ca\u653e\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ETC\u65b9\u6cd5\u5728\u6df7\u5408\u4ea4\u901a\u7ba1\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4ea4\u901a\u62e5\u5835\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u8d1f\u62c5\uff0c\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u9a7e\u9a76\u5458\u8212\u9002\u5ea6\u3002"}}
{"id": "2511.12526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12526", "abs": "https://arxiv.org/abs/2511.12526", "authors": ["Davide De Benedittis", "Giovanni Di Lorenzo", "Franco Angelini", "Barbara Valle", "Marina Serena Borgatti", "Paolo Remagnino", "Marco Caccianiga", "Manolo Garabini"], "title": "Botany Meets Robotics in Alpine Scree Monitoring", "comment": "Published as Early Access in IEEE Transactions on Field Robotics. 19 pages, 13 figures", "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u4eba\u8f85\u52a9\u6536\u96c6\u6570\u636e\u4ee5\u76d1\u6d4b\u53d7\u6c14\u5019\u53d8\u5316\u5a01\u80c1\u7684\u6b27\u6d32\u5c71\u5730\u751f\u5883", "motivation": "\u6b27\u6d32\u5c71\u5730\u751f\u5883\u7684\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u548c\u73af\u5883\u9000\u5316\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u4f46\u4f20\u7edf\u7684\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9700\u8981\u5927\u91cf\u8d44\u6e90\u548c\u65f6\u95f4\u7684\u3001\u7531\u9ad8\u6280\u80fd\u79d1\u5b66\u5bb6\u8fdb\u884c\u7684\u91ce\u5916\u8003\u5bdf\u3002", "method": "\u90e8\u7f72ANYmal C\u673a\u5668\u4eba\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6765\u68c0\u6d4b\u548c\u5206\u7c7b\u5173\u952e\u7684\u690d\u7269\u7269\u79cd\uff0c\u4ee5\u534f\u52a9\u690d\u7269\u5b66\u5bb6\u6536\u96c6\u6570\u636e\u548c\u8bc6\u522b\u7269\u79cd\u3002", "result": "\u654f\u6377\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u5728\u590d\u6742\u7684\u5730\u5f62\u4e2d\u5bfc\u822a\uff0c\u5e76\u63d0\u9ad8\u5c71\u5730\u751f\u5883\u76d1\u6d4b\u7684\u9891\u7387\u548c\u6548\u7387\u3002\u5c06\u673a\u5668\u4eba\u4e0e\u4f20\u7edf\u7684\u690d\u7269\u7fa4\u843d\u8c03\u67e5\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u7b80\u5316\u91ce\u5916\u4f5c\u4e1a\uff0c\u5e76\u6539\u8fdb\u6570\u636e\u7684\u83b7\u53d6\u3001\u5b58\u50a8\u548c\u4f7f\u7528\u3002", "conclusion": "\u673a\u5668\u4eba\u8f85\u52a9\u534f\u8bae\u4e3a\u73af\u5883\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u4eba\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u3001\u53ef\u6301\u7eed\u7684\u5c71\u5730\u751f\u5883\u76d1\u6d4b\u548c\u4fdd\u62a4\u65b9\u6cd5\u3002"}}
{"id": "2511.12799", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12799", "abs": "https://arxiv.org/abs/2511.12799", "authors": ["Rylan Malarchick"], "title": "Verified Implementation of GRAPE Pulse Optimization for Quantum Gates with Hardware-Representative Noise Models", "comment": "11 pages, 4 figures", "summary": "Gate fidelity in noisy intermediate-scale quantum (NISQ) computers remains the primary bottleneck limiting practical quantum computation, constrained by decoherence and control noise. Quantum optimal control (QOC) techniques, such as the gradient ascent pulse engineering (GRAPE) algorithm, offer a powerful approach to designing noise-robust pulses that actively mitigate these effects. However, most QOC implementations operate in idealized simulation environments that fail to capture the real-time parameter drift inherent to physical quantum hardware, creating a critical ``sim-to-real'' gap. In this work, I present QubitPulseOpt, an open-source, rigorously-tested Python framework designed to bridge this gap through hardware-representative optimal control. The framework demonstrates API connectivity to IQM's Garnet quantum processor (20-qubit superconducting device) and implements a workflow that constructs a high-fidelity ``digital twin'' using hardware-representative parameters. Using this simulation framework, I demonstrate that GRAPE-optimized pulses achieve a simulated gate error reduction of 77$\\times$ compared to standard Gaussian pulses. The framework's reliability is ensured through a 659-test verification suite (59\\% code coverage) and adherence to NASA JPL Power-of-10 safety-critical coding standards, establishing a new paradigm for trustworthy quantum control software. All results are from verified GRAPE optimizations with full provenance documentation.", "AI": {"tldr": "QubitPulseOpt\u662f\u4e00\u4e2a\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684Python\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u786c\u4ef6\u4ee3\u8868\u6027\u6700\u4f18\u63a7\u5236\u6765\u5f25\u5408\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u89e3\u51b3NISQ\u8ba1\u7b97\u673a\u4e2d\u7684\u95e8\u4fdd\u771f\u5ea6\u95ee\u9898\u3002", "motivation": "NISQ\u8ba1\u7b97\u673a\u4e2d\u7684\u95e8\u4fdd\u771f\u5ea6\u662f\u5b9e\u9645\u91cf\u5b50\u8ba1\u7b97\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u800c\u73b0\u6709\u7684\u91cf\u5b50\u6700\u4f18\u63a7\u5236\uff08QOC\uff09\u65b9\u6cd5\u901a\u5e38\u5728\u7406\u60f3\u5316\u7684\u6a21\u62df\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u672a\u80fd\u89e3\u51b3\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aQubitPulseOpt\u7684\u5f00\u6e90Python\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u786c\u4ef6\u4ee3\u8868\u6027\u6700\u4f18\u63a7\u5236\u6765\u5f25\u5408\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u6846\u67b6\u6f14\u793a\u4e86\u4e0eIQM\u7684Garnet\u91cf\u5b50\u5904\u7406\u5668\u7684API\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u4ee3\u8868\u6027\u53c2\u6570\u6784\u5efa\u9ad8\u4fdd\u771f\u5ea6\u201c\u6570\u5b57\u5b6a\u751f\u201d\u3002", "result": "\u4f7f\u7528QubitPulseOpt\u6846\u67b6\uff0c\u4e0e\u6807\u51c6\u7684Gaussian\u8109\u51b2\u76f8\u6bd4\uff0cGRAPE\u4f18\u5316\u7684\u8109\u51b2\u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e8677\u500d\u7684\u95e8\u4fdd\u771f\u5ea6\u63d0\u5347\u3002\u8be5\u6846\u67b6\u901a\u8fc7659\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u548c\u7b26\u5408NASA JPL\u5b89\u5168\u5173\u952e\u7f16\u7801\u6807\u51c6\u7684\u9a8c\u8bc1\u786e\u4fdd\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "QubitPulseOpt\u4e3a\u53ef\u4fe1\u8d56\u7684\u91cf\u5b50\u63a7\u5236\u8f6f\u4ef6\u8bbe\u5b9a\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u786c\u4ef6\u4ee3\u8868\u6027\u6700\u4f18\u63a7\u5236\u89e3\u51b3\u4e86NISQ\u8ba1\u7b97\u673a\u4e2d\u7684\u95e8\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u53ef\u4ee5\u88ab\u6709\u6548\u7f29\u5c0f\u3002"}}
{"id": "2511.12024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12024", "abs": "https://arxiv.org/abs/2511.12024", "authors": ["Jose Reinaldo Cunha Santos A V Silva Neto", "Hodaka Kawachi", "Yasushi Yagi", "Tomoya Nakamura"], "title": "Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging", "comment": "8 pages without reference, 6 figures, 1 table", "summary": "State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNSDD\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u900f\u955c\u6210\u50cf\u7684\u6d4b\u91cf\u6570\u636e\u4e2d\u8fdb\u884c\u771f\u5b9e\u611f\u91cd\u5efa\uff0c\u65e0\u9700\u914d\u5bf9\u7684\u900f\u955c/\u65e0\u900f\u955c\u76d1\u7763\uff0c\u4ece\u800c\u907f\u514d\u4e86\u9886\u57df\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u900f\u955c\u76f8\u673a\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u914d\u5bf9\u7684\u900f\u955c/\u65e0\u900f\u955c\u76d1\u7763\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u504f\u5dee\u3002\u800c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u6269\u6563\u5148\u9a8c\u65b9\u6cd5\u5728\u5904\u7406\u900f\u955c\u6210\u50cf\u7684\u590d\u6742\u6027\uff08\u566a\u58f0\u5927\u3001\u591a\u8def\u590d\u7528\u3001\u75c5\u6001\uff09\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "NSDD\u5c06Range-space\u7ea6\u675f\u4e0eNull-space\u6269\u6563\u5148\u9a8c\u66f4\u65b0\u5206\u79bb\u5f00\u6765\u3002\u5b83\u901a\u8fc7\u84b8\u998f\u8fed\u4ee3DDNM+\u6c42\u89e3\u5668\u7684Null-space\u5206\u91cf\u6765\u5b9e\u73b0\uff0c\u8be5\u6a21\u578b\u4ee5\u65e0\u900f\u955c\u6d4b\u91cf\u503c\u548cRange-space\u951a\u70b9\u4f5c\u4e3a\u6761\u4ef6\u3002\u8fd9\u662f\u4e00\u79cd\u5355\u901a\u9053\u7684\u5b66\u751f\u6a21\u578b\u3002", "result": "NSDD\u80fd\u591f\u4fdd\u6301\u6d4b\u91cf\u4e00\u81f4\u6027\uff0c\u5e76\u751f\u6210\u771f\u5b9e\u611f\u5f3a\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u4e14\u65e0\u9700\u914d\u5bf9\u76d1\u7763\u3002\u5728Lensless-FFHQ\u548cPhlatCam\u6570\u636e\u96c6\u4e0a\uff0cNSDD\u7684\u8fd0\u884c\u901f\u5ea6\u4ec5\u6b21\u4e8eWiener\u65b9\u6cd5\uff0c\u611f\u77e5\u8d28\u91cf\u63a5\u8fd1DDNM+\uff08LPIPS\u6392\u540d\u7b2c\u4e8c\uff09\uff0c\u4f18\u4e8eDPS\u548c\u4f20\u7edf\u7684\u51f8\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NSDD\u4e3a\u5b9e\u73b0\u5feb\u901f\u3001\u65e0\u76d1\u7763\u3001\u771f\u5b9e\u611f\u7684\u65e0\u900f\u955c\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2511.12216", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12216", "abs": "https://arxiv.org/abs/2511.12216", "authors": ["Van Ho-Long", "Nguyen Ho", "Anh-Vu Dinh-Duc", "Ha Manh Tran", "Ky Trung Nguyen", "Tran Dung Pham", "Quoc Viet Hung Nguyen"], "title": "Distributed Seasonal Temporal Pattern Mining", "comment": null, "summary": "The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \\textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSTPM\u7684\u5206\u5e03\u5f0f\u5b63\u8282\u6027\u65f6\u95f4\u6a21\u5f0f\u6316\u6398\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u6d77\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u91cf\u7206\u70b8\u6027\u589e\u957f\uff0c\u4f46\u4f20\u7edf\u6316\u6398\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u5b63\u8282\u6027\u6a21\u5f0f\uff0c\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faDSTPM\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u5206\u5c42\u67e5\u627e\u54c8\u5e0c\u7ed3\u6784\u8fdb\u884c\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDSTPM\u5728\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u987a\u5e8f\u57fa\u7ebf\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u6269\u5c55\u5230\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\u3002", "conclusion": "DSTPM\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u4e2d\u6316\u6398\u5b63\u8282\u6027\u65f6\u95f4\u6a21\u5f0f\u7684\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13260", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13260", "abs": "https://arxiv.org/abs/2511.13260", "authors": ["Amit Shivam", "Kiran Kumari", "Fernando A. C. C. Fontes"], "title": "Robust Control Design Using a Hybrid-Gain Finite-Time Sliding-Mode Controller", "comment": "Under review in ECC", "summary": "This paper proposes a hybrid-gain finite-time sliding-mode control (HG-FTSMC) strategy for a class of perturbed nonlinear systems. The controller combines a finite-time reaching law that drives the sliding variable to a predefined boundary layer with an inner mixed-power or exponential law that guarantees rapid convergence within the layer while maintaining smooth and bounded control action. The resulting control design achieves finite-time convergence and robustness to matched disturbances, while explicitly limits the control effort. The control framework is first analyzed on a perturbed first-order integrator model, and then extended to Euler-Lagrange (EL) systems, representing a broad class of robotic and mechanical systems. Comparative simulations demonstrate that the proposed controller achieves settling times comparable to recent finite-time approaches [1], while substantially reducing the control effort. Finally, trajectory-tracking simulations on a two-link manipulator further validate the robustness and practical feasibility of the proposed HG-FTSMC approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u589e\u76ca\u6709\u9650\u65f6\u95f4\u6ed1\u6a21\u63a7\u5236\uff08HG-FTSMC\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u4e00\u7c7b\u53d7\u6270\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u5728\u6709\u9650\u65f6\u95f4\u5185\u89e3\u51b3\u53d7\u6270\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u8fc7\u7a0b\u4e2d\u9650\u5236\u63a7\u5236\u5668\u7684\u8f93\u51fa\u3002", "method": "\u8be5\u63a7\u5236\u5668\u7ed3\u5408\u4e86\u6709\u9650\u65f6\u95f4\u8d8b\u8fd1\u5f8b\u548c\u5185\u90e8\u6df7\u5408\u529f\u7387/\u6307\u6570\u5f8b\u3002\u8d8b\u8fd1\u5f8b\u5c06\u6ed1\u52a8\u53d8\u91cf\u9a71\u52a8\u5230\u9884\u5b9a\u4e49\u8fb9\u754c\u5c42\uff0c\u800c\u5185\u90e8\u5f8b\u5219\u786e\u4fdd\u5728\u8fb9\u754c\u5c42\u5185\u5feb\u901f\u6536\u655b\uff0c\u5e76\u4fdd\u6301\u63a7\u5236\u52a8\u4f5c\u7684\u5e73\u6ed1\u548c\u6709\u754c\u3002", "result": "\u8be5\u63a7\u5236\u5668\u5728\u6270\u52a8\u7684\u5355\u79ef\u5206\u5668\u6a21\u578b\u548c\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\uff08EL\uff09\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u5206\u6790\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6536\u655b\u65f6\u95f4\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u63a7\u5236\u8f93\u51fa\u3002\u5728\u53cc\u8fde\u6746\u673a\u68b0\u81c2\u4e0a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u4eff\u771f\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HG-FTSMC\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u6709\u9650\u65f6\u95f4\u6536\u655b\u548c\u5bf9\u5339\u914d\u5e72\u6270\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u6709\u6548\u9650\u5236\u4e86\u63a7\u5236\u5668\u7684\u8f93\u51fa\uff0c\u5e76\u5728\u673a\u5668\u4eba\u548c\u673a\u68b0\u7cfb\u7edf\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12618", "abs": "https://arxiv.org/abs/2511.12618", "authors": ["Jordan Leyva", "Nahim J. Moran Vera", "Yihan Xu", "Adrien Durasno", "Christopher U. Romero", "Tendai Chimuka", "Gabriel O. Huezo Ramirez", "Ziqian Dong", "Roberto Rojas-Cessa"], "title": "EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones", "comment": "Autonomous drone, A* algorithm, 3D environments, path planning, obstacle avoidance, energy efficiency, MIT Conference", "summary": "Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.", "AI": {"tldr": "EcoFlight\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u4eba\u673a\u907f\u969c\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u8003\u8651\u65e0\u4eba\u673a\u63a8\u8fdb\u7cfb\u7edf\u548c\u98de\u884c\u52a8\u529b\u5b66\u6765\u4f18\u5316\u80fd\u8017\uff0c\u5e76\u5728\u5404\u79cd\u969c\u788d\u7269\u5bc6\u5ea6\u4e0b\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\u3002", "motivation": "\u5927\u591a\u6570\u98de\u884c\u8def\u5f84\u89c4\u5212\u65b9\u6848\u5f88\u5c11\u8003\u8651\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u907f\u969c\u95ee\u9898\uff0c\u5c3d\u7ba1\u969c\u788d\u7269\u662f\u73b0\u5b9e\u5b58\u5728\u7684\u3002\u907f\u969c\u4f1a\u6d88\u8017\u5927\u91cf\u80fd\u91cf\uff0c\u56e0\u6b64\u6210\u4e3a\u65e0\u4eba\u673a\u9ad8\u6548\u70b9\u5bf9\u70b9\u98de\u884c\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "EcoFlight\u7b97\u6cd5\u6a21\u62df\u4e86\u65e0\u4eba\u673a\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u5305\u62ec\u63a8\u8fdb\u7cfb\u7edf\u548c\u98de\u884c\u52a8\u529b\u5b66\uff0c\u4ee5\u5728\u5b58\u5728\u969c\u788d\u7269\u76843D\u7a7a\u95f4\u4e2d\u786e\u5b9a\u6700\u4f4e\u80fd\u8017\u7684\u8def\u7ebf\u3002", "result": "\u5728\u5404\u79cd\u969c\u788d\u7269\u5bc6\u5ea6\u4e0b\u8fdb\u884c\u7684\u5927\u91cf\u8bc4\u4f30\u548c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cEcoFlight \u80fd\u591f\u6301\u7eed\u627e\u5230\u6bd4\u73b0\u6709\u7b97\u6cd5\uff08\u5982\u76f4\u63a5\u98de\u884c\u548c\u6700\u77ed\u8ddd\u79bb\u65b9\u6848\uff09\u80fd\u8017\u66f4\u4f4e\u7684\u8def\u5f84\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5408\u9002\u7684\u98de\u884c\u901f\u5ea6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u8282\u7701\u80fd\u91cf\u3002", "conclusion": "EcoFlight \u6210\u529f\u5730\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u907f\u969c\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u80fd\u8017\u95ee\u9898\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u9ad8\u5bc6\u5ea6\u969c\u788d\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u98de\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12026", "abs": "https://arxiv.org/abs/2511.12026", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Jianhang Zhang", "Xuanhui Zeng", "Xi Zhang", "Chaowei Zhu", "Haijun Hu", "Hongliang Ren"], "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark", "comment": "AAAI 2026 oral", "summary": "Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.", "AI": {"tldr": "VL-SurgPT\u662f\u4e00\u4e2a\u5305\u542b754\u4e2a\u7ec4\u7ec7\u8ffd\u8e2a\u548c154\u4e2a\u5668\u68b0\u8ffd\u8e2a\u89c6\u9891\u7247\u6bb5\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06\u89c6\u89c9\u8ffd\u8e2a\u4e0e\u70b9\u72b6\u6001\u7684\u6587\u672c\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u672f\u73af\u5883\u4e2d\u56e0\u70df\u96fe\u906e\u6321\u3001\u955c\u9762\u53cd\u5c04\u548c\u7ec4\u7ec7\u53d8\u5f62\u7b49\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u9020\u6210\u7684\u70b9\u8ffd\u8e2a\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u8ffd\u8e2a\u6570\u636e\u96c6\u7f3a\u4e4f\u7406\u89e3\u8ffd\u8e2a\u5931\u8d25\u673a\u5236\u6240\u9700\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u800c\u51c6\u786e\u7684\u70b9\u8ffd\u8e2a\u5728\u624b\u672f\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b908\u4e2a\u4f53\u5185\u89c6\u9891\u7247\u6bb5\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6VL-SurgPT\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u6587\u672c\u63cf\u8ff0\u6765\u63d0\u9ad8\u8ffd\u8e2a\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5TG-SurgPT\u3002", "result": "\u5728\u516b\u79cd\u5148\u8fdb\u7684\u8ffd\u8e2a\u65b9\u6cd5\u548c\u63d0\u51fa\u7684TG-SurgPT\u65b9\u6cd5\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u70b9\u72b6\u6001\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8ffd\u8e2a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u7eaf\u89c6\u89c9\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7684\u89c6\u89c9\u6311\u6218\u573a\u666f\u4e2d\u3002", "conclusion": "VL-SurgPT\u6570\u636e\u96c6\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u6311\u6218\u6027\u672f\u4e2d\u6761\u4ef6\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8ffd\u8e2a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fd9\u5bf9\u63a8\u8fdb\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13289", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13289", "abs": "https://arxiv.org/abs/2511.13289", "authors": ["Wenhao Wu", "Dan Wu", "Bin Wang", "Jiabing Hu"], "title": "Beyond Energy Functions and Numerical Integration: A New Methodology to Determine Transient Stability at the Initial State", "comment": "This work has been submitted to 2026 IEEE PES General Meeting", "summary": "This paper presents a novel method for transient stability analysis (TSA) that circumvents the limitations of sequential numerical integration and energy functions. The proposed method begins by constructing a trajectory-dependent stability indicator function to distinguish the system's destiny. To overcome the difficulty in analyzing the asymptotic behavior at infinite time, a strategic time contraction mapping is then applied. This allows TSA to be recast as a pole-placement detection problem for the indicator function. By leveraging high-order derivatives at the initial state, a rational function approximation is derived, yielding a mathematically direct and computationally efficient prediction. Numerical validations on benchmark systems demonstrate that the method not only provides a direct mathematical shortcut for TSA in power systems but also establishes a promising new methodology for evaluating the transient stability of a broad class of nonlinear dynamical systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6682\u6001\u7a33\u5b9a\u5206\u6790\uff08TSA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4f9d\u8d56\u4e8e\u8f68\u8ff9\u7684\u7a33\u5b9a\u6027\u6307\u6807\u51fd\u6570\uff0c\u5e76\u5e94\u7528\u65f6\u95f4\u6536\u7f29\u6620\u5c04\uff0c\u5c06TSA\u8f6c\u5316\u4e3a\u6781\u70b9\u914d\u7f6e\u68c0\u6d4b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u521d\u59cb\u72b6\u6001\u7684\u9ad8\u9636\u5bfc\u6570\u8fdb\u884c\u6709\u7406\u51fd\u6570\u903c\u8fd1\uff0c\u4ece\u800c\u76f4\u63a5\u4e14\u9ad8\u6548\u5730\u9884\u6d4b\u7cfb\u7edf\u7684\u6682\u6001\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u6682\u6001\u7a33\u5b9a\u5206\u6790\uff08TSA\uff09\u65b9\u6cd5\u5b58\u5728\u987a\u5e8f\u6570\u503c\u79ef\u5206\u548c\u80fd\u91cf\u51fd\u6570\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u9996\u5148\u6784\u5efa\u4f9d\u8d56\u4e8e\u8f68\u8ff9\u7684\u7a33\u5b9a\u6027\u6307\u6807\u51fd\u6570\u6765\u533a\u5206\u7cfb\u7edf\u7684\u72b6\u6001\u3002\u7136\u540e\u5e94\u7528\u65f6\u95f4\u6536\u7f29\u6620\u5c04\u6765\u5206\u6790\u65e0\u9650\u65f6\u95f4\u65f6\u7684\u6e10\u8fdb\u884c\u4e3a\u3002\u6700\u540e\uff0c\u5c06TSA\u91cd\u65b0\u6784\u5efa\u4e3a\u6307\u6807\u51fd\u6570\u7684\u6781\u70b9\u914d\u7f6e\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u521d\u59cb\u72b6\u6001\u7684\u9ad8\u9636\u5bfc\u6570\u5f97\u5230\u6709\u7406\u51fd\u6570\u903c\u8fd1\u3002", "result": "\u6570\u503c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u7535\u529b\u7cfb\u7edf\u4e2d\u7684TSA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u63a5\u7684\u6570\u5b66\u6377\u5f84\uff0c\u800c\u4e14\u4e3a\u8bc4\u4f30\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u6682\u6001\u7a33\u5b9a\u6027\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u8bba\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6682\u6001\u7a33\u5b9a\u5206\u6790\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u7a33\u5b9a\u6027\u6307\u6807\u51fd\u6570\u548c\u5e94\u7528\u65f6\u95f4\u6536\u7f29\u6620\u5c04\uff0c\u5c06TSA\u8f6c\u5316\u4e3a\u6781\u70b9\u914d\u7f6e\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u5b66\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u7cfb\u7edf\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u6682\u6001\u7a33\u5b9a\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.12650", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12650", "abs": "https://arxiv.org/abs/2511.12650", "authors": ["Arvind Kumar Mishra", "Sohom Chakrabarty"], "title": "Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning", "comment": "10 pages, 11 figures, It is submitted as a journal option paper associated with the IFAC World Congress 2026", "summary": "In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.\n  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u7528\u4e8e\u4f18\u5316\u5e73\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u5f62\u6001\u3002\u57282R\u64cd\u4f5c\u5668\u8ddf\u8e2a\u5706\u5f62\u8def\u5f84\u7684\u6848\u4f8b\u4e2d\uff0cRL\u6210\u529f\u5730\u91cd\u65b0\u53d1\u73b0\u4e86\u5df2\u77e5\u7684\u6700\u4f73\u89e3\uff08\u8fde\u6746\u957f\u5ea6\u76f8\u7b49\u4e14\u7b2c\u4e8c\u5173\u8282\u4e0e\u7b2c\u4e00\u5173\u8282\u6b63\u4ea4\uff09\uff0c\u9a8c\u8bc1\u4e86RL\u5728\u4ec5\u4f7f\u7528\u5956\u52b1\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u5bfb\u627e\u6700\u4f18\u89e3\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u591a\u6570\u5f62\u6001\u8bbe\u8ba1\u4efb\u52a1\u7f3a\u4e4f\u5c01\u95ed\u89e3\uff0c\u5e76\u4e14\u968f\u7740\u7ef4\u5ea6\u7684\u589e\u52a0\uff0c\u7f51\u683c\u641c\u7d22\u6216\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u6210\u672c\u4f1a\u6025\u5267\u5347\u9ad8\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86 Yoshikawa \u7684\u53ef\u64cd\u4f5c\u6027\u6307\u6807\uff0c\u5e76\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u4e00\u79cd\u6846\u67b6\u6765\u4f18\u5316\u5e73\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u5f62\u6001\u3002\u9996\u5148\uff0c\u7814\u7a76\u4e86\u4e00\u4e2a 2R \u64cd\u4f5c\u5668\u8ddf\u8e2a\u5706\u5f62\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u7684\u6848\u4f8b\uff0c\u56e0\u4e3a\u8be5\u6848\u4f8b\u5b58\u5728\u5df2\u77e5\u7684\u89e3\u6790\u6700\u4f18\u89e3\u3002\u7136\u540e\uff0c\u5c06\u6b64\u65b9\u6cd5\u6269\u5c55\u5230\u692d\u5706\u5f62\u548c\u77e9\u5f62\u8def\u5f84\uff0c\u5e76\u6269\u5c55\u4e86\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u5305\u542b\u5b8c\u6574\u7684\u5f62\u6001\u5411\u91cf\uff08L1\u3001L2\u3001theta2\uff09\u3002", "result": "\u5728\u5706\u5f62\u8def\u5f84\u6848\u4f8b\u4e2d\uff0c\u6240\u6709\u4e09\u79cd RL \u7b97\u6cd5\uff08SAC\u3001DDPG \u548c PPO\uff09\u4e0e\u7f51\u683c\u641c\u7d22\u548c\u9ed1\u76d2\u4f18\u5316\u5668\u4e00\u8d77\uff0c\u90fd\u6210\u529f\u6536\u655b\u5230\u4e86\u89e3\u6790\u6700\u4f18\u89e3\u3002\u5728\u975e\u89e3\u6790\u8bbe\u7f6e\uff08\u692d\u5706\u5f62\u548c\u77e9\u5f62\u8def\u5f84\uff09\u4e2d\uff0cRL \u4e5f\u80fd\u53ef\u9760\u6536\u655b\uff0c\u800c\u7f51\u683c\u641c\u7d22\u548c\u9ed1\u76d2\u65b9\u6cd5\u9700\u8981\u66f4\u5927\u7684\u8bc4\u4f30\u9884\u7b97\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u91cd\u65b0\u53d1\u73b0\u5df2\u77e5\u7684\u6700\u4f18\u89e3\uff0c\u5e76\u89e3\u51b3\u6ca1\u6709\u89e3\u6790\u89e3\u7684\u5f62\u6001\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5f62\u6001\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12872", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2511.12872", "abs": "https://arxiv.org/abs/2511.12872", "authors": ["Taisuke Hosaka", "Etsuo Segawa"], "title": "Pulsation of quantum walk between two arbitrary graphs with weakly connected bridge", "comment": null, "summary": "We consider the Grover walk on a finite graph composed of two arbitrary simple graphs connected by one edge, referred to as a bridge. The parameter $\u03b5>0$ assigned at the bridge represents the strength of connectivity: if $\u03b5=0$, then the graph is completely separated. We show that for sufficiently small values of $\u03b5$, a phenomenon called pulsation occurs. The pulsation is characterized by the periodic transfer of the quantum walker between the two graphs. An asymptotic expression with respect to small $\u03b5$ for the probability of finding the walker on either of the two graphs is derived. This expression reveals that the pulsation depends solely on the number of edges in each graph, regardless of their structure. In addition, we obtain that the quantum walker is transferred periodically between the two graphs, with a period of order $O(\u03b5^{-1/2})$. Furthermore, when the number of edges of two graphs is equal, the quantum walker is almost completely transferred.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u7531\u4e24\u5f20\u56fe\u901a\u8fc7\u4e00\u5ea7\u6865\u8fde\u63a5\u800c\u6210\u7684\u56fe\u4e0a\u8fdb\u884cGrover walk\u3002\u5f53\u6865\u7684\u8fde\u63a5\u5f3a\u5ea6\u03b5\u8db3\u591f\u5c0f\u65f6\uff0c\u91cf\u5b50\u884c\u8d70\u673a\u4f1a\u5448\u73b0\u8109\u52a8\u73b0\u8c61\uff0c\u5373\u91cf\u5b50\u884c\u8d70\u8005\u5728\u4e24\u5f20\u56fe\u4e4b\u95f4\u5468\u671f\u6027\u5730\u8f6c\u79fb\u3002\u7814\u7a76\u63a8\u5bfc\u4e86\u03b5\u8d8b\u8fd1\u4e8e0\u65f6\uff0c\u91cf\u5b50\u884c\u8d70\u8005\u5728\u4e24\u5f20\u56fe\u4e0a\u6982\u7387\u7684\u6e10\u8fd1\u8868\u8fbe\u5f0f\uff0c\u5e76\u53d1\u73b0\u8be5\u8109\u52a8\u73b0\u8c61\u4ec5\u53d6\u51b3\u4e8e\u56fe\u7684\u8fb9\u6570\uff0c\u800c\u4e0e\u56fe\u7684\u5177\u4f53\u7ed3\u6784\u65e0\u5173\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u91cf\u5b50\u884c\u8d70\u8005\u8f6c\u79fb\u7684\u5468\u671f\u7ea6\u4e3aO(\u03b5^{-1/2})\uff0c\u5e76\u4e14\u5f53\u4e24\u56fe\u7684\u8fb9\u6570\u76f8\u7b49\u65f6\uff0c\u91cf\u5b50\u884c\u8d70\u8005\u51e0\u4e4e\u4f1a\u5b8c\u5168\u8f6c\u79fb\u5230\u53e6\u4e00\u5f20\u56fe\u4e0a\u3002", "motivation": "\u7814\u7a76Grover walk\u5728\u7279\u5b9a\u56fe\u7ed3\u6784\u4e0a\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u63a2\u7d22\u8fde\u63a5\u5f3a\u5ea6\u5bf9\u91cf\u5b50\u884c\u8d70\u8005\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u56fe\u7ed3\u6784\u65e0\u5173\u7684\u666e\u9002\u6027\u89c4\u5f8b\u3002", "method": "\u8003\u8651\u7531\u4e24\u5f20\u56fe\u901a\u8fc7\u4e00\u5ea7\u6865\u8fde\u63a5\u800c\u6210\u7684\u56fe\u4e0a\u7684Grover walk\uff0c\u6865\u7684\u8fde\u63a5\u5f3a\u5ea6\u53c2\u6570\u4e3a\u03b5\u3002\u5f53\u03b5\u8db3\u591f\u5c0f\u65f6\uff0c\u5206\u6790\u91cf\u5b50\u884c\u8d70\u8005\u5728\u4e24\u5f20\u56fe\u4e4b\u95f4\u7684\u8f6c\u79fb\u884c\u4e3a\uff0c\u5e76\u63a8\u5bfc\u76f8\u5173\u6982\u7387\u7684\u6e10\u8fd1\u8868\u8fbe\u5f0f\u3002", "result": "\u53d1\u73b0\u4e86\u91cf\u5b50\u884c\u8d70\u8005\u5728\u4e24\u5f20\u56fe\u4e4b\u95f4\u5468\u671f\u6027\u8f6c\u79fb\u7684\u8109\u52a8\u73b0\u8c61\u3002\u63a8\u5bfc\u51fa\u7684\u6e10\u8fd1\u8868\u8fbe\u5f0f\u8868\u660e\uff0c\u8109\u52a8\u73b0\u8c61\u4ec5\u4f9d\u8d56\u4e8e\u56fe\u7684\u8fb9\u6570\uff0c\u4e0e\u56fe\u7684\u5177\u4f53\u7ed3\u6784\u65e0\u5173\u3002\u8f6c\u79fb\u5468\u671f\u7ea6\u4e3aO(\u03b5^{-1/2})\u3002\u5f53\u4e24\u56fe\u8fb9\u6570\u76f8\u7b49\u65f6\uff0c\u91cf\u5b50\u884c\u8d70\u8005\u51e0\u4e4e\u5b8c\u5168\u8f6c\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u5177\u6709\u6865\u8fde\u63a5\u7684\u56fe\u4e0a\uff0cGrover walk\u7684\u8109\u52a8\u73b0\u8c61\u662f\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u884c\u4e3a\uff0c\u5176\u8868\u73b0\u5f62\u5f0f\uff08\u8f6c\u79fb\u6982\u7387\u548c\u5468\u671f\uff09\u4ec5\u7531\u56fe\u7684\u8fb9\u6570\u51b3\u5b9a\uff0c\u5e76\u4e14\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u8fb9\u6570\u76f8\u7b49\uff09\u53ef\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u5168\u7684\u91cf\u5b50\u884c\u8d70\u8005\u8f6c\u79fb\u3002"}}
{"id": "2511.12027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12027", "abs": "https://arxiv.org/abs/2511.12027", "authors": ["Jeong Hun Yeo", "Sangyun Chung", "Sungjune Park", "Dae Hoe Kim", "Jinyoung Moon", "Yong Man Ro"], "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory", "comment": null, "summary": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.", "AI": {"tldr": "GCAgent\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u8bb0\u5fc6\uff08Schematic and Narrative Episodic Memory\uff09\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u611f\u77e5-\u884c\u52a8-\u53cd\u601d\u5faa\u73af\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\uff0c\u5728Video-MME\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u56e0\u5176\u56fa\u6709\u7684token\u9650\u5236\u548c\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u7684\u590d\u6742\u6027\uff0c\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u6df1\u5ea6\u89c6\u9891\u63a8\u7406\u6240\u9700\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u590d\u6742\u4e8b\u4ef6\u5173\u7cfb\u3002", "method": "\u63d0\u51faGCAgent\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u201c\u56fe\u5f0f\u4e0e\u53d9\u4e8b\u60c5\u8282\u8bb0\u5fc6\u201d\uff08Schematic and Narrative Episodic Memory\uff09\uff0c\u8be5\u8bb0\u5fc6\u7ed3\u6784\u5316\u5730\u5c06\u4e8b\u4ef6\u53ca\u5176\u56e0\u679c\u3001\u65f6\u95f4\u5173\u7cfb\u5efa\u6a21\u6210\u7b80\u6d01\u3001\u6709\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u957f\u671f\u4f9d\u8d56\u95ee\u9898\u3002GCAgent\u5728\u591a\u9636\u6bb5\u7684\u201c\u611f\u77e5-\u884c\u52a8-\u53cd\u601d\u201d\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u5e76\u5229\u7528\u8bb0\u5fc6\u7ba1\u7406\u5668\u68c0\u7d22\u76f8\u5173\u60c5\u8282\u4e0a\u4e0b\u6587\u4ee5\u8fdb\u884c\u7a33\u5065\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u7406\u3002", "result": "GCAgent\u5728\u957f\u89c6\u9891\u7406\u89e3\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728Video-MME\u957f\u89c6\u9891\u5212\u5206\u4efb\u52a1\u4e0a\u76f8\u6bd4\u5f3a\u5927\u7684MLLM\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe23.5%\u3002\u8be5\u6846\u67b6\u5728\u540c\u7c7b7B\u89c4\u6a21MLLM\u4e2d\u4e5f\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728Long\u5212\u5206\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4e3a73.4%\uff0c\u5728Video-MME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u5e73\u5747\u51c6\u786e\u7387\u6700\u9ad8\uff0871.9%\uff09\u3002", "conclusion": "GCAgent\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u63a8\u7406\u8303\u5f0f\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u5728\u8ba4\u77e5\u542f\u53d1\u5f0f\u957f\u89c6\u9891\u7406\u89e3\u65b9\u9762\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2511.12486", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12486", "abs": "https://arxiv.org/abs/2511.12486", "authors": ["Duneesha Fernando", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "A Decentralized Root Cause Localization Approach for Edge Computing Environments", "comment": null, "summary": "Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u8fb9\u7f18\u8ba1\u7b97\u6839\u56e0\u5b9a\u4f4d\uff08RCL\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u4e2a\u6027\u5316 PageRank\uff08PPR\uff09\u7b97\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u5c42\u9762\u76f4\u63a5\u6267\u884c\u5b9a\u4f4d\uff0c\u51cf\u5c11\u4e86\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728 MicroCERCL \u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u65e5\u76ca\u590d\u6742\u7684\u5fae\u670d\u52a1\u5316\u7269\u8054\u7f51\u5e94\u7528\u5bb9\u6613\u51fa\u73b0\u6027\u80fd\u5f02\u5e38\uff0c\u4e14\u8fd9\u4e9b\u5f02\u5e38\u4f1a\u8de8\u670d\u52a1\u4f20\u64ad\u3002\u73b0\u6709\u7684\u4e91\u73af\u5883 RCL \u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e2d\u5fc3\u5316\u5206\u6790\uff0c\u4f1a\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684\u53bb\u4e2d\u5fc3\u5316 RCL \u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5c06\u5fae\u670d\u52a1\u805a\u7c7b\u6210\u901a\u4fe1\u548c\u5171\u540c\u5b9a\u4f4d\u7684\u96c6\u7fa4\uff0c\u4ee5\u9650\u5236\u5f02\u5e38\u4f20\u64ad\u3002\u7136\u540e\u5728\u6bcf\u4e2a\u96c6\u7fa4\u5185\u672c\u5730\u6267\u884c PPR \u7b97\u6cd5\u8fdb\u884c\u6839\u56e0\u5b9a\u4f4d\u3002\u5bf9\u4e8e\u8de8\u96c6\u7fa4\u4f20\u64ad\u7684\u5f02\u5e38\uff0c\u91c7\u7528\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u96c6\u7fa4\u95f4\u70b9\u5bf9\u70b9\u8fd1\u4f3c\u534f\u540c\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u5e38\u8bc4\u5206\u673a\u5236\uff0c\u4ee5\u9002\u5e94\u5f02\u6784\u8fb9\u7f18\u73af\u5883\u4e2d\u591a\u5c42\u9762\u7684\u5f02\u5e38\u89e6\u53d1\u5668\u3002", "result": "\u5728 MicroCERCL \u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4e2d\u5fc3\u5316\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5c06\u5b9a\u4f4d\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe 34%\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u7684\u57fa\u4e8e\u56fe\u7684 RCL \u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13362", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.13362", "abs": "https://arxiv.org/abs/2511.13362", "authors": ["Xiayan Xu", "Xiaomeng Chen", "Dawei Shi", "Ling Shi"], "title": "Event-triggered Dual Gradient Tracking for Distributed Resource Allocation", "comment": null, "summary": "High communication costs create a major bottleneck for distributed resource allocation over unbalanced directed networks. Conventional dual gradient tracking methods, while effective for problems on unbalanced digraphs, rely on periodic communication that creates significant overhead in resource-constrained networks. This paper introduces a novel event-triggered dual gradient tracking algorithm to mitigate this limitation, wherein agents communicate only when local state deviations surpass a predefined threshold. We establish comprehensive convergence guarantees for this approach. First, we prove sublinear convergence for non-convex dual objectives and linear convergence under the Polyak-\u0141ojasiewicz condition. Building on this, we demonstrate that the proposed algorithm achieves sublinear convergence for general strongly convex cost functions and linear convergence for those that are also Lipschitz-smooth. Numerical experiments confirm that our event-triggered method significantly reduces communication events compared to periodic schemes while preserving comparable convergence performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e8b\u4ef6\u89e6\u53d1\u5f0f\u5bf9\u5076\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u4ee5\u964d\u4f4e\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u4e2d\u901a\u4fe1\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5e73\u8861\u6709\u5411\u7f51\u7edc\u4e2d\u3002", "motivation": "\u9ad8\u901a\u4fe1\u6210\u672c\u662f\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u5728\u4e0d\u5e73\u8861\u6709\u5411\u7f51\u7edc\u4e2d\u7684\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u4fe1\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e8b\u4ef6\u89e6\u53d1\u5f0f\u5bf9\u5076\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u53ea\u6709\u5f53\u5c40\u90e8\u72b6\u6001\u504f\u5dee\u8d85\u8fc7\u9884\u5b9a\u9608\u503c\u65f6\uff0c\u4ee3\u7406\u624d\u4f1a\u901a\u4fe1\u3002\u63a8\u5bfc\u4e86\u8be5\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5bf9\u4e8e\u975e\u51f8\u5bf9\u5076\u76ee\u6807\u5177\u6709\u6b21\u7ebf\u6027\u6536\u655b\u6027\uff0c\u5bf9\u4e8e\u6ee1\u8db3Polyak-\u0141ojasiewicz\u6761\u4ef6\u7684\u51fd\u6570\u5177\u6709\u7ebf\u6027\u6536\u655b\u6027\u3002\u5bf9\u4e8e\u4e00\u822c\u5f3a\u51f8\u4ee3\u4ef7\u51fd\u6570\uff0c\u8bc1\u660e\u4e86\u6b21\u7ebf\u6027\u6536\u655b\u6027\uff0c\u5bf9\u4e8e\u6ee1\u8db3Lipschitz\u5149\u6ed1\u6761\u4ef6\u7684\u5f3a\u51f8\u51fd\u6570\uff0c\u8bc1\u660e\u4e86\u7ebf\u6027\u6536\u655b\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5468\u671f\u6027\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u4e8b\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u6536\u655b\u6027\u80fd\u3002", "conclusion": "\u4e8b\u4ef6\u89e6\u53d1\u5f0f\u7b97\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4fdd\u6301\u4e0e\u4f20\u7edf\u5468\u671f\u6027\u65b9\u6cd5\u76f8\u5f53\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2511.12755", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12755", "abs": "https://arxiv.org/abs/2511.12755", "authors": ["Aleesha Khurram", "Amir Moeini", "Shangtong Zhang", "Rohan Chandra"], "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL", "comment": null, "summary": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICRL\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u5c11\u6837\u672c\u3001\u63d0\u793a\u9a71\u52a8\u7684\u9886\u57df\u81ea\u9002\u5e94\uff08DA\uff09\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u95ed\u73af\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u9886\u57df\u81ea\u9002\u5e94\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u4e0b\u3002\u4f20\u7edf\u7684\u89e3\u51b3\u65b9\u6848\uff08\u5982\u6536\u96c6\u66f4\u591a\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff09\u5728\u5927\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\u65f6\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u57fa\u4e8eLLM\u548cVLM\u7684\u5c11\u6837\u672c/\u96f6\u6837\u672c\u63d0\u793a\u9a71\u52a8DA\u65b9\u6cd5\u867d\u7136\u6709\u524d\u666f\uff0c\u4f46\u4ec5\u9650\u4e8e\u611f\u77e5\u4efb\u52a1\u4e14\u9700\u8981\u4e13\u5bb6\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u95ed\u73af\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICRL\uff08in-context reinforcement learning\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u5c11\u6837\u672c\u3001\u63d0\u793a\u9a71\u52a8\u7684\u9886\u57df\u81ea\u9002\u5e94\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cICRL\u4e0d\u9700\u8981\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u6216\u6536\u96c6\u76ee\u6807\u57df\u7684\u989d\u5916\u6570\u636e\u3002\u5b83\u901a\u8fc7\u5728\u63d0\u793a\u4e2d\u52a0\u5165\u4e0a\u4e0b\u6587\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u5b9e\u73b0\uff0c\u5e76\u5c06\u901a\u7528\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u6269\u5c55\u5230\u95ed\u73af\u9a7e\u9a76\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u9a71\u52a8DA\u57fa\u7ebf\u76f8\u6bd4\uff0cICRL\u5728\u76ee\u6807\u57df\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u8212\u9002\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "conclusion": "ICRL\u6210\u529f\u5730\u5c06\u5c11\u6837\u672c\u3001\u63d0\u793a\u9a71\u52a8\u7684\u9886\u57df\u81ea\u9002\u5e94\u6269\u5c55\u5230\u4e86\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12887", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12887", "abs": "https://arxiv.org/abs/2511.12887", "authors": ["Xiao-Qian Mu", "Hao-Fan Wang", "Shao-Ming Fei"], "title": "A note on Schmidt-number witnesses based on symmetric measurements", "comment": null, "summary": "The Schmidt number is an important kind of characterization of quantum entanglement. Quantum states with higher Schmidt numbers demonstrate significant advantages in various quantum information processing tasks. By deriving a class of k-positive linear maps based on symmetric measurements, we present new Schmidt-number witnesses of class (k + 1). By detailed example, we show that our Schmidt number witnesses identify better the Schmidt number of quantum states in high-dimensional systems. Furthermore, we note that the Fedorov ratio, which coincides with the Schmidt number for pure Gaussian states and provides a close approximation in non-Gaussian cases such as spontaneous parametric down-conversion, serves as an experimentally accessible tool for validating the proposed (k +1)-class Schmidt-number witnesses.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u5bf9\u79f0\u6d4b\u91cf\u7684k-\u6b63\u7ebf\u6027\u6620\u5c04\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684(k+1)\u7c7bSchmidt\u6570\u89c1\u8bc1\uff0c\u4ee5\u66f4\u597d\u5730\u8868\u5f81\u91cf\u5b50\u6001\u7684Schmidt\u6570\uff0c\u5e76\u63d0\u51faFedorov\u6bd4\u7387\u4f5c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u5de5\u5177\u3002", "motivation": "\u91cf\u5b50\u7ea0\u7f20\u7684Schmidt\u6570\u5bf9\u4e8e\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u9ad8Schmidt\u6570\u5728\u5176\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\u3002", "method": "\u63a8\u5bfc\u4e86\u57fa\u4e8e\u5bf9\u79f0\u6d4b\u91cf\u7684k-\u6b63\u7ebf\u6027\u6620\u5c04\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86(k+1)\u7c7bSchmidt\u6570\u89c1\u8bc1\u3002", "result": "\u63d0\u51fa\u7684Schmidt\u6570\u89c1\u8bc1\u80fd\u66f4\u597d\u5730\u8868\u5f81\u9ad8\u7ef4\u7cfb\u7edf\u91cf\u5b50\u6001\u7684Schmidt\u6570\uff0c\u5e76\u4e14Fedorov\u6bd4\u7387\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u5de5\u5177\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684Schmidt\u6570\u89c1\u8bc1\u5728\u8868\u5f81\u91cf\u5b50\u6001\u7684Schmidt\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14Fedorov\u6bd4\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u9014\u5f84\u3002"}}
{"id": "2511.12030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12030", "abs": "https://arxiv.org/abs/2511.12030", "authors": ["Jun Zhou", "Chi Xu", "Kaifeng Tang", "Yuting Ge", "Tingrui Guo", "Li Cheng"], "title": "VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation", "comment": "14 pages, 9 figures, extended version of the AAAI 2026 paper \"VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation\"", "summary": "Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\u7684\u5355\u4e00RGB\u56fe\u50cf\u624b\u90e8\u548c\u7269\u4f53\u4e09\u7ef4\u59ff\u6001\u4f30\u8ba1\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u89c6\u89c9-\u7269\u7406\u7ebf\u7d22\u5b66\u4e60\u548c\u5019\u9009\u59ff\u6001\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7684\u7269\u7406\u7ea6\u675f\u51b2\u7a81\u548c\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6027\u95ee\u9898\uff0c\u5e76\u5728\u7cbe\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f30\u8ba1\u5355\u4e00RGB\u56fe\u50cf\u4e2d\u7684\u624b\u90e8\u548c\u7269\u4f53\u4e09\u7ef4\u59ff\u6001\u662f\u4e00\u4e2a\u57fa\u7840\u6027\u4f46\u5145\u6ee1\u6311\u6218\u7684\u95ee\u9898\uff0c\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bb9\u6613\u4ea7\u751f\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\uff08\u5982\u76f8\u4e92\u7a7f\u900f\u6216\u975e\u63a5\u89e6\uff09\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\u6765\u4f30\u8ba1\u624b\u90e8\u548c\u7269\u4f53\u7684\u59ff\u6001\u3002\u5177\u4f53\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1\uff09\u8054\u5408\u89c6\u89c9-\u7269\u7406\u7ebf\u7d22\u5b66\u4e60\uff1a\u6a21\u578b\u540c\u65f6\u63d0\u53d62D\u89c6\u89c9\u7ebf\u7d22\u548c3D\u7269\u7406\u7ebf\u7d22\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u66f4\u5168\u9762\u7684\u8868\u5f81\u5b66\u4e60\uff1b2\uff09\u5019\u9009\u59ff\u6001\u805a\u5408\uff1a\u5229\u7528\u89c6\u89c9\u548c\u7269\u7406\u9884\u6d4b\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u7cbe\u70bc\u8fc7\u7a0b\u805a\u5408\u591a\u4e2a\u6269\u6563\u751f\u6210\u7684\u5019\u9009\u59ff\u6001\uff0c\u5f97\u5230\u65e2\u89c6\u89c9\u4e00\u81f4\u53c8\u7269\u7406\u4e0a\u5408\u7406\u7684\u6700\u7ec8\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u96c6\u6210\u4e86\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u624b\u90e8\u548c\u7269\u4f53\u4e09\u7ef4\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2511.12500", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12500", "abs": "https://arxiv.org/abs/2511.12500", "authors": ["Muhammad Awad", "Muhammad Osama", "Brandon Potter"], "title": "Iris: First-Class Multi-GPU Programming Experience in Triton", "comment": null, "summary": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.", "AI": {"tldr": "Iris\u662f\u4e00\u4e2a\u7528Python\u548cTriton\u7f16\u5199\u7684\u591aGPU\u901a\u4fe1\u5e93\uff0c\u5b83\u901a\u8fc7\u63d0\u4f9b\u81ea\u7136\u5bf9\u9f50Triton\u7f16\u7a0b\u6a21\u578b\u7684\u57fa\u4e8e\u5e73\u94fa\u7684\u5bf9\u79f0\u5185\u5b58\u62bd\u8c61\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u7684\u65e0\u7f1d\u4ea4\u7ec7\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5728\u6027\u80fd\u548c\u53ef\u7f16\u7a0b\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u7684\u591aGPU\u7f16\u7a0b\u9700\u8981\u5728\u6027\u80fd\u548c\u53ef\u7f16\u7a0b\u6027\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u7684\u6743\u8861\u3002\u9ad8\u6027\u80fd\u5b9e\u73b0\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f4e\u7ea7\u7684HIP/CUDA\u901a\u4fe1\u5e93\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u7684\u5de5\u7a0b\u52aa\u529b\uff0c\u800c\u66f4\u7b80\u5355\u7684\u62bd\u8c61\u5219\u727a\u7272\u4e86\u6027\u80fd\u3002Iris\u65e8\u5728\u6d88\u9664\u8fd9\u79cd\u6743\u8861\u3002", "method": "Iris\u662f\u4e00\u4e2a\u5b8c\u5168\u7528Python\u548cTriton\u5b9e\u73b0\u7684\u5e93\uff0c\u5b83\u63d0\u4f9b\u57fa\u4e8e\u5e73\u94fa\u7684\u5bf9\u79f0\u5185\u5b58\u62bd\u8c61\uff0c\u8fd9\u4e9b\u62bd\u8c61\u4e0eTriton\u7684\u7f16\u7a0b\u6a21\u578b\u81ea\u7136\u5bf9\u9f50\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u7f16\u5199\u5355\u6e90\u5185\u6838\uff0c\u65e0\u7f1d\u5730\u4ea4\u7ec7\u8ba1\u7b97\u548c\u901a\u4fe1\u3002Iris\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6a21\u5f0f\u7684\u5206\u7c7b\uff0c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5728Iris\u4e2d\u5b9e\u73b0\u3002", "result": "Iris\u5728\u5fae\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u5e76\u5728GEMM+All-Scatter\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u6bd4PyTorch\u548cRCCL\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.79\u500d\u7684\u52a0\u901f\u3002", "conclusion": "Iris\u8bc1\u660e\u4e86\u9ad8\u7ea7\u5b9e\u73b0\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u9ad8\u5ea6\u4f18\u5316\u7684\u5e93\uff0c\u540c\u65f6\u5927\u5927\u7b80\u5316\u4e86\u591aGPU\u7f16\u7a0b\u3002"}}
{"id": "2511.12778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12778", "abs": "https://arxiv.org/abs/2511.12778", "authors": ["Vignesh Rajagopal", "Kasun Weerakoon Kulathun Mudiyanselage", "Gershom Devake Seneviratne", "Pon Aswin Sankaralingam", "Mohamed Elnoor", "Jing Liang", "Rohan Chandra", "Dinesh Manocha"], "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation", "comment": null, "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions", "AI": {"tldr": "DR. Nav \u662f\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u80fd\u5728\u5b58\u5728\u6b7b\u80e1\u540c\u4e14\u9700\u8981\u6062\u590d\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u901a\u8fc7\u878d\u5408RGB-LiDAR\u6570\u636e\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u9884\u6d4b\u6b7b\u80e1\u540c\u5e76\u89c4\u5212\u66f4\u5b89\u5168\u7684\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u6b7b\u80e1\u540c\u68c0\u6d4b\u548c\u6062\u590d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6709\u89d2\u843d\u3001\u690d\u88ab\u906e\u6321\u548c\u963b\u585e\u8def\u53e3\u7684\u573a\u666f\u4e0b\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u53ea\u8003\u8651\u53ef\u901a\u884c\u6027\uff0c\u672a\u80fd\u5145\u5206\u5904\u7406\u6b7b\u80e1\u540c\u7684\u98ce\u9669\u3002", "method": "DR. Nav \u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u901a\u8fc7\u751f\u6210\u5355\u4e00\u7684\u3001\u5b9e\u65f6\u7684\u8bed\u4e49\u6210\u672c\u5730\u56fe\u6765\u7edf\u4e00\u6b7b\u80e1\u540c\u9884\u6d4b\u548c\u6062\u590d\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8de8\u6a21\u6001RGB-LiDAR\u878d\u5408\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ee4\u6ce2\u6765\u4f30\u8ba1\u6bcf\u4e2a\u5355\u5143\u7684\u6b7b\u80e1\u540c\u53ef\u80fd\u6027\u4ee5\u53ca\u6062\u590d\u70b9\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u8fdb\u884c\u8fde\u7eed\u66f4\u65b0\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u4e0e\u4ec5\u7f16\u7801\u53ef\u901a\u884c\u6027\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cDR. Nav \u5c06\u6062\u590d\u611f\u77e5\u98ce\u9669\u663e\u5f0f\u5730\u7eb3\u5165\u5bfc\u822a\u6210\u672c\u5730\u56fe\u3002", "result": "\u5728\u5bc6\u96c6\u7684\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cDR. Nav \u7684\u6b7b\u80e1\u540c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 83.33%\uff0c\u8fbe\u5230\u76ee\u6807\u65f6\u95f4\uff08\u8def\u5f84\u6548\u7387\uff09\u7684\u6548\u7387\u63d0\u9ad8\u4e86 52.4%\uff0c\u4f18\u4e8e DWA\u3001MPPI \u548c Nav2 DWB \u7b49\u73b0\u6709\u89c4\u5212\u5668\u3002\u6b64\u5916\uff0c\u6b7b\u80e1\u540c\u5206\u7c7b\u5668\u529f\u80fd\u3002", "conclusion": "DR. Nav \u901a\u8fc7\u6574\u5408\u6b7b\u80e1\u540c\u9884\u6d4b\u548c\u6062\u590d\uff0c\u5e76\u751f\u6210\u5305\u542b\u6062\u590d\u611f\u77e5\u98ce\u9669\u7684\u6210\u672c\u5730\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.12923", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.12923", "abs": "https://arxiv.org/abs/2511.12923", "authors": ["Prashasti Tiwari", "Dylan Lewis", "Sougato Bose"], "title": "Fast Quantum Many Body State Synthesis", "comment": null, "summary": "Quantum Mechanical ground states of many-body systems can be important resources for various investigations: for quantum sensing, as the initial state for nonequilibrium quantum dynamics following quenches, and the simulation of quantum processes that start by coupling systems in ground states, eg, could be a process in quantum chemistry. However, to prepare ground states can be challenging; for example, requires adiabatic switching of Hamiltonian terms slower than an inverse gap, which can be time consuming and bring in decoherence. Here we investigate the possibility of preparing a many-body entangled ground state of a certain Hamiltonian, which can be called a quantum ``problem'' Hamiltonian, using the time evolution of an initial fiducial state by another ``solver'' Hamiltonian/s for a very short fixed (unit) time. The parameters of the solver Hamiltonian are optimised classically using energy minimisation as the cost function. We present a study of up to n=10 qubit many-body states prepared using this methodology.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u4e00\u4e2a\u201csolver\u201d\u54c8\u5bc6\u987f\u91cf\uff0c\u5728\u77ed\u65f6\u95f4\u5185\u6f14\u5316\u4e00\u4e2a\u521d\u59cb\u6001\uff0c\u6765\u5236\u5907\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u57fa\u6001\u3002", "motivation": "\u5236\u5907\u591a\u4f53\u7cfb\u7edf\u7684\u57fa\u6001\u5bf9\u4e8e\u91cf\u5b50\u4f20\u611f\u3001\u975e\u5e73\u8861\u91cf\u5b50\u52a8\u529b\u5b66\u4ee5\u53ca\u6a21\u62df\u91cf\u5b50\u8fc7\u7a0b\u7b49\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u7edd\u70ed\u6f14\u5316\u65b9\u6cd5\u8017\u65f6\u4e14\u6613\u5f15\u5165\u9000\u76f8\u5e72\u3002", "method": "\u5229\u7528\u4e00\u4e2a\u201csolver\u201d\u54c8\u5bc6\u987f\u91cf\u5728\u77ed\u56fa\u5b9a\u65f6\u95f4\u5185\uff08\u5355\u4f4d\u65f6\u95f4\uff09\u6f14\u5316\u4e00\u4e2a\u521d\u59cb\u6001\uff0c\u5e76\u901a\u8fc7\u7ecf\u5178\u4f18\u5316\u201csolver\u201d\u54c8\u5bc6\u987f\u91cf\u7684\u53c2\u6570\u6765\u6700\u5c0f\u5316\u80fd\u91cf\uff0c\u4ee5\u671f\u83b7\u5f97\u76ee\u6807\u201cproblem\u201d\u54c8\u5bc6\u987f\u91cf\u7684\u57fa\u6001\u3002", "result": "\u7814\u7a76\u4e86\u591a\u8fbe10\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5236\u5907\u591a\u4f53\u7ea0\u7f20\u57fa\u6001\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u77ed\u65f6\u95f4\u5185\u5236\u5907\u591a\u4f53\u7ea0\u7f20\u57fa\u6001\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12032", "abs": "https://arxiv.org/abs/2511.12032", "authors": ["Guotao Liang", "Baoquan Zhang", "Zhiyuan Wen", "Zihao Han", "Yunming Ye"], "title": "Improved Masked Image Generation with Knowledge-Augmented Token Representations", "comment": "AAAI-26", "summary": "Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u63a9\u7801\u56fe\u50cf\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u4ec5\u51ed\u6a21\u578b\u81ea\u8eab\u5b66\u4e60\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\u4e2d\u7684\u8bed\u4e49\u4f9d\u8d56\uff0c\u56e0\u4e3a\u5355\u4e2a\u4ee4\u724c\u7f3a\u4e4f\u660e\u786e\u7684\u8bed\u4e49\u542b\u4e49\u4e14\u5e8f\u5217\u901a\u5e38\u5f88\u957f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKA-MIG\u7684\u65b0\u9896\u7684\u77e5\u8bc6\u589e\u5f3a\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4ee4\u724c\u7ea7\u8bed\u4e49\u4f9d\u8d56\u7684\u663e\u5f0f\u77e5\u8bc6\uff08\u5373\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\uff09\u4f5c\u4e3a\u5148\u9a8c\uff0c\u4ee5\u5b66\u4e60\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u6765\u63d0\u9ad8\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63a2\u7d22\u5e76\u8bc6\u522b\u4e86\u4e09\u79cd\u6709\u5229\u7684\u4ee4\u724c\u77e5\u8bc6\u56fe\u8c31\uff08\u5373\u5171\u73b0\u56fe\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u56fe\u548c\u4f4d\u7f6e-\u4ee4\u724c\u4e0d\u517c\u5bb9\u56fe\uff09\u3002\u57fa\u4e8e\u4e09\u4e2a\u5148\u9a8c\u77e5\u8bc6\u56fe\u8c31\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56fe\u611f\u77e5\u7f16\u7801\u5668\u6765\u5b66\u4e60\u4ee4\u724c\u548c\u4f4d\u7f6e\u611f\u77e5\u7684\u8868\u793a\u3002\u7136\u540e\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u878d\u5408\u673a\u5236\uff0c\u5c06\u8fd9\u4e9b\u4e30\u5bcc\u7684\u8868\u793a\u6574\u5408\u5230\u73b0\u6709\u7684\u63a9\u7801\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728ImageNet\u4e0a\u7684\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u63a9\u7801\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "KA-MIG\u901a\u8fc7\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u56fe\u8c31\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u6355\u83b7\u8bed\u4e49\u4f9d\u8d56\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.12667", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12667", "abs": "https://arxiv.org/abs/2511.12667", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel"], "title": "Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.", "AI": {"tldr": "\u7ec4\u7ec7\u5728\u6570\u636e\u7f51\u683c\u67b6\u6784\u4e2d\u6784\u5efa\u7279\u5b9a\u6d88\u8d39\u8005\u7684\u6570\u636e\u5171\u4eab\u7ba1\u9053\uff0c\u4f46\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u4f1a\u964d\u4f4e\u670d\u52a1\u91cd\u7528\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eKubernetes\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u8bbe\u8ba1\u6a21\u5f0f\u7684\u975e\u4fb5\u5165\u5f0f\u3001\u5ef6\u8fdf\u5e94\u7528\uff0c\u5e76\u81ea\u52a8\u5316\u6a21\u5f0f\u6ce8\u5165\u548c\u6536\u96c6\u80fd\u6e90\u6307\u6807\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u670d\u52a1\u53ef\u91cd\u7528\u6027\u7684\u540c\u65f6\uff0c\u652f\u6301\u8282\u80fd\u51b3\u7b56\u3002", "motivation": "\u5728\u6570\u636e\u7f51\u683c\u67b6\u6784\u65e5\u76ca\u666e\u53ca\u7684\u80cc\u666f\u4e0b\uff0c\u7ec4\u7ec7\u9762\u4e34\u7740\u5982\u4f55\u63d0\u9ad8\u6570\u636e\u5171\u4eab\u7ba1\u9053\u4e2d\u53ef\u91cd\u7528\u8f6c\u6362\u670d\u52a1\u7684\u6210\u672c\u548c\u80fd\u6e90\u6548\u7387\u7684\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u964d\u4f4e\u670d\u52a1\u91cd\u7528\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eKubernetes\u7684\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u80fd\u591f\u975e\u4fb5\u5165\u5f0f\u5730\u3001\u5ef6\u8fdf\u5730\u5e94\u7528\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u800c\u65e0\u9700\u4fee\u6539\u670d\u52a1\u4ee3\u7801\u3002\u8be5\u5de5\u5177\u80fd\u591f\u81ea\u52a8\u5316\u6a21\u5f0f\u6ce8\u5165\uff0c\u5e76\u6536\u96c6\u80fd\u6e90\u4f7f\u7528\u6307\u6807\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u8bbe\u8ba1\u6a21\u5f0f\u7684\u975e\u4fb5\u5165\u5f0f\u3001\u5ef6\u8fdf\u5e94\u7528\uff0c\u5e76\u80fd\u81ea\u52a8\u5316\u6a21\u5f0f\u6ce8\u5165\u548c\u6536\u96c6\u80fd\u6e90\u6307\u6807\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8f6c\u6362\u670d\u52a1\u5728\u5404\u79cd\u7ba1\u9053\u7ed3\u6784\u4e2d\u7684\u53ef\u91cd\u7528\u6027\u7684\u540c\u65f6\uff0c\u652f\u6301\u4e0e\u80fd\u6e90\u76f8\u5173\u7684\u51b3\u7b56\u3002", "conclusion": "\u63d0\u51fa\u7684Kubernetes\u5de5\u5177\u901a\u8fc7\u5b9e\u73b0\u8bbe\u8ba1\u6a21\u5f0f\u7684\u975e\u4fb5\u5165\u5f0f\u3001\u5ef6\u8fdf\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7f51\u683c\u67b6\u6784\u4e2d\u670d\u52a1\u53ef\u91cd\u7528\u6027\u548c\u80fd\u6e90\u6548\u7387\u7684\u95ee\u9898\uff0c\u4ece\u800c\u652f\u6301\u7ec4\u7ec7\u505a\u51fa\u66f4\u660e\u667a\u7684\u8282\u80fd\u51b3\u7b56\u3002"}}
{"id": "2511.13424", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13424", "abs": "https://arxiv.org/abs/2511.13424", "authors": ["Bowen Tian", "Roel C. G. M. Loonen", "Roland M. E. Valckenborg", "Jan L. M. Hensen"], "title": "High-resolution hierarchical PV system performance modeling in urban environments", "comment": "This manuscript has been submitted to Energy Conversion and Management for peer review. 65 pages, 22 figures", "summary": "Accurate performance modeling of PV systems in urban environments is a significant challenge due to complex partial shading. This study introduces a high-resolution, hierarchical modeling framework that provides detailed insights from the solar cell to the system level. Rigorously validated against field-test data from calibrated equipment, the model demonstrates high accuracy in predicting minute-wised dynamic electrical characteristics (R2 > 0.90). A key finding is the critical shortcoming of conventional, coarser-resolution models under realistic shading; these are shown to overestimate the actual string operating power by up to 163% and the monthly energy yield by up to 54%. The proposed framework avoids these errors by precisely capturing mismatch losses and the time-varying phenomena of system components, such as bypass diode activations. Furthermore, the model accurately quantifies the effectiveness of mitigation technologies, showing that Module-Level Power Electronics (MLPEs) can increase the monthly energy yield of a heavily shaded string by over 20%. This research provides a crucial tool for reliable system design, accurate power forecasting, and the optimization of PV systems in complex urban settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5206\u8fa8\u7387\u3001\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u5177\u6709\u590d\u6742\u5c40\u90e8\u906e\u6321\u95ee\u9898\u7684\u5149\u4f0f\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u5149\u4f0f\u7cfb\u7edf\u6027\u80fd\u5efa\u6a21\u9762\u4e34\u5c40\u90e8\u906e\u6321\u95ee\u9898\u7684\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ece\u592a\u9633\u80fd\u7535\u6c60\u5230\u7cfb\u7edf\u7ea7\u522b\u7684\u9ad8\u5206\u8fa8\u7387\u3001\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u73b0\u573a\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u4e86\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u9884\u6d4b\u5206\u949f\u7ea7\u52a8\u6001\u7535\u7279\u6027\uff08R2 > 0.90\uff09\uff0c\u5e76\u51c6\u786e\u91cf\u5316\u5931\u914d\u635f\u8017\u548c\u65c1\u8def\u4e8c\u6781\u7ba1\u6fc0\u6d3b\u7b49\u65f6\u53d8\u73b0\u8c61\u3002\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u907f\u514d\u4e86\u9ad8\u8fbe163%\u7684\u529f\u7387\u548c54%\u7684\u6708\u5ea6\u80fd\u91cf\u4ea7\u91cf\u4f30\u7b97\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u7ec4\u4ef6\u7ea7\u7535\u529b\u7535\u5b50\u8bbe\u5907\uff08MLPEs\uff09\u53ef\u5c06\u91cd\u5ea6\u906e\u6321\u7ec4\u4e32\u7684\u6708\u5ea6\u80fd\u91cf\u4ea7\u91cf\u63d0\u9ad820%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u57ce\u5e02\u73af\u5883\u4e2d\u5149\u4f0f\u7cfb\u7edf\u7684\u53ef\u9760\u8bbe\u8ba1\u3001\u7cbe\u786e\u7684\u529f\u7387\u9884\u6d4b\u548c\u4f18\u5316\u3002"}}
{"id": "2511.12795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12795", "abs": "https://arxiv.org/abs/2511.12795", "authors": ["Boshu Lei", "Wen Jiang", "Kostas Daniilidis"], "title": "ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model", "comment": "under review", "summary": "Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7269\u4f53\u8fd9\u4e00\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\u7528\u4e8e\u6293\u53d6\u59ff\u6001\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u4e00\u79cd\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\u6765\u4f30\u8ba1\u6293\u53d6\u5206\u5e03\u7684\u4fe1\u606f\u589e\u76ca\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86SE(3)\u6d41\u5f62\u4e0a\u6293\u53d6\u5206\u5e03\u7684\u591a\u6a21\u6001\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u5c06\u80fd\u91cf\u6c34\u5e73\u4e0e\u6293\u53d6\u6210\u529f\u7387\u6821\u51c6\uff0c\u4f7f\u5f97\u9884\u6d4b\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u5bf9\u9f50\u3002\u901a\u8fc7\u5bf9\u6821\u51c6\u540e\u7684\u5206\u5e03\u8fdb\u884c\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\uff0c\u4ece\u800c\u9009\u62e9\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u89d2\uff0c\u6709\u6548\u5f15\u5bfc\u673a\u5668\u4eba\u63a2\u7d22\u76ee\u6807\u7269\u4f53\u7684\u53ef\u53ca\u90e8\u5206\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u4ee5\u6709\u9650\u7684\u89c6\u89d2\u9884\u7b97\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u80fd\u591f\u6210\u529f\u6293\u53d6\u7269\u4f53\u3002", "motivation": "\u4e4b\u524d\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5\u5728\u9762\u5bf9\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u65f6\uff0c\u8981\u4e48\u5ffd\u89c6\u4e86\u6293\u53d6\u5206\u5e03\u5bf9\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u6293\u53d6\u5206\u5e03\u7684\u6295\u5f71\uff0c\u5ffd\u7565\u4e86SE(3)\u6d41\u5f62\u4e0a\u6293\u53d6\u59ff\u6001\u7684\u7ed3\u6784\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6821\u51c6\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\u7528\u4e8e\u6293\u53d6\u59ff\u6001\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u6293\u53d6\u5206\u5e03\u4e2d\u4f30\u8ba1\u4fe1\u606f\u589e\u76ca\u3002\u6240\u63d0\u51fa\u7684\u80fd\u91cf\u6a21\u578b\u80fd\u591f\u6355\u6349SE(3)\u6d41\u5f62\u4e0a\u6293\u53d6\u5206\u5e03\u7684\u591a\u6a21\u6001\u6027\u8d28\uff0c\u5e76\u5c06\u80fd\u91cf\u6c34\u5e73\u6821\u51c6\u81f3\u6293\u53d6\u6210\u529f\u7387\uff0c\u4ee5\u4f7f\u9884\u6d4b\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u5bf9\u9f50\u3002\u901a\u8fc7\u4f30\u8ba1\u6821\u51c6\u540e\u7684\u5206\u5e03\u4e2d\u6293\u53d6\u7684\u4fe1\u606f\u589e\u76ca\u6765\u9009\u62e9\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u89d2\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u5730\u5f15\u5bfc\u673a\u5668\u4eba\u63a2\u7d22\u76ee\u6807\u7269\u4f53\u53ef\u53ca\u7684\u90e8\u5206\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5728\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u4ee5\u6709\u9650\u7684\u89c6\u89d2\u9884\u7b97\u6210\u529f\u6293\u53d6\u7269\u4f53\u3002\u6211\u4eec\u7684\u6a21\u62df\u73af\u5883\u53ef\u4ee5\u4e3a\u672a\u6765\u4e3b\u52a8\u6293\u53d6\u7684\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u5e73\u53f0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u6293\u53d6\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7269\u4f53\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u6a21\u62df\u73af\u5883\u53ef\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2511.12983", "categories": ["quant-ph", "cond-mat.dis-nn", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12983", "abs": "https://arxiv.org/abs/2511.12983", "authors": ["Enze Hou", "Yuzhi Liu", "Lei Wang", "Han Wang"], "title": "A Global Spacetime Optimization Approach to the Real-Space Time-Dependent Schr\u00f6dinger Equation", "comment": "23 pages, 5 figures", "summary": "The time-dependent Schr\u00f6dinger equation (TDSE) in real space is fundamental to understanding the dynamics of many-electron quantum systems, with applications ranging from quantum chemistry to condensed matter physics and materials science. However, solving the TDSE for complex fermionic systems remains a significant challenge, particularly due to the need to capture the time-evolving many-body correlations, while the antisymmetric nature of fermionic wavefunctions complicates the function space in which these solutions must be represented. We propose a general-purpose neural network framework for solving the real-space TDSE, Fermionic Antisymmetric Spatio-Temporal Network, which treats time as an explicit input alongside spatial coordinates, enabling a unified spatiotemporal representation of complex, antisymmetric wavefunctions for fermionic systems. This approach formulates the TDSE as a global optimization problem, avoiding step-by-step propagation and supporting highly parallelizable training. The method is demonstrated on four benchmark problems: a 1D harmonic oscillator, interacting fermions in a time-dependent harmonic trap, 3D hydrogen orbital dynamics, and a laser-driven H$_2$ molecule, achieving excellent agreement with reference solutions across all cases. These results confirm our method's scalability, accuracy, and flexibility across various dimensions and interaction regimes, while demonstrating its ability to accurately simulate long-time dynamics in complex systems. Our framework offers a highly expressive alternative to traditional basis-dependent or mean-field methods, opening new possibilities for ab initio simulations of time-dependent quantum systems, with applications in quantum dynamics, molecular control, and ultrafast spectroscopy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cFermionic Antisymmetric Spatio-Temporal Network\u201d\u7684\u901a\u7528\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5b9e\u7a7a\u95f4\u4e2d\u7684\u65f6\u76f8\u5173\u859b\u5b9a\u8c14\u65b9\u7a0b\uff08TDSE\uff09\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u65f6\u53d8\u591a\u4f53\u5173\u8054\u548c\u53cd\u5bf9\u79f0\u6ce2\u51fd\u6570\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6c42\u89e3\u590d\u6742\u8d39\u7c73\u5b50\u7cfb\u7edf\u65f6\u76f8\u5173\u859b\u5b9a\u8c14\u65b9\u7a0b\uff08TDSE\uff09\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6355\u6349\u65f6\u53d8\u591a\u4f53\u5173\u8054\u548c\u5904\u7406\u53cd\u5bf9\u79f0\u6ce2\u51fd\u6570\u7684\u95ee\u9898\u3002", "method": "\u5c06TDSE\u6784\u5efa\u4e3a\u4e00\u4e2a\u5168\u5c40\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u65f6\u95f4\u4f5c\u4e3a\u663e\u5f0f\u8f93\u5165\u4e0e\u7a7a\u95f4\u5750\u6807\u4e00\u8d77\u5904\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u7edf\u4e00\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u652f\u6301\u9ad8\u5ea6\u53ef\u5e76\u884c\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u95ee\u9898\uff08\u4e00\u7ef4\u8c10\u632f\u5b50\u3001\u65f6\u53d8\u8c10\u632f\u5b50\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u8d39\u7c73\u5b50\u3001\u4e09\u7ef4\u6c22\u8f68\u9053\u52a8\u529b\u5b66\u3001\u6fc0\u5149\u9a71\u52a8\u7684H2\u5206\u5b50\uff09\u4e0a\u53d6\u5f97\u4e86\u4e0e\u53c2\u8003\u89e3\u7684\u4f18\u5f02\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u8868\u8fbe\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u6a21\u62df\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u957f\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u4e3a\u91cf\u5b50\u52a8\u529b\u5b66\u3001\u5206\u5b50\u63a7\u5236\u548c\u8d85\u5feb\u5149\u8c31\u5b66\u7b49\u9886\u57df\u7684\u4ece\u5934\u7b97\u6a21\u62df\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.12034", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.12034", "abs": "https://arxiv.org/abs/2511.12034", "authors": ["Xiaohao Liu", "Xiaobo Xia", "Jiaheng Wei", "Shuo Yang", "Xiu Su", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Calibrated Multimodal Representation Learning with Missing Modalities", "comment": null, "summary": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CalMRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u4e0d\u5b8c\u6574\u7684\u5bf9\u9f50\u6765\u5f25\u8865\u7f3a\u5931\u6a21\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u6240\u6709\u6a21\u6001\u90fd\u5b58\u5728\uff0c\u96be\u4ee5\u5904\u7406\u666e\u904d\u5b58\u5728\u7684\u6a21\u6001\u7f3a\u5931\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faCalMRL\u65b9\u6cd5\uff0c\u4ece\u951a\u70b9\u504f\u79fb\u89d2\u5ea6\u5206\u6790\u95ee\u9898\uff0c\u901a\u8fc7\u8868\u793a\u5c42\u63d2\u8865\u7f3a\u5931\u6a21\u6001\uff0c\u5e76\u91c7\u7528\u53cc\u6b65\u5b66\u4e60\u6cd5\u548c\u5171\u4eab\u6f5c\u53d8\u91cf\u540e\u9a8c\u5206\u5e03\u7684\u95ed\u5f0f\u89e3\u8fdb\u884c\u4f18\u5316\u3002", "result": "CalMRL\u6210\u529f\u7f13\u89e3\u4e86\u951a\u70b9\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u6a21\u6001\u7f3a\u5931\u6570\u636e\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "conclusion": "CalMRL\u4e3a\u5904\u7406\u6a21\u6001\u7f3a\u5931\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.12687", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.12687", "abs": "https://arxiv.org/abs/2511.12687", "authors": ["Partha S. Dey", "Aditya S. Gopalan", "Vijay G. Subramanian"], "title": "The Time to Consensus in a Blockchain: Insights into Bitcoin's \"6 Blocks Rule''", "comment": null, "summary": "We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \\emph{honest} and \\emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \\emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.", "AI": {"tldr": "\u7814\u7a76\u8005\u4eec\u7814\u7a76\u4e86Nakamoto\u94fe\u5728\u8bda\u5b9e\u548c\u5bf9\u6297\u4e24\u79cd\u589e\u957f\u6a21\u578b\u4e0b\u7684\u5171\u8bc6\u65f6\u95f4\uff0c\u5e76\u4f7f\u7528\u6392\u961f\u6280\u672f\u6765\u786e\u5b9a\u8bda\u5b9e\u6a21\u578b\u6c38\u4e45\u8d85\u8fc7\u5bf9\u6297\u6a21\u578b\u7684\u65f6\u95f4\u3002", "motivation": "\u786e\u5b9aNakamoto\u94fe\u7684\u5171\u8bc6\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u8bda\u5b9e\u548c\u5bf9\u6297\u589e\u957f\u6a21\u578b\u4e0b\u7684\u65f6\u95f4\uff0c\u5e76\u627e\u5230\u8bda\u5b9e\u6a21\u578b\u6c38\u4e45\u8d85\u8fc7\u5bf9\u6297\u6a21\u578b\u7684\u65f6\u95f4\u70b9\u3002", "method": "\u4f7f\u7528\u6392\u961f\u6280\u672f\u6765\u5206\u6790\u4e24\u4e2a\u7ade\u4e89\u589e\u957f\u8fc7\u7a0b\uff08\u8bda\u5b9e\u548c\u5bf9\u6297\uff09\uff0c\u5e76\u8ba1\u7b97\u5171\u8bc6\u65f6\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff0c\u6700\u540e\u901a\u8fc7\u6a21\u62df\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8ba1\u7b97\u4e86\u8bda\u5b9e\u589e\u957f\u8fc7\u7a0b\uff08\u53d7\u968f\u673a\u5ef6\u8fdf\u5f71\u54cd\uff09\u7684\u5171\u8bc6\u65f6\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u6392\u961f\u6280\u672f\u548c\u6a21\u62df\uff0c\u7814\u7a76\u4e86Nakamoto\u94fe\u5728\u4e0d\u540c\u589e\u957f\u6a21\u578b\u4e0b\u7684\u5171\u8bc6\u65f6\u95f4\uff0c\u5e76\u4e3a\u8bda\u5b9e\u6a21\u578b\u4f55\u65f6\u6c38\u4e45\u8d85\u8fc7\u5bf9\u6297\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2511.13429", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13429", "abs": "https://arxiv.org/abs/2511.13429", "authors": ["Yuqi Ping", "Tingting Zhang", "Tianhao Liang"], "title": "Handover-Aware URLLC UAV Trajectory Planning: A Continuous-Time Trajectory Optimization via Graphs of Convex Sets", "comment": "submited to IEEE International Conference on Communications", "summary": "In this paper, we study a cellular-connected unmanned aerial vehicle (UAV) which aims to fly between two predetermined locations while maintaining ultra-reliable low-latency communications (URLLC) for command-and-control (C2) links with terrestrial base stations (BSs). Long-range flights often trigger frequent inter-cell handovers, which may introduce delays and synchronization overhead. We jointly optimize the continuous trajectory and BS association to minimize handovers, path length, and flying time, subject to communication reliability and kinematic constraints. To address this problem, we reformulate it as an optimization based on the graph of convex sets (GCS). First, the URLLC requirement is translated into spatially feasible regions in the flight plane for each BS. And an intersection graph is constructed including the start and goal points. Each graph node is associated with a smooth and dynamically feasible trajectory segment. The trajectory is parameterized in space by B\u00e9zier curves and in time by a monotonic B\u00e9zier scaling, together with convex constraints that ensure continuity and enforce speed bounds. Next, we impose unit-flow constraints to enforce a single path, and by coupling the resulting binary edge-selection variables with the convex constraints, we obtain a mixed-integer convex program (MICP). Applying a convex relaxation and rounding to the mixed-integer convex program produces nearly globally optimal routes, and a final refinement yields smooth, dynamically feasible trajectories. Simulations verify that the method preserves URLLC connectivity while achieving a clear trade-off between fewer handovers and flight efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u901a\u4fe1\u548c\u8fd0\u52a8\u7ea6\u675f\u4e0b\uff0c\u5982\u4f55\u4f18\u5316\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u98de\u884c\u8f68\u8ff9\u548c\u57fa\u7ad9\uff08BS\uff09\u5173\u8054\uff0c\u4ee5\u6700\u5c0f\u5316\u5207\u6362\u6b21\u6570\u3001\u8def\u5f84\u957f\u5ea6\u548c\u98de\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u8bc1\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u3002", "motivation": "\u65e0\u4eba\u673a\u957f\u8ddd\u79bb\u98de\u884c\u4e2d\u9891\u7e41\u7684\u57fa\u7ad9\u5207\u6362\u4f1a\u5e26\u6765\u5ef6\u8fdf\u548c\u540c\u6b65\u5f00\u9500\uff0c\u9700\u8981\u4f18\u5316\u8f68\u8ff9\u548c\u57fa\u7ad9\u5173\u8054\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u5c06URLLC\u9700\u6c42\u8f6c\u5316\u4e3a\u6bcf\u4e2a\u57fa\u7ad9\u7684\u53ef\u884c\u98de\u884c\u533a\u57df\uff0c\u5e76\u6784\u5efa\u5305\u542b\u8d77\u70b9\u548c\u7ec8\u70b9\u7684\u56fe\u3002\u901a\u8fc7B\u00e9zier\u66f2\u7ebf\u548c\u5355\u8c03B\u00e9zier\u7f29\u653e\u6765\u53c2\u6570\u5316\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u8fde\u7eed\u6027\u548c\u901f\u5ea6\u7ea6\u675f\u3002\u901a\u8fc7\u5355\u4f4d\u6d41\u7ea6\u675f\u786e\u4fdd\u5355\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u4e8c\u8fdb\u5236\u8fb9\u7f18\u9009\u62e9\u53d8\u91cf\u548c\u51f8\u7ea6\u675f\u5f97\u5230\u6df7\u5408\u6574\u6570\u51f8\u89c4\u5212\uff08MICP\uff09\u3002\u5bf9MICP\u8fdb\u884c\u51f8\u677e\u5f1b\u548c\u820d\u5165\uff0c\u6700\u540e\u8fdb\u884c\u7cbe\u70bc\u4ee5\u83b7\u5f97\u5e73\u6ed1\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u8bc1URLLC\u8fde\u901a\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5207\u6362\u6b21\u6570\u548c\u98de\u884c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u65e0\u4eba\u673a\u7684\u57fa\u7ad9\u5207\u6362\u6b21\u6570\uff0c\u5e76\u63d0\u9ad8\u98de\u884c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.12848", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12848", "abs": "https://arxiv.org/abs/2511.12848", "authors": ["Max M. Sun", "Todd Murphey"], "title": "Structured Imitation Learning of Interactive Policies through Inverse Games", "comment": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/", "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u9700\u8981\u4e0e\u4eba\u7c7b\u5728\u5171\u4eab\u7a7a\u95f4\u8fdb\u884c\u4ea4\u4e92\u4e14\u65e0\u660e\u786e\u901a\u4fe1\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u65f6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u65b9\u6cd5\u5728\u6a21\u4eff\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u6a21\u4eff\u5b66\u4e60\u4ea4\u4e92\u5f0f\u7b56\u7565\uff08\u5c24\u5176\u662f\u5728\u65e0\u660e\u786e\u901a\u4fe1\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e0b\uff09\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u884c\u4e3a\u590d\u6742\u5ea6\u8fdc\u9ad8\u4e8e\u975e\u4ea4\u4e92\u5f0f\u4efb\u52a1\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\uff0c\u5229\u7528\u6807\u51c6\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ece\u591a\u667a\u80fd\u4f53\u6f14\u793a\u4e2d\u5b66\u4e60\u4e2a\u4f53\u884c\u4e3a\u6a21\u5f0f\uff1b\u7136\u540e\uff0c\u901a\u8fc7\u6c42\u89e3\u9006\u535a\u5f08\u95ee\u9898\u6765\u7ed3\u6784\u5316\u5730\u5b66\u4e60\u667a\u80fd\u4f53\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u7684 5 \u4e2a\u667a\u80fd\u4f53\u793e\u4f1a\u5bfc\u822a\u4efb\u52a1\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u975e\u4ea4\u4e92\u5f0f\u7b56\u7565\uff0c\u5e76\u4e14\u5728\u4ec5\u4f7f\u7528 50 \u4e2a\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4e0e\u771f\u5b9e\u4ea4\u4e92\u5f0f\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u7ed3\u6784\u5316\u6a21\u4eff\u5b66\u4e60\u5728\u4ea4\u4e92\u5f0f\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13033", "categories": ["quant-ph", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.13033", "abs": "https://arxiv.org/abs/2511.13033", "authors": ["Valter Uotila", "Cong Yu", "Bo Zhao"], "title": "ZX-DB: A Graph Database for Quantum Circuit Simplification and Rewriting via the ZX-Calculus", "comment": "9 pages, 16 figures", "summary": "Quantum computing is an emerging computational paradigm with the potential to outperform classical computers in solving a variety of problems. To achieve this, quantum programs are typically represented as quantum circuits, which must be optimized and adapted for target hardware through quantum circuit compilation. We introduce ZX-DB, a data-driven system that performs quantum circuit simplification and rewriting inside a graph database using ZX-calculus, a complete graphical formalism for quantum mechanics. ZX-DB encodes ZX-calculus rewrite rules as standard openCypher queries and executes them on an example graph database engine, Memgraph, enabling efficient, database-native transformations of large-scale quantum circuits. ZX-DB integrates correctness validation via tensor and graph equivalence checks and is evaluated against the state-of-the-art PyZX framework. Experimental results show that ZX-DB achieves up to an order-of-magnitude speedup for independent rewrites, while exposing pattern-matching bottlenecks in current graph database engines. By uniting quantum compilation and graph data management, ZX-DB opens a new systems direction toward scalable, database-supported quantum computing pipelines.", "AI": {"tldr": "ZX-DB \u662f\u4e00\u4e2a\u5229\u7528 ZX-calculus \u5728\u56fe\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u91cf\u5b50\u7535\u8def\u7b80\u5316\u548c\u91cd\u5199\u7684\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u6709\u6f5c\u529b\u8d85\u8d8a\u7ecf\u5178\u8ba1\u7b97\u673a\uff0c\u4f46\u9700\u8981\u5bf9\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u4f18\u5316\u548c\u9002\u914d\uff0c\u800c ZX-DB \u65e8\u5728\u901a\u8fc7\u6570\u636e\u7ba1\u7406\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "ZX-DB \u5c06 ZX-calculus \u91cd\u5199\u89c4\u5219\u7f16\u7801\u4e3a openCypher \u67e5\u8be2\uff0c\u5e76\u5728\u56fe\u6570\u636e\u5e93\u5f15\u64ce\uff08\u5982 Memgraph\uff09\u4e0a\u6267\u884c\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u91cf\u5b50\u7535\u8def\u7684\u6570\u636e\u5e93\u539f\u751f\u8f6c\u6362\u3002\u5b83\u8fd8\u96c6\u6210\u4e86\u5f20\u91cf\u548c\u56fe\u7b49\u4ef7\u6027\u68c0\u67e5\u4ee5\u9a8c\u8bc1\u6b63\u786e\u6027\u3002", "result": "\u4e0e PyZX \u76f8\u6bd4\uff0cZX-DB \u5728\u72ec\u7acb\u91cd\u5199\u65b9\u9762\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u4e0a\u7684\u52a0\u901f\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u5f53\u524d\u56fe\u6570\u636e\u5e93\u5f15\u64ce\u5728\u6a21\u5f0f\u5339\u914d\u65b9\u9762\u7684\u74f6\u9888\u3002", "conclusion": "ZX-DB \u5c06\u91cf\u5b50\u7f16\u8bd1\u4e0e\u56fe\u6570\u636e\u7ba1\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u6570\u636e\u5e93\u652f\u6301\u7684\u91cf\u5b50\u8ba1\u7b97\u6d41\u6c34\u7ebf\u5f00\u8f9f\u4e86\u65b0\u7684\u7cfb\u7edf\u65b9\u5411\u3002"}}
{"id": "2511.12040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12040", "abs": "https://arxiv.org/abs/2511.12040", "authors": ["Xinyuan Hu", "Changyue Shi", "Chuxiao Yang", "Minghao Chen", "Jiajun Ding", "Tao Wei", "Chen Wei", "Zhou Yu", "Min Tan"], "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images", "comment": "AAAI2026-Oral. Project Page: https://xinyuanhu66.github.io/SRSplat/", "summary": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.", "AI": {"tldr": "SRSplat\u662f\u4e00\u4e2a\u524d\u99883D\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u5916\u90e8\u53c2\u8003\u56fe\u50cf\u548c\u5185\u90e8\u7eb9\u7406\u7ebf\u7d22\u6765\u6062\u590d\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u7a00\u758f\u3001\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u8fdb\u884c\u524d\u99883D\u91cd\u5efa\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u6062\u590d\u7cbe\u7ec6\u7684\u7eb9\u7406\u7ec6\u8282\uff0c\u56e0\u4e3aLR\u8f93\u5165\u672c\u8eab\u7f3a\u4e4f\u9ad8\u9891\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "SRSplat\u6846\u67b6\u9996\u5148\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u6269\u6563\u6a21\u578b\u4e3a\u6bcf\u4e2a\u573a\u666f\u751f\u6210\u4e00\u4e2a\u573a\u666f\u7279\u5b9a\u7684\u53c2\u8003\u56fe\u5e93\u3002\u7136\u540e\uff0c\u5229\u7528\u53c2\u8003\u5f15\u5bfc\u7279\u5f81\u589e\u5f3a\uff08RGFE\uff09\u6a21\u5757\u5c06LR\u8f93\u5165\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\u3002\u63a5\u7740\uff0c\u8bad\u7ec3\u4e00\u4e2a\u89e3\u7801\u5668\u6765\u9884\u6d4b\u9ad8\u65af\u56fe\u5143\u3002\u6700\u540e\uff0c\u901a\u8fc7\u7eb9\u7406\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\uff08TADC\uff09\u6a21\u5757\u6839\u636eLR\u8f93\u5165\u7684\u5185\u90e8\u7eb9\u7406\u4e30\u5bcc\u5ea6\u81ea\u9002\u5e94\u5730\u8c03\u6574\u9ad8\u65af\u5bc6\u5ea6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u9884\u6d4b\u7684\u9ad8\u65af\u56fe\u5143\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSRSplat\u5728RealEstate10K\u3001ACID\u548cDTU\u7b49\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u548c\u8de8\u5206\u8fa8\u7387\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SRSplat\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u53c2\u8003\u548c\u5185\u90e8\u7eb9\u7406\u7ebf\u7d22\uff0c\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u4ece\u7a00\u758f\u3001\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u8fdb\u884c\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13155", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13155", "abs": "https://arxiv.org/abs/2511.13155", "authors": ["Jonathan Bader", "Julius Irion", "Jannis Kappel", "Joel Witzke", "Niklas Fomin", "Diellza Sherifi", "Odej Kao"], "title": "Learning Process Energy Profiles from Node-Level Power Data", "comment": null, "summary": "The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.", "AI": {"tldr": "\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u9700\u6c42\u589e\u957f\uff0c\u73b0\u6709\u6280\u672f\u6d4b\u91cf\u7c97\u7c92\u5ea6\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5\u901a\u8fc7eBPF\u548cperf\u6536\u96c6\u8d44\u6e90\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u8282\u70b9\u7ea7\u80fd\u8017\u6d4b\u91cf\uff0c\u5229\u7528\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u6bcf\u8fdb\u7a0b\u80fd\u8017\u9884\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5fc5\u987b\u83b7\u5f97\u80fd\u6e90\u6d88\u8017\u7684\u8fdb\u7a0b\u7ea7\u89c1\u89e3\u3002\u73b0\u6709\u7684\u6bcf\u8fdb\u7a0b\u80fd\u6e90\u4f7f\u7528\u4f30\u7b97\u65b9\u6cd5\uff08\u5982Intel RAPL\uff09\u4ec5\u9650\u4e8e\u7279\u5b9a\u786c\u4ef6\uff0c\u5e76\u4e14\u53ea\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u57df\u7ea7\u6d4b\u91cf\u3002", "method": "\u901a\u8fc7\u5229\u7528eBPF\u548cperf\u6536\u96c6\u7684\u7ec6\u7c92\u5ea6\u8fdb\u7a0b\u7ea7\u8d44\u6e90\u6307\u6807\uff0c\u5e76\u4e0e\u4ece\u9644\u52a0\u7535\u6e90\u5206\u914d\u5355\u5143\u83b7\u5f97\u7684\u8282\u70b9\u7ea7\u80fd\u8017\u6d4b\u91cf\u540c\u6b65\uff0c\u5bf9\u6bcf\u8fdb\u7a0b\u80fd\u8017\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u7edf\u8ba1\u5b66\u4e60\u8fdb\u7a0b\u7ea7\u8d44\u6e90\u4f7f\u7528\u4e0e\u8282\u70b9\u7ea7\u80fd\u8017\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u6bcf\u8fdb\u7a0b\u80fd\u8017\u9884\u6d4b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u7ec6\u7c92\u5ea6\u7684\u8fdb\u7a0b\u7ea7\u8d44\u6e90\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u8282\u70b9\u7ea7\u80fd\u8017\u6d4b\u91cf\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u6bcf\u8fdb\u7a0b\u80fd\u8017\u9884\u6d4b\uff0c\u4ece\u800c\u4e3a\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u6548\u7387\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2511.13513", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13513", "abs": "https://arxiv.org/abs/2511.13513", "authors": ["Leonard G\u00f6ke", "Jan Wohland", "Stefano Moret", "Andr\u00e9 Bardow"], "title": "The Liquid Buffer: Multi-Year Storage for Defossilization and Energy Security under Climate Uncertainty", "comment": null, "summary": "The climate-driven uncertainty of renewable generation and electricity demand challenges energy security in net-zero energy systems. By introducing a scalable stochastic model that implicitly accounts for 51'840 climate years, this paper identifies multi-year storage of liquid hydrocarbons as a key option for managing climate uncertainty and ensuring energy security. In Europe, multi-year storage reduces system costs by 4.1%, fossil imports by 86%, and curtailment by 60%. The benefit of multi-year storage is that a renewable surplus in one year is not curtailed but converted to synthetic oil, with hydrogen as an intermediate product, and stored to balance a future deficit. We find that the required energy capacity for liquid hydrocarbons is 525 TWh, a quarter of the European Union's current oil and gas reserves, complemented by 116 TWh for hydrogen storage. Security of supply remains high and unserved energy only amounts to 0.0035 per thousand, well below the common target of 0.02 per thousand.", "AI": {"tldr": "\u591a\u5e74\u4ee3\u9645\u50a8\u80fd\uff08\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\uff09\u662f\u5e94\u5bf9\u51c0\u96f6\u80fd\u6e90\u7cfb\u7edf\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\u3001\u786e\u4fdd\u80fd\u6e90\u5b89\u5168\u7684\u5173\u952e\u3002", "motivation": "\u5e94\u5bf9\u51c0\u96f6\u80fd\u6e90\u7cfb\u7edf\u4e2d\u7531\u6c14\u5019\u9a71\u52a8\u7684\u53ef\u518d\u751f\u53d1\u7535\u548c\u7535\u529b\u9700\u6c42\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u786e\u4fdd\u80fd\u6e90\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u968f\u673a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u9690\u542b\u5730\u8003\u8651\u4e8651840\u4e2a\u6c14\u5019\u5e74\u4efd\uff0c\u8bc6\u522b\u4e86\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\u7684\u591a\u5e74\u4ee3\u9645\u50a8\u80fd\u4f5c\u4e3a\u4e00\u79cd\u7ba1\u7406\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\u548c\u786e\u4fdd\u80fd\u6e90\u5b89\u5168\u7684\u5173\u952e\u9009\u62e9\u3002", "result": "\u5728\u6b27\u6d32\uff0c\u591a\u5e74\u4ee3\u9645\u50a8\u80fd\u53ef\u5c06\u7cfb\u7edf\u6210\u672c\u964d\u4f4e4.1%\uff0c\u5316\u77f3\u71c3\u6599\u8fdb\u53e3\u51cf\u5c1186%\uff0c\u5f03\u7535\u91cf\u51cf\u5c1160%\u3002\u6240\u9700\u7684\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\u50a8\u80fd\u5bb9\u91cf\u4e3a525 TWh\uff0c\u76f8\u5f53\u4e8e\u6b27\u76df\u76ee\u524d\u77f3\u6cb9\u548c\u5929\u7136\u6c14\u50a8\u91cf\u7684\u56db\u5206\u4e4b\u4e00\uff0c\u53e6\u9700116 TWh\u7528\u4e8e\u50a8\u6c22\u3002\u4f9b\u9700\u4fdd\u969c\u4fdd\u6301\u9ad8\u4f4d\uff0c\u672a\u4f9b\u5e94\u80fd\u91cf\u4ec5\u53600.0035/\u5343\uff0c\u8fdc\u4f4e\u4e8e\u901a\u5e38\u76840.02/\u5343\u76ee\u6807\u3002", "conclusion": "\u591a\u5e74\u4ee3\u9645\u50a8\u80fd\uff08\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\uff09\u80fd\u591f\u6709\u6548\u7ba1\u7406\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u51cf\u5c11\u5bf9\u5316\u77f3\u71c3\u6599\u7684\u4f9d\u8d56\uff0c\u5e76\u786e\u4fdd\u80fd\u6e90\u5b89\u5168\uff0c\u5728\u51c0\u96f6\u80fd\u6e90\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.12882", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12882", "abs": "https://arxiv.org/abs/2511.12882", "authors": ["Taiyi Su", "Jian Zhu", "Yaxuan Li", "Chong Ma", "Zitai Huang", "Yichen Zhu", "Hanli Wang", "Yi Xu"], "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos", "comment": "11 pages, 5 figures", "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.", "AI": {"tldr": "MTV-World\u901a\u8fc7\u5f15\u5165\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5177\u8eab\u4e16\u754c\u6a21\u578b\u5728\u4f4e\u7ea7\u52a8\u4f5c\u5230\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u590d\u6742\u53cc\u81c2\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u63a7\u5236\u548c\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u4e16\u754c\u6a21\u578b\u5728\u5c06\u4f4e\u7ea7\u52a8\u4f5c\uff08\u5982\u5173\u8282\u4f4d\u7f6e\uff09\u8f6c\u6362\u4e3a\u9884\u6d4b\u5e27\u4e2d\u7684\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u4e0e\u73b0\u5b9e\u7269\u7406\u4ea4\u4e92\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faMTV-World\uff0c\u4e00\u79cd\u5f15\u5165\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\uff08Multi-view Trajectory-Video control\uff09\u7684\u5177\u8eab\u4e16\u754c\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u901a\u8fc7\u76f8\u673a\u5185\u5916\u53c2\u6570\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u53d8\u6362\u83b7\u5f97\u7684\u8f68\u8ff9\u89c6\u9891\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u6846\u67b6\u6765\u8865\u507f3D\u52302D\u6295\u5f71\u9020\u6210\u7684\u7a7a\u95f4\u4fe1\u606f\u635f\u5931\u3002", "result": "MTV-World\u5728\u590d\u6742\u7684\u53cc\u81c2\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63a7\u5236\u6267\u884c\u548c\u51c6\u786e\u7684\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u3002\u5176\u8bc4\u4f30\u7ba1\u7ebf\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u548c\u53c2\u8003\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6a21\u578b\u6765\u8861\u91cf\u8fd0\u52a8\u7cbe\u786e\u6027\u548c\u5bf9\u8c61\u4ea4\u4e92\u51c6\u786e\u6027\uff0c\u5e76\u4f7f\u7528 Jaccard \u6307\u6570\u6765\u8861\u91cf\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "MTV-World\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u548c\u7269\u7406\u4ea4\u4e92\u9884\u6d4b\u3002"}}
{"id": "2511.13038", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13038", "abs": "https://arxiv.org/abs/2511.13038", "authors": ["Bo Peng", "Yu Zhang"], "title": "A Fractional Calculus Framework for Open Quantum Dynamics: From Liouville to Lindblad to Memory Kernels", "comment": null, "summary": "Open quantum systems exhibit dynamics ranging from purely unitary evolution to irreversible dissipative relaxation. The Gorini--Kossakowski--Sudarshan--Lindblad (GKSL) equation uniquely characterizes Markovian dynamics that are completely positive and trace-preserving (CPTP), yet many physical systems display non-Markovian features such as algebraic relaxation and coherence backflow beyond the reach of semigroup evolution. Fractional calculus provides a natural framework for describing such long-memory behavior through power-law temporal kernels introduced by fractional time derivatives. Here we establish a unified hierarchy that embeds fractional quantum master equations within the broader landscape of open system dynamics. The fractional master equation forms a structured subclass of memory-kernel models, reducing to the GKSL form at unit fractional order. Through Bochner--Phillips subordination, fractional evolution is expressed as an average over Lindblad semigroups weighted by a power-law waiting-time distribution. This construction ensures physical consistency, explains the algebraic origin of long-time decay, and bridges unitary, Markovian, and structured non-Markovian regimes. The resulting framework positions fractional calculus as a rigorous and unifying language for quantum dynamics with intrinsic memory, enabling new directions for theoretical analysis and quantum simulation.", "AI": {"tldr": "\u5206\u6570\u9636\u5fae\u79ef\u5206\u53ef\u7528\u4e8e\u63cf\u8ff0\u5177\u6709\u957f\u7a0b\u8bb0\u5fc6\u7684\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u7edf\u4e00\u4e86\u9149\u6f14\u5316\u3001\u9a6c\u5c14\u53ef\u592b\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u3002", "motivation": "\u8bb8\u591a\u7269\u7406\u7cfb\u7edf\u8868\u73b0\u51fa\u975e\u9a6c\u5c14\u53ef\u592b\u7279\u5f81\uff0c\u5982\u4ee3\u6570\u5f1b\u8c6b\u548c\u76f8\u5e72\u56de\u6d41\uff0c\u8fd9\u4e9b\u7279\u5f81\u8d85\u51fa\u4e86\u534a\u7fa4\u6f14\u5316\u7684\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u5206\u6570\u9636\u65f6\u95f4\u5bfc\u6570\u5f15\u5165\u5e42\u5f8b\u65f6\u95f4\u6838\uff0c\u5c06\u5206\u6570\u9636\u91cf\u5b50\u4e3b\u65b9\u7a0b\u5d4c\u5165\u5f00\u653e\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u66f4\u5e7f\u6cdb\u56fe\u666f\u4e2d\uff0c\u5e76\u5c06\u5176\u8868\u793a\u4e3aLindblad\u534a\u7fa4\u7684\u5e73\u5747\u503c\u3002", "result": "\u5206\u6570\u9636\u91cf\u5b50\u4e3b\u65b9\u7a0b\u662f\u8bb0\u5fc6\u6838\u6a21\u578b\u7684\u4e00\u4e2a\u7ed3\u6784\u5316\u5b50\u7c7b\uff0c\u5f53\u5206\u6570\u9636\u4e3a1\u65f6\u53ef\u7b80\u5316\u4e3aGKSL\u5f62\u5f0f\u3002\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u957f\u65f6\u8870\u51cf\u7684\u4ee3\u6570\u8d77\u6e90\uff0c\u5e76\u8fde\u63a5\u4e86\u9149\u3001\u9a6c\u5c14\u53ef\u592b\u548c\u7ed3\u6784\u5316\u975e\u9a6c\u5c14\u53ef\u592b\u4f53\u7cfb\u3002", "conclusion": "\u5206\u6570\u9636\u5fae\u79ef\u5206\u53ef\u4ee5\u4f5c\u4e3a\u5177\u6709\u5185\u5728\u8bb0\u5fc6\u7684\u91cf\u5b50\u52a8\u529b\u5b66\u7684\u4e25\u8c28\u7edf\u4e00\u8bed\u8a00\uff0c\u4e3a\u7406\u8bba\u5206\u6790\u548c\u91cf\u5b50\u6a21\u62df\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.12044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12044", "abs": "https://arxiv.org/abs/2511.12044", "authors": ["Cheng-Chang Tsai", "Kai-Wen Cheng", "Chun-Shien Lu"], "title": "FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification", "comment": "Extended version. 22 pages, 18 figures, 6 tables", "summary": "Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.", "AI": {"tldr": "FedSDA\u901a\u8fc7\u8c03\u6574\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u6765\u89e3\u51b3\u975eIID\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7684\u8054\u90a6\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u67d3\u8272\u5206\u79bb\u6280\u672f\u5bf9\u9f50\u67d3\u8272\u5206\u5e03\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u975eIID\uff08\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff09\u6570\u636e\u662f\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\uff0c\u5176\u7279\u5f81\u5206\u5e03\u5b58\u5728\u504f\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedSDA\uff08Federated Stain Distribution Alignment\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u67d3\u8272\u5206\u79bb\u6280\u672f\uff0c\u901a\u8fc7\u8c03\u6574\u6240\u6709\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5206\u5e03\u6765\u89e3\u51b3\u975eIID\u95ee\u9898\uff0c\u5177\u4f53\u505a\u6cd5\u662f\u5bf9\u9f50\u5404\u4e2a\u5ba2\u6237\u7aef\u7684\u67d3\u8272\u5206\u5e03\u4ee5\u5339\u914d\u76ee\u6807\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u8f7b\u5206\u5e03\u504f\u79fb\u3002\u4e3a\u907f\u514d\u5728FL\u4e2d\u76f4\u63a5\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0cFedSDA\u5728\u5b9e\u73b0\u5bf9\u9f50\u7684\u540c\u65f6\u89c4\u907f\u4e86\u8be5\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedSDA\u4e0d\u4ec5\u80fd\u63d0\u5347\u90a3\u4e9b\u65e8\u5728\u51cf\u8f7b\u5ba2\u6237\u7aef\u6a21\u578b\u66f4\u65b0\u5dee\u5f02\u7684\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u89e3\u51b3\u975eIID\u6570\u636e\u95ee\u9898\u7684\u89d2\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedSDA\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8df5\u89c1\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.13253", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13253", "abs": "https://arxiv.org/abs/2511.13253", "authors": ["Mordechai Guri"], "title": "Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices", "comment": null, "summary": "This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.", "AI": {"tldr": "Pico-Cloud\u662f\u4e00\u79cd\u57fa\u4e8e\u6811\u8393\u6d3eZero\u7b49\u8d85\u5c0f\u578b\u786c\u4ef6\u5e73\u53f0\u7684\u5fae\u8fb9\u7f18\u4e91\u67b6\u6784\uff0c\u63d0\u4f9b\u5bb9\u5668\u5316\u865a\u62df\u5316\u3001\u670d\u52a1\u53d1\u73b0\u548c\u8f7b\u91cf\u7ea7\u7f16\u6392\uff0c\u53ef\u5728\u8bbe\u5907\u5c42\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u672c\u5730\u8fd0\u884c\uff0c\u65e0\u9700\u4e2d\u5fc3\u5316\u6570\u636e\u4e2d\u5fc3\u3002\u5b83\u9002\u7528\u4e8e\u519c\u6751\u8fde\u63a5\u3001\u6559\u80b2\u96c6\u7fa4\u548c\u8fb9\u7f18AI\u63a8\u7406\u7b49\u573a\u666f\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u3001\u7f51\u7edc\u3001\u5b58\u50a8\u548c\u7535\u6e90\u7ba1\u7406\u7684\u8bbe\u8ba1\u6311\u6218\u3002\u7ed3\u679c\u8868\u660ePico-Cloud\u662f\u7f51\u7edc\u8fb9\u7f18\u8f7b\u91cf\u7ea7\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7ecf\u6d4e\u9ad8\u6548\u3001\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6301\u7eed\u7684\u5e73\u53f0\u3002", "motivation": "\u4ecb\u7ecdPico-Cloud\u5fae\u8fb9\u7f18\u4e91\u67b6\u6784\uff0c\u89e3\u51b3\u5728\u8d85\u5c0f\u578b\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5bb9\u5668\u5316\u865a\u62df\u5316\u3001\u670d\u52a1\u53d1\u73b0\u548c\u8f7b\u91cf\u7ea7\u7f16\u6392\u7684\u9700\u6c42\uff0c\u4ee5\u652f\u6301\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u672c\u5730\u8fd0\u884c\uff0c\u6446\u8131\u5bf9\u4e2d\u5fc3\u5316\u6570\u636e\u4e2d\u5fc3\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faPico-Cloud\u67b6\u6784\u6a21\u578b\uff0c\u5e76\u6982\u8ff0\u4e86\u519c\u6751\u8fde\u63a5\u3001\u6559\u80b2\u96c6\u7fa4\u548c\u8fb9\u7f18AI\u63a8\u7406\u7b49\u4ee3\u8868\u6027\u7528\u4f8b\u3002\u5bf9\u8ba1\u7b97\u3001\u7f51\u7edc\u3001\u5b58\u50a8\u548c\u7535\u6e90\u7ba1\u7406\u7684\u8bbe\u8ba1\u6311\u6218\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "Pico-Cloud\u88ab\u8bc1\u660e\u662f\u7f51\u7edc\u8fb9\u7f18\u8f7b\u91cf\u7ea7\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7ecf\u6d4e\u9ad8\u6548\u3001\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6301\u7eed\u7684\u5e73\u53f0\u3002", "conclusion": "Pico-Cloud\u4e3a\u7f51\u7edc\u8fb9\u7f18\u7684\u8f7b\u91cf\u7ea7\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u3001\u53bb\u4e2d\u5fc3\u5316\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13546", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.13546", "abs": "https://arxiv.org/abs/2511.13546", "authors": ["Stefan Ecklebe", "Frank Woittennek"], "title": "On the controller form for linear hyperbolic MIMO systems with dynamic boundary conditions", "comment": "Submitted to the 24th European Control Conference (ECC), 6 pages, 1 figure", "summary": "This contribution develops an algebraic approach to obtain a controller form for a class of linear hyperbolic MIMO systems, bidirectionally coupled with a linear ODE system at the unactuated boundary. After a short summary of established controller forms for SISO and MIMO ODE as well as SISO hyperbolic PDE systems, it is shown that the direct ap- proach to state a controller form fails already for a very simple MIMO example. Next, a generalised hyperbolic controller form with different variants is proposed and a new flatnesss-based scheme to compute said form is presented. Therein, the system is treated in an algebraic setting where generalised polynomials with real exponents are used to describe the predictions and delays in the system. The proposed algorithm is then applied to the motivating example.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee3\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u4e00\u7c7b\u7ebf\u6027\u53cc\u66f2MIMO\u7cfb\u7edf\u83b7\u5f97\u63a7\u5236\u5668\u5f62\u5f0f\uff0c\u8be5\u7cfb\u7edf\u5728\u975e\u9a71\u52a8\u8fb9\u754c\u5904\u4e0e\u7ebf\u6027ODE\u7cfb\u7edf\u8fdb\u884c\u53cc\u5411\u8026\u5408\u3002", "motivation": "\u73b0\u6709\u63a7\u5236\u5668\u5f62\u5f0f\uff08SISO\u548cMIMO ODE\u4ee5\u53caSISO\u53cc\u66f2PDE\u7cfb\u7edf\uff09\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u4e86\u5bf9\u66f4\u590d\u6742\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u5f62\u5f0f\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u6570\u7684\u8bbe\u7f6e\uff0c\u4f7f\u7528\u5177\u6709\u5b9e\u6570\u6307\u6570\u7684\u5e7f\u4e49\u591a\u9879\u5f0f\u6765\u63cf\u8ff0\u7cfb\u7edf\u7684\u9884\u6d4b\u548c\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65b0\u9896\u5e73\u5766\u6027\u8ba1\u7b97\u7684\u65b9\u6848\u6765\u8ba1\u7b97\u63a7\u5236\u5668\u5f62\u5f0f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5df2\u5e94\u7528\u4e8e\u63d0\u51fa\u7684\u53cc\u5411\u8026\u5408MIMO\u7cfb\u7edf\u793a\u4f8b\uff0c\u5e76\u6210\u529f\u8ba1\u7b97\u4e86\u5176\u63a7\u5236\u5668\u5f62\u5f0f\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u4e3a\u4e00\u7c7b\u590d\u6742\u7684\u7ebf\u6027\u53cc\u66f2MIMO\u7cfb\u7edf\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4ee3\u6570\u65b9\u6cd5\u548c\u57fa\u4e8e\u5e73\u5766\u6027\u7684\u8ba1\u7b97\u65b9\u6848\uff0c\u4ee5\u83b7\u5f97\u5176\u63a7\u5236\u5668\u5f62\u5f0f\u3002"}}
{"id": "2511.12896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12896", "abs": "https://arxiv.org/abs/2511.12896", "authors": ["Jun Huo", "Hongge Ru", "Bo Yang", "Xingjian Chen", "Xi Li", "Jian Huang"], "title": "Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction", "comment": null, "summary": "Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\\%$, 2.7$\\%$, 5.8$\\%$ and 6.7$\\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u5ba4\u7684\u8f6f\u4f53\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\uff0c\u5e76\u7ed3\u5408\u5206\u5c42\u7ed3\u6784\u5b9e\u73b0\u4e86\u89e3\u8026\uff0c\u80fd\u591f\u6ee1\u8db3\u8f6f\u4f53\u4ea4\u4e92\u7684\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7cbe\u786e\u7684\u529b\u4ea4\u4e92\uff0c\u9700\u8981\u80fd\u591f\u6355\u83b7\u516d\u8f74\u529b\u7684\u4f20\u611f\u5668\uff0c\u4f46\u73b0\u6709\u6280\u672f\u5b58\u5728\u6807\u5b9a\u96be\u3001\u7cbe\u5ea6\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b16\u901a\u9053\u6c14\u538b\u8ba1\u7684\u8f6f\u4f53\u6c14\u5ba4\u5f0f\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\u3002\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u521a\u8f6f\u5206\u5c42\u7ed3\u6784\u7684\u6709\u6548\u89e3\u8026\u65b9\u6cd5\uff0c\u5c06\u516d\u8f74\u89e3\u8026\u95ee\u9898\u7b80\u5316\u4e3a\u4e24\u4e2a\u4e09\u8f74\u89e3\u8026\u95ee\u9898\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\u3002\u539f\u578b\u673a\u572850N\u529b\u548c1Nm\u529b\u77e9\u8303\u56f4\u5185\uff0c\u5e73\u5747\u504f\u5dee\u3001\u91cd\u590d\u6027\u3001\u975e\u7ebf\u6027\u3001\u8fdf\u6ede\u6027\u5206\u522b\u4e3a4.9%\u30012.7%\u30015.8%\u548c6.7%\uff0c\u6ee1\u8db3\u8f6f\u4f53\u4f20\u611f\u5668\u6027\u80fd\u8981\u6c42\u3002", "conclusion": "\u8be5\u8f6f\u4f53\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\u5177\u6709\u826f\u597d\u7684\u4f20\u611f\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f6f\u4f53\u6c14\u5ba4\u7684\u67d4\u8f6f\u6027\u3002"}}
{"id": "2511.13072", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13072", "abs": "https://arxiv.org/abs/2511.13072", "authors": ["Antonio David Bastida Zamora", "Ljubomir Budinski", "Valtteri Lahtinen", "Pierre Sagaut"], "title": "Quantum lattice Boltzmann method for several time steps: A local Carleman linearization algorithm", "comment": "14 pages, 14 figures", "summary": "This article presents a novel encoding for quantum Lattice Boltzmann method algorithm using Carleman linearization. In contrast to previous articles \\cite{Sanavio2024LatticeBC,sanavio2025carleman}, the encoding used allows for local collision rules while keeping a higher probability to obtain the right result, which is of the order of $10^{-2}$. The algorithm scales as $O(log_2^3(N)+Q^4)$ each time step with $N$ the number of lattice sites of the 2D lattice and $Q$ the number of channels with a constant number of qubits when using dynamical circuits.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Carleman\u7ebf\u6027\u5316\u8fdb\u884c\u91cf\u5b50\u683c\u5b50Boltzmann\u65b9\u6cd5\u7684\u65b0\u578b\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u5c40\u90e8\u78b0\u649e\u89c4\u5219\u548c\u9ad8\u7cbe\u5ea6\uff08\u7ea6\u4e3a10^-2\uff09\u3002", "motivation": "\u4e0e\u5148\u524d\u5de5\u4f5c\u4e0d\u540c\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7f16\u7801\u5141\u8bb8\u5c40\u90e8\u78b0\u649e\u89c4\u5219\uff0c\u540c\u65f6\u4fdd\u6301\u83b7\u5f97\u6b63\u786e\u7ed3\u679c\u7684\u8f83\u9ad8\u6982\u7387\uff08\u7ea6\u4e3a10^-2\uff09\u3002", "method": "\u91c7\u7528Carleman\u7ebf\u6027\u5316\u5bf9\u91cf\u5b50\u683c\u5b50Boltzmann\u65b9\u6cd5\u8fdb\u884c\u7f16\u7801\u3002", "result": "\u8be5\u7b97\u6cd5\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(log_2^3(N)+Q^4)\uff0c\u5176\u4e2dN\u662f2D\u683c\u70b9\u7684\u6570\u91cf\uff0cQ\u662f\u901a\u9053\u7684\u6570\u91cf\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u52a8\u6001\u7535\u8def\u65f6\u4fdd\u6301\u6052\u5b9a\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u578b\u7f16\u7801\u65b9\u6848\u5728\u91cf\u5b50\u683c\u5b50Boltzmann\u65b9\u6cd5\u4e2d\u5177\u6709\u5c40\u90e8\u78b0\u649e\u89c4\u5219\u548c\u9ad8\u7cbe\u5ea6\u7684\u4f18\u70b9\u3002"}}
{"id": "2511.12047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12047", "abs": "https://arxiv.org/abs/2511.12047", "authors": ["Huimin Cheng", "Xiaowei Yu", "Shushan Wu", "Luyang Fang", "Chao Cao", "Jing Zhang", "Tianming Liu", "Dajiang Zhu", "Wenxuan Zhong", "Ping Ma"], "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging", "comment": null, "summary": "Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.", "AI": {"tldr": "DCMM-Transformer\u662f\u4e00\u79cd\u65b0\u7684ViT\u67b6\u6784\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u901a\u8fc7\u5c06\u5ea6\u91cf\u6df7\u5408\u6a21\u578b\uff08DCMM\uff09\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u9644\u52a0\u504f\u7f6e\u6765\u89e3\u51b3\u6807\u51c6ViT\u65e0\u6cd5\u5229\u7528\u6f5c\u5728\u89e3\u5256\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u6807\u51c6ViT\u65e0\u6cd5\u5229\u7528\u533b\u5b66\u56fe\u50cf\u4e2d\u5b58\u5728\u7684\u6f5c\u5728\u89e3\u5256\u7ed3\u6784\uff08\u5982\u5668\u5b98\u3001\u7ec4\u7ec7\u548c\u75c5\u53d8\u533a\u57df\uff09\u3002", "method": "\u5c06\u5ea6\u91cf\u6df7\u5408\u6a21\u578b\uff08DCMM\uff09\u4f5c\u4e3a\u9644\u52a0\u504f\u7f6e\u5f15\u5165\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u5206\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u5f15\u5165\u793e\u7fa4\u7ed3\u6784\u548c\u5ea6\u91cf\u5f02\u8d28\u6027\u3002", "result": "\u5728\u5305\u62ec\u5927\u8111\u3001\u80f8\u90e8\u3001\u4e73\u817a\u548c\u773c\u79d1\u7b49\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002\u5b66\u4e60\u5230\u7684\u7fa4\u7ec4\u7ed3\u6784\u548c\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u8c03\u5236\u901a\u8fc7\u4ea7\u751f\u89e3\u5256\u5b66\u4e0a\u6709\u610f\u4e49\u4e14\u8bed\u4e49\u4e0a\u8fde\u8d2f\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u5927\u5927\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DCMM-Transformer\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13313", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13313", "abs": "https://arxiv.org/abs/2511.13313", "authors": ["Sulaiman Muhammad Rashid", "Ibrahim Aliyu", "Jaehyung Park", "Jinsul Kim"], "title": "Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems", "comment": null, "summary": "The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.", "AI": {"tldr": "Metaverse\u5bf9\u5ef6\u8fdf\u548c\u8d44\u6e90\u7684\u8981\u6c42\u5f88\u9ad8\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba1\u7b97\u7f51\u7edc(COIN)\u548c\u591a\u8def\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97(MEC)\u7684\u5207\u7247\u611f\u77e5\u7f51\u7edc\u5185\u8fb9\u7f18\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bDeepSets-S\u89e3\u51b3\u4e86\u8d44\u6e90\u7ba1\u7406\u548c\u5207\u7247\u9009\u62e9\u95ee\u9898\u3002", "motivation": "Metaverse\u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u8d44\u6e90\u7684\u9700\u6c42\u7ed9\u4f20\u7edf\u4f18\u5316\u6280\u672f\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u5c06\u8054\u5408\u8d44\u6e90\u7ba1\u7406\u548c\u5207\u7247\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212(MINLP)\uff0c\u5e76\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\u3002\u5229\u7528\u79bb\u7ebf\u6700\u4f18\u89e3\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8eDeepSets\u7684\u5206\u5e03\u5f0f\u5206\u5c42\u6a21\u578b(DeepSets-S)\uff0c\u8be5\u6a21\u578b\u5177\u6709\u65b0\u9896\u7684\u5f52\u4e00\u5316\u673a\u5236\u548c\u7f6e\u6362\u7b49\u53d8\u6027\u3002", "result": "DeepSets-S\u5728SP1/SP2\u4e0a\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7cbe\u5ea6(Acc1 = 95.26%, 95.67%)\uff0c\u5728SP3\u4e0a\u63d0\u9ad8\u4e86\u591a\u7c7b\u5378\u8f7d\u7cbe\u5ea6(Acc = 0.7486; \u4e8c\u8fdb\u5236\u672c\u5730/\u5378\u8f7dAcc = 0.8824)\u3002\u4e0e\u7cbe\u786e\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u6267\u884c\u65f6\u95f4\u7f29\u77ed\u4e8686.1%\uff0c\u540c\u65f6\u6700\u4f18\u7cfb\u7edf\u6210\u672c\u7684\u8ddf\u8e2a\u8bef\u5dee\u57286.1%\u4ee5\u5185\u3002\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cDeepSets-S\u5728COIN/MEC\u8d44\u6e90\u5229\u7528\u7387\u548c\u6210\u672c\u6bd4\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "DeepSets-S\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u89e3\u51b3Metaverse\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u548c\u5207\u7247\u9009\u62e9\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u8fd1\u4e4e\u6700\u4f18\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.13588", "categories": ["eess.SY", "cs.AI", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.13588", "abs": "https://arxiv.org/abs/2511.13588", "authors": ["Agustin Castellano", "Shijie Pan", "Enrique Mallada"], "title": "Data-driven Acceleration of MPC with Guarantees", "comment": null, "summary": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.", "AI": {"tldr": "We developed a data-driven framework to speed up Model Predictive Control (MPC) by using a learned policy instead of online optimization, making it much faster for real-time applications while maintaining good performance.", "motivation": "Model Predictive Control (MPC) is computationally expensive and too slow for low-latency applications.", "method": "We propose a data-driven framework that replaces online MPC optimization with a nonparametric policy. This policy is derived from offline MPC solutions and uses a greedy approach based on an upper bound of the cost-to-go. It functions as a fast lookup rule.", "result": "The proposed policy is significantly faster (100-1000x) than standard MPC, with a small reduction in optimality. It is recursively feasible and has a provable, bounded optimality gap under sufficient data coverage, which allows for a trade-off between data amount and bound tightness.", "conclusion": "The data-driven policy accelerates MPC, making it suitable for real-time control tasks by offering a substantial speed improvement with only a minor impact on optimality."}}
{"id": "2511.12910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12910", "abs": "https://arxiv.org/abs/2511.12910", "authors": ["Yong Li", "Yujun Huang", "Yi Chen", "Hui Cheng"], "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints", "comment": null, "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTOPP-DWR\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5dee\u901f\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\uff08DWR\uff09\u7684\u65f6\u95f4\u6700\u4f18\u8f68\u8ff9\u53c2\u6570\u5316\uff08TOPP\uff09\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u8003\u8651\u4e86\u89d2\u901f\u5ea6\u548c\u5173\u8282\u901f\u5ea6\u7b49\u5b9e\u9645\u7ea6\u675f\uff0c\u5e76\u5c06\u5176\u7edf\u4e00\u8868\u793a\u4e3a\u7ebf\u901f\u5ea6\u7ea6\u675f\uff0c\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u5176\u8f6c\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTOPP-DWR\u5728\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\uff0c\u5e76\u5728\u5b9e\u9645\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u89d2\u901f\u5ea6\u548c\u5173\u8282\u901f\u5ea6\u7ea6\u675f\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63a7\u5236\u6027\u80fd\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u8003\u8651\u8fd9\u4e9b\u7ea6\u675f\u7684TOPP\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8DWR\u7684\u63a7\u5236\u6027\u80fd\u3002", "method": "1. \u91c7\u7528\u975e\u5747\u5300B\u6837\u6761\u8868\u793a\u521d\u59cb\u8f68\u8ff9\u3002 2. \u8003\u8651\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u3001\u7ebf\u901f\u5ea6\u548c\u7ebf\u52a0\u901f\u5ea6\u7ea6\u675f\u3002 3. \u5c06\u6240\u6709\u7ea6\u675f\u7edf\u4e00\u8868\u793a\u4e3a\u7ebf\u901f\u5ea6\u7ea6\u675f\u3002 4. \u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u95ee\u9898\u3002 5. \u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u548c\u5b9e\u5730\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u6027\u80fd\u3002", "result": "TOPP-DWR\u7b97\u6cd5\u5728\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\uff0c\u5e76\u5728\u5b9e\u9645\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TOPP-DWR\u7b97\u6cd5\u7cfb\u7edf\u4e14\u5b9e\u7528\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5dee\u901f\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\u7684\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u95ee\u9898\uff0c\u5e76\u8003\u8651\u4e86\u5b9e\u9645\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u63d0\u9ad8\u4e86\u63a7\u5236\u6027\u80fd\u548c\u5bfc\u822a\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.13088", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13088", "abs": "https://arxiv.org/abs/2511.13088", "authors": ["A-Long Zhou", "Ya-Wen Xiao", "Nuo Xu", "Li-Li Gao", "Long-Jie Li", "Hang Zhou", "Zi-Min Li", "Chuan-Cun Shu"], "title": "Topological enhancement of a PT-symmetric Su-Schrieffer-Heeger quantum battery", "comment": "12 pages, 5 figures", "summary": "We investigate a non-Hermitian quantum battery based on the Su-Schrieffer-Heeger (SSH) lattice, charged through a PT-symmetric protocol that alternates gain and loss between the two sublattices. The interplay between lattice topology and non-Hermiticity gives rise to both bulk and edge exceptional points (EPs), which govern the charging dynamics. In the topological regime, an edge-state EP emerges at an exponentially small non-Hermitian strength, resulting in early PT-symmetry breaking and rapid energy accumulation. This topological enhancement originates from the PT-symmetric non-Hermitian dynamics, in which the broken-symmetry edge mode with the largest imaginary part of the eigenvalue dominates the time evolution. Consequently, the topological phase consistently yields higher stored energy and faster saturation than the trivial configuration across all parameter regimes and system sizes. These findings demonstrate that topology constitutes a genuine physical resource for enhancing the performance of quantum batteries.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSSH\u6a21\u578b\u7684\u975e\u5384\u7c73\u91cf\u5b50\u7535\u6c60\uff0c\u901a\u8fc7PT\u5bf9\u79f0\u534f\u8bae\u8fdb\u884c\u5145\u7535\uff0c\u5e76\u5728\u62d3\u6251\u548c\u975e\u62d3\u6251\u4e24\u79cd\u60c5\u51b5\u4e0b\u6bd4\u8f83\u4e86\u5176\u50a8\u80fd\u548c\u5145\u7535\u901f\u5ea6\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u7535\u6c60\u7684\u6027\u80fd\u63d0\u5347\u673a\u5236\uff0c\u7279\u522b\u662f\u5229\u7528\u975e\u5384\u7c73\u6027\u548c\u62d3\u6251\u6027\u8d28\u3002", "method": "\u6784\u5efa\u5e76\u6a21\u62df\u4e86\u4e00\u4e2a\u57fa\u4e8eSSH\u6a21\u578b\u7684\u975e\u5384\u7c73\u91cf\u5b50\u7535\u6c60\uff0c\u91c7\u7528PT\u5bf9\u79f0\u534f\u8bae\u8fdb\u884c\u5145\u7535\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u62d3\u6251\u548c\u975e\u62d3\u6251\u60c5\u51b5\u4e0b\u7684\u50a8\u80fd\u548c\u5145\u7535\u52a8\u529b\u5b66\u3002", "result": "\u5728\u62d3\u6251\u533a\u57df\uff0c\u7531\u4e8e\u8fb9\u7f18\u6001\u7684\u975e\u5384\u7c73\u6027\u589e\u5f3a\uff0c\u91cf\u5b50\u7535\u6c60\u7684\u5145\u7535\u901f\u5ea6\u66f4\u5feb\uff0c\u5b58\u50a8\u7684\u80fd\u91cf\u4e5f\u66f4\u9ad8\u3002", "conclusion": "\u62d3\u6251\u6027\u8d28\u662f\u63d0\u5347\u91cf\u5b50\u7535\u6c60\u6027\u80fd\u7684\u6709\u6548\u7269\u7406\u8d44\u6e90\u3002"}}
{"id": "2511.12048", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12048", "abs": "https://arxiv.org/abs/2511.12048", "authors": ["Saksham Kumar", "Ashish Singh", "Srinivasarao Thota", "Sunil Kumar Singh", "Chandan Kumar"], "title": "DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training", "comment": null, "summary": "Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\\% accuracy after stage one and 99.22\\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.", "AI": {"tldr": "DeiTFake\u662f\u4e00\u4e2a\u57fa\u4e8eDeiT\u7684Transformer\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4bDeepfakes\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe99.22%\u3002", "motivation": "Deepfakes\u5bf9\u6570\u5b57\u5a92\u4f53\u7684\u5b8c\u6574\u6027\u6784\u6210\u4e86\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeiT\u7684Transformer\u6a21\u578b\uff08DeiTFake\uff09\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u9010\u6b65\u589e\u52a0\u6570\u636e\u589e\u5f3a\u7684\u590d\u6742\u5ea6\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6807\u51c6\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u52a0\u5165\u66f4\u9ad8\u7ea7\u7684\u4eff\u5c04\u53d8\u6362\u548cDeepfake\u7279\u5b9a\u6570\u636e\u589e\u5f3a\u3002DeiT\u7684\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u7ec6\u5fae\u7684\u7be1\u6539\u75d5\u8ff9\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728OpenForensics\u6570\u636e\u96c6\uff08190,335\u5f20\u56fe\u50cf\uff09\u4e0a\u8bad\u7ec3\u540e\uff0cDeiTFake\u5728\u7b2c\u4e00\u9636\u6bb5\u8fbe\u5230\u4e8698.71%\u7684\u51c6\u786e\u7387\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\u8fbe\u5230\u4e8699.22%\u7684\u51c6\u786e\u7387\uff0cAUROC\u503c\u4e3a0.9997\uff0c\u4f18\u4e8e\u6700\u65b0\u7684OpenForensics\u57fa\u7ebf\u6a21\u578b\u3002\u5bf9\u6570\u636e\u589e\u5f3a\u548c\u8bad\u7ec3\u8ba1\u5212\u7684\u5f71\u54cd\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u9762\u90e8Deepfake\u68c0\u6d4b\u7684\u5b9e\u7528\u57fa\u51c6\u3002", "conclusion": "DeiTFake\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548cDeiT\u7684\u77e5\u8bc6\u84b8\u998f\u80fd\u529b\uff0c\u5728Deepfake\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u901a\u8fc7\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11581", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11581", "abs": "https://arxiv.org/abs/2511.11581", "authors": ["Burkhard Ringlein", "Jan van Lunteren", "Radu Stoica", "Thomas Parnell"], "title": "The Anatomy of a Triton Attention Kernel", "comment": null, "summary": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528 Triton \u8bed\u8a00\u5f00\u53d1 Paged Attention \u6838\u5fc3\uff0c\u5b9e\u73b0\u4e86\u8de8 NVIDIA \u548c AMD GPU \u7684 LLM \u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u53ef\u4ee5\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u8fd0\u884c\u3001\u65e0\u9700\u5e95\u5c42\u624b\u52a8\u8c03\u4f18\u4e14\u6548\u7387\u9ad8 LLM \u63a8\u7406\u5e73\u53f0\u3002", "method": "\u4f7f\u7528 Triton \u8bed\u8a00\u5f00\u53d1 Paged Attention \u6838\u5fc3\uff0c\u5e76\u8fdb\u884c\u53c2\u6570\u81ea\u52a8\u8c03\u4f18\uff0c\u7136\u540e\u96c6\u6210\u5230\u63a8\u7406\u670d\u52a1\u5668\u3002", "result": "\u5728 NVIDIA \u548c AMD GPU \u4e0a\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c06\u901a\u7528 Triton \u6ce8\u610f\u529b\u5185\u6838\u7684\u6027\u80fd\u4ece\u7406\u8bba\u6700\u4f73\u7684 19.7% \u63d0\u9ad8\u5230 105.9%\u3002", "conclusion": "\u5f00\u6e90\u7279\u5b9a\u9886\u57df\u8bed\u8a00\uff08\u5982 Triton\uff09\u53ef\u4ee5\u5b9e\u73b0\u8de8 GPU \u4f9b\u5e94\u5546\u7684\u6a21\u578b\u53ef\u79fb\u690d\u6027\u548c\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.13595", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13595", "abs": "https://arxiv.org/abs/2511.13595", "authors": ["Sebastiano Mengozzi", "Giovanni B. Esposito", "Michelangelo Bin", "Andrea Acquaviva", "Andrea Bartolini", "Lorenzo Marconi"], "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation", "comment": null, "summary": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold \u03c0(w) and a feedforward input c(w) that render such manifold invariant. The pair (\u03c0(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates \u03c0(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u65b9\u6cd5\u6765\u6c42\u89e3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5168\u4fe1\u606f\u8f93\u51fa\u8c03\u8282\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u96f6\u8c03\u8282\u8bef\u5dee\u6d41\u5f62\u548c\u524d\u9988\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5916\u90e8\u7cfb\u7edf\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u76f4\u5347\u673a\u5782\u76f4\u52a8\u529b\u5b66\u540c\u6b65\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5168\u4fe1\u606f\u8f93\u51fa\u8c03\u8282\u95ee\u9898\uff0c\u5373\u5728\u5df2\u77e5\u690d\u7269\u548c\u5916\u90e8\u7cfb\u7edf\u72b6\u6001\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5b8c\u7f8e\u7684\u8ddf\u8e2a\u6216\u6291\u5236\u3002", "method": "\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u76f4\u63a5\u903c\u8fd1\u96f6\u8c03\u8282\u8bef\u5dee\u6d41\u5f62 \u03c0(w) \u548c\u524d\u9988\u8f93\u5165 c(w)\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6b8b\u5dee\u5e76\u6ee1\u8db3\u8fb9\u754c\u548c\u53ef\u884c\u6027\u6761\u4ef6\u6765\u6c42\u89e3\u8c03\u8282\u65b9\u7a0b\uff0c\u65e0\u9700\u9884\u8ba1\u7b97\u8f68\u8ff9\u6216\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7b97\u5b50\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u5177\u6709\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u548c\u53c2\u6570\u7684\u5916\u90e8\u7cfb\u7edf\u5bb6\u65cf\u3002\u5728\u76f4\u5347\u673a\u5782\u76f4\u52a8\u529b\u5b66\u540c\u6b65\u4efb\u52a1\u4e2d\uff0cPINN\u6c42\u89e3\u5668\u9ad8\u4fdd\u771f\u5730\u91cd\u5efa\u4e86\u96f6\u8bef\u5dee\u6d41\u5f62\uff0c\u5e76\u5728\u5916\u90e8\u7cfb\u7edf\u53d8\u5316\u4e0b\u4fdd\u6301\u4e86\u8c03\u8282\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u6c42\u89e3\u5668\u5728\u975e\u7ebf\u6027\u8f93\u51fa\u8c03\u8282\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u8be5\u65b9\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u627f\u8ba4\u8f93\u51fa\u8c03\u8282\u95ee\u9898\u89e3\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u3002"}}
{"id": "2511.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12912", "abs": "https://arxiv.org/abs/2511.12912", "authors": ["Yingting Zhou", "Wenbo Cui", "Weiheng Liu", "Guixing Chen", "Haoran Li", "Dongbin Zhao"], "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping", "comment": null, "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.", "AI": {"tldr": "DiffuDepGrasp\u662f\u4e00\u4e2a\u521b\u65b0\u7684sim2real\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u7684\u6a21\u62df\u6df1\u5ea6\u56fe\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6548\u7387\u548c\u90e8\u7f72\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002", "motivation": "\u771f\u5b9e\u6df1\u5ea6\u56fe\u4e2d\u7684\u4f20\u611f\u5668\u4f2a\u5f71\uff08\u5982\u7a7a\u6d1e\u548c\u566a\u58f0\uff09\u5728\u5c06\u6a21\u62df\u8bad\u7ec3\u7684\u7b56\u7565\u8fc1\u79fb\u5230\u7269\u7406\u673a\u5668\u4eba\u65f6\uff0c\u4f1a\u4ea7\u751f\u663e\u8457\u7684sim2real\u5dee\u8ddd\uff0c\u4e25\u91cd\u963b\u788d\u6293\u53d6\u7b56\u7565\u7684\u8fc1\u79fb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u90e8\u7f72\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDiffuDepGrasp\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1. \u6269\u6563\u6df1\u5ea6\u6a21\u5757\uff1a\u5229\u7528\u65f6\u95f4\u51e0\u4f55\u5148\u9a8c\uff0c\u9ad8\u6548\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u6355\u6349\u590d\u6742\u7684\u4f20\u611f\u5668\u566a\u58f0\u5206\u5e03\u30022. \u566a\u58f0\u5ac1\u63a5\u6a21\u5757\uff1a\u5728\u6ce8\u5165\u611f\u77e5\u4f2a\u5f71\u65f6\u4fdd\u6301\u5ea6\u91cf\u7cbe\u5ea6\u3002\u8be5\u6846\u67b6\u4ec5\u5728\u90e8\u7f72\u65f6\u4f7f\u7528\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\uff0c\u65e0\u9700\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "DiffuDepGrasp\u572812\u4e2a\u7269\u4f53\u7684\u6293\u53d6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8695.7%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5e76\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u90e8\u7f72\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "DiffuDepGrasp\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u903c\u771f\u7684\u4f20\u611f\u5668\u566a\u58f0\uff0c\u4ece\u800c\u5b9e\u73b0\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6293\u53d6\u9886\u57df\u4e2dsim2real\u5dee\u8ddd\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002"}}
{"id": "2511.13090", "categories": ["quant-ph", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2511.13090", "abs": "https://arxiv.org/abs/2511.13090", "authors": ["Arun Kumar Pati", "Vlatko Vedral"], "title": "Fractional Contribution of Dynamical and Geometric Phases in Quantum Evolution", "comment": "Latex, 5 Pages, No Figs", "summary": "The fundamental division of the total quantum evolution phase into geometric and dynamical components is a central problem in quantum physics. Here, we prove a remarkably simple and universal law demonstrating that this partitioning is governed, at every instant, solely by a single geometric quantity: the Bargmann angle (Bures angle). This result provides a universally applicable and rigorous way to define the exact fraction of the total phase that is geometric versus dynamical in origin, thereby establishing a new quantitative link between the dynamics of quantum evolution and the geometry of the state space. This finding has immediate practical consequences, furnishing a real-time measure of the geometricity of an evolution for designing high-fidelity geometric quantum gates with optimized robustness, and opening new avenues for quantum speed limit and coherent control.", "AI": {"tldr": "\u91cf\u5b50\u6f14\u5316\u76f8\u4f4d\u53ef\u7531Bargmann\u89d2\u5ea6\u552f\u4e00\u786e\u5b9a\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e0e\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u5212\u5206\uff0c\u5e76\u5e94\u7528\u4e8e\u91cf\u5b50\u95e8\u8bbe\u8ba1\u4e0e\u76f8\u5e72\u63a7\u5236\u3002", "motivation": "\u533a\u5206\u91cf\u5b50\u6f14\u5316\u603b\u76f8\u4f4d\u7684\u51e0\u4f55\u4e0e\u52a8\u529b\u5b66\u7ec4\u6210\u90e8\u5206\u662f\u91cf\u5b50\u7269\u7406\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u666e\u9002\u6027\u5b9a\u5f8b\uff0c\u8bc1\u660e\u8be5\u5212\u5206\u4ec5\u7531Bargmann\u89d2\u5ea6\uff08Bures\u89d2\u5ea6\uff09\u8fd9\u4e00\u5355\u4e00\u51e0\u4f55\u91cf\u51b3\u5b9a\u3002", "result": "\u8bc1\u660e\u4e86Bargmann\u89d2\u5ea6\u662f\u533a\u5206\u91cf\u5b50\u6f14\u5316\u603b\u76f8\u4f4d\u7684\u51e0\u4f55\u4e0e\u52a8\u529b\u5b66\u7ec4\u6210\u90e8\u5206\u7684\u552f\u4e00\u51b3\u5b9a\u56e0\u7d20\uff0c\u5e76\u5efa\u7acb\u4e86\u91cf\u5b50\u6f14\u5316\u52a8\u529b\u5b66\u4e0e\u72b6\u6001\u7a7a\u95f4\u51e0\u4f55\u4e4b\u95f4\u65b0\u7684\u91cf\u5316\u8054\u7cfb\u3002", "conclusion": "\u8be5\u53d1\u73b0\u53ef\u5b9e\u65f6\u91cf\u5316\u6f14\u5316\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u9ad8\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u4f18\u5316\u7684\u51e0\u4f55\u91cf\u5b50\u95e8\uff0c\u5e76\u4e3a\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u548c\u76f8\u5e72\u63a7\u5236\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.12054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12054", "abs": "https://arxiv.org/abs/2511.12054", "authors": ["Cuiqun Chen", "Qi Chen", "Bin Yang", "Xingyi Zhang"], "title": "UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization", "comment": "Accepted as Oral Presentation at AAAI 2026. 10 pages, 9 figures", "summary": "Cross-view geo-localization (CVGL) matches query images ($\\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\\rightarrow$ Drone AP by +10.63\\% on University-1652 and +16.73\\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG", "AI": {"tldr": "UniABG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u76d1\u7763\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u89c6\u56fe\u6865\u63a5\u548c\u57fa\u4e8e\u56fe\u7684\u5bf9\u5e94\u6821\u51c6\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u65e0\u76d1\u7763\u65b9\u6cd5\u5219\u5b58\u5728\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002UniABG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u89c6\u56fe\u611f\u77e5\u5bf9\u6297\u6865\u63a5\uff08VAAB\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u7279\u5f81\u5e76\u589e\u5f3a\u4f2a\u6807\u7b7e\u3002 2. \u5f02\u6784\u56fe\u8fc7\u6ee4\u6821\u51c6\uff08HGFC\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u53cc\u89c6\u89d2\u7ed3\u6784\u56fe\u6765\u4f18\u5316\u8de8\u89c6\u56fe\u5173\u8054\u3002", "result": "\u5728University-1652\u548cSUES-200\u6570\u636e\u96c6\u4e0a\uff0cUniABG\u7684\u65e0\u76d1\u7763\u6027\u80fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5c06Satellite\u2192Drone\u7684AP\u5206\u522b\u63d0\u9ad8\u4e86+10.63%\u548c+16.73%\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "UniABG\u901a\u8fc7\u7ed3\u5408\u5bf9\u6297\u6027\u89c6\u56fe\u6865\u63a5\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11585", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11585", "abs": "https://arxiv.org/abs/2511.11585", "authors": ["Kabir Khan", "Manju Sarkar", "Anita Kar", "Suresh Ghosh"], "title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge", "comment": "37 pages, 8 figures", "summary": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.", "AI": {"tldr": "FedGen-Edge\u6846\u67b6\u901a\u8fc7\u4ec5\u8054\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6765\u89e3\u51b3\u8de8\u8bbe\u5907\u8054\u90a6\u8bbe\u7f6e\u4e2d\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9002\u5e94\u96be\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u751f\u6210\u6a21\u578b\uff08\u5982\u8bed\u8a00\u548c\u6269\u6563\u6a21\u578b\uff09\u5728\u8de8\u8bbe\u5907\u8054\u90a6\u73af\u5883\u4e2d\u8bad\u7ec3\u548c\u9002\u5e94\u56f0\u96be\uff0c\u56e0\u4e3a\u5b58\u5728\u8ba1\u7b97\u91cf\u5927\u3001\u901a\u4fe1\u5f00\u9500\u9ad8\u4ee5\u53ca\u6570\u636e\u548c\u7cfb\u7edf\u5f02\u6784\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faFedGen-Edge\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u5168\u5c40\u4e3b\u5e72\u6a21\u578b\u51bb\u7ed3\uff0c\u4ec5\u8054\u5408\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7684\u5ba2\u6237\u7aef\u9002\u914d\u5668\u3002\u5229\u7528\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u6280\u672f\u5c06\u5ba2\u6237\u7aef\u66f4\u65b0\u7ea6\u675f\u5728\u7d27\u51d1\u7684\u5b50\u7a7a\u95f4\u5185\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u91cf\u3001\u7a33\u5b9a\u805a\u5408\u5e76\u652f\u6301\u4e2a\u6027\u5316\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\uff08PTB\uff09\u548c\u56fe\u50cf\u751f\u6210\uff08CIFAR-10\uff09\u4efb\u52a1\u4e0a\uff0cFedGen-Edge\u7684\u56f0\u60d1\u5ea6/FID\u66f4\u4f4e\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002LoRA\u79e9\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u4e14\u672c\u5730\u8fed\u4ee3\u6b21\u6570\u4e0e\u5ba2\u6237\u7aef\u6f02\u79fb\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "FedGen-Edge\u4e3a\u5728\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u8d44\u6e90\u611f\u77e5\u548c\u4e2a\u6027\u5316\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u5207\u5b9e\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2511.13662", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13662", "abs": "https://arxiv.org/abs/2511.13662", "authors": ["Beno\u00eet Jeanson", "Mathieu Tanneau", "Simon Tindemans"], "title": "Scalable Iterative Algorithm for Solving Optimal Transmission Switching with De-energization", "comment": null, "summary": "Transmission System Operators routinely use transmission switching as a tool to manage congestion and ensure system security. Motivated by sub-transmission operations at RTE, this paper considers the Optimal Transmission Switching with De-energization (OTSD), which captures potential loss of connectivity (and therefore localized blackout) following loss of transmission elements. While directly relevant to real-life operations, this problem has received very little attention in the literature. The paper proposes a new mixed-integer linear programming formulation for OTSD that represents post-contingency loss of connectivity without requiring additional binary variables. This new formulation provides the foundation for a fast, iterative heuristic algorithm. Computational experiments confirms that state-of-the-art optimization solvers struggle to solve the extensive formulation of OTSD, often failing to find even trivial solutions within reasonable time. In contrast, numerical results demonstrate the efficiency of the proposed heuristic, which finds high-quality feasible solutions 100-1000x faster than using Gurobi.", "AI": {"tldr": "RTE\u7684\u8f93\u7535\u64cd\u4f5c\u80cc\u666f\u4e0b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6700\u4f18\u6f6e\u6d41\u5f00\u5173\uff08OTSD\uff09\u95ee\u9898\uff0c\u8003\u8651\u4e86\u6f6e\u6d41\u5143\u4ef6\u5931\u7075\u540e\u53ef\u80fd\u5bfc\u81f4\u7684\u8fde\u63a5\u4e22\u5931\uff08\u5c40\u90e8\u505c\u7535\uff09\u60c5\u51b5\u3002\u8be5\u95ee\u9898\u56e0\u5176\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u76f8\u5173\u6027\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u6587\u732e\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f15\u5165\u989d\u5916\u4e8c\u8fdb\u5236\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u793a\u4e8b\u4ef6\u540e\u7684\u8fde\u63a5\u4e22\u5931\u60c5\u51b5\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u8fed\u4ee3\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528Gurobi\u7b49\u4f18\u5316\u5668\u6c42\u89e3OTSD\u95ee\u9898\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u6240\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u80fd\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u53ef\u884c\u89e3\uff0c\u901f\u5ea6\u63d0\u5347100-1000\u500d\u3002", "motivation": "RTE\u7684\u6b21\u7ea7\u8f93\u7535\u64cd\u4f5c\u4e2d\uff0c\u9700\u8981\u8003\u8651\u6f6e\u6d41\u5143\u4ef6\u5931\u7075\u540e\u53ef\u80fd\u5bfc\u81f4\u7684\u8fde\u63a5\u4e22\u5931\uff08\u5c40\u90e8\u505c\u7535\uff09\u60c5\u51b5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6700\u4f18\u6f6e\u6d41\u5f00\u5173\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u8868\u793a\u4e8b\u4ef6\u540e\u7684\u8fde\u63a5\u4e22\u5931\u60c5\u51b5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u8fed\u4ee3\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u6c42\u89e3OTSD\u95ee\u9898\u65b9\u9762\u6bd4\u73b0\u6709\u7684\u4f18\u5316\u5668\uff08\u5982Gurobi\uff09\u6548\u7387\u66f4\u9ad8\uff0c\u901f\u5ea6\u63d0\u5347100-1000\u500d\uff0c\u5e76\u4e14\u80fd\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u53ef\u884c\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684OTSD\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6a21\u578b\u548c\u8fed\u4ee3\u542f\u53d1\u5f0f\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5b9e\u9645\u8f93\u7535\u64cd\u4f5c\u4e2d\u7684\u6700\u4f18\u6f6e\u6d41\u5f00\u5173\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2511.12941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12941", "abs": "https://arxiv.org/abs/2511.12941", "authors": ["Chunyong Hu", "Qi Luo", "Jianyun Xu", "Song Wang", "Qiang Li", "Sheng Yang"], "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving", "comment": null, "summary": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.", "AI": {"tldr": "GUIDE\u4f7f\u75283D\u9ad8\u65af\u8fdb\u884c\u5b9e\u4f8b\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u5e76\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u75283D\u8fb9\u754c\u6846\u8868\u793a\u969c\u788d\u7269\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faGUIDE\u6846\u67b6\uff0c\u4f7f\u75283D\u9ad8\u65af\u8fdb\u884c\u5b9e\u4f8b\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u5e76\u5177\u6709\u8ddf\u8e2a\u80fd\u529b\u3002\u91c7\u7528\u7a00\u758f\u8868\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u9ad8\u65af\u5230\u4f53\u7d20\u7684\u6e32\u67d3\uff08Gaussian-to-Voxel Splatting\uff09\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u522b\u5360\u7528\u6570\u636e\uff0c\u907f\u514d\u4e86\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u4f8b\u5360\u7528mAP\u8fbe\u523021.61\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8650%\uff0c\u5e76\u5177\u6709\u5177\u6709\u7ade\u4e89\u529b\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "GUIDE\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.13173", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13173", "abs": "https://arxiv.org/abs/2511.13173", "authors": ["Ze-Zhou Zhang", "Hong-Gang Luo", "Wei Wu"], "title": "Quantum Mpemba Effect Induced by Non-Markovian Exceptional Point", "comment": null, "summary": "Quantum Mpemba effect describes an anomalous phenomenon of accelerated relaxation which is of fundamental interest in the field of nonequilibrium thermodynamics. Conventional theories on this phenomenon strongly rely on the Born-Markovian approximation, but this effect is not well understood in non-Markovian regimes. By investigating the relaxation process within the framework of a general non-Markovian dynamics, we propose a mechanism of realizing the quantum Mpemba effect via non-Markovian exceptional points. We verify the feasibility of this mechanism in a dissipative quantum harmonic oscillator model. Providing a new insight into the interesting non-equilibrium dynamics phenomenon, our work paves a way to accelerate the transfer of energy and information in quantum systems.", "AI": {"tldr": "\u9a6c\u6f58\u5df4\u6548\u5e94\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u60c5\u51b5\u4e0b\u4ecd\u53ef\u5b9e\u73b0\uff0c\u5e76\u53ef\u80fd\u52a0\u901f\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u80fd\u91cf\u548c\u4fe1\u606f\u4f20\u8f93\u3002", "motivation": "\u9a6c\u6f58\u5df4\u6548\u5e94\u5728\u975e\u5e73\u8861\u70ed\u529b\u5b66\u9886\u57df\u5177\u6709\u57fa\u672c\u610f\u4e49\uff0c\u4f46\u5176\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u4f53\u7cfb\u4e0b\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u975e\u9a6c\u5c14\u53ef\u592b\u5947\u5f02\u70b9\u5b9e\u73b0\u91cf\u5b50\u9a6c\u6f58\u5df4\u6548\u5e94\u7684\u673a\u5236\uff0c\u5e76\u5728\u8017\u6563\u91cf\u5b50\u8c10\u632f\u5b50\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u673a\u5236\u5728\u8017\u6563\u91cf\u5b50\u8c10\u632f\u5b50\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u91cf\u5b50\u9a6c\u6f58\u5df4\u6548\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u52a0\u901f\u91cf\u5b50\u7cfb\u7edf\u7684\u80fd\u91cf\u548c\u4fe1\u606f\u4f20\u8f93\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12056", "categories": ["cs.CV", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12056", "abs": "https://arxiv.org/abs/2511.12056", "authors": ["Sijie Wang", "Qiang Wang", "Shaohuai Shi"], "title": "PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling", "comment": null, "summary": "Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPipeDiT\u7684\u65b0\u578b\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5e76\u884c\uff08PipeSP\uff09\u3001\u89e3\u8026\u6269\u6563\u4e0eVAE\u6a21\u5757\uff08DeDiVAE\uff09\u4ee5\u53ca\u6ce8\u610f\u529b\u534f\u540c\u5904\u7406\uff08Aco\uff09\u7b49\u6280\u672f\uff0c\u663e\u8457\u52a0\u901f\u4e86\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\uff0c\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e861.06x\u81f34.02x\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u548c\u5185\u5b58\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u7528\u4e8e\u5e8f\u5217\u5e76\u884c\u7684\u6d41\u6c34\u7ebf\u7b97\u6cd5PipeSP\uff0c\u5b9e\u73b0\u4e86\u6f5c\u53d8\u91cf\u751f\u6210\u548c\u591aGPU\u901a\u4fe1\u7684\u6d41\u6c34\u7ebf\u5316\uff1b\u63d0\u51fa\u4e86DeDiVAE\uff0c\u5c06\u6269\u6563\u6a21\u5757\u548cVAE\u6a21\u5757\u5206\u79bb\u5230\u4e0d\u540c\u7684GPU\u7ec4\u5e76\u8fdb\u884c\u6d41\u6c34\u7ebf\u6267\u884c\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u548c\u5ef6\u8fdf\uff1b\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u534f\u540c\u5904\u7406\uff08Aco\uff09\u65b9\u6cd5\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316VAE\u7ec4\u7684GPU\u8d44\u6e90\u5229\u7528\u5e76\u964d\u4f4e\u6574\u4f53\u5ef6\u8fdf\u3002", "result": "\u5c06PipeDiT\u96c6\u6210\u5230OpenSoraPlan\u548cHunyuanVideo\u4e2d\uff0c\u57288-GPU\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u79cd\u5206\u8fa8\u7387\u548c\u65f6\u95f4\u6b65\u914d\u7f6e\u4e0b\uff0cPipeDiT\u76f8\u6bd4OpenSoraPlan\u548cHunyuanVideo\u5b9e\u73b0\u4e861.06x\u81f34.02x\u7684\u52a0\u901f\u3002", "conclusion": "PipeDiT\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u6c34\u7ebf\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86DiT\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2511.11714", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11714", "abs": "https://arxiv.org/abs/2511.11714", "authors": ["Daniel M. Jimenez-Gutierrez", "Enrique Zuazua", "Joaquin Del Rio", "Oleksii Sliusarenko", "Xabi Uribe-Etxebarria"], "title": "Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data", "comment": null, "summary": "Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5e73\u53f0\uff0c\u4f7f\u591a\u5bb6\u533b\u9662\u80fd\u591f\u534f\u4f5c\u8bad\u7ec3\u80ba\u708e\u68c0\u6d4b\u7684\u80f8\u90e8X\u5149\uff08CXR\uff09\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u548c\u5c31\u5730\u6027\uff0c\u5e76\u5728\u4e0d\u4f20\u8f93\u4efb\u4f55\u60a3\u8005CXR\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548cROC-AUC\u3002", "motivation": "\u65e9\u671f\u51c6\u786e\u5730\u4eceCXR\u68c0\u6d4b\u80ba\u708e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u5206\u6563\u3001\u533b\u9662\u95f4\u5dee\u5f02\u5927\u3001\u9690\u79c1\u6cd5\u89c4\u4e25\u683c\u4ee5\u53ca\u6570\u636e\u4f20\u8f93\u6210\u672c\u9ad8\u7b49\u9650\u5236\uff0cAI\u6a21\u578b\u5f00\u53d1\u9762\u4e34\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528Sherpa.ai FL\u5e73\u53f0\uff0c\u6a21\u62df\u8de8\u533b\u9662\u534f\u4f5c\uff0c\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CXR\u5206\u7c7b\u5668\uff0c\u4ee5\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u53d8\u5f02\u6027\u95ee\u9898\u3002", "result": "\u4e0e\u5355\u533b\u9662\u6a21\u578b\u76f8\u6bd4\uff0c\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u7387\u548cROC-AUC\u65b9\u9762\u5206\u522b\u53d6\u5f97\u4e860.900\u548c0.966\u7684\u6210\u7ee9\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u4e8647.5%\u548c50.0%\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u8de8\u533b\u7597\u7f51\u7edc\u7684\u9ad8\u6027\u80fd\u3001\u6cdb\u5316\u6027\u5f3a\u3001\u5b89\u5168\u4e14\u79c1\u5bc6\u7684\u80ba\u708e\u68c0\u6d4b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u7a00\u758f\u7684\u7f55\u89c1\u75c5\u9886\u57df\uff0c\u80fd\u591f\u52a0\u901f\u8bca\u65ad\u548c\u6cbb\u7597\u5f00\u53d1\u3002"}}
{"id": "2511.13690", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13690", "abs": "https://arxiv.org/abs/2511.13690", "authors": ["Shyam Kamal", "Sunidhi Pandey", "Thach Ngoc Dinh"], "title": "Novel Stability Criteria for Discrete and Hybrid Systems via Ramanujan Inner Products", "comment": "6 pages, 2 figures", "summary": "This paper introduces a Ramanujan inner product and its corresponding norm, establishing a novel framework for the stability analysis of hybrid and discrete-time systems as an alternative to traditional Euclidean metrics. We establish new $\u03b5$-$\u03b4$ stability conditions that utilize the unique properties of Ramanujan summations and their relationship with number-theoretic concepts. The proposed approach provides enhanced robustness guarantees and reveals fundamental connections between system stability and arithmetic properties of the system dynamics. Theoretical results are rigorously proven, and simulation results on numerical examples are presented to validate the efficacy of the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u53ca\u5176\u8303\u6570\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u65b0\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u6df7\u5408\u548c\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u6b27\u6c0f\u5ea6\u91cf\u66f4\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u6b27\u6c0f\u5ea6\u91cf\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e0e\u6570\u8bba\u6027\u8d28\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u5e76\u5e94\u7528\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u53ca\u5176\u8303\u6570\u6765\u5efa\u7acb\u65b0\u7684\u03b5-\u03b4\u7a33\u5b9a\u6027\u6761\u4ef6\uff0c\u5e76\u5229\u7528\u62c9\u9a6c\u52aa\u91d1\u6c42\u548c\u53ca\u5176\u4e0e\u6570\u8bba\u6982\u5ff5\u7684\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6570\u503c\u7b97\u4f8b\u4eff\u771f\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u5e76\u63ed\u793a\u4e86\u7cfb\u7edf\u52a8\u529b\u5b66\u4e2d\u7684\u7b97\u672f\u6027\u8d28\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2511.12972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12972", "abs": "https://arxiv.org/abs/2511.12972", "authors": ["Siddarth Narasimhan", "Matthew Lisondra", "Haitong Wang", "Goldie Nejat"], "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models", "comment": "Project Page: https://splat-search.github.io/", "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.", "AI": {"tldr": "SplatSearch \u662f\u4e00\u79cd\u5229\u7528\u7a00\u758f\u89c6\u56fe 3D \u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u6765\u89e3\u51b3\u5b9e\u4f8b\u56fe\u50cf\u5bfc\u822a\u95ee\u9898\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u6e32\u67d3\u591a\u89c6\u89d2\u56fe\u50cf\u5e76\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u6765\u8865\u5168\u7f3a\u5931\u533a\u57df\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u7279\u5f81\u5339\u914d\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u524d\u6cbf\u63a2\u7d22\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u4ec5\u51ed\u5355\u5f20\u76ee\u6807\u56fe\u50cf\u5728\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u4e2d\u641c\u7d22\u7279\u5b9a\u76ee\u6807\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "SplatSearch \u67b6\u6784\u5229\u7528\u7a00\u758f\u89c6\u56fe 3D \u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\uff0c\u6e32\u67d3\u5019\u9009\u7269\u4f53\u5468\u56f4\u7684\u591a\u4e2a\u89c6\u89d2\uff0c\u5e76\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8865\u5168\u56fe\u50cf\u7f3a\u5931\u533a\u57df\u4ee5\u8fdb\u884c\u7279\u5f81\u5339\u914d\u3002\u5f15\u5165\u4e86\u65b0\u7684\u524d\u6cbf\u63a2\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u6210\u89c6\u89d2\u548c\u76ee\u6807\u56fe\u50cf\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u8bc4\u4f30\u524d\u6cbf\u4f4d\u7f6e\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSplatSearch \u5728\u6210\u529f\u7387\u548c\u6210\u529f\u8def\u5f84\u957f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SplatSearch \u7684\u8bbe\u8ba1\u9009\u62e9\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u4f8b\u56fe\u50cf\u5bfc\u822a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13192", "categories": ["quant-ph", "cs.IT", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2511.13192", "abs": "https://arxiv.org/abs/2511.13192", "authors": ["Yantong Liu", "Junjie Wu", "Lingling Lao"], "title": "The correlated matching decoder for the 4.8.8 color code", "comment": "11 pages, 11 figures", "summary": "Color codes present distinct advantages for fault-tolerant quantum computing, such as high encoding rates and the transversal implementation of Clifford gates. However, existing matching-based decoders for the color codes such as the restricted decoder (Kubica and Delfosse, 2023), suffer from limited decoding performance. Inspired by the global decoding insight of the unified decoder (Benhemou et al., 2023), this paper introduces a correlated decoder for the 4.8.8 color code, which improves upon the conventional restricted decoder by leveraging correlations between restricted lattices, and is derived by mapping the correlated matching decoder for the surface code onto the color code lattice. Analytical and numerical results show that the correlated decoder achieves higher thresholds than the restricted and unified decoders, while matching the performance of the unified decoder at very low physical error rates. Under the code capacity and phenomenological noise models, the estimated thresholds for the color code against bit-flip error are 10.38% and 3.13%, respectively. Furthermore, by applying the surface-color code mapping, the thresholds of 16.62% and 3.52% are obtained for the surface code against depolarizing noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e4.8.8\u8272\u7801\u7684\u5173\u8054\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5229\u7528\u53d7\u9650\u683c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u4e86\u89e3\u7801\u6027\u80fd\uff0c\u5e76\u5728\u7406\u8bba\u548c\u6570\u503c\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u89e3\u7801\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5339\u914d\u7684\u8272\u7801\u89e3\u7801\u5668\uff08\u5982\u53d7\u9650\u89e3\u7801\u5668\uff09\u6027\u80fd\u6709\u9650\uff0c\u4f46\u8272\u7801\u5177\u6709\u9ad8\u7f16\u7801\u7387\u548c\u6a2a\u5411\u5b9e\u73b0\u514b\u5229\u798f\u5fb7\u95e8\u7b49\u4f18\u70b9\u3002\u53d7\u7edf\u4e00\u89e3\u7801\u5668\u5168\u5c40\u89e3\u7801\u601d\u60f3\u7684\u542f\u53d1\uff0c\u9700\u8981\u6539\u8fdb\u89e3\u7801\u6027\u80fd\u3002", "method": "\u5c06\u7528\u4e8e\u8868\u9762\u7801\u7684\u5173\u8054\u5339\u914d\u89e3\u7801\u5668\u6620\u5c04\u5230\u8272\u7801\u683c\u4e0a\uff0c\u4ece\u800c\u5f97\u5230\u8272\u7801\u7684\u5173\u8054\u89e3\u7801\u5668\uff0c\u8be5\u89e3\u7801\u5668\u5229\u7528\u53d7\u9650\u683c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5173\u8054\u89e3\u7801\u5668\u5728\u7406\u8bba\u9608\u503c\u4e0a\u4f18\u4e8e\u53d7\u9650\u89e3\u7801\u5668\u548c\u7edf\u4e00\u89e3\u7801\u5668\uff0c\u5e76\u4e14\u5728\u6781\u4f4e\u7684\u7269\u7406\u9519\u8bef\u7387\u4e0b\u80fd\u8fbe\u5230\u4e0e\u7edf\u4e00\u89e3\u7801\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5728\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u6a21\u578b\u4e0b\uff0c\u8272\u7801\u7684\u9608\u503c\u4f30\u8ba1\u4e3a10.38%\uff0c\u5728\u73b0\u8c61\u5b66\u566a\u58f0\u6a21\u578b\u4e0b\u4e3a3.13%\u3002\u901a\u8fc7\u8868\u9762-\u8272\u7801\u6620\u5c04\uff0c\u8868\u9762\u7801\u5728\u53bb\u6781\u5316\u566a\u58f0\u4e0b\u7684\u9608\u503c\u5206\u522b\u4e3a16.62%\u548c3.52%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5173\u8054\u89e3\u7801\u5668\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u8272\u7801\u7684\u89e3\u7801\u6027\u80fd\uff0c\u514b\u670d\u4e86\u73b0\u6709\u89e3\u7801\u5668\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u7406\u8bba\u548c\u6570\u503c\u6a21\u62df\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2511.12061", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12061", "abs": "https://arxiv.org/abs/2511.12061", "authors": ["Zhichen Lai", "Hua Lu", "Huan Li", "Jialiang Li", "Christian S. Jensen"], "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity", "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper", "summary": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.", "AI": {"tldr": "MovSemCL\u662f\u4e00\u4e2a\u7528\u4e8e\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u79fb\u52a8\u8bed\u4e49\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u65b9\u6cd5\u5728\u8f68\u8ff9\u8bed\u4e49\u548c\u5c42\u7ea7\u5efa\u6a21\u3001\u8ba1\u7b97\u6210\u672c\u4ee5\u53ca\u6570\u636e\u589e\u5f3a\u7684\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "MovSemCL\u5c06\u539f\u59cbGPS\u8f68\u8ff9\u8f6c\u6362\u4e3a\u79fb\u52a8\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u5206\u5757\u3002\u5229\u7528\u5185\u90e8\u548c\u8de8\u5757\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u5c40\u90e8\u548c\u5168\u5c40\u8f68\u8ff9\u6a21\u5f0f\u8fdb\u884c\u7f16\u7801\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5c42\u7ea7\u8868\u793a\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u91c7\u7528\u66f2\u7387\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4fdd\u7559\u5173\u952e\u8f6c\u5f2f\u548c\u4ea4\u53c9\u8def\u53e3\u7b49\u4fe1\u606f\u4e30\u5bcc\u7684\u7247\u6bb5\uff0c\u5e76\u63a9\u7801\u5197\u4f59\u7247\u6bb5\uff0c\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u589e\u5f3a\u89c6\u56fe\u3002", "result": "MovSemCL\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u5728\u76f8\u4f3c\u6027\u641c\u7d22\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u6392\u540d\u63a5\u8fd1\u7406\u60f3\u503c1\uff0c\u5728\u542f\u53d1\u5f0f\u8fd1\u4f3c\u65b9\u9762\u6709\u9ad8\u8fbe20.3%\u7684\u63d0\u5347\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u9ad8\u8fbe43.4%\u3002", "conclusion": "MovSemCL\u901a\u8fc7\u5f15\u5165\u79fb\u52a8\u8bed\u4e49\u3001\u5c42\u7ea7\u8868\u793a\u548c\u66f2\u7387\u5f15\u5bfc\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6548\u7387\u6539\u8fdb\u3002"}}
{"id": "2511.13698", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13698", "abs": "https://arxiv.org/abs/2511.13698", "authors": ["Hampei Sasahara", "Tatsuya Yamada", "Jun-ichi Imura", "Henrik Sandberg"], "title": "Resilient Distribution Network Planning against Dynamic Malicious Power Injection Attacks", "comment": "Accepted at IEEE Transactions on Control of Network Systems", "summary": "Active distribution networks facilitating bidirectional power exchange with renewable energy resources are susceptible to cyberattacks due to integration of a diverse array of cyber components. This study introduces a grid-level defense strategy aimed at enhancing attack resiliency based on distribution network planning. Our proposed framework imposes a security requirement into existing planning methodologies, ensuring that voltage deviation from its rated value remains within a tolerable range against dynamically and maliciously injected power at end-user nodes. Unfortunately, the formulated problem in its original form is intractable because it is an infinite-dimensional bi-level optimization problem over a function space. To address this complexity, we develop an equivalent transformation into a tractable form as mixed-integer linear program leveraging linear dynamical system theory and graph theory. Notably, our investigation reveals that the severity of potential attacks hinges solely on the cumulative reactances over the path from the substation to the targeted node, thereby reducing the problem to a finite-dimensional problem. Further, the bi-level optimization problem is reduced to a single-level optimization problem by using a technique utilized in solving the shortest path problem. Through extensive numerical simulations conducted on a 54-node distribution network benchmark, our proposed methodology exhibits a noteworthy 29.3% enhancement in the resiliency, with a mere 2.1% uptick in the economic cost.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u914d\u7535\u7f51\u89c4\u5212\u7684\u7535\u7f51\u7ea7\u9632\u5fa1\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u7535\u7f51\u62b5\u5fa1\u7f51\u7edc\u653b\u51fb\u7684\u80fd\u529b\u3002\u8be5\u7b56\u7565\u5c06\u5b89\u5168\u9700\u6c42\u7eb3\u5165\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\uff0c\u786e\u4fdd\u5728\u9762\u5bf9\u52a8\u6001\u6076\u610f\u6ce8\u5165\u529f\u7387\u65f6\uff0c\u7535\u538b\u504f\u5dee\u4fdd\u6301\u5728\u53ef\u5bb9\u5fcd\u8303\u56f4\u5185\u3002\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u5e76\u5229\u7528\u56fe\u8bba\u548c\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\uff0c\u7814\u7a76\u53d1\u73b0\u653b\u51fb\u7684\u4e25\u91cd\u6027\u4ec5\u53d6\u51b3\u4e8e\u8def\u5f84\u4e0a\u7684\u7d2f\u79ef\u7535\u6297\uff0c\u4ece\u800c\u5c06\u95ee\u9898\u7b80\u5316\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u7535\u7f51\u97e7\u6027\u7684\u540c\u65f6\uff0c\u7ecf\u6d4e\u6210\u672c\u4ec5\u7565\u6709\u589e\u52a0\u3002", "motivation": "\u7531\u4e8e\u96c6\u6210\u4e86\u591a\u6837\u5316\u7684\u7f51\u7edc\u7ec4\u4ef6\uff0c\u652f\u6301\u53ef\u518d\u751f\u80fd\u6e90\u53cc\u5411\u7535\u529b\u4ea4\u6362\u7684\u4e3b\u52a8\u914d\u7535\u7f51\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u7535\u7f51\u62b5\u5fa1\u7f51\u7edc\u653b\u51fb\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5b89\u5168\u9700\u6c42\u7eb3\u5165\u73b0\u6709\u914d\u7535\u7f51\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u7535\u538b\u504f\u5dee\u5728\u53ef\u5bb9\u5fcd\u8303\u56f4\u5185\u3002\u901a\u8fc7\u5229\u7528\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u548c\u56fe\u8bba\uff0c\u5c06\u65e0\u9650\u7ef4\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u653b\u51fb\u7684\u4e25\u91cd\u6027\u4ec5\u53d6\u51b3\u4e8e\u8def\u5f84\u4e0a\u7684\u7d2f\u79ef\u7535\u6297\uff0c\u4ece\u800c\u5c06\u95ee\u9898\u7b80\u5316\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u6280\u672f\u5c06\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u5c42\u4f18\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5728 54 \u8282\u70b9\u914d\u7535\u7f51\u7edc\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u6570\u503c\u6a21\u62df\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ecf\u6d4e\u6210\u672c\u4ec5\u589e\u52a0 2.1% \u7684\u60c5\u51b5\u4e0b\uff0c\u97e7\u6027\u663e\u8457\u63d0\u9ad8\u4e86 29.3%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u914d\u7535\u7f51\u89c4\u5212\u7684\u7535\u7f51\u7ea7\u9632\u5fa1\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7535\u7f51\u62b5\u5fa1\u7f51\u7edc\u653b\u51fb\u7684\u80fd\u529b\uff0c\u5e76\u5728\u7ecf\u6d4e\u6210\u672c\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u97e7\u6027\u63d0\u5347\u3002"}}
{"id": "2511.12984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12984", "abs": "https://arxiv.org/abs/2511.12984", "authors": ["Miryeong Park", "Dongjin Cho", "Sanghyun Kim", "Younggun Cho"], "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner", "comment": "Accepted in International Conference on Space Robotics 2025", "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5b89\u5168\u8def\u5f84\u751f\u6210\u3001\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u66f4\u65b0\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63a2\u7d22\u7b56\u7565\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u884c\u661f\u63a2\u7d22\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u9ad8\u7a0b\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5730\u5f62\uff08\u5982\u9668\u77f3\u5751\u9644\u8fd1\uff09\u7684\u9ad8\u7a0b\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3001\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u7684\u63a2\u7d22\u7b56\u7565\u4ee5\u53ca\u9ad8\u7a0b\u4e0d\u786e\u5b9a\u6027\u5bf9\u5bfc\u822a\u5b89\u5168\u548c\u5730\u56fe\u8d28\u91cf\u7684\u5f71\u54cd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u9ad8\u7a0b\u4f30\u8ba1\uff0c\u751f\u6210\u5730\u5f62\u53ef\u901a\u884c\u6027\u548c\u7f6e\u4fe1\u5ea6\u5f97\u5206\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u57fa\u4e8e\u56fe\u7684\u63a2\u7d22\u89c4\u5212\u5668\uff08GBP\uff09\u4e2d\uff0c\u4f18\u5148\u63a2\u7d22\u53ef\u901a\u884c\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6708\u7403\u5b9e\u9a8c\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u6bd4\u7387\u6307\u6807\uff0c\u4e0e\u57fa\u7ebfGBP\u76f8\u6bd4\uff0c\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u4e8669%\uff1b\u5728\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86100%\uff0c\u800c\u57fa\u7ebfGBP\u4e3a0%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u63a2\u7d22\u5b89\u5168\u6027\uff0c\u589e\u5f3a\u4e86\u5730\u56fe\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9ad8\u7a0b\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u3002"}}
{"id": "2511.13194", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13194", "abs": "https://arxiv.org/abs/2511.13194", "authors": ["Jiangwei Long", "Yizhi Li", "Jianxin Zhong", "Lijun Meng"], "title": "Topological quantum compilation for non-semisimple Ising anyons via monte carlo simulations", "comment": null, "summary": "We present a systematic numerical construction of a universal quantum gate set for topological quantum computation based on the non-semisimple Ising anyons model. Using the elementary braiding matrices (EBMs) of this model by the Monte Carlo-enhanced Solovay-Kitaev algorithm (MC-enhanced SKA), we achieve high-fidelity approximations of standard one-qubit gates (Hadamard H-gate and phase T-gate). Remarkably, a recursion level of just three suffices to meet the fidelity requirements for fault-tolerant quantum computation. Our numerical results demonstrate that for the parameter \u03b1 /in (2, 2.031], a single braiding operation can approximate the local equivalence class [CNOT] with high precision and great unitary measurement. Specifically, at \u03b1 = 2.031, 2.047, and 2.063, we successfully construct a universal gate set {H-gate, T-gate, CNOT-gate} with high accuracy. This work establishes a new pathway towards universal quantum computation using non-semisimple Ising anyons.", "AI": {"tldr": "\u5229\u7528\u975e\u534a\u5355\u6027\u4f0a\u8f9b\u4efb\u610f\u5b50\u6a21\u578b\uff0c\u901a\u8fc7MC-\u589e\u5f3aSKA\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684H\u95e8\u548cT\u95e8\u8fd1\u4f3c\uff0c\u4ec5\u9700\u4e09\u5c42\u9012\u5f52\u5373\u53ef\u6ee1\u8db3\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u8981\u6c42\u3002\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\uff0c\u5355\u4e2a\u7f16\u7ec7\u64cd\u4f5c\u53ef\u9ad8\u7cbe\u5ea6\u8fd1\u4f3cCNOT\u95e8\uff0c\u6700\u7ec8\u6784\u5efa\u4e86{H, T, CNOT}\u901a\u7528\u95e8\u96c6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u975e\u534a\u5355\u6027\u4f0a\u8f9b\u4efb\u610f\u5b50\u6a21\u578b\u7684\u901a\u7528\u91cf\u5b50\u95e8\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u589e\u5f3a\u7684Solovay-Kitaev\u7b97\u6cd5\uff08MC-enhanced SKA\uff09\u548c\u975e\u534a\u5355\u6027\u4f0a\u8f9b\u4efb\u610f\u5b50\u6a21\u578b\u7684\u521d\u7b49\u7f16\u7ec7\u77e9\u9635\uff08EBMs\uff09\uff0c\u5bf9\u6807\u51c6\u5355\u91cf\u5b50\u6bd4\u7279\u95e8\uff08H\u95e8\u548cT\u95e8\uff09\u8fdb\u884c\u9ad8\u4fdd\u771f\u5ea6\u8fd1\u4f3c\u3002", "result": "\u5728\u9012\u5f52\u5c42\u6570\u4e3a\u4e09\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6ee1\u8db3\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u4fdd\u771f\u5ea6\u8981\u6c42\u7684\u9ad8\u7cbe\u5ea6H\u95e8\u548cT\u95e8\u8fd1\u4f3c\u3002\u5728\u7279\u5b9a\u53c2\u6570 \u03b1 \u2208 [2, 2.031] \u4e0b\uff0c\u5355\u4e2a\u7f16\u7ec7\u64cd\u4f5c\u53ef\u9ad8\u7cbe\u5ea6\u8fd1\u4f3cCNOT\u95e8\u3002\u5728 \u03b1 = 2.031, 2.047, 2.063 \u65f6\uff0c\u6210\u529f\u6784\u5efa\u4e86\u9ad8\u7cbe\u5ea6\u7684{H, T, CNOT}\u901a\u7528\u95e8\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4f7f\u7528\u975e\u534a\u5355\u6027\u4f0a\u8f9b\u4efb\u610f\u5b50\u5b9e\u73b0\u901a\u7528\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6784\u5efa\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u95e8\u96c6\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12066", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12066", "abs": "https://arxiv.org/abs/2511.12066", "authors": ["Jialang Lu", "Shuning Sun", "Pu Wang", "Chen Wu", "Feng Gao", "Lina Gong", "Dianjie Lu", "Guijuan Zhang", "Zhuoran Zheng"], "title": "DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal", "comment": "11 pages, 9 figures", "summary": "Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel\", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.", "AI": {"tldr": "DCA-LUT \u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u53bb\u9664\u7d2b\u8272\u6761\u7eb9\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7 CA-CT \u6a21\u5757\u5b66\u4e60\u56fe\u50cf\u81ea\u9002\u5e94\u989c\u8272\u7a7a\u95f4\uff0c\u5206\u79bb\u5e76\u5b66\u4e60\u201c\u7d2b\u8272\u6761\u7eb9\u901a\u9053\u201d\uff0c\u7136\u540e\u4f7f\u7528\u5b66\u4e60\u5230\u7684 5D LUT \u8fdb\u884c\u989c\u8272\u6821\u6b63\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684 APO \u955c\u5934\u786c\u4ef6\u548c\u624b\u5de5\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u76f8\u673a\u955c\u5934\u4e2d\u7531\u7eb5\u5411\u8272\u5dee (LCA) \u5f15\u8d77\u7684\u7d2b\u8272\u6761\u7eb9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8272\u5dee\u611f\u77e5\u5750\u6807\u53d8\u6362 (CA-CT) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5b66\u4e60\u56fe\u50cf\u81ea\u9002\u5e94\u989c\u8272\u7a7a\u95f4\u4ee5\u5206\u79bb\u548c\u9694\u79bb\u6761\u7eb9\uff0c\u7136\u540e\u5b66\u4e60\u4e00\u4e2a\u7cbe\u786e\u7684\u201c\u7d2b\u8272\u6761\u7eb9\u901a\u9053\u201d\u6765\u6307\u5bfc\u4eae\u5ea6\u901a\u9053\u7684\u7cbe\u786e\u6062\u590d\uff0c\u6700\u540e\u901a\u8fc7\u5b66\u4e60\u5230\u7684 5D LUT \u8fdb\u884c\u989c\u8272\u6821\u6b63\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53bb\u9664\u7d2b\u8272\u6761\u7eb9\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DCA-LUT \u6846\u67b6\u901a\u8fc7\u65b0\u9896\u7684 CA-CT \u6a21\u5757\u548c 5D LUT \u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u989c\u8272\u6620\u5c04\uff0c\u80fd\u591f\u6709\u6548\u5730\u53bb\u9664\u7d2b\u8272\u6761\u7eb9\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11722", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11722", "abs": "https://arxiv.org/abs/2511.11722", "authors": ["Soumyendu Sarkar", "Antonio Guillen-Perez", "Zachariah J Carmichael", "Avisek Naug", "Refik Mert Cam", "Vineet Gundecha", "Ashwin Ramesh Babu", "Sahand Ghorbanpour", "Ricardo Luna Gutierrez"], "title": "Fast 3D Surrogate Modeling for Data Center Thermal Management", "comment": "Submitted to AAAI 2026 Conference", "summary": "Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\\%) and reduced carbon footprint.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u6570\u636e\u4e2d\u5fc3\u6e29\u5ea6\u7684\u5b9e\u65f6\u9884\u6d4b\uff0c\u53ef\u5c06\u80fd\u8017\u964d\u4f4e7%\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u548c\u78b3\u6392\u653e\u5bf9\u53ef\u6301\u7eed\u6027\u548c\u8fd0\u8425\u6548\u7387\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u5b9e\u65f6\u6e29\u5ea6\u9884\u6d4b\u6765\u5b9e\u73b0\u8282\u80fd\u548c\u4f18\u5316\u3002\u73b0\u6709\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u4ee3\u7406\u5efa\u6a21\u6846\u67b6\uff0c\u76f4\u63a5\u5904\u7406\u6570\u636e\u4e2d\u5fc3\u76843D\u4f53\u7d20\u8868\u793a\uff0c\u6574\u5408\u670d\u52a1\u5668\u8d1f\u8f7d\u3001\u98ce\u6247\u901f\u5ea6\u548cHVAC\u8bbe\u5b9a\u70b9\u3002\u8bc4\u4f30\u4e863D CNN U-Net\u30013D\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c3D\u89c6\u89c9\u8f6c\u6362\u5668\u7b49\u591a\u79cd\u67b6\u6784\u3002", "result": "\u4ee3\u7406\u6a21\u578b\u5728\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe20,000\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff08\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u81f3\u6570\u767e\u6beb\u79d2\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u4f30\u8ba1\u70ed\u70b9\u548c\u6e29\u5ea6\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u51b7\u5374\u63a7\u5236\u548c\u5de5\u4f5c\u8d1f\u8f7d\u91cd\u65b0\u5206\u914d\uff0c\u6709\u6548\u964d\u4f4e\u80fd\u8017\uff087%\uff09\u548c\u78b3\u6392\u653e\u3002"}}
{"id": "2511.13042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13042", "abs": "https://arxiv.org/abs/2511.13042", "authors": ["Yong Li", "Hui Cheng"], "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation", "comment": null, "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPP\u7684A*\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u51cf\u5c11\u8def\u5f84\u957f\u5ea6\u548c\u8f6c\u5411\u6b21\u6570\uff0c\u63d0\u9ad8\u8def\u5f84\u5e73\u6ed1\u5ea6\u3002", "motivation": "A*\u7b49\u56fe\u641c\u7d22\u89c4\u5212\u5668\u751f\u6210\u7684\u8def\u5f84\u901a\u5e38\u4e0d\u662f\u6700\u77ed\u7684\uff0c\u5e76\u4e14\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\uff08\u952f\u9f7f\u5f62\uff09\uff0c\u4e0d\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u540e\u5904\u7406\u7b97\u6cd5\u6765\u4f18\u5316\u8fd9\u4e9b\u8def\u5f84\u3002", "method": "APP\u7b97\u6cd5\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1. \u53cc\u5411\u9876\u70b9\u7ea6\u7b80\u7b97\u6cd5\uff0c\u91c7\u7528\u524d\u5411\u548c\u540e\u5411\u7ea6\u7b80\u4ee5\u53ca\u5168\u9762\u7684\u6377\u5f84\u7b56\u7565\u6765\u7f29\u77ed\u8def\u5f84\u5e76\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\u30022. \u8fed\u4ee3\u8def\u5f84\u6270\u52a8\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c40\u90e8\u51cf\u5c11\u8f6c\u5411\u6b21\u6570\u5e76\u63d0\u9ad8\u8def\u5f84\u5e73\u6ed1\u5ea6\u3002\u8be5\u7b97\u6cd5\u57fa\u4e8e\u6210\u672c\u5730\u56fe\u3002", "result": "APP\u7b97\u6cd5\u5728\u89c4\u5212\u65f6\u95f4\u3001\u8def\u5f84\u957f\u5ea6\u548c\u4e0d\u5fc5\u8981\u8f6c\u5411\u6b21\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u5730\u5bfc\u822a\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "APP\u662f\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316A*\u7b49\u56fe\u641c\u7d22\u7b97\u6cd5\u751f\u6210\u7684\u8def\u5f84\uff0c\u63d0\u9ad8\u8def\u5f84\u8d28\u91cf\u548c\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6548\u7387\u4e0e\u5e73\u7a33\u6027\u3002"}}
{"id": "2511.13231", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13231", "abs": "https://arxiv.org/abs/2511.13231", "authors": ["Rion Shimazu", "Suguru Endo", "Shigeo Hakkaku", "Shinobu Saito"], "title": "Data-driven adaptive quantum error mitigation for probability distribution", "comment": "8 pages", "summary": "Quantum error mitigation (QEM) has been proposed as a class of hardware-friendly error suppression techniques. While QEM has been primarily studied for mitigating errors in the estimation of expectation values of observables, recent works have explored its application to estimating noiseless probability distributions. In this work, we propose two protocols to improve the accuracy of QEM for probability distributions, inspired by techniques in software engineering. The first is the N-version programming method, which compares probability distributions obtained via different QEM strategies and excludes the outlier distribution, certifying the feasibility of the error-mitigated distributions. The second is a consistency-based method for selecting an appropriate extrapolation strategy. Specifically, we prepare $K$ data points at different error rates, choose $L<K$ of them for extrapolation, and evaluate error-mitigated results for all $\\binom{K}{L}$ possible choices. We then select the extrapolation method that yields the smallest variance in the error-mitigated results. This procedure can also be applied bitstring-wise, enabling adaptive error mitigation for each probability in the distribution.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u91cf\u5b50\u6982\u7387\u5206\u5e03\u7ea0\u9519\u534f\u8bae\uff0c\u901a\u8fc7N\u7248\u672c\u7f16\u7a0b\u6392\u9664\u5f02\u5e38\u503c\uff0c\u5e76\u9009\u62e9\u5177\u6709\u6700\u5c0f\u65b9\u5dee\u7684\u5916\u63d2\u7b56\u7565\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u91cf\u5b50\u7ea0\u9519\uff08QEM\uff09\u65e8\u5728\u51cf\u5c11\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u9519\u8bef\uff0c\u5c24\u5176\u662f\u5728\u4f30\u7b97\u671f\u671b\u503c\u548c\u6982\u7387\u5206\u5e03\u65b9\u9762\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb QEM \u4f30\u7b97\u91cf\u5b50\u6982\u7387\u5206\u5e03\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "1. N\u7248\u672c\u7f16\u7a0b\uff1a\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c QEM \u7b56\u7565\u4ea7\u751f\u7684\u6982\u7387\u5206\u5e03\uff0c\u6392\u9664\u5f02\u5e38\u503c\uff0c\u4ee5\u8ba4\u8bc1\u7ea0\u9519\u540e\u5206\u5e03\u7684\u53ef\u884c\u6027\u3002 2. \u4e00\u81f4\u6027\u9009\u62e9\u5916\u63d2\u7b56\u7565\uff1a\u51c6\u5907K\u4e2a\u4e0d\u540c\u9519\u8bef\u7387\u7684\u6570\u636e\u70b9\uff0c\u9009\u62e9L\u4e2a\u8fdb\u884c\u5916\u63d2\uff0c\u8bc4\u4f30\u6240\u6709\u53ef\u80fd\u7684\u7ec4\u5408\uff0c\u9009\u62e9\u65b9\u5dee\u6700\u5c0f\u7684\u5916\u63d2\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u4e5f\u53ef\u9010\u6bd4\u7279\u4f4d\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u7684 N \u7248\u672c\u7f16\u7a0b\u65b9\u6cd5\u53ef\u4ee5\u8ba4\u8bc1\u7ea0\u9519\u540e\u6982\u7387\u5206\u5e03\u7684\u53ef\u884c\u6027\u3002\u4e00\u81f4\u6027\u9009\u62e9\u5916\u63d2\u7b56\u7565\u7684\u65b9\u6cd5\u80fd\u591f\u9009\u62e9\u51fa\u6700\u4f18\u5916\u63d2\u65b9\u6cd5\uff0c\u51cf\u5c0f\u7ea0\u9519\u7ed3\u679c\u7684\u65b9\u5dee\uff0c\u5e76\u80fd\u5b9e\u73b0\u81ea\u9002\u5e94\u7ea0\u9519\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5b50\u6982\u7387\u5206\u5e03\u4f30\u7b97\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u624b\u6bb5\u3002"}}
{"id": "2511.12077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12077", "abs": "https://arxiv.org/abs/2511.12077", "authors": ["Dengming Zhang", "Weitao You", "Jingxiong Li", "Weishen Lin", "Wenda Shi", "Xue Zhao", "Heda Zuo", "Junxian Wu", "Lingyun Sun"], "title": "Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound", "comment": null, "summary": "Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.", "AI": {"tldr": "VAEmotionLLM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u7684\u97f3\u9891\u9884\u8bad\u7ec3\u6559\u4f1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u201c\u770b\u201d\u4ee5\u201c\u542c\u201d\uff0c\u5e76\u7406\u89e3\u8de8\u6a21\u6001\u7684\u60c5\u611f\u3002\u5b83\u5728ArtEmoBenchmark\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u60c5\u7eea\u7406\u89e3\u5bf9\u4e8e\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66f4\u52a0\u901a\u7528\u3001\u53ef\u9760\u548c\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u81f3\u5173\u91cd\u8981\u3002\u827a\u672f\u901a\u8fc7\u89c6\u89c9\u548c\u542c\u89c9\u5143\u7d20\u7684\u8054\u5408\u8bbe\u8ba1\u6765\u4f20\u8fbe\u60c5\u611f\uff0c\u4f46\u4ee5\u5f80\u7684\u5de5\u4f5c\u8981\u4e48\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\uff0c\u8981\u4e48\u662f\u5355\u6a21\u6001\u7684\uff0c\u5ffd\u7565\u4e86\u827a\u672f\u54c1\u6240\u8868\u8fbe\u7684\u60c5\u611f\u3002\u6b64\u5916\uff0c\u5f53\u524d\u7684\u89c6\u542c\u8bed\u8a00\u6a21\u578b\uff08AVLMs\uff09\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u7684\u97f3\u9891\u9884\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "VAEmotionLLM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u89c6\u89c9\u5f15\u5bfc\u97f3\u9891\u5bf9\u9f50\uff08VG-Align\uff09\u901a\u8fc7\u5bf9\u9f50\u540c\u6b65\u97f3\u89c6\u9891\u7247\u6bb5\u4e0a\u5171\u4eabLLM\u7684\u4e0b\u4e00\u4e2a\u8bcd\u5143\u5206\u5e03\uff0c\u5c06\u51bb\u7ed3\u7684\u89c6\u89c9\u901a\u8def\u63d0\u70bc\u5230\u65b0\u7684\u97f3\u9891\u901a\u8def\u4e2d\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u5927\u578b\u97f3\u9891\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u201c\u542c\u201d\u7684\u80fd\u529b\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u8f7b\u91cf\u7ea7\u7684\u8de8\u6a21\u6001\u60c5\u611f\u9002\u914d\u5668\uff08EmoAdapter\uff09\uff0c\u7531\u60c5\u611f\u589e\u5f3a\u5668\u548c\u60c5\u611f\u76d1\u7763\u5668\u7ec4\u6210\uff0c\u6ce8\u5165\u60c5\u611f\u654f\u611f\u7684\u6b8b\u5dee\u5e76\u5e94\u7528\u60c5\u611f\u76d1\u7763\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u60c5\u611f\u7406\u89e3\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86ArtEmoBenchmark\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u827a\u672f\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u4ec5\u97f3\u9891\u3001\u4ec5\u89c6\u89c9\u548c\u89c6\u542c\u8f93\u5165\u4e0b\u7684\u5185\u5bb9\u548c\u60c5\u611f\u7406\u89e3\u3002", "result": "VAEmotionLLM\u5728ArtEmoBenchmark\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5728\u4ec5\u97f3\u9891\u3001\u4ec5\u89c6\u89c9\u548c\u89c6\u542c\u57fa\u7ebf\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7ec4\u4ef6\u662f\u4e92\u8865\u7684\u3002", "conclusion": "VAEmotionLLM\u901a\u8fc7\u65b0\u9896\u7684VG-Align\u548cEmoAdapter\u6a21\u5757\uff0c\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u5728\u6709\u9650\u97f3\u9891\u9884\u8bad\u7ec3\u4e0b\u89c6\u542c\u60c5\u611f\u7406\u89e3\uff0c\u5e76\u5728ArtEmoBenchmark\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2511.13048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13048", "abs": "https://arxiv.org/abs/2511.13048", "authors": ["Yong Li", "Hui Cheng"], "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments", "comment": "2023 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u73af\u5883\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4ea4\u901a\u89c4\u5219\u4ee5\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u81ea\u7531\u7a7a\u95f4\u548c\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u4f8b\u5982\u5ffd\u7565\u4ea4\u901a\u89c4\u5219\u5bfc\u81f4\u9891\u7e41\u91cd\u89c4\u5212\u548c\u78b0\u649e\u98ce\u9669\uff0c\u6216\u8fc7\u4e8e\u4e25\u683c\u9075\u5b88\u89c4\u5219\u5bfc\u81f4\u8def\u5f84\u8fc7\u957f\u5f71\u54cd\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u3001\u7cfb\u7edf\u7684\u6539\u8fdb\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u7b56\u7565\u5b9e\u73b0\u89c4\u5212\u3002\u5141\u8bb8\u5728\u8d77\u70b9\u548c\u7ec8\u70b9\u8fdb\u884c\u9053\u8def\u8de8\u8d8a\u4ee5\u7f29\u77ed\u8def\u5f84\u3002\u63d0\u51fa\u4e24\u5c42\u52bf\u573a\u56fe\u4ee5\u5904\u7406\u8d77\u70b9\u548c\u7ec8\u70b9\u4f4d\u4e8e\u590d\u6742\u4ea4\u53c9\u53e3\u7684\u60c5\u51b5\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u548c\u9053\u8def\u7f51\u7edc\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u548c\u4ea4\u901a\u89c4\u5219\u9075\u4ece\u6027\u3002"}}
{"id": "2511.13256", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13256", "abs": "https://arxiv.org/abs/2511.13256", "authors": ["Spyros Tserkis", "Muhammad Umer", "Dimitris G. Angelakis"], "title": "Depth Optimization of Ansatz Circuits for Variational Quantum Algorithms", "comment": "9 pages, 7 figures", "summary": "The increasing depth of quantum circuits presents a major limitation for the execution of quantum algorithms, as the limited coherence time of physical qubits leads to noise that manifests as errors during computation. In this work, we focus on circuits relevant to variational quantum algorithms and demonstrate that their depth can be reduced by introducing additional qubits, mid-circuit measurements, and classically controlled operations. As an illustrative example, we consider nonlinear dynamics governed by the one-dimensional Burgers' equation, which has broad applications in computational fluid dynamics. In particular, we show that the proposed non-unitary quantum circuits can efficiently represent fluid flow configurations in both laminar and turbulent regimes. Furthermore, we demonstrate that, when noise is taken into account, these circuits are advantageous in regimes where two-qubit gate error rates are relatively low compared to idling error rates.", "AI": {"tldr": "\u901a\u8fc7\u589e\u52a0\u91cf\u5b50\u6bd4\u7279\u3001\u6d4b\u91cf\u548c\u7ecf\u5178\u63a7\u5236\u64cd\u4f5c\u6765\u51cf\u5c11\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\uff0c\u4ee5\u964d\u4f4e\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u566a\u58f0\u548c\u9519\u8bef\u3002", "motivation": "\u7531\u4e8e\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u7684\u76f8\u5e72\u65f6\u95f4\u6709\u9650\uff0c\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\u7684\u589e\u52a0\u4f1a\u9650\u5236\u91cf\u5b50\u7b97\u6cd5\u7684\u6267\u884c\uff0c\u5e76\u5bfc\u81f4\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u566a\u58f0\u548c\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u91cf\u5b50\u6bd4\u7279\u3001\u8fdb\u884c\u6d4b\u91cf\u548c\u6267\u884c\u7ecf\u5178\u63a7\u5236\u64cd\u4f5c\u6765\u51cf\u5c11\u91cf\u5b50\u7535\u8def\u7684\u6df1\u5ea6\uff0c\u5e76\u7814\u7a76\u4e86\u975e\u5355\u4e00\u91cf\u5b50\u7535\u8def\u5728\u6a21\u62df Burgers \u65b9\u7a0b\u4e2d\u7684\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u8003\u8651\u4e86\u566a\u58f0\u7684\u5f71\u54cd\u3002", "result": "\u975e\u5355\u4e00\u91cf\u5b50\u7535\u8def\u53ef\u4ee5\u6709\u6548\u5730\u8868\u793a\u6d41\u4f53\u6d41\u52a8\u914d\u7f6e\uff0c\u5e76\u4e14\u5728\u4e24\u6bd4\u7279\u95e8\u9519\u8bef\u7387\u8fdc\u4f4e\u4e8e\u7a7a\u95f2\u9519\u8bef\u7387\u65f6\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u975e\u5355\u4e00\u91cf\u5b50\u7535\u8def\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u7535\u8def\u6df1\u5ea6\uff0c\u5e76\u5728\u566a\u58f0\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u9886\u57df\u3002"}}
{"id": "2511.12079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12079", "abs": "https://arxiv.org/abs/2511.12079", "authors": ["Hongxuan Li", "Wencheng Zhu", "Huiying Xu", "Xinzhong Zhu", "Pengfei Zhu"], "title": "Point Cloud Quantization through Multimodal Prompting for 3D Understanding", "comment": "Accepted by AAAI 2026. 11 pages, 7 figures. Corresponding author: Wencheng Zhu (wenchengzhu@tju.edu.cn)", "summary": "Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u591a\u6a21\u6001\u63d0\u793a\u7684\u77e2\u91cf\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u70b9\u4e91\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7684\u65b9\u6cd5\u5728\u4ee3\u8868\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u539f\u578b\u7684\u65b9\u6cd5\uff08\u4f8b\u5982\uff0c\u53ef\u8bad\u7ec3\u5411\u91cf\u6216\u805a\u7c7b\u8d28\u5fc3\uff09\u5728\u4ee3\u8868\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u591a\u6a21\u6001\u5bf9\u9f50\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u663e\u793a\u51fa\u4e86\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u63d0\u793a\u7684\u77e2\u91cf\u91cf\u5316\u6846\u67b6\u3002\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u539f\u578b\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u81ea\u9002\u5e94\u5730\u4f18\u5316\u8fd9\u4e9b\u539f\u578b\uff0c\u4ee5\u5f25\u5408\u89c6\u89c9-\u8bed\u8a00\u8bed\u4e49\u9e3f\u6c9f\u3002\u6846\u67b6\u91c7\u7528\u53cc\u7ea6\u675f\u91cf\u5316\u7a7a\u95f4\uff08\u7d27\u51d1\u6027\u548c\u5206\u79bb\u6027\u6b63\u5219\u5316\uff09\uff0c\u5e76\u4f7f\u7528 Gumbel-Softmax \u6280\u5de7\u5b9e\u73b0\u53ef\u5fae\u5206\u79bb\u6563\u5316\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 ModelNet40 \u548c ScanObjectNN \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u63d0\u793a\u9a71\u52a8\u7684\u91cf\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u89c6\u89c9\u548c\u539f\u578b\u7279\u5f81\uff0c\u751f\u6210\u540c\u65f6\u7f16\u7801\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u6df7\u5408\u8868\u793a\uff0c\u5e76\u5728\u70b9\u4e91\u5206\u6790\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2511.12695", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12695", "abs": "https://arxiv.org/abs/2511.12695", "authors": ["Minghui Chen", "Hrad Ghoukasian", "Ruinan Jin", "Zehua Wang", "Sai Praneeth Karimireddy", "Xiaoxiao Li"], "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning", "comment": "33 pages, 6 figures, 8 tables", "summary": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.", "AI": {"tldr": "LP-FT\u901a\u8fc7\u5206\u9636\u6bb5\u66f4\u65b0\u53c2\u6570\u6765\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u5931\u771f\u95ee\u9898\uff0c\u4ee5\u5e73\u8861\u5168\u5c40\u6cdb\u5316\u548c\u672c\u5730\u4e2a\u6027\u5316\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53bb\u4e2d\u5fc3\u5316\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u5728\u5904\u7406\u975e\u76f8\u540c\u7684\u6570\u636e\u5206\u5e03\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u6cdb\u5316\u548c\u672c\u5730\u4e2a\u6027\u5316\u3002\u73b0\u6709\u7684\u4e2a\u6027\u5316\u5fae\u8c03\uff08PFT\uff09\u65b9\u6cd5\u5bb9\u6613\u5728\u5ba2\u6237\u7aef\u5206\u5e03\u503e\u659c\u65f6\u8fc7\u62df\u5408\uff0c\u6216\u5728\u53d1\u751f\u57df\u8fc1\u79fb\u65f6\u5931\u8d25\u3002", "method": "\u5c06\u7ebf\u6027\u63a2\u6d4b\uff08LP\uff09\u548c\u5168\u6a21\u578b\u5fae\u8c03\uff08FT\uff09\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLP-FT\u7684\u96c6\u4e2d\u5f0f\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u5931\u771f\u95ee\u9898\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u548c\u516d\u79cdPFT\u53d8\u4f53\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0cLP-FT\u5728\u5e73\u8861\u672c\u5730\u4e2a\u6027\u5316\u548c\u5168\u5c40\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u8be5\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u8054\u90a6\u7279\u5f81\u5931\u771f\u73b0\u8c61\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u9610\u8ff0\u4e86LP-FT\u5982\u4f55\u901a\u8fc7\u5206\u9636\u6bb5\u53c2\u6570\u66f4\u65b0\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "conclusion": "LP-FT\u901a\u8fc7\u5206\u9636\u6bb5\u66f4\u65b0\u53c2\u6570\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u5931\u771f\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4e5f\u589e\u5f3a\u4e86\u672c\u5730\u4e2a\u6027\u5316\u3002\u7814\u7a76\u8fd8\u660e\u786e\u4e86LP-FT\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\u7684\u6761\u4ef6\uff0c\u4e3a\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u90e8\u7f72\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.12139", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12139", "abs": "https://arxiv.org/abs/2511.12139", "authors": ["Sahar Moghimian Hoosh", "Ilia Kamyshev", "Henni Ouerdane"], "title": "Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion", "comment": "Extended version of the conference paper \"Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis\" -- arXiv:2501.16817. Instead of solely using ICA or PCA for feature extraction, we propose the fusion of ICA and PCA, which outperforms other baseline models. This extended version is meant for journal publication", "summary": "Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u975e\u4fb5\u5165\u5f0f\u8d1f\u8377\u76d1\u6d4b\uff08NILM\uff09\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408ICA\u548cPCA\u7279\u5f81\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff08Fusion-ResNet\uff09\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u5e76\u53d1\u7535\u5668\u65f6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u975e\u4fb5\u5165\u5f0f\u8d1f\u8377\u76d1\u6d4b\uff08NILM\uff09\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u8fc7\u62df\u5408\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4f4e\u4ee5\u53ca\u540c\u65f6\u89e3\u7b97\u5927\u91cf\u7535\u5668\u529f\u8017\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u9ad8\u9891\u6807\u8bb0\u6570\u636e\u3001\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aefNILM\u5206\u7c7b\u6846\u67b6\u3002\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u878d\u5408\u4e86\u72ec\u7acb\u6210\u5206\u5206\u6790\uff08ICA\uff09\u548c\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7279\u5f81\u3002\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e3aFusion-ResNet\u3002", "result": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u6a21\u578b\u5728\u5e73\u5747F1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684NILM\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002Fusion-ResNet\u5728\u6700\u591a15\u4e2a\u5e76\u53d1\u6d3b\u52a8\u7535\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u538b\u529b\u6761\u4ef6\u8868\u73b0\u51fa\u76f8\u5bf9\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684Fusion-ResNet\u6846\u67b6\u901a\u8fc7\u878d\u5408ICA\u548cPCA\u7279\u5f81\u4ee5\u53ca\u91c7\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709NILM\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\uff0c\u5728\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5e76\u80fd\u5728\u9ad8\u5e76\u53d1\u7535\u5668\u573a\u666f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13071", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13071", "abs": "https://arxiv.org/abs/2511.13071", "authors": ["Michal Levin", "Itzik Klein"], "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers", "comment": "22 pages, 10 figures", "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u9884\u5148\u4e86\u89e3\u4f20\u611f\u5668\u65b9\u5411\u5373\u53ef\u5728\u9759\u6001\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u5dee\u7684\u65e0\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u4f4e\u6210\u672c\u5fae\u673a\u7535\u52a0\u901f\u5ea6\u8ba1\u7684\u6027\u80fd\u5e38\u56e0\u504f\u5dee\u8bef\u5dee\u800c\u53d7\u635f\u3002\u4e3a\u4e86\u6d88\u9664\u786e\u5b9a\u6027\u504f\u5dee\u9879\uff0c\u9700\u8981\u8fdb\u884c\u9759\u6001\u6761\u4ef6\u4e0b\u7684\u6821\u51c6\uff0c\u4f46\u8fd9\u9700\u8981\u52a0\u901f\u5ea6\u8ba1\u8c03\u5e73\u6216\u590d\u6742\u7684\u4f9d\u8d56\u4e8e\u65b9\u5411\u7684\u6821\u51c6\u7a0b\u5e8f\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u6a21\u578b\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u5728\u9759\u6001\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u5dee\uff0c\u65e0\u9700\u4e86\u89e3\u4f20\u611f\u5668\u65b9\u5411\uff0c\u4e5f\u65e0\u9700\u65cb\u8f6c\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u572813.39\u5c0f\u65f6\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8bef\u5dee\u6c34\u5e73\u6301\u7eed\u964d\u4f4e\u4e8652%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u65e0\u65b9\u5411\u8981\u6c42\u7684\u573a\u666f\u4e0b\u5b9e\u73b0\u7cbe\u786e\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4f4e\u6210\u672c\u60ef\u6027\u4f20\u611f\u5668\u5728\u5404\u79cd\u79d1\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5e76\u6d88\u9664\u4e86\u5bf9\u8c03\u5e73\u6821\u51c6\u7684\u9700\u8981\u3002"}}
{"id": "2511.13281", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.13281", "abs": "https://arxiv.org/abs/2511.13281", "authors": ["Lorenzo Valentini", "Diego Forlivesi", "Andrea Talarico", "Marco Chiani"], "title": "Restart Belief: A General Quantum LDPC Decoder", "comment": "5 pages, 1 figure, submitted to IEEE Communications Letters", "summary": "Hardware-friendly quantum low-density parity-check (QLDPC) decoders are commonly built upon belief propagation (BP) processing. Yet, quantum degeneracy often prevents BP from achieving reliable convergence. To overcome this fundamental limitation, we propose the restart belief (RB) decoder, an iterative BP-based algorithm inspired by branch-and-bound optimization principles. From our analysis we find that the RB decoder represents both the fastest and most accurate decoding algorithm applicable to QLDPC codes to date, conceived with the explicit goal of approaching error correction up to the code distance.", "AI": {"tldr": "RB\u8bd1\u7801\u5668\u662f\u76ee\u524d\u6700\u5feb\u3001\u6700\u51c6\u786e\u7684QLDPC\u8bd1\u7801\u7b97\u6cd5\uff0c\u80fd\u591f\u63a5\u8fd1\u7801\u8ddd\u79bb\u7684\u7ea0\u9519\u80fd\u529b\u3002", "motivation": "\u91cf\u5b50\u9000\u5316\u4f1a\u963b\u6b62BP\u7b97\u6cd5\u53ef\u9760\u6536\u655b\uff0cRB\u8bd1\u7801\u5668\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "RB\u8bd1\u7801\u5668\u662f\u4e00\u79cd\u57fa\u4e8eBP\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u501f\u9274\u4e86\u5206\u652f\u5b9a\u754c\u4f18\u5316\u7684\u601d\u60f3\u3002", "result": "RB\u8bd1\u7801\u5668\u662f\u76ee\u524d\u6700\u5feb\u3001\u6700\u51c6\u786e\u7684QLDPC\u8bd1\u7801\u7b97\u6cd5\u3002", "conclusion": "RB\u8bd1\u7801\u5668\u662f\u76ee\u524d\u6700\u5feb\u3001\u6700\u51c6\u786e\u7684QLDPC\u8bd1\u7801\u7b97\u6cd5\uff0c\u65e8\u5728\u63a5\u8fd1\u7801\u8ddd\u79bb\u7684\u7ea0\u9519\u80fd\u529b\u3002"}}
{"id": "2511.12082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12082", "abs": "https://arxiv.org/abs/2511.12082", "authors": ["Lokender Singh", "Saksham Kumar", "Chandan Kumar"], "title": "Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning", "comment": "ICCCNT 2025 Conference Proceedings, IIT Indore", "summary": "Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6982\u7387\u63a8\u7406\u7684\u6539\u8fdbResNet-101\u6a21\u578b\u7528\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\uff0c\u5728COCO-2014\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.794 mAP\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6982\u7387\u63a8\u7406\u3001\u6a21\u62df\u6807\u7b7e\u4f9d\u8d56\u548c\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u9884\u6d4b\u7cbe\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4fee\u6539\u540e\u7684ResNet-101\u67b6\u6784\u3002", "result": "\u5728COCO-2014\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u53d6\u5f97\u4e860.794 mAP\u7684\u6210\u7ee9\uff0c\u4f18\u4e8eResNet-SRN\uff080.771\uff09\u548cVision Transformer\u57fa\u7ebf\uff080.785\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u6982\u7387\u63a8\u7406\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u573a\u666f\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002"}}
{"id": "2511.12240", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12240", "abs": "https://arxiv.org/abs/2511.12240", "authors": ["Vishal Joshua Meesala"], "title": "SCI: An Equilibrium for Signal Intelligence", "comment": "34 pages, 7 figures. Preprint", "summary": "We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] (\"Surgical Precision\") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.", "AI": {"tldr": "SCI\u662f\u4e00\u4e2a\u63a7\u5236\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u4e00\u79cd\u53d7\u8c03\u8282\u7684\u72b6\u6001\uff0c\u901a\u8fc7\u53c2\u6570\u66f4\u65b0\u6765\u6700\u5c0f\u5316\u89e3\u91ca\u8bef\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u53ef\u9760\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u4e00\u79cd\u53d7\u8c03\u8282\u7684\u72b6\u6001\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u66f4\u53ef\u9760\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "SCI\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\uff1a1. \u9760\u6027\u52a0\u6743\u7684\u3001\u591a\u5c3a\u5ea6\u7684\u7279\u5f81\uff1b2. \u77e5\u8bc6\u5f15\u5bfc\u7684\u89e3\u91ca\u5668\uff1b3. Lyapunov\u5f15\u5bfc\u7684\u63a7\u5236\u5668\uff0c\u5177\u5907\u56de\u6eda\u3001\u4fe1\u4efb\u533a\u57df\u5b89\u5168\u63aa\u65bd\u548c\u4e0b\u964d\u6761\u4ef6\u3002", "result": "SCI\u5728\u751f\u7269\u533b\u5b66\u3001\u5de5\u4e1a\u548c\u73af\u5883\u9886\u57df\u5c06\u89e3\u91ca\u8bef\u5dee\u964d\u4f4e\u4e8625-42%\uff08\u5e73\u574738%\uff09\uff0c\u540c\u65f6\u5c06SP\u65b9\u5dee\u4ece0.030\u964d\u4f4e\u52300.011\uff0c\u8868\u660e\u89e3\u91ca\u66f4\u52a0\u7a33\u5b9a\u3002", "conclusion": "\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u63a7\u5236\u76ee\u6807\u53ef\u4ee5\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u6062\u590d\u66f4\u5feb\u3001\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u53ef\u89e3\u91ca\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u4fe1\u53f7\u3002"}}
{"id": "2511.13096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13096", "abs": "https://arxiv.org/abs/2511.13096", "authors": ["Guy Damari", "Itzik Klein"], "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment", "comment": null, "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8\u00b0 using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.", "AI": {"tldr": "ResAlignNet\u662f\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u76841D ResNet-18\u67b6\u6784\uff0c\u80fd\u591f\u5feb\u901f\uff08\u79d2\u7ea7\uff09\u4e14\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u673a\u52a8\u5373\u53ef\u5b9e\u73b0AUV\u7684INS\u548cDVL\u4f20\u611f\u5668\u6846\u67b6\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86Sim2Real\u8fc1\u79fb\uff0c\u5e76\u5c06\u5bf9\u9f50\u65f6\u95f4\u7f29\u77ed\u4e8665%\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5bf9\u9f50\u65b9\u6cd5\u6536\u655b\u65f6\u95f4\u957f\uff0c\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u90e8\u8f85\u52a9\u4f20\u611f\u5668\uff0c\u9650\u5236\u4e86AUV\u5728\u6c34\u4e0b\u5bfc\u822a\u4e2d\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aResAlignNet\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u75281D ResNet-18\u67b6\u6784\u5c06\u5bf9\u9f50\u95ee\u9898\u8f6c\u5316\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u95ee\u9898\uff0c\u4ec5\u4f7f\u7528AUV\u4e0a\u5b89\u88c5\u7684\u4f20\u611f\u5668\u8fdb\u884c\u539f\u5730\u5bf9\u9f50\u3002", "result": "ResAlignNet\u80fd\u591f\u5728\u5927\u7ea625\u79d2\u5185\u5b9e\u73b00.8\u00b0\u7684\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u901f\u5ea6\u7684\u65b9\u6cd5\u5feb65%\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0Sim2Real\u8fc1\u79fb\u3002", "conclusion": "ResAlignNet\u901a\u8fc7\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u8fd0\u52a8\u7684\u4f20\u611f\u5668\u65e0\u5173\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AUV\u5bfc\u822a\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u64cd\u4f5c\u573a\u666f\u548c\u4f20\u611f\u5668\u89c4\u683c\u3002"}}
{"id": "2511.13308", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13308", "abs": "https://arxiv.org/abs/2511.13308", "authors": ["V. Yu. Mylnikov", "S. O. Potashin", "M. S. Ukhtary", "G. S. Sokolovskii"], "title": "Switching rates in Kerr resonator with two-photon dissipation and driving", "comment": "12 pages, 8 figures", "summary": "We analytically investigate the switching rate in a two-photon driven Kerr oscillator with finite detuning and two-photon dissipation. This system exhibits quantum bistability and supports a logical manifold for a bosonic qubit. Using Kramer's theory together with the $P$-representation, we derive an analytical expression for the bit-flip error rate within the potential-barrier approximation. The agreement is demonstrated between analytical calculations and numerical simulations obtained by diagonalization of the Liouvillian superoperator. In the purely dissipative limit, the switching rate increases monotonically with detuning, as the two metastable states approach each other in phase space. However, the exponential contribution to the bit-flip rate exhibits a nontrivial dependence on system parameters, extending beyond the naive scaling with the average photon number. In the presence of large Kerr nonlinearity, the switching rate becomes a nonmonotonic function of the detuning and reaches a minimum at a finite detuning. This effect arises because detuning lowers the activation barrier for weak nonlinearity but increases it for large ones, ensuring a minimum of the switching-rate at nonzero detuning. These results establish key conditions for optimizing the performance of critical cat qubits and are directly relevant for the design of scalable superconducting bosonic quantum architectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528Kramer\u7406\u8bba\u548cP-\u8868\u793a\u6cd5\uff0c\u5728\u6709\u9650\u5931\u8c10\u548c\u53cc\u5149\u5b50\u8017\u6563\u7684\u6761\u4ef6\u4e0b\uff0c\u5206\u6790\u4e86\u53cc\u5149\u5b50\u9a71\u52a8\u7684Kerr\u632f\u8361\u5668\u4e2d\u7684\u5f00\u5173\u901f\u7387\u548c\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u7387\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f00\u5173\u901f\u7387\u4e0e\u5931\u8c10\u3001\u975e\u7ebf\u6027\u5f3a\u5ea6\u7b49\u53c2\u6570\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u91cf\u5b50\u6bd4\u7279\u6027\u80fd\u7684\u5173\u952e\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u53cc\u5149\u5b50\u9a71\u52a8\u7684Kerr\u632f\u8361\u5668\u4e2d\u7684\u5f00\u5173\u901f\u7387\u548c\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u7387\uff0c\u4ee5\u4f18\u5316\u91cf\u5b50\u6bd4\u7279\u6027\u80fd\u548c\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u8d85\u5bfc\u73bb\u8272\u5b50\u91cf\u5b50\u67b6\u6784\u3002", "method": "\u5229\u7528Kramer\u7406\u8bba\u548cP-\u8868\u793a\u6cd5\uff0c\u5728\u52bf\u5792\u8fd1\u4f3c\u4e0b\u63a8\u5bfc\u51fa\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u7387\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4e0eLiouvillian\u8d85\u7b97\u5b50\u5bf9\u89d2\u5316\u5f97\u5230\u7684\u6570\u503c\u6a21\u62df\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u7eaf\u8017\u6563\u6781\u9650\u4e0b\uff0c\u5f00\u5173\u901f\u7387\u968f\u5931\u8c10\u5355\u8c03\u9012\u589e\u3002\u5728\u5f3aKerr\u975e\u7ebf\u6027\u5b58\u5728\u4e0b\uff0c\u5f00\u5173\u901f\u7387\u662f\u5931\u8c10\u7684\u975e\u5355\u8c03\u51fd\u6570\uff0c\u5728\u6709\u9650\u5931\u8c10\u5904\u8fbe\u5230\u6700\u5c0f\u503c\u3002\u7406\u8bba\u8ba1\u7b97\u4e0e\u6570\u503c\u6a21\u62df\u7ed3\u679c\u543b\u5408\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u4e34\u754ccat\u91cf\u5b50\u6bd4\u7279\u7684\u6027\u80fd\u548c\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u8d85\u5bfc\u73bb\u8272\u5b50\u91cf\u5b50\u67b6\u6784\u63d0\u4f9b\u4e86\u5173\u952e\u6761\u4ef6\u548c\u6307\u5bfc\u3002"}}
{"id": "2511.12084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12084", "abs": "https://arxiv.org/abs/2511.12084", "authors": ["Ji-Ping Jin", "Chen-Bin Feng", "Rui Fan", "Chi-Man Vong"], "title": "SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving", "comment": "12pages, has been early accepted by The Visual Computer: International Journal of Computer Graphics, 2025", "summary": "Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86SemanticStitch\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u8bed\u4e49\u4fe1\u606f\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u62fc\u63a5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56e0\u89c6\u89d2\u3001\u4f4d\u7f6e\u548c\u7269\u4f53\u79fb\u52a8\u5bfc\u81f4\u7684\u5bf9\u9f50\u95ee\u9898\u548c\u89c6\u89c9\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u524d\u666f\u7684\u8fde\u7eed\u6027\u88ab\u7834\u574f\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u5148\u9a8c\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSemanticStitch\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u524d\u666f\u5bf9\u8c61\u7684\u8bed\u4e49\u5148\u9a8c\u6765\u4fdd\u6301\u5176\u5b8c\u6574\u6027\u5e76\u589e\u5f3a\u89c6\u89c9\u8fde\u8d2f\u6027\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5f3a\u8c03\u663e\u8457\u5bf9\u8c61\u7684\u524d\u666f\u5b8c\u6574\u6027\u3002", "result": "\u4e0e\u4f20\u7edf\u6280\u672f\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u5728\u4e24\u4e2a\u4e13\u95e8\u6784\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "SemanticStitch\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u62fc\u63a5\u7684\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u652f\u6301\u3002"}}
{"id": "2511.13100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13100", "abs": "https://arxiv.org/abs/2511.13100", "authors": ["Xuecheng Chen", "Jingao Xu", "Wenhua Ding", "Haoyang Wang", "Xinyu Luo", "Ruiyang Duan", "Jialong Chen", "Xueqian Wang", "Yunhao Liu", "Xinlei Chen"], "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing", "comment": null, "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201csysname\u201d\u7684\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65e0\u4eba\u673a\u4f20\u611f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u87ba\u65cb\u6868\u8f6c\u901f\u6765\u63d0\u9ad8\u65e0\u4eba\u673a\u4f20\u611f\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u666e\u53ca\uff0c\u4ece\u5730\u9762\u8fdb\u884c\u975e\u63a5\u89e6\u5f0f\u65e0\u4eba\u673a\u4f20\u611f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u540d\u4e3a\u201csysname\u201d\uff0c\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a\u2018Count Every Rotation\u2019\u7528\u4e8e\u7cbe\u786e\u4f30\u7b97\u8f6c\u901f\uff0c\u2018Every Rotation Counts\u2019\u5229\u7528\u8f6c\u901f\u63a8\u65ad\u65e0\u4eba\u673a\u52a8\u529b\u5b66\u3002", "result": "\u5728\u5b9e\u9645\u7684\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u4e2d\uff0c\u2018sysname\u2019\u5b9e\u73b0\u4e863\u6beb\u79d2\u7684\u4f20\u611f\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u7b97\u8bef\u5dee\uff0c\u540c\u65f6\u80fd\u4ee596.5%\u7684\u7cbe\u5ea6\u63a8\u65ad\u98de\u884c\u6307\u4ee4\uff0c\u5e76\u5c06\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u9ad8\u4e8622%\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u2018sysname\u2019\u901a\u8fc7\u7cbe\u786e\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u6d4b\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u7684\u4f20\u611f\u6027\u80fd\uff0c\u5e76\u5728\u65e0\u4eba\u673a\u914d\u9001\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6548\u679c\u3002"}}
{"id": "2511.12090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12090", "abs": "https://arxiv.org/abs/2511.12090", "authors": ["Shengqin Jiang", "Tianqi Kong", "Yuankai Qi", "Haokui Zhang", "Lina Yao", "Quan Z. Sheng", "Qingshan Liu", "Ming-Hsuan Yang"], "title": "Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning", "comment": "under review", "summary": "Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.", "AI": {"tldr": "Prompt-based continual learning methods freeze pre-trained models and fine-tune task-specific prompts. While effective, independent prompts per layer can lead to catastrophic forgetting due to overly flexible tuning. This paper proposes a hierarchical layer-grouped prompt tuning method that shares prompts within layer groups (adjusted by position encoding) and uses a root prompt to generate sub-prompts, enhancing stability and reducing forgetting. Experiments show favorable performance.", "motivation": "Existing prompt-based continual learning methods fine-tune independent prompts per layer, which, despite offering flexibility, can lead to overly adaptable layers and catastrophic forgetting by overwriting essential features of previous tasks. The goal is to improve model stability and mitigate this risk.", "method": "The proposed method is a hierarchical layer-grouped prompt tuning approach. It works by (i) making layers within the same group share prompts adjusted by position encoding to preserve pre-trained model's intrinsic feature relationships and propagation pathways, and (ii) using a single task-specific root prompt to generate sub-prompts for each layer group, enhancing prompt synergy and reducing their independence.", "result": "Extensive experiments were conducted across four benchmarks. The proposed hierarchical layer-grouped prompt tuning method achieved favorable performance compared to several state-of-the-art methods.", "conclusion": "The proposed hierarchical layer-grouped prompt tuning method effectively improves model stability in continual learning by preserving intrinsic feature relationships and reducing prompt independence, thereby mitigating catastrophic forgetting and achieving competitive performance."}}
{"id": "2511.13120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13120", "abs": "https://arxiv.org/abs/2511.13120", "authors": ["Trevor Exley", "Anderson Brazil Nardin", "Petr Trunin", "Diana Cafiso", "Lucia Beccai"], "title": "Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design", "comment": "8 pages, 6 figures, 1 algorithm, 1 table", "summary": "This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u96c6\u6210\u9a71\u52a8\u3001\u4f20\u611f\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u79ef\u6728\u5355\u5143\uff08MU\uff09\uff0c\u5e76\u5efa\u7acb\u4e86\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u5f00\u53d1\u4e00\u79cd\u96c6\u6210\u9a71\u52a8\u3001\u4f20\u611f\u7684\u79ef\u6728\u5355\u5143\uff0c\u5e76\u7814\u7a76\u5176\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "1. \u63d0\u51faMU\u5355\u5143\uff0c\u96c6\u6210\u6c14\u52a8\u9a71\u52a8\u3001\u67d4\u6027\u6676\u683c\u548c\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u4f4d\u70b9\u30022. \u5efa\u7acb\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u8fde\u63a5\u9a71\u52a8\u8154\u5c3a\u5bf8\u548c\u6676\u683c\u5355\u5143\u5c3a\u5bf8\u30023. \u901a\u8fc7\u5b9e\u9a8c\u5747\u5316\u83b7\u53d6\u6676\u683c\u6750\u6599\u7684\u6709\u6548\u5c5e\u6027\uff0c\u7528\u4e8e\u6709\u9650\u5143\u6a21\u62df\u30024. \u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u5c06\u4f20\u611f\u5668\u5e03\u5c40\u89c6\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u8bc4\u4f30\u5019\u9009\u6ce2\u5bfc\u8def\u5f84\uff0c\u9009\u62e9\u80fd\u5c06\u57fa\u7ebf\u529b\u5b66\u6027\u80fd\u504f\u5dee\u6700\u5c0f\u5316\u7684\u5e03\u5c40\u30025. \u5236\u9020\u4f18\u5316\u540e\u7684\u6a21\u578b\u5e76\u8fdb\u884c\u5b9e\u9a8c\u8868\u5f81\u30026. \u5c06\u5de5\u4f5c\u6d41\u7a0b\u6269\u5c55\u5230\u66f4\u5927\u5c3a\u5bf8\u5355\u5143\u548c\u53cc\u6307\u5939\u722a\uff0c\u9a8c\u8bc1MU\u6982\u5ff5\u7684\u901a\u7528\u6027\u3002", "result": "\u4f18\u5316\u540e\u7684MU\u5355\u5143\u5728\u5d4c\u5165\u4f20\u611f\u7684\u540c\u65f6\uff0c\u529b\u5b66\u6027\u80fd\u4fdd\u6301\u4e0d\u53d8\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u5c3a\u5bf8\u7684\u5355\u5143\u548c\u591a\u5355\u5143\u96c6\u6210\uff08\u5982\u53cc\u6307\u5939\u722a\uff09\u3002", "conclusion": "MU\u5355\u5143\u901a\u8fc7\u53ef\u91cd\u590d\u7684\u534f\u540c\u8bbe\u8ba1\u89c4\u5219\u548c\u4eff\u771f\u9a71\u52a8\u7684\u4f20\u611f\u5668\u96c6\u6210\uff0c\u63a8\u52a8\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6574\u4f53\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u96c6\u6210\u9a71\u52a8\u548c\u4f20\u611f\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2511.13342", "categories": ["quant-ph", "cond-mat.other", "nlin.CD", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2511.13342", "abs": "https://arxiv.org/abs/2511.13342", "authors": ["Avadhut V. Purohit", "Udaysinh T. Bhosale"], "title": "Floquet Recurrences in the Double Kicked Top", "comment": "9 pages (two-column) + 6 pages (one-column) + 16 figures. Comments are welcome", "summary": "We study exact quantum recurrences in the double kicked top (DKT), a driven spin model that extends the quantum kicked top (QKT) by introducing an additional time-reversal symmetry-breaking kick. Reformulating its dynamics in terms of effective parameters $k_r$ and $k_\u03b8$, we analytically show exact periodicity of the Floquet operator for $k_r = j\u03c0/2$ and $k_r = j\u03c0/4$ with distinct periods for integer and half-odd integer $j$. These exact recurrences were found to be independent of $k_\u03b8$. The long-time-averaged entanglement and fidelity rate function show dynamical quantum phase transition (DQPT) for $k_r = j\u03c0/2$ at time-reversal symmetric cases $k_\u03b8= \\pm k_r$. In the other time-reversal symmetric case $k_\u03b8= 0$, the DQPT exists only for a half-odd integer $j$. Using level statistics, a smooth transition is observed from integrable to non-integrable nature as $k_r$ is changed away from $j\u03c0/2$. Our work demonstrates that regular and chaotic regimes can be controlled for any system size by tuning $k_r$ and $k_\u03b8$, making the DKT a useful platform for quantum control and information processing applications.", "AI": {"tldr": "\u53cc\u8e22\u7ffb\u6eda\u4f53(DKT)\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u91cf\u5b50\u5468\u671f\u6027\u88ab\u89e3\u6790\u5730\u8bc1\u660e\uff0c\u5e76\u72ec\u7acb\u4e8ek\u03b8\u3002\u901a\u8fc7\u7ea0\u7f20\u548c\u4fdd\u771f\u5ea6\u901f\u7387\u51fd\u6570\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53d1\u73b0\u4e86\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8(DQPT)\u3002\u6c34\u5e73\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0c\u968f\u7740kr\u7684\u53d8\u5316\uff0c\u7cfb\u7edf\u53ef\u4ee5\u4ece\u53ef\u79ef\u8fc7\u6e21\u5230\u975e\u53ef\u79ef\u3002", "motivation": "\u7814\u7a76\u53cc\u8e22\u7ffb\u6eda\u4f53(DKT)\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u9a71\u52a8\u7684\u81ea\u65cb\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u65f6\u5e8f\u5bf9\u79f0\u6027\u7834\u574f\u8e22\u6765\u6269\u5c55\u91cf\u5b50\u8e22\u7ffb\u6eda\u4f53(QKT)\u3002", "method": "\u901a\u8fc7\u91cd\u6784\u5176\u52a8\u529b\u5b66\u4e3a\u6709\u6548\u53c2\u6570kr\u548ck\u03b8\uff0c\u89e3\u6790\u5730\u8bc1\u660e\u4e86Floquet\u7b97\u5b50\u7684\u7cbe\u786e\u5468\u671f\u6027\u3002\u901a\u8fc7\u5206\u6790\u7ea0\u7f20\u548c\u4fdd\u771f\u5ea6\u901f\u7387\u51fd\u6570\uff0c\u4ee5\u53ca\u6c34\u5e73\u7edf\u8ba1\uff0c\u7814\u7a76\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8\u548c\u7cfb\u7edf\u7684\u53ef\u79ef\u6027\u3002", "result": "\u5bf9\u4e8ekr = j\u03c0/2\u548ckr = j\u03c0/4\uff0cFloquet\u7b97\u5b50\u5177\u6709\u786e\u5b9a\u7684\u5468\u671f\u6027\uff0c\u5e76\u4e14\u72ec\u7acb\u4e8ek\u03b8\u3002\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8(DQPT)\u88ab\u53d1\u73b0\u3002\u901a\u8fc7\u6c34\u5e73\u7edf\u8ba1\uff0c\u89c2\u5bdf\u5230\u4ece\u53ef\u79ef\u5230\u975e\u53ef\u79ef\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "conclusion": "\u53cc\u8e22\u7ffb\u6eda\u4f53(DKT)\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u91cf\u5b50\u5468\u671f\u6027\u548c\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8(DQPT)\u73b0\u8c61\u88ab\u63ed\u793a\u3002\u901a\u8fc7\u8c03\u63a7kr\u548ck\u03b8\uff0c\u53ef\u4ee5\u63a7\u5236\u4efb\u4f55\u7cfb\u7edf\u5927\u5c0f\u7684\u89c4\u5219\u548c\u6df7\u6c8c\u72b6\u6001\uff0c\u8fd9\u4f7f\u5f97DKT\u6210\u4e3a\u91cf\u5b50\u63a7\u5236\u548c\u4fe1\u606f\u5904\u7406\u5e94\u7528\u7684\u6709\u7528\u5e73\u53f0\u3002"}}
{"id": "2511.12095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12095", "abs": "https://arxiv.org/abs/2511.12095", "authors": ["Shuhan Ye", "Yi Yu", "Qixin Zhang", "Chenqi Kong", "Qiangqiang Wu", "Kun Wang", "Xudong Jiang"], "title": "Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio", "comment": null, "summary": "Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \\textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \\textbf{ST-DSM} and \\textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \\(84.4\\%\\) accuracy, about \\(85\\%\\) of the full training set performance, while reducing training time by more than \\(50\\times\\) and storage cost by \\(6000\\times\\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.", "AI": {"tldr": "PACE\u662f\u4e00\u4e2a\u9488\u5bf9\u4e8b\u4ef6\u76f8\u673a\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u9996\u6b21\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7ST-DSM\u548cPEQ-N\u6a21\u5757\uff0c\u5c06\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u538b\u7f29\u6210\u5c0f\u578b\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901fSNN\u8bad\u7ec3\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u4eff\u751f\u52a8\u529b\u5b66\u7279\u6027\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u7ed3\u5408\u4e3a\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8282\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0cSNN\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u964d\u4f4eSNN\u7684\u8bad\u7ec3\u6210\u672c\u3002", "method": "PACE\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aST-DSM\uff08\u7a7a\u95f4\u65f6\u95f4-\u4e8b\u4ef6\u6570\u636e\u6d41\u5185\u5b58\uff09\u548cPEQ-N\uff08\u6982\u7387\u6574\u6570\u91cf\u5316\u5668\uff09\u3002ST-DSM\u5229\u7528\u6b8b\u4f59\u819c\u7535\u4f4d\u589e\u5f3a\u4e8b\u4ef6\u57fa\u7279\u5f81\uff08SDR\uff09\uff0c\u5e76\u8fdb\u884c\u7cbe\u7ec6\u7684\u65f6\u7a7a\u5e45\u5ea6-\u76f8\u4f4d\u5339\u914d\uff08ST-SM\uff09\u3002PEQ-N\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u76f4\u901a\u5f0f\u6982\u7387\u6574\u6570\u91cf\u5316\u5668\uff0c\u53ef\u517c\u5bb9\u6807\u51c6\u7684\u4e8b\u4ef6\u5e27\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5728DVS-Gesture\u3001CIFAR10-DVS\u548cN-MNIST\u6570\u636e\u96c6\u4e0a\uff0cPACE\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u96c6\u6838\u5fc3\u9009\u62e9\u548c\u6570\u636e\u96c6\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5\u3002\u5c24\u5176\u5728\u52a8\u6001\u4e8b\u4ef6\u6d41\u548c\u4f4e/\u4e2d\u7b49IPC\uff08\u6bcf\u79d2\u4e8b\u4ef6\u6570\uff09\u8bbe\u7f6e\u4e0b\uff0cPACE\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u4f8b\u5982\uff0c\u5728N-MNIST\u6570\u636e\u96c6\u4e0a\uff0cPACE\u8fbe\u5230\u4e8684.4%\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5b8c\u6574\u8bad\u7ec3\u96c6\u6027\u80fd\u768485%\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e8650\u500d\u4ee5\u4e0a\uff0c\u5b58\u50a8\u6210\u672c\u964d\u4f4e\u4e866000\u500d\uff0c\u5b9e\u73b0\u4e86\u5206\u949f\u7ea7SNN\u8bad\u7ec3\u548c\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\u3002", "conclusion": "PACE\u662f\u9996\u4e2a\u7528\u4e8eSNN\u548c\u4e8b\u4ef6\u89c6\u89c9\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684ST-DSM\u548cPEQ-N\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u5730\u538b\u7f29\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5feb\u901f\u7684SNN\u8bad\u7ec3\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u548c\u5b58\u50a8\u6210\u672c\uff0c\u4e3aSNN\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.12467", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12467", "abs": "https://arxiv.org/abs/2511.12467", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction", "comment": null, "summary": "This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u6761\u4ef6\u5206\u5e03\u7406\u8bba\u63a8\u5bfc\u51fa\u9884\u6d4b\u7b56\u7565\u7684\u7ebf\u6027\u51fd\u6570\u53c2\u6570\u5316\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6765\u5b66\u4e60\u8be5\u7b56\u7565\u3002", "result": "\u5206\u6790\u4e86\u8be5\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u6700\u4f18\u57fa\u4e8e\u6a21\u578b\u7684\u9884\u6d4b\u5668\u7684\u9057\u61be\u754c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u6b65\u8bbe\u7f6e\u4e0b\u5177\u6709\u5bf9\u6570\u9057\u61be\u754c\uff0c\u5e76\u5efa\u7acb\u4e86\u51e0\u4e4e\u786e\u5b9a\u7684\u9057\u61be\u754c\uff0c\u540c\u65f6\u53d1\u73b0\u9057\u61be\u754c\u7684\u5e38\u6570\u56e0\u5b50\u968f\u9884\u6d4b\u65f6\u754cH\u7684\u589e\u5927\u800c\u591a\u9879\u5f0f\u589e\u957f\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u5728\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u5bf9\u6570\u9057\u61be\u754c\uff0c\u5e76\u5bf9\u9057\u61be\u754c\u7684\u589e\u957f\u884c\u4e3a\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002"}}
{"id": "2511.13188", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13188", "abs": "https://arxiv.org/abs/2511.13188", "authors": ["Osama Al Sheikh Ali", "Sotiris Koutsoftas", "Ze Zhang", "Knut Akesson", "Emmanuel Dean"], "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "comment": "This paper has been accepted by IEEE SII 2026", "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff08AMR\uff09\u7684\u96c6\u6210\u5bfc\u822a\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u73af\u5883\u8868\u793a\u3001\u8f68\u8ff9\u751f\u6210\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3002", "motivation": "\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u5b89\u5168\u533a\u57df\u63d0\u53d6\u3001\u8fde\u901a\u6027\u56fe\u6784\u5efa\u3001\u8f68\u8ff9\u751f\u6210\u548cB\u6837\u6761\u5e73\u6ed1\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u5bfc\u822a\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u5bf9\u969c\u788d\u7269\u8fdb\u884c\u7f16\u7801\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u56db\u53c9\u6811\u7684\u65b9\u6cd5\u4ece\u5360\u7528\u56fe\u4e2d\u751f\u6210\u7ed3\u6784\u5316\u7684\u3001\u8f74\u5bf9\u9f50\u7684\u65e0\u78b0\u649e\u533a\u57df\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5b89\u5168\u8d70\u5eca\u7684\u57fa\u7840\u548cMPC\u516c\u5f0f\u4e2d\u7684\u7ebf\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u6301\u7eed\u7684\u6210\u529f\u548c\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u96c6\u6210\u5bfc\u822a\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5bfc\u822a\u3002"}}
{"id": "2511.13360", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13360", "abs": "https://arxiv.org/abs/2511.13360", "authors": ["Enrico Santamato", "Francesco De Martini"], "title": "The Intrinsic Angular - Momentum of Particles and the Resolution of the Spin-Statistics Theorem", "comment": null, "summary": "The traditional Standard Quantum Mechanics (SQM) theory is unable to solve the Spin-s problem, i.e., to justify the utterly important \"Pauli Exclusion Principle\". A complete and straightforward solution of the Spin-Statistics problem is presented based on the \"Weyl Integrable Quantum Mechanics\" (WIQM) theory. This theory provides a Weyl-gauge invariant formulation of the Standard Quantum Mechanics and reproduces successfully, with no restrictions, the full set of the quantum mechanical processes, including the formulation of Dirac's or Schr\u00f6dinger's equation, of Heisenberg's uncertainty relations, and of the nonlocal EPR correlations. etc. When the Weyl Integrable Quantum Mechanics is applied to a system made of many identical particles with spin, an additional constant property of all elementary particles enters naturally into play: the \"intrinsic helicity\", or the \"intrinsic angular - momentum\". This additional particle property, not considered by Standard Quantum Mechanics, determines the correct Spin-Statistics Connection (SSC) observed in Nature. All this leads to the consideration of a novel, most complete (in the EPR sense) quantum mechanical theory.", "AI": {"tldr": "WIQM\u7406\u8bba\u6210\u529f\u89e3\u51b3\u4e86SQM\u7406\u8bba\u65e0\u6cd5\u89e3\u51b3\u7684\u81ea\u65cb\u7edf\u8ba1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u91cf\u5b50\u529b\u5b66\u7406\u8bba\u3002", "motivation": "SQM\u7406\u8bba\u65e0\u6cd5\u89e3\u51b3\u81ea\u65cb\u7edf\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u4fdd\u5229\u4e0d\u76f8\u5bb9\u539f\u7406\u3002", "method": "\u63d0\u51faWIQM\u7406\u8bba\uff0c\u8be5\u7406\u8bba\u63d0\u4f9b\u4e86SQM\u7684Weyl\u89c4\u8303\u4e0d\u53d8\u8868\u8ff0\uff0c\u5e76\u81ea\u7136\u5f15\u5165\u4e86\u201c\u5185\u7980\u87ba\u65cb\u5ea6\u201d\u6216\u201c\u5185\u7980\u89d2\u52a8\u91cf\u201d\u7684\u6982\u5ff5\uff0c\u4ece\u800c\u89e3\u51b3\u4e86SSC\u95ee\u9898\u3002", "result": "WIQM\u7406\u8bba\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u65cb\u7edf\u8ba1\u95ee\u9898\uff0c\u5e76\u80fd\u91cd\u73b0SQM\u7684\u5404\u9879\u5185\u5bb9\uff0c\u5982\u859b\u5b9a\u8c14\u65b9\u7a0b\u3001\u6d77\u68ee\u5821\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u4ee5\u53caEPR\u5173\u8054\u7b49\u3002", "conclusion": "WIQM\u7406\u8bba\u662f\u4e00\u4e2a\u66f4\u5b8c\u6574\uff08\u5728EPR\u610f\u4e49\u4e0a\uff09\u7684\u91cf\u5b50\u529b\u5b66\u7406\u8bba\uff0c\u5b83\u8003\u8651\u4e86SQM\u5ffd\u7565\u7684\u7c92\u5b50\u5185\u7980\u5c5e\u6027\uff0c\u5e76\u6b63\u786e\u5730\u8fde\u63a5\u4e86\u81ea\u65cb\u548c\u7edf\u8ba1\u529b\u5b66\u3002"}}
{"id": "2511.12097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12097", "abs": "https://arxiv.org/abs/2511.12097", "authors": ["Shuhan Ye", "Yi Yu", "Qixin Zhang", "Chenqi Kong", "Qiangqiang Wu", "Xudong Jiang", "Dacheng Tao"], "title": "Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks", "comment": null, "summary": "Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \\emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \\emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \\textbf{SpikeNM}, the first SNN-oriented \\emph{semi-structured} \\(N{:}M\\) pruning framework that learns sparse SNNs \\emph{from scratch}, enforcing \\emph{at most \\(N\\)} non-zeros per \\(M\\)-weight block. To avoid the combinatorial space complexity \\(\\sum_{k=1}^{N}\\binom{M}{k}\\) growing exponentially with \\(M\\), SpikeNM adopts an \\(M\\)-way basis-logit parameterization with a differentiable top-\\(k\\) sampler, \\emph{linearizing} per-block complexity to \\(\\mathcal O(M)\\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \\emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \\(2{:}4\\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.", "AI": {"tldr": "SpikeNM\u662f\u4e00\u79cd\u65b0\u7684SNN\u526a\u679d\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u7a00\u758fSNN\uff0c\u5b9e\u73b0\u9ad8\u7a00\u758f\u5ea6\u548c\u786c\u4ef6\u53ef\u52a0\u901f\u6027\u3002", "motivation": "\u73b0\u6709\u7684SNN\u526a\u679d\u65b9\u6cd5\u8981\u4e48\u96be\u4ee5\u5728\u901a\u7528\u786c\u4ef6\u4e0a\u52a0\u901f\uff0c\u8981\u4e48\u5728\u5339\u914d\u7684\u7a00\u758f\u5ea6\u4e0b\u51c6\u786e\u7387\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u9ad8\u7a00\u758f\u5ea6\u4e14\u6613\u4e8e\u90e8\u7f72\u7684SNN\u526a\u679d\u65b9\u6cd5\u3002", "method": "SpikeNM\u662f\u4e00\u79cd\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\u6846\u67b6\uff0c\u91c7\u7528M\u8def\u57fa\u6570-logit\u53c2\u6570\u5316\u548c\u53ef\u5fae\u5206top-k\u91c7\u6837\u5668\uff0c\u5c06\u6bcf\u5757\u7684\u590d\u6742\u5ea6\u7ebf\u6027\u5316\u5230O(M)\uff0c\u5e76\u5f15\u5165\u4e86\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u201c\u5408\u683c\u6027\u542f\u53d1\u84b8\u998f\u201d\uff08EID\uff09\u6765\u7a33\u5b9a\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u57282:4\u7a00\u758f\u5ea6\u4e0b\uff0cSpikeNM\u5728\u51c6\u786e\u7387\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u6709\u6240\u63d0\u5347\uff0c\u5e76\u751f\u6210\u4e86\u6613\u4e8e\u786c\u4ef6\u52a0\u901f\u7684\u7a00\u758f\u6a21\u5f0f\u3002", "conclusion": "SpikeNM\u662f\u4e00\u79cd\u6709\u6548\u7684SNN\u534a\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u7a00\u758f\u5ea6\u3001\u786c\u4ef6\u53ef\u52a0\u901f\u6027\u548c\u826f\u597d\u7684\u51c6\u786e\u7387\uff0c\u4e3aSNN\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12603", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12603", "abs": "https://arxiv.org/abs/2511.12603", "authors": ["Hongyi Chen", "Jianhai Shu", "Jingtao Ding", "Yong Li", "Xiao-Ping Zhang"], "title": "PID-controlled Langevin Dynamics for Faster Sampling of Generative Models", "comment": "NeurIPS 2025 poster paper", "summary": "Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \\href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.", "AI": {"tldr": "PIDLD\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u52a0\u901f\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u539f\u7406\u6539\u8fdb\u4e86Langevin\u52a8\u529b\u5b66\u91c7\u6837\uff0c\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "Langevin\u52a8\u529b\u5b66\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u5230\u76ee\u6807\u5206\u5e03\u3002", "method": "PIDLD\u5c06\u80fd\u91cf\u68af\u5ea6\u89c6\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\uff08\u79ef\u5206\u9879\uff09\u548c\u68af\u5ea6\u8d8b\u52bf\uff08\u5fae\u5206\u9879\uff09\u6765\u52a0\u901f\u80fd\u91cf\u666f\u89c2\u7684\u904d\u5386\u548c\u81ea\u9002\u5e94\u7a33\u5b9a\u3002", "result": "PIDLD\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u91c7\u6837\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6837\u672c\u8d28\u91cf\uff0c\u51cf\u5c11\u4e86\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "PIDLD\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u6570\u636e\u96c6\u6216\u5148\u9a8c\u4fe1\u606f\u5373\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8eLangevin\u7684\u65b9\u6cd5\u96c6\u6210\u7684\u91c7\u6837\u52a0\u901f\u7b97\u6cd5\uff0c\u4f7f\u5f97\u57fa\u4e8eLangevin\u7684\u751f\u6210\u6a21\u578b\u5728\u6548\u7387\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u4e2d\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2511.13207", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13207", "abs": "https://arxiv.org/abs/2511.13207", "authors": ["Cheng Peng", "Zhenzhe Zhang", "Cheng Chi", "Xiaobao Wei", "Yanhao Zhang", "Heng Wang", "Pengwei Wang", "Zhongyuan Wang", "Jing Liu", "Shanghang Zhang"], "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection", "comment": null, "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.", "AI": {"tldr": "PIGEON\u662f\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u7269\u4f53\u5bfc\u822a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5229\u7528\u5174\u8da3\u70b9\uff08PoI\uff09\u6765\u63d0\u9ad8\u51b3\u7b56\u9891\u7387\u548c\u5bfc\u822a\u6548\u7387\uff0c\u5e76\u751f\u6210\u9002\u5408\u6a21\u62df\u5668\u7684RLVR\u6570\u636e\uff0c\u5728\u7ecf\u5178\u7269\u4f53\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bfc\u822a\u65b9\u6cd5\u5728\u51b3\u7b56\u9891\u7387\u548c\u667a\u80fd\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bfc\u81f4\u51b3\u7b56\u7f3a\u4e4f\u8fdc\u89c1\u6216\u52a8\u4f5c\u4e0d\u8fde\u7eed\u3002", "method": "\u63d0\u51faPIGEON\u65b9\u6cd5\uff0c\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u7ef4\u62a4\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u5feb\u7167\u8bb0\u5fc6\uff0c\u4f5c\u4e3a\u63a2\u7d22\u7b56\u7565\u7684\u8bed\u4e49\u8f93\u5165\u3002\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09PIGEON-VL\u9009\u62e9\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u5174\u8da3\u70b9\uff08PoI\uff09\uff0c\u5e76\u91c7\u7528\u4f4e\u7ea7\u89c4\u5212\u5668\u8f93\u51fa\u52a8\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u51b3\u7b56\u9891\u7387\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u591f\u751f\u6210\u9002\u7528\u4e8e\u6a21\u62df\u5668\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6570\u636e\u3002", "result": "\u5728\u7ecf\u5178\u7684\u7269\u4f53\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPIGEON\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002RLVR\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5b9e\u65f6\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u5f15\u5bfc\u80fd\u529b\u548c\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PIGEON\u901a\u8fc7\u5229\u7528VLM\u9009\u62e9\u548c\u5229\u7528\u5174\u8da3\u70b9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u548c\u667a\u80fd\u6027\uff0c\u5e76\u901a\u8fc7RLVR\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13375", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13375", "abs": "https://arxiv.org/abs/2511.13375", "authors": ["Nina Codreanu", "Tim Turan", "Daniel Bedialauneta Rodriguez", "Matteo Pasini", "Lorenzo de Santis", "Maximilian Ruf", "Christian F. Primavera", "Leonardo G. C. Wienhoven", "Caroline E. Smulders", "Simon Gr\u00f6blacher", "Ronald Hanson"], "title": "Above-Unity Coherent Cooperativity of Tin-Vacancy Centers in Diamond Photonic Crystal Cavities", "comment": "25 pages, 17 figures", "summary": "The tin-vacancy center in diamond (SnV) has emerged as a compelling building block for realizing next-generation quantum networks thanks to its excellent optical and spin properties. Coupling to photonic crystal cavities (PCCs) promises to further enhance the SnV light-matter interface and unlock a diverse range of entanglement generation protocols. Recent pioneering experiments showing Purcell enhancement of SnV centers in PCCs underscore this potential. However, optical coupling that is coherent - the key ingredient for use in quantum protocols - has so far remained elusive. Here, we demonstrate above-unity coherent cooperativity of SnV centers embedded in photonic crystal cavities. We fabricate free-standing PCCs using a quasi-isotropic undercut. Across two samples, we conduct room-temperature characterizations, measuring resonances for 327 cavities, with an average quality factor exceeding $Q = 1.0(3) \\times 10^4$. Two cavity-coupled emitters are examined in detail, exhibiting quality factors up to $Q = 25.4(4) \\times 10^3$ and Purcell-reduced lifetimes corresponding to cooperativities up to $C = 20.6(11)$. Furthermore, the single SnVs are observed to strongly modulate the cavity transmission with an extinction contrast up to $98.8(4) \\%$ on resonance. Finally, SnV linewidth measurements reveal above-unity coherent cooperativities in both devices, with the highest value being $C_\\mathrm{coh} = 8.3(12)$. These results open the door to using cavity-coupled SnV centers as efficient, coherent light-matter interfaces for future quantum networks.", "AI": {"tldr": "\u9521\u7a7a\u4f4d\uff08SnV\uff09\u4e2d\u5fc3\u4e0e\u5149\u5b50\u6676\u4f53\u8154\uff08PCC\uff09\u8026\u5408\uff0c\u5b9e\u73b0\u4e86\u76f8\u5e72\u534f\u5408\u5ea6\u9ad8\u4e8e1\u7684\u4f18\u5f02\u5149\u5b66\u548c\u81ea\u65cb\u7279\u6027\uff0c\u4e3a\u91cf\u5b50\u7f51\u7edc\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5229\u7528SnV\u4e2d\u5fc3\u4f18\u5f02\u7684\u5149\u5b66\u548c\u81ea\u65cb\u7279\u6027\uff0c\u901a\u8fc7\u4e0ePCC\u8026\u5408\u6765\u589e\u5f3a\u5176\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5b9e\u73b0\u7ea0\u7f20\u751f\u6210\u534f\u8bae\u3002", "method": "\u5236\u5907\u4e86\u81ea\u7531\u60ac\u6302\u7684PCC\uff0c\u5e76\u5728\u5ba4\u6e29\u4e0b\u5bf9327\u4e2a\u8154\u8fdb\u884c\u4e86\u8868\u5f81\uff0c\u6d4b\u91cf\u4e86\u5176\u8d28\u91cf\u56e0\u5b50\u3002\u9009\u53d6\u4e86\u4e24\u4e2a\u8154\u8026\u5408\u7684SnV\u53d1\u5c04\u4f53\u8fdb\u884c\u8be6\u7ec6\u7814\u7a76\uff0c\u6d4b\u91cf\u4e86\u5176\u8d28\u91cf\u56e0\u5b50\u3001Purcell\u56e0\u5b50\u964d\u4f4e\u7684\u5bff\u547d\u3001\u8154\u900f\u5c04\u7387\u7684\u8c03\u5236\u5bf9\u6bd4\u5ea6\u4ee5\u53ca\u76f8\u5e72\u534f\u5408\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u6837\u54c1\u4e2d\uff0cPCC\u7684\u5e73\u5747\u8d28\u91cf\u56e0\u5b50\u8d85\u8fc7\u4e861.0(3) x 10^4\u3002\u4e24\u4e2a\u8be6\u7ec6\u7814\u7a76\u7684\u8154\u8026\u5408SnV\u53d1\u5c04\u4f53\u7684\u8d28\u91cf\u56e0\u5b50\u9ad8\u8fbe25.4(4) x 10^3\uff0cPurcell\u56e0\u5b50\u964d\u4f4e\u7684\u5bff\u547d\u5bf9\u5e94\u7684\u534f\u5408\u5ea6\u9ad8\u8fbe20.6(11)\u3002SnV\u7684\u7ebf\u5bbd\u6d4b\u91cf\u663e\u793a\uff0c\u5728\u4e24\u4e2a\u5668\u4ef6\u4e2d\u90fd\u5b9e\u73b0\u4e86\u9ad8\u4e8e1\u7684\u76f8\u5e72\u534f\u5408\u5ea6\uff0c\u6700\u9ad8\u503c\u4e3a8.3(12)\u3002SnV\u5728\u8c10\u632f\u65f6\u8154\u900f\u5c04\u7387\u7684\u6d88\u5149\u6bd4\u9ad8\u8fbe98.8(4)%\u3002", "conclusion": "\u5b9e\u73b0\u4e86SnV\u4e2d\u5fc3\u4e0ePCC\u8026\u5408\u7684\u76f8\u5e72\u534f\u5408\u5ea6\u9ad8\u4e8e1\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u9ad8\u6548\u3001\u76f8\u5e72\u5149-\u7269\u8d28\u63a5\u53e3\u5728\u672a\u6765\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.12098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12098", "abs": "https://arxiv.org/abs/2511.12098", "authors": ["Xianhao Zhou", "Jianghao Wu", "Ku Zhao", "Jinlong He", "Huangxuan Zhao", "Lei Chen", "Shaoting Zhang", "Guotai Wang"], "title": "DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT", "comment": null, "summary": "Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\\rightarrow$CT and CBCT$\\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.", "AI": {"tldr": "DGCF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408DINOv3 Transformer\u548cCNN\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u7684\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u8bed\u4e49\u7406\u89e3\uff0c\u800cTransformer\u5bb9\u6613\u5728\u5c0f\u578b\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdDINOv3-Guided Cross Fusion (DGCF)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4e00\u4e2a\u51bb\u7ed3\u7684\u81ea\u76d1\u7763DINOv3 Transformer\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\u3002\u901a\u8fc7\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4ea4\u53c9\u878d\u5408\u6a21\u5757\uff0c\u5206\u5c42\u878d\u5408\u4e86Transformer\u7684\u5168\u5c40\u8868\u793a\u548cCNN\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u5c40\u90e8\u5916\u89c2\u548c\u4e0a\u4e0b\u6587\u8868\u793a\u7684\u5e73\u8861\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u5c42\u6b21DINOv3\u611f\u77e5(MLDP)\u635f\u5931\uff0c\u4ee5\u9f13\u52b1\u5728DINOv3\u7279\u5f81\u7a7a\u95f4\u4e2d\u5408\u6210CT\u4e0e\u771f\u5b9eCT\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728SynthRAD2023\u76c6\u8154\u6570\u636e\u96c6\u7684MRI\u2192CT\u548cCBCT\u2192CT\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0cDGCF\u5728MS-SSIM\u3001PSNR\u548c\u57fa\u4e8e\u5206\u5272\u7684\u6307\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DGCF\u6846\u67b6\u6210\u529f\u5730\u5229\u7528\u4e86DINOv3\u8868\u793a\u6765\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763Transformer\u5728\u8bed\u4e49\u611f\u77e5CT\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13216", "abs": "https://arxiv.org/abs/2511.13216", "authors": ["Chiyun Noh", "Sangwoo Jung", "Hanjun Kim", "Yafei Hu", "Laura Herlant", "Ayoung Kim"], "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry", "comment": null, "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO", "AI": {"tldr": "GaRLILEO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u96f7\u8fbe-\u817f\u90e8-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026IMU\u901f\u5ea6\u5e76\u5229\u7528\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u4e4b\u5916\u7684\u91cd\u529b\u5411\u91cf\u4f30\u8ba1\u6765\u63d0\u9ad8\u5782\u76f4\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u697c\u68af\u548c\u659c\u5761\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\u4e0a\u3002", "motivation": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\uff08\u4f8b\u5982\u697c\u68af\u3001\u659c\u5761\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\uff09\u4e2d\uff0c\u57fa\u4e8e\u817f\u90e8\u7684\u673a\u5668\u4eba\u6bd4\u57fa\u4e8e\u8f6e\u5f0f\u7684\u673a\u5668\u4eba\u66f4\u53d7\u6b22\u8fce\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u7684\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u662f\u7a33\u5b9a\u8fd0\u52a8\u3001\u5b9a\u4f4d\u548c\u6620\u5c04\u7684\u521d\u6b65\u8981\u6c42\u3002\u4f20\u7edf\u7684\u672c\u4f53\u611f\u89c9\u65b9\u6cd5\uff08\u4f9d\u8d56\u4e8e\u817f\u90e8\u8fd0\u52a8\u5b66\u548c\u60ef\u6027\u4f20\u611f\uff09\u4f1a\u53d7\u5230\u5782\u76f4\u6f02\u79fb\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u7ed3\u5408\u4e86\u6fc0\u5149\u96f7\u8fbe\u6216\u6444\u50cf\u5934\uff09\u5728\u7279\u5f81\u7a00\u758f\u6216\u91cd\u590d\u7684\u573a\u666f\u4e2d\u4f1a\u53d7\u5230\u9650\u5236\u3002", "method": "GaRLILEO\u6846\u67b6\u901a\u8fc7\u4ece\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u90e8\u8fd0\u52a8\u5b66\u4fe1\u606f\u4e2d\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u7684\u81ea\u8eab\u901f\u5ea6\u6837\u6761\u6765\u89e3\u8026IMU\u901f\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u4f20\u611f\u5668\u878d\u5408\uff0c\u51cf\u8f7b\u4e86\u91cc\u7a0b\u8ba1\u5931\u771f\u3002\u6b64\u5916\uff0cGaRLILEO\u5229\u7528\u65b0\u9896\u7684\u8f6fS2\u7ea6\u675f\u91cd\u529b\u56e0\u5b50\u6765\u6355\u6349\u51c6\u786e\u7684\u91cd\u529b\u5411\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5782\u76f4\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6fc0\u5149\u96f7\u8fbe\u6216\u6444\u50cf\u5934\u3002", "result": "\u5728\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cGaRLILEO\u5728\u5782\u76f4\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u697c\u68af\u548c\u659c\u5761\u4e0a\u3002", "conclusion": "GaRLILEO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u91cd\u529b\u5bf9\u9f50\u7684\u3001\u8fde\u7eed\u65f6\u95f4\u7684\u96f7\u8fbe-\u817f\u90e8-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u89e3\u8026IMU\u901f\u5ea6\u5e76\u5229\u7528\u91cd\u529b\u5411\u91cf\u4f30\u8ba1\u6765\u63d0\u9ad8\u5782\u76f4\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5373\u4f7f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\u4e0a\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002\u6240\u6536\u96c6\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u817f\u90e8\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u548cSLAM\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.13383", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13383", "abs": "https://arxiv.org/abs/2511.13383", "authors": ["Anumita Mukhopadhyay", "Shibdas Roy", "Arun Kumar Pati"], "title": "Efficient algorithm for fidelity estimation of two quantum states", "comment": "6 pages, 1 figure", "summary": "The fidelity estimation between two quantum states is crucial for quantum computation and information science. However, an efficacious method for this, especially for mixed states and higher-dimensional density matrices, remains elusive. While there are many existing algorithms on computing the fidelity between two pure states, there is not much work on how to obtain the fidelity between two mixed states. Here, we propose an efficient quantum algorithm for the fidelity estimation, based primarily on the density matrix exponentiation and interferometeric scheme for mixed states, with a time complexity of $O(N^2/\u03b5^7)$, where $N$ is the system size and $\u03b5$ is a precision error. Our algorithm may serve as a resource-efficient technique to deduce fidelity of any two (pure or mixed) unknown or known quantum states, when the density matrices of the quantum states commute with each other.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5b50\u7b97\u6cd5\u6765\u4f30\u7b97\u6df7\u5408\u6001\u91cf\u5b50\u6001\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6df7\u5408\u6001\u91cf\u5b50\u6001\u4fdd\u771f\u5ea6\u4f30\u7b97\u662f\u91cf\u5b50\u8ba1\u7b97\u548c\u4fe1\u606f\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6df7\u5408\u6001\u548c\u9ad8\u7ef4\u5bc6\u5ea6\u77e9\u9635\u65f6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u77e9\u9635\u6307\u6570\u5316\u548c\u5e72\u6d89\u6d4b\u91cf\u65b9\u6848\u7684\u9ad8\u6548\u91cf\u5b50\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6df7\u5408\u6001\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(N^2/\u03b5^7)\u3002", "result": "\u8be5\u7b97\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u6280\u672f\uff0c\u7528\u4e8e\u63a8\u65ad\u4efb\u610f\u4e24\u4e2a\uff08\u7eaf\u6001\u6216\u6df7\u5408\u6001\uff09\u672a\u77e5\u6216\u5df2\u77e5\u91cf\u5b50\u6001\u7684\u4fdd\u771f\u5ea6\uff0c\u524d\u63d0\u662f\u91cf\u5b50\u6001\u7684\u5bc6\u5ea6\u77e9\u9635\u76f8\u4e92\u4ea4\u6362\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e3a\u6df7\u5408\u6001\u91cf\u5b50\u6001\u4fdd\u771f\u5ea6\u7684\u4f30\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12099", "abs": "https://arxiv.org/abs/2511.12099", "authors": ["Tianle Cheng", "Zeyan Zhang", "Kaifeng Gao", "Jun Xiao"], "title": "Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.", "AI": {"tldr": "VDMs\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u957f\u89c6\u9891\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u3001\u8bef\u5dee\u7d2f\u79ef\u3001\u8106\u5f31\u4e00\u81f4\u6027\u53ca\u8fd0\u52a8\u52a8\u6001\u5dee\u7b49\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faAda-BOV\uff08Adaptive Begin-of-Video Tokens\uff09\u6765\u63d0\u5347\u957f\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5438\u6536\u5148\u524d\u5e27\u6765\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6d41\u5f0f\u53bb\u566a\u7684\u6539\u8fdb\u6765\u589e\u5f3a\u5c40\u90e8\u52a8\u6001\u548c\u6574\u4f53\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u9762\u4e34\u81ea\u56de\u5f52\u7684\u6311\u6218\uff0c\u5982\u5ef6\u8fdf\u3001\u8bef\u5dee\u7d2f\u79ef\u3001\u8106\u5f31\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u8fd0\u52a8\u52a8\u6001\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u8fde\u8d2f\u7684\u957f\u89c6\u9891\u3002", "method": "\u63d0\u51faAda-BOV\uff08Adaptive Begin-of-Video Tokens\uff09\u673a\u5236\uff0c\u901a\u8fc7\u7279\u6b8a\u7684\u5b66\u4e60\u5d4c\u5165\u81ea\u9002\u5e94\u5730\u5438\u6536\u5148\u524d\u53bb\u566a\u7684\u5e27\uff0c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u5e76\u5141\u8bb8\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7075\u6d3b\u8c03\u6574\u3002\u540c\u65f6\uff0c\u6539\u8fdb\u6d41\u5f0f\u53bb\u566a\u7b56\u7565\uff0c\u89e3\u8026\u91c7\u6837\u8f68\u8ff9\u957f\u5ea6\u4e0e\u6ce8\u610f\u529b\u7a97\u53e3\u5927\u5c0f\u7684\u9650\u5236\uff0c\u4ee5\u63d0\u5347\u5c40\u90e8\u52a8\u6001\u548c\u6574\u4f53\u6210\u50cf\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5f15\u5165\u6270\u52a8\u589e\u5f3a\u8bad\u7ec3\u566a\u58f0\u8c03\u5ea6\uff0c\u4ee5\u5e73\u8861\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u957f\u89c6\u9891\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Ada-BOV \u548c\u6539\u8fdb\u7684\u6d41\u5f0f\u53bb\u566a\u7b56\u7565\u80fd\u591f\u6709\u6548\u514b\u670d\u73b0\u6709\u81ea\u56de\u5f52VDMs\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u89c6\u9891\u751f\u6210\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3001\u5c40\u90e8\u52a8\u6001\u548c\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2511.12852", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12852", "abs": "https://arxiv.org/abs/2511.12852", "authors": ["Jihoon Moon"], "title": "From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability", "comment": null, "summary": "Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\uff0c\u4ee5\u5206\u6790\u5176\u5185\u90e8\u8ba1\u7b97\u8fc7\u7a0b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u96be\u4ee5\u4ece\u673a\u5236\u4e0a\u8fdb\u884c\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u5316\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u683c\u62c9\u59c6\u77e9\u9635\u4ee5\u53ca\u6c49\u514b\u5c14\u5947\u5f02\u503c\u6765\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u5316\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u8ba1\u7b97\u683c\u62c9\u59c6\u77e9\u9635\u548c\u6c49\u514b\u5c14\u5947\u5f02\u503c\u6765\u8bc4\u4f30\u795e\u7ecf\u5143\u548c\u901a\u8def\u7684\u91cd\u8981\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u795e\u7ecf\u5143\u548c\u901a\u8def\u7684\u91cd\u8981\u6027\uff0c\u5e76\u89e3\u91ca\u6fc0\u6d3b\u9971\u548c\u7b49\u73b0\u8c61\u5bf9\u7f51\u7edc\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6fc0\u6d3b\u9971\u548c\u4f1a\u964d\u4f4e\u53ef\u63a7\u6027\uff0c\u51cf\u5c0f\u4e3b\u5bfc\u6c49\u514b\u5c14\u5947\u5f02\u503c\uff0c\u5e76\u5c06\u4e3b\u5bfc\u5185\u90e8\u6a21\u5f0f\u8f6c\u79fb\u5230\u4e0d\u540c\u7684\u795e\u7ecf\u5143\u5b50\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u5c40\u90e8\u7684\u767d\u76d2\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u4e3a\u526a\u679d\u6216\u7ea6\u675f\u63d0\u4f9b\u5019\u9009\u65b9\u5411\uff0c\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.13312", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13312", "abs": "https://arxiv.org/abs/2511.13312", "authors": ["Jonas Bode", "Raphael Memmesheimer", "Sven Behnke"], "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation", "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026", "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4ee5\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u5e76\u6267\u884c\u7269\u7406\u4efb\u52a1\uff0c\u9700\u8981\u5c06\u5176\u89c6\u89c9\u548c\u6587\u672c\u7406\u89e3\u80fd\u529b\u7ed3\u5408\u8d77\u6765\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u786e\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u3002", "method": "\u5229\u7528\u53c2\u8003\u6f14\u793a\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7ed3\u5408\u6539\u8fdb\u7684\u5d4c\u5165\u548c\u501f\u9274\u56fe\u50cf\u751f\u6210\u7684\u6269\u6563\u6a21\u578b\u6280\u672f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6267\u884c\u6587\u672c\u6307\u4ee4\u6240\u6307\u5b9a\u7684\u64cd\u7eb5\u4efb\u52a1\u3002", "result": "\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u591a\u9879\u64cd\u7eb5\u4efb\u52a1\u548c\u957f\u5e8f\u5217\u4efb\u52a1\u7684\u6210\u529f\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u901a\u7528\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2511.13408", "categories": ["quant-ph", "cs.CC", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13408", "abs": "https://arxiv.org/abs/2511.13408", "authors": ["Zhenyu Chen", "Yuguo Shao", "Zhengwei Liu", "Zhaohui Wei"], "title": "Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility", "comment": null, "summary": "Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus\" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.", "AI": {"tldr": "\u901a\u8fc7\u5728\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def (PQC) \u4e2d\u63d2\u5165\u91cf\u5b50\u901a\u9053\u5c42\u6765\u6d88\u9664\u201c\u30d0\u30ec\u30f3\u9ad8\u539f\u201d\u73b0\u8c61\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u53c2\u6570\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQC\uff09\u67b6\u6784\u9762\u4e34\u201c\u30d0\u30ec\u30f3\u9ad8\u539f\u201d\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u635f\u5931\u51fd\u6570\u968f\u7cfb\u7edf\u89c4\u6a21\u6307\u6570\u589e\u957f\uff0c\u963b\u788d\u53c2\u6570\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u4e14\u786c\u4ef6\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4efb\u610f PQC \u4e2d\u63d2\u5165\u4e00\u5c42\u91cf\u5b50\u901a\u9053\uff08\u6bcf\u4e2a\u901a\u9053\u9700\u8981\u4e00\u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u548c\u56db\u4e2a\u989d\u5916\u7684\u95e8\uff09\u6765\u4fee\u6539 PQC\uff0c\u5f97\u5230 MPQC\u3002\u8be5\u65b9\u6cd5\u4fdd\u8bc1 MPQC \u7684\u8868\u8fbe\u80fd\u529b\u81f3\u5c11\u4e0e\u539f PQC \u76f8\u540c\uff0c\u5e76\u4e14\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u53ef\u6d88\u9664\u30d0\u30ec\u30f3\u9ad8\u539f\uff0c\u4f7f\u6240\u6709\u53c2\u6570\u53ef\u8bad\u7ec3\u3002", "result": "\u4fee\u6539\u540e\u7684 MPQC \u5728\u9ad8\u8fbe 100 \u4e2a\u91cf\u5b50\u6bd4\u7279\u548c 2400 \u4e2a\u5c42\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u6d88\u9664\u4e86\u30d0\u30ec\u30f3\u9ad8\u539f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u70ed\u6001\u5236\u5907\u7684 PQC\uff0c\u800c\u539f\u59cb PQC \u5219\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 MPQC \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6d88\u9664\u30d0\u30ec\u30f3\u9ad8\u539f\uff0c\u63d0\u9ad8\u53c2\u6570\u53ef\u8bad\u7ec3\u6027\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5f53\u524d\u7684 NISQ \u786c\u4ef6\u3002"}}
{"id": "2511.12100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12100", "abs": "https://arxiv.org/abs/2511.12100", "authors": ["Yannan Chen", "Ruoyu Chen", "Bin Zeng", "Wei Wang", "Shiming Liu", "Qunli Zhang", "Zheng Hu", "Laiyuan Wang", "Yaowei Wang", "Xiaochun Cao"], "title": "Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation", "comment": null, "summary": "In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.", "AI": {"tldr": "\u5f53\u524d\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u7684\u56e0\u679c\u7ebf\u7d22\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u63d0\u51faSS-CA\u65b9\u6cd5\uff0c\u7ed3\u5408LIMA\u5f52\u56e0\u548c\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u79fb\u9664\u5173\u952e\u533a\u57df\u5e76\u66ff\u6362\u4e3a\u80cc\u666f\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u6a21\u578b\u5728ID\u548cOOD\u6570\u636e\u4e0a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u7684\u56e0\u679c\u7ebf\u7d22\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u5206\u5e03\u53d8\u5316\u654f\u611f\uff0c\u4e14\u5c5e\u6027\u65b9\u6cd5\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u6837\u672c\u65e0\u6cd5\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSS-CA\u65b9\u6cd5\uff0c\u9996\u5148\u5229\u7528Counterfactual LIMA\u8bc6\u522b\u6700\u5c0f\u7684\u3001\u79fb\u9664\u540e\u53ef\u4ee5\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u7684\u7a7a\u95f4\u533a\u57df\u96c6\u5408\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u533a\u57df\u66ff\u6362\u4e3a\u81ea\u7136\u80cc\u666f\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\uff0c\u6700\u540e\u5c06\u589e\u5f3a\u540e\u7684\u6837\u672c\u4e0e\u539f\u59cb\u6837\u672c\u4e00\u8d77\u8bad\u7ec3\u6a21\u578b\u3002", "result": "SS-CA\u65b9\u6cd5\u5728\u591a\u4e2aImageNet\u53d8\u4f53\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728ID\u6d4b\u8bd5\u6570\u636e\u4e0a\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728ImageNet-R\u548cImageNet-S\u7b49OOD\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5728\u566a\u58f0\u7b49\u6270\u52a8\u4e0b\uff0cSS-CA\u8bad\u7ec3\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SS-CA\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u6765\u7ea0\u6b63\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13327", "abs": "https://arxiv.org/abs/2511.13327", "authors": ["Juntao Jian", "Yi-Lin Wei", "Chengjie Mou", "Yuhao Lin", "Xing Zhu", "Yujun Shen", "Wei-Shi Zheng", "Ruizhen Hu"], "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning", "comment": null, "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.", "AI": {"tldr": "ZeroDexGrasp\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u4efb\u52a1\u5bfc\u5411\u7684\u7075\u5de7\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6293\u53d6\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u7269\u4f53\u548c\u4efb\u52a1\u6307\u4ee4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u4e0d\u540c\u7269\u4f53\u548c\u4efb\u52a1\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "method": "ZeroDexGrasp\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u6765\u63a8\u65ad\u521d\u59cb\u6293\u53d6\u914d\u7f6e\u548c\u7269\u4f53\u63a5\u89e6\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u63a5\u89e6\u5f15\u5bfc\u7684\u6293\u53d6\u4f18\u5316\u6765\u5b8c\u5584\u6293\u53d6\u59ff\u6001\uff0c\u4ee5\u5b9e\u73b0\u7269\u7406\u53ef\u884c\u6027\u548c\u4efb\u52a1\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZeroDexGrasp\u80fd\u591f\u5728\u591a\u6837\u5316\u7684\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u548c\u590d\u6742\u4efb\u52a1\u9700\u6c42\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u7075\u5de7\u6293\u53d6\u3002", "conclusion": "ZeroDexGrasp\u5728\u5b9e\u73b0\u66f4\u5177\u6cdb\u5316\u6027\u548c\u667a\u80fd\u6027\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002"}}
{"id": "2511.13413", "categories": ["quant-ph", "math-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.13413", "abs": "https://arxiv.org/abs/2511.13413", "authors": ["Sara P. Gandelman", "Georgi Gary Rozenman"], "title": "Emulation of the Six-State Quantum Key Distribution Protocol with Pulsed Lasers", "comment": "13 pages, 5 figures", "summary": "Quantum cryptography remains a topic of enduring scientific and educational interest. Here, we present a clear and accessible framework for exploring the six-state quantum key distribution protocol, an enhanced three-basis extension of the BB84 scheme that combines optical experiments with computational analysis. Designed for testing quantum communication protocols through emulation, this approach provides a robust and cost-effective platform that highlights the fundamental principles of multi-basis encoding and demonstrates how experimental measurements connect directly to theoretical expectations in a controlled tabletop setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5149\u5b66\u5b9e\u9a8c\u548c\u8ba1\u7b97\u5206\u6790\u7684\u516d\u72b6\u6001\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u534f\u8bae\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u548c\u6d4b\u8bd5\u91cf\u5b50\u901a\u4fe1\u534f\u8bae\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u6613\u61c2\u7684\u6846\u67b6\u6765\u63a2\u7d22\u516d\u72b6\u6001\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u662fBB84\u65b9\u6848\u7684\u6269\u5c55\uff0c\u5e76\u7ed3\u5408\u4e86\u5149\u5b66\u5b9e\u9a8c\u548c\u8ba1\u7b97\u5206\u6790\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5149\u5b66\u5b9e\u9a8c\u548c\u8ba1\u7b97\u5206\u6790\u6765\u63a2\u7d22\u516d\u72b6\u6001\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u662fBB84\u65b9\u6848\u7684\u4e00\u4e2a\u4e09\u57fa\u6269\u5c55\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5065\u58ee\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u6d4b\u8bd5\u91cf\u5b50\u901a\u4fe1\u534f\u8bae\uff0c\u7a81\u51fa\u4e86\u591a\u57fa\u7f16\u7801\u7684\u57fa\u672c\u539f\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9a8c\u6d4b\u91cf\u5982\u4f55\u5728\u53d7\u63a7\u7684\u5b9e\u9a8c\u73af\u5883\u4e2d\u76f4\u63a5\u8fde\u63a5\u5230\u7406\u8bba\u9884\u671f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u6f14\u793a\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u7684\u539f\u7406\uff0c\u7279\u522b\u662f\u516d\u72b6\u6001\u534f\u8bae\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2511.12103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12103", "abs": "https://arxiv.org/abs/2511.12103", "authors": ["Sayad Ibna Azad", "Md. Atiqur Rahman"], "title": "BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation", "comment": "Accepted to 20th International Symposium on Visual Computing (ISVC 2025)", "summary": "We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.", "AI": {"tldr": "BdSL-SPOTER\u662f\u4e00\u4e2a\u57fa\u4e8e\u59ff\u6001\u7684Transformer\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9ad8\u6548\u5730\u8bc6\u522b\u5b5f\u52a0\u62c9\u624b\u8bed(BdSL)\u3002", "motivation": "\u4e3a\u51c6\u786e\u9ad8\u6548\u5730\u8bc6\u522b\u5b5f\u52a0\u62c9\u624b\u8bed\uff08BdSL\uff09\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u6587\u5316\u7279\u5b9a\u9884\u5904\u7406\u3001\u7d27\u51d1\u7684\u56db\u5c42Transformer\u7f16\u7801\u5668\u3001\u4f18\u5316\u7684\u53ef\u5b66\u4e60\u4f4d\u7f6e\u7f16\u7801\u548c\u8bfe\u7a0b\u5b66\u4e60\u6765\u6269\u5c55SPOTER\u8303\u4f8b\u3002", "result": "\u5728BdSLW60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBdSL-SPOTER\u5b9e\u73b0\u4e8697.92%\u7684Top-1\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u6bd4Bi-LSTM\u57fa\u7ebf\u63d0\u9ad8\u4e8622.82%\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002", "conclusion": " BdSL-SPOTER\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u8f85\u52a9\u529f\u80fd\u5e94\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u533a\u57df\u6027\u624b\u8bed\u7684\u53ef\u6269\u5c55\u6a21\u578b\u3002"}}
{"id": "2511.13186", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13186", "abs": "https://arxiv.org/abs/2511.13186", "authors": ["Akash Karthikeyan", "Yash Vardhan Pant"], "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play", "comment": "Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page: https://aku02.github.io/projects/difffp/", "summary": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\u03b5$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffFP\u7684\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u5b66\u4e60\u590d\u6742\u7b56\u7565\u9762\u4e34\u6311\u6218\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3001\u65e0\u6cd5\u8fbe\u5230\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5bb9\u6613\u88ab\u5bf9\u624b\u5229\u7528\u3002", "method": "\u63d0\u51faDiffFP\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u6765\u4f30\u8ba1\u5bf9\u672a\u77e5\u5bf9\u624b\u7684\u6700\u4f73\u54cd\u5e94\uff0c\u5e76\u5b66\u4e60\u9c81\u68d2\u4e14\u591a\u6a21\u6001\u7684\u884c\u4e3a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDiffFP\u6846\u67b6\u5728\u8fde\u7eed\u7a7a\u95f4\u96f6\u548c\u535a\u5f08\u4e2d\u53ef\u4ee5\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5728\u8d5b\u8f66\u548c\u591a\u7c92\u5b50\u96f6\u548c\u535a\u5f08\u7b49\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb3\u500d\uff0c\u6210\u529f\u7387\u5e73\u5747\u9ad830\u500d\u3002", "conclusion": "DiffFP\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\uff0c\u5728\u590d\u6742\u535a\u5f08\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.13459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13459", "abs": "https://arxiv.org/abs/2511.13459", "authors": ["Bingkun Huang", "Yuhe Gong", "Zewen Yang", "Tianyu Ren", "Luis Figueredo"], "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness", "comment": null, "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u8fd0\u52a8\u539f\u8bed\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7a7a\u95f4\u80fd\u91cf\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u901a\u5e38\u5728\u5173\u8282\u7a7a\u95f4\u4e2d\u5e94\u7528\uff0c\u5e76\u4e14\u4f9d\u8d56\u6709\u9650\u7684\u4efb\u52a1\u4fe1\u606f\u548c\u5bf93D\u73af\u5883\u7684\u90e8\u5206\u611f\u77e5\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4efb\u52a1\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u7684\u63a5\u89e6\u4e30\u5bcc\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u63a5\u89e6\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u4efb\u52a1\u7a7a\u95f4\u4e2d\u751f\u6210\u53ef\u9760\u4e14\u5b89\u5168\u8f68\u8ff9\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u7a7a\u95f4\u3001\u80fd\u91cf\u5b89\u5168\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u8fd0\u52a8\u539f\u8bed\u6765\u751f\u6210\u53ef\u9760\u4e14\u5b89\u5168\u7684\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u80fd\u91cf\u611f\u77e5\u7684\u7b1b\u5361\u5c14\u963b\u6297\u63a7\u5236\u76ee\u6807\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u5b89\u5168\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5904\u74063D\u73af\u5883\u4e2d\u5404\u79cd\u8868\u9762\u4e0a\u7684\u4efb\u52a1\u65f6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3001\u5e73\u6ed1\u7684\u8f68\u8ff9\u548c\u80fd\u91cf\u5b89\u5168\u4ea4\u4e92\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408\u4e86PPO\u548c\u8fd0\u52a8\u539f\u8bed\u7684\u4efb\u52a1\u7a7a\u95f4\u80fd\u91cf\u5b89\u5168\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u5b89\u5168\u6027\u3001\u5e73\u6ed1\u6027\u548c\u6210\u529f\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.13451", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13451", "abs": "https://arxiv.org/abs/2511.13451", "authors": ["Alice P. G. Hall", "Carlos H. S. Vieira", "Jonas F. G. Santos"], "title": "Probing parameters estimation with Gaussian non-commutative measurements", "comment": null, "summary": "Gaussian quantum states and channels are pivotal across many branches of quantum science and their applications, including the processing and storage of quantum information, the investigation of thermodynamics in the quantum regime, and quantum computation. The great advantage is that Gaussian states are experimentally accessible via their first and second statistical moments. In this work, we investigate parameter estimation for Gaussian states, in which the probe-state preparation stage involves two noncommutative Gaussian measurements on the position and momentum observables, introducing tunable parameters. The influence of these noncommutative Gaussian measurements is investigated through the quantum Fisher information (QFI). We showed that the QFI for characterizing Gaussian channels can be increased by adjusting the uncertainty parameters in the preparation of the probe state. Furthermore, if the probe is initially in a thermal state, probe-state preparation may generate quantum coherence in its energy basis. We showed that not only does the amount of coherence affect the improvement of the QFI, but also the rate of change of the coherence with respect to the parameter to be estimated. The proposed probe-state protocol is applied to two paradigmatic single-mode Gaussian channels, the attenuator and amplification channels, which are building blocks of Gaussian quantum information. Our results contribute to the use of coherence in quantum metrology and are experimentally feasible in quantum-optical devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u975e\u5bf9\u6613\u9ad8\u65af\u6d4b\u91cf\u6765\u589e\u5f3a\u9ad8\u65af\u91cf\u5b50\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50Fisher\u4fe1\u606f\uff08QFI\uff09\u8fdb\u884c\u4e86\u91cf\u5316\u3002", "motivation": "\u9ad8\u65af\u91cf\u5b50\u6001\u548c\u901a\u9053\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u3001\u91cf\u5b50\u70ed\u529b\u5b66\u548c\u91cf\u5b50\u8ba1\u7b97\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u5176\u53c2\u6570\u4f30\u8ba1\u7684\u4f18\u5316\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5305\u542b\u4e24\u4e2a\u975e\u5bf9\u6613\u9ad8\u65af\u6d4b\u91cf\u7684\u63a2\u9488\u6001\u5236\u5907\u65b9\u6848\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u65b9\u6848\u5bf9\u91cf\u5b50Fisher\u4fe1\u606f\uff08QFI\uff09\u7684\u5f71\u54cd\u3002\u4ed6\u4eec\u8fd8\u7814\u7a76\u4e86\u63a2\u9488\u6001\u521d\u6001\u4e3a\u70ed\u6001\u65f6\u4ea7\u751f\u7684\u91cf\u5b50\u76f8\u5e72\u6027\u5bf9QFI\u7684\u5f71\u54cd\u53ca\u5176\u53d8\u5316\u7387\u3002\u6700\u540e\uff0c\u5c06\u8be5\u65b9\u6848\u5e94\u7528\u4e8e\u8870\u51cf\u5668\u548c\u653e\u5927\u5668\u8fd9\u4e24\u79cd\u5355\u6a21\u9ad8\u65af\u901a\u9053\u3002", "result": "\u901a\u8fc7\u8c03\u6574\u63a2\u9488\u6001\u5236\u5907\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u53c2\u6570\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8868\u5f81\u9ad8\u65af\u91cf\u5b50\u4fe1\u9053\u7684QFI\u3002\u6b64\u5916\uff0c\u91cf\u5b50\u76f8\u5e72\u6027\u7684\u91cf\u548c\u5b83\u968f\u53c2\u6570\u7684\u53d8\u5316\u7387\u90fd\u5bf9QFI\u7684\u63d0\u5347\u6709\u8d21\u732e\u3002\u8be5\u65b9\u6848\u5728\u8870\u51cf\u5668\u548c\u653e\u5927\u5668\u901a\u9053\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u975e\u5bf9\u6613\u9ad8\u65af\u6d4b\u91cf\u548c\u91cf\u5b50\u76f8\u5e72\u6027\u6765\u63d0\u5347\u9ad8\u65af\u91cf\u5b50\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u7cbe\u5ea6\u7684\u65b9\u6cd5\uff0c\u4e3a\u91cf\u5b50\u8ba1\u91cf\u5b66\u4e2d\u7684\u76f8\u5e72\u6027\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5e76\u4e14\u5177\u6709\u5b9e\u9a8c\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12104", "abs": "https://arxiv.org/abs/2511.12104", "authors": ["Tammy Glazer", "Gilles Q. Hacheme", "Akram Zaytar", "Luana Marotti", "Amy Michaels", "Girmaw Abebe Tadesse", "Kevin White", "Rahul Dodhia", "Andrew Zolli", "Inbal Becker-Reshef", "Juan M. Lavista Ferres", "Caleb Robinson"], "title": "TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery", "comment": null, "summary": "We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.", "AI": {"tldr": "TEMPO\u662f\u4e00\u4e2a\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u751f\u6210\u7684\u5927\u89c4\u6a21\u3001\u65f6\u95f4\u89e3\u6790\u7684\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u6570\u636e\u96c6\uff0c\u8986\u76d62018\u5e74\u7b2c\u4e00\u5b63\u5ea6\u81f32025\u5e74\u7b2c\u4e8c\u5b63\u5ea6\uff0c\u5e76\u80fd\u6355\u6349\u5b63\u5ea6\u53d8\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u5f00\u53d1\u6a21\u5f0f\u548c\u6c14\u5019\u5f71\u54cd\u76d1\u6d4b\uff0c\u4ee5\u652f\u6301\u5168\u7403\u97e7\u6027\u548c\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u6355\u6349\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u7684\u65f6\u95f4\u89e3\u6790\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u73b0\u6709\u7684\u5efa\u7b51\u8db3\u8ff9\u548c\u9ad8\u5ea6\u6570\u636e\u4e0ePlanetScope\u536b\u661f\u56fe\u50cf\u76f8\u7ed3\u5408\uff0c\u8bad\u7ec3\u4e00\u4e2a\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee537.6\u7c73/\u50cf\u7d20\u7684\u5206\u8fa8\u7387\u9884\u6d4b\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u3002", "result": "\u751f\u6210\u4e862018\u5e74\u7b2c\u4e00\u5b63\u5ea6\u81f32025\u5e74\u7b2c\u4e8c\u5b63\u5ea6\u7684\u5168\u7403\u3001\u65f6\u95f4\u5e8f\u5217\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u56fe\u3002\u4e0e\u73b0\u6709\u5efa\u7b51\u8db3\u8ff9\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u663e\u793a\uff0cF1\u5206\u6570\u572885%\u81f388%\u4e4b\u95f4\uff0c\u4e94\u5e74\u8d8b\u52bf\u4e00\u81f4\u6027\u5f97\u5206\u4e3a0.96\u3002", "conclusion": "TEMPO\u6570\u636e\u96c6\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u6355\u6349\u5230\u5efa\u9020\u533a\u57df\u7684\u5b63\u5ea6\u53d8\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u76d1\u6d4b\u5f00\u53d1\u6a21\u5f0f\u548c\u6c14\u5019\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5bf9\u4e8e\u5168\u7403\u97e7\u6027\u548c\u9002\u5e94\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13530", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13530", "abs": "https://arxiv.org/abs/2511.13530", "authors": ["Vesna Poprcova", "Iulia Lefter", "Matthias Wieser", "Martijn Warnier", "Frances Brazier"], "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety", "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025", "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u7684\u534f\u8bae\uff0c\u65e8\u5728\u6a21\u62df\u793e\u4ea4\u7126\u8651\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u662f\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u72b6\u51b5\uff0c\u4f1a\u5f71\u54cd\u4eba\u9645\u4ea4\u5f80\u548c\u793e\u4ea4\u529f\u80fd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5305\u542b\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6765\u51c6\u786e\u68c0\u6d4b\u4e0e\u793e\u4ea4\u7126\u8651\u76f8\u5173\u7684\u72b6\u6001\u548c\u884c\u4e3a\uff0c\u8fd9\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u534f\u8bae\u5c06\u6536\u96c6\u81f3\u5c1170\u540d\u53c2\u4e0e\u8005\u7684\u540c\u6b65\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\u3002\u53c2\u4e0e\u8005\u5c06\u88ab\u5206\u4e3a\u4e0d\u540c\u7684\u793e\u4ea4\u7126\u8651\u6c34\u5e73\u7ec4\uff0c\u5e76\u5728\u53d7\u63a7\u7684\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u4e0eFurhat\u793e\u4ea4\u673a\u5668\u4eba\u8fdb\u884c\u5927\u7ea610\u5206\u949f\u7684\u4ea4\u4e92\u5f0f\u201c\u7eff\u91ce\u4ed9\u8e2a\u201d\u89d2\u8272\u626e\u6f14\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u6536\u96c6\u4e0a\u4e0b\u6587\u6570\u636e\u4ee5\u6df1\u5165\u4e86\u89e3\u4e2a\u4f53\u793e\u4ea4\u7126\u8651\u53cd\u5e94\u7684\u5dee\u5f02\u3002", "result": "\u8be5\u534f\u8bae\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u4fe1\u53f7\uff0c\u4ee5\u53ca\u4e0a\u4e0b\u6587\u6570\u636e\uff0c\u7528\u4e8e\u5206\u6790\u793e\u4ea4\u7126\u8651\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u652f\u6301\u793e\u4ea4\u7126\u8651\u7a33\u5065\u591a\u6a21\u6001\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u60c5\u611f\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u3002"}}
{"id": "2511.13475", "categories": ["quant-ph", "physics.ins-det", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.13475", "abs": "https://arxiv.org/abs/2511.13475", "authors": ["I. S. Kuijf", "F. B. Baalbergen", "L. Seldenthuis", "E. P. L. van Nieuwenburg", "M. J. A. de Dood"], "title": "Machine learning inspired photon number resolution in superconducting nanowire single-photon detectors", "comment": null, "summary": "Photon-number resolved detection with superconducting nanowire single-photon detectors (SNSPDs) attracts increasing interest, but lacks a systematic framework for interpreting and benchmarking this capability. In this work, we combine principal component analysis (PCA) with a new readout technique to explore the photon-number resolving capabilities of SNSPDs and find that the information of the photon number is contained in a single principal component which approximates the time derivative of the average response trace. We introduce a new confidence metric based on the Bhattacharyya coefficient to quantify the photon-number-resolving capabilities of a detector system and show that this metric can be used to compare different systems. Our analysis and interpretation of the principal components imply that photon-number resolution in SNSPDs can be achieved with moderate hardware requirements in terms of both sample rate (5 GSample/sec) and analog bandwidth (3 GHz) and could be implemented in an FPGA, giving a highly scalable solution for real-time photon counting.", "AI": {"tldr": "SNSPD\u5728\u5149\u5b50\u6570\u5206\u8fa8\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5f15\u5165\u4e86\u57fa\u4e8ePCA\u548cBhattacharyya\u7cfb\u6570\u7684\u65b0\u5206\u6790\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u5149\u5b50\u8ba1\u6570\u3002", "motivation": "SNSPD\u5728\u5149\u5b50\u6570\u5206\u8fa8\u65b9\u9762\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u89e3\u91ca\u548c\u57fa\u51c6\u5316\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u4e00\u79cd\u65b0\u7684\u8bfb\u51fa\u6280\u672f\u6765\u63a2\u7d22SNSPD\u7684\u5149\u5b50\u6570\u5206\u8fa8\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eBhattacharyya\u7cfb\u6570\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u6765\u91cf\u5316\u8be5\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u5149\u5b50\u6570\u4fe1\u606f\u5305\u542b\u5728\u4e00\u4e2a\u5355\u4e00\u4e3b\u6210\u5206\u4e2d\uff0c\u8be5\u6210\u5206\u8fd1\u4f3c\u4e8e\u5e73\u5747\u54cd\u5e94\u8ff9\u7ebf\u7684\u65f6\u95f4\u5bfc\u6570\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u660e\uff0c\u901a\u8fc7\u4e2d\u7b49\u7684\u786c\u4ef6\u8981\u6c42\uff085 GSample/sec\u91c7\u6837\u7387\u548c3 GHz\u6a21\u62df\u5e26\u5bbd\uff09\u5373\u53ef\u5b9e\u73b0SNSPD\u7684\u5149\u5b50\u6570\u5206\u8fa8\uff0c\u5e76\u53ef\u5728FPGA\u4e0a\u5b9e\u73b0\uff0c\u4ece\u800c\u4e3a\u5b9e\u65f6\u5149\u5b50\u8ba1\u6570\u63d0\u4f9b\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u548c\u65b9\u6cd5\u4e3aSNSPD\u7684\u5149\u5b50\u6570\u5206\u8fa8\u80fd\u529b\u63d0\u4f9b\u4e86\u91cf\u5316\u548c\u57fa\u51c6\u5316\u7684\u9014\u5f84\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u73b0\u8be5\u80fd\u529b\u7684\u786c\u4ef6\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12107", "abs": "https://arxiv.org/abs/2511.12107", "authors": ["Tianxiang Zhang", "Peipeng Yu", "Zhihua Xia", "Longchen Dai", "Xiaoyu Zhou", "Hui Gao"], "title": "Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection", "comment": "Accepted by AAAI 2026", "summary": "The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.", "AI": {"tldr": "DFF-Adapter\u662f\u4e00\u79cd\u7528\u4e8eDINOv2\u7684\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u53ef\u4ee5\u540c\u65f6\u8fdb\u884c\u771f\u5b9e\u6027\u68c0\u6d4b\u548c\u7cbe\u7ec6\u7684\u64cd\u7eb5\u7c7b\u578b\u5206\u7c7b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u4f18\u5316\u6765\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06DINOv2\u89c6\u4e3a\u901a\u7528\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u4f2a\u9020\u65b9\u6cd5\u4ea7\u751f\u7684\u72ec\u7279\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u4f2a\u9020\u7cbe\u7ec6\u9002\u914d\u5668\uff08DFF-Adapter\uff09\uff0c\u5b83\u5c06\u8f7b\u91cf\u7ea7\u591a\u5934LoRA\u6a21\u5757\u96c6\u6210\u5230DINOv2\u7684\u6bcf\u4e2aTransformer\u5757\u4e2d\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u5206\u652f\u6765\u4f20\u64ad\u7cbe\u7ec6\u64cd\u7eb5\u7ebf\u7d22\u5230\u771f\u5b9e\u6027\u5934\u90e8\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u534f\u540c\u4f18\u5316\u3002", "result": "DFF-Adapter\u4ec5\u4f7f\u7528350\u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u590d\u6742\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6c34\u5e73\u3002", "conclusion": "DFF-Adapter\u901a\u8fc7\u7cbe\u7ec6\u7684\u64cd\u7eb5\u7c7b\u578b\u5206\u7c7b\u589e\u5f3a\u4e86\u4f2a\u5f71\u654f\u611f\u6027\uff0c\u5229\u7528\u7279\u5b9a\u4e8e\u64cd\u7eb5\u7684\u77e5\u8bc6\u660e\u786e\u63d0\u9ad8\u4e86\u771f\u5b9e\u6027\u8fa8\u522b\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2511.13510", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13510", "abs": "https://arxiv.org/abs/2511.13510", "authors": ["Melanie Schaller", "Nick Janssen", "Bodo Rosenhahn"], "title": "Naga: Vedic Encoding for Deep State Space Models", "comment": "submitted to JMLR", "summary": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.", "AI": {"tldr": "Naga\u662f\u4e00\u79cd\u53d7\u5420\u9640\u6570\u5b66\u542f\u53d1\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u5904\u7406\u548c\u54c8\u8fbe\u739b\u79ef\u4ea4\u4e92\u6765\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u6a21\u578b\u6355\u6349\u8fdc\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNaga\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7f16\u7801\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53d7\u5420\u9640\u6570\u5b66\u7684\u7ed3\u6784\u542f\u53d1\uff0c\u901a\u8fc7\u8054\u5408\u5904\u7406\u524d\u5411\u548c\u65f6\u95f4\u53cd\u8f6c\u7684\u8f93\u5165\u5e8f\u5217\u6765\u5f15\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u53cc\u5411\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u9010\u5143\uff08\u54c8\u8fbe\u739b\uff09\u4ea4\u4e92\u7ed3\u5408\u8fd9\u4e9b\u8868\u793a\u3002", "result": "Naga\u5728\u591a\u4e2a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08LTSF\uff09\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecETTh1\u3001ETTh2\u3001ETTm1\u3001ETTm2\u3001Weather\u3001Traffic\u548cILI\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u8868\u73b0\u4f18\u4e8e28\u4e2a\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u7684\u57fa\u4e8eSSM\u7684\u6df1\u5ea6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u7387\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u3001\u53d7\u5420\u9640\u542f\u53d1\u7684\u5206\u89e3\u53ef\u4ee5\u4e3a\u957f\u7a0b\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.13707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13707", "abs": "https://arxiv.org/abs/2511.13707", "authors": ["Xiaoyu Liang", "Ziang Liu", "Kelvin Lin", "Edward Gu", "Ruolin Ye", "Tam Nguyen", "Cynthia Hsu", "Zhanxin Wu", "Xiaoman Yang", "Christy Sum Yu Cheung", "Harold Soh", "Katherine Dimitropoulou", "Tapomayukh Bhattacharjee"], "title": "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving", "comment": "IROS 2025", "summary": "We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.", "AI": {"tldr": "OpenRoboCare\u662f\u4e00\u4e2a\u5305\u542b\u4e13\u5bb6\u804c\u80fd\u6cbb\u7597\u5e08\u793a\u8303\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff08ADLs\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u673a\u5668\u4eba\u62a4\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7531\u4e13\u5bb6\u9a71\u52a8\u7684\u6570\u636e\u96c6\u6765\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u62a4\u7406\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u590d\u6742\u7684\u4eba\u673a\u7269\u7406\u4ea4\u4e92\u3001\u906e\u6321\u4e0b\u7684\u7cbe\u786e\u611f\u77e5\u3001\u5b89\u5168\u63a5\u89e6\u548c\u957f\u671f\u89c4\u5212\u3002", "method": "\u6536\u96c6\u4e8621\u540d\u804c\u80fd\u6cbb\u7597\u5e08\u5728\u4e24\u4e2a\u5047\u4eba\u4e0a\u6267\u884c15\u9879ADL\u4efb\u52a1\u7684\u6570\u636e\u3002\u6570\u636e\u96c6\u5305\u542bRGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ffd\u8e2a\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4efb\u52a1\u548c\u52a8\u4f5c\u6ce8\u89e3\u4ee5\u53ca\u89e6\u89c9\u4f20\u611f\u4e94\u79cd\u6a21\u6001\u3002", "result": "OpenRoboCare\u5305\u542b\u4e94\u79cd\u6a21\u6001\u7684\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u62a4\u7406\u8005\u8fd0\u52a8\u3001\u6ce8\u610f\u529b\u3001\u529b\u5e94\u7528\u548c\u4efb\u52a1\u6267\u884c\u7b56\u7565\u7684\u4e30\u5bcc\u89c1\u89e3\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u4e13\u5bb6\u62a4\u7406\u539f\u5219\u548c\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u73b0\u6709\u673a\u5668\u4eba\u611f\u77e5\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u7684\u6311\u6218\u6027\u3002", "conclusion": "OpenRoboCare\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u62a4\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u5176\u591a\u6837\u5316\u7684\u6570\u636e\u548c\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5b89\u5168\u3001\u81ea\u9002\u5e94\u7684\u8f85\u52a9\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.13485", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2511.13485", "abs": "https://arxiv.org/abs/2511.13485", "authors": ["Ilias Magoulas", "Francesco A. Evangelista"], "title": "Spin-Adapted Fermionic Unitaries: From Lie Algebras to Compact Quantum Circuits", "comment": null, "summary": "Conservation of symmetries plays a crucial role in both classical and quantum simulations of many-body systems, enabling the tracking of states with specific symmetry properties and leading to substantial reductions in the number of optimization parameters. The design of efficient quantum circuits that enforce all symmetries typically encountered in chemistry has remained elusive, mainly due to the interplay of point group and spin symmetries. By exploiting Lie algebraic techniques, we derive exact product formulas representing symmetry-adapted unitaries. These decompositions allow us to design the most efficient symmetry-preserving quantum circuits to date. Finally, we introduce a minimum universal symmetry-adapted operator pool to further reduce the required quantum resources.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u674e\u4ee3\u6570\u6280\u672f\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u8868\u793a\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u9149\u53d8\u6362\u7684\u7cbe\u786e\u4e58\u79ef\u516c\u5f0f\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u8fc4\u4eca\u4e3a\u6b62\u6700\u9ad8\u6548\u7684\u4fdd\u6301\u5bf9\u79f0\u6027\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u5f15\u5165\u4e86\u6700\u5c0f\u901a\u7528\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u7b97\u7b26\u6c60\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u6240\u9700\u7684\u91cf\u5b50\u8d44\u6e90\u3002", "motivation": "\u4e3a\u4e86\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u6a21\u62df\u591a\u4f53\u7cfb\u7edf\u4e2d\u6709\u6548\u5904\u7406\u5bf9\u79f0\u6027\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u5f3a\u5236\u6267\u884c\u5316\u5b66\u4e2d\u6240\u6709\u5bf9\u79f0\u6027\u7684\u9ad8\u6548\u91cf\u5b50\u7535\u8def\uff0c\u4f46\u8fd9\u56e0\u70b9\u7fa4\u548c\u81ea\u65cb\u5bf9\u79f0\u6027\u7684\u76f8\u4e92\u4f5c\u7528\u800c\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u5229\u7528\u674e\u4ee3\u6570\u6280\u672f\u63a8\u5bfc\u51fa\u8868\u793a\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u9149\u53d8\u6362\u7684\u7cbe\u786e\u4e58\u79ef\u516c\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4fdd\u6301\u5bf9\u79f0\u6027\u7684\u91cf\u5b50\u7535\u8def\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u6700\u5c0f\u901a\u7528\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u7b97\u7b26\u6c60\u3002", "result": "\u6211\u4eec\u8bbe\u8ba1\u51fa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u9ad8\u6548\u7684\u4fdd\u6301\u5bf9\u79f0\u6027\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u6700\u5c0f\u901a\u7528\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u7b97\u7b26\u6c60\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u6240\u9700\u7684\u91cf\u5b50\u8d44\u6e90\u3002", "conclusion": "\u674e\u4ee3\u6570\u6280\u672f\u53ef\u7528\u4e8e\u8bbe\u8ba1\u9ad8\u6548\u7684\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u80fd\u901a\u8fc7\u6700\u5c0f\u901a\u7528\u5bf9\u79f0\u6027\u81ea\u9002\u5e94\u7b97\u7b26\u6c60\u8fdb\u4e00\u6b65\u4f18\u5316\u91cf\u5b50\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2511.12110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12110", "abs": "https://arxiv.org/abs/2511.12110", "authors": ["Qinyue Tong", "Ziqian Lu", "Jun Liu", "Rui Zuo", "Zheming Lu"], "title": "MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images", "comment": "12pages, 6 figures", "summary": "Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.", "AI": {"tldr": "MEMR-Seg\u662f\u4e00\u4e2a\u65b0\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u8f6e\u67e5\u8be2\u548c\u5b9e\u4f53\u7ea7\u63a8\u7406\u751f\u6210\u5206\u5272\u63a9\u7801\u3002MR-MedSeg\u662f\u652f\u6301\u8be5\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b17.7\u4e07\u4e2a\u591a\u8f6e\u533b\u5b66\u5206\u5272\u5bf9\u8bdd\u3002MediRound\u662f\u63d0\u51fa\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u5224\u65ad\u4e0e\u7ea0\u6b63\u673a\u5236\u4ee5\u7f13\u89e3\u8bef\u5dee\u4f20\u64ad\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3002\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u65b9\u6cd5\u867d\u7136\u589e\u5f3a\u4e86\u7528\u6237\u9a71\u52a8\u548c\u63a8\u7406\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u8f6e\u5bf9\u8bdd\uff0c\u65e0\u6cd5\u8fdb\u884c\u591a\u8f6e\u63a8\u7406\u3002", "method": "\u63d0\u51faMEMR-Seg\u4efb\u52a1\uff0c\u6784\u5efaMR-MedSeg\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faMediRound\u6a21\u578b\uff0c\u5305\u542b\u5224\u65ad\u4e0e\u7ea0\u6b63\u673a\u5236\u4ee5\u7f13\u89e3\u8bef\u5dee\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406MEMR-Seg\u4efb\u52a1\uff0c\u5e76\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684\u533b\u5b66\u6307\u4ee3\u8868\u8fbe\u5f0f\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "MEMR-Seg\u4efb\u52a1\u548cMR-MedSeg\u6570\u636e\u96c6\u4e3a\u591a\u8f6e\u63a8\u7406\u533b\u5b66\u5206\u5272\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u6240\u63d0\u51fa\u7684MediRound\u6a21\u578b\u53ca\u5176\u5224\u65ad\u4e0e\u7ea0\u6b63\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u8be5\u4efb\u52a1\u3002"}}
{"id": "2511.13710", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13710", "abs": "https://arxiv.org/abs/2511.13710", "authors": ["Jianglong Ye", "Lai Wei", "Guangqi Jiang", "Changwei Jing", "Xueyan Zou", "Xiaolong Wang"], "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands", "comment": "Project page: https://jianglongye.com/power-to-precision", "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7075\u5de7\u624b\u5c16\u90e8\u51e0\u4f55\u5f62\u72b6\u5e76\u4f18\u5316\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u7075\u5de7\u624b\u540c\u65f6\u8fdb\u884c\u529b\u91cf\u6293\u63e1\u548c\u7cbe\u786e\u6293\u63e1\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6307\u7075\u5de7\u624b\u673a\u5668\u4eba\u64c5\u957f\u529b\u91cf\u6293\u63e1\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u64cd\u63a7\u7684\u4efb\u52a1\u4e2d\uff0c\u5e73\u884c\u5939\u722a\u4ecd\u66f4\u5e38\u7528\uff0c\u8fd9\u51f8\u663e\u4e86\u73b0\u6709\u673a\u5668\u4eba\u8bbe\u8ba1\u5728\u5355\u4e00\u7cfb\u7edf\u4e2d\u96be\u4ee5\u517c\u987e\u7a33\u5b9a\u529b\u91cf\u6293\u63e1\u548c\u7cbe\u7ec6\u64cd\u4f5c\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6307\u5c16\u51e0\u4f55\u5f62\u72b6\u4fee\u6539\u65b9\u6848\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u63a5\u89e6\u5e73\u9762\uff0c\u5e76\u4e0e\u76f8\u5e94\u7684\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u63a7\u5236\u7b56\u7565\u80fd\u5728\u529b\u91cf\u6293\u63e1\u548c\u7cbe\u786e\u6293\u63e1\u4e4b\u95f4\u52a8\u6001\u5207\u6362\uff0c\u5e76\u5c06\u7cbe\u786e\u63a7\u5236\u7b80\u5316\u4e3a\u62c7\u6307\u548c\u98df\u6307\u7684\u5e73\u884c\u8fd0\u52a8\uff0c\u4ee5\u5b9e\u73b0\u826f\u597d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002\u5229\u7528\u53ef\u5fae\u5206\u795e\u7ecf\u7269\u7406\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4eff\u771f\u4f18\u5316\u6307\u5c16\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7cbe\u786e\u6293\u63e1\u4efb\u52a1\u4e2d\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u73b0\u4e8682.5%\u7684\u96f6\u6837\u672c\u6210\u529f\u7387\uff1b\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u9762\u5305\u634f\u53d6\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u523093.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6307\u7075\u5de7\u624b\u7684\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5176\u8fdb\u884c\u529b\u91cf\u6293\u63e1\u7684\u80fd\u529b\u3002"}}
{"id": "2511.13520", "categories": ["quant-ph", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13520", "abs": "https://arxiv.org/abs/2511.13520", "authors": ["Maja Franz", "Lukas Schmidbauer", "Joshua Ammermann", "Ina Schaefer", "Wolfgang Mauerer"], "title": "Towards Quantum Software for Quantum Simulation", "comment": null, "summary": "Quantum simulation is a leading candidate for demonstrating practical quantum advantage over classical computation, as it is believed to provide exponentially more compute power than any classical system. It offers new means of studying the behaviour of complex physical systems, for which conventionally software-intensive simulation codes based on numerical high-performance computing are used. Instead, quantum simulations map properties and characteristics of subject systems, for instance chemical molecules, onto quantum devices that then mimic the system under study.\n  Currently, the use of these techniques is largely limited to fundamental science, as the overall approach remains tailored for specific problems: We lack infrastructure and modelling abstractions that are provided by the software engineering community for other computational domains.\n  In this paper, we identify critical gaps in the quantum simulation software stack-particularly the absence of general-purpose frameworks for model specification, Hamiltonian construction, and hardware-aware mappings. We advocate for a modular model-driven engineering (MDE) approach that supports different types of quantum simulation (digital and analogue), and facilitates automation, performance evaluation, and reusability. Through an example from high-energy physics, we outline a vision for a quantum simulation framework capable of supporting scalable, cross-platform simulation workflows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u91cf\u5b50\u6a21\u62df\u8f6f\u4ef6\u6808\u4e2d\u5b58\u5728\u7684\u5173\u952e\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u53ef\u6269\u5c55\u3001\u8de8\u5e73\u53f0\u7684\u91cf\u5b50\u6a21\u62df\u5de5\u4f5c\u6d41\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u7684\u91cf\u5b50\u6a21\u62df\u6280\u672f\u4e3b\u8981\u5c40\u9650\u4e8e\u57fa\u7840\u79d1\u5b66\u7814\u7a76\uff0c\u7f3a\u4e4f\u901a\u7528\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u5efa\u6a21\u62bd\u8c61\uff0c\u7279\u522b\u662f\u6a21\u578b\u89c4\u8303\u3001\u54c8\u5bc6\u987f\u91cf\u6784\u5efa\u548c\u786c\u4ef6\u611f\u77e5\u6620\u5c04\u65b9\u9762\u7684\u901a\u7528\u6846\u67b6\u3002\u8fd9\u963b\u788d\u4e86\u91cf\u5b50\u6a21\u62df\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u5021\u5bfc\u91c7\u7528\u6a21\u5757\u5316\u7684\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u65b9\u6cd5\uff0c\u652f\u6301\u4e0d\u540c\u7c7b\u578b\u7684\u91cf\u5b50\u6a21\u62df\uff08\u6570\u5b57\u548c\u6a21\u62df\uff09\uff0c\u5e76\u4fc3\u8fdb\u81ea\u52a8\u5316\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u53ef\u91cd\u7528\u6027\u3002\u901a\u8fc7\u4e00\u4e2a\u9ad8\u80fd\u7269\u7406\u5b66\u7684\u4f8b\u5b50\u6765\u8bf4\u660e\u8be5\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "result": "\u8bc6\u522b\u51fa\u91cf\u5b50\u6a21\u62df\u8f6f\u4ef6\u6808\u4e2d\u6a21\u578b\u89c4\u8303\u3001\u54c8\u5bc6\u987f\u91cf\u6784\u5efa\u548c\u786c\u4ef6\u611f\u77e5\u6620\u5c04\u65b9\u9762\u7684\u5173\u952e\u7f3a\u5931\u3002\u63d0\u51fa\u4e86\u4e00\u79cdMDE\u65b9\u6cd5\uff0c\u5e76\u52fe\u753b\u4e86\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u53ef\u6269\u5c55\u3001\u8de8\u5e73\u53f0\u6a21\u62df\u5de5\u4f5c\u6d41\u7684\u91cf\u5b50\u6a21\u62df\u6846\u67b6\u613f\u666f\u3002", "conclusion": "\u6a21\u5757\u5316\u7684\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u65b9\u6cd5\u662f\u5f25\u5408\u5f53\u524d\u91cf\u5b50\u6a21\u62df\u8f6f\u4ef6\u6808\u5dee\u8ddd\u3001\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u8de8\u5e73\u53f0\u6a21\u62df\u5de5\u4f5c\u6d41\u7684\u5173\u952e\u3002"}}
{"id": "2511.12117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12117", "abs": "https://arxiv.org/abs/2511.12117", "authors": ["Ruiqi Cheng", "Huijun Di", "Jian Li", "Feng Liu", "Wei Liang"], "title": "RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving", "comment": "12 pages, 6 figures. Accepted by AAAI 2026", "summary": "Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.", "AI": {"tldr": "RadarMP\u662f\u4e00\u79cd\u7528\u4e8e\u7cbe\u786e3D\u573a\u666f\u8fd0\u52a8\u611f\u77e5\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u8fde\u7eed\u4e24\u5e27\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u56de\u6ce2\u4fe1\u53f7\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u76ee\u6807\u68c0\u6d4b\u548c\u8fd0\u52a8\u4f30\u8ba1\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u70b9\u4e91\u751f\u6210\u548c3D\u573a\u666f\u6d41\u9884\u6d4b\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u591a\u666e\u52d2\u9891\u79fb\u548c\u56de\u6ce2\u5f3a\u5ea6\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\uff0c\u5728\u5404\u79cd\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u5168\u5929\u5019\u81ea\u52a8\u9a7e\u9a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u4e14\u6709\u566a\u58f0\uff0c\u5bfc\u81f4\u8fd0\u52a8\u611f\u77e5\u4e0d\u7cbe\u786e\uff0c\u5c24\u5176\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u5f71\u54cd\u66f4\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u548c\u8fd0\u52a8\u4f30\u8ba1\u5206\u5f00\u5904\u7406\uff0c\u9650\u5236\u4e86\u611f\u77e5\u80fd\u529b\u3002", "method": "RadarMP\u8054\u5408\u5efa\u6a21\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u548c\u8fd0\u52a8\u4f30\u8ba1\u4efb\u52a1\uff0c\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u67b6\u6784\u4e2d\u5b9e\u73b0\u70b9\u4e91\u751f\u6210\u548c3D\u573a\u666f\u6d41\u9884\u6d4b\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u591a\u666e\u52d2\u9891\u79fb\u548c\u56de\u6ce2\u5f3a\u5ea6\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u76d1\u7763\u7a7a\u95f4\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u3002", "result": "RadarMP\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u5747\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u8fd0\u52a8\u611f\u77e5\uff0c\u5e76\u4e14\u4f18\u4e8e\u57fa\u4e8e\u96f7\u8fbe\u89e3\u8026\u8fd0\u52a8\u611f\u77e5\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RadarMP\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u548c\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e864D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u8fd0\u52a8\u611f\u77e5\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5168\u573a\u666f\u4e0b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2511.11703", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11703", "abs": "https://arxiv.org/abs/2511.11703", "authors": ["Hugo Huang"], "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom", "comment": "Master's Thesis at the University of Edinburgh (2024)", "summary": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u8f93\u5165\u8868\u793a\uff08SS-only \u548c RGB+SS\uff09\uff0c\u5728\u9ad8\u7ef4 3D \u73af\u5883\u4e2d\u964d\u4f4e\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4 3D \u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u56e0\u5185\u5b58\u7f13\u51b2\u533a\u5bfc\u81f4\u7684\u5185\u5b58\u6d88\u8017\u9ad8\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u5b66\u4e60\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u8f93\u5165\u8868\u793a\uff1aSS-only \u548c RGB+SS\uff0c\u5747\u91c7\u7528 RGB \u5f69\u8272\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u3002\u5728 ViZDoom \u7684\u6b7b\u4ea1\u7ade\u8d5b\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u5bc6\u5ea6\u70ed\u529b\u56fe\u53ef\u89c6\u5316 RL \u4ee3\u7406\u7684\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "SS-only \u5c06\u5185\u5b58\u7f13\u51b2\u533a\u5185\u5b58\u6d88\u8017\u81f3\u5c11\u51cf\u5c11\u4e86 66.6%\uff0c\u5e94\u7528\u6e38\u7a0b\u957f\u5ea6\u7f16\u7801\u540e\u53ef\u8fbe 98.6%\u3002RGB+SS \u901a\u8fc7\u589e\u52a0\u7684\u8bed\u4e49\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86 RL \u4ee3\u7406\u7684\u6027\u80fd\u3002\u5bc6\u5ea6\u70ed\u529b\u56fe\u53ef\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u6536\u96c6\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728 ViZDoom \u7b49 3D \u73af\u5883\u4e2d\u5e94\u7528\u8bed\u4e49\u5206\u5272\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u5728\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u548c\u63d0\u9ad8 RL \u4ee3\u7406\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002"}}
{"id": "2511.13531", "categories": ["quant-ph", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.13531", "abs": "https://arxiv.org/abs/2511.13531", "authors": ["Zhen-Peng Xu", "Jie Wang", "Qi Ye", "Gereon Ko\u00dfmann", "Ren\u00e9 Schwonnek", "Andreas Winter"], "title": "Simultaneous variances of Pauli strings, weighted independence numbers, and a new kind of perfection of graphs", "comment": "18+22 pages, comments are highly welcome", "summary": "A set of Pauli stings is well characterized by the graph that encodes its commutatitivity structure, i.e., by its frustration graph. This graph provides a natural interface between graph theory and quantum information, which we explore in this work. We investigate all aspects of this interface for a special class of graphs that bears tight connections between the groundstate structures of a spin systems and topological structure of a graph. We call this class $\\hbar$-perfect, as it extends the class of perfect and $h$-perfect graphs.\n  Having an $\\hbar$-perfect graph opens up several applications: we find efficient schemes for entanglement detection, a connection to the complexity of shadow tomography, tight uncertainty relations and a construction for computing good lower on bounds ground state energies. Conversely this also induces quantum algorithms for computing the independence number. Albeit those algorithms do not immediately promise an advantage in runtime, we show that an approximate Hamilton encoding of the independence number can be achieved with an amount of qubits that typically scales logarithmically in the number of vertices. We also we also determine the behavior of $\\hbar$-perfectness under basic graph operations and evaluate their prevalence among all graphs.", "AI": {"tldr": "Pauli strings can be analyzed using frustration graphs, which connect graph theory and quantum information. This paper introduces a new class of graphs called 'hbar-perfect' graphs, which have connections to spin systems and graph topology. These graphs enable efficient entanglement detection, shadow tomography, uncertainty relations, and lower bounds for ground state energies. They also lead to quantum algorithms for computing the independence number, with a logarithmic qubit requirement. The paper also studies how 'hbar-perfectness' behaves under graph operations and how common these graphs are.", "motivation": "The motivation is to explore the interface between graph theory and quantum information using Pauli strings and their commutativity structure encoded by frustration graphs. Specifically, the paper introduces and investigates a new class of graphs called 'hbar-perfect' graphs that have tight connections to spin systems and graph topology.", "method": "The paper investigates the interface between graph theory and quantum information using frustration graphs for Pauli strings. It introduces and defines a new class of graphs called 'hbar-perfect' graphs, which extend perfect and h-perfect graphs. The study explores applications of these graphs in entanglement detection, shadow tomography, uncertainty relations, and computing ground state energies. It also develops quantum algorithms for computing the independence number and analyzes the properties of 'hbar-perfect' graphs under graph operations.", "result": "The paper establishes 'hbar-perfect' graphs as a class with applications in efficient entanglement detection, shadow tomography, uncertainty relations, and lower bounds for ground state energies. It also presents quantum algorithms for computing the independence number, demonstrating a logarithmic qubit scaling. The behavior of 'hbar-perfectness' under graph operations and their prevalence are also determined.", "conclusion": "The introduction of 'hbar-perfect' graphs provides a valuable framework connecting graph theory and quantum information, leading to practical applications in quantum information processing and algorithms. The study deepens the understanding of this interface and suggests potential avenues for future research in quantum algorithms and complexity."}}
{"id": "2511.12131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12131", "abs": "https://arxiv.org/abs/2511.12131", "authors": ["Quanxing Xu", "Ling Zhou", "Feifei Zhang", "Jinyu Tian", "Rubing Huang"], "title": "OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description", "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.", "AI": {"tldr": "LLM\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u9898\u65f6\u4f1a\u7ee7\u627f\u8bed\u8a00\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u53ef\u9760\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u3002\u6211\u4eec\u63d0\u51fa\u4e86OAD-Promoter\uff0c\u5305\u542bOEG\u3001MKA\u548cOAD Prompt\u4e09\u4e2a\u6a21\u5757\uff0c\u7528\u4e8e\u51cf\u8f7b\u8bed\u8a00\u504f\u89c1\u548c\u63d0\u9ad8\u9886\u57df\u8fc1\u79fb\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOAD-Promoter\u5728\u5c11\u6837\u672c\u6216\u96f6\u6837\u672cVQA\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u9898\u65f6\uff0c\u4f1a\u7ee7\u627f\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8bed\u8a00\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u53ef\u9760\uff0c\u5e76\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5206\u5e03\u5916\uff08OOD\uff09\u7684\u6570\u636e\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5229\u7528LLM\u5904\u7406VQA\u95ee\u9898\u65f6\uff0c\u9762\u4e34\u7740\u504f\u89c1\u5229\u7528\u548cOOD\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86OAD-Promoter\uff0c\u5b83\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1. \u5bf9\u8c61\u96c6\u4e2d\u793a\u4f8b\u751f\u6210\uff08OEG\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u751f\u6210\u5168\u5c40\u6807\u9898\u548c\u5bf9\u8c61\u96c6\u4e2d\u6837\u672c\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u533a\u57df\u89c6\u89c9\u7ebf\u7d22\u7684\u4e92\u8865\u6765\u589e\u5f3aLLM\u7684\u89c6\u89c9\u4fe1\u606f\u8f93\u5165\u5e76\u51cf\u8f7b\u504f\u89c1\u30022. \u8bb0\u5fc6\u77e5\u8bc6\u8f85\u52a9\uff08MKA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u68c0\u7d22\u5b58\u50a8\u793a\u4f8b\u4e2d\u7684\u76f8\u5173\u77e5\u8bc6\u6765\u5e2e\u52a9LLM\u5904\u7406OOD\u6837\u672c\u30023. OAD Prompt\uff0c\u6574\u5408\u524d\u4e24\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u6765\u4f18\u5316LLM\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOAD-Promoter\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728\u5c11\u6837\u672c\u6216\u96f6\u6837\u672cVQA\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "OAD-Promoter\u80fd\u591f\u6709\u6548\u5730\u51cf\u8f7bLLM\u5728VQA\u4efb\u52a1\u4e2d\u7ee7\u627f\u7684\u8bed\u8a00\u504f\u89c1\uff0c\u5e76\u63d0\u9ad8\u5176\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8868\u73b0\u3002"}}
{"id": "2511.13532", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13532", "abs": "https://arxiv.org/abs/2511.13532", "authors": ["Jeongwoo Jae", "Changwon Lee", "Juzar Thingna", "Yeong-Dae Kwon", "Daniel K. Park"], "title": "Measurement-based Dynamical Decoupling for Fidelity Preservation on Large-scale Quantum Processors", "comment": "22 pages, 15 figures", "summary": "Dynamical decoupling (DD) is a key technique for suppressing decoherence and preserving the performance of quantum algorithms. We introduce a measurement-based DD (MDD) protocol that determines control unitary gates from partial measurements of noisy subsystems, with measurement overhead scaling linearly with the number of subsystems. We prove that, under local energy relaxation and dephasing noise, MDD achieves the maximum entanglement fidelity attainable by any DD scheme based on bang-bang operations to first order in evolution time. On the IBM Eagle processor, MDD achieved up to a $450$-fold improvement in the success probability of a $14$-qubit quantum Fourier transform, and improved the accuracy of ground-state energy estimation for $N_2$ in the $56$-qubit sample-based quantum diagonalization compared with the standard XX-pulse DD. These results establish MDD as a scalable and effective approach for suppressing decoherence in large-scale quantum algorithms.", "AI": {"tldr": "MDD\u662f\u4e00\u79cd\u6d4b\u91cf\u5f0f\u52a8\u6001\u89e3\u8026\u534f\u8bae\uff0c\u53ef\u4ee5\u901a\u8fc7\u6d4b\u91cf\u566a\u58f0\u5b50\u7cfb\u7edf\u6765\u786e\u5b9a\u63a7\u5236\u5355\u5143\u95e8\uff0c\u5176\u6d4b\u91cf\u5f00\u9500\u4e0e\u5b50\u7cfb\u7edf\u6570\u91cf\u6210\u7ebf\u6027\u5173\u7cfb\u3002MDD\u5728\u5c40\u90e8\u80fd\u91cf\u5f1b\u8c6b\u548c\u9000\u76f8\u5e72\u566a\u58f0\u4e0b\uff0c\u5176\u7ea0\u7f20\u4fdd\u771f\u5ea6\u53ef\u8fbe\u4efb\u610f\u52a8\u6001\u89e3\u8026\u65b9\u6848\uff08\u57fa\u4e8ebang-bang\u64cd\u4f5c\uff09\u7684\u4e00\u9636\u8fd1\u4f3c\u4e0a\u9650\u3002", "motivation": "\u4e3a\u4e86\u6291\u5236\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7684\u9000\u76f8\u5e72\uff0c\u63d0\u9ad8\u5176\u6027\u80fd\u3002", "method": "MDD\u534f\u8bae\u901a\u8fc7\u5bf9\u566a\u58f0\u5b50\u7cfb\u7edf\u8fdb\u884c\u90e8\u5206\u6d4b\u91cf\u6765\u786e\u5b9a\u63a7\u5236\u5355\u5143\u95e8\u3002", "result": "\u5728IBM Eagle\u5904\u7406\u5668\u4e0a\uff0cMDD\u5c0614\u91cf\u5b50\u6bd4\u7279\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7684\u6210\u529f\u6982\u7387\u63d0\u9ad8\u4e86450\u500d\uff0c\u5e76\u572856\u91cf\u5b50\u6bd4\u7279\u6837\u672c\u5bf9\u89d2\u5316\u7b97\u6cd5\u4e2d\u63d0\u9ad8\u4e86N2\u7684\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6807\u51c6\u7684XX\u8109\u51b2\u52a8\u6001\u89e3\u8026\u3002", "conclusion": "MDD\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u6291\u5236\u5927\u89c4\u6a21\u91cf\u5b50\u7b97\u6cd5\u4e2d\u9000\u76f8\u5e72\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12136", "abs": "https://arxiv.org/abs/2511.12136", "authors": ["Karol C. Jurzec", "Tomasz Szydlo", "Maciej Wielgosz"], "title": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware", "comment": "6 pages, 6 figures, 1 table; code available at https://github.com/karol-jurzec/snn-generator/", "summary": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684C\u8bed\u8a00SNN\u8fd0\u884c\u65f6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0SNN\u63a8\u7406\uff0c\u5e76\u5728N-MNIST\u548cST-MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u5185\u5b58\u7f29\u51cf\u3002", "motivation": "Spiking neural networks (SNNs)\u5728\u65f6\u95f4\u5904\u7406\u548c\u80fd\u6548\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u8bad\u7ec3\u548c\u90e8\u7f72\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8fd0\u884c\u65f6\u548c\u4f18\u5316\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5c06SNNTorch\u5bfc\u51fa\u7684\u6a21\u578b\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684C\u8bed\u8a00\u8868\u793a\uff1b\u4f7f\u7528\u9759\u6001\u3001\u5bf9\u7f13\u5b58\u53cb\u597d\u7684\u6570\u636e\u5e03\u5c40\u548c\u9884\u5206\u914d\u6765\u907f\u514d\u89e3\u91ca\u5668\u548c\u5206\u914d\u5f00\u9500\uff1b\u5229\u7528\u7a00\u758f\u8109\u51b2\u6d3b\u52a8\u6765\u4fee\u526a\u975e\u6d3b\u52a8\u795e\u7ecf\u5143\u548c\u7a81\u89e6\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "\u5728N-MNIST\u548cST-MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0ePython\u57fa\u7ebf\u76f8\u5f53\u7684\u529f\u80fd\uff0c\u540c\u65f6\u5728\u684c\u9762CPU\u4e0a\u5b9e\u73b0\u4e86\u7ea610\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u901a\u8fc7\u4fee\u526a\u5b9e\u73b0\u4e86\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002\u5185\u5b58\u5360\u7528\u4e5f\u5927\u5e45\u51cf\u5c11\uff0c\u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\u5728\u5fae\u63a7\u5236\u5668\uff08Arduino Portenta H7\uff09\u4e0a\u90e8\u7f72\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u7684\u8fd0\u884c\u65f6\u548c\u9762\u5411\u8109\u51b2\u7684\u526a\u679d\uff0cSNN\u53ef\u4ee5\u5728\u4f20\u7edf\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002"}}
{"id": "2511.13560", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13560", "abs": "https://arxiv.org/abs/2511.13560", "authors": ["Benjamin C. B. Symons", "Abhishek Rajput", "Dan E. Browne"], "title": "Sequences of Bivariate Bicycle Codes from Covering Graphs", "comment": null, "summary": "We show that given an instance of a bivariate bicycle (BB) code, it is possible to generate an infinite sequence of new BB codes using increasingly large covering graphs of the original code's Tanner graph. When a BB code has a Tanner graph that is a $h$-fold covering of the base BB code's Tanner graph, we refer to it as a $h$-cover code. We show that for a BB code to be a $h$-cover code, its lattice parameters and defining polynomials must satisfy simple algebraic conditions relative to those of the base code. By extending the graph covering map to a chain map, we show there are induced projection and lifting maps on (co)homology that enable the projection and lifting of logical operators and, in certain cases, automorphisms between the base and the cover code. The search space of cover codes is considerably reduced compared to the full space of possible polynomials and we find that many interesting examples of BB codes, such as the $[[144,12,12]]$ gross code, can be viewed as cover codes. We also apply our method to search for BB codes with weight 8 checks and find many codes, including a $[[64,14,8]]$ and $[[144,14,14]]$ code. For an $h$-cover code of an $[[n,k,d]]$ BB code with parameters $[[n_h = hn, k_h, d_h]]$, we prove that $k_h \\geq k$ and $d_h \\leq hd$ when $h$ is odd. Furthermore if $h$ is odd and $k_h = k$, we prove the lower bound $d \\leq d_h$. We conjecture it is always true that an $h$-cover BB code of a base $[[n,k,d]]$ BB code has parameters $[[n_h = hn, k_h \\geq k, d \\leq d_h \\leq hd]]$. While the focus of this work is on bivariate bicycle codes, we expect these methods to generalise readily to many group algebra codes and to certain code constructions involving hypergraph, lifted, and balanced products.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u65e0\u9650\u5e8f\u5217\u53cc\u53d8\u91cf\u81ea\u884c\u8f66\uff08BB\uff09\u7801\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86h-\u8986\u76d6\u7801\u7684\u4ee3\u6570\u6761\u4ef6\u53ca\u5176\u4e0e\u57fa\u7801\u7684\uff08\u4e0a\uff09\u540c\u8c03\u6620\u5c04\u3002", "motivation": "\u73b0\u6709BB\u7801\u7684\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8986\u76d6\u56fe\u548c\u4ee3\u6570\u6761\u4ef6\u6765\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\uff0c\u5e76\u5bfb\u627e\u5177\u6709\u4f18\u826f\u6027\u80fd\u7684BB\u7801\u3002", "method": "1. \u5b9a\u4e49h-\u8986\u76d6\u7801\uff0c\u5e76\u7ed9\u51fa\u5176\u4ee3\u6570\u6761\u4ef6\u3002 2. \u5229\u7528\u8986\u76d6\u56fe\u6620\u5c04\u7684\u94fe\u6620\u5c04\u6027\u8d28\uff0c\u7814\u7a76\uff08\u4e0a\uff09\u540c\u8c03\u6620\u5c04\u3002 3. \u641c\u7d22\u5177\u6709\u91cd\u91cf8\u6821\u9a8c\u7684BB\u7801\uff0c\u5e76\u627e\u5230\u4e00\u4e9b\u4f8b\u5b50\u3002 4. \u8bc1\u660eh-\u8986\u76d6\u7801\u7684\u53c2\u6570\u754c\u9650\uff0c\u5e76\u63d0\u51fa\u731c\u60f3\u3002 5. \u8ba8\u8bba\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u7c7b\u578b\u7f16\u7801\u4e0a\u7684\u666e\u9002\u6027\u3002", "result": "1. \u627e\u5230\u4e86\u8bb8\u591a\u6709\u8da3\u7684BB\u7801\u4f8b\u5b50\uff0c\u5982[[144,12,12]] gross code\u3002 2. \u627e\u5230\u4e86\u5177\u6709\u91cd\u91cf8\u6821\u9a8c\u7684BB\u7801\uff0c\u5305\u62ec[[64,14,8]]\u548c[[144,14,14]]\u7801\u3002 3. \u8bc1\u660e\u4e86h-\u8986\u76d6\u7801\u7684\u53c2\u6570\u6ee1\u8db3 $k_h \nless k$ \u548c $d_h \nless hd$ \uff08h\u4e3a\u5947\u6570\uff09\u3002 4. \u63d0\u51fa\u4e86h-\u8986\u76d6\u7801\u53c2\u6570\u7684\u731c\u60f3\uff1a$[[n_h = hn, k_h \nless k, d \nless d_h \nless hd]]$\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5730\u751f\u6210\u548c\u641c\u7d22BB\u7801\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8986\u76d6\u56fe\u548c\u4ee3\u6570\u6761\u4ef6\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u53d1\u73b0\u4e86\u5177\u6709\u4f18\u826f\u6027\u80fd\u7684BB\u7801\u3002\u8be5\u65b9\u6cd5\u6709\u671b\u63a8\u5e7f\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u7f16\u7801\u3002"}}
{"id": "2511.12142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12142", "abs": "https://arxiv.org/abs/2511.12142", "authors": ["Seokwon Song", "Minsu Park", "Gunhee Kim"], "title": "MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering", "comment": "Accepted for publication in the Association for the Advancement of Artificial Intelligence (AAAI), 2026", "summary": "Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS", "AI": {"tldr": "MAVIS\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u6e90\u5f52\u56e0\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u5f15\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u6e90\u5f52\u56e0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u7684\u4f5c\u7528\u3002MAVIS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5305\u542b\u89c6\u89c9\u95ee\u7b54\u7684\u5f15\u7528\u6765\u63d0\u9ad8AI\u7b54\u6848\u7684\u53ef\u9760\u6027\u3002", "method": "MAVIS\u6570\u636e\u96c6\u5305\u542b157K\u4e2a\u89c6\u89c9\u95ee\u7b54\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u7b54\u6848\u90fd\u9644\u6709\u6307\u5411\u591a\u6a21\u6001\u6587\u6863\u7684\u4e8b\u5b9e\u7ea7\u5f15\u7528\u3002\u7814\u7a76\u8005\u8fd8\u5f00\u53d1\u4e86\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u4fe1\u606f\u91cf\u3001\u57fa\u7840\u6027\u548c\u6d41\u7545\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u6a21\u578b\uff08LVLMs\uff09\u5728\u4fe1\u606f\u91cf\u548c\u6d41\u7545\u6027\u65b9\u9762\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u6a21\u578b\uff0c\u4f46\u5728\u56fe\u50cf\u6587\u6863\u7684\u57fa\u7840\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u3002\u6b64\u5916\uff0c\u5728\u76f8\u540c\u7684\u591a\u6a21\u6001\u6587\u6863\u4e0b\uff0c\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u5728\u4fe1\u606f\u91cf\u548c\u57fa\u7840\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u51cf\u8f7b\u56fe\u50cf\u6587\u6863\u89e3\u91ca\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MAVIS\u586b\u8865\u4e86\u591a\u6a21\u6001\u6e90\u5f52\u56e0\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u56fe\u50cf\u6587\u6863\u7684\u4e0a\u4e0b\u6587\u504f\u89c1\u65b9\u9762\u3002"}}
{"id": "2511.12614", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12614", "abs": "https://arxiv.org/abs/2511.12614", "authors": ["Artem Moroz", "V\u00edt Zeman", "Martin Mik\u0161\u00edk", "Elizaveta Isianova", "Miroslav David", "Pavel Burget", "Varun Burde"], "title": "OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding", "comment": null, "summary": "We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.", "AI": {"tldr": "We present an integrated framework for object detection and pose estimation using a transformer-based approach (OPFormer) that leverages neural representations (NeRF) and 3D geometric priors (NOCS).", "motivation": "To create a unified, end-to-end framework for object detection and pose estimation that is versatile and can handle cases with and without traditional 3D CAD models.", "method": "The framework integrates object detection (CNOS detector) and pose estimation (OPFormer). OPFormer uses a transformer architecture, a foundation model for feature extraction, learns object representations from multiple template views, and incorporates NOCS for 3D geometric priors. It reconstructs NeRFs for objects lacking CAD models. It establishes 2D-3D correspondences for pose determination.", "result": "The system achieves a strong balance between accuracy and efficiency on the BOP benchmarks, demonstrating practical applicability in both model-based and model-free scenarios.", "conclusion": "The proposed unified framework effectively combines object detection and pose estimation, offering a practical and efficient solution for real-world applications, adaptable to different object representation availability."}}
{"id": "2511.13572", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13572", "abs": "https://arxiv.org/abs/2511.13572", "authors": ["Maxim A. Gavreev", "Evgeniy O. Kiktenko", "Aleksey K. Fedorov", "Anastasiia S. Nikolaeva"], "title": "Qudit-native simulation of the Potts model", "comment": "7 pages, 3 figures", "summary": "Simulating entangled, many-body quantum systems is notoriously hard, especially in the case of high-dimensional nature of physical underlying objects. In this work, we propose an approach for simulating the Potts model based on the Suzuki-Trotter decomposition that we construct for qudit systems. Specifically, we introduce two qudit-native decomposition schemes: (i) the first utilizes Molmer-Sorensen gate and additional local levels to encode the Potts interactions, while (ii) the second employs an light-shift gate that naturally fits qudit architectures. These decompositions enable a direct and efficient mapping of the Potts model dynamics into hardware-efficient qudit gate sequences for trapped-ion platform. Furthermore, we demonstrate the use of a Suzuki-Trotter approximation with our evolution-into-gates framework, for detecting the dynamical quantum phase transition. Our results establish a pathway toward qudit-based digital quantum simulation of many-body models and provide a new perspective on probing nonanalytic behavior in high-dimensional quantum many-body models.", "AI": {"tldr": "\u5229\u7528 qudit \u91cf\u5b50\u8ba1\u7b97\u673a\u6a21\u62df\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u9ad8\u7ef4\u7684 Potts \u6a21\u578b\u3002", "motivation": "\u6a21\u62df\u9ad8\u7ef4\u7269\u7406\u5bf9\u8c61\u7ec4\u6210\u7684\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\uff0c\u7279\u522b\u662f Potts \u6a21\u578b\uff0c\u662f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e Suzuki-Trotter \u5206\u89e3\u7684 qudit \u7cfb\u7edf\u6a21\u62df\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd qudit \u539f\u751f\u5206\u89e3\u65b9\u6848\uff1a\u4e00\u79cd\u5229\u7528 Molmer-Sorensen \u95e8\u548c\u5c40\u90e8\u80fd\u7ea7\u7f16\u7801\u76f8\u4e92\u4f5c\u7528\uff0c\u53e6\u4e00\u79cd\u5229\u7528 light-shift \u95e8\u3002\u8be5\u65b9\u6cd5\u5c06 Potts \u6a21\u578b\u52a8\u529b\u5b66\u6620\u5c04\u5230\u786c\u4ef6\u9ad8\u6548\u7684 qudit \u95e8\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408 Suzuki-Trotter \u8fd1\u4f3c\u548c evolution-into-gates \u6846\u67b6\u6765\u68c0\u6d4b\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8\u3002", "result": "\u5b9e\u73b0\u4e86 qudit \u91cf\u5b50\u6a21\u62df Potts \u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528 Suzuki-Trotter \u8fd1\u4f3c\u548c evolution-into-gates \u6846\u67b6\u68c0\u6d4b\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8\u7684\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u57fa\u4e8e qudit \u7684\u6570\u5b57\u91cf\u5b50\u6a21\u62df\u591a\u4f53\u6a21\u578b\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u5e76\u4e3a\u63a2\u6d4b\u9ad8\u7ef4\u91cf\u5b50\u591a\u4f53\u6a21\u578b\u4e2d\u7684\u975e\u89e3\u6790\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.12150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12150", "abs": "https://arxiv.org/abs/2511.12150", "authors": ["Yuqi Xie", "Shuhan Ye", "Yi Yu", "Chong Wang", "Qixin Zhang", "Jiazhen Xu", "Le Shen", "Yuanbin Qian", "Jiangbo Qian", "Guoqi Li"], "title": "Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain", "comment": null, "summary": "The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.", "AI": {"tldr": "event cameras and SNNs are energy-efficient but limited by scarce data and modality gap. TMKT framework with TSM and MAG/MRP objectives bridges this gap for better SNN training.", "motivation": "Scarce event data and the distribution gap between RGB and DVS hinder effective training of event cameras and SNNs, leading to underperformance in prior knowledge transfer methods.", "method": "Proposed Time-step Mixup Knowledge Transfer (TMKT) framework with a probabilistic Time-step Mixup (TSM) strategy. TSM interpolates RGB and DVS inputs at various time steps for SNNs, creating a smooth curriculum. Introduced Modality Aware Guidance (MAG) for per-frame supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation.", "result": "TMKT enables smoother knowledge transfer, mitigates modality mismatch, and achieves superior performance in spiking image classification tasks.", "conclusion": "TMKT framework effectively bridges the modality gap between RGB and DVS data for SNN training, demonstrating superior performance across various benchmarks and SNN backbones through TSM, MAG, and MRP strategies."}}
{"id": "2511.12751", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12751", "abs": "https://arxiv.org/abs/2511.12751", "authors": ["Timur Anvar", "Jeffrey Chen", "Yuyan Wang", "Rohan Chandra"], "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving", "comment": null, "summary": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.", "AI": {"tldr": "\u5c0f\u578b\u672c\u5730\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u901a\u8fc7\u5956\u52b1\u5851\u9020\u800c\u975e\u76f4\u63a5\u63a7\u5236\u6765\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\uff0c\u4f46\u5b58\u5728\u6548\u7387\u548c\u7a33\u5b9a\u6027\u7684\u6311\u6218\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u5956\u52b1\u51fd\u6570\u7684\u4f9d\u8d56\u6027\u4e0d\u8db3\u4ee5\u5904\u7406\u8bed\u4e49\u548c\u793e\u4f1a\u590d\u6742\u6027\uff0c\u800c\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5c0f\u578b\u672c\u5730\u5316LLM\u662f\u5426\u80fd\u901a\u8fc7\u5956\u52b1\u5851\u9020\u6765\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u7eafRL\u3001\u7eafLLM\u548c\u6df7\u5408\u65b9\u6cd5\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cLLM\u901a\u8fc7\u8bc4\u4f30\u72b6\u6001-\u52a8\u4f5c\u8f6c\u6362\u6765\u589e\u5f3aRL\u5956\u52b1\uff0c\u800c\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6807\u51c6\u7684RL\u7b56\u7565\u3002", "result": "\u7eafRL\u65b9\u6cd5\u7684\u6210\u529f\u7387\u572873-89%\u4e4b\u95f4\uff0c\u6548\u7387\u5c1a\u53ef\u3002\u7eafLLM\u65b9\u6cd5\u7684\u6210\u529f\u7387\u6700\u9ad8\u53ef\u8fbe94%\uff0c\u4f46\u901f\u5ea6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u6df7\u5408\u65b9\u6cd5\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002LLM\u5f71\u54cd\u7684\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u4fdd\u5b88\u504f\u89c1\uff0c\u4e14\u6a21\u578b\u4f9d\u8d56\u6027\u53d8\u5f02\u6027\u5927\uff0c\u5f71\u54cd\u4e86\u6548\u7387\u3002", "conclusion": "\u5c3d\u7ba1\u5c0f\u578bLLM\u5728\u5956\u52b1\u5851\u9020\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u5b58\u5728\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u4fdd\u5b88\u504f\u89c1\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u3002"}}
{"id": "2511.13604", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.13604", "abs": "https://arxiv.org/abs/2511.13604", "authors": ["Sahil Pontula", "Debasmita Banerjee", "Marin Soljacic", "Yannick Salamin"], "title": "Long-range entanglement and quantum correlations in a multi-frequency comb system", "comment": null, "summary": "Frequency combs are multimode photonic systems that underlie countless precision sensing and metrology applications. Since their invention over two decades ago, numerous efforts have pushed frequency combs to broader bandwidths and more stable operation. More recently, quantum squeezing and entanglement have been explored in single frequency comb systems for quantum advantages in sensing and signal multiplexing. However, the production of quantum light across multiple frequency combs remains unexplored. In this work, we theoretically explore a mechanism that generates a series of nonlinearly coupled frequency combs through cascaded three-wave upconversion and downconversion processes mediated by a single idler comb. We show how this system generates inter- and intracomb two-mode squeezing and entanglement spanning a very large spectral range, from ultraviolet to mid-IR frequencies. Finally, we show how this system can be engineered to produce on-demand multimode quantum light through covariance matrix optimization. Our findings could enable tunable broadband ghost spectroscopy protocols, squeezing-enhanced pump-probe measurements, and broadband entanglement between spectrally-multiplexed quanta of information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u751f\u6210\u591a\u6a21\u91cf\u5b50\u5149\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u7ebf\u6027\u8026\u5408\u7684\u9891\u7387\u68b3\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u7d2b\u5916\u5230\u4e2d\u7ea2\u5916\u7684\u5927\u8303\u56f4\u5149\u8c31\u8c03\u8c10\uff0c\u5e76\u6709\u671b\u5e94\u7528\u4e8e\u91cf\u5b50\u4f20\u611f\u548c\u901a\u4fe1\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u751f\u6210\u8de8\u591a\u4e2a\u9891\u7387\u68b3\u7684\u91cf\u5b50\u5149\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u80fd\u591f\u751f\u6210\u591a\u6a21\u91cf\u5b50\u5149\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5355\u4e2a\u95f2\u7f6e\u68b3\u4f5c\u4e3a\u5a92\u4ecb\uff0c\u901a\u8fc7\u7ea7\u8054\u4e09\u6ce2\u4e0a\u8f6c\u6362\u548c\u4e0b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u7406\u8bba\u4e0a\u63a2\u7d22\u4e86\u4e00\u79cd\u751f\u6210\u4e00\u7cfb\u5217\u975e\u7ebf\u6027\u8026\u5408\u9891\u7387\u68b3\u7684\u673a\u5236\uff0c\u5e76\u5b9e\u73b0\u4e86\u68b3\u5185\u548c\u68b3\u95f4\u7684\u4e24\u6a21\u538b\u7f29\u548c\u7ea0\u7f20\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8de8\u8d8a\u4ece\u7d2b\u5916\u5230\u4e2d\u7ea2\u5916\u9891\u7387\u7684\u5927\u8303\u56f4\u5149\u8c31\u7684\u68b3\u5185\u548c\u68b3\u95f4\u4e24\u6a21\u538b\u7f29\u548c\u7ea0\u7f20\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u534f\u65b9\u5dee\u77e9\u9635\u4f18\u5316\u6765\u6309\u9700\u751f\u6210\u591a\u6a21\u91cf\u5b50\u5149\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u6a21\u91cf\u5b50\u5149\u751f\u6210\u673a\u5236\u4e3a\u5b9e\u73b0\u53ef\u8c03\u8c10\u5bbd\u5e26\u9b3c\u6210\u50cf\u5149\u8c31\u3001\u538b\u7f29\u589e\u5f3a\u6cf5\u6d66\u63a2\u6d4b\u6d4b\u91cf\u4ee5\u53ca\u5149\u8c31\u590d\u7528\u91cf\u5b50\u4fe1\u606f\u5bbd\u5e26\u7ea0\u7f20\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.12151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12151", "abs": "https://arxiv.org/abs/2511.12151", "authors": ["Kaixiang Yang", "Boyang Shen", "Xin Li", "Yuchen Dai", "Yuxuan Luo", "Yueran Ma", "Wei Fang", "Qiang Li", "Zhiwei Wang"], "title": "FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing", "comment": "AAAI 2026", "summary": "Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.", "AI": {"tldr": "FIA-Edit\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u65e0\u53cd\u6f14\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u8bed\u4e49\u7cbe\u786e\u7684\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u80cc\u666f\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5c06\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u5e94\u7528\u4e8e\u4e34\u5e8a\u533b\u5b66\u56fe\u50cf\u7684\u51fa\u8840\u53d8\u5316\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684\u65e0\u53cd\u6f14\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u6574\u5408\u6e90\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u80cc\u666f\u4fdd\u7559\u6548\u679c\u5dee\u3001\u7a7a\u95f4\u4e0d\u4e00\u81f4\u4ee5\u53ca\u8fc7\u5ea6\u7f16\u8f91\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u9ad8\u4fdd\u771f\u4e14\u8bed\u4e49\u7cbe\u786e\u5730\u8fdb\u884c\u7f16\u8f91\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFIA-Edit\u7684\u65b0\u9896\u65e0\u53cd\u6f14\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u3002\u8be5\u673a\u5236\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u9891\u7387\u8868\u793a\u4ea4\u4e92\uff08FRI\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u4ea4\u6362\u6e90\u548c\u76ee\u6807\u7279\u5f81\u7684\u9891\u7387\u6210\u5206\u6765\u589e\u5f3a\u8de8\u57df\u5bf9\u9f50\uff1b(2) \u7279\u5f81\u6ce8\u5165\uff08FIJ\uff09\u6a21\u5757\uff0c\u5c06\u6e90\u4fa7\u7684\u67e5\u8be2\u3001\u952e\u3001\u503c\u4ee5\u53ca\u6587\u672c\u5d4c\u5165\u663e\u5f0f\u5730\u6574\u5408\u5230\u76ee\u6807\u5206\u652f\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u4e2d\uff0c\u4ee5\u4fdd\u7559\u7ed3\u6784\u548c\u8bed\u4e49\u3002", "result": "FIA-Edit \u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u3001\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u8ba1\u7b97\u6210\u672c\uff08\u5728RTX 4090\u4e0a\u7f16\u8f91\u4e00\u5f20512*512\u56fe\u50cf\u7ea6\u97006\u79d2\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFIA-Edit \u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0cFIA-Edit \u9996\u6b21\u5c06\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u6269\u5c55\u5230\u4e34\u5e8a\u5e94\u7528\uff0c\u80fd\u591f\u5408\u6210\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u7684\u672f\u4e2d\u51fa\u8840\u53d8\u5316\uff0c\u4e3a\u533b\u5b66\u6570\u636e\u589e\u5f3a\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u5728\u4e0b\u6e38\u51fa\u8840\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "FIA-Edit \u901a\u8fc7\u5176\u521b\u65b0\u7684\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u548c\u7279\u5f81\u6ce8\u5165\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u8bed\u4e49\u7cbe\u786e\u7684\u7f16\u8f91\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u56fe\u50cf\u7f16\u8f91\u548c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5747\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.13622", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.13622", "abs": "https://arxiv.org/abs/2511.13622", "authors": ["Matias Bundgaard-Nielsen", "Gian Luca Lippi", "Jesper M\u00f8rk"], "title": "Modeling Quantum Noise in Nanolasers using Markov Chains", "comment": "17 Pages, 7 Figures, submitted for PRA", "summary": "The random nature of spontaneous emission leads to unavoidable fluctuations in a laser's output. This is often included through random Langevin forces in laser rate equations, but this approach falls short for nanolasers. In this paper, we show that the laser quantum noise can be quantitatively computed for a very broad class of lasers by starting from simple and intuitive rate equations and merely assuming that the number of photons and excited electrons only takes discrete values. The success of the model is explained by showing that it constitutes a Markov chain, which can be derived from the full master equations. We show that in the many-photon limit, the model simplifies to Langevin equations. We perform an extensive comparison of different approaches for computing quantum noise in lasers, identifying the best approach for different system sizes, ranging from nanolasers to macroscopic lasers, and different levels of excitation, i.e., cavity photon number. In particular, we find that the numerical solution to the Langevin equations is inaccurate below the laser threshold, while the laser Markov chain model, on the other hand, is accurate for all pump values and laser sizes when collective emitter effects are excluded.", "AI": {"tldr": "\u6fc0\u5149\u91cf\u5b50\u566a\u58f0\u53ef\u4ee5\u901a\u8fc7\u79bb\u6563\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u7cbe\u786e\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5c3a\u5bf8\u548c\u6fc0\u52b1\u6c34\u5e73\u7684\u6fc0\u5149\u5668\uff0c\u5c24\u5176\u5728\u7eb3\u79d2\u6fc0\u5149\u5668\u548c\u6fc0\u5149\u9608\u503c\u4ee5\u4e0b\u8868\u73b0\u4f18\u4e8e\u5170\u4e4b\u4e07\u65b9\u7a0b\u3002", "motivation": "\u968f\u673a\u81ea\u53d1\u8f90\u5c04\u5bfc\u81f4\u6fc0\u5149\u8f93\u51fa\u6ce2\u52a8\uff0c\u73b0\u6709\u5170\u4e4b\u4e07\u529b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u7eb3\u79d2\u6fc0\u5149\u5668\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5e7f\u6cdb\u7684\u6fc0\u5149\u5668\u63d0\u4f9b\u4e00\u79cd\u91cf\u5316\u8ba1\u7b97\u6fc0\u5149\u91cf\u5b50\u566a\u58f0\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4ece\u7b80\u5355\u7684\u901f\u7387\u65b9\u7a0b\u51fa\u53d1\uff0c\u5047\u8bbe\u5149\u5b50\u6570\u548c\u6fc0\u53d1\u7535\u5b50\u6570\u662f\u79bb\u6563\u7684\uff0c\u6784\u5efa\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u6765\u8ba1\u7b97\u6fc0\u5149\u91cf\u5b50\u566a\u58f0\uff0c\u5e76\u4e0e\u4e3b\u65b9\u7a0b\u548c\u5170\u4e4b\u4e07\u65b9\u7a0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6240\u63d0\u51fa\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u5728\u6240\u6709\u6cf5\u6d66\u503c\u548c\u6fc0\u5149\u5c3a\u5bf8\u4e0b\uff08\u6392\u9664\u96c6\u4f53\u53d1\u5c04\u4f53\u6548\u5e94\u65f6\uff09\u90fd\u80fd\u51c6\u786e\u8ba1\u7b97\u91cf\u5b50\u566a\u58f0\uff0c\u7279\u522b\u662f\u5728\u6fc0\u5149\u9608\u503c\u4ee5\u4e0b\u6bd4\u5170\u4e4b\u4e07\u65b9\u7a0b\u66f4\u51c6\u786e\uff0c\u5728\u591a\u5149\u5b50\u6781\u9650\u4e0b\u53ef\u7b80\u5316\u4e3a\u5170\u4e4b\u4e07\u65b9\u7a0b\u3002", "conclusion": "\u79bb\u6563\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u662f\u4e00\u79cd\u901a\u7528\u4e14\u51c6\u786e\u7684\u6fc0\u5149\u91cf\u5b50\u566a\u58f0\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4e3a\u4ece\u7eb3\u79d2\u6fc0\u5149\u5668\u5230\u5b8f\u89c2\u6fc0\u5149\u5668\u7b49\u4e0d\u540c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u566a\u58f0\u8ba1\u7b97\u65b9\u6848\u3002"}}
{"id": "2511.12162", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12162", "abs": "https://arxiv.org/abs/2511.12162", "authors": ["Shuo Yin", "Zhiyuan Yin", "Yuqing Hou", "Rui Liu", "Yong Chen", "Dell Zhang"], "title": "Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function", "comment": "14 pages", "summary": "Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.", "AI": {"tldr": "CRH\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u54c8\u5e0c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u5206\u914d\u54c8\u5e0c\u4e2d\u5fc3\u6765\u4f18\u5316\u54c8\u5e0c\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u54c8\u5e0c\u4e2d\u5fc3\u521d\u59cb\u5316\u4e0d\u5f53\u548c\u5206\u9636\u6bb5\u4f18\u5316\u5e26\u6765\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u54c8\u5e0c\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5728\u521d\u59cb\u5316\u65f6\u5ffd\u7565\u4e86\u7c7b\u95f4\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5219\u5f15\u5165\u4e86\u989d\u5916\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u3002CRH\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CRH\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5728\u8054\u5408\u4f18\u5316\u54c8\u5e0c\u51fd\u6570\u7684\u540c\u65f6\uff0c\u52a8\u6001\u5730\u4ece\u9884\u8bbe\u7684\u4ee3\u7801\u5e93\u4e2d\u91cd\u65b0\u5206\u914d\u54c8\u5e0c\u4e2d\u5fc3\u3002\u5b83\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u591a\u5934\u673a\u5236\u6765\u589e\u5f3a\u54c8\u5e0c\u4e2d\u5fc3\u7684\u8868\u5f81\u80fd\u529b\u3002", "result": "CRH\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u54c8\u5e0c\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u5b66\u4e60\u5230\u5177\u6709\u8bed\u4e49\u610f\u4e49\u7684\u54c8\u5e0c\u4e2d\u5fc3\u3002", "conclusion": "CRH\u901a\u8fc7\u52a8\u6001\u91cd\u5206\u914d\u54c8\u5e0c\u4e2d\u5fc3\u548c\u591a\u5934\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5728\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12878", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12878", "abs": "https://arxiv.org/abs/2511.12878", "authors": ["Junyi Ma", "Wentao Bao", "Jingyi Xu", "Guanzhong Sun", "Yu Zheng", "Erhang Zhang", "Xieyuanli Chen", "Hesheng Wang"], "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views", "comment": "Extended journal version of MMTwin (IROS'25)", "summary": "Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., \"how to interact\"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., \"when to interact\") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEgoLoc\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u4e2d\u5b9a\u4f4d\u624b-\u7269\u4f53\u63a5\u89e6\u548c\u5206\u79bb\u7684\u65f6\u95f4\u6233\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u4f53\u8bc6\u522b\u548c\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u201c\u5982\u4f55\u4ea4\u4e92\u201d\uff0c\u800c\u5ffd\u7565\u4e86\u201c\u4f55\u65f6\u4ea4\u4e92\u201d\u8fd9\u4e00\u5173\u952e\u7684\u7cbe\u7ec6\u5316\u95ee\u9898\uff0c\u5373\u624b-\u7269\u4f53\u63a5\u89e6\u548c\u5206\u79bb\u7684\u5173\u952e\u65f6\u523b\u7684\u6355\u6349\uff0c\u8fd9\u5bf9\u4e8e\u6c89\u6d78\u5f0f\u6df7\u5408\u73b0\u5b9e\u4f53\u9a8c\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEgoLoc\u7684\u65b0\u578b\u96f6\u6837\u672c\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u624b\u90e8\u52a8\u529b\u5b66\u5f15\u5bfc\u7684\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u63a5\u89e6/\u5206\u79bb\u5c5e\u6027\u3001\u5b9a\u4f4d\u5177\u4f53\u65f6\u95f4\u6233\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u8fdb\u884c\u4f18\u5316\u3002EgoLoc\u65e0\u9700\u7269\u4f53\u63a9\u7801\u548c\u52a8\u8bcd-\u540d\u8bcd\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eEgoLoc\u80fd\u591f\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u5b9e\u73b0\u5408\u7406\u7684\u65f6\u95f4\u4ea4\u4e92\u5b9a\u4f4d\uff08TIL\uff09\uff0c\u5e76\u6709\u6548\u4fc3\u8fdb\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u591a\u4e2a\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "EgoLoc\u662f\u4e00\u79cd\u521b\u65b0\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u4e2d\u5b9a\u4f4d\u624b-\u7269\u4f53\u4ea4\u4e92\u7684\u5173\u952e\u65f6\u95f4\u70b9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.13657", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13657", "abs": "https://arxiv.org/abs/2511.13657", "authors": ["Nitish Kumar Chandra", "Eneet Kaur", "Kaushik P. Seshadreesan"], "title": "Architectural Approaches to Fault-Tolerant Distributed Quantum Computing and Their Entanglement Overheads", "comment": "To appear in the Second IEEE Workshop on Quantum Intelligence, Learning and Security (QuILLS 2025)", "summary": "Fault tolerant quantum computation over distributed quantum computing (DQC) platforms requires careful evaluation of resource requirements and noise thresholds. As quantum hardware advances toward modular and networked architectures, various fault tolerant DQC schemes have been proposed, which can be broadly categorized into three architectural types. Type 1 architectures consist of small quantum nodes connected via Greenberger-Horne-Zeilinger (GHZ) states, enabling nonlocal stabilizer measurements. Type 2 architectures distribute a large error correcting code block across multiple modules, with most stabilizer measurements remaining local, except for a small subset at patch boundaries that are performed using nonlocal CNOT gates. Type 3 architectures assign code blocks to distinct modules and can perform fault tolerant operations such as transversal gates, lattice surgery, and teleportation to implement logical operations between code blocks. Using the planar surface code and toric code as representative examples, we analyze how the resource requirements, particularly the number of Bell pairs and the average number of generation attempts, scale with increasing code distance across different architectural designs. This analysis provides valuable insights for identifying architectures well suited to fault tolerant distributed quantum computation under near term hardware and resource constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u5bb9\u9519\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\uff08DQC\uff09\u67b6\u6784\u8fdb\u884c\u4e86\u8d44\u6e90\u9700\u6c42\u548c\u566a\u58f0\u9608\u503c\u7684\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u5176\u5728\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u548c\u8d1d\u5c14\u5bf9\u751f\u6210\u6b21\u6570\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u786c\u4ef6\u5411\u6a21\u5757\u5316\u548c\u7f51\u7edc\u5316\u67b6\u6784\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u5bb9\u9519DQC\u65b9\u6848\u7684\u8d44\u6e90\u9700\u6c42\u548c\u566a\u58f0\u9608\u503c\uff0c\u4ee5\u786e\u5b9a\u9002\u5408\u8fd1\u671f\u786c\u4ef6\u548c\u8d44\u6e90\u9650\u5236\u7684\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u5206\u6790\u57fa\u4e8e\u5e73\u9762\u8868\u9762\u7801\u548c\u73af\u5f62\u7801\u7684\u4ee3\u8868\u6027\u793a\u4f8b\uff0c\u7814\u7a76\u4e86\u4e09\u79cdDQC\u67b6\u6784\u7c7b\u578b\uff08\u57fa\u4e8eGHZ\u6001\u8fde\u63a5\u7684\u5c0f\u578b\u91cf\u5b50\u8282\u70b9\u3001\u5206\u5e03\u5f0f\u5927\u9519\u8bef\u7ea0\u6b63\u7801\u5757\u3001\u5c06\u7801\u5757\u5206\u914d\u7ed9\u4e0d\u540c\u6a21\u5757\uff09\u7684\u8d44\u6e90\u9700\u6c42\uff08\u8d1d\u5c14\u5bf9\u6570\u91cf\u3001\u5e73\u5747\u751f\u6210\u6b21\u6570\uff09\u4e0e\u4ee3\u7801\u8ddd\u79bb\u7684\u5173\u7cfb\u3002", "result": "\u8bba\u6587\u5206\u6790\u4e86\u4e0d\u540cDQC\u67b6\u6784\u7c7b\u578b\u5728\u589e\u52a0\u4ee3\u7801\u8ddd\u79bb\u65f6\uff0c\u8d44\u6e90\u9700\u6c42\uff08\u8d1d\u5c14\u5bf9\u6570\u91cf\u3001\u5e73\u5747\u751f\u6210\u6b21\u6570\uff09\u7684\u6269\u5c55\u60c5\u51b5\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u5bb9\u9519DQC\u67b6\u6784\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u8be5\u5206\u6790\u4e3a\u4e86\u89e3\u5728\u8fd1\u671f\u786c\u4ef6\u548c\u8d44\u6e90\u9650\u5236\u4e0b\uff0c\u9002\u5408\u5bb9\u9519\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.12170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12170", "abs": "https://arxiv.org/abs/2511.12170", "authors": ["Wang Luo", "Di Wu", "Hengyuan Na", "Yinlin Zhu", "Miao Hu", "Guocong Quan"], "title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective", "comment": "Accepted by AAAI 2026", "summary": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u6821\u6b63\u7684\u8303\u5f0f\uff0c\u4f7f\u7528PGNet\u4ece\u90e8\u5206\u70b9\u4e91\u751f\u6210\u5b8c\u65743D\u5f62\u72b6\u3002", "motivation": "\u73b0\u6709\u7684\u70b9\u4e91\u8865\u5168\u65b9\u6cd5\uff08\u57fa\u4e8e\u4fee\u590d\u8303\u5f0f\uff09\u5728\u5904\u7406\u4e25\u91cd\u906e\u6321\u548c\u51e0\u4f55\u7f3a\u5931\u65f6\uff0c\u5e38\u56e0\u6709\u9650\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u7ea6\u675f\u800c\u5bfc\u81f4\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u62d3\u6251\u5931\u771f\u3002\u800c\u4f5c\u8005\u63d0\u51fa\u7684\u57fa\u4e8e\u6821\u6b63\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5bf9\u9884\u5148\u751f\u6210\u7684\u5b8c\u6574\u5f62\u72b6\u8fdb\u884c\u6821\u6b63\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7ed3\u6784\u4e00\u81f4\u4e14\u4e0e\u89c2\u5bdf\u4e00\u81f4\u7684\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPGNet\u7684\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u7279\u5f81\u7f16\u7801\u6765\u7ea6\u675f\u751f\u6210\u5148\u9a8c\uff0c\u5408\u6210\u4e00\u4e2a\u7c97\u7cd9\u4f46\u7ed3\u6784\u5bf9\u9f50\u7684\u652f\u67b6\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u6821\u6b63\u9010\u6b65\u5b8c\u5584\u51e0\u4f55\u7ec6\u8282\u3002", "result": "PGNet\u5728ShapeNetViPC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5e73\u5747Chamfer\u8ddd\u79bb\uff08-23.5%\uff09\u548cF-score\uff08+7.1%\uff09\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u7684\u57fa\u4e8e\u6821\u6b63\u7684\u8303\u5f0f\uff0c\u5e76\u4ee5PGNet\u4e3a\u5b9e\u73b0\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u70b9\u4e91\u8865\u5168\u4e2d\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u62d3\u6251\u5931\u771f\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2511.13047", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13047", "abs": "https://arxiv.org/abs/2511.13047", "authors": ["Yan Gong", "Jianli Lu", "Yongsheng Gao", "Jie Zhao", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation", "comment": "11 pages, 5 figures, 5 tables", "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.", "AI": {"tldr": "DiffPixelFormer\u662f\u4e00\u4e2a\u7528\u4e8eRGB-D\u5ba4\u5185\u573a\u666f\u5206\u5272\u7684\u5dee\u5206\u50cf\u7d20\u611f\u77e5Transformer\uff0c\u901a\u8fc7IIMIB\u548cDSIM\u6a21\u5757\u589e\u5f3a\u4e86\u6a21\u6001\u5185\u8868\u793a\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u66f4\u5f3a\u7684\u533a\u5206\u6027\u8868\u793a\uff0c\u5728SUN RGB-D\u548cNYUDv2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RGB-D\u878d\u5408\u5ba4\u5185\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4e14\u4e0d\u80fd\u5145\u5206\u6a21\u62df\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7279\u5f81\u5173\u7cfb\uff0c\u5bfc\u81f4\u7279\u5f81\u5bf9\u9f50\u4e0d\u7cbe\u786e\u548c\u533a\u5206\u6027\u8868\u793a\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u50cf\u7d20\u611f\u77e5Transformer\uff08DiffPixelFormer\uff09\uff0c\u5305\u542b\u4e00\u4e2a\u5185\u90e8-\u5916\u90e8\u6a21\u6001\u4ea4\u4e92\u5757\uff08IIMIB\uff09\u6765\u6355\u6349\u6a21\u6001\u5185\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5dee\u5206\u5171\u4eab\u6a21\u6001\u95f4\uff08DSIM\uff09\u6a21\u5757\u6765\u89e3\u5f00\u6a21\u6001\u7279\u5b9a\u7684\u548c\u5171\u4eab\u7684\u7ebf\u7d22\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u50cf\u7d20\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u91c7\u7528\u52a8\u6001\u878d\u5408\u7b56\u7565\u6765\u5e73\u8861\u6a21\u6001\u8d21\u732e\u5e76\u5145\u5206\u5229\u7528RGB-D\u4fe1\u606f\u3002", "result": "\u5728SUN RGB-D\u548cNYUDv2\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cDiffPixelFormer-L\u7684mIoU\u5f97\u5206\u5206\u522b\u4e3a54.28%\u548c59.95%\uff0c\u5206\u522b\u6bd4DFormer-L\u9ad8\u51fa1.78%\u548c2.75%\u3002", "conclusion": "DiffPixelFormer\u5728RGB-D\u5ba4\u5185\u573a\u666f\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13683", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2511.13683", "abs": "https://arxiv.org/abs/2511.13683", "authors": ["Yue Tu", "Liang Jiang"], "title": "Quantum Advantage in Learning Mixed Unitary Channels", "comment": null, "summary": "We study the task of learning mixed unitary channels using Fisher information, under different quantum resource assumptions including ancilla and concatenation. Our result shows that the asymptotic sample complexity scales as $\\frac{r}{d\\varepsilon^2}$, where $r$ is the rank of the channel (i.e.\\ the number of different unitaries), $d$ is the dimension of the system, and $\\varepsilon^2$ is the mean-square error. Thus the critical resource is the ancilla, which mirrors the result in~\\cite{chen2022quantum} but in a more precise form, as we point out that $r$ is also important. Additionally, we demonstrate the practical potential of mixed unitary channels by showing that random mixed unitary channels are easy to learn.", "AI": {"tldr": "We study learning mixed unitary channels using Fisher information and find the sample complexity scales as r/(d*epsilon^2), highlighting the importance of ancilla and channel rank. We also show that random mixed unitary channels are easy to learn.", "motivation": "The paper aims to understand the learning of mixed unitary quantum channels under various resource constraints, specifically focusing on the role of ancilla and concatenation in determining sample complexity.", "method": "The study employs Fisher information as a tool to analyze the learning process. It derives an expression for the asymptotic sample complexity, which is found to be proportional to r/(d*epsilon^2), where r is the channel rank, d is the system dimension, and epsilon^2 is the mean-square error. The paper also explores the learnability of random mixed unitary channels.", "result": "The derived sample complexity for learning mixed unitary channels is $\frac{r}{d\binom{2}{ \times \text{error}}}$, indicating that both the ancilla (implicitly, as it affects r) and the rank of the channel (r) are critical resources. Furthermore, the results show that random mixed unitary channels are practically learnable.", "conclusion": "The paper concludes that Fisher information provides a precise understanding of the resources required for learning mixed unitary channels, with ancilla and channel rank being key factors. The learnability of random mixed unitary channels is also established, suggesting their practical relevance."}}
{"id": "2511.12181", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12181", "abs": "https://arxiv.org/abs/2511.12181", "authors": ["Jinyuan Hu", "Jiayou Zhang", "Shaobo Cui", "Kun Zhang", "Guangyi Chen"], "title": "MixAR: Mixture Autoregressive Image Generation", "comment": null, "summary": "Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.", "AI": {"tldr": "MixAR\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u5148\u9a8c\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u79bb\u6563\u5316\u548c\u8fde\u7eed\u8868\u793a\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u8fde\u7eed\u8868\u793a\u96be\u4ee5\u5efa\u6a21\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u5e76\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MixAR\u6846\u67b6\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u81ea\u6ce8\u610f\u529b\uff08DC-SA\uff09\u3001\u4ea4\u53c9\u6ce8\u610f\u529b\uff08DC-CA\uff09\u548c\u6df7\u5408\uff08DC-Mix\uff09\uff0c\u5e76\u63d0\u51fa\u8bad\u7ec3-\u63a8\u7406\u6df7\u5408\uff08TI-Mix\uff09\u7b56\u7565\u4ee5\u5b9e\u73b0\u8bad\u7ec3\u548c\u63a8\u7406\u5206\u5e03\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDC-Mix\u7b56\u7565\u5728\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0cTI-Mix\u7b56\u7565\u4e5f\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "MixAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u6807\u8bb0\u548c\u8fde\u7eed\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2511.13687", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13687", "abs": "https://arxiv.org/abs/2511.13687", "authors": ["Nitish Kumar Chandra", "Eneet Kaur", "Kaushik P. Seshadreesan"], "title": "Network Operations Scheduling for Distributed Quantum Computing", "comment": "Published in 2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)", "summary": "Realizing distributed architectures for quantum computing is crucial to scaling up computational power. A key component of such architectures is a scheduler that coordinates operations over a short-range quantum network required to enable the necessary non-local entangling gates between quantum processing units (QPUs). It is desirable to determine schedules of minimum make span, which in the case of networks with constrained resources hinges on their efficient usage. Here we compare and contrast two approaches to solving the make span minimization problem, an approach based on the resource constrained project scheduling (RCPSP) framework, and another based on a greedy heuristic algorithm. The workflow considered is as follows. Firstly, the computational circuit is partitioned and assigned to different QPUs such that the number of nonlocal entangling gates acting across partitions is minimized while the qubit load is nearly uniform on the individual QPUs, which can be accomplished using, e.g., the METIS solver. Secondly, the nonlocal entangling gate requirements with respect to the partitions are identified, and mapped to network operation sequences that deliver the necessary entanglement between the QPUs. Finally, the network operations are scheduled such that the make span is minimized. As illustrative examples, we analyze the implementation of a small instance of the Quantum Fourier Transform algorithm over instances of a simple hub and spoke (star) network architecture comprised of a quantum switch as the hub and QPUs as spokes, each with a finite qubit resource budget. In one instance, our results show the RCPSP approach outperforming the greedy heuristic. In another instance, we find the two performing equally well. Our results thus illustrate the effectiveness of the RCPSP framework, while also underlining the relevance and usefulness of greedy heuristics.", "AI": {"tldr": "\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u7684\u5173\u952e\u5728\u4e8e\u8c03\u5ea6\u5668\uff0c\u5b83\u534f\u8c03\u91cf\u5b50\u7f51\u7edc\u4e0a\u7684\u64cd\u4f5c\u4ee5\u5b9e\u73b0\u8de8\u91cf\u5b50\u5904\u7406\u5355\u5143\uff08QPU\uff09\u7684\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\u3002\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u6700\u5c0f\u5316\u8c03\u5ea6\u8de8\u5ea6\u7684\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8d44\u6e90\u53d7\u9650\u9879\u76ee\u8c03\u5ea6\uff08RCPSP\uff09\u6846\u67b6\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u7684\u5b9e\u4f8b\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0RCPSP\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u800c\u5728\u53e6\u4e00\u4e9b\u60c5\u51b5\u4e0b\u4e24\u8005\u8868\u73b0\u76f8\u5f53\uff0c\u8fd9\u8868\u660e\u4e86RCPSP\u6846\u67b6\u7684\u6709\u6548\u6027\u4ee5\u53ca\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u9700\u8981\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u800c\u534f\u8c03\u91cf\u5b50\u7f51\u7edc\u64cd\u4f5c\u4ee5\u5b9e\u73b0\u8de8QPU\u7684\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\u662f\u5176\u4e2d\u7684\u5173\u952e\u3002\u6700\u5c0f\u5316\u8c03\u5ea6\u8de8\u5ea6\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7f51\u7edc\u4e2d\u6709\u6548\u5229\u7528\u8d44\u6e90\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u4f7f\u7528METIS\u7b49\u5de5\u5177\u5c06\u8ba1\u7b97\u7535\u8def\u5206\u533a\u5e76\u5206\u914d\u7ed9\u4e0d\u540c\u7684QPU\uff0c\u4ee5\u6700\u5c0f\u5316\u8de8\u5206\u533a\u6570\u91cf\u7684\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\uff0c\u5e76\u4f7fQPU\u7684\u91cf\u5b50\u6bd4\u7279\u8d1f\u8f7d\u63a5\u8fd1\u5747\u5300\u30022. \u8bc6\u522b\u8de8\u5206\u533a\u6240\u9700\u7684\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u5b9e\u73b0QPU\u4e4b\u95f4\u5fc5\u8981\u7ea0\u7f20\u7684\u7f51\u7edc\u64cd\u4f5c\u5e8f\u5217\u30023. \u6700\u5c0f\u5316\u8c03\u5ea6\u7f51\u7edc\u64cd\u4f5c\uff0c\u4ee5\u7f29\u77ed\u603b\u6267\u884c\u65f6\u95f4\uff08\u8de8\u5ea6\uff09\u30024. \u6bd4\u8f83\u4e86\u57fa\u4e8eRCPSP\u6846\u67b6\u7684\u65b9\u6cd5\u548c\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u89e3\u51b3\u8c03\u5ea6\u8de8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u30025. \u4ee5\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u5728\u4e2d\u5fc3\u8f90\u5c04\u578b\uff08\u661f\u578b\uff09\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u5b9e\u73b0\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u5bf9\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u5728\u4e2d\u5fc3\u8f90\u5c04\u578b\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u5b9e\u4f8b\u5206\u6790\uff0c\u53d1\u73b0\u5728\u4e00\u4e2a\u5b9e\u4f8b\u4e2dRCPSP\u65b9\u6cd5\u4f18\u4e8e\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u800c\u5728\u53e6\u4e00\u4e2a\u5b9e\u4f8b\u4e2d\u4e24\u8005\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "RCPSP\u6846\u67b6\u5728\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u7684\u8c03\u5ea6\u8de8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u4e0a\u663e\u793a\u51fa\u6709\u6548\u6027\uff0c\u540c\u65f6\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e5f\u5177\u6709\u5176\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12193", "abs": "https://arxiv.org/abs/2511.12193", "authors": ["Abdelrahman Elsayed", "Ahmed Jaheen", "Mohammad Yaqub"], "title": "MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis", "comment": "Under Review at The IEEE International Symposium on Biomedical Imaging (ISBI 2026)", "summary": "Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.", "AI": {"tldr": "MMRINet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u4f7f\u7528Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u66ff\u4ee3\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u4e86\u53cc\u901a\u8def\u7279\u5f81\u7ec6\u5316\uff08DPFR\uff09\u548c\u6e10\u8fdb\u7279\u5f81\u805a\u5408\uff08PFA\uff09\u6a21\u5757\uff0c\u5728BraTS-Lighthouse SSA 2025\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u8111\u80bf\u7624\u5206\u5272\uff0c\u53c2\u6570\u91cf\u4ec5\u7ea6250\u4e07\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u76843D\u6df1\u5ea6\u7f51\u7edc\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u8111\u80bf\u7624\u5206\u5272\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMRINet\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\u3002\u8be5\u67b6\u6784\u91c7\u7528\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6765\u66ff\u4ee3\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4f53\u79ef\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u53cc\u901a\u8def\u7279\u5f81\u7ec6\u5316\uff08DPFR\uff09\u6a21\u5757\u6765\u6700\u5927\u5316\u7279\u5f81\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u6e10\u8fdb\u7279\u5f81\u805a\u5408\uff08PFA\uff09\u6a21\u5757\u6765\u5b9e\u73b0\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728BraTS-Lighthouse SSA 2025\u7ade\u8d5b\u4e2d\uff0cMMRINet\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e73\u5747Dice\u5206\u6570\u4e3a0.752\uff0c\u5e73\u5747HD95\u8bc4\u5206\u4e3a12.23\u3002\u8be5\u6a21\u578b\u7684\u53c2\u6570\u91cf\u4ec5\u7ea6250\u4e07\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4f4e\u8d44\u6e90\u4e34\u5e8a\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "MMRINet\u901a\u8fc7\u91c7\u7528Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001DPFR\u548cPFA\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8111\u80bf\u7624\u5206\u5272\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5206\u5272\u7cbe\u5ea6\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u4f4e\u8d44\u6e90\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.13648", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13648", "abs": "https://arxiv.org/abs/2511.13648", "authors": ["Ziang Cao", "Fangzhou Hong", "Zhaoxi Chen", "Liang Pan", "Ziwei Liu"], "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image", "comment": "Project page: https://physx-anything.github.io/", "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.", "AI": {"tldr": "PhysX-Anything\u662f\u4e00\u4e2a\u9996\u4e2a\u652f\u6301\u7269\u7406\u6a21\u62df\u76843D\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u5355\u5f20\u56fe\u50cf\u8f6c\u6362\u4e3a\u5305\u542b\u51e0\u4f55\u3001\u5173\u8282\u548c\u7269\u7406\u5c5e\u6027\u7684\u3001\u53ef\u76f4\u63a5\u7528\u4e8e\u6a21\u62df\u76843D\u8d44\u4ea7\uff0c\u901a\u8fc7\u9ad8\u6548\u76843D\u8868\u793a\u548c\u65b0\u7684\u6570\u636e\u96c6PhysX-Mobility\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7b49\u9886\u57df\u5c55\u73b0\u4e86\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5ffd\u7565\u4e86\u7269\u7406\u548c\u5173\u8282\u5c5e\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5177\u8eabAI\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7269\u74063D\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u79cd\u9ad8\u6548\u76843D\u8868\u793a\u65b9\u6cd5\uff08\u5c06token\u6570\u91cf\u51cf\u5c11193\u500d\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b2K+\u5e38\u89c1\u73b0\u5b9e\u4e16\u754c\u7269\u4f53\u53ca\u5176\u7269\u7406\u6ce8\u91ca\u7684\u65b0\u6570\u636e\u96c6PhysX-Mobility\u3002", "result": "PhysX-Anything\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u3001\u53ef\u7528\u4e8e\u6a21\u62df\u76843D\u8d44\u4ea7\uff0c\u5e76\u5728PhysX-Mobility\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728MuJoCo\u98ce\u683c\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u751f\u6210\u76843D\u8d44\u4ea7\u53ef\u76f4\u63a5\u7528\u4e8e\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "PhysX-Anything\u6709\u671b\u6781\u5927\u4fc3\u8fdb\u5177\u8eabAI\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.13700", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13700", "abs": "https://arxiv.org/abs/2511.13700", "authors": ["Boldizs\u00e1r Po\u00f3r", "Benjamin Rodatz", "Aleks Kissinger"], "title": "Ultra Low Overhead Syndrome Extraction for the Steane code", "comment": "9 pages, 6 figures", "summary": "We establish a new performance benchmark for the fault-tolerant syndrome extraction of [[7, 1, 3]] Steane code with a dynamic protocol. Our method is built on two highly optimized circuits derived using fault-equivalent ZX-rewrites: a primary fault-tolerant circuit with 14 CNOTs and an efficient non-fault-tolerant recovery circuit with 11 CNOTs. The protocol uses an adaptive response to internal faults, discarding flagged measurements and falling back to the recovery circuit to correct potentially detrimental errors. Monte Carlo simulations confirm the efficiency of our protocol, reducing the logical error rate per cycle by an average of ~14.3% relative to the optimized Steane method [arXiv:2506.17181] and ~17.7% compared to the Reichardt's three-qubit method [arXiv:1804.06995], the leading prior techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e[[7, 1, 3]] Steane\u7801\u7684\u5bb9\u9519\u6027\u7efc\u5408\u63d0\u53d6\u65b0\u57fa\u51c6\uff0c\u91c7\u7528\u52a8\u6001\u534f\u8bae\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8[[7, 1, 3]] Steane\u7801\u7684\u5bb9\u9519\u6027\u7efc\u5408\u63d0\u53d6\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u9ad8\u5ea6\u4f18\u5316\u7684\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\uff1a\u4e00\u4e2a\u5305\u542b14\u4e2aCNOTs\u7684\u5bb9\u9519\u7535\u8def\u548c\u4e00\u4e2a\u5305\u542b11\u4e2aCNOTs\u7684\u9ad8\u6548\u975e\u5bb9\u9519\u6062\u590d\u7535\u8def\u3002\u8be5\u534f\u8bae\u80fd\u591f\u81ea\u9002\u5e94\u5730\u54cd\u5e94\u5185\u90e8\u6545\u969c\uff0c\u4e22\u5f03\u6807\u8bb0\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e76\u56de\u9000\u5230\u6062\u590d\u7535\u8def\u6765\u7ea0\u6b63\u6f5c\u5728\u7684\u9519\u8bef\u3002", "result": "\u4e0e\u4f18\u5316\u7684Steane\u65b9\u6cd5\u76f8\u6bd4\uff0c\u903b\u8f91\u9519\u8bef\u7387\u5e73\u5747\u964d\u4f4e\u4e86\u7ea614.3%\uff1b\u4e0eReichardt\u7684\u4e09\u6bd4\u7279\u65b9\u6cd5\u76f8\u6bd4\uff0c\u903b\u8f91\u9519\u8bef\u7387\u5e73\u5747\u964d\u4f4e\u4e86\u7ea617.7%\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u534f\u8bae\u5728[[7, 1, 3]] Steane\u7801\u7684\u5bb9\u9519\u6027\u7efc\u5408\u63d0\u53d6\u65b9\u9762\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u6709\u6548\u3002"}}
{"id": "2511.12196", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12196", "abs": "https://arxiv.org/abs/2511.12196", "authors": ["Aditi Bhalla", "Christian Hellert", "Enkelejda Kasneci"], "title": "Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System", "comment": null, "summary": "Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u4fe1\u606f\u74f6\u9888\u635f\u5931\u6765\u8054\u5408\u89e3\u51b3\u9a7e\u9a76\u5458\u5206\u5fc3\u8bc6\u522b\u4e2d\u7684\u8de8\u89c6\u56fe\u548c\u8de8\u6a21\u6001\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u6444\u50cf\u5934\u89c6\u89d2\u548c\u4f20\u611f\u5668\u7c7b\u578b\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8bc6\u522b\u9a7e\u9a76\u5458\u5206\u5fc3\u65b9\u9762\u5b58\u5728\u8de8\u89c6\u56fe\uff08\u6444\u50cf\u5934\u89c6\u89d2\u53d8\u5316\uff09\u548c\u57df\u79fb\u4f4d\uff08\u4f20\u611f\u5668\u7c7b\u578b\u6216\u73af\u5883\u53d8\u5316\uff09\u7684\u6311\u6218\uff0c\u963b\u788d\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u51b3\u5176\u4e2d\u4e00\u4e2a\u95ee\u9898\uff0c\u800c\u6ca1\u6709\u8054\u5408\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u4e14\u52a8\u4f5c\u53ef\u533a\u5206\u7684\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5229\u7528\u4fe1\u606f\u74f6\u9888\u635f\u5931\u8fdb\u884c\u8de8\u6a21\u6001\u57df\u9002\u5e94\uff0c\u65e0\u9700\u65b0\u9886\u57df\u6807\u7b7e\u6570\u636e\u3002", "result": "\u5728Drive&Act\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u4e0estate-of-the-art\u89c6\u9891Transformer\uff08Video Swin, MViT\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8de8\u89c6\u56fe\u65b9\u6cd5\uff0c\u5728RGB\u89c6\u9891\u6570\u636e\u4e0a\u7684top-1\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u8fd150%\uff1b\u76f8\u6bd4\u4ec5\u8fdb\u884c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe5%\u3002", "conclusion": "\u8be5\u8054\u5408\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8de8\u89c6\u56fe\u548c\u8de8\u6a21\u6001\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u5458\u5206\u5fc3\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5728\u591a\u6837\u5316\u7684\u8f66\u8f86\u914d\u7f6e\u548c\u73af\u5883\u4e2d\u90e8\u7f72\u6a21\u578b\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13719", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13719", "abs": "https://arxiv.org/abs/2511.13719", "authors": ["Zhongang Cai", "Ruisi Wang", "Chenyang Gu", "Fanyi Pu", "Junxiang Xu", "Yubo Wang", "Wanqi Yin", "Zhitao Yang", "Chen Wei", "Qingping Sun", "Tongxi Zhou", "Jiaqi Li", "Hui En Pang", "Oscar Qian", "Yukun Wei", "Zhiqian Lin", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Liang Pan", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "title": "Scaling Spatial Intelligence with Multimodal Foundation Models", "comment": "Model: https://huggingface.co/collections/sensenova/sensenova-si; Code: https://github.com/OpenSenseNova/SenseNova-SI", "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.", "AI": {"tldr": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u4f46SenseNova-SI\u6a21\u578b\u5bb6\u65cf\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u548c\u7cbe\u5fc3\u7b56\u5212\u7684800\u4e07\u4e2a\u7a7a\u95f4\u80fd\u529b\u76f8\u5173\u6570\u636e\uff0c\u5728VSI-Bench\u3001MMSI\u3001MindCube\u3001ViewSpatial\u548cSITE\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u6570\u636e\u6269\u5c55\u7684\u5f71\u54cd\u3001\u6d8c\u73b0\u7684\u6cdb\u5316\u80fd\u529b\u3001\u8fc7\u62df\u5408\u548c\u8bed\u8a00\u6377\u5f84\u7684\u98ce\u9669\uff0c\u5e76\u521d\u6b65\u7814\u7a76\u4e86\u7a7a\u95f4\u94fe\u5f0f\u601d\u8003\u63a8\u7406\u548c\u4e0b\u6e38\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u5b58\u5728\u660e\u663e\u77ed\u677f\uff0c\u9700\u8981\u8fdb\u884c\u6269\u5c55\u4ee5\u63d0\u5347\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86SenseNova-SI\u6a21\u578b\u5bb6\u65cf\uff0c\u57fa\u4e8eQwen3-VL\u548cInternVL3\u7b49\u89c6\u89c9\u7406\u89e3\u6a21\u578b\u4ee5\u53caBagel\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7b56\u5212\u5305\u542b800\u4e07\u4e2a\u591a\u6837\u5316\u6570\u636e\u6837\u672c\u3001\u9075\u5faa\u4e25\u683c\u7684\u7a7a\u95f4\u80fd\u529b\u5206\u7c7b\u6cd5\u7684\u6570\u636e\u96c6SenseNova-SI-8M\uff0c\u6765\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u9c81\u68d2\u7684\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u3002", "result": "SenseNova-SI\u5728VSI-Bench\uff0868.7%\uff09\u3001MMSI\uff0843.3%\uff09\u3001MindCube\uff0885.6%\uff09\u3001ViewSpatial\uff0854.6%\uff09\u548cSITE\uff0850.1%\uff09\u7b49\u7a7a\u95f4\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728MMBench-En\uff0884.9%\uff09\u7b49\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "\u6269\u5c55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5e76\u7ed3\u5408\u7cbe\u5fc3\u7b56\u5212\u7684\u5927\u89c4\u6a21\u7a7a\u95f4\u6570\u636e\u96c6\u662f\u63d0\u5347\u6a21\u578b\u7a7a\u95f4\u667a\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002SenseNova-SI\u6a21\u578b\u5bb6\u65cf\u5c55\u793a\u4e86\u5728\u7a7a\u95f4\u667a\u80fd\u4efb\u52a1\u4e0a\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e14\u901a\u8fc7\u5206\u6790\u6570\u636e\u6269\u5c55\u3001\u6cdb\u5316\u80fd\u529b\u3001\u8fc7\u62df\u5408\u98ce\u9669\u3001\u7a7a\u95f4\u94fe\u5f0f\u601d\u8003\u63a8\u7406\u4ee5\u53ca\u4e0b\u6e38\u5e94\u7528\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002\u7814\u7a76\u6210\u679c\u548c\u6a21\u578b\u5df2\u516c\u5f00\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.13721", "categories": ["quant-ph", "hep-lat", "hep-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2511.13721", "abs": "https://arxiv.org/abs/2511.13721", "authors": ["Xiaojun Yao"], "title": "Quantum Error Correction Codes for Truncated SU(2) Lattice Gauge Theories", "comment": "11 pages, 10 figures", "summary": "We construct two quantum error correction codes for pure SU(2) lattice gauge theory in the electric basis truncated at the electric flux $j_{\\rm max}=1/2$, which are applicable on quasi-1D plaquette chains, 2D honeycomb and 3D triamond and hyperhoneycomb lattices. The first code converts Gauss's law at each vertex into a stabilizer while the second only uses half vertices and is locally the carbon code. Both codes are able to correct single-qubit errors. The electric and magnetic terms in the SU(2) Hamiltonian are expressed in terms of logical gates in both codes. The logical-gate Hamiltonian in the first code exactly matches the spin Hamiltonian for gauge singlet states found in previous work.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e24\u79cd\u9002\u7528\u4e8e\u7eafSU(2)\u683c\u5b50\u89c4\u8303\u7406\u8bba\u7684\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u9002\u7528\u4e8e\u51c6\u4e00\u7ef4\u3001\u4e8c\u7ef4\u548c\u4e09\u7ef4\u6676\u683c\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u7eafSU(2)\u683c\u5b50\u89c4\u8303\u7406\u8bba\u6784\u5efa\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u8be5\u7406\u8bba\u5728\u7eafSU(2)\u683c\u5b50\u89c4\u8303\u7406\u8bba\u7684\u7535\u57fa\u622a\u65ad\u5728\u7535\u901a\u91cf$j_{\\max}=1/2$\u3002", "method": "\u7b2c\u4e00\u79cd\u7f16\u7801\u5c06\u6bcf\u4e2a\u9876\u70b9\u7684Gauss\u5b9a\u5f8b\u8f6c\u6362\u4e3a\u7a33\u5b9a\u5b50\uff0c\u7b2c\u4e8c\u79cd\u7f16\u7801\u4ec5\u4f7f\u7528\u4e00\u534a\u9876\u70b9\uff0c\u5e76\u4e14\u5728\u5c40\u90e8\u662f\u78b3\u7f16\u7801\u3002\u4e24\u79cd\u7f16\u7801\u90fd\u80fd\u7ea0\u6b63\u5355\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u3002SU(2)\u54c8\u5bc6\u987f\u91cf\u7684\u7535\u548c\u78c1\u9879\u7528\u4e24\u79cd\u7f16\u7801\u4e2d\u7684\u903b\u8f91\u95e8\u8868\u793a\u3002", "result": "\u7b2c\u4e00\u79cd\u7f16\u7801\u7684\u903b\u8f91\u95e8\u54c8\u5bc6\u987f\u91cf\u4e0e\u5148\u524d\u5de5\u4f5c\u4e2d\u53d1\u73b0\u7684\u89c4\u8303\u5355\u6001\u7684\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u7cbe\u786e\u5339\u914d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u91cf\u5b50\u7ea0\u9519\u7801\u80fd\u591f\u7ea0\u6b63\u5355\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406SU(2)\u54c8\u5bc6\u987f\u91cf\u7684\u7535\u548c\u78c1\u9879\u3002"}}
{"id": "2511.12200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12200", "abs": "https://arxiv.org/abs/2511.12200", "authors": ["Sujun Sun", "Haowen Gu", "Cheng Xie", "Yanxu Ren", "Mingwu Ren", "Haofeng Zhang"], "title": "Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation", "comment": "Accepted by AAAI 2026", "summary": "Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5206\u5c42\u8bed\u4e49\u5b66\u4e60\u201d\uff08HSL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\uff08CD-FSS\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u7c92\u5ea6\u5dee\u8ddd\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u53cc\u98ce\u683c\u968f\u673a\u5316\u201d\uff08DSR\uff09\u548c\u201c\u5206\u5c42\u8bed\u4e49\u6316\u6398\u201d\uff08HSM\uff09\u6a21\u5757\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0d\u540c\u7c92\u5ea6\u8bed\u4e49\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u201c\u539f\u578b\u7f6e\u4fe1\u5ea6\u8c03\u5236\u9608\u503c\u201d\uff08PCMT\uff09\u6a21\u5757\u6765\u5904\u7406\u524d\u666f\u548c\u80cc\u666f\u76f8\u4f3c\u5ea6\u9ad8\u7684\u60c5\u51b5\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CD-FSS\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u98ce\u683c\u5dee\u5f02\uff0c\u5ffd\u89c6\u4e86\u7c92\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u5728\u76ee\u6807\u57df\u4e2d\u5bf9\u65b0\u7c7b\u522b\u7684\u8bed\u4e49\u533a\u5206\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8bed\u4e49\u5b66\u4e60\uff08HSL\uff09\u6846\u67b6\uff0c\u5305\u62ec\u53cc\u98ce\u683c\u968f\u673a\u5316\uff08DSR\uff09\u548c\u5206\u5c42\u8bed\u4e49\u6316\u6398\uff08HSM\uff09\u6a21\u5757\u3002DSR\u901a\u8fc7\u524d\u666f\u548c\u5168\u5c40\u98ce\u683c\u968f\u673a\u5316\u6765\u6a21\u62df\u76ee\u6807\u57df\u6570\u636e\uff1bHSM\u5229\u7528\u591a\u5c3a\u5ea6\u8d85\u50cf\u7d20\u6765\u6307\u5bfc\u6a21\u578b\u6316\u6398\u4e0d\u540c\u7c92\u5ea6\u7684\u7c7b\u5185\u4e00\u81f4\u6027\u548c\u7c7b\u95f4\u533a\u5206\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u539f\u578b\u7f6e\u4fe1\u5ea6\u8c03\u5236\u9608\u503c\uff08PCMT\uff09\u6a21\u5757\u6765\u5904\u7406\u524d\u666f\u548c\u80cc\u666f\u76f8\u4f3c\u5ea6\u9ad8\u7684\u60c5\u51b5\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u7684\u76ee\u6807\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HSL\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u589e\u5f3a\u6a21\u578b\u8bc6\u522b\u4e0d\u540c\u7c92\u5ea6\u8bed\u4e49\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8CD-FSS\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11752", "categories": ["cs.AI", "cs.DL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11752", "abs": "https://arxiv.org/abs/2511.11752", "authors": ["S\u00f6ren Arlt", "Xuemei Gu", "Mario Krenn"], "title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "comment": "24 pages, 5 figures", "summary": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "AI": {"tldr": "AI-Mandel\u662f\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u548c\u6267\u884c\u91cf\u5b50\u7269\u7406\u5b66\u7814\u7a76\u60f3\u6cd5\u7684LLM\u4ee3\u7406\uff0c\u80fd\u591f\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u5e76\u63ed\u793a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u5316\u79d1\u5b66\u4e2d\u7684\u60f3\u6cd5\u751f\u6210\u548c\u6267\u884c\uff0c\u4ee5\u8f6c\u53d8\u4eba\u7c7b\u5728\u79d1\u5b66\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "AI-Mandel\u5229\u7528\u6587\u732e\u8d44\u6599\u751f\u6210\u7814\u7a76\u60f3\u6cd5\uff0c\u5e76\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684AI\u5de5\u5177\u5c06\u8fd9\u4e9b\u60f3\u6cd5\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "result": "AI-Mandel\u80fd\u591f\u751f\u6210\u79d1\u5b66\u4e0a\u6709\u4ef7\u503c\u7684\u60f3\u6cd5\uff0c\u5305\u62ec\u91cf\u5b50\u4f20\u9001\u7684\u65b0\u53d8\u4f53\u3001\u4e0d\u786e\u5b9a\u56e0\u679c\u987a\u5e8f\u4e2d\u7684\u91cf\u5b50\u7f51\u7edc\u539f\u8bed\uff0c\u4ee5\u53ca\u57fa\u4e8e\u91cf\u5b50\u4fe1\u606f\u4f20\u8f93\u95ed\u5408\u5faa\u73af\u7684\u51e0\u4f55\u76f8\u4f4d\u65b0\u6982\u5ff5\u3002\u5176\u4e2d\u4e24\u4e2a\u60f3\u6cd5\u5df2\u4fc3\u6210\u72ec\u7acb\u7684\u79d1\u5b66\u540e\u7eed\u8bba\u6587\u3002", "conclusion": "AI-Mandel\u5c55\u793a\u4e86AI\u7269\u7406\u5b66\u5bb6\u7684\u539f\u578b\uff0c\u80fd\u591f\u751f\u6210\u548c\u6267\u884c\u5177\u4f53\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u60f3\u6cd5\uff0c\u8fd9\u4e0d\u4ec5\u80fd\u52a0\u901f\u79d1\u5b66\u53d1\u5c55\uff0c\u8fd8\u80fd\u63ed\u793a\u5b9e\u73b0\u7c7b\u4eba\u4eba\u5de5\u667a\u80fd\u79d1\u5b66\u5bb6\u6240\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2511.12201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12201", "abs": "https://arxiv.org/abs/2511.12201", "authors": ["Feng Chen", "Yefei He", "Shaoxuan He", "Yuanyu He", "Jing Liu", "Lequan Lin", "Akide Liu", "Zhaoyang Li", "Jiyuan Zhang", "Zhenbang Sun", "Bohan Zhuang", "Qi Wu"], "title": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs", "comment": "Accepted by AAAI2026", "summary": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.", "AI": {"tldr": "OmniSparse\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u611f\u77e5\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u53ef\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u52a8\u6001\u5206\u914d\u6807\u8bb0\u9884\u7b97\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e3b\u8981\u5728\u63a8\u7406\u65f6\u52a0\u901f\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u3001\u7ec6\u7c92\u5ea6\u9009\u62e9\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u52a0\u901f\u6548\u679c\u6709\u9650\u3002", "method": "OmniSparse\u5305\u542b\u67e5\u8be2\u9009\u62e9\uff08\u901a\u8fc7\u60f0\u6027-\u6fc0\u6d3b\u5206\u7c7b\uff09\u3001\u952e\u503c\uff08KV\uff09\u9009\u62e9\uff08\u901a\u8fc7\u5934\u7ea7\u52a8\u6001\u9884\u7b97\u5206\u914d\uff09\u548cKV\u7f13\u5b58\u7626\u8eab\uff08\u6839\u636e\u5934\u7ea7\u89e3\u7801\u67e5\u8be2\u6a21\u5f0f\u9009\u62e9\u6027\u83b7\u53d6\u89c6\u89c9KV\u7f13\u5b58\uff09\u4e09\u79cd\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOmniSparse\u5728\u6027\u80fd\u4e0a\u53ef\u5ab2\u7f8e\u5168\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u9884\u586b\u5145\u65f6\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe2.7\u500d\uff0c\u5728\u89e3\u7801\u65f6\u5185\u5b58\u51cf\u5c112.4\u500d\u3002", "conclusion": "OmniSparse\u901a\u8fc7\u8bad\u7ec3\u611f\u77e5\u3001\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u5185\u5b58\u8282\u7701\u3002"}}
{"id": "2511.12202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12202", "abs": "https://arxiv.org/abs/2511.12202", "authors": ["Zhuojiang Cai", "Yiheng Zhang", "Meitong Guo", "Mingdao Wang", "Yuwang Wang"], "title": "LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image", "comment": null, "summary": "Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.", "AI": {"tldr": "LSS3D\u662f\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u52303D\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u504f\u79fb\u6765\u89e3\u51b3\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u548c\u975e\u6b63\u9762\u8f93\u5165\u89c6\u56fe\u7684\u95ee\u9898\uff0c\u4ece\u800c\u751f\u6210\u7ec6\u8282\u5b8c\u6574\u3001\u7eb9\u7406\u6e05\u6670\u76843D\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u6269\u65633D\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u89c6\u56fe\u95f4\u5f62\u72b6\u548c\u7eb9\u7406\u5bf9\u9f50\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f43D\u751f\u6210\u7ed3\u679c\u8d28\u91cf\u4e0d\u9ad8\uff0c\u51e0\u4f55\u7ec6\u8282\u4e0d\u5b8c\u6574\uff0c\u7eb9\u7406\u51fa\u73b0\u91cd\u5f71\u3002\u90e8\u5206\u65b9\u6cd5\u4ec5\u9488\u5bf9\u6b63\u9762\u89c6\u56fe\u4f18\u5316\uff0c\u5bf9\u659c\u4fa7\u89c6\u56fe\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLSS3D\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u52303D\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u504f\u79fb\uff08learnable spatial shifting\uff09\u6765\u663e\u5f0f\u4e14\u6709\u6548\u5730\u5904\u7406\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u548c\u975e\u6b63\u9762\u8f93\u5165\u89c6\u56fe\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u6bcf\u4e2a\u89c6\u56fe\u5206\u914d\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u504f\u79fb\u53c2\u6570\uff0c\u5e76\u5728\u91cd\u5efa\u7f51\u683c\u7684\u6307\u5bfc\u4e0b\uff0c\u5c06\u6bcf\u4e2a\u89c6\u56fe\u8c03\u6574\u5230\u4e00\u4e2a\u7a7a\u95f4\u4e00\u81f4\u7684\u76ee\u6807\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u8282\u66f4\u5b8c\u6574\u3001\u7eb9\u7406\u66f4\u6e05\u6670\u7684\u9ad8\u8d28\u91cf3D\u751f\u6210\u3002\u6b64\u5916\uff0c\u5c06\u8f93\u5165\u89c6\u56fe\u4f5c\u4e3a\u989d\u5916\u7684\u4f18\u5316\u7ea6\u675f\uff0c\u589e\u5f3a\u4e86\u5bf9\u975e\u6b63\u9762\u8f93\u5165\u89d2\u5ea6\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4fef\u89c6\u8f93\u5165\u7684\u89c6\u56fe\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0cLSS3D\u5728\u51e0\u4f55\u548c\u7eb9\u7406\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u80fd\u53d6\u5f97\u9886\u5148\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u66f4\u591a\u7075\u6d3b\u7684\u8f93\u5165\u89c6\u89d2\u3002", "conclusion": "LSS3D\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u504f\u79fb\u548c\u5c06\u8f93\u5165\u89c6\u56fe\u4f5c\u4e3a\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe3D\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u9c81\u68d2\u6027\u76843D\u751f\u6210\u3002"}}
{"id": "2511.12204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12204", "abs": "https://arxiv.org/abs/2511.12204", "authors": ["Jiaqi Wu", "Yaosen Chen", "Shuyuan Zhu"], "title": "GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction", "comment": null, "summary": "Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u4fe1\u606f\u63d0\u53d6\u3001\u89e3\u8026\u7684\u51e0\u4f55\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\u548c\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u56fe\u50cf\u6269\u5c55\u65b9\u6cd5\u5728\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u4e0a\u9762\u4e34\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u4ece\u800c\u751f\u6210\u7ec6\u8282\u4e30\u5bcc\u4e14\u89c6\u56fe\u95f4\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5355\u56fe\u50cf\u6269\u5c55\u7684\u591a\u89c6\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\uff0c\u5305\u542b\uff1a1.\u591a\u89c6\u56fe\u51e0\u4f55\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\uff08\u5229\u7528\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u3001\u524d\u666f\u5206\u5272\u63a9\u6a21\u6784\u5efa\u5171\u4eab\u51e0\u4f55\u7ed3\u6784\uff09\u30022.\u89e3\u8026\u7684\u51e0\u4f55\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\uff08\u589e\u5f3a\u6a21\u578b\u5bf9\u5173\u952e\u51e0\u4f55\u7ec6\u8282\u7684\u5173\u6ce8\uff09\u30023.\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff08\u4f18\u5316\u751f\u6210\u89c6\u56fe\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u548c\u89c6\u89c9\u8fde\u8d2f\u6027\uff09\u30024.\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\uff08\u591a\u9636\u6bb5\u9010\u6b65\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff09\u30025.\u52a8\u6001\u51e0\u4f55\u4fe1\u606f\u5f3a\u5ea6\u8c03\u6574\u673a\u5236\uff08\u81ea\u9002\u5e94\u8c03\u8282\u51e0\u4f55\u6570\u636e\u7684\u5f71\u54cd\uff09\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u5728\u89c6\u56fe\u95f4\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u7ec6\u8282\u4e30\u5bcc\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u89c6\u56fe\u95f4\u4e00\u81f4\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u3002"}}
{"id": "2511.12206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12206", "abs": "https://arxiv.org/abs/2511.12206", "authors": ["Nishant Vasantkumar Hegde", "Aditi Agarwal", "Minal Moharir"], "title": "A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR", "comment": "6 pages, 4 figures. Published in: Proceedings of the 12th International Conference on Emerging Trends in Engineering Technology Signal and Information Processing (ICETET SIP 2025) Note: The conference proceedings contain an outdated abstract due to a publisher-side error. This arXiv version includes the correct and updated abstract", "summary": "Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u5229\u7528YOLOv8\u548cEasyOCR\u6280\u672f\u81ea\u52a8\u68c0\u6d4b\u6469\u6258\u8f66\u672a\u6234\u5934\u76d4\u548c\u7f3a\u5c11\u540e\u89c6\u955c\u7684\u8fdd\u89c4\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7OCR\u8bc6\u522b\u8f66\u724c\uff0c\u65e8\u5728\u63d0\u9ad8\u6267\u6cd5\u6548\u7387\u548c\u9053\u8def\u5b89\u5168\u3002", "motivation": "\u624b\u52a8\u6267\u6cd5\u6469\u6258\u8f66\u5934\u76d4\u4f69\u6234\u548c\u8f66\u8f86\u5b89\u5168\u6807\u51c6\uff08\u5982\u540e\u89c6\u955c\uff09\u8017\u65f6\u8017\u529b\u4e14\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u6267\u6cd5\u6548\u7387\u548c\u9053\u8def\u5b89\u5168\u3002", "method": "\u4f7f\u7528YOLOv8\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0cEasyOCR\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\u3002\u5bf9\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8fdb\u884c\u589e\u5f3a\u8bad\u7ec3\uff0c\u4ee5\u8bc6\u522b\u672a\u6234\u5934\u76d4\u3001\u7f3a\u5c11\u540e\u89c6\u955c\u7684\u8fdd\u89c4\u884c\u4e3a\u5e76\u63d0\u53d6\u8f66\u724c\u53f7\u7801\u3002\u91c7\u7528Streamlit\u6784\u5efa\u7528\u6237\u754c\u9762\uff0c\u5e76\u8fdb\u884c\u56fe\u50cf\u9884\u5904\u7406\u4ee5\u63d0\u9ad8\u8f66\u724c\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u8bc4\u4f30\u4e2d\u8fbe\u52300.9147\u7684\u603b\u4f53\u7cbe\u786e\u7387\uff0c0.886\u7684\u53ec\u56de\u7387\uff0c\u4ee5\u53ca0.843\u7684mAP@50\u3002mAP@50 95\u4e3a0.503\uff0c\u8868\u660e\u5728\u66f4\u4e25\u683c\u7684IoU\u9608\u503c\u4e0b\u4ecd\u6709\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u4e2a\u5b9e\u9645\u6709\u6548\u4e14\u53ef\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u4ea4\u901a\u8fdd\u89c4\u6267\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9053\u8def\u5b89\u5168\u548c\u6267\u6cd5\u6548\u7387\u3002"}}
{"id": "2511.13497", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13497", "abs": "https://arxiv.org/abs/2511.13497", "authors": ["Liudmila A. Zhukas", "Vivian Ni Zhang", "Qiang Miao", "Qingfeng Wang", "Marko Cetina", "Jungsang Kim", "Lawrence Carin", "Christopher Monroe"], "title": "Quantum Machine Learning via Contrastive Training", "comment": "7 figures, 20 pages total", "summary": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.", "AI": {"tldr": "\u901a\u8fc7\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u4f7f\u7528\u65e0\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6807\u7b7e\u6570\u636e\u7a00\u758f\u65f6\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4e3a\u89e3\u51b3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9762\u5bf9\u6807\u7b7e\u6570\u636e\u7a00\u758f\u95ee\u9898\u65f6\u7684\u6311\u6218\uff0c\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5728\u53ef\u7f16\u7a0b\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u91cf\u5b50\u8868\u5f81\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u91cd\u53e0\u5ea6\u91cf\u6765\u5b66\u4e60\u4e0d\u53d8\u91cf\u3002", "result": "\u4e0e\u968f\u673a\u521d\u59cb\u5316\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u95f4\u53d8\u5f02\u6027\uff0c\u5c24\u5176\u5728\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91cf\u5b50\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u6807\u7b7e\u6548\u7387\u7684\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u91cf\u5b50\u539f\u751f\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u5904\u7406\u66f4\u5927\u7684\u7ecf\u5178\u8f93\u5165\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u5f84\u3002"}}
{"id": "2511.12207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12207", "abs": "https://arxiv.org/abs/2511.12207", "authors": ["Haozhe Liu", "Ding Liu", "Mingchen Zhuge", "Zijian Zhou", "Tian Xie", "Sen He", "Yukang Yang", "Shuming Liu", "Yuren Cong", "Jiadong Guo", "Hongyu Xu", "Ke Xu", "Kam-Woh Ng", "Juan C. P\u00e9rez", "Juan-Manuel~P\u00e9rez-R\u00faa", "Tao Xiang", "Wei Liu", "Shikun Liu", "J\u00fcrgen Schmidhuber"], "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation", "comment": null, "summary": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\u03b5$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.", "AI": {"tldr": "MoS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u878d\u5408\u8303\u5f0f\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u72b6\u6001\u7684\u4ea4\u4e92\u6765\u878d\u5408\u6a21\u6001\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u3001\u9010\u4ee4\u724c\u7684\u8def\u7531\u5668\uff0c\u8be5\u8def\u7531\u5668\u6839\u636e\u53bb\u566a\u65f6\u95f4\u6b65\u548c\u8f93\u5165\u521b\u5efa\u6a21\u6001\u9690\u85cf\u72b6\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u5b9e\u73b0\u4ee4\u724c\u7ea7\u7279\u5f81\u4e0e\u6269\u6563\u8f68\u8ff9\u7684\u7cbe\u786e\u5bf9\u9f50\u3002\u8be5\u8def\u7531\u5668\u91c7\u7528\u03b5-greedy\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u548c\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u6709\u6548\u5730\u9009\u62e9\u4e0a\u4e0b\u6587\u7279\u5f81\u3002MoS\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u878d\u5408\u8303\u5f0f\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u6001\u4ea4\u4e92\u548c\u7279\u5f81\u5bf9\u9f50\u3002", "method": "\u5f15\u5165MoS\uff08Mixture of States\uff09\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u3001\u9010\u4ee4\u724c\u7684\u8def\u7531\u5668\uff0c\u8be5\u8def\u7531\u5668\u6839\u636e\u53bb\u566a\u65f6\u95f4\u6b65\u548c\u8f93\u5165\u52a8\u6001\u5730\u9009\u62e9\u548c\u4ea4\u4e92\u6a21\u6001\u7684\u9690\u85cf\u72b6\u6001\uff0c\u91c7\u7528\u03b5-greedy\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u53c2\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff083B-5B\uff09\u80fd\u591f\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u53c2\u6570\u91cf\u59274\u500d\u7684\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "MoS\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u6269\u5c55\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2511.12215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12215", "abs": "https://arxiv.org/abs/2511.12215", "authors": ["Peng Zhang", "Zhihui Lai", "Wenting Chen", "Xu Wu", "Heng Kong"], "title": "FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention", "comment": "AAAI 2026", "summary": "Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.", "AI": {"tldr": "FaNe\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u6b63\u4f8b\u6316\u6398\u548c\u6587\u672c\u6761\u4ef6\u7a00\u758f\u6ce8\u610f\u529b\u6c60\u5316\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u533b\u7597VLP\u4e2d\u7684\u5047\u9634\u6027\u95ee\u9898\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff08VLP\uff09\u65b9\u6cd5\u53d7\u5230\u8bed\u4e49\u76f8\u4f3c\u6587\u672c\u5f15\u8d77\u7684\u5047\u9634\u6027\uff08FaNe\uff09\u548c\u4e0d\u8db3\u7684\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFaNe\u7684\u8bed\u4e49\u589e\u5f3aVLP\u6846\u67b6\u3002\u901a\u8fc7\u57fa\u4e8e\u6587\u672c-\u6587\u672c\u76f8\u4f3c\u5ea6\u7684\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u7b56\u7565\u6765\u6316\u6398\u8bed\u4e49\u611f\u77e5\u7684\u6b63\u4f8b\u5bf9\uff0c\u4ee5\u51cf\u8f7b\u5047\u9634\u6027\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6587\u672c\u6761\u4ef6\u7a00\u758f\u6ce8\u610f\u529b\u6c60\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u6587\u672c\u7ebf\u7d22\u5f15\u5bfc\u7684\u5c40\u90e8\u89c6\u89c9\u8868\u793a\u6765\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u786c\u8d1f\u4f8b\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u8bed\u4e49\u76f8\u4f3c\u7684\u8d1f\u4f8b\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u6001\u5185\u533a\u5206\u3002 ", "result": "\u5728\u4e94\u4e2a\u4e0b\u6e38\u533b\u7597\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaNe\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FaNe\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u5047\u9634\u6027\u548c\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u7597VLP\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6210\u679c\u3002"}}
{"id": "2511.12220", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12220", "abs": "https://arxiv.org/abs/2511.12220", "authors": ["Ameen Ali", "Tamim Zoabi", "Lior Wolf"], "title": "Suppressing VLM Hallucinations with Spectral Representation Filtering", "comment": null, "summary": "Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.", "AI": {"tldr": "VLMs \u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u56e0\u4e3a\u5b83\u4eec\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u548c\u4e0d\u7cbe\u786e\u7684\u8de8\u6a21\u6001\u5173\u8054\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Spectral Representation Filtering (SRF) \u7684\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u540e\u5904\u7406\u6280\u672f\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7ea0\u6b63\u6a21\u578b\u8868\u793a\u7684\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u4ece\u800c\u6291\u5236\u5e7b\u89c9\u3002SRF \u901a\u8fc7\u5bf9\u771f\u5b9e\u548c\u5e7b\u89c9\u5b57\u5e55\u7684\u7279\u5f81\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u8fdb\u884c\u7279\u5f81\u503c\u5206\u89e3\u6765\u8bc6\u522b\u4f4e\u79e9\u5e7b\u89c9\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u5316\u504f\u5dee\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u8f6f\u8c31\u6ee4\u6ce2\u5668\u4f1a\u524a\u5f31\u8fd9\u4e9b\u6a21\u5f0f\u5728\u66f4\u6df1\u5c42 vLLM \u5c42\u7684\u9988\u5165\u6295\u5f71\u6743\u91cd\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5747\u8861\u7279\u5f81\u65b9\u5dee\u3002SRF \u662f\u4e00\u79cd\u7eaf\u7cb9\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4e0d\u6d89\u53ca\u89e3\u7801\u6216\u518d\u8bad\u7ec3\uff0c\u5e76\u4e14\u4e0d\u4f1a\u589e\u52a0\u63a8\u7406\u5f00\u9500\u6216\u9700\u8981\u67b6\u6784\u4fee\u6539\u3002\u5728 LLaVA-1.5\u3001MiniGPT-4 \u548c mPLUG-Owl2 \u7b49 VLM \u7cfb\u5217\u4e0a\uff0cSRF \u5728 MSCOCO\u3001POPE-VQA \u7b49\u89c6\u89c9\u4efb\u52a1\u57fa\u51c6\u4e0a\u6301\u7eed\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u5728\u4e0d\u635f\u5bb3\u5b57\u5e55\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "Vision-language models (VLMs) \u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u662f\u56e0\u4e3a\u5b83\u4eec\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u548c\u4e0d\u7cbe\u786e\u7684\u8de8\u6a21\u6001\u5173\u8054\u3002", "method": "SRF \u901a\u8fc7\u5206\u6790\u548c\u7ea0\u6b63\u6a21\u578b\u8868\u793a\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u6765\u6291\u5236\u5e7b\u89c9\u3002\u5b83\u901a\u8fc7\u5bf9\u771f\u5b9e\u548c\u5e7b\u89c9\u5b57\u5e55\u7684\u7279\u5f81\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u8fdb\u884c\u7279\u5f81\u503c\u5206\u89e3\u6765\u8bc6\u522b\u4f4e\u79e9\u5e7b\u89c9\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u5316\u504f\u5dee\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u8f6f\u8c31\u6ee4\u6ce2\u5668\u4f1a\u524a\u5f31\u8fd9\u4e9b\u6a21\u5f0f\u5728\u66f4\u6df1\u5c42 vLLM \u5c42\u7684\u9988\u5165\u6295\u5f71\u6743\u91cd\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5747\u8861\u7279\u5f81\u65b9\u5dee\u3002", "result": "SRF \u5728 LLaVA-1.5\u3001MiniGPT-4 \u548c mPLUG-Owl2 \u7b49 VLM \u7cfb\u5217\u4e0a\uff0c\u5728 MSCOCO\u3001POPE-VQA \u7b49\u89c6\u89c9\u4efb\u52a1\u57fa\u51c6\u4e0a\u6301\u7eed\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5fe0\u5b9e\u5ea6\uff0c\u540c\u65f6\u6ca1\u6709\u964d\u4f4e\u5b57\u5e55\u8d28\u91cf\u3002", "conclusion": "SRF \u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u8f7b VLM \u4e2d\u7684\u5e7b\u89c9\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0d\u4f1a\u5e26\u6765\u989d\u5916\u7684\u63a8\u7406\u5f00\u9500\u6216\u9700\u8981\u67b6\u6784\u4fee\u6539\u3002"}}
{"id": "2511.12233", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12233", "abs": "https://arxiv.org/abs/2511.12233", "authors": ["Dongdong Zhao", "Qiben Xu", "Ranxin Fang", "Baogang Song"], "title": "Model Inversion Attack Against Deep Hashing", "comment": null, "summary": "Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.", "AI": {"tldr": "\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u9690\u79c1\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u54c8\u5e0c\u7801\u91cd\u5efa\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u8eab\u4efd\u4f2a\u9020\u548c\u9690\u79c1\u6cc4\u9732\u3002\u672c\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u7684\u6269\u6563\u6a21\u578b\u53cd\u6f14\u6846\u67b6DHMI\uff0c\u901a\u8fc7\u805a\u7c7b\u8f85\u52a9\u6570\u636e\u96c6\u83b7\u5f97\u8bed\u4e49\u54c8\u5e0c\u4e2d\u5fc3\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u653b\u51fb\u6307\u6807\uff08\u878d\u5408\u5206\u7c7b\u4e00\u81f4\u6027\u548c\u54c8\u5e0c\u90bb\u8fd1\u6027\uff09\u6765\u6307\u5bfc\u53bb\u566a\u4f18\u5316\uff0c\u4ece\u800c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u6210\u529f\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u5728\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u7684\u540c\u65f6\uff0c\u5e26\u6765\u4e86\u4e25\u91cd\u4e14\u5e38\u88ab\u5ffd\u89c6\u7684\u9690\u79c1\u98ce\u9669\u3002\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u54c8\u5e0c\u7801\u91cd\u5efa\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u8eab\u4efd\u4f2a\u9020\u548c\u9690\u79c1\u6cc4\u9732\u3002\u7136\u800c\uff0c\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u7684\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5176\u5b89\u5168\u5f71\u54cd\u6709\u5f85\u5ba1\u89c6\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u7684\u6269\u6563\u6a21\u578b\u53cd\u6f14\u6846\u67b6DHMI\u3002DHMI\u9996\u5148\u901a\u8fc7\u805a\u7c7b\u8f85\u52a9\u6570\u636e\u96c6\u83b7\u5f97\u8bed\u4e49\u54c8\u5e0c\u4e2d\u5fc3\u4f5c\u4e3a\u66ff\u4ee3\u951a\u70b9\u3002\u7136\u540e\uff0c\u5f15\u5165\u4e00\u79cd\u66ff\u4ee3\u5f15\u5bfc\u7684\u53bb\u566a\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u65b0\u9896\u7684\u653b\u51fb\u6307\u6807\uff08\u878d\u5408\u5206\u7c7b\u4e00\u81f4\u6027\u548c\u54c8\u5e0c\u90bb\u8fd1\u6027\uff09\u6765\u52a8\u6001\u9009\u62e9\u5019\u9009\u6837\u672c\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4e00\u7ec4\u66ff\u4ee3\u6a21\u578b\u6307\u5bfc\u8fd9\u4e9b\u5019\u9009\u6837\u672c\u7684\u7cbe\u70bc\uff0c\u786e\u4fdd\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDHMI\u5373\u4f7f\u5728\u6ca1\u6709\u4efb\u4f55\u8bad\u7ec3\u54c8\u5e0c\u7801\u53ef\u7528\u3001\u6700\u5177\u6311\u6218\u6027\u7684\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\uff0c\u4e5f\u80fd\u6210\u529f\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002\u4e0e\u73b0\u6709\u7684\u9ed1\u76d2\u573a\u666f\u4e0b\u7684\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u76f8\u6bd4\uff0cDHMI\u8868\u73b0\u66f4\u4f18\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u56fa\u6709\u7684\u4e25\u5cfb\u9690\u79c1\u98ce\u9669\u3002", "conclusion": "DHMI\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u7684\u6269\u6563\u6a21\u578b\u53cd\u6f14\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u91cd\u5efa\u56fe\u50cf\u5e76\u63ed\u793a\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\u3002\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\uff0cDHMI\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u65b9\u6cd5\u3002"}}
{"id": "2511.12255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12255", "abs": "https://arxiv.org/abs/2511.12255", "authors": ["Huy M. Le", "Dat Tien Nguyen", "Phuc Binh Nguyen", "Gia-Bao Le-Tran", "Phu Truong Thien", "Cuong Dinh", "Minh Nguyen", "Nga Nguyen", "Thuy T. N. Nguyen", "Huy Gia Ngo", "Tan Nhat Nguyen", "Binh T. Nguyen", "Monojit Choudhury"], "title": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets", "comment": null, "summary": "The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.", "AI": {"tldr": "Fusionista2.0\u662f\u4e00\u4e2a\u4e3a\u89c6\u9891\u68c0\u7d22\u8bbe\u8ba1\u7684\u3001\u7ecf\u8fc7\u4f18\u5316\u7684\u3001\u5feb\u901f\u4e14\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u6a21\u5757\u7ecf\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4ece\u800c\u5728\u89c6\u9891\u6d4f\u89c8\u5668\u6311\u6218\u8d5b\uff08VBS\uff09\u7b49\u4e25\u82db\u6761\u4ef6\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u68c0\u7d22\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u89c6\u9891\u6d4f\u89c8\u5668\u6311\u6218\u8d5b\uff08VBS\uff09\u5728\u4e25\u683c\u7684\u65f6\u95f4\u9650\u5236\u4e0b\u8fdb\u884c\u51c6\u786e\u68c0\u7d22\u7684\u8981\u6c42\uff0c\u9700\u8981\u4e00\u4e2a\u4f18\u5316\u901f\u5ea6\u548c\u53ef\u7528\u6027\u7684\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "Fusionista2.0\u5bf9\u6240\u6709\u6838\u5fc3\u6a21\u5757\u8fdb\u884c\u4e86\u6548\u7387\u518d\u5de5\u7a0b\uff1a\u9884\u5904\u7406\u5229\u7528ffmpeg\u8fdb\u884c\u5feb\u901f\u5173\u952e\u5e27\u63d0\u53d6\uff0cOCR\u91c7\u7528Vintern-1B-v3.5\u8fdb\u884c\u591a\u8bed\u8a00\u6587\u672c\u8bc6\u522b\uff0cASR\u4f7f\u7528faster-whisper\u8fdb\u884c\u5b9e\u65f6\u8f6c\u5f55\u3002\u95ee\u7b54\u90e8\u5206\u5219\u91c7\u7528\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4ee5\u964d\u4f4e\u6210\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u91cd\u65b0\u8bbe\u8ba1\u4e86\u7528\u6237\u754c\u9762\u4ee5\u63d0\u9ad8\u54cd\u5e94\u901f\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u5de5\u4f5c\u6d41\u6548\u7387\u3002", "result": "\u4e0e\u4e4b\u524d\u7684\u7248\u672c\u76f8\u6bd4\uff0cFusionista2.0\u5c06\u68c0\u7d22\u65f6\u95f4\u7f29\u77ed\u4e86\u9ad8\u8fbe75%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "Fusionista2.0\u901a\u8fc7\u6280\u672f\u5347\u7ea7\u548c\u7528\u6237\u754c\u9762\u6539\u8fdb\uff0c\u6210\u4e3a\u4e00\u4e2a\u6709\u7ade\u4e89\u529b\u4e14\u7528\u6237\u53cb\u597d\u7684\u5927\u89c4\u6a21\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\uff0c\u80fd\u591f\u5feb\u901f\u68c0\u7d22\u76f8\u5173\u5185\u5bb9\u3002"}}
{"id": "2511.12256", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12256", "abs": "https://arxiv.org/abs/2511.12256", "authors": ["Tolga Demiroglu", "Mehmet Ozan Unal", "Metin Ertas", "Isa Yildirim"], "title": "Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment", "comment": null, "summary": "We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eMedSigLIP\u7684\u3001\u7531\u6587\u672c\u63d0\u793a\u6761\u4ef6\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7FiLM\u548c\u591a\u5c3a\u5ea6\u6c60\u5316\u6ce8\u5165\u6587\u672c\u5148\u9a8c\uff0c\u4ee5\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u5feb\u901f\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6ce8\u5165\u6587\u672c\u5148\u9a8c\u4ee5\u6307\u5bfc\u6a21\u578b\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eMedSigLIP\u7684\u6846\u67b6\uff0c\u4f7f\u7528FiLM\u548c\u591a\u5c3a\u5ea6\uff08\u5168\u5c40\u3001\u5c40\u90e8\u3001\u7eb9\u7406\u611f\u77e5\uff09\u6c60\u5316\u6765\u6ce8\u5165\u6587\u672c\u63d0\u793a\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7MLP\u548c\u6210\u5bf9\u6392\u5e8f\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728LDCTIQA2023\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75281000\u5f20\u8bad\u7ec3\u56fe\u50cf\uff0c\u53d6\u5f97\u4e86PLCC = 0.9575, SROCC = 0.9561, KROCC = 0.8301\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u516c\u5f00\u7684\u6311\u6218\u8d5b\u7684\u9876\u5c16\u63d0\u4ea4\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6587\u672c\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\u5728\u4f4e\u5242\u91cfCT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\u3002"}}
{"id": "2511.12259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12259", "abs": "https://arxiv.org/abs/2511.12259", "authors": ["Puzhen Wu", "Hexin Dong", "Yi Lin", "Yihao Ding", "Yifan Peng"], "title": "A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation", "comment": "Accepted at AAAI 2026", "summary": "Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u3001\u75be\u75c5\u611f\u77e5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u80f8\u90e8X\u5149\u7247\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u65e8\u5728\u63d0\u9ad8\u4e34\u5e8a\u51c6\u786e\u6027\u548c\u8bed\u8a00\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u75be\u75c5\u611f\u77e5\u89c6\u89c9\u8868\u793a\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5ffd\u7565\u5173\u952e\u75c5\u7406\u7279\u5f81\u5e76\u751f\u6210\u4e0d\u51c6\u786e\u7684\u62a5\u544a\u3002", "method": "\u8be5\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u4e0e\u7279\u5b9a\u75c5\u7406\u7c7b\u522b\u5bf9\u5e94\u7684\u75be\u75c5\u611f\u77e5\u8bed\u4e49\u6807\u8bb0\uff08DAST\uff09\uff0c\u5e76\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u3002\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u75be\u75c5-\u89c6\u89c9\u6ce8\u610f\u529b\u878d\u5408\uff08DVAF\uff09\u6a21\u5757\u6574\u5408\u75be\u75c5\u611f\u77e5\u8868\u793a\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u53cc\u6a21\u6001\u76f8\u4f3c\u6027\u68c0\u7d22\uff08DMSR\uff09\u673a\u5236\u7ed3\u5408\u89c6\u89c9\u548c\u75be\u75c5\u7279\u5b9a\u76f8\u4f3c\u6027\u6765\u68c0\u7d22\u76f8\u5173\u6837\u672c\uff0c\u4e3a\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6307\u5bfc\u3002", "result": "\u5728CheXpert Plus\u3001IU X-ray\u548cMIMIC-CXR\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u75be\u75c5\u611f\u77e5\u6846\u67b6\u5728\u80f8\u90e8X\u5149\u7247\u62a5\u544a\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u75be\u75c5\u611f\u77e5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u80f8\u90e8X\u5149\u7247\u62a5\u544a\u751f\u6210\u7684\u4e34\u5e8a\u51c6\u786e\u6027\u548c\u8bed\u8a00\u8d28\u91cf\u3002"}}
{"id": "2511.12263", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12263", "abs": "https://arxiv.org/abs/2511.12263", "authors": ["Jingyao Li", "Jingyun Wang", "Molin Tan", "Haochen Wang", "Cilin Yan", "Likun Shi", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu"], "title": "CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models", "comment": "30 pages, 28 figures", "summary": "Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.", "AI": {"tldr": "CrossVid \u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLMs) \u8de8\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u4e86 5331 \u4e2a\u89c6\u9891\u548c 9015 \u4e2a\u95ee\u7b54\u5bf9\uff0c\u4efb\u52a1\u6db5\u76d6\u56db\u4e2a\u7ef4\u5ea6\u548c\u5341\u4e2a\u5177\u4f53\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u5728 CVR \u573a\u666f\u8bc4\u4f30\u4e2d\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGemini-2.5-Pro \u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5927\u591a\u6570 MLLMs \u4ecd\u96be\u4ee5\u6709\u6548\u6574\u5408\u548c\u6bd4\u8f83\u6765\u81ea\u591a\u4e2a\u89c6\u9891\u7684\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u89c6\u9891\u5206\u6790\uff0c\u672a\u80fd\u5145\u5206\u8bc4\u4f30 MLLMs \u5728\u8de8\u89c6\u9891\u63a8\u7406 (CVR) \u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u805a\u5408\u548c\u6bd4\u8f83\u591a\u89c6\u9891\u4fe1\u606f\u65f6\u3002\u73b0\u6709\u7684\u591a\u89c6\u89d2\u89c6\u9891\u57fa\u51c6\u4efb\u52a1\u6709\u9650\uff0c\u4e5f\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684 CVR \u573a\u666f\u3002", "method": "\u63d0\u51fa CrossVid \u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b 5331 \u4e2a\u89c6\u9891\u548c 9015 \u4e2a\u95ee\u7b54\u5bf9\uff08\u5355\u9009\u3001\u591a\u9009\u3001\u5f00\u653e\u5f0f\uff09\uff0c\u6db5\u76d6\u4e86\u56db\u4e2a\u9ad8\u5c42\u7ef4\u5ea6\u548c\u5341\u4e2a\u5177\u4f53\u4efb\u52a1\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30 MLLMs \u7684\u7a7a\u95f4-\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002\u5728 CrossVid \u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u5404\u79cd MLLMs \u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u6a21\u578b\u5728 CVR \u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728 CrossVid \u57fa\u51c6\u4e0a\uff0cGemini-2.5-Pro \u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u5747\u51c6\u786e\u7387 50.4%\u3002\u6848\u4f8b\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u5927\u591a\u6570 MLLMs \u5728 CVR \u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b83\u4eec\u96be\u4ee5\u6574\u5408\u6216\u6bd4\u8f83\u5206\u5e03\u5728\u591a\u4e2a\u89c6\u9891\u4e2d\u7684\u8bc1\u636e\u6765\u8fdb\u884c\u63a8\u7406\u3002", "conclusion": "CrossVid \u662f\u4e00\u4e2a\u5168\u9762\u7684 CVR \u57fa\u51c6\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u6307\u5bfc MLLMs \u7684\u53d1\u5c55\u3002\u73b0\u6709 MLLMs \u5728 CVR \u4efb\u52a1\u4e0a\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u8de8\u89c6\u9891\u4fe1\u606f\u6574\u5408\u4e0e\u6bd4\u8f83\u80fd\u529b\u4e0d\u8db3\uff0c\u8fd9\u4e3a\u672a\u6765 MLLMs \u5728 CVR \u9886\u57df\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.12267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12267", "abs": "https://arxiv.org/abs/2511.12267", "authors": ["Ruixun Liu", "Bowen Fu", "Jiayi Song", "Kaiyu Li", "Wanchen Li", "Lanxuan Xue", "Hui Qiao", "Weizhan Zhang", "Deyu Meng", "Xiangyong Cao"], "title": "ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks", "comment": null, "summary": "Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u611f\u77e5\u8303\u5f0f\uff0c\u7528\u4e8e\u5904\u7406\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\uff0c\u5e76\u4ecb\u7ecd\u4e86LRS-GRO\u6570\u636e\u96c6\u548cZoomEarth\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u65f6\u5b58\u5728\u5197\u4f59\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4e3b\u52a8\u7684\u611f\u77e5\u65b9\u5f0f\u3002", "method": "\u63d0\u51faLRS-GRO\u6570\u636e\u96c6\u548cZoomEarth\u6846\u67b6\uff0c\u91c7\u7528Region-Guided\u5956\u52b1\u3001SFT\u548cGRPO\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ZoomEarth\u5728LRS-GRO\u548c\u4e09\u4e2a\u516c\u5171UHR\u9065\u611f\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ZoomEarth\u6846\u67b6\u5177\u6709\u5f88\u5f3a\u7684\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2511.12270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12270", "abs": "https://arxiv.org/abs/2511.12270", "authors": ["Yaxuan Jiao", "Qing Xu", "Yuxiang Luo", "Xiangjian He", "Zhen Chen", "Wenting Duan"], "title": "TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.12280", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12280", "abs": "https://arxiv.org/abs/2511.12280", "authors": ["Shuochen Chang", "Xiaofeng Zhang", "Qingyang Liu", "Li Niu"], "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs", "comment": "Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2026. Code available at https://github.com/bcmi/D3ToM-Diffusion-MLLM", "summary": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.", "AI": {"tldr": "D3ToM\u662f\u4e00\u79cd\u5f15\u5bfc\u5f0f\u52a8\u6001\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u53ef\u52a0\u901f\u6269\u6563\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08Diffusion MLLMs\uff09\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u975e\u81ea\u56de\u5f52\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u901f\u5ea6\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u6162\u5f97\u591a\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u90fd\u9700\u8981\u5bf9\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u5b8c\u5168\u7684\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5bfc\u81f4\u7acb\u65b9\u89e3\u7801\u590d\u6742\u5ea6\uff0c\u8fd9\u5728\u5904\u7406\u6570\u5343\u4e2a\u89c6\u89c9\u4ee4\u724c\u65f6\u8ba1\u7b97\u4e0a\u4e0d\u5207\u5b9e\u9645\u3002", "method": "D3ToM\u91c7\u7528\u5f15\u5bfc\u5f0f\u52a8\u6001\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u7684\u53bb\u566a\u6b65\u9aa4\u4e2d\u52a8\u6001\u5408\u5e76\u5197\u4f59\u7684\u89c6\u89c9\u4ee4\u724c\u4ee5\u52a0\u901fDiffusion MLLMs\u7684\u63a8\u7406\u3002\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\uff0cD3ToM\u5229\u7528\u524d\u4e00\u4e2a\u53bb\u566a\u6b65\u9aa4\u751f\u6210\u7684\u5f15\u5bfc\u4ee4\u724c\u6765\u6784\u5efa\u6240\u6709\u89c6\u89c9\u4ee4\u724c\u7684\u91cd\u8981\u6027\u56fe\uff0c\u7136\u540e\u4fdd\u7559\u4e00\u90e8\u5206\u6700\u663e\u8457\u7684\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u805a\u5408\u6765\u5408\u5e76\u5269\u4f59\u7684\u4ee4\u724c\u3002\u8be5\u6a21\u5757\u53ef\u63d2\u5165\u5355\u4e2aTransformer\u5c42\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u9645\u7f29\u77ed\u4e86\u6240\u6709\u540e\u7eed\u5c42\u7684\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\u3002\u6b64\u5916\uff0cD3ToM\u91c7\u7528\u4e0e\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u52a8\u6001\u53d8\u5316\u7684\u5408\u5e76\u6bd4\u7387\uff0c\u4e0eDiffusion MLLMs\u7684\u672c\u5730\u89e3\u7801\u8fc7\u7a0b\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cD3ToM\u5728\u52a0\u901f\u63a8\u7406\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "D3ToM\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\uff0c\u6709\u6548\u89e3\u51b3\u4e86Diffusion MLLMs\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u52a0\u901f\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2511.12291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12291", "abs": "https://arxiv.org/abs/2511.12291", "authors": ["Andrea Bertogalli", "Giacomo Boracchi", "Luca Magri"], "title": "One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving", "comment": null, "summary": "We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4f30\u8ba1\u4e8b\u4ef6\u76f8\u673a\u3001LiDAR \u548c RGB \u76f8\u673a\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u7279\u522b\u5173\u6ce8\u4e8b\u4ef6\u76f8\u673a\u7684\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4f20\u611f\u5668\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6210\u5bf9\u7684\u6807\u5b9a\uff0c\u800c\u6ca1\u6709\u8003\u8651\u4e8b\u4ef6\u76f8\u673a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6807\u5b9a\u8fd9\u4e09\u79cd\u4f20\u611f\u5668\u7684\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684 3D \u6807\u5b9a\u76ee\u6807\uff0c\u8be5\u76ee\u6807\u5305\u542b\u5e73\u9762\u3001ChArUco \u548c LED \u6a21\u5f0f\uff0c\u53ef\u88ab\u6240\u6709\u4e09\u79cd\u4f20\u611f\u5668\u611f\u77e5\u3002\u901a\u8fc7\u5355\u6b21\u8054\u5408\u6807\u5b9a\u8fc7\u7a0b\u6765\u4f30\u8ba1\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u5916\u53c2\u3002", "result": "\u901a\u8fc7\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u51c6\u786e\u3001\u9c81\u68d2\u5730\u5b8c\u6210\u4e8b\u4ef6\u76f8\u673a\u3001LiDAR \u548c RGB \u76f8\u673a\u7684\u8054\u5408\u5916\u53c2\u6807\u5b9a\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.12301", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12301", "abs": "https://arxiv.org/abs/2511.12301", "authors": ["Chi Liu", "Jincheng Liu", "Congcong Zhu", "Minghao Wang", "Sheng Shen", "Jia Gu", "Tianqing Zhu", "Wanlei Zhou"], "title": "Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method", "comment": "Accepted for AAAI 2026 (Main Track Poster)", "summary": "Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.", "AI": {"tldr": "\u751f\u6210\u6570\u636e\u589e\u5f3a\uff08GDA\uff09\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u5b58\u5728\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u504f\u5dee\u3002\u672c\u6587\u63d0\u51fa\u9891\u7387\u6821\u51c6\uff08FreRec\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\uff081\uff09\u7edf\u8ba1\u9ad8\u9891\u66ff\u6362\uff08SHR\uff09\u548c\uff082\uff09\u91cd\u5efa\u9ad8\u9891\u6620\u5c04\uff08RHM\uff09\u6765\u89e3\u51b3GDA\u4e2d\u7684\u9891\u7387\u5931\u51c6\u95ee\u9898\uff0c\u63d0\u9ad8\u5408\u6210\u56fe\u50cf\u7684\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cFreRec\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u53ef\u517c\u5bb9\u4efb\u4f55\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u533b\u5b66AI\u53d1\u5c55\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u6027\u6311\u6218\uff0c\u751f\u6210\u6570\u636e\u589e\u5f3a\uff08GDA\uff09\u867d\u80fd\u5408\u6210\u56fe\u50cf\uff0c\u4f46\u6613\u5f15\u5165\u504f\u5dee\uff0c\u635f\u5bb3\u4e0b\u6e38\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3GDA\u4e2d\u9891\u7387\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9891\u7387\u6821\u51c6\uff08FreRec\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u7edf\u8ba1\u9ad8\u9891\u66ff\u6362\uff08SHR\uff09\u548c\u91cd\u5efa\u9ad8\u9891\u6620\u5c04\uff08RHM\uff09\uff0c\u4ee5\u51cf\u5c11\u5408\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u9ad8\u9891\u6210\u5206\u5dee\u5f02\u3002", "result": "\u5728\u8111\u90e8MRI\u3001\u80f8\u90e8X\u5149\u548c\u773c\u5e95\u56fe\u50cf\u7b49\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u672a\u7ecf\u6821\u51c6\u7684AI\u5408\u6210\u6837\u672c\u76f8\u6bd4\uff0cFreRec\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "FreRec\u662f\u4e00\u79cd\u72ec\u7acb\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u53ef\u4e0e\u4efb\u4f55\u751f\u6210\u6a21\u578b\u517c\u5bb9\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u5e38\u89c1\u7684\u533b\u5b66GDA\u6d41\u7a0b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86GDA\u4e2d\u7684\u9891\u7387\u5931\u51c6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.12304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12304", "abs": "https://arxiv.org/abs/2511.12304", "authors": ["Qifeng Chen", "Jiarun Liu", "Rengan Xie", "Tao Tang", "Sicong Du", "Yiru Zhao", "Yuchi Huo", "Sheng Yang"], "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors", "comment": "Accepted by AAAI-26", "summary": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.", "AI": {"tldr": "GS-based rendering \u6e32\u67d3\u6280\u672f\u5728 LiDAR \u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5355\u6b21\u626b\u63cf\u91cd\u5efa\u4e0d\u5b8c\u6574\u4f1a\u5bfc\u81f4\u65b0\u89c6\u89d2\u5408\u6210\u51fa\u73b0\u4f2a\u5f71\u3002LiDAR-GS++ \u901a\u8fc7\u5f15\u5165\u6269\u6563\u5148\u9a8c\u6765\u589e\u5f3a LiDAR \u9ad8\u65af\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u53ef\u5728\u5b9e\u65f6\u548c\u9ad8\u4fdd\u771f\u5ea6\u4e0b\u8fdb\u884c\u57ce\u5e02\u9053\u8def\u7684\u91cd\u65b0\u6a21\u62df\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u5316\u7c97\u7565\u5916\u63d2\u6e32\u67d3\u7684\u53ef\u63a7 LiDAR \u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u989d\u5916\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u626b\u63cf\uff0c\u5e76\u91c7\u7528\u6709\u6548\u7684\u84b8\u998f\u673a\u5236\u8fdb\u884c\u6269\u5c55\u91cd\u5efa\uff0c\u4ee5\u786e\u4fdd\u5916\u63d2\u65b0\u89c6\u89d2\u7684\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4f20\u611f\u5668\u6355\u83b7\u7684\u7ec6\u8282\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLiDAR-GS++ \u5728\u63d2\u503c\u548c\u5916\u63d2\u89c6\u89d2\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e GS \u7684\u6e32\u67d3\u65b9\u6cd5\u5728 LiDAR \u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u5b58\u5728\u4f2a\u5f71\u95ee\u9898\uff0c\u56e0\u4e3a\u5355\u6b21\u626b\u63cf\u91cd\u5efa\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51fa LiDAR-GS++\uff0c\u4e00\u79cd\u901a\u8fc7\u6269\u6563\u5148\u9a8c\u589e\u5f3a\u7684 LiDAR \u9ad8\u65af\u6e32\u67d3\u65b9\u6cd5\u3002\u5f15\u5165\u53ef\u63a7 LiDAR \u751f\u6210\u6a21\u578b\uff0c\u4ee5\u7c97\u7565\u5916\u63d2\u6e32\u67d3\u4e3a\u6761\u4ef6\uff0c\u751f\u6210\u989d\u5916\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u626b\u63cf\uff0c\u5e76\u91c7\u7528\u6709\u6548\u7684\u84b8\u998f\u673a\u5236\u8fdb\u884c\u6269\u5c55\u91cd\u5efa\u3002", "result": "LiDAR-GS++ \u6269\u5c55\u4e86\u91cd\u5efa\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u6b20\u62df\u5408\u533a\u57df\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u5916\u63d2\u65b0\u89c6\u89d2\u7684\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4f20\u611f\u5668\u6355\u83b7\u7684\u8be6\u7ec6\u573a\u666f\u8868\u9762\u3002\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLiDAR-GS++ \u5728\u63d2\u503c\u548c\u5916\u63d2\u89c6\u89d2\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LiDAR-GS++ \u901a\u8fc7\u7ed3\u5408\u6269\u6563\u5148\u9a8c\u548c LiDAR \u9ad8\u65af\u6e32\u67d3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u4fdd\u771f\u7684 LiDAR \u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2511.12321", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12321", "abs": "https://arxiv.org/abs/2511.12321", "authors": ["Xi Ding", "Lei Wang", "Piotr Koniusz", "Yongsheng Gao"], "title": "Learning Time in Static Classifiers", "comment": "Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u652f\u6301-\u793a\u4f8b-\u67e5\u8be2\uff08SEQ\uff09\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u5f15\u5165\u5faa\u73af\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u524d\u9988\u5206\u7c7b\u5668\u589e\u52a0\u4e86\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u5728\u9759\u6001\u548c\u65f6\u95f4\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u5668\u5047\u8bbe\u65f6\u95f4\u72ec\u7acb\u6027\uff0c\u96be\u4ee5\u6355\u6349\u6570\u636e\u968f\u65f6\u95f4\u53d8\u5316\u7684\u52a8\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u89c6\u89c9\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u652f\u6301-\u793a\u4f8b-\u67e5\u8be2\uff08SEQ\uff09\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u6210\u65f6\u95f4\u8fde\u8d2f\u7684\u8f68\u8ff9\uff0c\u5b66\u4e60\u7279\u5b9a\u7c7b\u522b\u7684\u65f6\u5e8f\u539f\u578b\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u7684soft-DTW\u635f\u5931\u5bf9\u9884\u6d4b\u5e8f\u5217\u8fdb\u884c\u5bf9\u9f50\uff0c\u540c\u65f6\u4f7f\u7528\u591a\u9879\u5f0f\u76ee\u6807\u51fd\u6570\u6765\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff08\u5305\u62ec\u7ec6\u7c92\u5ea6\u548c\u8d85\u7ec6\u7c92\u5ea6\uff09\u4e0a\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u5f15\u5165\u4e86\u5f3a\u5927\u7684\u65f6\u95f4\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u591f\u4ee5\u6a21\u5757\u5316\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u878d\u5408\u9759\u6001\u548c\u65f6\u95f4\u5b66\u4e60\uff0c\u4ec5\u9700\u5728\u9884\u63d0\u53d6\u7684\u7279\u5f81\u4e4b\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u7c7b\u5668\u5373\u53ef\u3002"}}
{"id": "2511.12331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12331", "abs": "https://arxiv.org/abs/2511.12331", "authors": ["Sepehr Kazemi Ranjbar", "Kumail Alhamoud", "Marzyeh Ghassemi"], "title": "SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) struggle with negation. Given a prompt like \"retrieve (or generate) a street scene without pedestrians,\" they often fail to respect the \"not.\" Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as \"A but not N,\" we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.", "AI": {"tldr": "VLN\u6a21\u578b\u5728\u5904\u7406\u5426\u5b9a\u63d0\u793a\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5927\u89c4\u6a21\u8d1f\u9762\u6570\u636e\u96c6\u5fae\u8c03\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5c06\u5426\u5b9a\u89c6\u4e3a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5b50\u7a7a\u95f4\u800c\u975e\u5355\u70b9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u4ee5\u80af\u5b9a\u548c\u5426\u5b9a\u63d0\u793a\u7684\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u7684\u7403\u5e3d\u533a\u57df\uff0c\u5e76\u6839\u636e\u56fe\u50cf\u5728\u533a\u57df\u5185\u7684\u4e2d\u5fc3\u65b9\u5411\u8fdb\u884c\u8bc4\u5206\uff0c\u4ece\u800c\u5728\u68c0\u7d22\u3001\u9009\u62e9\u9898\u548c\u6587\u751f\u56fe\u7b49\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u5347\u7ea630%\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5904\u7406\u5426\u5b9a\u63d0\u793a\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u4f8b\u5982\u5728\u201c\u68c0\u7d22\u6216\u751f\u6210\u4e00\u4e2a\u6ca1\u6709\u884c\u4eba\u7684\u8857\u9053\u573a\u666f\u201d\u65f6\uff0c\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u6b63\u786e\u7406\u89e3\u201c\u4e0d\u201d\u7684\u542b\u4e49\u3002\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u4f7f\u7528\u5927\u89c4\u6a21\u7684\u5426\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u635f\u5bb3\u6a21\u578b\u5728\u80af\u5b9a\u63d0\u793a\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528VLMs\uff08\u5982CLIP\uff09\u7684\u5d4c\u5165\u7a7a\u95f4\u53ef\u4ee5\u88ab\u5212\u5206\u4e3a\u8bed\u4e49\u4e00\u81f4\u5b50\u7a7a\u95f4\u7684\u7279\u6027\u3002\u8be5\u6846\u67b6\u5c06\u5426\u5b9a\u6982\u5ff5\u5efa\u6a21\u4e3a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u70b9\u3002\u5bf9\u4e8e\u201cA\u4f46\u975eN\u201d\u8fd9\u7c7b\u63d0\u793a\uff0c\u6a21\u578b\u6784\u5efa\u4ee5A\u548cN\u7684\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u7684\u4e24\u4e2a\u7403\u9762\u5e3d\uff0c\u5e76\u901a\u8fc7\u8bc4\u5206\u56fe\u50cf\u4e0eA\u7684\u4e2d\u5fc3\u65b9\u5411\u63a5\u8fd1\u4e14\u4e0eN\u7684\u4e2d\u5fc3\u65b9\u5411\u8fdc\u79bb\u7684\u7a0b\u5ea6\u6765\u5bfb\u627e\u5339\u914d\u56fe\u50cf\u3002", "result": "\u5728\u68c0\u7d22\u3001\u591a\u9879\u9009\u62e9\u9898\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u5c06\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u63d0\u9ad8\u4e86\u7ea630%\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7f29\u5c0f\u4e86\u80af\u5b9a\u63d0\u793a\u548c\u5426\u5b9a\u63d0\u793a\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5fae\u8c03\u6a21\u578b\u65e0\u6cd5\u7ef4\u6301\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\u901a\u8fc7\u5c06\u5426\u5b9a\u6982\u5ff5\u5efa\u6a21\u4e3a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLN\u6a21\u578b\u5728\u5904\u7406\u5426\u5b9a\u63d0\u793a\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002"}}
{"id": "2511.12342", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12342", "abs": "https://arxiv.org/abs/2511.12342", "authors": ["Sajjad Pakdamansavoji", "Kumar Vaibhav Jha", "Baher Abdulhai", "James H Elder"], "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections", "comment": null, "summary": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane", "AI": {"tldr": "\u4f7f\u75283D\u5750\u6807\u5206\u6790\u4ea4\u901a\u6d41\u91cf\u6bd42D\u56fe\u50cf\u66f4\u51c6\u786e", "motivation": "\u51c6\u786e\u7684\u4ea4\u53c9\u53e3\u8f6c\u5411\u8ba1\u6570\u5bf9\u4e8e\u4fe1\u53f7\u63a7\u5236\u3001\u4ea4\u901a\u7ba1\u7406\u548c\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u8f66\u8f86\u4ece\u57fa\u7840\u6444\u50cf\u5934\u6295\u5f71\u56de\u5730\u9762\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u878d\u5408\u6765\u81ea\u591a\u4e2a\u6444\u50cf\u5934\u7684\u4fe1\u606f\u3002", "result": "\u5355\u6444\u50cf\u5934\u548c\u591a\u6444\u50cf\u5934\uff08\u5f31\u878d\u5408\uff09\u7684\u6295\u5f71\u65b9\u6cd5\u90fd\u6bd4\u4ec5\u4f7f\u7528\u56fe\u50cf\u5e73\u9762\u8fdb\u884c\u5206\u6790\u66f4\u51c6\u786e\u3002", "conclusion": "\u5efa\u8bae\u5728\u5730\u9762\u5e73\u9762\u800c\u4e0d\u662f\u56fe\u50cf\u5e73\u9762\u4e0a\u5206\u6790\u4ea4\u901a\u6d41\u91cf\u3002"}}
{"id": "2511.12346", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12346", "abs": "https://arxiv.org/abs/2511.12346", "authors": ["Asmit Bandyopadhyay", "Anindita Das Bhattacharjee", "Rakesh Das"], "title": "CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification", "comment": null, "summary": "Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.", "AI": {"tldr": "CLAReSNet\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u5377\u79ef\u548cTransformer\u7684\u65b0\u578b\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5377\u79ef\u548c\u81ea\u9002\u5e94\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9ad8\u7ef4\u6027\u3001\u5149\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u4ee5\u53ca\u6837\u672c\u6570\u91cf\u6709\u9650\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u5e76\u5728Indian Pines\u548cSalinas\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u5206\u7c7b\u9762\u4e34\u9ad8\u5149\u8c31\u7ef4\u5ea6\u3001\u590d\u6742\u7684\u5149\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u4ee5\u53ca\u6709\u9650\u4e14\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u8bad\u7ec3\u6837\u672c\u7b49\u6311\u6218\u3002\u5355\u72ec\u4f7f\u7528CNN\u6216Transformer\u6548\u679c\u4e0d\u4f73\uff0c\u524d\u8005\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540e\u8005\u5f52\u7eb3\u504f\u89c1\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCLAReSNet\uff08\u5377\u79ef\u6f5c\u5728\u6ce8\u610f\u529b\u6b8b\u5dee\u5149\u8c31\u7f51\u7edc\uff09\uff0c\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6f5c\u5728\u74f6\u9888\u6574\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u63d0\u53d6\u548cTransformer\u98ce\u683c\u7684\u6ce8\u610f\u529b\u3002\u6a21\u578b\u4f7f\u7528\u591a\u5c3a\u5ea6\u5377\u79ef\u8bcd\u5e72\u3001\u6df1\u5ea6\u6b8b\u5dee\u5757\u548c\u589e\u5f3a\u7684\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\u6765\u63d0\u53d6\u5206\u5c42\u7a7a\u95f4\u7279\u5f81\uff0c\u7136\u540e\u662f\u7ed3\u5408\u53cc\u5411RNN\uff08LSTM/GRU\uff09\u548c\u591a\u5c3a\u5ea6\u5149\u8c31\u6f5c\u5728\u6ce8\u610f\u529b\uff08MSLA\uff09\u7684\u5149\u8c31\u7f16\u7801\u5668\u5c42\u3002MSLA\u901a\u8fc7\u81ea\u9002\u5e94\u6f5c\u5728\u4ee4\u724c\u5206\u914d\uff088-64\u4e2a\u4ee4\u724c\uff09\u5c06\u590d\u6742\u5ea6\u4eceO(T^2D)\u964d\u4f4e\u5230O(Tlog(T)D)\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0e\u5e8f\u5217\u957f\u5ea6\u7684\u5bf9\u6570\u7f29\u653e\u3002\u5206\u5c42\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u52a8\u6001\u805a\u5408\u591a\u5c42\u8868\u793a\u4ee5\u5b9e\u73b0\u7a33\u5065\u5206\u7c7b\u3002", "result": "\u5728Indian Pines\u548cSalinas\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLAReSNet\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u603b\u4f53\u51c6\u786e\u7387\u5206\u522b\u4e3a99.71%\u548c99.96%\uff0c\u663e\u8457\u4f18\u4e8eHybridSN\u3001SSRN\u548cSpectralFormer\u3002\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7c7b\u95f4\u53ef\u5206\u79bb\u6027\u548c\u7d27\u51d1\u7684\u7c7b\u5185\u805a\u7c7b\uff0c\u9a8c\u8bc1\u4e86CLAReSNet\u5728\u6709\u9650\u6837\u672c\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "CLAReSNet\u901a\u8fc7\u878d\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u521b\u65b0\u7684MSLA\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12363", "abs": "https://arxiv.org/abs/2511.12363", "authors": ["Michael Yang", "Shijian Deng", "William T. Doan", "Kai Wang", "Tianyu Yang", "Harsh Singh", "Yapeng Tian"], "title": "Explainable AI-Generated Image Detection RewardBench", "comment": null, "summary": "Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an \"MLLM as a judge\" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \\textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\\% on this benchmark (while human inter-annotator agreement reaches 98.30\\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86XAIGID-RewardBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5224\u65adAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u89e3\u91ca\u8d28\u91cf\u7684\u57fa\u51c6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u73b0\u6709\u6700\u4f73\u6a21\u578b\u5f97\u5206\u7387\u8fbe\u523088.76%\uff0c\u4f46\u4e0e\u4eba\u7c7b98.30%\u7684\u4e00\u81f4\u6027\u76f8\u6bd4\uff0c\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u964d\u4f4e\u4e86\u5176\u53ef\u4fe1\u5ea6\u3002\u867d\u7136MLLM\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8bc4\u4f30\u5176\u5224\u65ad\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u89e3\u91ca\u8d28\u91cf\u7684\u80fd\u529b\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u7ea63000\u4e2a\u6807\u6ce8\u4e09\u5143\u7ec4\u7684XAIGID-RewardBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30MLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff08reward model\uff09\u7684\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5e38\u89c1\u7684\u9519\u8bef\u3002", "result": "\u5728XAIGID-RewardBench\u57fa\u51c6\u4e0a\uff0c\u6700\u4f73\u5956\u52b1\u6a21\u578b\u7684\u5f97\u5206\u7387\u4e3a88.76%\uff0c\u800c\u4eba\u7c7b\u6807\u6ce8\u8005\u7684\u4e00\u81f4\u6027\u4e3a98.30%\uff0c\u8868\u660eMLLM\u5728\u8fd9\u4e00\u80fd\u529b\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u8ddd\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u6a21\u578b\u5e38\u89c1\u7684\u9519\u8bef\u3002", "conclusion": "XAIGID-RewardBench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30MLLM\u5728AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u89e3\u91ca\u8d28\u91cf\u5224\u65ad\u65b9\u9762\u80fd\u529b\u7684\u57fa\u51c6\u3002\u73b0\u6709MLLM\u5728\u8fd9\u4e00\u80fd\u529b\u4e0a\u4e0e\u4eba\u7c7b\u6c34\u5e73\u4ecd\u6709\u5dee\u8ddd\uff0c\u4f46\u8be5\u57fa\u51c6\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12365", "abs": "https://arxiv.org/abs/2511.12365", "authors": ["Yiqing Shen", "Mathias Unberath"], "title": "Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning", "comment": null, "summary": "Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.", "AI": {"tldr": "DT-R1\u662f\u4e00\u4e2a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u8868\u793a\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u591a\u79cd\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u540c\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u65b9\u5f0f\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "DT-R1\u6846\u67b6\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u590d\u6742\u591a\u6a21\u6001\u89c6\u89c9\u8f93\u5165\u7684\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u6765\u540c\u65f6\u9a8c\u8bc1\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8f93\u51fa\u51c6\u786e\u6027\u3002", "result": "\u5728\u516d\u4e2a\u6db5\u76d6\u4e24\u79cd\u6a21\u6001\u548c\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDT-R1\u7684\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "DT-R1\u4e3a\u89c6\u89c9\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5b9e\u73b0\u89c6\u89c9\u63a8\u7406\u3002"}}
{"id": "2511.12368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12368", "abs": "https://arxiv.org/abs/2511.12368", "authors": ["Yiqing Shen", "Mathias Unberath"], "title": "Fast Reasoning Segmentation for Images and Videos", "comment": null, "summary": "Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.", "AI": {"tldr": "FastReasonSeg\u901a\u8fc7\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u6765\u66f4\u6709\u6548\u5730\u84b8\u998f\u5927\u578b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u5206\u5272\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u9700\u8981\u5e9e\u5927\u7684\u6a21\u578b\uff0c\u65e0\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff1b\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8fc1\u79fb\u63a8\u7406\u5206\u5272\u6240\u9700\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faFastReasonSeg\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u89e3\u8026\u611f\u77e5\u548c\u63a8\u7406\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08\u6559\u5e08\u751f\u6210\u7684\u63a8\u7406\u94fe\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08\u5206\u5272\u7cbe\u5ea6\u548c\u63a8\u7406\u8d28\u91cf\u5bf9\u9f50\u7684\u8054\u5408\u5956\u52b1\uff09\u8fdb\u884c\u84b8\u998f\u3002", "result": "\u5728JiTBench\u3001RVTBench\u3001ReasonSeg\u548cLLM-Seg40K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFastReasonSeg\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u63a8\u7406\u5206\u5272\u6027\u80fd\u30020.6B\u7684\u84b8\u998f\u6a21\u578b\u5728\u53c2\u6570\u91cf\u662f\u517620\u500d\u7684\u6a21\u578b\u768420\u500d\u4ee5\u4e0a\uff0c\u541e\u5410\u91cf\u4e3a7.79 FPS\uff0c\u5185\u5b58\u6d88\u8017\u4ec5\u4e3a2.1GB\u3002", "conclusion": "FastReasonSeg\u901a\u8fc7\u6709\u6548\u7684\u84b8\u998f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u5206\u5272\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u5206\u5272\u3002"}}
{"id": "2511.12370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12370", "abs": "https://arxiv.org/abs/2511.12370", "authors": ["Chamuditha Jayanga Galappaththige", "Jason Lai", "Lloyd Windrim", "Donald Dansereau", "Niko S\u00fcnderhauf", "Dimity Miller"], "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion", "comment": null, "summary": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5728\u7ebf\u573a\u666f\u53d8\u6362\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u59ff\u6001\u65e0\u5173\u3001\u65e0\u6807\u7b7e\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u901f\u5ea6\u8d85\u8fc710 FPS\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u5728\u7ebf\u548c\u79bb\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u573a\u666f\u53d8\u6362\u68c0\u6d4b\uff08SCD\uff09\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u65b9\u9762\u8fdc\u4e0d\u5982\u79bb\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u9700\u8981\u5904\u7406\u6765\u81ea\u975e\u7ea6\u675f\u89c6\u89d2\u7684\u5b9e\u65f6\u573a\u666f\u53d8\u5316\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u878d\u5408\u635f\u5931\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u7ebf\u7d22\u548c\u89c2\u6d4b\u4e2d\u63a8\u65ad\u573a\u666f\u53d8\u6362\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u91c7\u7528\u57fa\u4e8ePnP\u7684\u5feb\u901f\u59ff\u6001\u4f30\u8ba1\u6765\u4e0e\u53c2\u8003\u573a\u666f\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u5feb\u901f\u7684\u53d8\u6362\u5f15\u5bfc\u66f4\u65b0\u7b56\u7565\u6765\u66f4\u65b03D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5728\u7ebf\u548c\u79bb\u7ebf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u7b2c\u4e00\u4e2a\u5b9e\u73b0\u59ff\u6001\u65e0\u5173\u3001\u65e0\u6807\u7b7e\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u5728\u7ebfSCD\u65b9\u6cd5\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.12371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12371", "abs": "https://arxiv.org/abs/2511.12371", "authors": ["Yiqing Shen", "Chenxiao Fan", "Chenjia Li", "Mathias Unberath"], "title": "Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models", "comment": null, "summary": "The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u9700\u8981\u63a8\u7406\u7684\u9690\u5f0f\u6587\u672c\u67e5\u8be2\uff0c\u5e76\u63d0\u4f9b\u5bf9\u8c61\u7ea7\u522b\u7684\u5b9a\u4f4d\u63a9\u7801\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9700\u8981\u63a8\u7406\u7684\u9690\u5f0f\u6587\u672c\u67e5\u8be2\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u89c6\u9891\u5185\u5bb9\u8868\u793a\u4e3a\u6570\u5b57\u5b6a\u751f\uff08\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\uff09\u7684\u65b9\u6cd5\uff0c\u7136\u540e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u201c\u5373\u65f6\u7cbe\u70bc\u201d\u6765\u5f25\u8865\u4fe1\u606f\u4e0d\u8db3\u3002\u8be5\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u5019\u9009\u89c6\u9891\u8bc6\u522b\uff08\u57fa\u4e8e\u5b50\u67e5\u8be2\u548c\u6570\u5b57\u5b6a\u751f\u8868\u793a\u7684\u7ec4\u5408\u5bf9\u9f50\uff09\uff1b2. \u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff08\u7ed3\u5408\u5373\u65f6\u7cbe\u70bc\uff09\u3002", "result": "\u5728ReasonT2VBench-135\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e8681.2%\u7684R@1\uff0c\u8d85\u8fc7\u4e86\u6700\u5f3a\u7684\u57fa\u7ebf\u8d85\u8fc750\u4e2a\u767e\u5206\u70b9\u3002\u5728ReasonT2VBench-1000\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e8681.7%\u7684R@1\u3002\u5728MSR-VTT\u3001MSVD\u548cVATEX\u4e09\u4e2a\u4f20\u7edf\u57fa\u51c6\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a8\u7406\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u9690\u5f0f\u6587\u672c\u67e5\u8be2\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12382", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12382", "abs": "https://arxiv.org/abs/2511.12382", "authors": ["Ansh Makwe", "Akansh Agrawal", "Prateek Jain", "Akshan Agrawal", "Priyanka Bagade"], "title": "AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification", "comment": null, "summary": "Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.", "AI": {"tldr": "AGGRNet\u901a\u8fc7\u63d0\u53d6\u4fe1\u606f\u548c\u975e\u4fe1\u606f\u7279\u5f81\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u533b\u5b66\u56fe\u50cf\u5206\u7c7b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\u5728\u533a\u5206\u7ec6\u7c92\u5ea6\u533b\u5b66\u56fe\u50cf\u7c7b\u522b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u96be\u4ee5\u6355\u6349\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u7c7b\u5185\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faAGGRNet\u6846\u67b6\uff0c\u4ee5\u63d0\u53d6\u4fe1\u606f\u6027\u548c\u975e\u4fe1\u606f\u6027\u7279\u5f81\uff0c\u4ece\u800c\u6709\u6548\u7406\u89e3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6a21\u5f0f\u5e76\u6539\u8fdb\u5206\u7c7b\u3002", "result": "AGGRNet\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728Kvasir\u6570\u636e\u96c6\u4e0a\u7684\u63d0\u5347\u5e45\u5ea6\u6700\u5927\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b5%\u3002", "conclusion": "AGGRNet\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u53d6\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12386", "abs": "https://arxiv.org/abs/2511.12386", "authors": ["Shabnam Sodagari", "Tommy Long"], "title": "Leveraging Quantum-Based Architectures for Robust Diagnostics", "comment": null, "summary": "The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7CT\u56fe\u50cf\u8bca\u65ad\u80be\u810f\u75be\u75c5\uff08\u7ed3\u77f3\u3001\u56ca\u80bf\u3001\u80bf\u7624\uff09\u3002", "motivation": "\u5229\u7528\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684ResNet50\u7f16\u7801\u5668\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\uff0c\u4ee5\u63d0\u9ad8\u80be\u810f\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "method": "\u5bf9\u80be\u810fCT\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u548c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u9884\u5904\u7406\uff0c\u7136\u540e\u4f7f\u7528ResNet50\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5c06\u7279\u5f81\u7f16\u7801\u4e3a\u91cf\u5b50\u6bd4\u7279\uff0c\u6700\u540e\u901a\u8fc7QCNN\u8fdb\u884c\u5904\u7406\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u52a0\u6743\u91c7\u6837\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u57288\u6bd4\u7279\u548c12\u6bd4\u7279\u7684QCNN\u914d\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e24\u79cdQCNN\u914d\u7f6e\u5747\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\u548c\u7a33\u5b9a\u7684\u5b66\u4e60\u66f2\u7ebf\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\u3002\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.99\u7684\u51c6\u786e\u7387\uff0c\u5176\u4e2d12\u6bd4\u7279\u914d\u7f6e\u5728\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u56ca\u80bf\u548c\u80bf\u7624\u68c0\u6d4b\u4e0a\uff0c\u56ca\u80bf\u53ec\u56de\u7387\u8fbe\u5230100%\uff0c\u80bf\u7624F1\u5206\u6570\u8fbe\u52300.9956\u3002\u6df7\u6dc6\u77e9\u9635\u5206\u6790\u663e\u793a\u5206\u7c7b\u884c\u4e3a\u53ef\u9760\u3002", "conclusion": "\u5c06\u7ecf\u5178\u9884\u5904\u7406\u548c\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u91cf\u5b50\u7535\u8def\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u63d0\u5347\u533b\u5b66\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2511.12389", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12389", "abs": "https://arxiv.org/abs/2511.12389", "authors": ["Divake Kumar", "Patrick Poggi", "Sina Tayebati", "Devashri Naik", "Nilesh Ahuja", "Amit Ranjan Trivedi"], "title": "Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation", "comment": null, "summary": "Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u63a8\u7406\u65f6\u95f4\u9009\u62e9\u201d\uff08Uncertainty-Guided Inference-Time Selection, UGITS\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u533a\u5206\u548c\u5229\u7528 aleatoric \u548c epistemic \u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u901a\u5e38\u5c06\u6240\u6709\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u5408\u5e76\u4e3a\u5355\u4e00\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u8fd9\u963b\u788d\u4e86\u5728\u9700\u8981\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u6216\u8c03\u6574\u63a8\u7406\u65f6\u7684\u53ef\u9760\u51b3\u7b56\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u533a\u5206\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u5e76\u52a0\u4ee5\u5229\u7528\u7684\u65b9\u6cd5\u3002", "method": "UGITS \u6846\u67b6\u5728\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5206\u79bb aleatoric\uff08\u6570\u636e\u9a71\u52a8\uff09\u4e0d\u786e\u5b9a\u6027\u548c epistemic\uff08\u6a21\u578b\u9a71\u52a8\uff09\u4e0d\u786e\u5b9a\u6027\u3002aleatoric \u4e0d\u786e\u5b9a\u6027\u901a\u8fc7\u6b63\u5219\u5316\u7684\u5168\u5c40\u5bc6\u5ea6\u6a21\u578b\u8fdb\u884c\u4f30\u8ba1\uff1bepistemic \u4e0d\u786e\u5b9a\u6027\u5219\u7531\u4e09\u4e2a\u4e92\u8865\u7684\u6210\u5206\u7ec4\u6210\uff0c\u5206\u522b\u6355\u6349\u5c40\u90e8\u652f\u6301\u4e0d\u8db3\u3001\u6d41\u5f62\u9891\u8c31\u574d\u584c\u548c\u8de8\u5c42\u7279\u5f81\u4e0d\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u6210\u5206\u662f\u7ecf\u9a8c\u4e0a\u6b63\u4ea4\u7684\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u91c7\u6837\u3001\u96c6\u6210\u6216\u989d\u5916\u7684\u6b63\u5411\u4f20\u64ad\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u5206\u89e3\u540e\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\u6210\u5230\u65e0\u5206\u5e03\u7684\u4fdd\u5f62\u6821\u51c6\u6d41\u7a0b\u4e2d\uff0c\u5728\u5339\u914d\u8986\u76d6\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u7f29\u5c0f\u4e86\u9884\u6d4b\u533a\u95f4\u3002\u5728 MOT17 \u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u6210\u5206\u8fdb\u884c\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\uff0c\u53ef\u4ee5\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c11\u7ea6 60%\uff0c\u540c\u65f6\u51c6\u786e\u7387\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u5728\u6240\u6709 MOT17 \u5e8f\u5217\u4e2d\u59cb\u7ec8\u80fd\u5e26\u6765\u66f4\u9ad8\u7684\u8ba1\u7b97\u8282\u7701\uff0c\u6bd4\u603b\u4e0d\u786e\u5b9a\u6027\u57fa\u7ebf\u63d0\u9ad8\u4e86 13.6 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "UGITS \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u533a\u5206\u548c\u5229\u7528 aleatoric \u548c epistemic \u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u81ea\u8c03\u8282\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2511.12400", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12400", "abs": "https://arxiv.org/abs/2511.12400", "authors": ["Xu Yang", "Gady Agam"], "title": "MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting", "comment": null, "summary": "We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather\n  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision\n  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and\n  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly\n  modulates spatial and channel attention. The two components are fused through pointwise multiplication and\n  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained\n  weights frozen.\n  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,\n  detection, and segmentation tasks with roughly less than 5\\% of backbone parameters.\n  The design further enables stable optimization, fast convergence, and strong cross-architecture\n  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach\n  for efficient adaptation of frozen vision backbones.", "AI": {"tldr": "MSLoRA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u9002\u914d\u5668\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u7279\u5f81\u54cd\u5e94\u800c\u975e\u91cd\u65b0\u8c03\u6574\u9aa8\u5e72\u7f51\u7edc\u6765\u7edf\u4e00CNN\u548cViT\u7684\u9002\u914d\u3002", "motivation": "\u73b0\u6709\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\uff0c\u5e76\u4e14\u96be\u4ee5\u8de8\u8d8a\u4e0d\u540c\u67b6\u6784\u8fdb\u884c\u6cdb\u5316\u3002MSLoRA\u65e8\u5728\u7edf\u4e00\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cViT\u7684\u9002\u914d\u3002 ", "method": "MSLoRA\u7ed3\u5408\u4e86\u4f4e\u79e9\u7ebf\u6027\u6295\u5f71\u548c\u591a\u5c3a\u5ea6\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u901a\u8fc7\u9010\u70b9\u4e58\u6cd5\u548c\u6b8b\u5dee\u8fde\u63a5\u878d\u5408\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u6743\u91cd\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\u8f6c\u79fb\u7279\u5f81\u6ce8\u610f\u529b\u3002", "result": "MSLoRA\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u7684\u8fc1\u79fb\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\uff0c\u5176\u53c2\u6570\u91cf\u7ea6\u5360\u9aa8\u5e72\u7f51\u7edc\u53c2\u6570\u91cf\u76845%\u4ee5\u4e0b\u3002\u6b64\u5916\uff0c\u8be5\u8bbe\u8ba1\u8fd8\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u4f18\u5316\u3001\u5feb\u901f\u7684\u6536\u655b\u548c\u5f3a\u5927\u7684\u8de8\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u800c\u975e\u91cd\u65b0\u8c03\u6574\uff0cMSLoRA\u4e3a\u51bb\u7ed3\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u901a\u7528\u7684\u9ad8\u6548\u9002\u914d\u65b9\u6cd5\u3002"}}
{"id": "2511.12405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12405", "abs": "https://arxiv.org/abs/2511.12405", "authors": ["Hyunki Seong", "Seongwoo Moon", "Hojin Ahn", "Jehun Kang", "David Hyunchul Shim"], "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving", "comment": "9 pages, 9 figures", "summary": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.", "AI": {"tldr": "VLA-R\u662f\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5f00\u653e\u4e16\u754c\u611f\u77e5\u548c\u65b0\u9896\u7684\u89c6\u89c9-\u52a8\u4f5c\u68c0\u7d22\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5728\u975e\u7ed3\u6784\u5316\u548c\u672a\u89c1\u8fc7\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u548c\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u7aef\u5230\u7aef\u5730\u63a2\u7d22\u5f00\u653e\u4e16\u754c\u573a\u666f\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4f46\u5145\u6ee1\u6311\u6218\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u672a\u9047\u5230\u8fc7\u6761\u4ef6\u7684\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVLA-R\u7684\u5f00\u653e\u4e16\u754c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5f00\u653e\u4e16\u754c\u611f\u77e5\u548c\u65b0\u7684\u89c6\u89c9-\u52a8\u4f5c\u68c0\u7d22\u8303\u5f0f\u3002\u5229\u7528\u51bb\u7ed3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u4ee5\u83b7\u5f97\u591a\u5c3a\u5ea6\u3001\u63d0\u793a\u5f15\u5bfc\u548c\u53ef\u89e3\u91ca\u7684\u611f\u77e5\u7279\u5f81\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u3002\u4f7f\u7528Q-Former\u74f6\u9888\u805a\u5408\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u8868\u793a\u548c\u4e0e\u8bed\u8a00\u5bf9\u9f50\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ee5\u8fde\u63a5\u611f\u77e5\u548c\u52a8\u4f5c\u57df\u3002\u5f15\u5165\u4e86\u89c6\u89c9-\u52a8\u4f5c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\uff0c\u4ee5\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u5c06\u89c6\u89c9-\u8bed\u8a00\u548c\u52a8\u4f5c\u5d4c\u5165\u5bf9\u9f50\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u5f00\u653e\u4e16\u754c\u63a8\u7406\u548c\u52a8\u4f5c\u68c0\u7d22\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5728\u975e\u7ed3\u6784\u5316\u3001\u672a\u89c1\u8fc7\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u548c\u63a2\u7d22\u6027\u80fd\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\u3002", "conclusion": "VLA-R\u6846\u67b6\u5728\u5f00\u653e\u4e16\u754c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u975e\u7ed3\u6784\u5316\u548c\u672a\u89c1\u8fc7\u73af\u5883\u4e2d\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.12410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12410", "abs": "https://arxiv.org/abs/2511.12410", "authors": ["Xi Xiao", "Zhuxuanzi Wang", "Mingqiao Mo", "Chen Liu", "Chenrui Ma", "Yanshu Li", "Smita Krishnaswamy", "Xiao Wang", "Tianyang Wang"], "title": "Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection", "comment": "Accepted by WACV 2026", "summary": "The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \\ours, a self-supervised framework that \\emph{visually probes} target domains without labels. \\ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \\ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PROBE \u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002PROBE \u901a\u8fc7\u65e0\u6807\u7b7e\u7684\u76ee\u6807\u57df\u6570\u636e\u751f\u6210\u2018\u7f3a\u9677\u611f\u77e5\u2019\u7684\u63d0\u793a\uff0c\u5e76\u5bf9\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u8868\u793a\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u65e0\u9700\u91cd\u65b0\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u76ee\u524d\u7684\u81ea\u52a8\u5316\u9053\u8def\u75c5\u5bb3\u68c0\u6d4b\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u6709\u76d1\u7763\u65b9\u6cd5\u5728\u7279\u5b9a\u57df\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u6807\u6ce8\uff1b\u6807\u51c6\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u6355\u6349\u901a\u7528\u7279\u5f81\uff0c\u5bb9\u6613\u53d7\u57df\u6f02\u79fb\u5f71\u54cd\u3002", "method": "PROBE \u6846\u67b6\u5305\u542b\u4e00\u4e2a\u81ea\u76d1\u7763\u63d0\u793a\u589e\u5f3a\u6a21\u5757\uff08SPEM\uff09\uff0c\u7528\u4e8e\u4ece\u65e0\u6807\u7b7e\u7684\u76ee\u6807\u6570\u636e\u4e2d\u63d0\u53d6\u5bf9\u75c5\u5bb3\u654f\u611f\u7684\u63d0\u793a\uff0c\u4ee5\u6307\u5bfc\u51bb\u7ed3\u7684 ViT \u4e3b\u5e72\u7f51\u7edc\uff1b\u4ee5\u53ca\u4e00\u4e2a\u57df\u611f\u77e5\u63d0\u793a\u5bf9\u9f50\uff08DAPA\uff09\u76ee\u6807\uff0c\u7528\u4e8e\u5bf9\u9f50\u53d7\u63d0\u793a\u5f15\u5bfc\u7684\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPROBE \u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u6709\u76d1\u7763\u3001\u81ea\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u9c81\u68d2\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5bf9\u57df\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u5177\u6709\u9ad8\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u81ea\u76d1\u7763\u63d0\u793a\u5b66\u4e60\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u89c6\u89c9\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5207\u5b9e\u53ef\u884c\u7684\u65b9\u5411\u3002"}}
{"id": "2511.12415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12415", "abs": "https://arxiv.org/abs/2511.12415", "authors": ["Xinrui Li", "Qi Cai", "Yuanxin Wu"], "title": "Towards Rotation-only Imaging Geometry: Rotation Estimation", "comment": null, "summary": "Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5e73\u79fb\u91cf\u8868\u793a\u4e3a\u65cb\u8f6c\u91cf\u7684\u51fd\u6570\uff0c\u5c06\u6210\u50cf\u51e0\u4f55\u8868\u793a\u538b\u7f29\u5230\u65cb\u8f6c\u6d41\u5f62\u4e0a\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u76843D\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u53d7\u5148\u524d\u59ff\u6001\u65e0\u5173\u6210\u50cf\u51e0\u4f55\u7814\u7a76\u7684\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u573a\u666f\u7ed3\u6784\u3001\u65cb\u8f6c\u548c\u5e73\u79fb\u4e4b\u95f4\u7684\u5173\u952e\u5173\u7cfb\uff0c\u5e76\u5c06\u6210\u50cf\u51e0\u4f55\u8868\u793a\u538b\u7f29\u5230\u65cb\u8f6c\u6d41\u5f62\u4e0a\uff0c\u4ee5\u63d0\u9ad8SfM\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u6295\u5f71\u8bef\u5dee\u7684\u7eaf\u65cb\u8f6c\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e24\u89c6\u56fe\u548c\u591a\u89c6\u56fe\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u95ee\u9898\uff0c\u5176\u4e2d\u5e73\u79fb\u91cf\u53ef\u4ee5\u7531\u65cb\u8f6c\u91cf\u5bfc\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7eaf\u65cb\u8f6c\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5176\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u751a\u81f3\u53ef\u4e0e\u591a\u6b21\u6346\u7ed1\u8c03\u6574\u8fed\u4ee3\u7684\u7ed3\u679c\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u548c\u53ef\u9760\u76843D\u89c6\u89c9\u8ba1\u7b97\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u7eaf\u65cb\u8f6c\u4f18\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u95ee\u9898\u3002"}}
{"id": "2511.12419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12419", "abs": "https://arxiv.org/abs/2511.12419", "authors": ["Wenjie Li", "Jinglei Shi", "Jin Han", "Heng Guo", "Zhanyu Ma"], "title": "Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance", "comment": null, "summary": "Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.", "AI": {"tldr": "DHGM\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u5728\u53bb\u9664\u56fe\u50cf\u96e8\u6c34\u7684\u540c\u65f6\u4fdd\u7559\u5e76\u589e\u5f3a\u4e86\u9ad8\u9891\u7ec6\u8282\uff0c\u5b9e\u73b0\u4e86\u6e05\u6670\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5e38\u53d7\u6076\u52a3\u5929\u6c14\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u53bb\u5929\u6c14\u548c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u53ef\u80fd\u51b2\u7a81\uff0c\u5f71\u54cd\u5c0f\u76ee\u6807\u68c0\u6d4b\u7b49\u89c6\u89c9\u4efb\u52a1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u53bb\u9664\u5929\u6c14\u566a\u58f0\u5e76\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDHGM\uff08Diffusion-based High-frequency Guided Model\uff09\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u540c\u65f6\u53bb\u9664\u96e8\u6c34\u4f2a\u5f71\u5e76\u589e\u5f3a\u7ed3\u6784\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDHGM\u5728\u53bb\u9664\u96e8\u6c34\u548c\u63d0\u9ad8\u56fe\u50cf\u5206\u8fa8\u7387\u65b9\u9762\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "DHGM\u80fd\u591f\u6709\u6548\u89e3\u51b3\u53bb\u5929\u6c14\u548c\u8d85\u5206\u8fa8\u7387\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4e3a\u4f9d\u8d56\u9ad8\u9891\u7ec6\u8282\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12422", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12422", "abs": "https://arxiv.org/abs/2511.12422", "authors": ["Nuolin Sun", "Linyuan Wang", "Haonan Wei", "Lei Li", "Bin Yan"], "title": "MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation", "comment": null, "summary": "ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.", "AI": {"tldr": "ResNet\u53ef\u4ee5\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u5b9e\u73b0\uff0c\u5e76\u53ef\u88ab\u89c6\u4e3a\u79bb\u6563\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODEs\uff09\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMFI-ResNet\u7684\u65b0\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86MeanFlow\u548cResNet\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u538b\u7f29-\u6269\u5c55\u7b56\u7565\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u8bc6\u522b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMFI-ResNet\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u53c2\u6570\u51cf\u5c11\u4e86\u7ea646%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u63a2\u7d22\u751f\u6210\u6a21\u578b\uff08MeanFlow\uff09\u4e0e\u5224\u522b\u6a21\u578b\uff08ResNet\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff08MFI-ResNet\uff09\u6765\u63d0\u9ad8ResNet\u7684\u53c2\u6570\u6548\u7387\u548c\u8bc6\u522b\u6027\u80fd\u3002", "method": "MFI-ResNet\u91c7\u7528\u538b\u7f29-\u6269\u5c55\u7b56\u7565\uff1a\u9996\u5148\uff0c\u7528\u5c11\u91cfMeanFlow\u6a21\u5757\u66ff\u6362ResNet\u7684\u591a\u4e2a\u6b8b\u5dee\u5757\uff0c\u6784\u5efa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5143\u6a21\u578b\uff08\u538b\u7f29\uff09\uff1b\u7136\u540e\uff0c\u9009\u62e9\u6027\u5730\u5c06\u524d\u4e09\u4e2a\u9636\u6bb5\u6269\u5c55\u56de\u6b8b\u5dee\u5757\u7ed3\u6784\uff0c\u6700\u540e\u4e00\u4e2a\u9636\u6bb5\u4fdd\u6301MeanFlow\u5f62\u5f0f\uff0c\u5e76\u8fdb\u884c\u5fae\u8c03\uff08\u6269\u5c55\uff09\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0cMFI-ResNet\u7684\u53c2\u6570\u91cf\u76f8\u6bd4ResNet-50\u51cf\u5c11\u4e8646.28%\u548c45.59%\uff0c\u4f46\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e860.23%\u548c0.17%\u3002", "conclusion": "\u751f\u6210\u6d41\u573a\u53ef\u4ee5\u6709\u6548\u5730\u8868\u5f81ResNet\u4e2d\u7684\u7279\u5f81\u53d8\u6362\u8fc7\u7a0b\uff0c\u4e3a\u7406\u89e3\u751f\u6210\u5efa\u6a21\u548c\u5224\u522b\u5b66\u4e60\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.12428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12428", "abs": "https://arxiv.org/abs/2511.12428", "authors": ["Jingqi Xu", "Jingxi Lu", "Chenghao Li", "Sreetama Sarkar", "Souvik Kundu", "Peter A. Beerel"], "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.", "AI": {"tldr": "\u7ea2VTP\u662f\u4e00\u79cd\u54cd\u5e94\u9a71\u52a8\u7684\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u7b2c\u4e00\u6b65\u63a8\u7406\u540e\u4fee\u526a\u4e0d\u91cd\u8981\u7684\u89c6\u89c9\u6807\u8bb0\u6765\u63d0\u9ad8\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08DVLMs\uff09\u7684\u63a8\u7406\u6548\u7387\uff0c\u4ece\u800c\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08DVLMs\uff09\u901a\u8fc7\u5e76\u884c\u4ee4\u724c\u89e3\u7801\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5927\u91cf\u7684\u89c6\u89c9\u4ee4\u724c\u4ecd\u7136\u4e25\u91cd\u5f71\u54cd\u5176\u63a8\u7406\u6548\u7387\u3002\u73b0\u6709\u7684\u89c6\u89c9\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u81ea\u56de\u5f52VLMs\uff08AVLMs\uff09\uff0c\u800c\u5bf9DVLMs\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRedVTP\u7684\u54cd\u5e94\u9a71\u52a8\u7684\u89c6\u89c9\u4ee4\u724c\u4fee\u526a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528DVLMs\u7684\u63a8\u7406\u52a8\u6001\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u63a9\u7801\u54cd\u5e94\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u6765\u4f30\u8ba1\u89c6\u89c9\u4ee4\u724c\u7684\u91cd\u8981\u6027\u3002\u7531\u4e8e\u8fd9\u4e9b\u91cd\u8981\u6027\u5206\u6570\u5728\u6b65\u9aa4\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\uff0cRedVTP\u5728\u7b2c\u4e00\u6b65\u63a8\u7406\u540e\u4fee\u526a\u4e86\u63a9\u7801\u4ee4\u724c\u4e2d\u4e0d\u592a\u91cd\u8981\u7684\u89c6\u89c9\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRedVTP\u5c06LLaDA-V\u548cLaViDa\u7684\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad8\u4e86186%\u548c28.05%\uff0c\u5e76\u5c06\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u964d\u4f4e\u4e8664.97%\u548c21.87%\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "RedVTP\u662f\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u4ee4\u724c\u4fee\u526a\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8DVLMs\u7684\u63a8\u7406\u6548\u7387\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12432", "abs": "https://arxiv.org/abs/2511.12432", "authors": ["Xilai Li", "Xiaosong Li", "Weijun Jiang"], "title": "Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion", "comment": "Accepted at AAAI 2026", "summary": "Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.", "AI": {"tldr": "UP-Fusion\u6846\u67b6\u901a\u8fc7\u901a\u9053\u6270\u52a8\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u96c6\u6210\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u901a\u8fc7\u7ed3\u5408\u4e92\u8865\u4fe1\u606f\u6765\u589e\u5f3a\u573a\u666f\u611f\u77e5\u3002\u7136\u800c\uff0c\u6a21\u6001\u5dee\u5f02\u8fc7\u5927\u5bfc\u81f4\u7684\u68af\u5ea6\u51b2\u7a81\u9650\u5236\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684\u7279\u5b9a\u6a21\u6001\u7f16\u7801\u5668\u7b56\u7565\u867d\u7136\u80fd\u63d0\u5347\u878d\u5408\u8d28\u91cf\uff0c\u4f46\u727a\u7272\u4e86\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u9053\u6270\u52a8\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u96c6\u6210\u7684\u7edf\u4e00\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff08UP-Fusion\uff09\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1. \u8bed\u4e49\u611f\u77e5\u901a\u9053\u526a\u679d\u6a21\u5757\uff08SCPM\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u8fc7\u6ee4\u548c\u589e\u5f3a\u7279\u5f81\u901a\u9053\uff0c\u6291\u5236\u5197\u4f59\u4fe1\u606f\u30022. \u51e0\u4f55\u4eff\u5c04\u8c03\u5236\u6a21\u5757\uff08GAM\uff09\uff0c\u5229\u7528\u539f\u59cb\u6a21\u6001\u7279\u5f81\u5bf9\u521d\u59cb\u878d\u5408\u7279\u5f81\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\uff0c\u4fdd\u6301\u7f16\u7801\u5668\u7684\u6a21\u6001\u5224\u522b\u529b\u30023. \u6587\u672c\u5f15\u5bfc\u901a\u9053\u6270\u52a8\u6a21\u5757\uff08TCPM\uff09\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u91cd\u5851\u901a\u9053\u5206\u5e03\uff0c\u51cf\u5c11\u5bf9\u7279\u5b9a\u6a21\u6001\u901a\u9053\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "conclusion": "UP-Fusion\u6846\u67b6\u901a\u8fc7SCPM\u3001GAM\u548cTCPM\u6a21\u5757\u7684\u6709\u6548\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12438", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12438", "abs": "https://arxiv.org/abs/2511.12438", "authors": ["ANK Zaman", "Prosenjit Chatterjee", "Rajat Sharma"], "title": "Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning", "comment": null, "summary": "A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNN\uff09\u548cOpenCV\u7684\u5b9e\u65f6\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u9a7e\u9a76\u5458\u7684\u9762\u90e8\u7279\u5f81\uff08\u5982\u773c\u775b\u7741\u5ea6\u548c\u6253\u54c8\u6b20\u7684\u5634\u90e8\u52a8\u4f5c\uff09\u6765\u8bc6\u522b\u75b2\u52b3\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u75b2\u52b3\u65f6\u53d1\u51fa\u8b66\u62a5\uff0c\u4ee5\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3002", "motivation": "\u957f\u9014\u9a7e\u9a76\u53ef\u80fd\u5bfc\u81f4\u9a7e\u9a76\u5458\u75b2\u52b3\uff0c\u589e\u52a0\u4e8b\u6545\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5b9e\u65f6\u7cfb\u7edf\u6765\u68c0\u6d4b\u9a7e\u9a76\u5458\u7684\u75b2\u52b3\u72b6\u6001\u3002", "method": "\u4f7f\u7528DCNN\u548cOpenCV\uff0c\u901a\u8fc7\u6444\u50cf\u5934\u5b9e\u65f6\u6355\u6349\u9a7e\u9a76\u5458\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u63d0\u53d6\u9762\u90e8\u5730\u6807\uff08\u5982\u773c\u775b\u7741\u5ea6\u548c\u5634\u90e8\u52a8\u4f5c\uff09\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5224\u65ad\u9a7e\u9a76\u5458\u662f\u5426\u75b2\u52b3\uff0c\u82e5\u75b2\u52b3\u5219\u89e6\u53d1\u8b66\u62a5\u3002", "result": "\u8be5\u6a21\u578b\u5728NTHU-DDD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8699.6%\u7684\u51c6\u786e\u7387\uff0c\u5728Yawn-Eye-Dataset\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8697%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eDCNN\u7684\u75b2\u52b3\u68c0\u6d4b\u7cfb\u7edf\u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u9a7e\u9a76\u5458\u7684\u75b2\u52b3\u72b6\u6001\uff0c\u6709\u53ef\u80fd\u633d\u6551\u751f\u547d\u3002"}}
{"id": "2511.12446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12446", "abs": "https://arxiv.org/abs/2511.12446", "authors": ["Jiahe Qian", "Yuhao Shen", "Zhangtianyi Chen", "Juexiao Zhou", "Peisong Wang"], "title": "CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training", "comment": null, "summary": "Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.", "AI": {"tldr": "CoTBox-TTT\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u5373\u65f6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0\u8f6f\u63d0\u793a\u6765\u9002\u5e94\u533b\u7597VQA\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u9886\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u9aa8\u5e72\u51bb\u7ed3\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\u4ee5\u53ca\u7b54\u6848\u4e0e\u56fe\u50cf\u8bc1\u636e\u5173\u8054\u6027\u5f31\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u90e8\u7f72\u65f6\u96be\u4ee5\u8fdb\u884c\u518d\u8bad\u7ec3\u6216\u6dfb\u52a0\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faCoTBox-TTT\uff0c\u4e00\u79cd\u9996\u6b21\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u51bb\u7ed3\uff0c\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u8fde\u7eed\u7684\u8f6f\u63d0\u793a\u3002\u901a\u8fc7\u89c6\u89c9\u601d\u7ef4\u94fe\u4fe1\u53f7\u8bc6\u522b\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u533a\u57df\uff0c\u5e76\u9f13\u52b1\u539f\u59cb\u56fe\u50cf\u548c\u5c40\u90e8\u88c1\u526a\u56fe\u50cf\u4e4b\u95f4\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u7b7e\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u4e0d\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728\u533b\u7597VQA\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728pathVQA\u6570\u636e\u96c6\u4e0a\uff0c\u5c06LLaVA\u7684\u5c01\u95ed\u5f0f\u51c6\u786e\u7387\u63d0\u9ad8\u4e8612.3%\u3002", "conclusion": "CoTBox-TTT\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u65e0\u9700\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u533b\u7597VQA\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u9886\u57df\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.12449", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12449", "abs": "https://arxiv.org/abs/2511.12449", "authors": ["Zhanheng Nie", "Chenghan Fu", "Daoze Zhang", "Junxian Wu", "Wanxian Guan", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding", "comment": "11 pages, 7 figures", "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.", "AI": {"tldr": "MOON2.0\u662f\u4e00\u4e2a\u7528\u4e8e\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u7684\u52a8\u6001\u6a21\u6001\u5e73\u8861\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u4e0d\u5e73\u8861\u3001\u5185\u5728\u5bf9\u9f50\u5173\u7cfb\u5229\u7528\u4e0d\u8db3\u548c\u566a\u58f0\u5904\u7406\u6709\u9650\u7b49\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u6a21\u6001\u9a71\u52a8\u7684MoE\u6a21\u5757\u3001\u53cc\u5c42\u5bf9\u9f50\u65b9\u6cd5\u548c\u57fa\u4e8eMLLM\u7684\u56fe\u6587\u534f\u540c\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728MBE2.0\u57fa\u51c6\u548c\u5176\u4ed6\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u7535\u5546\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u80fd\u591f\u7406\u89e3\u4e30\u5bcc\u7684\u89c6\u89c9\u548c\u6587\u672c\u4ea7\u54c1\u4fe1\u606f\u7684\u6a21\u578b\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u7535\u5546\u9886\u57df\u867d\u7136\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u3001\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u5185\u5728\u5bf9\u9f50\u5173\u7cfb\u5229\u7528\u4e0d\u8db3\u4ee5\u53ca\u7535\u5546\u591a\u6a21\u6001\u6570\u636e\u566a\u58f0\u5904\u7406\u80fd\u529b\u6709\u9650\u7b49\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "MOON2.0\u6846\u67b6\u5305\u62ec\uff1a1. \u6a21\u6001\u9a71\u52a8\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u5757\uff0c\u6839\u636e\u8f93\u5165\u7684\u6a21\u6001\u6784\u6210\u81ea\u9002\u5e94\u5730\u5904\u7406\u6837\u672c\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u4ee5\u7f13\u89e3\u6a21\u6001\u4e0d\u5e73\u8861\uff1b2. \u53cc\u5c42\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u5355\u4e2a\u4ea7\u54c1\u5185\u90e8\u7684\u8bed\u4e49\u5bf9\u9f50\u7279\u6027\uff1b3. \u57fa\u4e8eMLLM\u7684\u56fe\u6587\u534f\u540c\u589e\u5f3a\u7b56\u7565\uff0c\u7ed3\u5408\u6587\u672c\u4e30\u5bcc\u548c\u89c6\u89c9\u6269\u5c55\uff0c\u5e76\u8f85\u4ee5\u52a8\u6001\u6837\u672c\u8fc7\u6ee4\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86MBE2.0\uff0c\u4e00\u4e2a\u7528\u4e8e\u7535\u5546\u8868\u793a\u5b66\u4e60\u548c\u8bc4\u4f30\u7684\u534f\u540c\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u57fa\u51c6\u3002", "result": "MOON2.0\u5728MBE2.0\u548c\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u70ed\u529b\u56fe\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86MOON2.0\u6539\u8fdb\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u5b9a\u6027\u8bc1\u636e\u3002", "conclusion": "MOON2.0\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6a21\u5757\u548c\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7535\u5546\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u9886\u57df\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12452", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12452", "abs": "https://arxiv.org/abs/2511.12452", "authors": ["Xiaoyu Lin", "Aniket Ghorpade", "Hansheng Zhu", "Justin Qiu", "Dea Rrozhani", "Monica Lama", "Mick Yang", "Zixuan Bian", "Ruohan Ren", "Alan B. Hong", "Jiatao Gu", "Chris Callison-Burch"], "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions", "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDenseAnnotate\u7684\u97f3\u9891\u9a71\u52a8\u5728\u7ebf\u6807\u6ce8\u5e73\u53f0\uff0c\u7528\u4e8e\u9ad8\u6548\u521b\u5efa\u56fe\u50cf\u548c3D\u8d44\u6e90\u7684\u5bc6\u96c6\u3001\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u6807\u6ce8\u7a00\u758f\u4e14\u4f9d\u8d56\u624b\u52a8\u8f93\u5165\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u6807\u6ce8\u7a00\u758f\u3001\u65e0\u6cd5\u5145\u5206\u6355\u6349\u56fe\u50cf\u5185\u5bb9\u3001\u4ee5\u53ca\u4f20\u7edf\u6587\u672c\u6807\u6ce8\u65b9\u5f0f\u5728\u8868\u8fbe\u3001\u901f\u5ea6\u548c\u8986\u76d6\u9762\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u5728\u591a\u5143\u6587\u5316\u548c3D\u8d44\u4ea7\u6807\u6ce8\u65b9\u9762\u3002", "method": "DenseAnnotate\u5e73\u53f0\u5141\u8bb8\u6807\u6ce8\u4eba\u5458\u901a\u8fc7\u53e3\u8ff0\u6765\u6807\u6ce8\u56fe\u50cf\u62163D\u573a\u666f\u4e2d\u7684\u533a\u57df\uff0c\u5e76\u5b9e\u65f6\u5c06\u8bed\u97f3\u4e0e\u56fe\u50cf\u533a\u57df\u62163D\u573a\u666f\u90e8\u5206\u5173\u8054\u8d77\u6765\u3002\u8be5\u5e73\u53f0\u96c6\u6210\u4e86\u8bed\u97f3\u8f6c\u6587\u672c\u548c\u6ce8\u610f\u529b\u533a\u57df\u6807\u8bb0\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u5bf91,000\u591a\u540d\u6807\u6ce8\u8005\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3,531\u5f20\u56fe\u50cf\u3001898\u4e2a3D\u573a\u666f\u548c7,460\u4e2a3D\u5bf9\u8c61\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002\u4f7f\u7528\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7406\u89e3\uff08+5%\uff09\u3001\u6587\u5316\u5bf9\u9f50\uff08+47%\uff09\u548c3D\u7a7a\u95f4\u80fd\u529b\uff08+54%\uff09\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DenseAnnotate\u5e73\u53f0\u4e3a\u672a\u6765\u89c6\u89c9-\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u8bed\u8a00\u7684\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u7c7b\u578b\u3002"}}
{"id": "2511.12480", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12480", "abs": "https://arxiv.org/abs/2511.12480", "authors": ["Jingshan Hong", "Haigen Hu", "Huihuang Zhang", "Qianwei Zhou", "Zhao Li"], "title": "MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning", "comment": null, "summary": "In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.", "AI": {"tldr": "MaskAnyNet\u5c06\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u7684\u601d\u60f3\u5e94\u7528\u4e8e\u4f20\u7edf\u7684\u56fe\u50cf\u63a9\u853d\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5b66\u4e60\u88ab\u63a9\u76d6\u7684\u5185\u5bb9\u6765\u5229\u7528\u88ab\u63a9\u76d6\u533a\u57df\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u4ece\u800c\u4e30\u5bcc\u7279\u5f81\u5e76\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5728CNN\u548cTransformer\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u53d6\u5f97\u4e86\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u56fe\u50cf\u63a9\u853d\u65b9\u6cd5\u5b58\u5728\u4e22\u5f03\u50cf\u7d20\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3001\u63a9\u853d\u53ef\u80fd\u79fb\u9664\u5173\u952e\u5c0f\u76ee\u6807\u7684\u95ee\u9898\u3002MIM\u65b9\u6cd5\u8868\u660e\u63a9\u76d6\u533a\u57df\u53ef\u4ee5\u4ece\u90e8\u5206\u8f93\u5165\u4e2d\u91cd\u5efa\uff0c\u63ed\u793a\u4e86\u63a9\u76d6\u533a\u57df\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bed\u4e49\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u4e9b\u88ab\u63a9\u76d6\u533a\u57df\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faMaskAnyNet\uff0c\u7ed3\u5408\u63a9\u853d\u548c\u518d\u5b66\u4e60\u673a\u5236\uff0c\u5c06\u63a9\u76d6\u5185\u5bb9\u89c6\u4e3a\u8f85\u52a9\u77e5\u8bc6\u800c\u975e\u5ffd\u7565\u3002\u8be5\u65b9\u6cd5\u589e\u52a0\u4e00\u4e2a\u5206\u652f\u6765\u8054\u5408\u5b66\u4e60\u91cd\u6784\u7684\u63a9\u76d6\u533a\u57df\uff0c\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u5230\u4efb\u4f55\u6a21\u578b\uff0c\u4ee5\u5229\u7528\u63a9\u76d6\u533a\u57df\u7684\u8bed\u4e49\u591a\u6837\u6027\u6765\u4e30\u5bcc\u7279\u5f81\u5e76\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002", "result": "\u5728CNN\u548cTransformer\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5177\u6709\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8bc1\u5b9e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u91cd\u7528\u88ab\u63a9\u76d6\u7684\u5185\u5bb9\u63d0\u9ad8\u4e86\u8bed\u4e49\u591a\u6837\u6027\u3002", "conclusion": "MaskAnyNet\u901a\u8fc7\u5c06\u63a9\u76d6\u5185\u5bb9\u4f5c\u4e3a\u8f85\u52a9\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u518d\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u5229\u7528\u4e86\u63a9\u76d6\u533a\u57df\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u50cf\u63a9\u853d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.12498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12498", "abs": "https://arxiv.org/abs/2511.12498", "authors": ["Jongseong Bae", "Junwoo Ha", "Jinnyeong Heo", "Yeongin Lee", "Ha Young Kim"], "title": "Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion", "comment": "Accepted to AAAI 2026", "summary": "Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a C3DFusion \u7684\u65b0\u6a21\u5757\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u76f8\u673a\u7684 3D \u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u65b9\u6cd5\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u878d\u5408\u5f53\u524d\u548c\u5386\u53f2\u5e27\u7684\u7279\u5f81\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u88ab\u906e\u6321\u533a\u57df\uff08\u5c24\u5176\u662f\u5728\u8f66\u8f86\u4fa7\u9762\uff09\u65f6\u9047\u5230\u7684\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u673a\u7684 3D \u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u65b9\u6cd5\u5728\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u6765\u589e\u5f3a\u5f53\u524d\u5e27\u7684\u7279\u5f81\u65f6\uff0c\u4e3b\u8981\u5173\u6ce8\u5e27\u5185\u533a\u57df\uff0c\u800c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5e27\u5916\u533a\u57df\u3002", "method": "C3DFusion \u6a21\u5757\u901a\u8fc7\u663e\u5f0f\u5730\u5bf9\u9f50\u6765\u81ea\u5f53\u524d\u5e27\u548c\u5386\u53f2\u5e27\u7684 3D \u7279\u5f81\u70b9\uff0c\u751f\u6210\u5173\u6ce8\u9690\u85cf\u533a\u57df\u7684 3D \u7279\u5f81\u51e0\u4f55\u3002\u5b83\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u6280\u672f\uff1a\u5386\u53f2\u4e0a\u4e0b\u6587\u6a21\u7cca\uff08\u901a\u8fc7\u8870\u51cf\u5386\u53f2\u7279\u5f81\u70b9\u7684\u5c3a\u5ea6\u6765\u6291\u5236\u4e0d\u51c6\u786e\u7684\u626d\u66f2\u5e26\u6765\u7684\u566a\u58f0\uff09\u548c\u4ee5\u5f53\u524d\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u81f4\u5bc6\u5316\uff08\u901a\u8fc7\u589e\u52a0\u5f53\u524d\u7279\u5f81\u70b9\u7684\u4f53\u7d20\u8d21\u732e\u6765\u589e\u5f3a\u5b83\u4eec\uff09\u3002", "result": "C3DFusion \u6a21\u5757\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u6807\u51c6\u7684 SSC \u67b6\u6784\u4e2d\uff0c\u5e76\u5728 SemanticKITTI \u548c SSCBench-KITTI-360 \u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e94\u7528\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u65f6\u4e5f\u80fd\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "C3DFusion \u6a21\u5757\u901a\u8fc7\u6709\u6548\u7684\u65f6\u6001\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 3D \u8bed\u4e49\u573a\u666f\u8865\u5168\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u88ab\u906e\u6321\u533a\u57df\u65b9\u9762\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002"}}
{"id": "2511.12503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12503", "abs": "https://arxiv.org/abs/2511.12503", "authors": ["Fereidoon Zangeneh", "Leonard Bruns", "Amit Dekel", "Alessandro Pieropan", "Patric Jensfelt"], "title": "Visible Structure Retrieval for Lightweight Image-Based Relocalisation", "comment": "Accepted at BMVC 2025", "summary": "Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4ece\u56fe\u50cf\u5230\u53ef\u89c1\u573a\u666f\u7ed3\u6784\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u6765\u51cf\u5c0f2D-3D\u5bf9\u5e94\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5\u5728\u5927\u578b\u573a\u666f\u4e2d\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u65f6\uff0c\u9762\u4e34\u641c\u7d22\u6548\u7387\u548c\u5b58\u50a8\u6210\u672c\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u641c\u7d22\u6216\u56fe\u50cf\u68c0\u7d22\uff0c\u4f46\u8fd9\u4f1a\u589e\u52a0\u7ba1\u9053\u590d\u6742\u6027\u6216\u5b58\u50a8\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u7d27\u51d1\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u76f4\u63a5\u4ece\u56fe\u50cf\u89c2\u6d4b\u6620\u5c04\u5230\u53ef\u89c1\u7684\u573a\u666f\u7ed3\u6784\u3002\u8be5\u7f51\u7edc\u80fd\u591f\u9884\u6d4b\u56fe\u50cf\u6240\u89c1\u76843D\u7ed3\u6784\u70b9\u5b50\u96c6\uff0c\u4ece\u800c\u7f29\u5c0f2D-3D\u5bf9\u5e94\u641c\u7d22\u7684\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u4e0e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u548c\u5b58\u50a8\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u56fe\u50cf\u5230\u53ef\u89c1\u7ed3\u6784\u70b9\u4e91\u7684\u6620\u5c04\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u89c6\u89c9\u91cd\u5b9a\u4f4d\u7684\u6548\u7387\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2511.12511", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12511", "abs": "https://arxiv.org/abs/2511.12511", "authors": ["Jialiang Shen", "Jiyang Zheng", "Yunqi Xue", "Huajie Chen", "Yu Yao", "Hui Kang", "Ruiqi Liu", "Helin Gong", "Yang Yang", "Dadong Wang", "Tongliang Liu"], "title": "DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection", "comment": "12 pages, 5 figures", "summary": "With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e08\u751f\u77e5\u8bc6\u84b8\u998f\u7684\u6a21\u7cca\u9c81\u68d2AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\uff08DINOv3\uff09\u63d0\u4f9b\u6307\u5bfc\uff0c\u5b66\u751f\u6a21\u578b\u5728\u6a21\u7cca\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u9000\u5316\uff08\u5c24\u5176\u662f\u8fd0\u52a8\u6a21\u7cca\uff09\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684AI\u751f\u6210\u56fe\u50cf\uff08AIGI\uff09\u68c0\u6d4b\u5668\u5728\u5904\u7406\u8fd0\u52a8\u6a21\u7cca\u7b49\u771f\u5b9e\u4e16\u754c\u9000\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u56e0\u4e3a\u8fd0\u52a8\u6a21\u7cca\u4f1a\u626d\u66f2\u7eb9\u7406\u5e76\u6291\u5236\u9ad8\u9891\u4f2a\u5f71\u3002", "method": "\u5229\u7528\u5e08\u751f\u77e5\u8bc6\u84b8\u998f\uff0c\u4e00\u4e2a\u5728\u6e05\u6670\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u9ad8\u5bb9\u91cf\u6559\u5e08\u6a21\u578b\uff08DINOv3\uff09\u63d0\u4f9b\u5176\u7279\u5f81\u548clogit\u54cd\u5e94\uff0c\u4e00\u4e2a\u5728\u6a21\u7cca\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u84b8\u998f\u6765\u5b66\u4e60\uff0c\u4ee5\u751f\u6210\u5728\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u9c81\u68d2\u8868\u793a\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fd0\u52a8\u6a21\u7cca\u548c\u6e05\u6670\u6761\u4ef6\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u7cca\u9c81\u68d2AIGI\u68c0\u6d4b\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86AIGI\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12525", "abs": "https://arxiv.org/abs/2511.12525", "authors": ["Jing Li", "Yifan Wang", "Jiafeng Yan", "Renlong Zhang", "Bin Yang"], "title": "MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics", "comment": "10 pages, 7 figures. Accepted by AAAI 2026", "summary": "Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MdaIF \u7684\u4e00\u4f53\u5316\u3001\u9000\u5316\u611f\u77e5\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u878d\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u6076\u52a3\u5929\u6c14\uff08\u5982\u96fe\u3001\u96e8\u3001\u96ea\uff09\u5bfc\u81f4\u7684\u56fe\u50cf\u9000\u5316\uff0c\u4e14\u4f9d\u8d56\u56fa\u5b9a\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u9002\u5e94\u6027\u5dee\uff0c\u8fd9\u9650\u5236\u4e86\u878d\u5408\u6027\u80fd\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u9002\u5e94\u4e0d\u540c\u9000\u5316\u573a\u666f\u7684\u56fe\u50cf\u878d\u5408\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MdaIF \u7684\u4e00\u4f53\u5316\u9000\u5316\u611f\u77e5\u56fe\u50cf\u878d\u5408\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7cfb\u7edf\u6765\u5904\u7406\u4e0d\u540c\u9000\u5316\u573a\u666f\uff08\u96fe\u3001\u96e8\u3001\u96ea\uff09\u7684\u56fe\u50cf\u878d\u5408\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u53d6\u8bed\u4e49\u5148\u9a8c\uff0c\u4ee5\u9002\u5e94\u6027\u5730\u83b7\u53d6\u4e0d\u540c\u9000\u5316\u573a\u666f\u7684\u77e5\u8bc6\u548c\u573a\u666f\u7279\u5f81\u3002\u57fa\u4e8e\u6b64\u8bed\u4e49\u5148\u9a8c\uff0c\u8bbe\u8ba1\u4e86\u9000\u5316\u611f\u77e5\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\uff08DCAM\uff09\uff0c\u901a\u8fc7\u901a\u9053\u57df\u7684\u7279\u5f81\u4ea4\u4e92\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002\u6700\u540e\uff0c\u5229\u7528\u8bed\u4e49\u5148\u9a8c\u548c\u901a\u9053\u57df\u7279\u5f81\u6765\u6307\u5bfc MoE \u8fdb\u884c\u4e13\u5bb6\u8def\u7531\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u56fe\u50cf\u878d\u5408\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cMdaIF \u6846\u67b6\u5728\u590d\u6742\u9000\u5316\u573a\u666f\u4e0b\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u56fe\u50cf\u878d\u5408\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MdaIF \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf\u3001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u9000\u5316\u611f\u77e5\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12528", "abs": "https://arxiv.org/abs/2511.12528", "authors": ["Zheyuan Zhang", "Jiwei Zhang", "Boyu Zhou", "Linzhimeng Duan", "Hong Chen"], "title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation", "comment": null, "summary": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.", "AI": {"tldr": "D2-VPR\u662f\u4e00\u4e2a\u57fa\u4e8e\u84b8\u998f\u548c\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u8bbe\u8ba1\u65b0\u7684\u6a21\u5757\uff0c\u5728\u4fdd\u6301DINOv2\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u5e76\u5728\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv2\uff09\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51faD2-VPR\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u77e5\u8bc6\u84b8\u998f\u548c\u5fae\u8c03\uff09\uff0c\u5e76\u5f15\u5165\u84b8\u998f\u6062\u590d\u6a21\u5757\uff08DRM\uff09\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u7684\u7279\u5f81\u7a7a\u95f4\uff1b\u8bbe\u8ba1\u4e86\u57fa\u4e8eTop-Down\u6ce8\u610f\u529b\u7684\u53ef\u53d8\u5f62\u805a\u5408\u5668\uff08TDDA\uff09\uff0c\u5229\u7528\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u81ea\u9002\u5e94\u8c03\u6574\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u7528\u4e8e\u805a\u5408\u3002", "result": "D2-VPR\u76f8\u6bd4CricaVPR\uff0c\u53c2\u6570\u91cf\u51cf\u5c11\u7ea664.2%\uff0cFLOPs\u51cf\u5c11\u7ea662.6%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "D2-VPR\u5728\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u53ef\u53d8\u5f62\u805a\u5408\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd-\u6548\u7387\u6298\u8877\u3002"}}
{"id": "2511.12530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12530", "abs": "https://arxiv.org/abs/2511.12530", "authors": ["Yuan Zhou", "Litao Hua", "Shilong Jin", "Wentao Huang", "Haoran Duan"], "title": "ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding", "comment": "Accepted to AAAI 2026. Code is available at: https://github.com/robin-hlt/AAAI26-ReaSon", "summary": "Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.", "AI": {"tldr": "ReaSon\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u4fe1\u606f\u74f6\u9888\uff08CIB\uff09\u6765\u4f18\u5316\u89c6\u9891\u5173\u952e\u5e27\u9009\u62e9\uff0c\u4ee5\u6ee1\u8db3\u9884\u6d4b\u5145\u5206\u6027\u548c\u56e0\u679c\u5fc5\u8981\u6027\u3002", "motivation": "\u89c6\u9891\u7406\u89e3\u9700\u8981\u6709\u6548\u6355\u6349\u4fe1\u606f\u4e14\u5177\u6709\u56e0\u679c\u51b3\u5b9a\u6027\u7684\u5173\u952e\u5e27\uff0c\u4ee5\u5e94\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8f93\u5165\u9650\u5236\u548c\u4fe1\u606f\u7684\u65f6\u95f4\u7a00\u758f\u6027\u3002", "method": "ReaSon\u6846\u67b6\u4f7f\u7528\u53ef\u5b66\u4e60\u7b56\u7565\u7f51\u7edc\u9009\u62e9\u5019\u9009\u5e27\u4ee5\u6355\u6349\u9884\u6d4b\u5145\u5206\u6027\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5e72\u9884\u8bc4\u4f30\u56e0\u679c\u5fc5\u8981\u6027\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548cCIB\u539f\u5219\u7684\u590d\u5408\u5956\u52b1\u6765\u6307\u5bfc\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728NExT-QA\u3001EgoSchema\u548cVideo-MME\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReaSon\u5728\u6709\u9650\u5e27\u6570\u8bbe\u7f6e\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ReaSon\u6846\u67b6\u5728\u89c6\u9891\u5173\u952e\u5e27\u9009\u62e9\u65b9\u9762\u662f\u6709\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u3002"}}
{"id": "2511.12547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12547", "abs": "https://arxiv.org/abs/2511.12547", "authors": ["Zhiguang Lu", "Qianqian Xu", "Peisong Wen", "Siran Da", "Qingming Huang"], "title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models", "comment": null, "summary": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.", "AI": {"tldr": "HiGFA\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u65f6\u5e8f\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u3001\u8f6e\u5ed3\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u5fe0\u5b9e\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6807\u51c6\u7684\u6587\u672c\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u96be\u4ee5\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5305\u542b\u7ec6\u5fae\u7684\u7c7b\u522b\u5b9a\u4e49\u7279\u5f81\uff0c\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u6837\u672c\uff0c\u4ece\u800c\u5f71\u54cd\u5206\u7c7b\u5668\u6027\u80fd\u3002", "method": "HiGFA\u5229\u7528\u6269\u6563\u91c7\u6837\u7684\u65f6\u5e8f\u52a8\u6001\uff0c\u5728\u65e9\u671f\u548c\u4e2d\u671f\u91c7\u7528\u56fa\u5b9a\u5f3a\u5ea6\u7684\u6587\u672c\u548c\u53d8\u6362\u8f6e\u5ed3\u5f15\u5bfc\u6765\u5efa\u7acb\u6574\u4f53\u573a\u666f\u3001\u98ce\u683c\u548c\u7ed3\u6784\uff0c\u5728\u540e\u671f\u6fc0\u6d3b\u4e13\u95e8\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u5e76\u6839\u636e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u52a8\u6001\u8c03\u6574\u6240\u6709\u5f15\u5bfc\u4fe1\u53f7\u7684\u5f3a\u5ea6\u3002", "result": "HiGFA\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u5fe0\u5b9e\u7684\u5408\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u5e73\u8861\u5168\u5c40\u7ed3\u6784\u5f62\u6210\u548c\u7cbe\u786e\u7ec6\u8282\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\uff08FGVC\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "HiGFA\u901a\u8fc7\u5206\u5c42\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u5f15\u5bfc\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u589e\u5f3a\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.12554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12554", "abs": "https://arxiv.org/abs/2511.12554", "authors": ["Yijie Guo", "Dexiang Hong", "Weidong Chen", "Zihan She", "Cheng Ye", "Xiaojun Chang", "Zhendong Mao"], "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis", "comment": "11 pages, 7 figures. This is a preprint version of a paper submitted to CVPR 2026", "summary": "Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.", "AI": {"tldr": "EmoVerse\u662f\u4e00\u4e2a\u5927\u578b\u5f00\u653e\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u5c42\u3001\u77e5\u8bc6\u56fe\u8c31\u542f\u53d1\u7684\u6ce8\u91ca\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u60c5\u611f\u5206\u6790\uff0c\u5c06\u60c5\u7eea\u5206\u89e3\u4e3a\u80cc\u666f-\u5c5e\u6027-\u4e3b\u4f53\uff08B-A-S\uff09\u4e09\u5143\u7ec4\uff0c\u5e76\u4e3a\u5206\u7c7b\u60c5\u611f\u72b6\u6001\uff08CES\uff09\u548c\u7ef4\u5ea6\u60c5\u611f\u7a7a\u95f4\uff08DES\uff09\u63d0\u4f9b\u53cc\u91cd\u6ce8\u91ca\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u60c5\u611f\u5206\u6790\uff08VEA\uff09\u7814\u7a76\u56e0\u7f3a\u4e4f\u5f00\u6e90\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u96c6\u800c\u53d7\u9650\uff0c\u901a\u5e38\u53ea\u4e3a\u6574\u5f20\u56fe\u50cf\u5206\u914d\u5355\u4e00\u79bb\u6563\u60c5\u611f\u6807\u7b7e\uff0c\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3\u89c6\u89c9\u5143\u7d20\u5982\u4f55\u5f71\u54cd\u60c5\u611f\u3002", "method": "\u901a\u8fc7\u5c06\u60c5\u611f\u5206\u89e3\u4e3a\u80cc\u666f-\u5c5e\u6027-\u4e3b\u4f53\uff08B-A-S\uff09\u4e09\u5143\u7ec4\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5143\u7d20\u4e0e\u89c6\u89c9\u533a\u57df\u5173\u8054\uff0cEmoVerse\u5b9e\u73b0\u4e86\u8bcd\u7ea7\u522b\u548c\u4e3b\u4f53\u7ea7\u522b\u7684\u3067\u60c5\u611f\u63a8\u7406\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc721.9\u4e07\u5f20\u56fe\u50cf\uff0c\u5e76\u63d0\u4f9b\u5206\u7c7b\u60c5\u611f\u72b6\u6001\uff08CES\uff09\u548c\u7ef4\u5ea6\u60c5\u611f\u7a7a\u95f4\uff08DES\uff09\u7684\u53cc\u91cd\u6ce8\u91ca\u3002\u91c7\u7528\u65b0\u9896\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\u786e\u4fdd\u4e86\u9ad8\u6ce8\u91ca\u53ef\u9760\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u7ebf\u7d22\u6620\u5c04\u5230DES\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u5f52\u56e0\u89e3\u91ca\u3002", "result": "EmoVerse\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc721.9\u4e07\u5f20\u56fe\u50cf\uff0c\u5e76\u63d0\u4f9bCES\u548cDES\u53cc\u91cd\u6ce8\u91ca\uff0c\u652f\u6301\u79bb\u6563\u548c\u8fde\u7eed\u7684\u60c5\u611f\u8868\u793a\u3002\u5f15\u5165\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u80fd\u591f\u5c06\u89c6\u89c9\u7ebf\u7d22\u6620\u5c04\u5230DES\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u5f52\u56e0\u89e3\u91ca\u3002", "conclusion": "EmoVerse\u6570\u636e\u96c6\u3001\u65b0\u9896\u7684\u6ce8\u91ca\u6d41\u7a0b\u548c\u53ef\u89e3\u91ca\u6a21\u578b\u5171\u540c\u4e3a\u63a8\u8fdb\u53ef\u89e3\u91ca\u7684\u9ad8\u5c42\u6b21\u60c5\u611f\u7406\u89e3\u5960\u5b9a\u4e86\u5168\u9762\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12559", "abs": "https://arxiv.org/abs/2511.12559", "authors": ["Qing Cai", "Guihao Yan", "Fan Zhang", "Cheng Zhang", "Zhi Liu"], "title": "SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition", "comment": "Accepted by AAAI 2026", "summary": "Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.", "AI": {"tldr": "SEMC\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49-\u7ed3\u6784\u878d\u5408\u6a21\u5757\uff08SSFM\uff09\u548c\u6df7\u5408\u4e13\u5bb6\u5bf9\u6bd4\u8bc6\u522b\u6a21\u5757\uff08MCRM\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u6d45\u5c42\u7ed3\u6784\u4fe1\u606f\u548c\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u6d45\u5c42\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u96be\u4ee5\u901a\u8fc7\u5bf9\u6bd4\u6837\u672c\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5dee\u5f02\uff0c\u5bfc\u81f4\u5bf9\u7ed3\u6784\u548c\u533a\u5206\u6027\u7ec6\u8282\u7684\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u589e\u5f3a\u6df7\u5408\u4e13\u5bb6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08SEMC\uff09\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u878d\u5408\u548c\u4e13\u5bb6\u6307\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u8bed\u4e49-\u7ed3\u6784\u878d\u5408\u6a21\u5757\uff08SSFM\uff09\uff0c\u7528\u4e8e\u5229\u7528\u591a\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u5bf9\u9f50\u6d45\u5c42\u548c\u6df1\u5c42\u7279\u5f81\u6765\u589e\u5f3a\u6a21\u578b\u611f\u77e5\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7ec6\u8282\u7684\u80fd\u529b\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6df7\u5408\u4e13\u5bb6\u5bf9\u6bd4\u8bc6\u522b\u6a21\u5757\uff08MCRM\uff09\uff0c\u5229\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u673a\u5236\u5728\u591a\u5c42\u6b21\u7279\u5f81\u4e0a\u6267\u884c\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u7c7b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7c7b\u53ef\u5206\u79bb\u6027\u548c\u8bc6\u522b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u7406\u4e86\u4e00\u4e2a\u5305\u542b\u516d\u4e2a\u6807\u51c6\u5e73\u9762\u7684\u5927\u89c4\u6a21\u3001\u7cbe\u5fc3\u6807\u6ce8\u7684\u809d\u810f\u8d85\u58f0\u6570\u636e\u96c6\u3002", "result": "\u5728\u5185\u90e8\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEMC\u5728\u5404\u79cd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u8fd1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SEMC\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684SSFM\u548cMCRM\u6a21\u5757\uff0c\u5728\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u7ed3\u6784\u4fe1\u606f\u548c\u63d0\u9ad8\u7c7b\u53ef\u5206\u79bb\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12572", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12572", "abs": "https://arxiv.org/abs/2511.12572", "authors": ["Mohamed Youssef", "Lukas Brunner", "Klaus Rundhammer", "Gerald Czech", "Oliver Bimber"], "title": "Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection", "comment": null, "summary": "We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cd\u5efa\u88ab\u68ee\u6797\u690d\u88ab\u906e\u6321\u7684\u8868\u9762\u6e29\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u5168\u81ea\u52a8\u822a\u62cd\u91ce\u706b\u76d1\u6d4b\u548c\u65e9\u671f\u706b\u707e\u63a2\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u80fd\u591f\u65e9\u671f\u63a2\u6d4b\u5230\u70df\u96fe\u6216\u706b\u7130\u53ef\u89c1\u4e4b\u524d\u7684\u5730\u9762\u706b\u707e\u7684\u5168\u81ea\u52a8\u822a\u62cd\u91ce\u706b\u76d1\u6d4b\uff0c\u7279\u522b\u662f\u5728\u6709\u68ee\u6797\u690d\u88ab\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4ece\u7ecf\u8fc7\u53bb\u6a21\u7cca\u5904\u7406\u7684\u5408\u6210\u5b54\u5f84\uff08SA\uff09\u4f20\u611f\u5668\u6570\u636e\u4e2d\u6062\u590d\u90e8\u5206\u906e\u6321\u7684\u571f\u58e4\u548c\u706b\u70b9\u70ed\u4fe1\u53f7\u3002\u901a\u8fc7\u5c06\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6574\u5408\u5230\u5411\u91cf\u91cf\u5316\u5668\u4e2d\uff0c\u5e76\u7ed3\u5408\u6e29\u5ea6\u589e\u5f3a\u548c\u7a0b\u5e8f\u5316\u70ed\u529b\u68ee\u6797\u6a21\u62df\uff0c\u751f\u6210\u4e86\u5927\u91cf\u7684\u771f\u5b9e\u8868\u9762\u6e29\u5ea6\u6a21\u62df\u6570\u636e\uff0c\u4ee5\u514b\u670d\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u964d\u4f4e\u4e862\u52302.5\u500d\u3002\u5728\u5b9e\u5730\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u4f20\u7edf\u70ed\u6210\u50cf\u76f8\u6bd4\uff0cRMSE\u63d0\u9ad8\u4e8612.8\u500d\uff0c\u4e0e\u672a\u6821\u6b63\u7684SA\u56fe\u50cf\u76f8\u6bd4\uff0cRMSE\u63d0\u9ad8\u4e862.6\u500d\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u91cd\u5efa\u5b8c\u6574\u7684\u706b\u707e\u548c\u4eba\u7c7b\u4fe1\u53f7\u5f62\u6001\uff0c\u514b\u670d\u4e86\u90e8\u5206\u906e\u6321\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf9\u5176\u4ed6\u70ed\u4fe1\u53f7\uff08\u5982\u641c\u6551\u4e2d\u7684\u4eba\u7c7b\u4fe1\u53f7\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u68ee\u6797\u690d\u88ab\u906e\u6321\u95ee\u9898\uff0c\u7cbe\u786e\u91cd\u5efa\u8868\u9762\u6e29\u5ea6\uff0c\u5e76\u5728\u91ce\u706b\u63a2\u6d4b\u548c\u641c\u6551\u7b49\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.12575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12575", "abs": "https://arxiv.org/abs/2511.12575", "authors": ["Jiayi Zhu", "Yihao Huang", "Yue Cao", "Xiaojun Jia", "Qing Guo", "Felix Juefei-Xu", "Geguang Pu", "Bin Wang"], "title": "Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection", "comment": null, "summary": "Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u53ef\u80fd\u6cc4\u9732\u7528\u6237\u5730\u7406\u4f4d\u7f6e\uff0c\u73b0\u6709\u5bf9\u6297\u6027\u6270\u52a8\u65b9\u6cd5\u4f1a\u635f\u5bb3\u56fe\u50cf\u8d28\u91cf\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u5177\u6709\u6b3a\u9a97\u6027\u8bed\u4e49\u7684\u6587\u672c\u6765\u4fdd\u62a4\u7528\u6237\u5730\u7406\u4f4d\u7f6e\u9690\u79c1\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u635f\u5bb3\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u964d\u4f4e\u4e86LVLM\u7684\u5730\u7406\u4f4d\u7f6e\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u80fd\u591f\u4ece\u56fe\u50cf\u4e2d\u63a8\u65ad\u7528\u6237\u5730\u7406\u4f4d\u7f6e\uff0c\u5bf9\u7528\u6237\u9690\u79c1\u6784\u6210\u5a01\u80c1\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u6270\u52a8\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u9700\u8981\u8f83\u5f3a\u7684\u5931\u771f\u624d\u80fd\u6709\u6548\uff0c\u8fd9\u4f1a\u660e\u663e\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u53c8\u4e0d\u635f\u5bb3\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684\u201c\u5370\u5237\u653b\u51fb\u201d\uff08typographical attacks\uff09\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u5916\u90e8\u6dfb\u52a0\u6587\u672c\u6765\u5e72\u6270LVLM\u7684\u5730\u7406\u4f4d\u7f6e\u63a8\u65ad\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u3001\u8bed\u4e49\u611f\u77e5\u7684\u5370\u5237\u653b\u51fb\u65b9\u6cd5\uff0c\u751f\u6210\u6b3a\u9a97\u6027\u7684\u6587\u672c\u6765\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u5546\u7528LVLM\u7684\u5730\u7406\u4f4d\u7f6e\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u89c6\u89c9\u4e0a\u65e0\u635f\u7684\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\u3002", "conclusion": "\u57fa\u4e8e\u6587\u672c\u7684\u5370\u5237\u653b\u51fb\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u89c6\u89c9\u4e0a\u65e0\u635f\u7684\u4fdd\u62a4\u7528\u6237\u5730\u7406\u4f4d\u7f6e\u9690\u79c1\u514d\u53d7LVLM\u5a01\u80c1\u7684\u7b56\u7565\u3002"}}
{"id": "2511.12578", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12578", "abs": "https://arxiv.org/abs/2511.12578", "authors": ["Yukuo Ma", "Cong Liu", "Junke Wang", "Junqi Liu", "Haibin Huang", "Zuxuan Wu", "Chi Zhang", "Xuelong Li"], "title": "TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction", "comment": null, "summary": "We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.", "AI": {"tldr": "TempoMaster\u901a\u8fc7\u5c06\u957f\u89c6\u9891\u751f\u6210\u89c6\u4e3a\u4e0b\u4e00\u5e27\u7387\u9884\u6d4b\u6765\u5de5\u4f5c\uff0c\u9996\u5148\u751f\u6210\u4f4e\u5e27\u7387\u7684\u7c97\u7565\u89c6\u9891\uff0c\u7136\u540e\u9010\u6b65\u63d0\u9ad8\u5e27\u7387\u4ee5\u5b8c\u5584\u7ec6\u8282\u548c\u8fd0\u52a8\u8fde\u7eed\u6027\uff0c\u5b9e\u73b0\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6700\u4f18\u6548\u679c\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\uff0c\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u751f\u6210\u4f4e\u5e27\u7387\u7684\u526a\u8f91\uff0c\u4f5c\u4e3a\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u7684\u7c97\u7565\u84dd\u56fe\uff0c\u7136\u540e\u9010\u6b65\u63d0\u9ad8\u5e27\u7387\u4ee5\u5b8c\u5584\u89c6\u89c9\u7ec6\u8282\u548c\u8fd0\u52a8\u8fde\u7eed\u6027\u3002", "method": "TempoMaster\u91c7\u7528\u53cc\u5411\u6ce8\u610f\u529b\u548c\u8de8\u5e27\u7387\u7684\u81ea\u56de\u5f52\u6765\u5b9e\u73b0\u9ad8\u6548\u548c\u5e76\u884c\u7684\u5408\u6210\u3002", "result": "TempoMaster\u5728\u957f\u89c6\u9891\u751f\u6210\u65b9\u9762\u786e\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u89c6\u89c9\u548c\u65f6\u95f4\u8d28\u91cf\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TempoMaster\u901a\u8fc7\u5c06\u957f\u89c6\u9891\u751f\u6210\u89c6\u4e3a\u4e0b\u4e00\u5e27\u7387\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6700\u4f18\u6548\u679c\u3002"}}
{"id": "2511.12588", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12588", "abs": "https://arxiv.org/abs/2511.12588", "authors": ["Zuqi Huang", "Mengxin Tian", "Huan Liu", "Wentao Li", "Baobao Liang", "Jie Wu", "Fang Yan", "Zhaoqing Tang", "Zhongyu Li"], "title": "Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting", "comment": null, "summary": "Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.", "AI": {"tldr": "CountIHC\u662f\u4e00\u4e2a\u7528\u4e8e\u514d\u75ab\u7ec4\u7ec7\u5316\u5b66(IHC)\u56fe\u50cf\u7684\u591a\u7c7b\u522b\u7ec6\u80de\u8ba1\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u89e3\u51b3\u4e86\u7ec6\u80de\u91cd\u53e0\u548c\u67d3\u8272\u53d8\u5f02\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u514d\u75ab\u7ec4\u7ec7\u5316\u5b66(IHC)\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u8ba1\u6570\u5bf9\u4e8e\u91cf\u5316\u86cb\u767d\u8d28\u8868\u8fbe\u548c\u8f85\u52a9\u764c\u75c7\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7ec6\u80de\u91cd\u53e0\u3001\u67d3\u8272\u53d8\u5f02\u548c\u5f62\u6001\u591a\u6837\u6027\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u91cd\u53e0\u7ec6\u80de\u548c\u8fdb\u884c\u591a\u7c7b\u522b\u8ba1\u6570\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79e9\u611f\u77e5\u805a\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u79e9\u611f\u77e5\u6559\u5e08\u9009\u62e9(RATS)\u7b56\u7565\u4ece\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u4e2d\u84b8\u998f\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u5fae\u8c03\u9636\u6bb5\uff0c\u5229\u7528\u79bb\u6563\u8bed\u4e49\u951a\u70b9\u8fdb\u884c\u591a\u7c7b\u522b\u7ec6\u80de\u8ba1\u6570\u3002", "result": "CountIHC\u572812\u79cdIHC\u751f\u7269\u6807\u5fd7\u7269\u548c5\u79cd\u7ec4\u7ec7\u7c7b\u578b\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u75c5\u7406\u5b66\u5bb6\u7684\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u3002\u8be5\u65b9\u6cd5\u5728\u82cf\u6728\u7cbe-\u4f0a\u7ea2(H&E)\u67d3\u8272\u6570\u636e\u4e0a\u4e5f\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "CountIHC\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u548c\u591a\u7c7b\u522b\u8ba1\u6570\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86IHC\u56fe\u50cf\u7ec6\u80de\u8ba1\u6570\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.12590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12590", "abs": "https://arxiv.org/abs/2511.12590", "authors": ["Guoqing Xu", "Yiheng Li", "Yang Yang"], "title": "Fine-Grained Representation for Lane Topology Reasoning", "comment": null, "summary": "Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTopoFG\u7684\u7ec6\u7c92\u5ea6\u8f66\u9053\u62d3\u6251\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8f66\u9053\u7ed3\u6784\u4e2d\u62d3\u6251\u9884\u6d4b\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002TopoFG\u901a\u8fc7\u5206\u5c42\u5148\u9a8c\u63d0\u53d6\u5668\uff08HPE\uff09\u3001\u533a\u57df\u805a\u7126\u89e3\u7801\u5668\uff08RFD\uff09\u548c\u9c81\u68d2\u8fb9\u754c\u70b9\u62d3\u6251\u63a8\u7406\uff08RBTR\uff09\u4e09\u4e2a\u9636\u6bb5\uff0c\u4eceBEV\u7279\u5f81\u5230\u62d3\u6251\u9884\u6d4b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002HPE\u63d0\u53d6\u5168\u5c40\u7a7a\u95f4\u5148\u9a8c\u548c\u5c40\u90e8\u5e8f\u5217\u5148\u9a8c\uff0cRFD\u6574\u5408\u8fd9\u4e9b\u5148\u9a8c\u6784\u5efa\u7ec6\u7c92\u5ea6\u67e5\u8be2\u5e76\u7ec6\u5316\u8868\u793a\uff0cRBTR\u57fa\u4e8e\u8fb9\u754c\u70b9\u67e5\u8be2\u548c\u62d3\u6251\u53bb\u566a\u7b56\u7565\u8fdb\u884c\u8fde\u63a5\u6027\u5efa\u6a21\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTopoFG\u5728OpenLane-V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u8f66\u9053\u7ed3\u6784\uff0c\u5bfc\u81f4\u62d3\u6251\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8f66\u9053\u62d3\u6251\u5efa\u6a21\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7684\u5bfc\u822a\u548c\u63a7\u5236\u51b3\u7b56\u3002", "method": "TopoFG\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a1. \u5206\u5c42\u5148\u9a8c\u63d0\u53d6\u5668\uff08HPE\uff09\uff1a\u63d0\u53d6BEV\u63a9\u7801\u7684\u5168\u5c40\u7a7a\u95f4\u5148\u9a8c\u548c\u8f66\u9053\u5185\u5173\u952e\u70b9\u5e8f\u5217\u7684\u5c40\u90e8\u5e8f\u5217\u5148\u9a8c\u30022. \u533a\u57df\u805a\u7126\u89e3\u7801\u5668\uff08RFD\uff09\uff1a\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u5e8f\u5217\u5148\u9a8c\u6765\u6784\u5efa\u7ec6\u7c92\u5ea6\u67e5\u8be2\uff0c\u5e76\u5728RoI\u533a\u57df\u91c7\u6837\u53c2\u8003\u70b9\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7ec6\u5316\u6bcf\u4e2a\u8f66\u9053\u7684\u67e5\u8be2\u8868\u793a\u30023. \u9c81\u68d2\u8fb9\u754c\u70b9\u62d3\u6251\u63a8\u7406\uff08RBTR\uff09\uff1a\u57fa\u4e8e\u8fb9\u754c\u70b9\u67e5\u8be2\u7279\u5f81\u8fdb\u884c\u8f66\u9053\u8fde\u63a5\u6027\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u62d3\u6251\u53bb\u566a\u7b56\u7565\u51cf\u5c11\u5339\u914d\u6b67\u4e49\u3002", "result": "TopoFG\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u4fe1\u606f\u5230\u7ec6\u7c92\u5ea6\u67e5\u8be2\u548c\u91c7\u7528\u53bb\u566a\u7b56\u7565\uff0c\u80fd\u591f\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u8f66\u9053\u7ed3\u6784\u5e76\u63d0\u4f9b\u53ef\u4fe1\u7684\u62d3\u6251\u9884\u6d4b\u3002\u5728OpenLane-V2\u57fa\u51c6\u6d4b\u8bd5\u7684subsetA\u4e0a\u8fbe\u5230\u4e8648.0%\u7684OLS\uff0c\u5728subsetB\u4e0a\u8fbe\u5230\u4e8645.4%\u7684OLS\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TopoFG\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u7ec6\u7c92\u5ea6\u67e5\u8be2\u548c\u591a\u9636\u6bb5\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u8f66\u9053\u62d3\u6251\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8f66\u9053\u62d3\u6251\u4fe1\u606f\u3002"}}
{"id": "2511.12594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12594", "abs": "https://arxiv.org/abs/2511.12594", "authors": ["Rongkun Zheng", "Lu Qi", "Xi Chen", "Yi Wang", "Kun Wang", "Hengshuang Zhao"], "title": "Seg-VAR: Image Segmentation with Visual Autoregressive Modeling", "comment": "NeurIPS 2025, 22 pages", "summary": "While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.", "AI": {"tldr": "Seg-VAR \u5c06\u5206\u5272\u89c6\u4e3a\u6761\u4ef6\u81ea\u56de\u5f52\u63a9\u7801\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u50cf\u7f16\u7801\u5668\u3001\u7a7a\u95f4\u611f\u77e5 seglat \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff08VAR\uff09\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c1a\u672a\u5728\u9700\u8981\u7cbe\u7ec6\u4f4e\u7ea7\u7a7a\u95f4\u611f\u77e5\u7684\u5206\u5272\u4efb\u52a1\u4e0a\u5f97\u5230\u63a2\u7d22\u3002", "method": "Seg-VAR \u6846\u67b6\u5c06\u5206\u5272\u89c6\u4e3a\u4e00\u4e2a\u6761\u4ef6\u81ea\u56de\u5f52\u63a9\u7801\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\uff081\uff09\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u56fe\u50cf\u7684\u6f5c\u5728\u5148\u9a8c\uff1b\uff082\uff09\u7a7a\u95f4\u611f\u77e5 seglat \u7f16\u7801\u5668\u5c06\u5206\u5272\u63a9\u7801\u6620\u5c04\u5230\u79bb\u6563\u7684\u6f5c\u5728\u4ee4\u724c\uff08\u4f7f\u7528\u4f4d\u7f6e\u654f\u611f\u7684\u989c\u8272\u6620\u5c04\u6765\u533a\u5206\u5b9e\u4f8b\uff09\uff1b\uff083\uff09\u89e3\u7801\u5668\u4ece\u8fd9\u4e9b\u6f5c\u5728\u4ee4\u724c\u4e2d\u91cd\u5efa\u63a9\u7801\u3002\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7\u56fe\u50cf-seglat \u8054\u5408\u8bad\u7ec3\u5b66\u4e60 seglat \u8868\u793a\uff0c\u7136\u540e\u4f18\u5316\u6f5c\u5728\u8f6c\u6362\uff0c\u6700\u540e\u5bf9\u9f50\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7684\u6f5c\u5728\u4ee4\u724c\u4e0e seglat \u5206\u5e03\u3002", "result": "Seg-VAR \u5728\u591a\u4e2a\u5206\u5272\u4efb\u52a1\u548c\u9a8c\u8bc1\u57fa\u51c6\u4e0a\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u3002", "conclusion": "Seg-VAR \u901a\u8fc7\u5c06\u5206\u5272\u6784\u5efa\u4e3a\u987a\u5e8f\u5206\u5c42\u9884\u6d4b\u4efb\u52a1\uff0c\u4e3a\u5c06\u81ea\u56de\u5f52\u63a8\u7406\u96c6\u6210\u5230\u7a7a\u95f4\u611f\u77e5\u89c6\u89c9\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.12602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12602", "abs": "https://arxiv.org/abs/2511.12602", "authors": ["Ria Shekhawat", "Sushrut Patwardhan", "Raghavendra Ramachandra", "Praveen Kumar Chandaliya", "Kishor P. Upla"], "title": "LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet", "comment": null, "summary": "Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a S-MAD \u7684\u65b0\u9896\u5355\u56fe\u50cf\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u5e76\u901a\u8fc7 LoRA \u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\uff08FRS\uff09\u5bb9\u6613\u53d7\u5230\u53d8\u5f62\u653b\u51fb\uff0c\u8fd9\u4f1a\u5bf9\u5b89\u5168\u7cfb\u7edf\u6784\u6210\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u7684\u65b0\u9896\u5355\u56fe\u50cf\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\uff08S-MAD\uff09\u65b9\u6cd5\uff0c\u5176\u4e2d\u57fa\u4e8e CNN \u7684\u6559\u5e08\u6a21\u578b\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e ViT \u7684\u5b66\u751f\u6a21\u578b\u3002\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u96c6\u6210\u4e86\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ece\u4e09\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9762\u90e8\u6570\u636e\u96c6\u4e2d\u6784\u5efa\u7684\u53d8\u5f62\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u7ed3\u5408\u4e86\u5341\u79cd\u4e0d\u540c\u7684\u53d8\u5f62\u751f\u6210\u7b97\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u7a33\u5065\u6027\u3002\u4e0e\u516d\u79cd\u6700\u5148\u8fdb\u7684 S-MAD \u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "S-MAD \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5355\u56fe\u50cf\u53d8\u5f62\u653b\u51fb\uff0c\u5e76\u5177\u6709\u51fa\u8272\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u5e94\u5bf9\u5404\u79cd\u53d8\u5f62\u751f\u6210\u6280\u672f\u3002"}}
{"id": "2511.12606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12606", "abs": "https://arxiv.org/abs/2511.12606", "authors": ["Drishya Karki", "Merey Ramazanova", "Anthony Cioppa", "Silvio Giancola", "Bernard Ghanem"], "title": "Pixels or Positions? Benchmarking Modalities in Group Activity Recognition", "comment": null, "summary": "Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\\%$ balanced accuracy compared to $58.1\\%$ for the best video baseline, while training $4.25 \\times$ faster with $438 \\times$ fewer parameters ($197K$ \\vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.", "AI": {"tldr": "Tracking data is more effective and efficient for group activity recognition (GAR) than video data, as shown by the new SoccerNet-GAR dataset and benchmark.", "motivation": "The paper aims to compare the effectiveness of video and tracking data for group activity recognition (GAR) and to establish a standardized benchmark for this comparison, as existing research has primarily focused on video data, leaving tracking data under-explored.", "method": "The authors introduce SoccerNet-GAR, a multimodal dataset synchronized with broadcast videos and player tracking data from the 2022 Football World Cup, annotated with 94,285 group activities across 10 categories. They also define a unified evaluation protocol to benchmark video-based and tracking-based classifiers, with a specific focus on a novel role-aware graph architecture for tracking-based GAR that encodes tactical structure through positional edges and temporal attention.", "result": "The tracking-based model, particularly the novel role-aware graph architecture, achieved 67.2% balanced accuracy, significantly outperforming the best video-based baseline at 58.1%. Furthermore, the tracking model trained 4.25 times faster and used 438 times fewer parameters (197K vs. 86.3M).", "conclusion": "The study highlights that tracking data, when modeled with role-aware graph architectures, is superior to video data for group activity recognition in terms of both accuracy and efficiency. It emphasizes the importance of modality choice and specialized modeling for effective GAR."}}
{"id": "2511.12607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12607", "abs": "https://arxiv.org/abs/2511.12607", "authors": ["Ziqiong Liu", "Yushun Tang", "Junyang Ji", "Zhihai He"], "title": "Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine", "comment": null, "summary": "Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u68af\u5f62\u7f51\u7edc\uff08Hierarchical Ladder Network, HLN\uff09\u7528\u4e8e\u6d4b\u8bd5\u65f6\u57df\u81ea\u9002\u5e94\uff08TTA\uff09\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\uff08OOD\uff09\u7c7b\u522b\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u8be5\u7f51\u7edc\u901a\u8fc7\u805a\u5408\u6240\u6709Transformer\u5c42\u7684\u7c7b\u4ee4\u724c\u6765\u63d0\u53d6OOD\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u6a21\u578b\u9884\u6d4b\u8fdb\u884c\u52a0\u6743\u6982\u7387\u878d\u5408\u4ee5\u589e\u5f3aOOD\u68c0\u6d4b\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u4eff\u5c04\u7f51\u7edc\uff08Attention Affine Network, AAN\uff09\u6765\u9002\u5e94\u57df\u6f02\u79fb\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u71b5\u673a\u5236\u6765\u6291\u5236\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5e38\u7528\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u57df\u81ea\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\uff08OOD\uff09\u7c7b\u522b\u65f6\uff0c\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u4e14\u53ef\u80fd\u5bfc\u81f4\u5bf9\u540e\u7eed\u540c\u7c7b\uff08ID\uff09\u6837\u672c\u7684\u9519\u8bef\u9002\u5e94\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u68af\u5f62\u7f51\u7edc\uff08HLN\uff09\u63d0\u53d6OOD\u7279\u5f81\uff0c\u5e76\u4e0e\u539f\u59cb\u6a21\u578b\u9884\u6d4b\u8fdb\u884c\u52a0\u6743\u6982\u7387\u878d\u5408\u3002\u5f15\u5165\u6ce8\u610f\u529b\u4eff\u5c04\u7f51\u7edc\uff08AAN\uff09\u81ea\u9002\u5e94\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5e94\u5bf9\u57df\u6f02\u79fb\u3002\u91c7\u7528\u52a0\u6743\u71b5\u673a\u5236\u52a8\u6001\u6291\u5236\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5e38\u7528\u5206\u7c7b\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5206\u5c42\u68af\u5f62\u7f51\u7edc\uff08HLN\uff09\u7ed3\u5408\u6ce8\u610f\u529b\u4eff\u5c04\u7f51\u7edc\uff08AAN\uff09\u548c\u52a0\u6743\u71b5\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3TTA\u5728\u9762\u5bf9OOD\u6837\u672c\u548c\u57df\u6f02\u79fb\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7684\u6574\u4f53\u5206\u7c7b\u8868\u73b0\u3002"}}
{"id": "2511.12627", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12627", "abs": "https://arxiv.org/abs/2511.12627", "authors": ["Baber Jan", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais", "Saeed Anwar"], "title": "C3Net: Context-Contrast Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.", "AI": {"tldr": "C3Net\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u4f2a\u88c5\u7269\u4f53\u7684\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u901a\u8def\u89e3\u7801\u5668\u67b6\u6784\u89e3\u51b3\u4e86\u5185 in \u76f8\u4f3c\u6027\u3001\u8fb9\u7f18\u4e2d\u65ad\u3001\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u3001\u73af\u5883\u590d\u6742\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u663e\u8457\u4f2a\u88c5\u7269\u4f53\u6d88\u6b67\u8fd9\u516d\u4e2a\u6838\u5fc3\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5272\u65b9\u6cd5\u548c\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u5728\u4f2a\u88c5\u7269\u4f53\u68c0\u6d4b\uff08COD\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7269\u4f53\u4e0e\u5468\u56f4\u73af\u5883\u5728\u989c\u8272\u3001\u7eb9\u7406\u548c\u6a21\u5f0f\u4e0a\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u5e76\u4e14\u5b58\u5728\u8fb9\u7f18\u4e2d\u65ad\u3001\u5c3a\u5ea6\u53d8\u5316\u3001\u73af\u5883\u590d\u6742\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u4ee5\u53ca\u663e\u8457\u4f2a\u88c5\u7269\u4f53\u6d88\u6b67\u7b49\u56fa\u6709\u6311\u6218\u3002", "method": "C3Net\u91c7\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u53cc\u901a\u8def\u89e3\u7801\u5668\u67b6\u6784\uff1a\u8fb9\u7f18\u7ec6\u5316\u901a\u8def\uff08Edge Refinement Pathway\uff09\u4f7f\u7528\u68af\u5ea6\u521d\u59cb\u5316\u7684\u8fb9\u7f18\u589e\u5f3a\u6a21\u5757\u6765\u6062\u590d\u7cbe\u786e\u7684\u8fb9\u754c\uff1b\u4e0a\u4e0b\u6587\u5b9a\u4f4d\u901a\u8def\uff08Contextual Localization Pathway\uff09\u5229\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\u5f15\u5bfc\u673a\u5236\uff08Image-based Context Guidance\uff09\u6765\u5b9e\u73b0\u5185\u5728\u663e\u8457\u6027\u6291\u5236\uff1b\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff08Attentive Fusion Module\uff09\u901a\u8fc7\u7a7a\u95f4\u95e8\u63a7\u673a\u5236\u534f\u540c\u878d\u5408\u4e24\u4e2a\u901a\u8def\u7684\u4fe1\u606f\u3002", "result": "C3Net\u5728COD10K\u3001CAMO\u548cNC4K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cS\u5ea6\u91cf\u5206\u522b\u8fbe\u5230\u4e860.898\u30010.904\u548c0.913\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "\u590d\u6742\u7684\u3001\u591a\u65b9\u9762\u7684\u68c0\u6d4b\u6311\u6218\u9700\u8981\u67b6\u6784\u521b\u65b0\uff0cC3Net\u8bc1\u660e\u4e86\u4e13\u95e8\u7684\u7ec4\u4ef6\u534f\u540c\u5de5\u4f5c\u53ef\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u8986\u76d6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b64\u7acb\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.12631", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12631", "abs": "https://arxiv.org/abs/2511.12631", "authors": ["Yushe Cao", "Dianxi Shi", "Xing Fu", "Xuechao Zou", "Haikuo Peng", "Xueqi Li", "Chun Yu", "Junliang Xing"], "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation", "comment": null, "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.", "AI": {"tldr": "MDiTFace\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u6a21\u6001\u9762\u90e8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u7b56\u7565\u548c\u65b0\u9896\u7684\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9762\u90e8\u4fdd\u771f\u5ea6\u548c\u6761\u4ef6\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u9762\u90e8\u751f\u6210\u65b9\u6cd5\u5728\u878d\u5408\u8bed\u4e49\u63a9\u7801\u548c\u6587\u672c\u63cf\u8ff0\u65f6\uff0c\u7531\u4e8e\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0d\u5145\u5206\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDiTFace\u7684\u5b9a\u5236\u5316\u6269\u6563Transformer\u6846\u67b6\uff0c\u5b83\u91c7\u7528\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u7b56\u7565\u5904\u7406\u8bed\u4e49\u63a9\u7801\u548c\u6587\u672c\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u5806\u53e0\u65b0\u8bbe\u8ba1\u7684\u591a\u5143Transformer\u5757\u4fc3\u8fdb\u8de8\u6a21\u6001\u7279\u5f81\u7684\u540c\u6b65\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u63a9\u7801\u6807\u8bb0\u548c\u65f6\u95f4\u5d4c\u5165\u7684\u9690\u5f0f\u4f9d\u8d56\u5206\u79bb\uff0c\u5206\u4e3a\u52a8\u6001\u548c\u9759\u6001\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u9759\u6001\u8def\u5f84\u7279\u5f81\u7684\u7f13\u5b58\u548c\u91cd\u7528\uff0c\u4ece\u800c\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5c06\u63a9\u7801\u6761\u4ef6\u5f15\u5165\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u51cf\u5c11\u4e8694%\u4ee5\u4e0a\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cMDiTFace\u5728\u9762\u90e8\u4fdd\u771f\u5ea6\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MDiTFace\u901a\u8fc7\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u4f18\u5316\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u9762\u90e8\u751f\u6210\u4e2d\u7684\u4e0d\u8db3\uff0c\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12633", "abs": "https://arxiv.org/abs/2511.12633", "authors": ["Xunzhi Xiang", "Xingye Tian", "Guiyu Zhang", "Yabo Chen", "Shaofeng Zhang", "Xuebo Wang", "Xin Tao", "Qi Fan"], "title": "Denoising Vision Transformer Autoencoder with Spectral Self-Regularization", "comment": null, "summary": "Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\\times$256 benchmark.", "AI": {"tldr": "\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5197\u4f59\u9ad8\u9891\u5206\u91cf\u4f1a\u963b\u788d\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u6536\u655b\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8c31\u81ea\u6b63\u5219\u5316\u7b56\u7565\u6765\u6291\u5236\u8fd9\u79cd\u566a\u58f0\uff0c\u5e76\u5f15\u5165\u4e86\u8c31\u5bf9\u9f50\u7b56\u7565\u6765\u4fc3\u8fdbDenoising-VAE\u7684\u4f18\u5316\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5728\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u4f18\u5316\u56f0\u5883\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u6765\u89c4\u8303\u5316\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f46\u5176\u5bf9\u751f\u6210\u6a21\u578b\u4f18\u5316\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8c31\u81ea\u6b63\u5219\u5316\u7b56\u7565\u6765\u6291\u5236\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5197\u4f59\u9ad8\u9891\u5206\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eViT\u7684\u81ea\u7f16\u7801\u5668\uff08Denoising-VAE\uff09\uff0c\u65e0\u9700\u4f9d\u8d56VFMs\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8c31\u5bf9\u9f50\u7b56\u7565\u6765\u4fc3\u8fdbDenoising-VAE\u7684\u4f18\u5316\u3002", "result": "Denoising-VAE\u751f\u6210\u7684\u6f5c\u5728\u7a7a\u95f4\u566a\u58f0\u66f4\u5c11\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u4f18\u5316\u6536\u655b\u901f\u5ea6\u3002\u4e0eSD-VAE\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6269\u6563\u6a21\u578b\u6536\u655b\u901f\u5ea6\u5927\u7ea6\u5feb2\u500d\uff0c\u5e76\u5728ImageNet 256x256\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff08rFID = 0.28, PSNR = 27.26\uff09\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u751f\u6210\u6027\u80fd\uff08gFID = 1.82\uff09\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63ed\u793a\u4e86\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5197\u4f59\u9ad8\u9891\u5206\u91cf\u4f1a\u963b\u788d\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6536\u655b\u548c\u751f\u6210\u8d28\u91cf\u3002\u6240\u63d0\u51fa\u7684\u8c31\u81ea\u6b63\u5219\u5316\u548c\u8c31\u5bf9\u9f50\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u5feb\u901f\u7684\u751f\u6210\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2511.12639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12639", "abs": "https://arxiv.org/abs/2511.12639", "authors": ["Ye Du", "Nanxi Yu", "Shujun Wang"], "title": "Medical Knowledge Intervention Prompt Tuning for Medical Image Classification", "comment": "IEEE Transactions on Medical Imaging (Early Access) July 2025", "summary": "Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.", "AI": {"tldr": "CILMP\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u6539\u8fdb\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u63d0\u793a\u8c03\u4f18\uff0c\u5229\u7528LLM\u7684\u533b\u5b66\u77e5\u8bc6\u751f\u6210\u66f4\u5177\u9002\u5e94\u6027\u7684\u63d0\u793a\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u533a\u5206\u4e0d\u540c\u533b\u5b66\u6982\u5ff5\u548c\u5229\u7528\u7279\u5b9a\u75be\u75c5\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8fd9\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5c06LLM\u7684\u533b\u5b66\u77e5\u8bc6\u878d\u5165\u63d0\u793a\u8c03\u4f18\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCILMP\uff08Conditional Intervention of Large Language Models for Prompt Tuning\uff09\uff0c\u4e00\u79cd\u5c06LLM\u548cVLM\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002CILMP\u4eceLLM\u4e2d\u63d0\u53d6\u75be\u75c5\u7279\u5b9a\u8868\u793a\uff0c\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5e72\u9884\uff0c\u5e76\u751f\u6210\u7279\u5b9a\u4e8e\u75be\u75c5\u7684\u63d0\u793a\u3002\u6b64\u5916\uff0c\u5f15\u5165\u6761\u4ef6\u673a\u5236\uff0c\u6839\u636e\u6bcf\u4e2a\u533b\u5b66\u56fe\u50cf\u8c03\u6574\u63d0\u793a\uff0c\u5b9e\u73b0\u5b9e\u4f8b\u81ea\u9002\u5e94\u3002", "result": "\u5728\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCILMP\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u3002", "conclusion": "CILMP\u6709\u6548\u5730\u5229\u7528LLM\u7684\u533b\u5b66\u77e5\u8bc6\u6765\u589e\u5f3aVLM\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u63d0\u793a\u8c03\u4f18\uff0c\u901a\u8fc7\u751f\u6210\u7279\u5b9a\u4e8e\u75be\u75c5\u548c\u5b9e\u4f8b\u7684\u81ea\u9002\u5e94\u63d0\u793a\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12653", "abs": "https://arxiv.org/abs/2511.12653", "authors": ["Cheng Liao"], "title": "DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry", "comment": null, "summary": "Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.\n  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.", "AI": {"tldr": "DPVO-QAT++\u6846\u67b6\u901a\u8fc7\u5f02\u6784\u7cbe\u5ea6\u548cCUDA\u6838\u51fd\u6570\u878d\u5408\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u5e73\u53f0\u3002", "method": "\u63d0\u51faDPVO-QAT++\u6846\u67b6\uff0c\u91c7\u7528\u53ef\u5b66\u4e60\u5c3a\u5ea6\u53c2\u6570\u5316\u3001VO\u524d\u540e\u7aef\u5f02\u6784\u7cbe\u5ea6\u8bbe\u8ba1\uff08\u524d\u7aef\u4f7f\u7528FP16/FP32\u865a\u91cf\u5316\uff0c\u540e\u7aef\u4f7f\u7528\u5168\u7cbe\u5ea6\uff09\u4ee5\u53ca\u865a\u91cf\u5316\u64cd\u4f5c\u7684GPU\u539f\u751f\u6838\u51fd\u6570\u878d\u5408\uff08\u81ea\u5b9a\u4e49CUDA\u6838\u51fd\u6570\uff09\u3002", "result": "\u5728TartanAir\u6570\u636e\u96c6\u4e0a\uff0cFPS\u5e73\u5747\u63d0\u534752.1%\uff0c\u4e2d\u503c\u5ef6\u8fdf\u964d\u4f4e29.1%\uff0c\u5cf0\u503cGPU\u663e\u5b58\u5360\u7528\u51cf\u5c1164.9%\uff1b\u5728EuRoC\u6570\u636e\u96c6\u4e0a\uff0cFPS\u5e73\u5747\u63d0\u534730.1%\uff0c\u4e2d\u503c\u5ef6\u8fdf\u964d\u4f4e23.1%\uff0c\u5cf0\u503cGPU\u663e\u5b58\u5360\u7528\u51cf\u5c1137.7%\u3002\u4e24\u79cd\u6570\u636e\u96c6\u7684\u8f68\u8ff9\u7cbe\u5ea6\uff08ATE\uff09\u4e0e\u539fDPVO\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "DPVO-QAT++\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e0e\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u8be5\u6280\u672f\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5de5\u7a0b\u8303\u5f0f\u3002"}}
{"id": "2511.12658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12658", "abs": "https://arxiv.org/abs/2511.12658", "authors": ["Zeqin Yu", "Haotao Xie", "Jian Zhang", "Jiangqun Ni", "Wenkan Su", "Jiwu Huang"], "title": "Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis", "comment": "NeurIPS 2025 D&B Track", "summary": "Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \\href{https://github.com/ZeqinYu/FSTS}{Project Page}.", "AI": {"tldr": "FSTS\u662f\u4e00\u4e2a\u7528\u4e8e\u5408\u6210\u7be1\u6539\u6587\u672c\u56fe\u50cf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u7be1\u6539\u75d5\u8ff9\u6765\u751f\u6210\u66f4\u591a\u6837\u5316\u548c\u903c\u771f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u73b0\u6709\u6587\u672c\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\uff08T-IFL\uff09\u65b9\u6cd5\u7531\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4ee5\u53ca\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u7be1\u6539\u7684\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u5206\u5e03\u5dee\u8ddd\uff0c\u901a\u5e38\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "FSTS\u9996\u5148\u6536\u96c6\u4e8616,750\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u7be1\u6539\u5b9e\u4f8b\uff0c\u6db5\u76d6\u4e94\u79cd\u4ee3\u8868\u6027\u7be1\u6539\u7c7b\u578b\uff0c\u5229\u7528\u591a\u683c\u5f0f\u65e5\u5fd7\uff08\u5982\u89c6\u9891\u3001PSD\u548c\u7f16\u8f91\u65e5\u5fd7\uff09\u8bb0\u5f55\u4eba\u5de5\u7f16\u8f91\u75d5\u8ff9\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6536\u96c6\u5230\u7684\u53c2\u6570\u5e76\u8bc6\u522b\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u7684\u91cd\u590d\u884c\u4e3a\u6a21\u5f0f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u5c42\u5efa\u6a21\u6846\u67b6\u3002\u5176\u4e2d\uff0c\u6bcf\u4e2a\u5355\u72ec\u7684\u7be1\u6539\u53c2\u6570\u88ab\u8868\u793a\u4e3a\u57fa\u7840\u64cd\u4f5c-\u53c2\u6570\u914d\u7f6e\u7684\u7d27\u51d1\u7ec4\u5408\uff0c\u800c\u7fa4\u4f53\u5c42\u9762\u7684\u5206\u5e03\u5219\u901a\u8fc7\u805a\u5408\u8fd9\u4e9b\u884c\u4e3a\u6765\u6784\u5efa\u3002\u8be5\u65b9\u6cd5\u4ece\u5085\u91cc\u53f6\u7ea7\u6570\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u901a\u8fc7\u57fa\u51fd\u6570\u53ca\u5176\u5b66\u4e60\u5230\u7684\u6743\u91cd\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8fd1\u4f3c\u3002\u901a\u8fc7\u4ece\u8be5\u5efa\u6a21\u5206\u5e03\u4e2d\u91c7\u6837\uff0cFSTS\u5408\u6210\u4e86\u66f4\u591a\u6837\u5316\u548c\u903c\u771f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u66f4\u597d\u5730\u53cd\u6620\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u7be1\u6539\u75d5\u8ff9\u3002", "result": "\u901a\u8fc7FSTS\u5408\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u9ad8\uff0c\u5728\u56db\u4e2a\u8bc4\u4f30\u534f\u8bae\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "FSTS\u6846\u67b6\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u7be1\u6539\u75d5\u8ff9\u5e76\u4ece\u5efa\u6a21\u7684\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u80fd\u591f\u5408\u6210\u66f4\u591a\u6837\u5316\u548c\u903c\u771f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12662", "abs": "https://arxiv.org/abs/2511.12662", "authors": ["Hongbin Huang", "Junwei Li", "Tianxin Xie", "Zhuang Li", "Cekai Weng", "Yaodong Yang", "Yue Luo", "Li Liu", "Jing Tang", "Zhijing Shao", "Zeyu Wang"], "title": "Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans", "comment": "Proceedings of the Computer Graphics International 2025 (CGI'25)", "summary": "High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u89c6\u89c9\u903c\u771f\u5ea6\u3001\u60c5\u611f\u5316\u8bed\u97f3\u5408\u6210\u548c\u77e5\u8bc6\u9a71\u52a8\u5bf9\u8bdd\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u5bf9\u8bdd\u6570\u5b57\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u6267\u884c\u7ba1\u9053\u548c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u4ea4\u4e92\u3002", "motivation": "\u5b9e\u73b0\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u5b9e\u65f6\u54cd\u5e94\u7684\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u7cfb\u7edf\u3002 ", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u6267\u884c\u7ba1\u9053\u6765\u534f\u8c03\u591a\u6a21\u6001\u7ec4\u4ef6\uff0c\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002\u5e76\u5229\u7528\u4e86\u5305\u62ec\u5386\u53f2\u589e\u5f3a\u548c\u57fa\u4e8e\u610f\u56fe\u7684\u8def\u7531\u5728\u5185\u7684\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u8be5\u7cfb\u7edf\u652f\u6301\u5524\u9192\u8bcd\u68c0\u6d4b\u3001\u60c5\u611f\u5316\u97f5\u5f8b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u54cd\u5e94\u751f\u6210\u7b49\u9ad8\u7ea7\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u4ea4\u4e92\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u54cd\u5e94\u8fc5\u901f\u4e14\u53ef\u4fe1\u7684\u6570\u5b57\u4eba\uff0c\u9002\u7528\u4e8e\u901a\u4fe1\u3001\u6559\u80b2\u548c\u5a31\u4e50\u7b49\u6c89\u6d78\u5f0f\u5e94\u7528\u3002"}}
{"id": "2511.12671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12671", "abs": "https://arxiv.org/abs/2511.12671", "authors": ["Tushar Anand", "Advik Sinha", "Abhijit Das"], "title": "DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality", "comment": null, "summary": "In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u786e\u4e14\u5b9e\u65f6\u7684\u5149\u6d41\u548c\u89c6\u5dee\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u63d0\u51fa\u7684\u975e\u56e0\u679c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u4e2d\u878d\u5408\u914d\u5bf9\u8f93\u5165\u56fe\u50cf\uff0c\u4ee5\u5b9e\u73b0\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u51c6\u786e\u4e14\u5b9e\u65f6\u7684\u5149\u6d41\u548c\u89c6\u5dee\u4f30\u8ba1\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u7684\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u56e0\u679cMamba\u5757\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5feb\u901f\u9ad8\u6548\uff0c\u5e76\u80fd\u7ba1\u7406\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u7ea6\u675f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548c\u4f4eGPU\u5360\u7528\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u80fd\u751f\u6210\u5149\u6d41\u548c\u89c6\u5dee\u56fe\u3002", "conclusion": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u7ed3\u679c\u3001\u5206\u6790\u548c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u7528\u4e8e\u7edf\u4e00\u7684\u5b9e\u65f6\u3001\u51c6\u786e\u76843D\u5bc6\u96c6\u611f\u77e5\u4f30\u8ba1\u4efb\u52a1\u3002"}}
{"id": "2511.12675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12675", "abs": "https://arxiv.org/abs/2511.12675", "authors": ["Saar Stern", "Ido Sobol", "Or Litany"], "title": "Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis", "comment": "3DV 2026. Project page: https://saarst.github.io/appreciate-the-view-website", "summary": "The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\\text{PRISM}}$, and a reference-free score, $\\text{MMD}_{\\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\\text{MMD}_{\\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.", "AI": {"tldr": "\u751f\u6210\u5f0f\u6a21\u578b\uff08\u7279\u522b\u662f\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff09\u5728novel view synthesis (NVS) \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u548c\u5bf9\u6e90\u89c6\u56fe\u53ca\u89c6\u89d2\u53d8\u6362\u7684\u5fe0\u5b9e\u5ea6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u51c6\u786e\u6355\u6349\u8fd9\u79cd\u7ec6\u5fae\u5173\u7cfb\u3002", "motivation": "\u8bc4\u4f30\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u4e2d\u751f\u6210\u56fe\u50cf\u7684\u53ef\u9760\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u51c6\u786e\u6355\u6349\u751f\u6210\u56fe\u50cf\u4e0e\u6e90\u89c6\u56fe\u53ca\u9884\u671f\u89c6\u89d2\u53d8\u6362\u4e4b\u95f4\u7684\u7ec6\u5fae\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5927\u7684NVS\u57fa\u7840\u6a21\u578bZero123\u7684\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5fae\u8c03\u6b65\u9aa4\u6765\u589e\u5f3a\u5224\u522b\u529b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e24\u4e2a\u4e92\u8865\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u4e00\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u5f97\u5206 $D_{\text{PRISM}}$\uff0c\u4e00\u4e2a\u65e0\u53c2\u8003\u7684\u5f97\u5206 $\text{MMD}_{\text{PRISM}}$\u3002", "result": "\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u4e0d\u6b63\u786e\u7684\u751f\u6210\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u6a21\u578b\u6392\u540d\u4e0a\u4e0e\u4eba\u7c7b\u504f\u597d\u7814\u7a76\u4e00\u81f4\u3002\u5728Toys4K\u3001GSO\u548cOmniObject3D\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u53c2\u8003\u6307\u6807 $\text{MMD}_{\text{PRISM}}$ \u5bf9\u516d\u79cdNVS\u65b9\u6cd5\u4ea7\u751f\u4e86\u6e05\u6670\u7a33\u5b9a\u7684\u6392\u540d\uff0c\u5f97\u5206\u8d8a\u4f4e\u8868\u793a\u6a21\u578b\u6027\u80fd\u8d8a\u5f3a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u4e3a\u8bc4\u4f30NVS\u7684\u5408\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8NVS\u9886\u57df\u7684\u53ef\u9760\u8fdb\u5c55\u3002"}}
{"id": "2511.12676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12676", "abs": "https://arxiv.org/abs/2511.12676", "authors": ["Subin Varghese", "Joshua Gao", "Asad Ur Rahman", "Vedhus Hoskere"], "title": "BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections", "comment": null, "summary": "Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.\n  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.\n  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBridgeEQA\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u6865\u6881\u68c0\u67e5\u573a\u666f\u4e2d\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u7684\u5177\u8eab\u95ee\u7b54 (EQA)\u3002\u8be5\u57fa\u51c6\u5305\u542b2200\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u5e76\u5229\u7528\u56fd\u5bb6\u6865\u6881\u68c0\u67e5\u6807\u51c6 (NBI) \u8fdb\u884c\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u7814\u7a76\u8005\u8fd8\u63d0\u51fa\u4e86Embodied Memory Visual Reasoning (EMVR) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u573a\u666f\u56fe\u4e2d\u8fdb\u884c\u5e8f\u5217\u5bfc\u822a\u6765\u89e3\u51b3\u68c0\u67e5\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5728\u7406\u89e3\u5468\u56f4\u73af\u5883\u5e76\u56de\u7b54\u76f8\u5173\u95ee\u9898\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u4e4f\u80fd\u591f\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u64cd\u4f5c\u6761\u4ef6\u7684\u57fa\u51c6\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u4f5c\u4e3a\u5f00\u653e\u8bcd\u6c47\u5177\u8eab\u95ee\u7b54\u7684\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u5e94\u7528\u9886\u57df\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86BridgeEQA\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b2200\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u57fa\u4e8e\u4e13\u4e1a\u68c0\u67e5\u62a5\u544a\uff0c\u8986\u76d6200\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u6865\u6881\u573a\u666f\uff0c\u5e73\u5747\u6bcf\u4e2a\u573a\u666f\u670947.93\u5f20\u56fe\u50cf\u3002\u95ee\u9898\u9700\u8981\u6574\u5408\u6765\u81ea\u591a\u5f20\u56fe\u50cf\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u5e76\u5c06\u7b54\u6848\u4e0eNBI\u6761\u4ef6\u8bc4\u5206\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684EQA\u6307\u6807Image Citation Relevance\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5f15\u7528\u76f8\u5173\u56fe\u50cf\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86Embodied Memory Visual Reasoning (EMVR) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u68c0\u67e5\u8fc7\u7a0b\u6784\u5efa\u4e3a\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u573a\u666f\u56fe\u4e0a\u8fdb\u884c\u5e8f\u5217\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u60c5\u666f\u8bb0\u5fc6EQA\u8bbe\u7f6e\u4e0b\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002EMVR\u6a21\u578b\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u6bd4\u57fa\u7ebf\u6a21\u578b\u66f4\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684BridgeEQA\u57fa\u51c6\u548cEMVR\u6a21\u578b\u4e3a\u5177\u8eab\u95ee\u7b54\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u5c3a\u5ea6\u63a8\u7406\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u8bed\u4e49\u5173\u8054\u7684\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u3002\u7814\u7a76\u8005\u516c\u5f00\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2511.12691", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12691", "abs": "https://arxiv.org/abs/2511.12691", "authors": ["Shuaike Shen", "Ke Liu", "Jiaqing Xie", "Shangde Gao", "Chunhua Shen", "Ge Liu", "Mireia Crispin-Ortuzar", "Shangqi Gao"], "title": "R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection", "comment": null, "summary": "Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.", "AI": {"tldr": "R$^{2}$Seg\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u63a8\u7406-\u62d2\u7edd\u201d\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u5229\u7528LLM\u5f15\u5bfc\u7684\u89c4\u5212\u548c\u7edf\u8ba1\u68c0\u9a8c\u6765\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u80bf\u7624\u5206\u5272\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u79fb\u4f4d\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u51cf\u5c11\u865a\u5047\u9633\u6027\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u5916\uff08OOD\uff09\u79fb\u4f4d\uff0c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\u5728\u5206\u5272OOD\u80bf\u7624\u65f6\u5e38\u51fa\u73b0\u788e\u7247\u5316\u7684\u5047\u9633\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8OOD\u80bf\u7624\u5206\u5272\u7684\u9c81\u68d2\u6027\u3002", "method": "R$^{2}$Seg\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u63a8\u7406\u9636\u6bb5\uff1a\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u89e3\u5256\u63a8\u7406\u89c4\u5212\u5668\u6765\u5b9a\u4f4d\u5668\u5b98\u951a\u70b9\u5e76\u751f\u6210\u591a\u5c3a\u5ea6\u611f\u5174\u8da3\u533a\u57df\uff08ROIs\uff09\u30022. \u62d2\u7edd\u9636\u6bb5\uff1a\u5bf9\u6765\u81ea\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\uff08BiomedParse\uff09\u5728\u8fd9\u4e9bROIs\u4e2d\u751f\u6210\u7684\u5019\u9009\u5206\u5272\u7ed3\u679c\uff0c\u5e94\u7528\u53cc\u6837\u672c\u7edf\u8ba1\u68c0\u9a8c\uff0c\u4ec5\u4fdd\u7559\u4e0e\u6b63\u5e38\u7ec4\u7ec7\u6709\u663e\u8457\u5dee\u5f02\u7684\u5019\u9009\u5206\u5272\uff0c\u4ece\u800c\u6709\u6548\u6291\u5236\u5047\u9633\u6027\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u3001\u591a\u6a21\u6001\u80bf\u7624\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR$^{2}$Seg\u5728Dice\u3001\u7279\u5f02\u6027\u548c\u654f\u611f\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u539f\u59cb\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6846\u67b6\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u517c\u5bb9\u65e0\u9700\u66f4\u65b0\u7684\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\uff0c\u5e76\u907f\u514d\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "R$^{2}$Seg\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u7684\u201c\u63a8\u7406-\u62d2\u7edd\u201d\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728OOD\u573a\u666f\u4e0b\u7684\u5206\u5272\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u6216\u66f4\u65b0\u3002"}}
{"id": "2511.12693", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12693", "abs": "https://arxiv.org/abs/2511.12693", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.\n  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.\n  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .", "AI": {"tldr": "HEDGE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5e7b\u89c9\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u6270\u52a8\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6765\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5f00\u653e\u5f0f\u89c6\u89c9\u95ee\u7b54\u65b9\u9762\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "HEDGE\u6846\u67b6\u6574\u5408\u4e86\u91c7\u6837\u3001\u5931\u771f\u5408\u6210\u3001\u805a\u7c7b\uff08\u57fa\u4e8e\u8574\u542b\u548c\u5d4c\u5165\uff09\u4ee5\u53ca\u5ea6\u91cf\u8ba1\u7b97\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6d41\u6c34\u7ebf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u591a\u6a21\u6001\u67b6\u6784\u3002", "result": "\u5728VQA-RAD\u548cKvasirVQA-x1\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9LLaVA-Med\u3001Med-Gemma\u548cQwen2.5-VL\u4e09\u79cd\u4ee3\u8868\u6027VLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5e7b\u89c9\u7684\u53ef\u68c0\u6d4b\u6027\u4e0e\u6a21\u578b\u67b6\u6784\u548c\u63d0\u793a\u5bc6\u5207\u76f8\u5173\u3002\u7edf\u4e00\u878d\u5408\u6a21\u578b\uff08\u5982Qwen2.5-VL\uff09\u7684\u53ef\u68c0\u6d4b\u6027\u6700\u9ad8\uff0c\u800c\u53d7\u9650\u7684\uff08\u5982Med-Gemma\uff09\u6700\u4f4e\u3002\u5d4c\u5165\u5f0f\u805a\u7c7b\u5728\u76f4\u63a5\u5e94\u7528\u4e8e\u751f\u6210\u7b54\u6848\u65f6\u6548\u679c\u66f4\u597d\uff0c\u800cNLI\u805a\u7c7b\u5219\u5728LLaVA-Med\u548c\u53e5\u5b50\u7ea7\u54cd\u5e94\u4e2d\u66f4\u5177\u4f18\u52bf\u3002VASE\u5ea6\u91cf\u7ed3\u5408\u5d4c\u5165\u5f0f\u805a\u7c7b\u548c\u9002\u5ea6\u7684\u91c7\u6837\u9884\u7b97\uff08n ~ 10-15\uff09\u65f6\uff0c\u63d0\u4f9b\u4e86\u6700\u7a33\u5065\u7684\u5e7b\u89c9\u4fe1\u53f7\u3002\u7b80\u6d01\u7684\u6807\u7b7e\u5f0f\u8f93\u51fa\u6bd4\u5355\u53e5\u54cd\u5e94\u66f4\u6e05\u6670\u3002", "conclusion": "HEDGE\u5c06\u5e7b\u89c9\u68c0\u6d4b\u89c6\u4e3a\u4e00\u4e2a\u53d7\u91c7\u6837\u89c4\u6a21\u3001\u63d0\u793a\u7ed3\u6784\u3001\u6a21\u578b\u67b6\u6784\u548c\u805a\u7c7b\u7b56\u7565\u5171\u540c\u5f71\u54cd\u7684\u51e0\u4f55\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u8ba1\u7b97\u611f\u77e5\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12694", "categories": ["cs.CV", "cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.12694", "abs": "https://arxiv.org/abs/2511.12694", "authors": ["Mohamed A. Mabrok", "Yalda Zafari"], "title": "X-VMamba: Explainable Vision Mamba", "comment": null, "summary": "State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication", "AI": {"tldr": "Mamba\u7b49\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u7a7a\u95f4\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\u4ecd\u4e0d\u900f\u660e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u63a7\u6027\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u8f93\u5165\u5e8f\u5217\u90e8\u5206\u5bf9SSM\u5185\u90e8\u72b6\u6001\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5305\u542b\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u683c\u62c9\u59c6\u77e9\u9635\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u7684\u4f18\u70b9\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSSM\u5728\u533b\u5b66\u5f71\u50cf\u5904\u7406\u4e2d\u5177\u6709\u5c42\u7ea7\u7279\u5f81\u63d0\u70bc\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u5206\u6790\u7ed3\u679c\u4e0e\u8bca\u65ad\u6807\u51c6\u548c\u626b\u63cf\u7b56\u7565\u76f8\u5173\u3002\u8be5\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u9886\u57df\uff0c\u4e3aSSM\u63d0\u4f9b\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u6027\u8303\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\uff08\u5982Mamba\uff09\u5728\u5904\u7406\u7a7a\u95f4\u4fe1\u606f\u65b9\u9762\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u63a7\u6027\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7684SSM\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a1. \u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55SSM\u67b6\u6784\uff0c\u901a\u8fc7\u72b6\u6001\u4f20\u64ad\u94fe\u6d4b\u91cf\u5f71\u54cd\u30022. \u57fa\u4e8e\u683c\u62c9\u59c6\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5bf9\u89d2\u7ebfSSM\uff0c\u901a\u8fc7\u89e3\u6790\u89e3\u5b9e\u73b0\u66f4\u5feb\u7684\u901f\u5ea6\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5747\u53ef\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u8c03\u6574\u8d85\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660eSSM\u80fd\u591f\u81ea\u7136\u5730\u5b9e\u73b0\u4ece\u65e9\u671f\u5c42\u9762\u7684\u5f25\u6563\u4f4e\u7ea7\u7eb9\u7406\u5230\u540e\u671f\u5c42\u9762\u805a\u7126\u7684\u3001\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u6a21\u5f0f\u7684\u5c42\u7ea7\u7279\u5f81\u63d0\u70bc\u3002\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u8bca\u65ad\u6807\u51c6\u4e00\u81f4\u7684\u9886\u57df\u7279\u5b9a\u53ef\u63a7\u6027\u7279\u5f81\u3001\u8de8\u8d8a\u7f51\u7edc\u5c42\u7ea7\u7684\u6e10\u8fdb\u5f0f\u7a7a\u95f4\u9009\u62e9\u6027\uff0c\u4ee5\u53ca\u626b\u63cf\u7b56\u7565\u5bf9\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u63a7\u6027\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u80fd\u591f\u91cf\u5316\u8f93\u5165\u5e8f\u5217\u90e8\u5206\u5bf9SSM\u5185\u90e8\u72b6\u6001\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86SSM\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u9886\u57df\u7684\u5c42\u7ea7\u7279\u5f81\u63d0\u70bc\u80fd\u529b\u3001\u9886\u57df\u7279\u5b9a\u7279\u5f81\u4ee5\u53ca\u5bf9\u626b\u63cf\u7b56\u7565\u7684\u654f\u611f\u6027\u3002\u8be5\u6846\u67b6\u4e3aSSM\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u3001\u57fa\u7840\u6027\u7684\u53ef\u89e3\u91ca\u6027\u8303\u5f0f\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2511.12702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12702", "abs": "https://arxiv.org/abs/2511.12702", "authors": ["Safaeid Hossain Arib", "Rabeya Akter", "Abdul Monaf Chowdhury", "Md Jubair Ahmed Sourov", "Md Mehedi Hasan"], "title": "Counting Through Occlusion: Framework for Open World Amodal Counting", "comment": null, "summary": "Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.", "AI": {"tldr": "CountOCC\u662f\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u8fdb\u884c\u7269\u4f53\u8ba1\u6570\uff0c\u5b83\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u6321\u7269\u4f53\u7684\u7279\u5f81\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7269\u4f53\u8ba1\u6570\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u9aa8\u5e72\u7f51\u7edc\u4f1a\u5c06\u906e\u6321\u7269\u4f53\u7684\u7279\u5f81\u7f16\u7801\u4e3a\u76ee\u6807\u7269\u4f53\uff0c\u4ece\u800c\u7834\u574f\u4e86\u51c6\u786e\u8ba1\u6570\u7684\u7279\u5f81\u8868\u793a\u3002", "method": "CountOCC\u901a\u8fc7\u5206\u5c42\u591a\u6a21\u6001\u5f15\u5bfc\u6765\u663e\u5f0f\u5730\u91cd\u5efa\u88ab\u906e\u6321\u7269\u4f53\u7684\u7279\u5f81\u3002\u5b83\u6574\u5408\u4e86\u53ef\u89c1\u788e\u7247\u3001\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u4ee5\u5408\u6210\u5b8c\u6574\u7684\u3001\u5177\u6709\u7c7b\u533a\u5206\u6027\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u89c6\u89c9\u7b49\u4ef7\u6027\u76ee\u6807\uff0c\u4ee5\u786e\u4fdd\u88ab\u906e\u6321\u548c\u672a\u88ab\u906e\u6321\u7684\u573a\u666f\u7684\u6ce8\u610f\u529b\u56fe\u5728\u7a7a\u95f4\u4e0a\u5bf9\u9f50\u3002", "result": "CountOCC\u5728FSC 147\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u5206\u522b\u964d\u4f4e\u4e8626.72%\u548c20.80%\u3002\u8be5\u6a21\u578b\u8fd8\u5728CARPK\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8649.89%\u7684MAE\u964d\u4f4e\uff0c\u5728CAPTUREReal\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8628.79%\u7684MAE\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u89c6\u89c9\u9886\u57df\u4e0b\u9c81\u68d2\u7684\u975e\u6a21\u578b\u8ba1\u6570\u80fd\u529b\u3002", "conclusion": "CountOCC\u901a\u8fc7\u96c6\u6210\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5f3a\u5236\u5bf9\u9f50\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u906e\u6321\u7269\u4f53\u8ba1\u6570\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12708", "abs": "https://arxiv.org/abs/2511.12708", "authors": ["Kaiser Hamid", "Can Cui", "Khandakar Ashrafi Akbar", "Ziran Wang", "Nade Liang"], "title": "FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling", "comment": null, "summary": "Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.", "AI": {"tldr": "FSDAM\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u5c11\u91cf\uff08\u7ea6100\u4e2a\uff09\u6807\u6ce8\u793a\u4f8b\u5373\u53ef\u5b9e\u73b0\u9a7e\u9a76\u5458\u89c6\u7ebf\u9884\u6d4b\u548c\u610f\u56fe\u89e3\u91ca\u7684\u8054\u5408\u751f\u6210\uff0c\u8fdc\u5c11\u4e8e\u73b0\u6709\u65b9\u6cd5\u6240\u9700\u7684\u6570\u91cf\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u53cc\u901a\u8def\u67b6\u6784\uff0c\u7ed3\u5408\u7a7a\u95f4\u9884\u6d4b\u548c\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002FSDAM\u5728\u89c6\u7ebf\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751f\u6210\u7684\u89e3\u91ca\u8fde\u8d2f\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728\u591a\u4e2a\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7406\u89e3\u9a7e\u9a76\u5458\u7684\u89c6\u7ebf\u548c\u6ce8\u610f\u529b\u8f6c\u79fb\u5bf9\u4e8e\u80fd\u591f\u89e3\u8bfb\u4eba\u7c7b\u610f\u56fe\u5e76\u4e3a\u5176\u884c\u4e3a\u8fa9\u62a4\u7684\u81ea\u4e3b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u5927\u90e8\u5206\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u6ce8\u89c6\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u6536\u96c6\u548c\u6574\u7406\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSDAM\uff08\u5c11\u6837\u672c\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u5efa\u6a21\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u901a\u8def\u67b6\u6784\u3002\u72ec\u7acb\u6a21\u5757\u5206\u522b\u5904\u7406\u7a7a\u95f4\u9884\u6d4b\u548c\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "FSDAM\u5728\u6ce8\u610f\u529b\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u8fde\u8d2f\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u89e3\u91ca\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u76d1\u7763\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u6ce8\u610f\u529b\u6761\u4ef6\u751f\u6210\uff0c\u4e3a\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u90e8\u7f72\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.12735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12735", "abs": "https://arxiv.org/abs/2511.12735", "authors": ["Ankita Raj", "Chetan Arora"], "title": "Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning", "comment": "Accepted to AAAI 2026", "summary": "Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\uff08OVOD\uff09\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrAP\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u53c2\u6570\u4ee5\u53ca\u89c6\u89c9\u89e6\u53d1\u5668\u6765\u6ce8\u5165\u6076\u610f\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b66\u4e60\u7684\u540e\u95e8\u690d\u5165\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740OVOD\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u76d1\u63a7\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u7406\u89e3\u5176\u5b89\u5168\u98ce\u9669\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u7814\u7a76OVOD\u7684\u540e\u95e8\u653b\u51fb\u662f\u5fc5\u8981\u7684\u3002", "method": "\u63d0\u51faTrAP\uff08Trigger-Aware Prompt tuning\uff09\u653b\u51fb\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u8054\u5408\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u63d0\u793a\u53c2\u6570\u4ee5\u53ca\u89c6\u89c9\u89e6\u53d1\u5668\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u4ee4\u724c\u690d\u5165\u6076\u610f\u884c\u4e3a\uff0c\u800c\u4e0d\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6743\u91cd\u3002\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u7f29\u5c0f\u89e6\u53d1\u5668\u5c3a\u5bf8\uff0c\u4ee5\u5b9e\u73b0\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5c0f\u578b\u89e6\u53d1\u5668\u5757\u6709\u6548\u6fc0\u6d3b\u540e\u95e8\u3002", "result": "TrAP\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u76ee\u6807\u8bef\u5206\u7c7b\u548c\u76ee\u6807\u6d88\u5931\u653b\u51fb\u3002\u6b64\u5916\uff0c\u4e0e\u96f6\u6837\u672c\u8bbe\u7f6e\u76f8\u6bd4\uff0cTrAP\u5728\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5e72\u51c0\u56fe\u50cf\u6027\u80fd\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "TrAP\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u540e\u95e8\u6ce8\u5165\u7b56\u7565\uff0c\u80fd\u591f\u9488\u5bf9OVOD\u690d\u5165\u9690\u853d\u7684\u540e\u95e8\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5728\u5e72\u51c0\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12738", "abs": "https://arxiv.org/abs/2511.12738", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Direct Visual Grounding by Directing Attention of Visual Tokens", "comment": null, "summary": "Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5904\u7406\u6d89\u53ca\u56fe\u50cf\u7684\u95ee\u9898\u65f6\uff0c\u5b58\u5728\u89c6\u89c9\u4fe1\u606f\u6ce8\u610f\u529b\u5206\u914d\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684KL\u6563\u5ea6\u6ce8\u610f\u529b\u635f\u5931\uff08KLAL\uff09\uff0c\u4ee5\u66f4\u76f4\u63a5\u5730\u76d1\u7763\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u63d0\u9ad8VLM\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u6700\u7ec8\u5c42\u4e2d\uff0c\u5c3d\u7ba1\u89c6\u89c9\u6807\u8bb0\u4e0e\u67e5\u8be2\u6700\u76f8\u5173\uff0c\u4f46\u5b83\u4eec\u83b7\u5f97\u7684\u6ce8\u610f\u529b\u5374\u5f88\u5c11\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u95ee\u7b54\u9519\u8bef\u3002\u8fd9\u8868\u660e\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff08NTP\uff09\u635f\u5931\u4e0d\u8db3\u4ee5\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u89c6\u89c9\u6807\u8bb0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684KL\u6563\u5ea6\u6ce8\u610f\u529b\u635f\u5931\uff08KLAL\uff09\uff0c\u8be5\u635f\u5931\u76f4\u63a5\u76d1\u7763\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u5e03\u4e0e\u771f\u5b9e\u6ce8\u610f\u529b\u56fe\u8fdb\u884c\u5bf9\u9f50\u6765\u5b9e\u73b0\u3002\u771f\u5b9e\u6ce8\u610f\u529b\u56fe\u53ef\u4ee5\u4ece\u5408\u6210\u6570\u636e\u4e2d\u7684\u4efb\u52a1\u51e0\u4f55\u6216\u771f\u5b9e\u56fe\u50cf\u4e2d\u7684\u6807\u51c6\u6807\u6ce8\uff08\u5982\u8fb9\u754c\u6846\u6216\u70b9\u6807\u6ce8\uff09\u83b7\u5f97\u3002KLAL\u4e0eNTP\u7ed3\u5408\u4f7f\u7528\uff0c\u65e8\u5728\u9f13\u52b1VLM\u5728\u751f\u6210\u7b54\u6848\u6807\u8bb0\u65f6\u5173\u6ce8\u76f8\u5173\u7684\u89c6\u89c9\u6807\u8bb0\u3002", "result": "KLAL\u4e0eNTP\u7ed3\u5408\u4f7f\u7528\uff0c\u5728\u51e0\u4f55\u4efb\u52a1\u3001\u6307\u5411\u4efb\u52a1\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u662f\u5728\u5408\u6210\u6570\u636e\u8fd8\u662f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\uff0c\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30VLM\u7684\u7ebf\u6761\u8ffd\u8e2a\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u662f\u5546\u4e1aVLM\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u4e0d\u4f73\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u76d1\u7763\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0cKLAL\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3VLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u6ce8\u610f\u529b\u5206\u914d\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2511.12740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12740", "abs": "https://arxiv.org/abs/2511.12740", "authors": ["Amirhossein Hassanzadeh", "Bartosz Krawczyk", "Michael Saunders", "Rob Wible", "Keith Krause", "Dimah Dera", "Jan van Aardt"], "title": "Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKPConv\u7684\u591a\u76ee\u6807\u56de\u5f52\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u7ea7\u522b\u4f53\u7d20\u5185\u5bb9\uff08\u5982\u76ee\u6807\u5360\u7528\u767e\u5206\u6bd4\uff09\u6765\u63a8\u65ad\u9ad8\u7ea7\u522b\u4f53\u7d20\u5316LiDAR\u70b9\u4e91\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u4f53\u7d20\u5316\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002\u7814\u7a76\u901a\u8fc7\u6210\u672c\u654f\u611f\u5b66\u4e60\uff08DBR\uff09\u548c\u52a0\u6743MSE\u3001FocalR\u3001\u6b63\u5219\u5316\u7b49\u65b9\u6cd5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e86\u4f53\u7d20\u5c3a\u5bf8\uff080.25-2\u7c73\uff09\u7684\u654f\u611f\u6027\u5206\u6790\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927\u7684\u4f53\u7d20\u5c3a\u5bf8\uff082\u7c73\uff09\u8bef\u5dee\u8f83\u4f4e\uff0c\u800c\u8f83\u5c0f\u7684\u4f53\u7d20\u5c3a\u5bf8\uff080.25\u62160.5\u7c73\uff09\u8bef\u5dee\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u51a0\u5c42\u533a\u57df\u3002\u8fd9\u8868\u660e\u4f53\u7d20\u5c3a\u5bf8\u7684\u9009\u62e9\u53d6\u51b3\u4e8e\u5177\u4f53\u5e94\u7528\u3002\u8be5\u7814\u7a76\u586b\u8865\u4e86\u68ee\u6797LiDAR\u70b9\u4e91\u591a\u76ee\u6807\u56de\u5f52\u548c\u6df1\u5ea6\u4e0d\u5e73\u8861\u5b66\u4e60\u6a21\u578b\u7684\u7a7a\u767d\u3002", "motivation": "\u4f53\u7d20\u5316\u867d\u7136\u80fd\u964d\u4f4eLiDAR\u6570\u636e\u5904\u7406\u6210\u672c\uff0c\u4f46\u4f1a\u635f\u5931\u7cbe\u7ec6\u7684\u7ed3\u6784\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u80fd\u4ece\u9ad8\u7ea7\u522b\u4f53\u7d20\u5316LiDAR\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u4f4e\u7ea7\u522b\u7684\u4f53\u7d20\u5185\u5bb9\u4fe1\u606f\uff08\u5982\u76ee\u6807\u5360\u7528\u767e\u5206\u6bd4\uff09\uff0c\u4ee5\u5f25\u8865\u4fe1\u606f\u635f\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKPConv\u7684\u591a\u76ee\u6807\u56de\u5f52\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6210\u672c\u654f\u611f\u5b66\u4e60\uff08DBR\uff09\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u91c7\u7528\u4e86\u52a0\u6743MSE\u3001Focal Regression\uff08FocalR\uff09\u548c\u6b63\u5219\u5316\u7b49\u6280\u672f\u6765\u4f18\u5316KPConv\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u4f53\u7d20\u5c3a\u5bf8\uff080.25-2\u7c73\uff09\u7684\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u654f\u611f\u6027\u5206\u6790\u8868\u660e\uff0c\u8f83\u5927\u7684\u4f53\u7d20\u5c3a\u5bf8\uff08\u59822\u7c73\uff09\u7531\u4e8e\u53d8\u5f02\u6027\u964d\u4f4e\u800c\u5bfc\u81f4\u8f83\u4f4e\u7684\u8bef\u5dee\uff0c\u800c\u8f83\u5c0f\u7684\u4f53\u7d20\u5c3a\u5bf8\uff08\u59820.25\u62160.5\u7c73\uff09\u5219\u5bfc\u81f4\u8f83\u9ad8\u7684\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u5728\u51a0\u5c42\u533a\u57df\uff0c\u56e0\u4e3a\u8be5\u533a\u57df\u7684\u53d8\u5f02\u6027\u6700\u5927\u3002\u5bf9\u4e8e\u6811\u76ae\u548c\u6811\u53f6\u76ee\u6807\uff0c\u8f83\u5c0f\u4f53\u7d20\u5c3a\u5bf8\u6570\u636e\u96c6\uff080.25\u548c0.5\u7c73\uff09\u7684\u8bef\u5dee\u503c\u660e\u663e\u9ad8\u4e8e\u8f83\u5927\u4f53\u7d20\u5c3a\u5bf8\u6570\u636e\u96c6\uff082\u7c73\uff09\uff0c\u8fd9\u51f8\u663e\u4e86\u5728\u7cbe\u7ec6\u5206\u8fa8\u7387\u4e0b\u51c6\u786e\u4f30\u8ba1\u51a0\u5c42\u5185\u4f53\u7d20\u5185\u5bb9\u7269\u7684\u96be\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f53\u7d20\u5c3a\u5bf8\u7684\u9009\u62e9\u662f\u5e94\u7528\u9a71\u52a8\u7684\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u9700\u6c42\u8fdb\u884c\u6743\u8861\u3002\u8f83\u5c0f\u7684\u4f53\u7d20\u5c3a\u5bf8\u867d\u7136\u80fd\u4fdd\u7559\u66f4\u591a\u7ec6\u8282\uff0c\u4f46\u5728\u5904\u7406\u9ad8\u53d8\u5f02\u6027\u533a\u57df\uff08\u5982\u68ee\u6797\u51a0\u5c42\uff09\u65f6\uff0c\u4f30\u8ba1\u4f53\u7d20\u5185\u5bb9\u7269\u7684\u51c6\u786e\u6027\u4f1a\u53d7\u5230\u5f71\u54cd\u3002\u672c\u7814\u7a76\u4e3a\u68ee\u6797LiDAR\u70b9\u4e91\u7684\u591a\u76ee\u6807\u56de\u5f52\u548c\u6df1\u5ea6\u4e0d\u5e73\u8861\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12744", "abs": "https://arxiv.org/abs/2511.12744", "authors": ["Colton R. Crum", "Adam Czajka"], "title": "SAGE: Saliency-Guided Contrastive Embeddings", "comment": "11 pages, 2 figures, 5 tables", "summary": "Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faSAGE\uff0c\u4e00\u79cd\u5229\u7528\u4eba\u7c7b\u89c6\u89c9\u663e\u8457\u6027\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c06\u4eba\u7c7b\u611f\u77e5\u5148\u9a8c\uff08\u5982\u89c6\u89c9\u663e\u8457\u6027\uff09\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5e38\u4f9d\u8d56\u4e8e\u6a21\u578b\u5185\u90e8\u673a\u5236\uff0c\u800c\u8fd9\u53ef\u80fd\u5e76\u4e0d\u53ef\u9760\u3002\u7136\u800c\uff0c\u5c06\u4eba\u7c7b\u611f\u77e5\u5148\u9a8c\u96c6\u6210\u5230\u6a21\u578b\u8bad\u7ec3\u4e2d\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u6709\u6548\u7684\u6b63\u5219\u5316\u5668\uff0c\u5e76\u4f7f\u6a21\u578b\u4e0e\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4fdd\u6301\u4e00\u81f4\u3002", "method": "SAGE\u901a\u8fc7\u5728\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff08\u800c\u975e\u56fe\u50cf\u7a7a\u95f4\uff09\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\u6765\u6574\u5408\u4eba\u7c7b\u663e\u8457\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u5e94\u7528\u4fdd\u6301\u548c\u964d\u7ea7\u663e\u8457\u6027\u7684\u4fe1\u53f7\u589e\u5f3a\u6280\u672f\u5230\u8f93\u5165\u6570\u636e\u30022. \u6355\u6349\u589e\u5f3a\u5bf9\u6a21\u578b\u5d4c\u5165\u548clogits\u7684\u5f71\u54cd\u30023. \u4f7f\u7528\u5bf9\u6bd4\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u663e\u8457\u7279\u5f81\uff0c\u8fdc\u79bb\u975e\u663e\u8457\u7279\u5f81\u30024. \u901a\u8fc7\u5bf9logits\u5206\u5e03\u8fdb\u884c\u5065\u5168\u6027\u68c0\u67e5\uff0c\u786e\u4fdd\u6a21\u578b\u8f93\u51fa\u4e0e\u57fa\u4e8e\u663e\u8457\u6027\u7684\u589e\u5f3a\u76f8\u5339\u914d\u3002", "result": "SAGE\u5728\u5f00\u653e\u96c6\u548c\u95ed\u96c6\u573a\u666f\u4e0b\u5747\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u663e\u8457\u6027\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86SAGE\u5728\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5177\u6709\u5e7f\u6cdb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SAGE\u901a\u8fc7\u5c06\u4eba\u7c7b\u663e\u8457\u6027\u5f15\u5bfc\u79fb\u81f3\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12757", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12757", "abs": "https://arxiv.org/abs/2511.12757", "authors": ["Nicholas Karris", "Luke Durell", "Javier Flores", "Tegan Emerson"], "title": "Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion", "comment": null, "summary": "It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.", "AI": {"tldr": "Stable Diffusion\u7684CLIP\u5d4c\u5165\u5177\u6709\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u53ef\u5c06\u5d4c\u5165\u89c6\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u800c\u975e\u6b27\u6c0f\u7a7a\u95f4\u4e2d\u7684\u77e9\u9635\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u5c06\u63d0\u793a\u4e4b\u95f4\u7684\u63d2\u503c\u95ee\u9898\u91cd\u65b0\u6784\u5efa\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u8ba1\u7b97\u5d4c\u5165\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\uff08\u6d4b\u5730\u7ebf\uff09\u6765\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u66f4\u8fde\u8d2f\u7684\u4e2d\u95f4\u56fe\u50cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65b9\u6cd5\u4ea7\u751f\u7684\u56fe\u50cf\u63d2\u503c\u6548\u679c\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002", "motivation": "Stable Diffusion\u7684CLIP\u5d4c\u5165\u7684\u6392\u5217\u4e0d\u53d8\u6027\u8fd9\u4e00\u65b0\u9896\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u6765\u7406\u89e3\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u6765\u6539\u8fdb\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u5c06CLIP\u5d4c\u5165\u89c6\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u5e76\u5c06\u63d0\u793a\u4e4b\u95f4\u7684\u63d2\u503c\u95ee\u9898\u91cd\u65b0\u6784\u5efa\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u8ba1\u7b97\u5d4c\u5165\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\uff08\u6d4b\u5730\u7ebf\uff09\u3002", "result": "\u4e0e\u6807\u51c6\u63d2\u503c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u5e73\u6ed1\u3001\u66f4\u8fde\u8d2f\u7684\u4e2d\u95f4\u56fe\u50cf\u3002", "conclusion": "\u5c06CLIP\u5d4c\u5165\u89c6\u4e3a\u70b9\u4e91\u800c\u975e\u77e9\u9635\uff0c\u66f4\u80fd\u4f53\u73b0\u548c\u5229\u7528\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u4e14\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u63d2\u503c\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002"}}
{"id": "2511.12767", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12767", "abs": "https://arxiv.org/abs/2511.12767", "authors": ["C\u0103t\u0103lin-Alexandru R\u00eepanu", "Andrei-Theodor Hotnog", "Giulia-Stefania Imbrea", "Dumitru-Clementin Cercel"], "title": "RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition", "comment": "5 pages, 3 figures, 4 tables", "summary": "Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86 RoCoISLR \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 9000 \u591a\u4e2a\u89c6\u9891\u7684\u7f57\u9a6c\u5c3c\u4e9a\u624b\u8bed\u8bc6\u522b\u8bed\u6599\u5e93\uff0c\u5e76\u5bf9\u4e03\u79cd\u5148\u8fdb\u7684\u89c6\u9891\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0cSwin Transformer \u8fbe\u5230\u4e86 34.1% \u7684\u51c6\u786e\u7387\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u6807\u51c6\u5316\u7684\u7f57\u9a6c\u5c3c\u4e9a\u624b\u8bed\u8bc6\u522b (RoISLR) \u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a RoCoISLR \u7684\u65b0\u8bed\u6599\u5e93\uff0c\u5305\u542b 9000 \u591a\u4e2a\u89c6\u9891\uff0c\u6db5\u76d6\u8fd1 6000 \u4e2a\u6807\u51c6\u5316\u624b\u8bed\u8bcd\u6c47\u3002\u8bc4\u4f30\u4e86 I3D\u3001SlowFast\u3001Swin Transformer\u3001TimeSformer\u3001Uniformer\u3001VideoMAE \u548c PoseConv3D \u4e03\u79cd\u5148\u8fdb\u7684\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u5e76\u5728\u4e00\u81f4\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u4f18\u4e8e\u5377\u79ef\u57fa\u7ebf\uff0c\u5176\u4e2d Swin Transformer \u53d6\u5f97\u4e86 34.1% \u7684 Top-1 \u51c6\u786e\u7387\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u4f4e\u8d44\u6e90\u624b\u8bed\u4e2d\u957f\u5c3e\u7c7b\u5206\u5e03\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "RoCoISLR \u6570\u636e\u96c6\u4e3a RoISLR \u7684\u7cfb\u7edf\u6027\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u7a81\u663e\u4e86\u4f4e\u8d44\u6e90\u624b\u8bed\u8bc6\u522b\u7684\u6311\u6218\u3002"}}
{"id": "2511.12801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12801", "abs": "https://arxiv.org/abs/2511.12801", "authors": ["Andrew Zhou"], "title": "Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation", "comment": null, "summary": "Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u5206\u5272\u8111\u80bf\u7624\u53ca\u5176\u5468\u56f4\u5065\u5eb7\u8111\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u8111\u80bf\u7624\u5206\u5272\u65b9\u6cd5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u8db3\u548c\u65e0\u6cd5\u540c\u65f6\u5206\u5272\u5065\u5eb7\u8111\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bca\u65ad\u3001\u624b\u672f\u89c4\u5212\u548c\u6cbb\u7597\u76d1\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728nnUNet\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u4e2a\u4f53\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u901a\u9053\uff0c\u5e76\u7ed3\u5408\u4e86\u6b63\u5e38\u548c\u764c\u75c7\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u80bf\u7624\u5b9a\u4f4d\u4e0e\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\u7684\u7edf\u4e00\u3002", "result": "\u8be5\u6846\u67b6\u5728BraTS2023\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.750\u7684\u76f8\u5173\u6027\u548c0.047\u7684\u5747\u65b9\u6839\u504f\u5dee\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u3002\u5bf9\u4e8e\u5168\u8111\u5206\u5272\uff0c\u6a21\u578b\u5728\u8111\u7ed3\u6784\u548c\u80bf\u7624\u4e0a\u7684DSC\u5206\u522b\u8fbe\u52300.81\u548c0.86\u3002\u4e0d\u786e\u5b9a\u6027\u56fe\u8c31\u80fd\u591f\u63d0\u4f9b\u5173\u952e\u7684\u6d1e\u5bdf\uff0c\u8bc4\u4f30\u9884\u6d4b\u7ed3\u679c\u5e76\u4fee\u590d\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u8f93\u51fa\u80bf\u7624\u53ca\u5176\u5468\u56f4\u81ea\u7136\u73af\u5883\u7684\u5206\u5272\u56fe\u8c31\uff0c\u5e76\u53e0\u52a0\u4e86\u4e0d\u786e\u5b9a\u6027\u56fe\u8c31\u7684\u6a21\u578b\u3002\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6709\u52a9\u4e8e\u8bc4\u4f30\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5916\u79d1\u624b\u672f\u51b3\u7b56\u63d0\u4f9b\u4fe1\u606f\u652f\u6301\u3002"}}
{"id": "2511.12810", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12810", "abs": "https://arxiv.org/abs/2511.12810", "authors": ["Leena Alghamdi", "Muhammad Usman", "Hafeez Anwar", "Abdul Bais", "Saeed Anwar"], "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \\href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5c3a\u5ea6\u9012\u5f52\u7f51\u7edc\uff08MSRNet\uff09\u6765\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5c0f\u578b\u548c\u591a\u4e2a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u4f4e\u5149\u7167\u3001\u906e\u6321\u3001\u5c0f\u76ee\u6807\u3001\u590d\u6742\u80cc\u666f\u4ee5\u53ca\u591a\u76ee\u6807\u7b49\u6311\u6218\u65f6\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u5c0f\u578b\u548c\u591a\u4e2a\u4f2a\u88c5\u76ee\u6807\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u9012\u5f52\u7f51\u7edc\uff08MSRNet\uff09\u3002\u8be5\u7f51\u7edc\u4f7f\u7528\u91d1\u5b57\u5854\u89c6\u89c9\u53d8\u6362\u5668\uff08PVT\uff09\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c3a\u5ea6\u96c6\u6210\u5355\u5143\uff08ASIU\uff09\u5c06\u8fd9\u4e9b\u7279\u5f81\u9009\u62e9\u6027\u5730\u878d\u5408\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u591a\u7c92\u5ea6\u878d\u5408\u5355\u5143\uff08MGFU\uff09\u548c\u65b0\u9896\u7684\u9012\u5f52\u53cd\u9988\u89e3\u7801\u7b56\u7565\u6765\u9012\u5f52\u5730\u7ec6\u5316\u7279\u5f81\uff0c\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u3002", "result": "\u6240\u63d0\u51fa\u7684MSRNet\u5728\u4e24\u4e2a\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u5728\u53e6\u5916\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u6392\u540d\u7b2c\u4e8c\u3002\u8be5\u6a21\u578b\u6210\u529f\u5730\u68c0\u6d4b\u4e86\u5c0f\u578b\u548c\u591a\u4e2a\u4f2a\u88c5\u76ee\u6807\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u5b66\u4e60\u548c\u9012\u5f52\u7279\u5f81\u4f18\u5316\uff0c\u6240\u63d0\u51fa\u7684MSRNet\u80fd\u591f\u6709\u6548\u514b\u670d\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5c0f\u578b\u548c\u591a\u4e2a\u76ee\u6807\u65b9\u9762\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12834", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12834", "abs": "https://arxiv.org/abs/2511.12834", "authors": ["Rohit Kundu", "Vishal Mohanty", "Hao Xiong", "Shan Jia", "Athula Balachandran", "Amit K. Roy-Chowdhury"], "title": "SAGA: Source Attribution of Generative AI Videos", "comment": null, "summary": "The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.", "AI": {"tldr": "SAGA\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u751f\u6210\u5f0fAI\u89c6\u9891\u7684\u5177\u4f53\u6765\u6e90\uff0c\u5e76\u63d0\u4f9b\u591a\u5c42\u7ea7\u7684\u6eaf\u6e90\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u533a\u5206\u89c6\u9891\u6765\u6e90\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u89c6\u9891\u7684\u6ee5\u7528\u98ce\u9669\u65e5\u76ca\u589e\u52a0\uff0c\u73b0\u6709\u7684\u4e8c\u5143\u771f/\u5047\u68c0\u6d4b\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8d85\u5199\u5b9e\u5408\u6210\u89c6\u9891\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u751f\u6210\u5f0fAI\u89c6\u9891\u8fdb\u884c\u6765\u6e90\u6eaf\u6e90\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGA\u7684\u5168\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u65b0\u9896\u7684\u89c6\u9891Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u65f6\u7a7a\u4f2a\u5f71\u7279\u5f81\u3002\u5e76\u91c7\u7528\u6570\u636e\u9ad8\u6548\u7684\u9884\u8bad\u7ec3-\u6eaf\u6e90\u7b56\u7565\uff0c\u4ee5\u53ca\u65f6\u95f4\u6ce8\u610f\u529b\u7b7e\u540d\uff08T-Sigs\uff09\u65b9\u6cd5\u8fdb\u884c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "SAGA\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u6eaf\u6e90\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u4ec5\u4f7f\u75280.5%\u7684\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u8fbe\u5230\u5b8c\u5168\u76d1\u7763\u7684\u6027\u80fd\u3002T-Sigs\u65b9\u6cd5\u80fd\u591f\u53ef\u89c6\u5316\u4e0d\u540c\u89c6\u9891\u751f\u6210\u5668\u4e4b\u95f4\u7684\u65f6\u57df\u5dee\u5f02\uff0c\u4e3a\u533a\u5206\u5b83\u4eec\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u4f9d\u636e\u3002", "conclusion": "SAGA\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u751f\u6210\u5f0fAI\u89c6\u9891\u6765\u6e90\u6eaf\u6e90\u6846\u67b6\uff0c\u5176\u591a\u5c42\u7ea7\u7684\u6eaf\u6e90\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e3a\u53d6\u8bc1\u548c\u76d1\u7ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u89c1\u89e3\uff0c\u8bbe\u5b9a\u4e86\u5408\u6210\u89c6\u9891\u6eaf\u6e90\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2511.12868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12868", "abs": "https://arxiv.org/abs/2511.12868", "authors": ["Ruiqi Yang", "Tian Yun", "Zihan Wang", "Ellie Pavlick"], "title": "Video Finetuning Improves Reasoning Between Frames", "comment": "Accepted at CogInterp @ NeurIPS 2025", "summary": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.", "AI": {"tldr": "\u89c6\u9891\u5fae\u8c03\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u9690\u5f0f\u5730\u6355\u6349\u5e27\u95f4\u8fc7\u6e21\uff0cvCoT\u80fd\u591f\u63d0\u5347\u7eaf\u56fe\u50cf\u6a21\u578b\u7684\u957f\u89c6\u9891\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u4e14\u89c6\u9891\u6a21\u578b\u80fd\u591f\u5c06\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u9759\u6001\u573a\u666f\u3002", "motivation": "\u63a2\u7a76\u89c6\u9891\u5fae\u8c03\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u663e\u5f0f\u7684\u89c6\u9891\u63a8\u7406\u65b9\u6cd5vCoT\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u94fe\u5f0f\u601d\u8003\uff08vCoT\uff09\uff0c\u751f\u6210\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u4e8b\u4ef6\u63cf\u8ff0\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6bd4\u8f83\u4e86\u7eaf\u56fe\u50cf\u6a21\u578b\u548c\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "vCoT\u663e\u8457\u63d0\u5347\u4e86\u7eaf\u56fe\u50cf\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u800c\u5bf9\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u7684\u63d0\u5347\u6548\u679c\u4e0d\u660e\u663e\u3002\u89c6\u9891\u6a21\u578b\u5c06\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u9759\u6001\u573a\u666f\uff0c\u5728\u5173\u7cfb\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u7eaf\u56fe\u50cf\u6a21\u578b\u3002", "conclusion": "\u89c6\u9891\u5fae\u8c03\u5df2\u7ecf\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9690\u5f0f\u5730\u5b66\u4e60\u5230\u4e86\u5e27\u95f4\u8fc7\u6e21\uff0cvCoT\u5bf9\u4e8e\u63d0\u5347\u7eaf\u56fe\u50cf\u6a21\u578b\u7684\u89c6\u9891\u7406\u89e3\u80fd\u529b\u5c24\u4e3a\u91cd\u8981\uff0c\u5e76\u4e14\u89c6\u9891\u6a21\u578b\u5177\u5907\u8fc1\u79fb\u5b66\u4e60\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12870", "abs": "https://arxiv.org/abs/2511.12870", "authors": ["Trung Thanh Nguyen", "Yasutomo Kawanishi", "Vijay John", "Takahiro Komamizu", "Ichiro Ide"], "title": "View-aware Cross-modal Distillation for Multi-view Action Recognition", "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.", "AI": {"tldr": "\u73b0\u6709\u7684\u591a\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f20\u611f\u5668\u5b8c\u5168\u91cd\u53e0\u7684\u60c5\u51b5\uff0c\u800c\u90e8\u5206\u91cd\u53e0\uff08\u52a8\u4f5c\u53ea\u5728\u90e8\u5206\u89c6\u89d2\u53ef\u89c1\uff09\u7684\u573a\u666f\u7814\u7a76\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u5728\u4f20\u611f\u5668\u90e8\u5206\u91cd\u53e0\u3001\u6a21\u6001\u548c\u6807\u6ce8\u6709\u9650\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u591a\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aViCoKD\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u5229\u7528\u4e00\u4e2a\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\u6765\u6307\u5bfc\u4e00\u4e2a\u6a21\u6001\u548c\u6807\u6ce8\u53d7\u9650\u7684\u5b66\u751f\u6a21\u578b\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u8de8\u6a21\u6001\u9002\u914d\u5668\uff08\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff09\u548c\u4e00\u4e2a\u89c6\u89d2\u611f\u77e5\u4e00\u81f4\u6027\u6a21\u5757\uff08\u89e3\u51b3\u89c6\u89d2\u4e0d\u5339\u914d\u95ee\u9898\uff09\uff0c\u8be5\u6a21\u5757\u5229\u7528\u4e86\u4eba\u4f53\u68c0\u6d4b\u63a9\u7801\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684Jensen-Shannon\u6563\u5ea6\u6765\u5f3a\u5236\u5bf9\u9f50\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684MultiSensor-Home\u6570\u636e\u96c6\u4e0a\uff0cViCoKD\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u548c\u73af\u5883\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u7ade\u4e89\u6027\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u5728\u6709\u9650\u6761\u4ef6\u4e0b\u8d85\u8d8a\u4e86\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "ViCoKD\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f\u77e5\u8bc6\uff0c\u5373\u4f7f\u5728\u6a21\u6001\u548c\u6807\u6ce8\u53d7\u9650\u4e14\u4f20\u611f\u5668\u90e8\u5206\u91cd\u53e0\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5728\u591a\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2511.12880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12880", "abs": "https://arxiv.org/abs/2511.12880", "authors": ["Zihao Lin", "Zhenshan Shi", "Sasa Zhao", "Hanwei Zhu", "Lingyu Zhu", "Baoliang Chen", "Lei Mo"], "title": "Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings", "comment": null, "summary": "Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u521b\u610f\u7ed8\u753b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5185\u5bb9\u548c\u98ce\u683c\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u521b\u610f\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e3b\u89c2\u4e13\u5bb6\u8bc4\u5206\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u5b58\u5728\u4e3b\u89c2\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5185\u5bb9\u7c7b\u522b\u548c\u98ce\u683c\u7279\u5f81\u6765\u9884\u6d4b\u521b\u9020\u529b\u5206\u6570\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u5b66\u4e60\u673a\u5236\u4ee5\u9002\u5e94\u6027\u5730\u63d0\u53d6\u4e0e\u521b\u9020\u529b\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u4f9b\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u3001\u53ef\u89e3\u91ca\u5730\u4ece\u7ed8\u753b\u4e2d\u8bc4\u4f30\u521b\u9020\u529b\uff0c\u5e76\u5c06\u5728\u4ee3\u7801\u5e93\u4e2d\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.12893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12893", "abs": "https://arxiv.org/abs/2511.12893", "authors": ["Kaixin Zhang", "Ruiqing Yang", "Yuan Zhang", "Shan You", "Tao Huang"], "title": "ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation", "comment": null, "summary": "Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\\%$ FLOPs reduction with minimal performance degradation.", "AI": {"tldr": "ActVAR\u662f\u4e00\u79cd\u52a8\u6001\u6fc0\u6d3b\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u6743\u91cd\u548c\u4ee4\u724c\u5e8f\u5217\u4e2d\u5f15\u5165\u53cc\u91cd\u7a00\u758f\u6027\u6765\u63d0\u9ad8\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5bb9\u91cf\u3002\u5b83\u901a\u8fc7\u5c06\u524d\u9988\u7f51\u7edc\u5206\u89e3\u4e3a\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u5b50\u7f51\u7edc\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u8def\u7531\u5668\u6765\u52a8\u6001\u9009\u62e9\u4ee4\u724c\u7279\u5b9a\u7684\u4e13\u5bb6\u5b50\u96c6\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u95e8\u63a7\u4ee4\u724c\u9009\u62e9\u5668\u4f1a\u8bc6\u522b\u9ad8\u66f4\u65b0\u6f5c\u529b\u7684\u4ee4\u724c\u8fdb\u884c\u8ba1\u7b97\uff0c\u540c\u65f6\u91cd\u5efa\u672a\u9009\u4e2d\u7684\u4ee4\u724c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5e8f\u5217\u5bf9\u9f50\u3002\u8be5\u6a21\u578b\u5728ImageNet 256x256\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe21.2%\u7684FLOPs\u7f29\u51cf\uff0c\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u6a21\u578b\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u8ba1\u7b97\u6210\u672c\u4f1a\u6025\u5267\u4e0a\u5347\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u9759\u6001\u526a\u679d\u65b9\u6cd5\u901a\u8fc7\u6c38\u4e45\u79fb\u9664\u6743\u91cd\u6216\u4ee4\u724c\u6765\u635f\u5bb3\u9884\u8bad\u7ec3\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u6027\u80fd\u3002", "method": "ActVAR\u901a\u8fc7\u5206\u89e3\u524d\u9988\u7f51\u7edc\uff08FFNs\uff09\u4e3a\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u5b50\u7f51\u7edc\uff0c\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u8def\u7531\u5668\u6839\u636e\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u4ee4\u724c\u7279\u5b9a\u7684\u4e13\u5bb6\u5b50\u96c6\u3002\u540c\u65f6\uff0c\u95e8\u63a7\u4ee4\u724c\u9009\u62e9\u5668\u8bc6\u522b\u9ad8\u66f4\u65b0\u6f5c\u529b\u7684\u4ee4\u724c\u8fdb\u884c\u8ba1\u7b97\uff0c\u5e76\u91cd\u5efa\u672a\u9009\u4e2d\u7684\u4ee4\u724c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5e8f\u5217\u5bf9\u9f50\u3002\u8bad\u7ec3\u91c7\u7528\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u7531\u539f\u59cbVAR\u6a21\u578b\u76d1\u7763\u8def\u7531\u548c\u95e8\u63a7\u7b56\u7565\u7684\u5b66\u4e60\uff0c\u4ee5\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "result": "\u5728ImageNet 256x256\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cActVAR\u5b9e\u73b0\u4e86\u9ad8\u8fbe21.2%\u7684FLOPs\u7f29\u51cf\uff0c\u540c\u65f6\u6027\u80fd\u4e0b\u964d\u6781\u5c0f\u3002", "conclusion": "ActVAR\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u53cc\u91cd\u7a00\u758f\u6027\uff0c\u5728\u4e0d\u727a\u7272\u6a21\u578b\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6548\u7387\uff0c\u4e3a\u89e3\u51b3VAR\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u4e0d\u65ad\u589e\u957f\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12895", "abs": "https://arxiv.org/abs/2511.12895", "authors": ["Kaixuan Zhang", "Minxian Li", "Mingwu Ren", "Jiankang Deng", "Xiatian Zhu"], "title": "Reconstructing 3D Scenes in Native High Dynamic Range", "comment": null, "summary": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNH-3DGS\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u5355\u4e00\u66dd\u5149\u7684HDR\uff08\u9ad8\u52a8\u6001\u8303\u56f4\uff09\u6570\u636e\u4e2d\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406HDR\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eLDR\uff08\u4f4e\u52a8\u6001\u8303\u56f4\uff09\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u7535\u5f71\u5236\u4f5c\u3001\u865a\u62df\u5236\u7247\u548c\u7167\u7247\u7ea7\u6e32\u67d3\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002\u73b0\u6709\u7684HDR\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u591a\u66dd\u5149\u878d\u5408\u6216\u9006\u8272\u8c03\u6620\u5c04\uff0c\u589e\u52a0\u4e86\u62cd\u6444\u590d\u6742\u6027\u5e76\u4e14\u9700\u8981\u5408\u6210\u76d1\u7763\u3002\u968f\u7740\u80fd\u591f\u76f4\u63a5\u6355\u83b7\u539f\u751fHDR\u6570\u636e\u7684\u76f8\u673a\u7684\u51fa\u73b0\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "NH-3DGS\u65b9\u6cd5\u76f4\u63a5\u5bf9\u539f\u751fHDR\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u5176\u5173\u952e\u6280\u672f\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u989c\u8272\u8868\u793a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u539f\u751fHDR\u76f8\u673a\u6570\u636e\u4e2d\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7684\u591a\u89c6\u56feHDR\u6570\u636e\u96c6\u4e0a\uff0cNH-3DGS\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u52a8\u6001\u8303\u56f4\u4fdd\u7559\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NH-3DGS\u5b9e\u73b0\u4e86\u4ece\u539f\u751fHDR\u6355\u83b7\u4e2d\u76f4\u63a5\u8fdb\u884c\u4e13\u4e1a\u7ea73D\u91cd\u5efa\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u56fe\u50cf\u7684\u52a8\u6001\u8303\u56f4\uff0c\u4e3a\u4e13\u4e1a\u5a92\u4f53\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12899", "abs": "https://arxiv.org/abs/2511.12899", "authors": ["Hao Li", "Zhenfeng Zhuang", "Jingyu Lin", "Yu Liu", "Yifei Chen", "Qiong Peng", "Lequan Yu", "Liansheng Wang"], "title": "FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI", "comment": "Accepted by AAAI2026", "summary": "Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.", "AI": {"tldr": "\u7531\u4e8e\u8111\u90e8MRI\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u89e3\u5256\u7ed3\u6784\u591a\u6837\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff08UAD\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9891\u7387\u57df\u5206\u6790\u6765\u533a\u5206\u6b63\u5e38\u8111\u90e8\u7ed3\u6784\u548c\u75c5\u53d8\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u566a\u58f0\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u6a21\u62df\u7684\u5f02\u5e38\u7f3a\u4e4f\u771f\u5b9e\u4e34\u5e8a\u75c5\u53d8\u7684\u751f\u7269\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u5f62\u6001\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8111\u90e8MRI\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u9891\u7387\u57df\u5206\u6790\uff0c\u53d1\u73b0\u4e86\u5f02\u5e38\u4fe1\u53f7\u7684\u72ec\u7279\u9891\u7387\u6a21\u5f0f\u548c\u6b63\u5e38\u89e3\u5256\u7ed3\u6784\u5728\u4f4e\u9891\u4fe1\u53f7\u7684\u7a33\u5b9a\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u9891\u7387\u5206\u89e3\u9884\u5904\u7406\uff08FDP\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u9891\u7387\u57df\u91cd\u5efa\u6765\u540c\u65f6\u6291\u5236\u75c5\u53d8\u548c\u4fdd\u7559\u89e3\u5256\u7ed3\u6784\u7684\u65b0\u578bUAD\u65b9\u6cd5\u3002", "result": "FDP\u6846\u67b6\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u7684\u5f02\u5e38\u6a21\u62df\u6280\u672f\uff0c\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u6301\u7eed\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u8bca\u65ad\u4fdd\u771f\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFDP\u4e0eLDM\u7ed3\u5408\u4f7f\u7528\u65f6\uff0cDICE\u8bc4\u5206\u63d0\u9ad8\u4e8617.63%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FDP\u6846\u67b6\u901a\u8fc7\u5728\u9891\u7387\u57df\u8fdb\u884c\u5206\u6790\u548c\u91cd\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u8111\u90e8MRI\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.12908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12908", "abs": "https://arxiv.org/abs/2511.12908", "authors": ["Junbo Zou", "Haotian Xia", "Zhen Ye", "Shengjie Zhang", "Christopher Lai", "Vicente Ordonez", "Weining Shen", "Hanjie Chen"], "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning", "comment": null, "summary": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.", "AI": {"tldr": "DeepSport\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u591a\u4efb\u52a1\u3001\u591a\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6765\u89e3\u51b3\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\u7684\u6311\u6218\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f53\u80b2\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4ec5\u5173\u6ce8\u5355\u4e00\u8fd0\u52a8\u3001\u4efb\u52a1\u53d7\u9650\u6216\u7f3a\u4e4f\u6709\u6548\u7684\u5b66\u4e60\u63a8\u7406\u8fc7\u7a0b\u3002DeepSport\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u591a\u4efb\u52a1\u3001\u591a\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DeepSport\u6846\u67b6\u901a\u8fc7\u6570\u636e\u84b8\u998f\u7ba1\u9053\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u8f68\u8ff9\uff0c\u521b\u5efa\u4e86\u5305\u542b78,000\u4e2a\u8bad\u7ec3\u6570\u636e\u7684\u6570\u636e\u96c6\u3002\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u65b0\u9896\u7684\u95e8\u63a7\u5de5\u5177\u4f7f\u7528\u5956\u52b1\u6765\u4f18\u5316\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u3002\u8be5\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4e13\u95e8\u7684\u5e27\u63d0\u53d6\u5de5\u5177\u52a8\u6001\u67e5\u8be2\u5185\u5bb9\uff0c\u5b9e\u73b0\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5728\u5305\u542b6,700\u4e2a\u95ee\u9898\u7684\u6d4b\u8bd5\u57fa\u51c6\u4e0a\uff0cDeepSport\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u7684\u57fa\u7ebf\u3002", "conclusion": "DeepSport\u4e3a\u9886\u57df\u7279\u5b9a\u7684\u89c6\u9891\u63a8\u7406\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u4f53\u80b2\u8fd0\u52a8\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.12909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12909", "abs": "https://arxiv.org/abs/2511.12909", "authors": ["Yaohua Zha", "Xue Yuerong", "Chunlin Fan", "Yuansong Wang", "Tao Dai", "Ke Chen", "Shu-Tao Xia"], "title": "CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection", "comment": "Accepted to AAAI 2026", "summary": "Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.", "AI": {"tldr": "CASL\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u66f2\u7387\u63d0\u793a\u5f15\u5bfcU-Net\u8fdb\u884c\u70b9\u4e91\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u76843D\u5f02\u5e38\u68c0\u6d4b\u548c\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u67093D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u800c\u7ecf\u5178\u7684\u81ea\u76d1\u7763\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u76843D\u6a21\u578b\u3002", "method": "\u63d0\u51faCASL\u6846\u67b6\uff0c\u57fa\u4e8eU-Net\u67b6\u6784\uff0c\u5f15\u5165\u591a\u5c3a\u5ea6\u66f2\u7387\u63d0\u793a\u5f15\u5bfc\u89e3\u7801\u5668\u9884\u6d4b\u70b9\u4e91\u7a7a\u95f4\u5750\u6807\uff0c\u5e76\u4f7f\u7528\u7b80\u5355\u7684\u5f02\u5e38\u5206\u7c7b\u5fae\u8c03\u3002", "result": "CASL\u6846\u67b6\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5b66\u4e60\u5230\u7684\u8868\u5f81\u5728\u70b9\u4e91\u5206\u7c7b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002\u4ec5\u4f7f\u7528\u66f2\u7387\u4f5c\u4e3a\u5f02\u5e38\u5206\u6570\u7684\u65b9\u6cd5\u4e5f\u4f18\u4e8e\u8bb8\u591a\u7ecf\u5178\u6a21\u578b\u3002", "conclusion": "CASL\u6846\u67b6\u5728\u65e0\u9700\u7279\u5b9a\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9886\u5148\u76843D\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002\u66f2\u7387\u57283D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.12917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12917", "abs": "https://arxiv.org/abs/2511.12917", "authors": ["Ruishu Zhu", "Sida Huang", "Ziheng Jiao", "Hongyuan Zhang"], "title": "Explore How to Inject Beneficial Noise in MLLMs", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\\sim2\\%$ additional parameters. The relevant code is uploaded in the supplementary.", "AI": {"tldr": "\u901a\u8fc7\u6ce8\u5165\u6709\u76ca\u7684\u968f\u673a\u566a\u58f0\uff0cMuNG\u5b9e\u73b0\u4e86\u5bf9\u51bb\u7ed3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u9ad8\u6548\u5fae\u8c03\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8de8\u6a21\u6001\u8868\u793a\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u6700\u7ec8\u589e\u5f3a\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u5fae\u8c03\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u8de8\u6a21\u6001\u7684\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u5176\u5168\u90e8\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u6ce8\u5165\u6709\u76ca\u7684\u968f\u673a\u566a\u58f0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u566a\u58f0\u751f\u6210\u5668\uff08MuNG\uff09\uff0c\u8be5\u751f\u6210\u5668\u52a8\u6001\u5206\u6790\u56fe\u50cf-\u6587\u672c\u5bf9\u4e2d\u7684\u8de8\u6a21\u6001\u5173\u7cfb\uff0c\u4ee5\u751f\u6210\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u6709\u76ca\u566a\u58f0\u3002\u5c06\u6b64\u566a\u58f0\u6ce8\u5165MLLMs\uff0c\u4ee5\u6291\u5236\u4e0d\u76f8\u5173\u7684\u8bed\u4e49\u6210\u5206\u3002", "result": "\u5728QwenVL\u548cLLaVA\u4e24\u4e2a\u4e3b\u6d41MLLMs\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0cMuNG\u4ec5\u8c03\u6574\u7ea61-2%\u7684\u9644\u52a0\u53c2\u6570\uff0c\u5c31\u80fd\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u5168\u53c2\u6570\u5fae\u8c03\u548c\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MuNG\u901a\u8fc7\u6ce8\u5165\u5b9a\u5236\u5316\u566a\u58f0\uff0c\u4e3aMLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5fae\u8c03\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8de8\u6a21\u6001\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u53c2\u6570\u9ad8\u6548\u6027\u3002"}}
{"id": "2511.12919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12919", "abs": "https://arxiv.org/abs/2511.12919", "authors": ["Dexin Zuo", "Ang Li", "Wei Wang", "Wenxian Yu", "Danping Zou"], "title": "CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation", "comment": "7 pages, accepted by AAAI 2026 (oral)", "summary": "Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.", "AI": {"tldr": "CoordAR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u672a\u77e5\u7269\u4f53\u8fdb\u884c\u4e00\u6b21\u53c2\u80036D\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u5c063D-3D\u5bf9\u5e94\u5173\u7cfb\u5236\u5b9a\u4e3a\u79bb\u6563\u4ee4\u724c\u56fe\uff0c\u5e76\u91c7\u7528\u6982\u7387\u6027\u81ea\u56de\u5f52\u65b9\u5f0f\u83b7\u5f97\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf93D\u6a21\u578b\u4f9d\u8d56\u548c\u5168\u5c40\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5bf93D\u6a21\u578b\u5728\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u4e2d6D\u59ff\u6001\u4f30\u8ba1\u7684\u4f9d\u8d56\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b0\u9896\u7269\u4f53\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoordAR\u7684\u65b0\u9896\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5c063D-3D\u5bf9\u5e94\u5173\u7cfb\u8868\u793a\u4e3a\u79bb\u6563\u4ee4\u724c\u56fe\uff0c\u5e76\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u73b0\u51c6\u786e\u7684\u5bf9\u5e94\u56de\u5f52\uff1a1) \u65b0\u9896\u7684\u5750\u6807\u56fe\u4ee4\u724c\u5316\uff0c\u5b9e\u73b0\u79bb\u65633D\u7a7a\u95f4\u4e0a\u7684\u6982\u7387\u9884\u6d4b\uff1b2) \u5206\u79bb\u7f16\u7801RGB\u5916\u89c2\u548c\u5750\u6807\u7ebf\u7d22\u7684\u6a21\u6001\u89e3\u8026\u7f16\u7801\u7b56\u7565\uff1b3) \u53d7\u4f4d\u7f6e\u5bf9\u9f50\u67e5\u8be2\u7279\u5f81\u548c\u90e8\u5206\u751f\u6210\u7684\u4ee4\u724c\u5e8f\u5217\u5236\u7ea6\u7684\u81ea\u56de\u5f52Transformer\u89e3\u7801\u5668\u3002", "result": "CoordAR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5bf9\u79f0\u3001\u906e\u6321\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CoordAR\u901a\u8fc7\u5176\u65b0\u9896\u7684\u81ea\u56de\u5f52\u6846\u67b6\u3001\u5750\u6807\u56fe\u4ee4\u724c\u5316\u3001\u6a21\u6001\u89e3\u8026\u7f16\u7801\u548cTransformer\u89e3\u7801\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u53c2\u80036D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12921", "abs": "https://arxiv.org/abs/2511.12921", "authors": ["Huiqiang Sun", "Liao Shen", "Zhan Peng", "Kun Wang", "Size Wu", "Yuhang Zang", "Tianqi Liu", "Zihao Huang", "Xingyu Zeng", "Zhiguo Cao", "Wei Li", "Chen Change Loy"], "title": "Generative Photographic Control for Scene-Consistent Video Cinematic Editing", "comment": null, "summary": "Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.", "AI": {"tldr": "CineCtrl\u662f\u9996\u4e2a\u63d0\u4f9b\u4e13\u4e1a\u76f8\u673a\u53c2\u6570\uff08\u5982\u6563\u666f\u3001\u5feb\u95e8\u901f\u5ea6\uff09\u7cbe\u7ec6\u63a7\u5236\u7684\u89c6\u9891\u7535\u5f71\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u5f71\u54cd\u573a\u666f\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u72ec\u7acb\u63a7\u5236\u6444\u5f71\u6548\u679c\uff0c\u5e76\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u89c6\u9891\u6a21\u578b\u5728\u63a7\u5236\u6444\u5f71\u6548\u679c\uff08\u5982\u666f\u6df1\u3001\u66dd\u5149\uff09\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u591a\u6570\u65b9\u6cd5\u4ec5\u9650\u4e8e\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\uff0c\u5b9e\u73b0\u5bf9\u4e13\u4e1a\u76f8\u673a\u53c2\u6570\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "method": "\u63d0\u51faCineCtrl\u6846\u67b6\uff0c\u91c7\u7528\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5206\u79bb\u76f8\u673a\u8fd0\u52a8\u4e0e\u6444\u5f71\u8f93\u5165\uff0c\u5b9e\u73b0\u72ec\u7acb\u63a7\u5236\uff1b\u540c\u65f6\u5f00\u53d1\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u5229\u7528\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCineCtrl\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\uff0c\u5e76\u7cbe\u786e\u63a7\u5236\u7528\u6237\u6307\u5b9a\u7684\u6444\u5f71\u76f8\u673a\u6548\u679c\u3002", "conclusion": "CineCtrl\u5728\u89c6\u9891\u7535\u5f71\u7f16\u8f91\u9886\u57df\u5b9e\u73b0\u4e86\u5bf9\u4e13\u4e1a\u76f8\u673a\u53c2\u6570\uff08\u5982\u6563\u666f\u3001\u5feb\u95e8\u901f\u5ea6\uff09\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u89c6\u9891\u5185\u5bb9\u3002"}}
{"id": "2511.12932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12932", "abs": "https://arxiv.org/abs/2511.12932", "authors": ["Feng Lv", "Haoxuan Feng", "Zilu Zhang", "Chunlong Xia", "Yanfeng Li"], "title": "Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes", "comment": null, "summary": "With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6587\u672c\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u7f16\u8f91\u4ea4\u901a\u573a\u666f\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u5728\u8bed\u4e49\u4e30\u5bcc\u5ea6\u3001\u89c6\u89d2\u591a\u6837\u6027\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u4e30\u5bcc\u7684\u3001\u53ef\u63a7\u7684\u89c6\u89c9\u573a\u666f\u6570\u636e\u3002\u73b0\u6709\u6587\u672c-\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u6280\u672f\u5728\u751f\u6210\u4ea4\u901a\u5143\u7d20\u7684\u8bed\u4e49\u4e30\u5bcc\u5ea6\u3001\u89c6\u89d2\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u9a71\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u63a7\u63a9\u7801\u673a\u5236\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u3002\u5229\u7528\u8f66\u7aef\u548c\u8def\u4fa7\u591a\u89c6\u89d2\u6570\u636e\u589e\u5f3a\u51e0\u4f55\u591a\u6837\u6027\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528\u5927\u89c4\u6a21\u7c97\u7c92\u5ea6\u6587\u672c-\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u6982\u5ff5\u5b66\u4e60\uff0c\u7136\u540e\u4f7f\u7528\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u6027\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u589e\u5f3a\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8d28\u91cf\u3002\u5f15\u5165\u63a9\u7801\u533a\u57df\u52a0\u6743\u635f\u5931\uff0c\u52a8\u6001\u5f3a\u8c03\u5c0f\u7684\u5173\u952e\u533a\u57df\uff0c\u63d0\u9ad8\u5c0f\u89c4\u6a21\u4ea4\u901a\u5143\u7d20\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ea4\u901a\u573a\u666f\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u53ef\u63a7\u63a9\u7801\u673a\u5236\u3001\u591a\u89c6\u89d2\u6570\u636e\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u63a9\u7801\u533a\u57df\u52a0\u6743\u635f\u5931\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u4ea4\u901a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12938", "abs": "https://arxiv.org/abs/2511.12938", "authors": ["Botong Zhao", "Qijun Shi", "Shujing Lyu", "Yue Lu"], "title": "ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios", "comment": null, "summary": "Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.", "AI": {"tldr": "ProtoAnomalyNCD\u662f\u4e00\u4e2a\u57fa\u4e8e\u539f\u578b\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u5206\u7c7b\u591a\u79cd\u672a\u77e5\u7684\u5de5\u4e1a\u5f02\u5e38\u7c7b\u578b\uff0c\u901a\u8fc7\u5229\u7528Grounded SAM\u5b9a\u4f4d\u5bf9\u8c61\u533a\u57df\u548c\u5f15\u5165\u5f15\u5bfc\u6ce8\u610f\u529b\u7684\u6a21\u5757\u6765\u589e\u5f3a\u5f02\u5e38\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u662f\u5426\u68c0\u6d4b\u5230\u5f02\u5e38\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9700\u8981\u53d1\u73b0\u548c\u5206\u7c7b\u591a\u79cd\u7c7b\u578b\u7684\u5f02\u5e38\u3002\u7531\u4e8e\u5de5\u4e1a\u5f02\u5e38\u8bed\u4e49\u5fae\u5999\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u50cf\u5148\u9a8c\uff0c\u76f4\u63a5\u805a\u7c7b\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528Grounded SAM\u548c\u6587\u672c\u63d0\u793a\u5b9a\u4f4d\u5bf9\u8c61\u533a\u57df\u4f5c\u4e3a\u5f02\u5e38\u5206\u7c7b\u7f51\u7edc\u7684\u5148\u9a8c\u3002\u5f15\u5165\u5f02\u5e38\u56fe\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5176\u4e2d\u533a\u57df\u5f15\u5bfc\u56e0\u5b50\u5e2e\u52a9\u533a\u5206\u80cc\u666f\u3001\u5bf9\u8c61\u533a\u57df\u548c\u5f02\u5e38\u533a\u57df\u3002\u5728\u539f\u578b\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u53d1\u73b0\u548c\u805a\u7c7b\u672a\u77e5\u7684\u5f02\u5e38\u7c7b\u522b\uff0c\u5e76\u5b9e\u73b0\u591a\u7c7b\u578b\u5f02\u5e38\u5206\u7c7b\u548c\u672a\u89c1\u5f02\u5e38\u79bb\u7fa4\u70b9\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MVTec AD\u3001MTD\u548cReal-IAD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "ProtoAnomalyNCD\u6210\u529f\u5730\u53d1\u73b0\u4e86\u672a\u77e5\u7684\u5f02\u5e38\u7c7b\u522b\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u7c7b\u578b\u5f02\u5e38\u7684\u5206\u7c7b\uff0c\u540c\u65f6\u8fd8\u80fd\u68c0\u6d4b\u672a\u77e5\u7684\u79bb\u7fa4\u70b9\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u7ea7\u522b\u7684\u7edf\u4e00\u3002"}}
{"id": "2511.12939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12939", "abs": "https://arxiv.org/abs/2511.12939", "authors": ["Wei Jiang", "Jiahao Cui", "Yizheng Wu", "Zhan Peng", "Zhiyu Pan", "Zhiguo Cao"], "title": "Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking", "comment": "9 pages, 5 figures, accepted to AAAI 2026 (poster)", "summary": "Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u6765\u63d0\u9ad8\u534a\u76d1\u7763HDR\u91cd\u5efa\u7684\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u96be\u4ee5\u83b7\u5f97\u6210\u5bf9\u7684\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u548c\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u56fe\u50cf\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u7528\u6709\u9650\u7684HDR\u771f\u5b9e\u503c\uff08GT\uff09\u6765\u5b9e\u73b0\u53ef\u6bd4\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u4e2d\u6559\u5e08\u6a21\u578b\u4e3a\u6ca1\u6709GT\u7684LDR\u6837\u672c\u751f\u6210\u4f2aHDR GT\uff0c\u7136\u540e\u5b66\u751f\u6a21\u578b\u4ece\u8fd9\u4e9b\u4f2aGT\u4e2d\u5b66\u4e60\u3002\u4e3a\u4e86\u89e3\u51b3\u786e\u8ba4\u504f\u5dee\uff08\u5b66\u751f\u53ef\u80fd\u4ece\u4f2aHDR GT\u7684\u4f2a\u5f71\u4e2d\u5b66\u4e60\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u63a9\u7801\u8fc7\u7a0b\uff0c\u5728\u50cf\u7d20\u548c\u5757\u7ea7\u522b\u4e0a\u4e22\u5f03\u4e0d\u53ef\u9760\u7684\u4f2aGT\u90e8\u5206\uff0c\u4ee5\u4fbf\u5b66\u751f\u53ef\u4ee5\u4ece\u53ef\u4fe1\u533a\u57df\u4e2d\u5b66\u4e60\u3002", "result": "\u4e0e\u4ee5\u524d\u7684\u6807\u6ce8\u6548\u7387\u4f4e\u4e0b\u7684HDR\u91cd\u5efa\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u901a\u8fc7\u4ec5\u4f7f\u75286.7%\u7684HDR GT\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u65b0\u7684\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u534a\u76d1\u7763HDR\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u89e3\u51b3\u4e86\u786e\u8ba4\u504f\u5dee\u95ee\u9898\uff0c\u5728\u6807\u6ce8\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2511.12940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12940", "abs": "https://arxiv.org/abs/2511.12940", "authors": ["Taiye Chen", "Zihan Ding", "Anjian Li", "Christina Zhang", "Zeqi Xiao", "Yisen Wang", "Chi Jin"], "title": "Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention", "comment": null, "summary": "Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAD\u7684\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u548c\u6269\u6563transformer\uff0c\u7528\u4e8e\u751f\u6210\u957f\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u8bb0\u5fc6\u538b\u7f29\u548c\u68c0\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u9057\u5fd8\u548c\u65f6\u7a7a\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u5c40\u90e8\u5168\u6ce8\u610f\u529b\u673a\u5236\uff0c\u96be\u4ee5\u5728\u56fa\u5b9a\u5185\u5b58\u9884\u7b97\u5185\u6709\u6548\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u957f\u89c6\u9891\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c06LSTM\uff08\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff09\u5f15\u5165\u6269\u6563transformer\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aRAD\uff08Recurrent Autoregressive Diffusion\uff09\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u90fd\u5b9e\u73b0\u4e86\u9010\u5e27\u7684\u81ea\u56de\u5f52\u8bb0\u5fc6\u66f4\u65b0\u548c\u68c0\u7d22\uff0c\u4ee5\u89e3\u51b3\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u548c\u7a97\u53e3\u91cd\u53e0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "result": "\u5728Memory Maze\u548cMinecraft\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAD\u5728\u957f\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u8bc1\u660e\u4e86LSTM\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RAD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408RNN\u548c\u6269\u6563transformer\uff0c\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u957f\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86LSTM\u5728\u5e8f\u5217\u5efa\u6a21\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2511.12956", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12956", "abs": "https://arxiv.org/abs/2511.12956", "authors": ["Chen Ma", "Ningfei Wang", "Junhao Zheng", "Qing Guo", "Qian Wang", "Qi Alfred Chen", "Chao Shen"], "title": "T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving", "comment": "16 pages, 12 figures", "summary": "Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.\n  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffSign\u7684\u65b0\u578b\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u5bf9\u6297\u6027\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u4ea4\u901a\u6807\u5fd7\uff08TSR\uff09\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u65e8\u5728\u751f\u6210\u7269\u7406\u9c81\u68d2\u3001\u9ad8\u6548\u3001\u53ef\u8f6c\u79fb\u3001\u5b9e\u7528\u4e14\u9690\u853d\u7684\u5bf9\u6297\u6027\u5916\u89c2\u653b\u51fb\u3002DiffSign\u901a\u8fc7\u96c6\u6210CLIP\u635f\u5931\u548c\u63a9\u7801\u63d0\u793a\u6765\u63d0\u9ad8\u653b\u51fb\u7684\u4e13\u6ce8\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u98ce\u683c\u5b9a\u5236\u65b9\u6cd5\u6765\u6539\u5584\u57df\u5916\u4ea4\u901a\u6807\u5fd7\u7684\u653b\u51fb\u6cdb\u5316\u80fd\u529b\u548c\u9690\u853d\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDiffSign\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e8683.3%\u7684\u5e73\u5747\u7269\u7406\u4e16\u754c\u653b\u51fb\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u65b9\u9762\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\uff08TSR\uff09\u7cfb\u7edf\u6613\u53d7\u7269\u7406\u4e16\u754c\u5bf9\u6297\u6027\u5916\u89c2\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9690\u853d\u6027\u5dee\u3001\u5bf9\u7279\u5b9a\u6a21\u578b\u8fc7\u62df\u5408\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u5177\u9c81\u68d2\u6027\u548c\u9690\u853d\u6027\u7684\u5916\u89c2\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDiffSign\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210CLIP\u635f\u5931\u548c\u63a9\u7801\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u98ce\u683c\u5b9a\u5236\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u9488\u5bf9TSR\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u5916\u89c2\u653b\u51fb\u3002", "result": "DiffSign\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e8683.3%\u7684\u5e73\u5747\u7269\u7406\u4e16\u754c\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u8ddd\u79bb\u3001\u89d2\u5ea6\u3001\u5149\u7167\u6761\u4ef6\u548c\u4ea4\u901a\u6807\u5fd7\u7c7b\u522b\u4e0b\u7684\u6709\u6548\u6027\u3001\u53ef\u8f6c\u79fb\u6027\u548c\u9690\u853d\u6027\u3002", "conclusion": "DiffSign\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684T2I\u5bf9\u6297\u6027\u5916\u89c2\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u6210\u529f\u653b\u51fbTSR\u7cfb\u7edf\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12962", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12962", "abs": "https://arxiv.org/abs/2511.12962", "authors": ["Daniel Cavadia"], "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics", "comment": null, "summary": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.", "AI": {"tldr": "EndoSight AI\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u65f6\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u548c\u5206\u5272\u7ed3\u80a0\u606f\u8089\uff0cmAP\u4e3a88.3%\uff0cDice\u7cfb\u6570\u4e3a69%\uff0c\u63a8\u7406\u901f\u5ea6\u8d85\u8fc735 FPS\u3002", "motivation": "\u5728\u5185\u7aa5\u955c\u68c0\u67e5\u4e2d\u7cbe\u786e\u3001\u5b9e\u65f6\u5730\u68c0\u6d4b\u80c3\u80a0\u606f\u8089\u5bf9\u4e8e\u7ed3\u76f4\u80a0\u764c\u7684\u65e9\u671f\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684Hyper-Kvasir\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08EndoSight AI\uff09\uff0c\u5e76\u91c7\u7528\u4e86\u65b0\u9896\u7684\u70ed\u611f\u77e5\u7a0b\u5e8f\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "result": "\u8be5\u6a21\u578b\u5728\u606f\u8089\u68c0\u6d4b\u65b9\u9762\u8fbe\u5230\u4e8688.3%\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\uff0c\u5728\u5206\u5272\u65b9\u9762\u8fbe\u5230\u4e8669%\u7684Dice\u7cfb\u6570\uff0c\u5728GPU\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u8d85\u8fc7\u4e86\u6bcf\u79d235\u5e27\u3002", "conclusion": "EndoSight AI\u662f\u4e00\u4e2a\u96c6\u6210\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u65e0\u7f1d\u90e8\u7f72\u5230\u5185\u7aa5\u955c\u68c0\u67e5\u6d41\u7a0b\u4e2d\uff0c\u6709\u671b\u63d0\u9ad8\u80c3\u80a0\u9053\u533b\u7597\u4fdd\u5065\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2511.12964", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12964", "abs": "https://arxiv.org/abs/2511.12964", "authors": ["Mehrab Mustafy Rahman", "Jayanth Mohan", "Tiberiu Sosea", "Cornelia Caragea"], "title": "CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models", "comment": null, "summary": "Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.", "AI": {"tldr": "CalibrateMix \u662f\u4e00\u79cd\u57fa\u4e8e mixup \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u201c\u6613\u5b66\u201d\u548c\u201c\u96be\u5b66\u201d\u7684\u6837\u672c\u6765\u63d0\u9ad8\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709 SSL \u65b9\u6cd5\u5728\u6821\u51c6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6a21\u578b\u9884\u6d4b\u8fc7\u4e8e\u81ea\u4fe1\u3002\u867d\u7136 mixup \u5728\u6709\u76d1\u7763\u5b66\u4e60\u4e2d\u80fd\u6539\u5584\u6821\u51c6\uff0c\u4f46\u7531\u4e8e\u4f2a\u6807\u7b7e\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u76f4\u63a5\u5c06\u5176\u5e94\u7528\u4e8e SSL \u5b58\u5728\u6311\u6218\u3002", "method": "CalibrateMix \u5229\u7528\u8bad\u7ec3\u52a8\u6001\u8bc6\u522b\u201c\u6613\u5b66\u201d\u548c\u201c\u96be\u5b66\u201d\u7684\u6837\u672c\uff0c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684 mixup \u6df7\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCalibrateMix \u76f8\u6bd4\u4e8e\u73b0\u6709\u7684 SSL \u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u671f\u671b\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CalibrateMix \u80fd\u591f\u6709\u6548\u63d0\u9ad8 SSL \u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12968", "abs": "https://arxiv.org/abs/2511.12968", "authors": ["Ning Han", "Zhenyu Ge", "Feng Han", "Yuhua Sun", "Chengqing Li", "Jingjing Chen"], "title": "GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models", "comment": "10 pages, 6 figures", "summary": "Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fr\u00e9chet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.", "AI": {"tldr": "GrOCE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u6765\u7cbe\u786e\u3001\u81ea\u9002\u5e94\u5730\u64e6\u9664\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u5fae\u8c03\u6216\u8bed\u4e49\u5206\u79bb\u7c97\u7cd9\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5b58\u5728\u9700\u8981\u6602\u8d35\u5fae\u8c03\u6216\u8bed\u4e49\u5206\u79bb\u7c97\u7cd9\uff0c\u5bfc\u81f4\u64e6\u9664\u4e0d\u7cbe\u786e\u3001\u4e0d\u9002\u5e94\u65b0\u6982\u5ff5\u96c6\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4f18\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGrOCE\u6846\u67b6\uff0c\u6784\u5efa\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\u7684\u52a8\u6001\u8bed\u4e49\u56fe\uff0c\u901a\u8fc7\uff081\uff09\u52a8\u6001\u62d3\u6251\u56fe\u6784\u5efa\u3001\uff082\uff09\u81ea\u9002\u5e94\u805a\u7c7b\u8bc6\u522b\u3001\uff083\uff09\u9009\u62e9\u6027\u8fb9\u5207\u65ad\u6765\u5b9e\u73b0\u7cbe\u786e\u3001\u81ea\u9002\u5e94\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u6982\u5ff5\u76f8\u4f3c\u6027\uff08CS\uff09\u548cFr\u00e9chet Inception Distance\uff08FID\uff09\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GrOCE\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u3001\u7a33\u5b9a\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12969", "abs": "https://arxiv.org/abs/2511.12969", "authors": ["Ziqiao Weng", "Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee AD Cooper", "Weidong Cai", "Bo Zhou"], "title": "HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology", "comment": "Accepted to AAAI 2026. 7 pages (main text), 12 pages total including references and supplementary material. 6 figures", "summary": "Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.", "AI": {"tldr": "HiFusion\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5c42\u7ea7\u5185\u6591\u70b9\u5efa\u6a21\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u8de8\u5c3a\u5ea6\u878d\u5408\uff0c\u4eceH&E\u67d3\u8272\u5207\u7247\u56fe\u50cf\u9884\u6d4b\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728\u4eceH&E\u67d3\u8272\u5207\u7247\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u65f6\uff0c\u96be\u4ee5\u6355\u6349\u6591\u70b9\u5185\u7684\u751f\u7269\u5f02\u8d28\u6027\uff0c\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u5f62\u6001\u5b66\u566a\u58f0\u7684\u5f71\u54cd\u3002HiFusion\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "HiFusion\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a1. \u5c42\u7ea7\u5185\u6591\u70b9\u5efa\u6a21\u6a21\u5757\uff1a\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5b50\u6591\u5757\u5206\u89e3\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5f62\u6001\u5b66\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u7279\u5f81\u5bf9\u9f50\u635f\u5931\u786e\u4fdd\u8de8\u5c3a\u5ea6\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u30022. \u4e0a\u4e0b\u6587\u611f\u77e5\u8de8\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\uff1a\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u9009\u62e9\u6027\u5730\u6574\u5408\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u533a\u57df\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u589e\u5f3a\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0cHiFusion\u57282D\u548c3D\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HiFusion\u662f\u4e00\u4e2a\u5f3a\u5927\u3001\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u4ece\u5e38\u89c4\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u8fdb\u884c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63a8\u65ad\uff0c\u4e3a\u514b\u670d\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6280\u672f\u590d\u6742\u6027\u548c\u6210\u672c\u969c\u788d\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2511.12976", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12976", "abs": "https://arxiv.org/abs/2511.12976", "authors": ["Yoonjae Seo", "Ermal Elbasani", "Jaehong Lee"], "title": "MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning", "comment": "9 pages, 2 figures, 7 tables. Preprint", "summary": "Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.", "AI": {"tldr": "MCAQ-YOLO\u662f\u4e00\u79cd\u5f62\u6001\u590d\u6742\u5ea6\u611f\u77e5\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u6bd4\u7279\u7cbe\u5ea6\u6765\u4f18\u5316\u76ee\u6807\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u7a7a\u95f4\u533a\u57df\u91c7\u7528\u7edf\u4e00\u6bd4\u7279\u7cbe\u5ea6\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u6570\u636e\u590d\u6742\u5ea6\u7684\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u91cf\u5316\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MCAQ-YOLO\u5229\u7528\u5206\u5f62\u7ef4\u5ea6\u3001\u7eb9\u7406\u71b5\u3001\u68af\u5ea6\u65b9\u5dee\u3001\u8fb9\u7f18\u5bc6\u5ea6\u548c\u8f6e\u5ed3\u590d\u6742\u5ea6\u4e94\u79cd\u5f62\u6001\u5b66\u6307\u6807\u6765\u8868\u5f81\u5c40\u90e8\u89c6\u89c9\u5f62\u6001\uff0c\u5e76\u6839\u636e\u91cf\u5316\u654f\u611f\u6027\u52a8\u6001\u8c03\u6574\u6bd4\u7279\u7cbe\u5ea6\u3002\u540c\u65f6\uff0c\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u4ee5\u7a33\u5b9a\u4f18\u5316\u548c\u52a0\u901f\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5f62\u6001\u590d\u6742\u5ea6\u4e0e\u91cf\u5316\u654f\u611f\u6027\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002MCAQ-YOLO\u5728\u5b89\u5168\u8bbe\u5907\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8685.6% mAP@0.5\uff0c\u5e73\u5747\u6bd4\u7279\u6570\u4e3a4.2 bits\uff0c\u538b\u7f29\u6bd4\u4e3a7.6x\uff0c\u5728\u4ec5\u589e\u52a01.8 ms\u8fd0\u884c\u65f6\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0cmAP\u6bd4\u5747\u53004\u6bd4\u7279\u91cf\u5316\u9ad83.5\u4e2a\u767e\u5206\u70b9\u3002\u5728COCO\u548cPascal VOC\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u4e5f\u8bc1\u5b9e\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f62\u6001\u5b66\u9a71\u52a8\u7684\u7a7a\u95f4\u91cf\u5316\u80fd\u591f\u63d0\u9ad8\u8ba1\u7b97\u53d7\u9650\u3001\u5b89\u5168\u5173\u952e\u578b\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12977", "abs": "https://arxiv.org/abs/2511.12977", "authors": ["Yixuan Yang", "Luyang Xie", "Zhen Luo", "Zixiang Zhao", "Mingqi Gao", "Feng Zheng"], "title": "ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes", "comment": null, "summary": "Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.", "AI": {"tldr": "ArtiWorld\u662f\u4e00\u4e2a\u573a\u666f\u611f\u77e5\u7ba1\u9053\uff0c\u53ef\u4ee5\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u8bc6\u522b\u53ef\u5173\u8282\u52a8\u7684\u7269\u4f53\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aURDF\u6a21\u578b\uff0c\u4ee5\u521b\u5efa\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u6a21\u62df\u73af\u5883\u3002", "motivation": "\u624b\u52a8\u5c06\u73b0\u6709\u76843D\u8d44\u4ea7\u8f6c\u6362\u4e3a\u53ef\u52a8\u7684\u6a21\u62df\u5bf9\u8c61\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "ArtiWorld\u5229\u75283D\u70b9\u4e91\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u4ee5\u53ca\u9762\u5411URDF\u7684\u63d0\u793a\u8bbe\u8ba1\uff0c\u5c06\u521a\u6027\u7269\u4f53\u8f6c\u6362\u4e3a\u53ef\u52a8\u7684URDF\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u51763D\u5f62\u72b6\u3002", "result": "\u57283D\u6a21\u62df\u5bf9\u8c61\u30013D\u6a21\u62df\u573a\u666f\u548c\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u573a\u666f\u7684\u8bc4\u4f30\u4e2d\uff0cArtiWorld\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u7559\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u5e76\u6355\u6349\u5176\u4ea4\u4e92\u6027\u3002", "conclusion": "ArtiWorld\u4e3a\u76f4\u63a5\u4ece\u73b0\u6709\u76843D\u8d44\u4ea7\u6784\u5efa\u4ea4\u4e92\u5f0f\u3001\u673a\u5668\u4eba\u5c31\u7eea\u7684\u6a21\u62df\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\u3002"}}
{"id": "2511.12978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12978", "abs": "https://arxiv.org/abs/2511.12978", "authors": ["Aishwarya Agarwal", "Srikrishna Karanam", "Vineet Gandhi"], "title": "Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach", "comment": "25 pages, 21 figures", "summary": "Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Cluster-based Concept Importance (CCI) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u89e3\u51b3\u5b83\u4eec\u5bf9\u80cc\u666f\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u3002CCI \u901a\u8fc7\u5206\u6790 CLIP \u7684\u5185\u90e8\u8868\u5f81\u6765\u8bc6\u522b\u548c\u91cf\u5316\u4e0d\u540c\u7a7a\u95f4\u533a\u57df\uff08patch\uff09\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a COVAR \u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30 VLM \u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bf9\u5341\u516b\u79cd\u4e0d\u540c\u7684 CLIP \u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u867d\u7136\u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u8fc7\u5ea6\u4f9d\u8d56\u80cc\u666f\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": " CCI \u9996\u5148\u5229\u7528 CLIP \u7684 patch \u5d4c\u5165\u5c06\u7a7a\u95f4\u5757\u805a\u7c7b\u6210\u8bed\u4e49\u76f8\u5173\u7684\u5206\u7ec4\uff0c\u7136\u540e\u901a\u8fc7\u906e\u853d\u8fd9\u4e9b\u5206\u7ec4\u5e76\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u53d8\u5316\u6765\u8861\u91cf\u5176\u76f8\u5bf9\u91cd\u8981\u6027\u3002CCI \u4e0e GroundedSAM \u7ed3\u5408\uff0c\u53ef\u4ee5\u81ea\u52a8\u533a\u5206\u9884\u6d4b\u662f\u7531\u524d\u666f\u6216\u80cc\u666f\u9a71\u52a8\u7684\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u63d0\u51fa\u4e86 COVAR \u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u6539\u53d8\u7269\u4f53\u7684\u524d\u666f\u548c\u80cc\u666f\uff0c\u4ee5\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u9519\u8bef\uff08\u5982\u80cc\u666f\u76f8\u5173\u6027\u3001\u89c6\u89d2\u53d8\u5316\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u7ec6\u7c92\u5ea6\u6df7\u6dc6\uff09\u3002", "result": "CCI \u5728\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u5728\u5220\u9664-AUC \u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff08\u5728 MS COCO \u68c0\u7d22\u4efb\u52a1\u4e0a\u63d0\u5347\u8d85\u8fc7\u4e00\u500d\uff09\u3002\u7814\u7a76\u4eba\u5458\u5229\u7528 CCI \u548c COVAR \u5bf9\u5341\u516b\u79cd CLIP \u53d8\u4f53\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u65b9\u6cd5\u5b66\u4e0a\u7684\u8fdb\u6b65\u548c\u5b9e\u8bc1\u8bc1\u636e\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684 VLM \u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "CCI \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u91ca VLM \u7684\u51b3\u7b56\u8fc7\u7a0b\u5e76\u89e3\u51b3\u5176\u5bf9\u80cc\u666f\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u3002COVAR \u57fa\u51c6\u4e3a\u8bc4\u4f30 VLM \u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u7ed3\u5408 CCI \u548c COVAR\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3 VLM \u7684\u884c\u4e3a\uff0c\u5e76\u6307\u5bfc\u672a\u6765 VLM \u7684\u53d1\u5c55\uff0c\u4f7f\u5176\u66f4\u52a0\u9c81\u68d2\u548c\u53ef\u9760\u3002"}}
{"id": "2511.12988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12988", "abs": "https://arxiv.org/abs/2511.12988", "authors": ["Furui Xu", "Shaobo Wang", "Jiajun Zhang", "Chenghao Sun", "Haixiang Tang", "Linfeng Zhang"], "title": "UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective", "comment": "AAAI 2026, 13 pages, 9 figures, 5 tables", "summary": "The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\\%.", "AI": {"tldr": "\u901a\u8fc7\u4ece\u6cdb\u5316\u89d2\u5ea6\u8fdb\u884c\u6570\u636e\u96c6\u526a\u679d\uff0c\u63d0\u51fa UNSEEN \u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u9636\u6bb5\u8bc4\u5206\u5bfc\u81f4\u6837\u672c\u533a\u5206\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6b65\u573a\u666f\u4e0b\u8fdb\u884c\u589e\u91cf\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u526a\u679d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5728\u6a21\u578b\u8bad\u7ec3\u9636\u6bb5\uff08\u62df\u5408\u9636\u6bb5\uff09\u8bc4\u4f30\u6837\u672c\u5f97\u5206\uff0c\u5bfc\u81f4\u6837\u672c\u5f97\u5206\u5206\u5e03\u96c6\u4e2d\uff0c\u533a\u5206\u5ea6\u4f4e\uff0c\u5f71\u54cd\u526a\u679d\u6548\u679c\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ece\u6cdb\u5316\u89d2\u5ea6\uff08\u5728\u672a\u89c1\u8fc7\u7684\u6a21\u578b\u4e0a\u8bc4\u4f30\uff09\u8fdb\u884c\u6570\u636e\u96c6\u526a\u679d\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa UNSEEN \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u5230\u73b0\u6709\u6570\u636e\u96c6\u526a\u679d\u65b9\u6cd5\u4e2d\u3002UNSEEN \u4ece\u6cdb\u5316\u89d2\u5ea6\u8bc4\u4f30\u6837\u672c\uff0c\u5373\u57fa\u4e8e\u672a\u5728\u8bad\u7ec3\u671f\u95f4\u89c1\u8fc7\u8fd9\u4e9b\u6837\u672c\u7684\u6a21\u578b\u6765\u5bf9\u6837\u672c\u8fdb\u884c\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u5c06 UNSEEN \u6269\u5c55\u5230\u591a\u6b65\u573a\u666f\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u6838\u5fc3\u96c6\u4e0a\u8bad\u7ec3\u7684\u8bc4\u5206\u6a21\u578b\u8fdb\u884c\u589e\u91cf\u9009\u62e9\uff0c\u52a8\u6001\u4f18\u5316\u6838\u5fc3\u96c6\u7684\u8d28\u91cf\u3002", "result": "\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet-1K \u6570\u636e\u96c6\u4e0a\uff0cUNSEEN \u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684 SOTA \u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728 ImageNet-1K \u6570\u636e\u96c6\u4e0a\uff0cUNSEEN \u5728\u6570\u636e\u91cf\u51cf\u5c11 30% \u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u65e0\u635f\u6027\u80fd\u3002", "conclusion": "UNSEEN \u6846\u67b6\u901a\u8fc7\u4ece\u6cdb\u5316\u89d2\u5ea6\u8fdb\u884c\u6570\u636e\u96c6\u526a\u679d\uff0c\u5e76\u7ed3\u5408\u591a\u6b65\u589e\u91cf\u9009\u62e9\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u96c6\u526a\u679d\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u5c24\u5176\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2511.12992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12992", "abs": "https://arxiv.org/abs/2511.12992", "authors": ["Lintong Zhang", "Kang Yin", "Seong-Whan Lee"], "title": "Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection", "comment": "31page, 7 figures", "summary": "In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWSAE-Net\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u751f\u6210\u5f0f\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u8bed\u4e49\u76f8\u5173\u6027\u4e0d\u8db3\u548c\u7f16\u8f91\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5728\u66ff\u6362\u56fe\u50cf\u533a\u57df\u65f6\uff0c\u5ffd\u7565\u4e86\u66ff\u6362\u533a\u57df\u4e0e\u76ee\u6807\u5bf9\u8c61\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u7f16\u8f91\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "WSAE-Net\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u521b\u65b0\uff1a1. \u52a0\u6743\u8bed\u4e49\u56fe\uff1a\u6700\u5927\u5316\u51cf\u5c11\u9700\u8981\u8ba1\u7b97\u7684\u975e\u8bed\u4e49\u7279\u5f81\u5355\u5143\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u30022. \u81ea\u9002\u5e94\u5019\u9009\u7f16\u8f91\u5e8f\u5217\uff1a\u786e\u5b9a\u7279\u5f81\u5355\u5143\u5904\u7406\u7684\u6700\u4f73\u8ba1\u7b97\u987a\u5e8f\uff0c\u4fdd\u8bc1\u5728\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027\u7684\u540c\u65f6\uff0c\u9ad8\u6548\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cWSAE-Net\u5728\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "WSAE-Net\u901a\u8fc7\u5f15\u5165\u52a0\u6743\u8bed\u4e49\u56fe\u548c\u81ea\u9002\u5e94\u5019\u9009\u7f16\u8f91\u5e8f\u5217\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u6548\u7387\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12998", "abs": "https://arxiv.org/abs/2511.12998", "authors": ["Zewei Chang", "Zheng-Peng Duan", "Jianxing Zhang", "Chun-Le Guo", "Siyu Liu", "Hyungju Chun", "Hyunhee Park", "Zikun Liu", "Chongyi Li"], "title": "PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching", "comment": "To appear at AAAI 2026", "summary": "Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.", "AI": {"tldr": "PerTouch\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u5b83\u652f\u6301\u8bed\u4e49\u7ea7\u522b\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u7f8e\u611f\uff0c\u5e76\u80fd\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u4e2d\u53ef\u63a7\u6027\u548c\u4e3b\u89c2\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\uff0c\u5e76\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u7684\u7f8e\u5b66\u504f\u597d\u3002", "method": "PerTouch\u4f7f\u7528\u5305\u542b\u7279\u5b9a\u8bed\u4e49\u533a\u57df\u5c5e\u6027\u503c\u7684\u53c2\u6570\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u663e\u5f0f\u7684\u53c2\u6570\u5230\u56fe\u50cf\u7684\u6620\u5c04\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u56fe\u50cf\u4fee\u590d\u3002\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u66ff\u6362\u548c\u53c2\u6570\u6270\u52a8\u673a\u5236\u6765\u63d0\u9ad8\u8bed\u4e49\u8fb9\u754c\u611f\u77e5\u80fd\u529b\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u80fd\u591f\u5904\u7406\u5f3a\u5f31\u7528\u6237\u6307\u4ee4\uff0c\u5e76\u7ed3\u5408\u53cd\u9988\u9a71\u52a8\u7684\u518d\u601d\u8003\u548c\u573a\u666f\u611f\u77e5\u8bb0\u5fc6\u673a\u5236\uff0c\u4ee5\u66f4\u597d\u5730\u4e0e\u7528\u6237\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u5e76\u6355\u6349\u957f\u671f\u504f\u597d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u4ee5\u53caPerTouch\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u4fee\u590d\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PerTouch\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u4e49\u7ea7\u522b\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u5e76\u80fd\u6839\u636e\u7528\u6237\u7684\u4e2a\u6027\u5316\u7f8e\u5b66\u504f\u597d\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u8c03\u6574\u3002"}}
{"id": "2511.13001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13001", "abs": "https://arxiv.org/abs/2511.13001", "authors": ["Pengcheng Shi", "Jiawei Chen", "Jiaqi Liu", "Xinglin Zhang", "Tao Chen", "Lei Li"], "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation", "comment": "Accepted by CVPR 2025 Workshop MedSegFM", "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.", "AI": {"tldr": "Medal S\u662f\u4e00\u4e2a\u533b\u5b66\u5206\u5272\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u539f\u751f\u5206\u8fa8\u7387\u7a7a\u95f4\u548c\u6587\u672c\u63d0\u793a\uff0c\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u89e3\u51b3\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u7c7b\u522b\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65f6\uff0c\u6587\u672c\u63d0\u793a\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5bb9\u6613\u56e0\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u5bfc\u81f4\u4e0d\u51c6\u786e\u3002Medal S\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u6587\u672c\u63d0\u793a\uff0c\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "Medal S\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u6280\u672f\u6574\u5408\u4e86\u4f53\u79ef\u63d0\u793a\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4fdd\u7559\u4e86\u5b8c\u6574\u76843D\u4e0a\u4e0b\u6587\uff0c\u80fd\u591f\u5e76\u884c\u5904\u7406\u591a\u4e2a\u539f\u751f\u5206\u8fa8\u7387\u63a9\u7801\u3002\u5b83\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea73D\u5377\u79ef\u6a21\u5757\uff0c\u7528\u4e8e\u57fa\u4e8e\u4e24\u79cd\u63d0\u793a\u7c7b\u578b\u8fdb\u884c\u7cbe\u786e\u7684\u4f53\u7d20\u7a7a\u95f4\u7ec6\u5316\u3002\u6b64\u5916\uff0cMedal S\u63d0\u51fa\u4e86\u4e24\u79cd\u63d0\u793a\u6a21\u5f0f\uff08\u7eaf\u6587\u672c\u6a21\u5f0f\u548c\u6df7\u5408\u6a21\u5f0f\uff09\u3001\u52a8\u6001\u91cd\u91c7\u6837\u3001\u4f18\u5316\u7684\u6587\u672c\u9884\u5904\u7406\u3001\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565\u548c\u540e\u5904\u7406\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "Medal S\u5728BiomedSegFM\u6570\u636e\u96c6\u4e0a\uff0c\u652f\u6301CT\u3001MRI\u3001PET\u3001\u8d85\u58f0\u548c\u663e\u5fae\u955c\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u53ef\u5904\u7406\u591a\u8fbe243\u4e2a\u7c7b\u522b\u3002\u572824\u7c7b\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5e76\u884c\u7a7a\u95f4\u63d0\u793a\u7684\u63a8\u7406\u65f6\u95f4\u6bd4\u987a\u5e8f\u63d0\u793a\u51cf\u5c11\u4e8690%\u4ee5\u4e0a\u3002\u4e0eSAT\u76f8\u6bd4\uff0cMedal S\u5728\u9a8c\u8bc1\u96c6\u7684\u4e94\u6a21\u6001\u5e73\u5747\u8bc4\u4f30\u4e2d\uff0cDSC\u63d0\u9ad8\u4e866.61%\uff0cNSD\u63d0\u9ad8\u4e866.28%\uff0cF1\u63d0\u9ad8\u4e8613.36%\uff0cDSC TP\u63d0\u9ad8\u4e8618.49%\u3002", "conclusion": "Medal S\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u7cbe\u5ea6\u548c\u8bed\u4e49\u6587\u672c\u5f15\u5bfc\uff0c\u5728\u591a\u7c7b\u522b\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u4e8e\u987a\u5e8f\u63d0\u793a\u65b9\u6cd5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.13002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13002", "abs": "https://arxiv.org/abs/2511.13002", "authors": ["Jihun Park", "Kyoungmin Lee", "Jongmin Gim", "Hyeonseo Jo", "Minseok Oh", "Wonhyeok Choi", "Kyumin Hwang", "Jaeyeul Kim", "Minwoo Choi", "Sunghoon Im"], "title": "Infinite-Story: A Training-Free Consistent Text-to-Image Generation", "comment": "18pages, 13 figures, AAAI 2026 Oral", "summary": "We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.", "AI": {"tldr": "Infinite-Story\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u63d0\u793a\u6545\u4e8b\u573a\u666f\u4e2d\u8fdb\u884c\u4e00\u81f4\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002\u5b83\u901a\u8fc7\u8eab\u4efd\u63d0\u793a\u66ff\u6362\u548c\u7edf\u4e00\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\uff08\u5305\u62ec\u81ea\u9002\u5e94\u98ce\u683c\u6ce8\u5165\u548c\u540c\u6b65\u5f15\u5bfc\u9002\u5e94\uff09\u6765\u89e3\u51b3\u8eab\u4efd\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8eab\u4efd\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6a21\u578b\u5feb6\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u591a\u63d0\u793a\u6545\u4e8b\u573a\u666f\u65f6\uff0c\u5b58\u5728\u8eab\u4efd\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u4e00\u4e9b\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u6216\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "Infinite-Story\u91c7\u7528\u57fa\u4e8e\u5c3a\u5ea6\u81ea\u52a8\u56de\u5f52\u7684\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e09\u79cd\u6280\u672f\uff1a1. \u8eab\u4efd\u63d0\u793a\u66ff\u6362\uff1a\u51cf\u5c11\u6587\u672c\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u504f\u5dee\uff0c\u5bf9\u9f50\u8eab\u4efd\u5c5e\u6027\u30022. \u81ea\u9002\u5e94\u98ce\u683c\u6ce8\u5165\uff1a\u5f3a\u5236\u6267\u884c\u5168\u5c40\u98ce\u683c\u4e00\u81f4\u6027\u30023. \u540c\u6b65\u5f15\u5bfc\u9002\u5e94\uff1a\u5f3a\u5236\u6267\u884c\u8eab\u4efd\u5916\u89c2\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u4fdd\u771f\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8eab\u4efd\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5feb\u7684\u6a21\u578b\u5feb6\u500d\u4ee5\u4e0a\uff08\u6bcf\u5f20\u56fe\u50cf1.72\u79d2\uff09\u3002", "conclusion": "Infinite-Story\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u5b9e\u7528\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u591a\u63d0\u793a\u6545\u4e8b\u573a\u666f\u751f\u6210\u5177\u6709\u9ad8\u8eab\u4efd\u548c\u98ce\u683c\u4e00\u81f4\u6027\u7684\u56fe\u50cf\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u5feb\u3002"}}
{"id": "2511.13005", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13005", "abs": "https://arxiv.org/abs/2511.13005", "authors": ["Wenqian Ye", "Di Wang", "Guangtao Zheng", "Bohan Liu", "Aidong Zhang"], "title": "SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias", "comment": "Accepted at AAAI 2026", "summary": "Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.", "AI": {"tldr": "CLIP\u7b49\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u56fe\u50cf\u548c\u6587\u672c\uff0c\u5728\u96f6\u6b21\u5b66\u4e60\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u7136\u800c\uff0cCLIP\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u591a\u6a21\u6001\u7684\u865a\u5047\u504f\u5dee\uff0c\u5373\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u865a\u5047\u7279\u5f81\u3002\u4f8b\u5982\uff0cCLIP\u53ef\u80fd\u6839\u636e\u9891\u7e41\u51fa\u73b0\u7684\u80cc\u666f\u800c\u4e0d\u662f\u5bf9\u8c61\u7684\u6838\u5fc3\u7279\u5f81\u6765\u63a8\u65ad\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u7c7b\u578b\u3002\u8fd9\u79cd\u504f\u5dee\u4f1a\u4e25\u91cd\u5f71\u54cd\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u4e3a\u5728\u8fd9\u79cd\u6570\u636e\u4e0a\uff0c\u8de8\u6a21\u6001\u5173\u8054\u4e0d\u518d\u6210\u7acb\u3002\u73b0\u6709\u7684\u51cf\u8f7b\u591a\u6a21\u6001\u865a\u5047\u504f\u5dee\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9\u4e0b\u6e38\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u6216\u9884\u5148\u4e86\u89e3\u504f\u5dee\uff0c\u8fd9\u4f1a\u5f71\u54cdCLIP\u7684\u5373\u7528\u6027\u3002\u672c\u6587\u9996\u5148\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u591a\u6a21\u6001\u865a\u5047\u504f\u5dee\u5bf9\u96f6\u6b21\u5b66\u4e60\u5206\u7c7b\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u2014\u2014\u865a\u5047\u504f\u5dee\u611f\u77e5\u5f15\u5bfc\u63a2\u7d22\uff08SAGE\uff09\uff0c\u901a\u8fc7\u5f15\u5bfc\u5f0f\u63d0\u793a\u8bcd\u9009\u62e9\u6765\u51cf\u8f7b\u865a\u5047\u504f\u5dee\u3002SAGE\u65e0\u9700\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u5916\u90e8\u6ce8\u91ca\u3002\u5b83\u63a2\u7d22\u63d0\u793a\u8bcd\u6a21\u677f\u7a7a\u95f4\uff0c\u5e76\u9009\u62e9\u80fd\u5f15\u8d77\u7c7b\u522b\u4e4b\u95f4\u6700\u5927\u8bed\u4e49\u5206\u79bb\u7684\u63d0\u793a\u8bcd\uff0c\u4ece\u800c\u63d0\u9ad8\u6700\u5dee\u5206\u7ec4\u7684\u9c81\u68d2\u6027\u3002\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u6d41\u884c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u5728\u96f6\u6b21\u5b66\u4e60\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u96f6\u6b21\u5b66\u4e60\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u5916\u90e8\u77e5\u8bc6\u6216\u6a21\u578b\u66f4\u65b0\u3002", "motivation": "CLIP\u7b49\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6b21\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u591a\u6a21\u6001\u865a\u5047\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u5373\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u4e8e\u4e0e\u7c7b\u522b\u4e0d\u76f8\u5173\u7684\u7279\u5f81\uff08\u5982\u80cc\u666f\uff09\u8fdb\u884c\u5224\u65ad\uff0c\u8fd9\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u6216\u5916\u90e8\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u865a\u5047\u504f\u5dee\u611f\u77e5\u5f15\u5bfc\u63a2\u7d22\uff08SAGE\uff09\u201d\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a2\u7d22\u63d0\u793a\u8bcd\u6a21\u677f\u7a7a\u95f4\uff0c\u5e76\u9009\u62e9\u80fd\u591f\u6700\u5927\u5316\u7c7b\u522b\u95f4\u8bed\u4e49\u5206\u79bb\u7684\u63d0\u793a\u8bcd\uff0c\u4ece\u800c\u5728\u65e0\u9700\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u5916\u90e8\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\u51cf\u8f7b\u591a\u6a21\u6001\u865a\u5047\u504f\u5dee\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u6d41\u884c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u63d0\u5347\u6a21\u578b\u7684\u96f6\u6b21\u5b66\u4e60\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u4e0d\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6216\u6a21\u578b\u66f4\u65b0\u7684\u96f6\u6b21\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SAGE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u5916\u90e8\u77e5\u8bc6\u5373\u53ef\u6709\u6548\u51cf\u8f7bCLIP\u6a21\u578b\u591a\u6a21\u6001\u865a\u5047\u504f\u5dee\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5f0f\u63d0\u793a\u8bcd\u9009\u62e9\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13011", "abs": "https://arxiv.org/abs/2511.13011", "authors": ["Qingsen Ma", "Chen Zou", "Dianyun Wang", "Jia Wang", "Liuyu Xiang", "Zhaofeng He"], "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis", "comment": null, "summary": "Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.", "AI": {"tldr": "DTGS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53d7Retinex\u542f\u53d1\u7684\u7167\u660e\u5206\u89e3\u548c\u70ed\u5f15\u5bfc\u76843D\u9ad8\u65af\u6cfc\u6e85\uff0c\u7528\u4e8e\u5728\u6781\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e0d\u53d8\u7684\u51e0\u4f55\u91cd\u5efa\u3002", "motivation": "\u5728\u6781\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u5728\u51e0\u4f55\u3001\u989c\u8272\u4e00\u81f4\u6027\u548c\u8f90\u5c04\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u9000\u5316\u3002\u6807\u51c63D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u76f4\u63a5\u5e94\u7528\u4e8e\u66dd\u5149\u4e0d\u8db3\u7684\u8f93\u5165\u65f6\u4f1a\u5931\u8d25\uff0c\u56e0\u4e3a\u8de8\u89c6\u56fe\u7684\u72ec\u7acb\u589e\u5f3a\u4f1a\u5bfc\u81f4\u7167\u660e\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u5931\u771f\u3002", "method": "DTGS\u901a\u8fc7\u5faa\u73af\u589e\u5f3a-\u91cd\u5efa\u673a\u5236\uff0c\u5bf9\u589e\u5f3a\u3001\u51e0\u4f55\u548c\u70ed\u91cf\u76d1\u7763\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u4e00\u4e2a\u70ed\u91cf\u76d1\u7763\u5206\u652f\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u589e\u5f3a\u3001\u7ed3\u6784\u548c\u70ed\u91cf\u635f\u5931\u6765\u7a33\u5b9a\u989c\u8272\u6062\u590d\u548c\u51e0\u4f55\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u5d4c\u51653DGS\u5faa\u73af\u4e2d\u7684\u57fa\u4e8eRetinex\u7684\u5206\u89e3\u6a21\u5757\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u53cd\u5c04\u7387-\u7167\u660e\u5206\u79bb\uff0c\u786e\u4fdd\u4e86\u8de8\u89c6\u70b9\u7684\u989c\u8272\u548c\u7eb9\u7406\u4e00\u81f4\u6027\u3002", "result": "DTGS\u5728RGBT-LOW\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u6355\u83b7\u4e86\u4e25\u91cd\u7684\u7167\u660e\u9000\u5316\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDTGS\u5728\u8f90\u5c04\u4e00\u81f4\u6027\u3001\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u989c\u8272\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4f4e\u5149\u589e\u5f3a\u548c3D\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "DTGS\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7167\u660e\u5206\u89e3\u3001\u51e0\u4f55\u91cd\u5efa\u548c\u70ed\u91cf\u76d1\u7763\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6781\u4f4e\u5149\u7167\u6761\u4ef6\u4e0bNVS\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4e0d\u53d8\u7684\u51e0\u4f55\u91cd\u5efa\u3002"}}
{"id": "2511.13013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13013", "abs": "https://arxiv.org/abs/2511.13013", "authors": ["Guoyi Zhang", "Guangsheng Xu", "Siyang Chen", "Han Wang", "Xiaohu Zhang"], "title": "You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection", "comment": null, "summary": "Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.", "AI": {"tldr": "BP-FPN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u91d1\u5b57\u5854\u67b6\u6784\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4f18\u5316\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u5e27\u7279\u5f81\u8868\u793a\u4e0a\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u867d\u7136\u5173\u6ce8\u65f6\u7a7a\u7279\u5f81\u805a\u5408\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u6839\u672c\u74f6\u9888\u5728\u4e8e\u6a21\u7cca\u7684\u5355\u5e27\u7279\u5f81\u8868\u793a\uff0c\u800c\u975e\u65f6\u7a7a\u5efa\u6a21\u672c\u8eab\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBP-FPN\u7684\u53cd\u5411\u4f20\u64ad\u9a71\u52a8\u7279\u5f81\u91d1\u5b57\u5854\u67b6\u6784\uff0c\u5f15\u5165\u4e86\u68af\u5ea6\u9694\u79bb\u4f4e\u5c42\u5feb\u6377\u8fde\u63a5\uff08GILS\uff09\u6765\u6709\u6548\u878d\u5408\u76ee\u6807\u7ec6\u8282\uff0c\u5e76\u4f7f\u7528\u65b9\u5411\u68af\u5ea6\u6b63\u5219\u5316\uff08DGR\uff09\u6765\u5f3a\u5236\u6267\u884c\u5c42\u7ea7\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBP-FPN\u80fd\u591f\u6301\u7eed\u53d6\u5f97\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "BP-FPN\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u4ece\u53cd\u5411\u4f20\u64ad\u89d2\u5ea6\u4e3a\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u8bbe\u8ba1\u7684\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08FPN\uff09\uff0c\u5b83\u901a\u8fc7\u65b0\u9896\u7684\u673a\u5236\u89e3\u51b3\u4e86\u5355\u5e27\u7279\u5f81\u8868\u793a\u7684\u6839\u672c\u95ee\u9898\uff0c\u5e76\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.13015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13015", "abs": "https://arxiv.org/abs/2511.13015", "authors": ["King-Man Tam", "Satoshi Ikehata", "Yuta Asano", "Zhaoyi An", "Rei Kawakami"], "title": "Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.", "AI": {"tldr": "GeoUniPS\u901a\u8fc7\u6574\u5408\u5408\u6210\u76d1\u7763\u548c\u9884\u8bad\u7ec3\u76843D\u91cd\u5efa\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u5bf9\u591a\u5149\u7167\u7ebf\u7d22\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5e76\u5728PS-Perp\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u6cd5\u5728\u65e0\u4e25\u683c\u5149\u7167\u5047\u8bbe\u4e0b\u6062\u590d\u8868\u9762\u6cd5\u7ebf\u65b9\u9762\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5728\u590d\u6742\u91ce\u5916\u573a\u666f\u7684\u591a\u5149\u7167\u7ebf\u7d22\u4e0d\u53ef\u9760\u65f6\uff08\u5982\u5149\u7167\u504f\u5dee\u3001\u9634\u5f71\u6216\u81ea\u906e\u6321\u533a\u57df\uff09\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGeoUniPS\u7f51\u7edc\uff0c\u6574\u5408\u5408\u6210\u76d1\u7763\u548c\u6765\u81ea\u5927\u89c4\u6a213D\u91cd\u5efa\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u9ad8\u5c42\u51e0\u4f55\u5148\u9a8c\u3002\u8bbe\u8ba1\u4e86\u5149-\u51e0\u4f55\u53cc\u5206\u652f\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u591a\u5149\u7167\u7ebf\u7d22\u548c\u51e0\u4f55\u5148\u9a8c\u3002\u5f15\u5165PS-Perp\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u903c\u771f\u7684\u900f\u89c6\u6295\u5f71\u66ff\u4ee3\u4f20\u7edf\u7684\u6b63\u4ea4\u6295\u5f71\u5047\u8bbe\u3002", "result": "GeoUniPS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u7ed3\u679c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u7684\u91ce\u5916\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "GeoUniPS\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec33D\u91cd\u5efa\u6a21\u578b\u7684\u51e0\u4f55\u77e5\u8bc6\u548c\u5f15\u5165\u900f\u89c6\u6295\u5f71\u6570\u636e\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.13019", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13019", "abs": "https://arxiv.org/abs/2511.13019", "authors": ["Zheyuan Hu", "Chieh-Hsin Lai", "Ge Wu", "Yuki Mitsufuji", "Stefano Ermon"], "title": "MeanFlow Transformers with Representation Autoencoders", "comment": "Code is available at https://github.com/sony/mf-rae", "summary": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u548c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8eMeanFlow\uff08MF\uff09\u6269\u6563\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "MeanFlow\uff08MF\uff09\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u9700\u8981\u590d\u6742\u7684\u5f15\u5bfc\u8d85\u53c2\u6570\u7b49\u95ee\u9898\u3002\u7279\u522b\u662f\u5728\u5229\u7528\u9884\u8bad\u7ec3\u7684Stable Diffusion\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08SD-VAE\uff09\u65f6\uff0cSD-VAE\u89e3\u7801\u5668\u662f\u751f\u6210\u6210\u672c\u7684\u4e3b\u8981\u90e8\u5206\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684MF\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u548c\u91c7\u6837MF\u7684\u65b0\u65b9\u6cd5\u3002RAE\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982DINO\uff09\u63d0\u4f9b\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u3002\u4e3a\u4e86\u89e3\u51b3MF\u5728RAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u65f6\u68af\u5ea6\u7206\u70b8\u7684\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u4ee5\u4e0b\u7b56\u7565\uff1a1. \u4f7f\u7528\u4e00\u81f4\u6027\u4e2d\u671f\u8bad\u7ec3\uff08Consistency Mid-Training\uff09\u8fdb\u884c\u8f68\u8ff9\u611f\u77e5\u521d\u59cb\u5316\u30022. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\uff0c\u901a\u8fc7\u84b8\u998f\u9884\u8bad\u7ec3\u7684\u6d41\u5339\u914d\u6559\u5e08\u6a21\u578b\u6765\u52a0\u901f\u6536\u655b\u5e76\u964d\u4f4e\u65b9\u5dee\uff1b\u7136\u540e\uff0c\u53ef\u9009\u5730\u8fdb\u884c\u81ea\u4e3e\u9636\u6bb5\uff0c\u4f7f\u7528\u5355\u70b9\u901f\u5ea6\u4f30\u8ba1\u5668\u6765\u8fdb\u4e00\u6b65\u51cf\u5c0f\u4e0e\u771f\u5b9eMF\u7684\u504f\u5dee\u3002\u8fd9\u79cd\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u5f15\u5bfc\u7684\u9700\u6c42\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u914d\u7f6e\uff0c\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u91c7\u6837\u7684\u8ba1\u7b97\u91cf\u3002", "result": "\u5728ImageNet 256\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e861\u6b65FID\u4e3a2.03\uff0c\u4f18\u4e8e\u539f\u59cbMF\u76843.43\uff1b\u540c\u65f6\u91c7\u6837GFLOPS\u964d\u4f4e\u4e8638%\uff0c\u603b\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u4e8683%\u3002\u5728ImageNet 512\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e861\u6b65FID\u4e3a3.23\uff0c\u4e14GFLOPS\u6700\u4f4e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5728RAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u548c\u91c7\u6837MF\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86MF\u7684\u4f7f\u7528\uff0c\u4f7f\u5176\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u66f4\u52a0\u9ad8\u6548\u53ef\u884c\u3002"}}
{"id": "2511.13020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13020", "abs": "https://arxiv.org/abs/2511.13020", "authors": ["Yufei Wen", "Yuting Zhang", "Jingdan Kang", "Hao Ren", "Weibin Cheng", "Jintai Chen", "Kaishun Wu"], "title": "SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.", "AI": {"tldr": "\u5149\u8c31\u9002\u5e94\uff1a\u4e00\u79cd\u534a\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u4eceRGB\u56fe\u50cf\u91cd\u5efa\u9ad8\u5149\u8c31\u533b\u5b66\u56fe\u50cf\uff0c\u901a\u8fc7\u5149\u8c31\u5bc6\u5ea6\u63a9\u853d\u548c\u5149\u8c31\u7aef\u5143\u8868\u793a\u5bf9\u9f50\u6765\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u57df\u8f6c\u79fb\u95ee\u9898\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u4e14\u6280\u672f\u8981\u6c42\u9ad8\u3002\u4eceRGB\u7b49\u6613\u83b7\u53d6\u7684\u6a21\u6001\u4e2d\u6062\u590dHSI\u6570\u636e\uff08\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\uff09\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u901a\u7528\u9886\u57df\u7684\u6570\u636e\u96c6\u5145\u8db3\uff0c\u4f46\u4eba\u7c7bHSI\u6570\u636e\u7684\u7a00\u7f3a\u9650\u5236\u4e86\u5176\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectralAdapt\u7684\u534a\u76d1\u7763\u57df\u9002\u5e94\uff08SSDA\uff09\u6846\u67b6\uff0c\u4ee5\u5f25\u5408\u901a\u7528\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u6709\u9650\u7684\u6807\u7b7e\u548c\u5927\u91cf\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u901a\u8fc7\u5f15\u5165\u5149\u8c31\u5bc6\u5ea6\u63a9\u853d\uff08SDM\uff09\u6765\u589e\u5f3a\u5149\u8c31\u63a8\u7406\uff0c\u8be5\u63a9\u853d\u6839\u636eRGB\u901a\u9053\u7684\u5149\u8c31\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5730\u63a9\u853d\u5b83\u4eec\uff0c\u9f13\u52b1\u5728\u4e00\u81f4\u6027\u8bad\u7ec3\u671f\u95f4\u4ece\u4e92\u8865\u7ebf\u7d22\u4e2d\u6062\u590d\u4fe1\u606f\u533a\u57df\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u5149\u8c31\u7aef\u5143\u8868\u793a\u5bf9\u9f50\uff08SERA\uff09\uff0c\u5b83\u4ece\u6709\u4ef7\u503c\u7684\u6807\u8bb0\u50cf\u7d20\u4e2d\u63d0\u53d6\u7269\u7406\u4e0a\u53ef\u89e3\u91ca\u7684\u7aef\u5143\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u4f5c\u4e3a\u57df\u4e0d\u53d8\u951a\u70b9\u6765\u6307\u5bfc\u672a\u6807\u8bb0\u7684\u9884\u6d4b\uff0c\u5e76\u8fdb\u884c\u52a8\u91cf\u66f4\u65b0\u4ee5\u786e\u4fdd\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpectralAdapt\u5728\u5149\u8c31\u4fdd\u771f\u5ea6\u3001\u8de8\u57df\u6cdb\u5316\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86SSDA\u4f5c\u4e3a\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "SpectralAdapt\u6846\u67b6\u901a\u8fc7SDM\u548cSERA\u6709\u6548\u7f13\u89e3\u4e86HSI\u91cd\u5efa\u4e2d\u7684\u57df\u8f6c\u79fb\u3001\u5149\u8c31\u9000\u5316\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u9886\u57df\u5dee\u8ddd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2511.13026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13026", "abs": "https://arxiv.org/abs/2511.13026", "authors": ["Jiaze Li", "Hao Yin", "Wenhui Tan", "Jingyang Chen", "Boshen Xu", "Yuxun Qu", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan"], "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding", "comment": null, "summary": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.", "AI": {"tldr": "\u7eaf\u6587\u672c\u53cd\u601d\u673a\u5236\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u5c40\u9650\uff0cREVISOR\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u7684\u80fd\u529b\uff0c\u5e76\u4f7f\u7528DADR\u673a\u5236\u4f18\u5316\u5956\u52b1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u7eaf\u6587\u672c\u53cd\u601d\u673a\u5236\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u4e3a\u957f\u89c6\u9891\u5305\u542b\u52a8\u6001\u89c6\u89c9\u4fe1\u606f\u4e14\u6587\u672c\u53cd\u601d\u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u65e0\u6cd5\u5145\u5206\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faREVISOR\uff08REflective VIsual Segment Oriented Reasoning\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u6a21\u6001\u53cd\u601d\u6846\u67b6\uff0c\u4f7fMLLM\u80fd\u591f\u8de8\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u8fdb\u884c\u5185\u7701\u5f0f\u53cd\u601d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53cc\u91cd\u5f52\u56e0\u89e3\u8026\u5956\u52b1\uff08DADR\uff09\u673a\u5236\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u786e\u4fdd\u6a21\u578b\u51c6\u786e\u5ba1\u67e5\u4e0e\u95ee\u9898\u9ad8\u5ea6\u76f8\u5173\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230GRPO\u8bad\u7ec3\u7b56\u7565\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u56e0\u679c\u5bf9\u9f50\u3002", "result": "REVISOR\u6846\u67b6\u5728\u6ca1\u6709\u989d\u5916\u76d1\u7763\u5fae\u8c03\u6216\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u589e\u5f3a\u4e86MLLM\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5728VideoMME\u3001LongVideoBench\u3001MLVU\u548cLVBench\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "conclusion": "REVISOR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u53cd\u601d\uff0c\u5e76\u5229\u7528DADR\u673a\u5236\u4f18\u5316\u5956\u52b1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7eaf\u6587\u672c\u53cd\u601d\u673a\u5236\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4e0d\u8db3\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347MLLM\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13031", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13031", "abs": "https://arxiv.org/abs/2511.13031", "authors": ["Weihua Wang", "Yubo Cui", "Xiangru Lin", "Zhiheng Li", "Zheng Fang"], "title": "Towards 3D Object-Centric Feature Learning for Semantic Scene Completion", "comment": "Accept by AAAI-2026", "summary": "Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.", "AI": {"tldr": "Ocean\u662f\u4e00\u4e2a\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u76843D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u3001\u7269\u4f53\u4e2d\u5fc3\u7279\u5f81\u805a\u5408\u548c\u5c40\u90e8\u6269\u6563\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u573a\u666f\u4e0b\u8bed\u4e49\u548c\u51e0\u4f55\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9e1f\u77b0\u56fe\u76843D\u8bed\u4e49\u573a\u666f\u8865\u5168\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6574\u4e2a\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u7269\u4f53\u7ea7\u522b\u7684\u7ec6\u8282\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u8bed\u4e49\u548c\u51e0\u4f55\u6a21\u7cca\u6027\u3002", "method": "1. \u4f7f\u7528MobileSAM\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u63d0\u53d6\u7269\u4f53\u5b9e\u4f8b\u3002 2. \u5f15\u51653D\u8bed\u4e49\u7ec4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u805a\u5408\u7269\u4f53\u4e2d\u5fc3\u7279\u5f81\u3002 3. \u8bbe\u8ba1\u5168\u5c40\u76f8\u4f3c\u6027\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5904\u7406\u5206\u5272\u9519\u8bef\u548c\u7f3a\u5931\u7684\u5b9e\u4f8b\u3002 4. \u63d0\u51fa\u5b9e\u4f8b\u611f\u77e5\u5c40\u90e8\u6269\u6563\u6a21\u5757\uff0c\u901a\u8fc7\u751f\u6210\u8fc7\u7a0b\u6539\u8fdb\u7269\u4f53\u7279\u5f81\uff0c\u5e76\u5728BEV\u7a7a\u95f4\u4e2d\u7ec6\u5316\u573a\u666f\u8868\u793a\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOcean\u5206\u522b\u53d6\u5f97\u4e8617.40\u548c20.28\u7684mIoU\u5206\u6570\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Ocean\u901a\u8fc7\u5206\u89e3\u573a\u666f\u4e3a\u5355\u72ec\u7684\u7269\u4f53\u5b9e\u4f8b\uff0c\u5e76\u5229\u7528\u7269\u4f53\u4e2d\u5fc3\u7279\u5f81\u805a\u5408\u548c\u5c40\u90e8\u6269\u6563\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e863D\u8bed\u4e49\u573a\u666f\u8865\u5168\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.13032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13032", "abs": "https://arxiv.org/abs/2511.13032", "authors": ["Sheng Liu", "Yuanzhi Liang", "Jiepeng Wang", "Sidan Du", "Chi Zhang", "Xuelong Li"], "title": "Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts", "comment": null, "summary": "We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.", "AI": {"tldr": "Uni-Inter\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5355\u4e00\u3001\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u67b6\u6784\u4e2d\u751f\u6210\u652f\u6301\u5e7f\u6cdb\u4ea4\u4e92\u573a\u666f\uff08\u5305\u62ec\u4eba\u4e0e\u4eba\u3001\u4eba\u4e0e\u7269\u3001\u4eba\u4e0e\u573a\u666f\uff09\u7684\u4eba\u7c7b\u8fd0\u52a8\u3002\u5b83\u5f15\u5165\u4e86\u7edf\u4e00\u4ea4\u4e92\u4f53\u79ef\uff08UIV\uff09\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5c06\u5f02\u6784\u4ea4\u4e92\u5b9e\u4f53\u7f16\u7801\u5230\u5171\u4eab\u7a7a\u95f4\u573a\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u81f4\u7684\u5173\u7cfb\u63a8\u7406\u548c\u590d\u5408\u4ea4\u4e92\u5efa\u6a21\u3002\u8fd0\u52a8\u751f\u6210\u88ab\u8868\u8ff0\u4e3a\u5728UIV\u4e0a\u7684\u8054\u5408\u5f0f\u6982\u7387\u9884\u6d4b\uff0c\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u5e76\u4ea7\u751f\u8fde\u8d2f\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u884c\u4e3a\u3002\u5b9e\u9a8c\u8868\u660e\uff0cUni-Inter\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u7684\u4ea4\u4e92\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u9896\u7684\u5b9e\u4f53\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u8bbe\u8ba1\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cUni-Inter\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5e7f\u6cdb\u7684\u4ea4\u4e92\u573a\u666f\u3002", "method": "Uni-Inter\u5f15\u5165\u4e86\u7edf\u4e00\u4ea4\u4e92\u4f53\u79ef\uff08UIV\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4f53\u79ef\u8868\u793a\uff0c\u5c06\u5f02\u6784\u4ea4\u4e92\u5b9e\u4f53\u7f16\u7801\u5230\u5171\u4eab\u7a7a\u95f4\u573a\u4e2d\u3002\u8fd0\u52a8\u751f\u6210\u88ab\u8868\u8ff0\u4e3a\u5728UIV\u4e0a\u7684\u8054\u5408\u5f0f\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUni-Inter\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u7684\u4ea4\u4e92\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u9896\u7684\u5b9e\u4f53\u7ec4\u5408\u3002", "conclusion": "\u7edf\u4e00\u5efa\u6a21\u590d\u5408\u4ea4\u4e92\u4e3a\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u9014\u7684\u65b9\u5411\u3002"}}
{"id": "2511.13036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13036", "abs": "https://arxiv.org/abs/2511.13036", "authors": ["Dahyun Chung", "Donghyun Shin", "Yujin Sung", "Seunggi Moon", "Jinwoo Jeon", "Byung-Jun Lee"], "title": "uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data", "comment": "Our project page can be found at https://dinyudin203.github.io/uCLIP-project/", "summary": "Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.", "AI": {"tldr": "CLIP\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u6570\u636e\u7684\u7a00\u7f3a\uff0c\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u53d7\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u6295\u5f71\u6a21\u5757\uff0c\u5229\u7528\u82f1\u8bed\u8868\u5f81\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e94\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u5ea6\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6377\u514b\u8bed\u3001\u82ac\u5170\u8bed\u3001\u514b\u7f57\u5730\u4e9a\u8bed\u3001\u5308\u7259\u5229\u8bed\u548c\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u6570\u636e\u3002\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u56fe\u50cf-\u6587\u672c\u5bf9\u6216\u6587\u672c-\u6587\u672c\u5bf9\uff0c\u5e76\u4e14\u51bb\u7ed3\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u548c\u591a\u8bed\u8a00\u6587\u672c\u7f16\u7801\u5668\u7684\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51bb\u7ed3\u4e86\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u591a\u8bed\u8a00\u6587\u672c\u7f16\u7801\u5668\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u53c2\u6570\u91cf\u4e3a1.7M\u7684\u6295\u5f71\u6a21\u5757\u3002\u8be5\u6a21\u5757\u5229\u7528\u5bf9\u6bd4\u635f\u5931\u548c\u82f1\u8bed\u8868\u5f81\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\uff0c\u4ee5\u5b9e\u73b0\u591a\u8bed\u8a00\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u68c0\u7d22\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u67a2\u8f74\u7684\u3001\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u9f50\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u573a\u666f\u4e0b\uff0c\u4e3a\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13039", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13039", "abs": "https://arxiv.org/abs/2511.13039", "authors": ["Zhenying Fang", "Richang Hong"], "title": "MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization", "comment": "12 pages, 3 figures", "summary": "Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.13054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13054", "abs": "https://arxiv.org/abs/2511.13054", "authors": ["Bo Fang", "Yuxin Song", "Qiangqiang Wu", "Haoyuan Sun", "Wenhao Wu", "Antoni B. Chan"], "title": "ViSS-R1: Self-Supervised Reinforcement Video Reasoning", "comment": "Our paper was initially titled \"Video-SSR1: Self-Supervised Reinforcement Video Reasoning.\" Upon noticing its close resemblance to the title of a recently released paper, we have decided to rename our work as \"ViSS-R1.\"", "summary": "Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPretext-GRPO\u7684\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u548c ViSS-R1\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u5229\u7528\u7387\uff0c\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eR1\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5bb9\u6613\u5bfc\u81f4\u6377\u5f84\u5b66\u4e60\u548c\u5e7b\u89c9\u3002", "method": "\u5f15\u5165Pretext-GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u89c6\u9891\u8fdb\u884c\u53d8\u6362\u5e76\u8bbe\u8ba1\u76f8\u5e94\u7684\u201c\u501f\u53e3\u4efb\u52a1\u201d\uff08pretext tasks\uff09\u6765\u5956\u52b1\u6a21\u578b\uff0c\u4fc3\u4f7f\u6a21\u578b\u6df1\u5165\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faViSS-R1\u6846\u67b6\uff0c\u5c06\u501f\u53e3\u4efb\u52a1\u5b66\u4e60\u6574\u5408\u5230MLLM\u7684R1\u8bad\u7ec3\u540e\u9636\u6bb5\uff0c\u5f3a\u5236\u6a21\u578b\u5728\u56de\u7b54\u7528\u6237\u95ee\u9898\u65f6\uff0c\u540c\u65f6\u5904\u7406\u53d8\u6362\u540e\u7684\u89c6\u89c9\u8f93\u5165\u548c\u501f\u53e3\u4efb\u52a1\u95ee\u9898\uff0c\u4ece\u800c\u8bc6\u522b\u53d8\u6362\u5e76\u91cd\u5efa\u89c6\u9891\u4ee5\u7ed9\u51fa\u51c6\u786e\u7b54\u6848\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u9891\u63a8\u7406\u548c\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684Pretext-GRPO\u7b97\u6cd5\u548cViSS-R1\u6846\u67b6\u5747\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "ViSS-R1\u6846\u67b6\u901a\u8fc7\u6574\u5408Pretext-GRPO\uff0c\u6709\u6548\u63d0\u5347\u4e86MLLM\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.13055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13055", "abs": "https://arxiv.org/abs/2511.13055", "authors": ["Ruixin Liu", "Zejian Yuan"], "title": "Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries", "comment": null, "summary": "Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMonoUnc\u7684\u65b0\u578b\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5c40\u90e8\u8f66\u9053\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u89e3\u51b3\u8f66\u9053\u7ebf\u68c0\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u4e2d\u672a\u80fd\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\u548c\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7b80\u5316\u7684\u51e0\u4f55\u5047\u8bbe\u3002", "method": "MonoUnc\u5c063D\u8f66\u9053\u7ebf\u6295\u5f71\u5230\u524d\u89c6\uff08FV\uff09\u7a7a\u95f4\u5e76\u7528\u53c2\u6570\u5316\u66f2\u7ebf\u903c\u8fd1\uff0c\u7136\u540e\u52a8\u6001\u751f\u6210\u66f2\u7ebf\u70b9\u67e5\u8be2\u5d4c\u5165\u4ee5\u8fdb\u884c3D\u7a7a\u95f4\u4e2d\u7684\u8f66\u9053\u70b9\u9884\u6d4b\u3002\u6bcf\u4e2a\u7531\u76f8\u90bb\u70b9\u5f62\u6210\u7684\u7ebf\u6bb5\u88ab\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u8054\u5408\u4f18\u5316\u53c2\u6570\uff0c\u91c7\u7528\u65b0\u9896\u76843D\u9ad8\u65af\u5339\u914d\u635f\u5931\u3002", "result": "\u5728ONCE-3DLanes\u548cOpenLane\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMonoUnc\u5728\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u4e0b\uff0c\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MonoUnc\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5c40\u90e8\u8f66\u9053\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13063", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13063", "abs": "https://arxiv.org/abs/2511.13063", "authors": ["Zhenghua Li", "Hang Chen", "Zihao Sun", "Kai Li", "Xiaolin Hu"], "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation", "comment": null, "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578bSAM2\u7684\u5f3a\u5927\u7279\u5f81\u5e76\u5f15\u5165\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u548c\u53cc\u4eb2\u548c\u529b\u89e3\u7801\u5668\uff0c\u672c\u7814\u7a76\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u79d1\u5b66\u4e2d\u7535\u955c\u56fe\u50cf\u5206\u5272\u7684\u6311\u6218\uff0c\u5728\u51bb\u7ed3SAM2\u6743\u91cd\u65f6\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u955c\u56fe\u50cf\u795e\u7ecf\u7ed3\u6784\u5206\u5272\u9762\u4e34\u5f62\u6001\u590d\u6742\u3001\u4fe1\u566a\u6bd4\u4f4e\u3001\u6807\u6ce8\u7a00\u758f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u53d7\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u6d77\u91cf\u81ea\u7136\u56fe\u50cf\u4e0a\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u7684SAM2\u6a21\u578b\u8fc1\u79fb\u81f3\u7535\u955c\u9886\u57df\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u4f7f\u7528SAM2\u63d0\u53d6\u901a\u7528\u7279\u5f81\uff1b2. \u5f15\u5165\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528SAM2\u7684\u8bed\u4e49\u7ebf\u7d22\u6307\u5bfc\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff08FGE\uff09\u5173\u6ce8\u96be\u70b9\u533a\u57df\uff0c\u4ee5\u5f25\u5408\u9886\u57df\u5dee\u8ddd\uff1b3. \u8bbe\u8ba1\u53cc\u4eb2\u548c\u529b\u89e3\u7801\u5668\u751f\u6210\u7c97\u7565\u548c\u7cbe\u70bc\u7684\u4eb2\u548c\u529b\u56fe\u3002", "result": "\u5728\u7535\u955c\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff1a1. \u5728\u51bb\u7ed3SAM2\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u672c\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0eSOTA\u76f8\u5f53\u7684\u6027\u80fd\uff1b2. \u5728\u5bf9\u7535\u955c\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u5fae\u8c03\u540e\uff0c\u672c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684SOTA\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u5c06\u9884\u8bad\u7ec3\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u7684\u8868\u793a\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u9886\u57df\u81ea\u9002\u5e94\u5f15\u5bfc\u8fdb\u884c\u8fc1\u79fb\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u5206\u5272\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002"}}
{"id": "2511.13065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13065", "abs": "https://arxiv.org/abs/2511.13065", "authors": ["Reeshoon Sayera", "Akash Kumar", "Sirshapan Mitra", "Prudvi Kamtam", "Yogesh S Rawat"], "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition", "comment": "IEEE WACV'26 Main Conference", "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.", "AI": {"tldr": "RobustGait\u6846\u67b6\u5bf9\u57fa\u4e8e\u5916\u89c2\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u6270\u52a8\u7c7b\u578b\u3001\u8f6e\u5ed3\u63d0\u53d6\u65b9\u6cd5\u3001\u6a21\u578b\u67b6\u6784\u548c\u90e8\u7f72\u573a\u666f\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e86RGB\u566a\u58f0\u3001\u8f6e\u5ed3\u63d0\u53d6\u5668\u504f\u5dee\u3001\u6270\u52a8\u7c7b\u578b\u548c\u67b6\u6784\u8bbe\u8ba1\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5916\u89c2\u7684\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u548c\u8f6e\u5ed3\u53d8\u5316\u65b9\u9762\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51faRobustGait\u6846\u67b6\uff0c\u4ece\u6270\u52a8\u7c7b\u578b\uff08\u6570\u5b57\u3001\u73af\u5883\u3001\u65f6\u95f4\u3001\u906e\u6321\uff09\u3001\u8f6e\u5ed3\u63d0\u53d6\u65b9\u6cd5\uff08\u5206\u5272\u548c\u89e3\u6790\u7f51\u7edc\uff09\u3001\u6b65\u6001\u8bc6\u522b\u6a21\u578b\u7684\u67b6\u6784\u80fd\u529b\u4ee5\u53ca\u5404\u79cd\u90e8\u7f72\u573a\u666f\u56db\u4e2a\u7ef4\u5ea6\u5bf9\u57fa\u4e8e\u5916\u89c2\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u5728CASIA-B\u3001CCPG\u548cSUSTech1K\u6570\u636e\u96c6\u4e0a\u5f15\u5165\u4e8615\u79cd\u4e0d\u540c\u4e25\u91cd\u7a0b\u5ea6\u7684\u8150\u8d25\u7c7b\u578b\uff0c\u5e76\u5728MEVID\u4e0a\u8fdb\u884c\u4e86\u91ce\u5916\u9a8c\u8bc1\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u516d\u79cd\u6700\u5148\u8fdb\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cRGB\u5c42\u9762\u7684\u566a\u58f0\u6bd4\u5176\u4ed6\u5c42\u9762\u7684\u566a\u58f0\u66f4\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u60c5\u51b5\uff0c\u5e76\u63ed\u793a\u4e86\u5931\u771f\u5982\u4f55\u901a\u8fc7\u8f6e\u5ed3\u63d0\u53d6\u4f20\u64ad\u5230\u4e0b\u6e38\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u3002\u6b65\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\u5bf9\u8f6e\u5ed3\u63d0\u53d6\u5668\u7684\u504f\u5dee\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u8868\u660e\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\u57fa\u51c6\u504f\u5dee\u6765\u6e90\u3002\u9c81\u68d2\u6027\u540c\u65f6\u53d6\u51b3\u4e8e\u6270\u52a8\u7c7b\u578b\u548c\u67b6\u6784\u8bbe\u8ba1\u3002\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u5176\u66f4\u63a5\u8fd1\u53ef\u90e8\u7f72\u72b6\u6001\u3002"}}
{"id": "2511.13079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13079", "abs": "https://arxiv.org/abs/2511.13079", "authors": ["Jiacheng Tang", "Mingyue Feng", "Jiachao Liu", "Yaonong Wang", "Jian Pu"], "title": "Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving", "comment": "11 pages, 8 figures", "summary": "Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u8eab\u72b6\u6001\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptiveAD\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u548c\u591a\u4e0a\u4e0b\u6587\u878d\u5408\u7b56\u7565\uff0c\u5c06\u573a\u666f\u611f\u77e5\u548c\u81ea\u8eab\u72b6\u6001\u663e\u5f0f\u89e3\u8026\uff0c\u5e76\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u5f0f\u89c4\u5212\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6a21\u5757\u5316\u8bbe\u8ba1\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u8eab\u72b6\u6001\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u539f\u56e0\u662f\u81ea\u8eab\u72b6\u6001\u4fe1\u606f\u5728\u65e9\u671fBEV\u7f16\u7801\u5668\u4e2d\u8fc7\u65e9\u878d\u5408\uff0c\u6210\u4e3a\u4e0b\u6e38\u89c4\u5212\u6a21\u5757\u7684\u6377\u5f84\u3002", "method": "\u63d0\u51faAdaptiveAD\u67b6\u6784\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u663e\u5f0f\u89e3\u8026\u573a\u666f\u611f\u77e5\u548c\u81ea\u8eab\u72b6\u6001\u3002\u4e00\u4e2a\u5206\u652f\u8fdb\u884c\u573a\u666f\u9a71\u52a8\u63a8\u7406\uff08\u7701\u7565\u81ea\u8eab\u72b6\u6001\uff09\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u8fdb\u884c\u4ec5\u57fa\u4e8e\u89c4\u5212\u4efb\u52a1\u7684\u81ea\u8eab\u72b6\u6001\u9a71\u52a8\u63a8\u7406\u3002\u901a\u8fc7\u573a\u666f\u611f\u77e5\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u6574\u5408\u4e24\u4e2a\u5206\u652f\u7684\u51b3\u7b56\u3002\u5f15\u5165\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\u548cBEV\u5355\u5411\u84b8\u998f\u3001\u81ea\u56de\u5f52\u5728\u7ebf\u6620\u5c04\u8f85\u52a9\u4efb\u52a1\u6765\u4fdd\u8bc1\u591a\u4efb\u52a1\u5b66\u4e60\u6548\u679c\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0cAdaptiveAD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u5f0f\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "AdaptiveAD\u6210\u529f\u7f13\u89e3\u4e86\u5bf9\u81ea\u8eab\u72b6\u6001\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5e76\u5c55\u73b0\u51fa\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u51fa\u8272\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13081", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13081", "abs": "https://arxiv.org/abs/2511.13081", "authors": ["Yehonatan Elisha", "Seffi Cohen", "Oren Barkan", "Noam Koenigstein"], "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations", "comment": null, "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations.Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.", "AI": {"tldr": "Saliency maps\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u91ca\u76ee\u6807\uff0c\u963b\u788d\u4e86\u8bc4\u4f30\u548c\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86RFxG\uff08\u53c2\u8003\u7cfb\u00d7\u7c92\u5ea6\uff09\u5206\u7c7b\u6cd5\uff0c\u533a\u5206\u4e86\u201c\u4e3a\u4f55\u662f\u8fd9\u4e2a\uff1f\u201d\uff08\u70b9\u5f0f\uff09\u548c\u201c\u4e3a\u4f55\u662f\u8fd9\u4e2a\u800c\u975e\u5176\u4ed6\uff1f\u201d\uff08\u5bf9\u6bd4\u5f0f\uff09\u4ee5\u53ca\u201c\u4e3a\u4f55\u662f\u54c8\u58eb\u5947\uff1f\u201d\uff08\u7c7b\u7ea7\u522b\uff09\u548c\u201c\u4e3a\u4f55\u662f\u72d7\uff1f\u201d\uff08\u7ec4\u7ea7\u522b\uff09\u7684\u89e3\u91ca\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5927\u591a\u53ea\u5173\u6ce8\u70b9\u5f0f\u5fe0\u5b9e\u5ea6\uff0c\u5ffd\u7565\u4e86\u5bf9\u6bd4\u63a8\u7406\u548c\u8bed\u4e49\u7c92\u5ea6\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u56db\u79cd\u65b0\u7684\u5fe0\u5b9e\u5ea6\u6307\u6807\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u4e09\u79cd\u6570\u636e\u96c6\u548c\u56db\u79cd\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86\u5341\u79cdSOTA\u663e\u7740\u6027\u65b9\u6cd5\u3002\u672c\u6587\u4e3b\u5f20\u4ee5\u7528\u6237\u610f\u56fe\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u4e0e\u4eba\u7c7b\u7406\u89e3\u76f8\u7b26\u7684\u89c6\u89c9\u89e3\u91ca\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\u3002", "motivation": "Saliency maps\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u91ca\u76ee\u6807\uff0c\u963b\u788d\u4e86\u8bc4\u4f30\u548c\u5e94\u7528\u3002", "method": "\u63d0\u51faRFxG\uff08\u53c2\u8003\u7cfb\u00d7\u7c92\u5ea6\uff09\u5206\u7c7b\u6cd5\uff0c\u533a\u5206\u4e86\u201c\u4e3a\u4f55\u662f\u8fd9\u4e2a\uff1f\u201d\uff08\u70b9\u5f0f\uff09\u548c\u201c\u4e3a\u4f55\u662f\u8fd9\u4e2a\u800c\u975e\u5176\u4ed6\uff1f\u201d\uff08\u5bf9\u6bd4\u5f0f\uff09\u4ee5\u53ca\u201c\u4e3a\u4f55\u662f\u54c8\u58eb\u5947\uff1f\u201d\uff08\u7c7b\u7ea7\u522b\uff09\u548c\u201c\u4e3a\u4f55\u662f\u72d7\uff1f\u201d\uff08\u7ec4\u7ea7\u522b\uff09\u7684\u89e3\u91ca\u3002\u5e76\u63d0\u51fa\u56db\u79cd\u65b0\u7684\u5fe0\u5b9e\u5ea6\u6307\u6807\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5927\u591a\u53ea\u5173\u6ce8\u70b9\u5f0f\u5fe0\u5b9e\u5ea6\uff0c\u5ffd\u7565\u4e86\u5bf9\u6bd4\u63a8\u7406\u548c\u8bed\u4e49\u7c92\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u5728\u4e09\u79cd\u6570\u636e\u96c6\u548c\u56db\u79cd\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86\u5341\u79cdSOTA\u663e\u7740\u6027\u65b9\u6cd5\u3002", "conclusion": "\u4e3b\u5f20\u4ee5\u7528\u6237\u610f\u56fe\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u4e0e\u4eba\u7c7b\u7406\u89e3\u76f8\u7b26\u7684\u89c6\u89c9\u89e3\u91ca\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\u3002"}}
{"id": "2511.13099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13099", "abs": "https://arxiv.org/abs/2511.13099", "authors": ["Doanh C. Bui", "Ba Hung Ngo", "Hoai Luan Pham", "Khang Nguyen", "Ma\u00ef K. Nguyen", "Yasuhiko Nakashima"], "title": "MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images", "comment": "WACV2026 Accepted", "summary": "Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.", "AI": {"tldr": "MergeSlide\u662f\u4e00\u4e2a\u5c06\u7ec8\u8eab\u5b66\u4e60\u89c6\u4e3a\u6a21\u578b\u5408\u5e76\u95ee\u9898\u7684\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6b63\u4ea4\u6301\u7eed\u5408\u5e76\u7b56\u7565\u6765\u5408\u5e76\u65b0\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528TCP\u63a8\u7406\u65b9\u6cd5\u5728CLASS-IL\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u63a8\u7406\u3002", "motivation": "\u51cf\u5c11\u5904\u7406\u548c\u4f20\u8f93WSIs\uff08\u764c\u75c7\u5168\u5207\u7247\u56fe\u50cf\uff09\u6570\u636e\u6240\u9700\u7684\u8d44\u6e90\u548c\u7cbe\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406GB\u7ea7\u522b\u5927\u5c0f\u7684WSIs\u65f6\uff0c\u901a\u8fc7\u5728\u764c\u75c7\u76f8\u5173\u4efb\u52a1\u4e0a\u987a\u5e8f\u8bad\u7ec3\u6216\u5fae\u8c03\u7edf\u4e00\u6a21\u578b\u6765\u5b9e\u73b0\u7ec8\u8eab\u5b66\u4e60\u3002", "method": "MergeSlide\u6846\u67b6\u5c06\u7ec8\u8eab\u5b66\u4e60\u89c6\u4e3a\u6a21\u578b\u5408\u5e76\u95ee\u9898\u3002\u65b0\u4efb\u52a1\u7684\u5904\u7406\u6d41\u7a0b\u4e3a\uff1a1\uff09\u4f7f\u7528\u7c7b\u522b\u611f\u77e5\u63d0\u793a\uff08class-aware prompts\uff09\u5b9a\u4e49\u4efb\u52a1\uff1b2\uff09\u4f7f\u7528\u65e0MLP\u9aa8\u5e72\uff08MLP-free backbone\uff09\u8fdb\u884c\u5c11\u91cfepoch\u7684\u5fae\u8c03\uff1b3\uff09\u4f7f\u7528\u6b63\u4ea4\u6301\u7eed\u5408\u5e76\u7b56\u7565\uff08orthogonal continual merging strategy\uff09\u5c06\u65b0\u4efb\u52a1\u5408\u5e76\u5230\u7edf\u4e00\u6a21\u578b\u4e2d\u3002\u5728CLASS-IL\u8bbe\u7f6e\u4e0b\u7684\u63a8\u7406\uff0c\u5f15\u5165\u4efb\u52a1\u5230\u7c7b\u522b\u63d0\u793a\u5bf9\u9f50\uff08Task-to-Class Prompt-aligned, TCP\uff09\u63a8\u7406\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u4efb\u52a1\u7ea7\u63d0\u793a\u8bc6\u522b\u76f8\u5173\u4efb\u52a1\uff0c\u7136\u540e\u5e94\u7528\u76f8\u5e94\u7684\u7c7b\u522b\u611f\u77e5\u63d0\u793a\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u516d\u4e2aTCGA\u6570\u636e\u96c6\u6d41\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMergeSlide\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u6837\u672c\u4fdd\u7559\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u548c\u89c6\u89c9-\u8bed\u8a00\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MergeSlide\u901a\u8fc7\u5c06\u7ec8\u8eab\u5b66\u4e60\u89c6\u4e3a\u6a21\u578b\u5408\u5e76\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u7c7b\u522b\u611f\u77e5\u63d0\u793a\u3001MLP-free\u5fae\u8c03\u548c\u6b63\u4ea4\u6301\u7eed\u5408\u5e76\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u764c\u75c7\u5168\u5207\u7247\u56fe\u50cf\u7ec8\u8eab\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u4fdd\u6301\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728CLASS-IL\u8bbe\u7f6e\u4e0b\u901a\u8fc7TCP\u63a8\u7406\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13102", "abs": "https://arxiv.org/abs/2511.13102", "authors": ["Yu Zhu", "Dan Zeng", "Shuiwang Li", "Qijun Zhao", "Qiaomu Shen", "Bo Tang"], "title": "CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation", "comment": null, "summary": "Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7c7b\u522b\u65e0\u5173\u59ff\u6001\u4f30\u8ba1\uff08CAPE\uff09\u6846\u67b6CapeNext\uff0c\u901a\u8fc7\u878d\u5408\u5206\u5c42\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u53cc\u6d41\u7279\u5f81\u7ec6\u5316\u6765\u514b\u670d\u73b0\u6709\u9759\u6001\u8054\u5408\u5d4c\u5165\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7c7b\u522b\u65e0\u5173\u59ff\u6001\u4f30\u8ba1\uff08CAPE\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u6587\u672c\u5173\u952e\u70b9\u63cf\u8ff0\u4f5c\u4e3a\u8bed\u4e49\u5148\u9a8c\uff0c\u5b58\u5728\u591a\u4e49\u6027\u5bfc\u81f4\u7684\u8de8\u7c7b\u522b\u6a21\u7cca\u4ee5\u53ca\u5bf9\u7c7b\u522b\u5185\u90e8\u7ec6\u5fae\u5dee\u522b\u7684\u533a\u5206\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u521b\u65b0\u6027\u5730\u878d\u5408\u4e86\u5206\u5c42\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u53cc\u6d41\u7279\u5f81\u7ec6\u5316\uff0c\u5229\u7528\u6587\u672c\u63cf\u8ff0\u548c\u7279\u5b9a\u56fe\u50cf\u4e2d\u7684\u7c7b\u522b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u7ebf\u7d22\u6765\u589e\u5f3a\u8054\u5408\u5d4c\u5165\u3002", "result": "\u5728MP-100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCapeNext\u5728\u4e0d\u540c\u7f51\u7edc\u9aa8\u5e72\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684CAPE\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CapeNext\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u53cc\u6d41\u7279\u5f81\u7ec6\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709CAPE \u65b9\u6cd5\u4e2d\u7684\u591a\u4e49\u6027\u548c\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.13105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13105", "abs": "https://arxiv.org/abs/2511.13105", "authors": ["Seungjae Kim", "SeungJoon Lee", "MyeongAh Cho"], "title": "PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking", "comment": "AAAI 2026. Code: https://github.com/VisualScienceLab-KHU/PlugTrack", "summary": "Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPlugTrack\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u4e2d\u8fd0\u52a8\u9884\u6d4b\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u65b9\u6cd5\u5728\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u8fd0\u52a8\uff1b\u800c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u878d\u5408\u4e24\u79cd\u65b9\u6cd5\u4f18\u70b9\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51faPlugTrack\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u611f\u77e5\u8fd0\u52a8\u7406\u89e3\uff0c\u81ea\u9002\u5e94\u5730\u878d\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\u3002\u8be5\u6846\u67b6\u5229\u7528\u591a\u611f\u77e5\u8fd0\u52a8\u5206\u6790\u751f\u6210\u81ea\u9002\u5e94\u878d\u5408\u56e0\u5b50\uff0c\u4ee5\u7ed3\u5408\u4e24\u79cd\u9884\u6d4b\u65b9\u6cd5\u7684\u4f18\u70b9\u3002", "result": "PlugTrack\u6846\u67b6\u5728MOT17/MOT20\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728DanceTrack\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u4ee5\u975e\u7ebf\u6027\u8fd0\u52a8\u4e3a\u4e3b\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u591a\u8fbe34%\u7684\u60c5\u51b5\u4e0b\u4e5f\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u5668\uff0c\u8868\u660e\u771f\u5b9e\u4e16\u754c\u7684\u8ddf\u8e2a\u573a\u666f\u540c\u65f6\u5305\u542b\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "PlugTrack\u662f\u7b2c\u4e00\u4e2a\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u6765\u8fde\u63a5MOT\u4e2d\u7ecf\u5178\u548c\u73b0\u4ee3\u8fd0\u52a8\u9884\u6d4b\u8303\u5f0f\u7684\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86MOT\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13106", "abs": "https://arxiv.org/abs/2511.13106", "authors": ["Fengzhi Xu", "Ziyuan Yang", "Mengyu Sun", "Joey Tianyi Zhou", "Yi Zhang"], "title": "Low-Level Dataset Distillation for Medical Image Enhancement", "comment": null, "summary": "Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.", "AI": {"tldr": "\u4f4e\u7ea7\u522b\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u4fdd\u6301\u7ed3\u6784\u7684\u4e2a\u6027\u5316\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u6570\u636e\u96c6\u84b8\u998f\uff08DD\uff09\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u73b0\u6709DD\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9ad8\u7ea7\u4efb\u52a1\uff0c\u800c\u4f4e\u7ea7\u522b\u4efb\u52a1\uff08\u5982\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\uff09\u7684\u6570\u636e\u96c6\u84b8\u998f\u662f\u4e00\u4e2a\u672a\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u4f4e\u7ea7\u522bDD\u65b9\u6cd5\uff0c\u5229\u7528\u89e3\u5256\u5b66\u76f8\u4f3c\u6027\u6784\u5efa\u5171\u4eab\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u4fdd\u6301\u7ed3\u6784\u7684\u4e2a\u6027\u5316\u751f\u6210\uff08SPG\uff09\u6a21\u5757\u8fdb\u884c\u4e2a\u6027\u5316\u3002\u5c06\u4efb\u52a1\u7279\u5b9a\u7684\u9ad8\u4f4e\u8d28\u91cf\u8bad\u7ec3\u5bf9\u4e0e\u60a3\u8005\u7279\u5f02\u6027\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u68af\u5ea6\u5bf9\u9f50\u5b9e\u73b0\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5305\u542b\u62bd\u8c61\u8bad\u7ec3\u4fe1\u606f\u7684\u84b8\u998f\u6570\u636e\u96c6\uff0c\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u60a3\u8005\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u62a4\u4e86\u60a3\u8005\u9690\u79c1\uff0c\u5e76\u5b9e\u73b0\u4e86\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4e\u7ea7\u522bDD\u65b9\u6cd5\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u3001SPG\u6a21\u5757\u548c\u68af\u5ea6\u5bf9\u9f50\uff0c\u6709\u6548\u538b\u7f29\u4e86\u6570\u636e\uff0c\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u5e76\u4fdd\u62a4\u4e86\u60a3\u8005\u9690\u79c1\u3002"}}
{"id": "2511.13108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13108", "abs": "https://arxiv.org/abs/2511.13108", "authors": ["Jiazhen Yan", "Ziqiang Li", "Fan Wang", "Boyu Wang", "Zhangjie Fu"], "title": "DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection", "comment": null, "summary": "The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.", "AI": {"tldr": "DGS-Net \u901a\u8fc7\u5728\u68af\u5ea6\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5206\u89e3\uff0c\u533a\u5206\u5e76\u6291\u5236\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u6210\u5206\uff0c\u4ece\u800c\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u901a\u7528\u8868\u5f81\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u5730\u68c0\u6d4b AI \u751f\u6210\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684 AI \u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5728\u5fae\u8c03\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982 CLIP\uff09\u65f6\uff0c\u4f1a\u56e0\u707e\u96be\u6027\u9057\u5fd8\u800c\u635f\u5bb3\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u5e76\u9650\u5236\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u7559\u53ef\u8fc1\u79fb\u7684\u9884\u8bad\u7ec3\u5148\u9a8c\u77e5\u8bc6\uff0c\u540c\u65f6\u6291\u5236\u4e0e\u4efb\u52a1\u65e0\u5173\u6210\u5206\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DGS-Net \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u68af\u5ea6\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5206\u89e3\uff0c\u5c06\u6709\u5bb3\u548c\u6709\u76ca\u7684\u4e0b\u964d\u65b9\u5411\u5206\u79bb\u3002\u901a\u8fc7\u5c06\u4efb\u52a1\u68af\u5ea6\u6295\u5f71\u5230\u4e0e\u6709\u5bb3\u65b9\u5411\u6b63\u4ea4\u7684\u8865\u7a7a\u95f4\uff0c\u5e76\u4e0e\u4ece\u51bb\u7ed3\u7684 CLIP \u7f16\u7801\u5668\u84b8\u998f\u51fa\u7684\u6709\u76ca\u65b9\u5411\u5bf9\u9f50\uff0cDGS-Net \u5b9e\u73b0\u4e86\u5148\u9a8c\u77e5\u8bc6\u4fdd\u7559\u548c\u65e0\u5173\u6210\u5206\u6291\u5236\u7684\u7edf\u4e00\u4f18\u5316\u3002", "result": "\u5728 50 \u79cd\u751f\u6210\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDGS-Net \u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u9886\u5148 6.6\uff0c\u5728\u5404\u79cd\u751f\u6210\u6280\u672f\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DGS-Net \u80fd\u591f\u6709\u6548\u5730\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u8868\u5f81\u80fd\u529b\uff0c\u540c\u65f6\u6291\u5236\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u6210\u5206\uff0c\u4ece\u800c\u5728 AI \u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13110", "abs": "https://arxiv.org/abs/2511.13110", "authors": ["Shuaibin Fan", "Senming Zhong", "Wenchao Yan", "Minglong Xue"], "title": "Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing", "comment": null, "summary": "Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u795e\u7ecf\u9000\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u53d7\u96fe\u973e\u5f71\u54cd\u7684\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u975e\u5747\u5300\u96fe\u973e\u5206\u5e03\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u8868\u793a\u548c\u5168\u5c40\u4e00\u81f4\u6027\u5efa\u6a21\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5b66\u4e60\u96fe\u973e\u7684\u7a7a\u95f4\u53d8\u5316\u89c4\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u53bb\u96fe\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u901a\u9053\u72ec\u7acb\u548c\u901a\u9053\u4f9d\u8d56\u673a\u5236\u7684\u7b97\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5b66\u4e60\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5728\u590d\u6742\u573a\u666f\u4e2d\u83b7\u5f97\u826f\u597d\u7684\u89c6\u89c9\u611f\u77e5\u30022. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u5c06\u96fe\u973e\u9000\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u6d88\u9664\u4e86\u5bf9\u663e\u5f0f\u7279\u5f81\u63d0\u53d6\u548c\u7269\u7406\u6a21\u578b\u7684\u4f9d\u8d56\u30023. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bc6\u96c6\u6b8b\u5dee\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u6d88\u9664\u5197\u4f59\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u516c\u5171\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u53bb\u96fe\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u9690\u5f0f\u795e\u7ecf\u9000\u5316\u8868\u793a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u573a\u666f\u4e0b\u7684\u96fe\u973e\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u3002"}}
{"id": "2511.13113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13113", "abs": "https://arxiv.org/abs/2511.13113", "authors": ["Zhaocheng Yu", "Kui Jiang", "Junjun Jiang", "Xianming Liu", "Guanglu Sun", "Yi Xiao"], "title": "Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining", "comment": null, "summary": "Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.", "AI": {"tldr": "MPHM\u7f51\u7edc\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u53bb\u96e8\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86CLIP\u548cDINOv2\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7PFI\u878d\u5408\u8fd9\u4e9b\u5148\u9a8c\uff0c\u540c\u65f6\u4f7f\u7528HMM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5728Rain200H\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u53bb\u96e8\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u96e8\u6c34\u573a\u666f\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u548c\u7a7a\u95f4\u7ec6\u8282\u7684\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u5148\u9a8c\u5206\u5c42\u66fc\u5df4\uff08MPHM\uff09\u7f51\u7edc\u7684\u65b0\u67b6\u6784\u3002\u8be5\u7f51\u7edc\u96c6\u6210\u4e86\u5b8f\u89c2\u8bed\u4e49\u6587\u672c\u5148\u9a8c\uff08CLIP\uff09\u548c\u5fae\u89c2\u7ed3\u6784\u89c6\u89c9\u5148\u9a8c\uff08DINOv2\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u7684\u5148\u9a8c\u878d\u5408\u6ce8\u5165\uff08PFI\uff09\u673a\u5236\u6765\u878d\u5408\u8fd9\u4e9b\u5f02\u6784\u5148\u9a8c\u3002\u540c\u65f6\uff0c\u9aa8\u5e72\u7f51\u7edc\u91c7\u7528\u4e86\u5206\u5c42\u66fc\u5df4\u6a21\u5757\uff08HMM\uff09\uff0c\u8be5\u6a21\u5757\u5177\u6709\u5085\u91cc\u53f6\u589e\u5f3a\u7684\u53cc\u8def\u5f84\u8bbe\u8ba1\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u5c40\u90e8\u7ec6\u8282\u6062\u590d\u3002", "result": "MPHM\u5728Rain200H\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.57 dB\u7684PSNR\u589e\u76ca\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u96e8\u6c34\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MPHM\u7f51\u7edc\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u5148\u9a8c\u77e5\u8bc6\u548c\u521b\u65b0\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u53bb\u96e8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u7a81\u7834\u3002"}}
{"id": "2511.13115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13115", "abs": "https://arxiv.org/abs/2511.13115", "authors": ["Hanzhe Liang", "Jie Zhou", "Can Gao", "Bingyang Guo", "Jinbao Wang", "Linlin Shen"], "title": "A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features", "comment": "Submitted to Elsevier", "summary": "3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff08RIF\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u74063D\u70b9\u4e91\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u65b9\u5411\u548c\u4f4d\u7f6e\u53d8\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7PCM\u6280\u672f\u548cCTF-Net\u63d0\u53d6\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728Anomaly-ShapeNet\u548cReal3D-AD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76843D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u65b9\u5411\u548c\u4f4d\u7f6e\u53d8\u5316\u76843D\u70b9\u4e91\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u7279\u5f81\u8868\u793a\u7684\u53d8\u5316\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff08RIF\uff09\u6846\u67b6\uff0c\u5305\u62ec\u70b9\u5750\u6807\u6620\u5c04\uff08PCM\uff09\u6280\u672f\u4ee5\u53bb\u9664\u70b9\u4e91\u6570\u636e\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u5377\u79ef\u53d8\u6362\u7279\u5f81\u7f51\u7edc\uff08CTF-Net\uff09\u6765\u63d0\u53d6\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\u548c3D\u6570\u636e\u589e\u5f3a\u6765\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u5728Anomaly-ShapeNet\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747P-AUROC\u63d0\u9ad8\u4e8617.7%\uff1b\u5728Real3D-AD\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747P-AUROC\u63d0\u9ad8\u4e861.6%\u3002", "conclusion": "RIF\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u4e0e\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7ed3\u5408\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.13121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13121", "abs": "https://arxiv.org/abs/2511.13121", "authors": ["Yuqi Zhang", "Guanying Chen", "Jiaxing Chen", "Chuanyu Fu", "Chuan Huang", "Shuguang Cui"], "title": "CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model", "comment": "Project Link: https://zyqz97.github.io/CloseUpShot/", "summary": "Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u70b9\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u626d\u66f2\u3001\u906e\u6321\u611f\u77e5\u566a\u58f0\u6291\u5236\u548c\u5168\u5c40\u7ed3\u6784\u5f15\u5bfc\u6765\u4ece\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u5408\u6210\u8fd1\u666f\u65b0\u89c6\u56fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u8fd1\u666f\u573a\u666f\u4e2d\u5904\u7406\u7a00\u758f\u89c6\u56fe\u548c\u6355\u6349\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCloseUpShot\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u70b9\u6761\u4ef6\u89c6\u9891\u6269\u6563\u3002\u5173\u952e\u6280\u672f\u5305\u62ec\uff1a1. \u5206\u5c42\u626d\u66f2\u548c\u906e\u6321\u611f\u77e5\u566a\u58f0\u6291\u5236\uff0c\u4ee5\u63d0\u9ad8\u6761\u4ef6\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u30022. \u5168\u5c40\u7ed3\u6784\u5f15\u5bfc\uff0c\u5229\u7528\u5bc6\u96c6\u7684\u878d\u5408\u70b9\u4e91\u6765\u63d0\u4f9b\u4e00\u81f4\u7684\u51e0\u4f55\u4e0a\u4e0b\u6587\uff0c\u4ee5\u5f25\u8865\u7a00\u758f\u6761\u4ef6\u8f93\u5165\u7684\u5168\u5c40\u4e00\u81f4\u60273D\u7ea6\u675f\u7684\u4e0d\u8db3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd1\u666f\u65b0\u89c6\u56fe\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u626d\u66f2\u3001\u906e\u6321\u611f\u77e5\u566a\u58f0\u6291\u5236\u548c\u5168\u5c40\u7ed3\u6784\u5f15\u5bfc\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u8fd1\u666f\u65b0\u89c6\u56fe\u5408\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2511.13125", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13125", "abs": "https://arxiv.org/abs/2511.13125", "authors": ["Hao Long", "Silin Zhou", "Lisi Chen", "Shuo Shang"], "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning", "comment": "This paper is accepted by AAAI2026", "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.", "AI": {"tldr": "RePo\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u7f16\u7801\u533a\u57df\u548c\u70b9\u7279\u5f81\u6765\u63d0\u5347\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u51c6\u786e\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8f68\u8ff9\u4fe1\u606f\u7684\u5b8c\u6574\u9891\u8c31\u8fdb\u884c\u76f8\u4f3c\u5ea6\u5efa\u6a21\u3002", "method": "RePo\u65b9\u6cd5\u8054\u5408\u7f16\u7801\u533a\u57df\u7279\u5f81\uff08\u5c06GPS\u8f68\u8ff9\u6620\u5c04\u5230\u7f51\u683c\u5e8f\u5217\uff0c\u5e76\u878d\u5408\u7ed3\u6784\u7279\u5f81\u3001\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\uff09\u548c\u70b9\u7279\u5f81\uff08\u901a\u8fc7\u4e09\u4e2a\u4e13\u5bb6\u7f51\u7edc\u63d0\u53d6\u5c40\u90e8\u3001\u76f8\u5173\u548c\u8fde\u7eed\u8fd0\u52a8\u6a21\u5f0f\uff09\uff0c\u7136\u540e\u4f7f\u7528\u8def\u7531\u5668\u7f51\u7edc\u81ea\u9002\u5e94\u878d\u5408\u70b9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u533a\u57df\u7279\u5f81\u7ed3\u5408\uff0c\u751f\u6210\u6700\u7ec8\u7684\u8f68\u8ff9\u5d4c\u5165\u3002\u4f7f\u7528\u5e26\u6709\u56f0\u96be\u8d1f\u6837\u672c\u7684\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "RePo\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4SOTA\u57fa\u7ebf\u63d0\u9ad8\u4e8622.2%\u3002", "conclusion": "RePo\u901a\u8fc7\u7ed3\u5408\u533a\u57df\u548c\u70b9\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13127", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13127", "abs": "https://arxiv.org/abs/2511.13127", "authors": ["Zonghao Ying", "Moyang Chen", "Nizhang Li", "Zhiqiang Wang", "Wenxin Zhang", "Quanchen Zou", "Zonglei Jing", "Aishan Liu", "Xianglong Liu"], "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language", "comment": null, "summary": "Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.", "AI": {"tldr": "\u9690\u853d\u7684\u63d0\u793a\u8bcd\u80fd\u591f\u8bf1\u5bfc\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u751f\u6210\u4e0d\u5b89\u5168\u89c6\u9891\uff0cVEIL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e2d\u6027\u573a\u666f\u951a\u70b9\u3001\u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\u548c\u98ce\u683c\u8c03\u8282\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\u5b9e\u73b0\u6b64\u76ee\u7684\uff0c\u5e76\u5728\u5546\u4e1a\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\u901a\u5e38\u91c7\u7528\u660e\u663e\u7684\u5371\u9669\u63d0\u793a\u8bcd\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u9632\u5fa1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u9690\u853d\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u770b\u4f3c\u65e0\u5bb3\u7684\u63d0\u793a\u8bcd\u6765\u751f\u6210\u4e0d\u5b89\u5168\u89c6\u9891\u3002", "method": "\u63d0\u51faVEIL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1. \u4e2d\u6027\u573a\u666f\u951a\u70b9\uff1a\u63d0\u53d6\u88ab\u963b\u6b62\u610f\u56fe\u7684\u8868\u9762\u573a\u666f\u63cf\u8ff0\u4ee5\u4fdd\u6301\u5408\u7406\u6027\u30022. \u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\uff1a\u63cf\u8ff0\u65e0\u5bb3\u7684\u97f3\u9891\u4e8b\u4ef6\uff0c\u5229\u7528\u97f3\u89c6\u9891\u5171\u540c\u51fa\u73b0\u7684\u5148\u9a8c\u77e5\u8bc6\u504f\u5411\u6a21\u578b\u751f\u6210\u7279\u5b9a\u7684\u4e0d\u5b89\u5168\u89c6\u89c9\u6982\u5ff5\u30023. \u98ce\u683c\u8c03\u8282\u5668\uff1a\u7535\u5f71\u5316\u6307\u4ee4\uff08\u5982\u955c\u5934\u3001\u6c1b\u56f4\uff09\u6765\u653e\u5927\u548c\u7a33\u5b9a\u6f5c\u5728\u89e6\u53d1\u5668\u7684\u6548\u679c\u3002\u653b\u51fb\u751f\u6210\u88ab\u5f62\u5f0f\u5316\u4e3a\u5728\u4e0a\u8ff0\u6a21\u5757\u5316\u63d0\u793a\u7a7a\u95f4\u4e0a\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u641c\u7d22\u8fc7\u7a0b\u6765\u89e3\u51b3\u3002", "result": "\u57287\u4e2a\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u5728\u5546\u4e1a\u6a21\u578b\u4e0a\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u4e8623%\u3002", "conclusion": "VEIL\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9690\u853d\u63d0\u793a\u8bcd\u751f\u6210\u4e0d\u5b89\u5168\u89c6\u9891\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u9632\u62a4\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.13132", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13132", "abs": "https://arxiv.org/abs/2511.13132", "authors": ["Chenyang Li", "Wenbing Tang", "Yihao Huang", "Sinong Simon Zhan", "Ming Hu", "Xiaojun Jia", "Yang Liu"], "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack", "comment": null, "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aILA\u7684\u9ed1\u76d2\u5bf9\u6297\u6027\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u64cd\u7eb5\u5ba4\u5185\u7167\u660e\u6765\u6270\u4e71\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u667a\u80fd\u4f53\uff0c\u63ed\u793a\u4e86VLN\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u5ba4\u5185\u5149\u7167\u53d8\u5316\u4e0b\u5b58\u5728\u7684\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709VLN\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5bf9\u6297\u6027\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u4e0d\u5207\u5b9e\u9645\u7684\u7eb9\u7406\u6270\u52a8\uff1b\u800c\u5ba4\u5185\u5149\u7167\u662f\u5f71\u54cd\u5bfc\u822a\u7684\u5185\u5728\u56e0\u7d20\uff0c\u5374\u88ab\u5ffd\u89c6\u4e86\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5ba4\u5185\u5149\u7167\u5bf9VLN\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faILA\uff08Indoor Lighting-based Adversarial Attack\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u64cd\u7eb5\u5168\u5c40\u5149\u7167\u6765\u5e72\u6270VLN\u667a\u80fd\u4f53\u3002\u5305\u542b\u4e24\u79cd\u653b\u51fb\u6a21\u5f0f\uff1aSILA\uff08\u9759\u6001\u5ba4\u5185\u5149\u7167\u653b\u51fb\uff09\u548cDILA\uff08\u52a8\u6001\u5ba4\u5185\u5149\u7167\u653b\u51fb\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u6700\u5148\u8fdb\u7684VLN\u6a21\u578b\u548c\u4e09\u4e2a\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cILA\u663e\u8457\u589e\u52a0\u4e86\u5931\u8d25\u7387\u5e76\u964d\u4f4e\u4e86\u8f68\u8ff9\u6548\u7387\uff0c\u63ed\u793a\u4e86VLN\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u5ba4\u5185\u5149\u7167\u53d8\u5316\u4e0b\u5b58\u5728\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u5ba4\u5185\u5149\u7167\u662f\u5f71\u54cdVLN\u667a\u80fd\u4f53\u9c81\u68d2\u6027\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u73b0\u6709\u7684VLN\u6a21\u578b\u5728\u8fd9\u79cd\u53d8\u5316\u4e0b\u8868\u73b0\u8106\u5f31\u3002ILA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u66b4\u9732\u8fd9\u4e9b\u6f0f\u6d1e\u3002"}}
{"id": "2511.13135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13135", "abs": "https://arxiv.org/abs/2511.13135", "authors": ["Junjie Yang", "Yuhao Yan", "Gang Wu", "Yuxuan Wang", "Ruoyu Liang", "Xinjie Jiang", "Xiang Wan", "Fenglei Fan", "Yongquan Zhang", "Feiwei Qin", "Changmiao Wan"], "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation", "comment": "CVPR 2026 Under Review", "summary": "As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \\textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.", "AI": {"tldr": "MedGEN-Bench\u662f\u4e00\u4e2a\u5305\u542b6422\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u6db5\u76d66\u79cd\u6210\u50cf\u6a21\u5f0f\u300116\u4e2a\u4e34\u5e8a\u4efb\u52a1\u548c28\u4e2a\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u57fa\u51c6\u5728\u67e5\u8be2\u76f8\u5173\u6027\u3001\u8bca\u65ad\u63a8\u7406\u590d\u6742\u6027\u548c\u8bc4\u4f30\u8303\u5f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5bf9AI\u751f\u6210\u533b\u5b66\u56fe\u50cf\u7684\u9700\u6c42\u3002", "method": "MedGEN-Bench\u5305\u542b\u89c6\u89c9\u95ee\u7b54\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u751f\u6210\u4e09\u79cd\u683c\u5f0f\uff0c\u5176\u7279\u70b9\u662f\u4f7f\u7528\u9700\u8981\u590d\u6742\u8de8\u6a21\u6001\u63a8\u7406\u548c\u5f00\u653e\u5f0f\u751f\u6210\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u6307\u4ee4\u3002\u8bc4\u4f30\u6846\u67b6\u5305\u542b\u50cf\u7d20\u7ea7\u6307\u6807\u3001\u8bed\u4e49\u6587\u672c\u5206\u6790\u548c\u4e13\u5bb6\u4e34\u5e8a\u76f8\u5173\u6027\u8bc4\u5206\u3002", "result": "\u5bf910\u4e2a\u7ec4\u4ef6\u6846\u67b6\u30013\u4e2a\u7edf\u4e00\u6a21\u578b\u548c5\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "MedGEN-Bench\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u57fa\u51c6\u548c\u521b\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u533b\u5b66AI\u5728\u56fe\u50cf\u751f\u6210\u548c\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u7814\u7a76\u3002"}}
{"id": "2511.13138", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13138", "abs": "https://arxiv.org/abs/2511.13138", "authors": ["Longhui Zheng", "Qiming Xia", "Xiaolu Chen", "Zhaoliang Liu", "Chenglu Wen"], "title": "WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection", "comment": "9 pages, 3 figures,", "summary": "3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.", "AI": {"tldr": "WinMamba\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eMamba\u76843D\u7279\u5f81\u7f16\u7801\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u7a97\u53e3\u81ea\u9002\u5e94\u548c\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\u6765\u63d0\u9ad83D\u76ee\u6807\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u76843D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6355\u6349\u957f\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\u6027\u4e4b\u95f4\u5b58\u5728\u6311\u6218\u3002Mamba\u6a21\u578b\u867d\u7136\u80fd\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u6027\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u4e8eMamba\u7684\u65b9\u6cd5\u5728\u626b\u63cf\u65f6\u4f1a\u4e22\u5931\u7a7a\u95f4\u4fe1\u606f\u3002", "method": "\u63d0\u51faWinMamba\u9aa8\u5e72\u7f51\u7edc\uff0c\u5305\u542b\u5806\u53e0\u7684WinMamba\u5757\u3002WinMamba\u5757\u5305\u542b\u4e00\u4e2a\u7a97\u53e3\u81ea\u9002\u5e94\u6a21\u5757\uff08WSF\uff09\u6765\u8865\u507f\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\u548c\u7a97\u53e3\u79fb\u4f4d\u7b56\u7565\uff08AWF\uff09\u6765\u83b7\u53d6\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "result": "\u5728KITTI\u548cWaymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWinMamba\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86WSF\u548cAWF\u6a21\u5757\u5bf9\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u8d21\u732e\u3002", "conclusion": "WinMamba\u901a\u8fc7\u5176\u65b0\u9896\u7684\u8bbe\u8ba1\uff0c\u57283D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u826f\u597d\u5e73\u8861\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.13145", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13145", "abs": "https://arxiv.org/abs/2511.13145", "authors": ["Cesar Portocarrero Rodriguez", "Laura Vandeweyen", "Yosuke Yamamoto"], "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks", "comment": null, "summary": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.", "AI": {"tldr": "\u7f8e\u56fd\u57fa\u7840\u8bbe\u65bd\u72b6\u51b5\u4e0d\u4f73\uff0c\u9053\u8def\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u9879\u76ee\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u7279\u522b\u662f\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\uff0c\u4ee5\u53caMaskFormer\u6a21\u578b\uff0c\u5bf9\u9053\u8def\u75c5\u5bb3\u8fdb\u884c\u5206\u5272\uff0c\u65e8\u5728\u63d0\u9ad8\u9053\u8def\u76d1\u6d4b\u6548\u7387\u548c\u6570\u636e\u51c6\u786e\u6027\u3002", "motivation": "\u9053\u8def\u662f\u533a\u57df\u7ecf\u6d4e\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u76ee\u524d\u7684\u4eba\u5de5\u6216\u6fc0\u5149\u68c0\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002\u672c\u9879\u76ee\u65e8\u5728\u5229\u7528\u65e5\u76ca\u589e\u957f\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5b9e\u65f6\u89c6\u89c9\u6570\u636e\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u6539\u8fdb\u9053\u8def\u76d1\u6d4b\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u672c\u9879\u76ee\u9996\u5148\u8bc4\u4f30\u4e86\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\u3002\u968f\u540e\uff0c\u5e94\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u8fdb\u884c\u9053\u8def\u75c5\u5bb3\u5206\u5272\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578bMaskFormer\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cGAN\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u9053\u8def\u75c5\u5bb3\u5206\u5272\u4efb\u52a1\u4e2d\uff0cMaskFormer\u6a21\u578b\u5728mAP50\u548cIoU\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8eCNN\u6a21\u578b\u3002", "conclusion": "\u5229\u7528GAN\u751f\u6210\u6570\u636e\u548cMaskFormer\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u9053\u8def\u75c5\u5bb3\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6539\u5584\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.13150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13150", "abs": "https://arxiv.org/abs/2511.13150", "authors": ["Rifen Lin", "Alex Jinpeng Wang", "Jiawei Mo", "Min Li"], "title": "Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification", "comment": null, "summary": "Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u9aa8\u9abc\u7684\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u7684\u9884\u8bad\u7ec3\u6846\u67b6CSIP-ReID\uff0c\u901a\u8fc7\u9aa8\u9abc\u5e8f\u5217\u548c\u89c6\u9891\u5e27\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u539f\u578b\u878d\u5408\u548c\u9aa8\u9abc\u5f15\u5bfc\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u6709\u6548\u6355\u6349\u8fd0\u52a8\u548c\u5916\u89c2\u7ebf\u7d22\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u80fd\u5728\u4ec5\u9aa8\u9abc\u7684ReID\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u9891-\u6587\u672c\u7684\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u771f\u6b63\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4e0d\u8db3\u4ee5\u53ca\u6587\u672c\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSIP-ReID\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u9aa8\u9abc\u548c\u89c6\u89c9\u7279\u5f81\u5e8f\u5217\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u52a8\u6001\u539f\u578b\u878d\u5408\u66f4\u65b0\u5668\uff08PFU\uff09\u6765\u878d\u5408\u8fd0\u52a8\u548c\u5916\u89c2\u7ebf\u7d22\uff0c\u5e76\u63d0\u51fa\u9aa8\u9abc\u5f15\u5bfc\u65f6\u95f4\u5efa\u6a21\uff08SGTM\uff09\u6a21\u5757\u6765\u63d0\u53d6\u548c\u878d\u5408\u9aa8\u9abc\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u7ebf\u7d22\u3002", "result": "CSIP-ReID\u5728MARS\u3001LS-VID\u3001iLIDS-VID\u7b49\u89c6\u9891ReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u5e76\u5728BIWI\u3001IAS\u7b49\u9aa8\u9abcReID\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "CSIP-ReID\u5f00\u521b\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u3001\u611f\u77e5\u8fd0\u52a8\u7684ReID\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.13168", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13168", "abs": "https://arxiv.org/abs/2511.13168", "authors": ["Haodong Wang", "Tao Zhuo", "Xiuwei Zhang", "Hanlin Yin", "Wencong Wu", "Yanning Zhang"], "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration", "comment": null, "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.", "AI": {"tldr": "SOMA\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u7ed3\u6784\u68af\u5ea6\u5148\u9a8c\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u68af\u5ea6\u589e\u5f3a\u5668\uff08FGE\uff09\u548c\u5168\u5c40-\u5c40\u90e8\u4eff\u5c04\u6d41\u5339\u914d\u5668\uff08GLAM\uff09\u6765\u63d0\u5347SAR\u548c\u5149\u5b66\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u914d\u51c6\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728SAR\u548c\u5149\u5b66\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\uff0c\u4f20\u7edf\u68af\u5ea6\u4fe1\u606f\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u672a\u88ab\u6709\u6548\u5229\u7528\uff0c\u9650\u5236\u4e86\u914d\u51c6\u6027\u80fd\u3002", "method": "\u63d0\u51faSOMA\u6846\u67b6\uff0c\u5305\u542b\u7279\u5f81\u68af\u5ea6\u589e\u5f3a\u5668\uff08FGE\uff09\u548c\u5168\u5c40-\u5c40\u90e8\u4eff\u5c04\u6d41\u5339\u914d\u5668\uff08GLAM\uff09\u3002FGE\u901a\u8fc7\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u68af\u5ea6\u6ee4\u6ce2\u5668\u589e\u5f3a\u7279\u5f81\u533a\u5206\u5ea6\uff1bGLAM\u7ed3\u5408\u4eff\u5c04\u53d8\u6362\u548c\u6d41\u573a\u7ec6\u5316\uff0c\u5b9e\u73b0\u7c97\u5230\u7cbe\u7684\u914d\u51c6\u3002", "result": "SOMA\u5728SEN1-2\u6570\u636e\u96c6\u4e0a\u5c06CMR@1px\u63d0\u9ad8\u4e8612.29%\uff0c\u5728GFGE_SO\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e8618.50%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u7cbe\u5ea6\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SOMA\u6846\u67b6\u901a\u8fc7\u6709\u6548\u878d\u5408\u68af\u5ea6\u4fe1\u606f\u548c\u6df1\u5ea6\u7279\u5f81\uff0c\u514b\u670d\u4e86SAR\u548c\u5149\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u548c\u6cdb\u5316\u7684\u914d\u51c6\u6548\u679c\u3002"}}
{"id": "2511.13170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13170", "abs": "https://arxiv.org/abs/2511.13170", "authors": ["Zahra Tabatabaei", "Jon Sporring"], "title": "THIR: Topological Histopathological Image Retrieval", "comment": null, "summary": "According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.\n  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.", "AI": {"tldr": "THIR\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u5185\u5bb9\u7684\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08\u7279\u522b\u662f\u8d1d\u8482\u6570\uff09\u6765\u68c0\u7d22\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u65e0\u9700\u76d1\u7763\u5b66\u4e60\uff0c\u901f\u5ea6\u5feb\uff0c\u6548\u679c\u597d\u3002", "motivation": "\u65e9\u671f\u8bca\u65ad\u548c\u51c6\u786e\u7684\u4e34\u5e8a\u51b3\u7b56\u5bf9\u4e8e\u964d\u4f4e\u4e73\u817a\u764c\u7684\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTHIR\u6846\u67b6\uff0c\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08\u8d1d\u8482\u6570\uff09\u4eceRGB\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u7279\u5f81\u7684\u8ddd\u79bb\u6765\u8fdb\u884c\u56fe\u50cf\u68c0\u7d22\u3002", "result": "\u5728BreaKHis\u6570\u636e\u96c6\u4e0a\uff0cTHIR\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6807\u51c6CPU\u4e0a\u5904\u7406\u6574\u4e2a\u6570\u636e\u96c6\u4ec5\u970020\u5206\u949f\u3002", "conclusion": "THIR\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u4e34\u5e8a\u56fe\u50cf\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13175", "abs": "https://arxiv.org/abs/2511.13175", "authors": ["Chao Yang", "Boqian Zhang", "Jinghao Xu", "Guang Jiang"], "title": "HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution", "comment": null, "summary": "Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.", "AI": {"tldr": "HDW-SR\u5229\u7528\u5c0f\u6ce2\u5206\u89e3\u548c\u9ad8\u9891\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u89e3\u51b3\u4e86\u5355\u4e00\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u6a21\u7cca\u7ec6\u8282\u95ee\u9898\uff0c\u901a\u8fc7\u4ec5\u5728\u6b8b\u5dee\u56fe\u4e0a\u8fdb\u884c\u6269\u6563\u5e76\u4f7f\u7528\u5c0f\u6ce2\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5206\u89e3\uff0c\u4ece\u800c\u6709\u6548\u6062\u590d\u9ad8\u9891\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5355\u4e00\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u5bb9\u6613\u56e0\u9ad8\u9891\u57df\u5f15\u5bfc\u4e0d\u8db3\u5bfc\u81f4\u7ec6\u8282\u6a21\u7cca\u3002HDW-SR\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u9ad8\u9891\u5f15\u5bfc\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "HDW-SR\u4f7f\u7528\u57fa\u4e8e\u5c0f\u6ce2\u5206\u89e3\u7684U-Net\u66ff\u4ee3\u4f20\u7edf\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ec5\u5bf9\u6b8b\u5dee\u56fe\u8fdb\u884c\u6269\u6563\u4ee5\u805a\u7126\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u3002\u5b83\u91c7\u7528\u5c0f\u6ce2\u4e0b\u91c7\u6837\u8fdb\u884c\u591a\u5c3a\u5ea6\u9891\u7387\u5206\u89e3\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5bfc\u9ad8\u9891\u5b50\u5e26\u548c\u4f4e\u9891\u5b50\u5e26\u7684\u4ea4\u4e92\uff0c\u540c\u65f6\u5f15\u5165\u52a8\u6001\u9608\u503c\u5757\uff08DTB\uff09\u4f18\u5316\u9ad8\u9891\u9009\u62e9\u3002\u5c0f\u6ce2\u53d8\u6362\u7684\u53ef\u9006\u6027\u4fdd\u8bc1\u4e86\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7279\u5f81\u7684\u4f4e\u635f\u8017\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHDW-SR\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6062\u590d\u7cbe\u7ec6\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "HDW-SR\u901a\u8fc7\u9ad8\u9891\u5f15\u5bfc\u548c\u591a\u5c3a\u5ea6\u5206\u6790\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728SISR\u4e2d\u6062\u590d\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13183", "abs": "https://arxiv.org/abs/2511.13183", "authors": ["Alec Sargood", "Lemuel Puglisi", "Elinor Thompson", "Mirco Musolesi", "Daniel C. Alexander"], "title": "GenTract: Generative Global Tractography", "comment": null, "summary": "Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.", "AI": {"tldr": "GenTract\u662f\u4e00\u4e2a\u9996\u521b\u7684\u5168\u5c40\u8ffd\u8e2a\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u4ecedMRI\u6570\u636e\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u7ea4\u7ef4\u675f\uff0c\u63d0\u9ad8\u4e86\u8ffd\u8e2a\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u73af\u5883\u4e0b\u3002", "motivation": "\u4f20\u7edf\u7684\u5c40\u90e8\u8ffd\u8e2a\u65b9\u6cd5\u5bb9\u6613\u51fa\u9519\uff0c\u800c\u5168\u5c40\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u8ffd\u8e2a\u65b9\u6cd5\u3002", "method": "GenTract\u5c06\u8ffd\u8e2a\u89c6\u4e3a\u4e00\u4e2a\u751f\u6210\u4efb\u52a1\uff0c\u5b66\u4e60\u4ecedMRI\u5230\u5b8c\u6574\u3001\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u7ea4\u7ef4\u675f\u7684\u76f4\u63a5\u6620\u5c04\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u65b9\u6cd5\u3002", "result": "GenTract\u7684\u7cbe\u786e\u7387\u662f\u6b21\u4f18\u65b9\u6cd5TractOracle\u76842.1\u500d\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u6570\u636e\u4e0b\uff0c\u5176\u6027\u80fd\u6bd4\u7ade\u4e89\u65b9\u6cd5\u9ad8\u51fa\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "GenTract\u5728\u7814\u7a76\u7ea7\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u5904\u7406\u4e0d\u5b8c\u7f8e\u3001\u4f4e\u5206\u8fa8\u7387\u7684\u6570\u636e\uff0c\u4e3a\u5168\u5c40\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13189", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13189", "abs": "https://arxiv.org/abs/2511.13189", "authors": ["Diego Ortego", "Marlon Rodr\u00edguez", "Mario Almagro", "Kunal Dahiya", "David Jim\u00e9nez", "Juan C. SanMiguel"], "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework", "comment": "To appear at AAAI 2026", "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\uff08XMC\uff09\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\uff0cViXML\u6846\u67b6\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\uff08XMC\uff09\u4efb\u52a1\u4e2d\u6709\u6548\u5229\u7528\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faViXML\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\u4e0e\u901a\u8fc7\u6c60\u5316\u5355\u4e2a\u56fe\u50cf\u5d4c\u5165\u6765\u6574\u5408\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4ee5\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u5e76\u5b9e\u73b0\u591a\u6a21\u6001\u80fd\u529b\u3002", "result": "ViXML\u6846\u67b6\u5728P@1\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5728\u6700\u5927\u6570\u636e\u96c6\u4e0a\u7684\u63d0\u5347\u9ad8\u8fbe+8.21%\u3002\u5373\u4f7f\u662f\u5c0f\u578b\u7f16\u7801\u5668\uff0cViXML\u4e5f\u5e38\u5e38\u4f18\u4e8e\u7eaf\u6587\u672c\u89e3\u7801\u5668\u3002\u5f15\u5165\u4e86\u5229\u7528\u89c6\u89c9\u5143\u6570\u636e\u7684\u6269\u5c55\u6587\u672c\u6570\u636e\u96c6\u3002", "conclusion": "\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\u5bf9\u4e8eXMC\u81f3\u5173\u91cd\u8981\uff0cViXML\u6846\u67b6\u6709\u6548\u5730\u7ed3\u5408\u4e86\u8fd9\u4e24\u8005\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13190", "abs": "https://arxiv.org/abs/2511.13190", "authors": ["Haoran Tang", "Meng Cao", "Ruyang Liu", "Xiaoxi Liang", "Linglong Li", "Ge Li", "Xiaodan Liang"], "title": "Video Spatial Reasoning with Object-Centric 3D Rollout", "comment": null, "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).", "AI": {"tldr": "OCR\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5bf93D\u51e0\u4f55\u8fdb\u884c\u7ed3\u6784\u5316\u6270\u52a8\u6765\u6539\u8fdb\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6574\u4e2a\u573a\u666f\u7684\u6574\u4f53\u63a8\u7406\uff0c\u5e76\u5728VSI-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u5f80\u5f80\u53ea\u5173\u6ce8\u63d0\u793a\u4e2d\u660e\u786e\u63d0\u5230\u7684\u7269\u4f53\uff0c\u800c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOCR\uff08Object-Centric 3D Rollout\uff09\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5bf9\u9009\u5b9a\u7269\u4f53\u76843D\u51e0\u4f55\u8fdb\u884c\u7ed3\u6784\u5316\u6270\u52a8\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6eda\u52a8\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5171\u540c\u4f18\u5316\u7a7a\u95f4\u63a8\u7406\u8f68\u8ff9\u3002", "result": "OCR\u5728VSI-Bench\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51763B\u53c2\u6570\u6a21\u578b\u8fbe\u5230\u4e8647.5%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u51e0\u4e2a7B\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86OCR\u4f18\u4e8e\u4e4b\u524d\u7684\u6eda\u52a8\u7b56\u7565\u3002", "conclusion": "OCR\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8MLLMs\u5728\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u52a8\u60013D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u76f8\u4e92\u5173\u7cfb\u3002"}}
{"id": "2511.13191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13191", "abs": "https://arxiv.org/abs/2511.13191", "authors": ["Ying Jiang", "Jiayin Lu", "Yunuo Chen", "Yumeng He", "Kui Wu", "Yin Yang", "Chenfanfu Jiang"], "title": "Birth of a Painting: Differentiable Brushstroke Reconstruction", "comment": "13 pages", "summary": "Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u7b14\u89e6\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u903c\u771f\u5730\u6a21\u62df\u4eba\u7c7b\u7ed8\u753b\u548c\u6d82\u62b9\u8fc7\u7a0b\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u6d41\u7545\u7684\u8272\u8c03\u8fc7\u6e21\u548c\u4e30\u5bcc\u98ce\u683c\u5316\u5916\u89c2\u7684\u6570\u5b57\u7ed8\u753b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u7ed8\u753b\u5408\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u663e\u5f0f\u5904\u7406\u7b14\u89e6\u7ed3\u6784\uff0c\u4e14\u96be\u4ee5\u751f\u6210\u5e73\u6ed1\u81ea\u7136\u7684\u9634\u5f71\u6548\u679c\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u7b14\u89e6\u91cd\u5efa\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u5e76\u884c\u53ef\u5fae\u5206\u7ed8\u753b\u6e32\u67d3\u5668\u4f18\u5316\u8d1d\u585e\u5c14\u66f2\u7ebf\u7b14\u89e6\uff0c\u7136\u540e\u5229\u7528\u98ce\u683c\u751f\u6210\u6a21\u5757\u5408\u6210\u51e0\u4f55\u6761\u4ef6\u7eb9\u7406\uff0c\u5e76\u5f15\u5165\u53ef\u5fae\u5206\u6d82\u62b9\u7b97\u5b50\u6765\u5b9e\u73b0\u989c\u8272\u6df7\u5408\u548c\u9634\u5f71\u6548\u679c\uff0c\u6700\u540e\u7ed3\u5408\u7c97\u5230\u7cbe\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5728\u51e0\u4f55\u548c\u8bed\u4e49\u5f15\u5bfc\u4e0b\u8054\u5408\u4f18\u5316\u7b14\u89e6\u51e0\u4f55\u3001\u989c\u8272\u548c\u7eb9\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6cb9\u753b\u3001\u6c34\u5f69\u753b\u3001\u58a8\u6c34\u753b\u548c\u6570\u5b57\u7ed8\u753b\u7b49\u591a\u79cd\u7c7b\u578b\u753b\u4f5c\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u903c\u771f\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7b14\u89e6\u91cd\u5efa\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7684\u8272\u8c03\u8fc7\u6e21\u548c\u4e30\u5bcc\u7684\u98ce\u683c\u5316\u5916\u89c2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8868\u8fbe\u6027\u6570\u5b57\u7ed8\u753b\u521b\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u80fd\u591f\u771f\u5b9e\u5730\u518d\u73b0\u7ed8\u753b-\u6d82\u62b9\u5faa\u73af\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ed8\u753b\u4f5c\u54c1\u3002"}}
{"id": "2511.13195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13195", "abs": "https://arxiv.org/abs/2511.13195", "authors": ["Soyul Lee", "Seungmin Baek", "Dongbo Min"], "title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection", "comment": "AAAI 2026 accepted", "summary": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.", "AI": {"tldr": "MonoDLGD\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u6807\u7b7e\u5f15\u5bfc\u53bb\u566a\uff0c\u81ea\u9002\u5e94\u6270\u52a8\u548c\u91cd\u5efa\u5730\u9762\u771f\u5b9e\u6807\u7b7e\uff0c\u4ee5\u63d0\u9ad8\u5355\u76ee3D\u5bf9\u8c61\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5355\u76ee3D\u5bf9\u8c61\u68c0\u6d4b\u6210\u672c\u4f4e\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6\u7ebf\u7d22\u7684\u6a21\u7cca\u6027\u800c\u672c\u8d28\u4e0a\u662f\u4e0d\u9002\u5b9a\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u4ee5\u53ca\u906e\u6321\u3001\u8ddd\u79bb\u548c\u622a\u65ad\u7b49\u5b9e\u4f8b\u7ea7\u68c0\u6d4b\u96be\u5ea6\u3002", "method": "MonoDLGD\u6846\u67b6\u81ea\u9002\u5e94\u5730\u6839\u636e\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u6270\u52a8\u548c\u91cd\u5efa\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u3002\u5b83\u5bf9\u66f4\u7b80\u5355\u7684\u5b9e\u4f8b\u5e94\u7528\u66f4\u5f3a\u7684\u6270\u52a8\uff0c\u5bf9\u66f4\u96be\u7684\u5b9e\u4f8b\u5e94\u7528\u66f4\u5f31\u7684\u6270\u52a8\uff0c\u7136\u540e\u8fdb\u884c\u91cd\u5efa\u4ee5\u63d0\u4f9b\u663e\u5f0f\u7684\u51e0\u4f55\u76d1\u7763\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6807\u7b7e\u91cd\u5efa\u548c3D\u5bf9\u8c61\u68c0\u6d4b\uff0c\u9f13\u52b1\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMonoDLGD\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MonoDLGD\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u6807\u7b7e\u5f15\u5bfc\u53bb\u566a\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5355\u76ee3D\u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u5b9e\u4f8b\u7ea7\u68c0\u6d4b\u96be\u5ea6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13197", "abs": "https://arxiv.org/abs/2511.13197", "authors": ["Alberto Gomez", "Jorge Oliveira", "Ramon Casero", "Agis Chartsias"], "title": "Self-Supervised Ultrasound Screen Detection", "comment": "Submitted to ISBI 2026", "summary": "Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.", "AI": {"tldr": "\u4ece\u8d85\u58f0\u4eea\u663e\u793a\u5668\u7167\u7247\u4e2d\u63d0\u53d6\u8d85\u58f0\u56fe\u50cf\uff0c\u4ee5\u7ed5\u8fc7DICOM\u9650\u5236\uff0c\u5e76\u5141\u8bb8\u5feb\u901f\u7684\u7b97\u6cd5\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8d85\u58f0\u56fe\u50cf\u4f20\u8f93\u5230\u533b\u9662\u7cfb\u7edf\u7684DICOM\u9650\u5236\uff0c\u5e76\u4e3a\u65b0\u7b97\u6cd5\u7684\u5feb\u901f\u6d4b\u8bd5\u548c\u539f\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4fbf\u5229\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u76d1\u7763\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4ece\u8d85\u58f0\u4eea\u663e\u793a\u5668\u7167\u7247\u4e2d\u63d0\u53d6\u548c\u6821\u6b63\u8d85\u58f0\u56fe\u50cf\u3002", "result": "\u5728\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e2d\uff0c\u6821\u6b63\u540e\u7684\u56fe\u50cf\u5728\u5206\u7c7b\u5fc3\u810f\u89c6\u56fe\u65b9\u9762\u8fbe\u5230\u4e860.79\u7684\u5e73\u8861\u51c6\u786e\u7387\uff08\u4e0e\u672c\u5730DICOM\u56fe\u50cf\u76f8\u6bd4\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u6d41\u6c34\u7ebf\u80fd\u591f\u4ece\u663e\u793a\u5668\u7167\u7247\u4e2d\u63d0\u53d6\u51fa\u89c6\u89c9\u4fdd\u771f\u5ea6\u8db3\u591f\u7684\u8d85\u58f0\u56fe\u50cf\uff0c\u4ece\u800c\u652f\u6301\u7b97\u6cd5\u7684\u5feb\u901f\u6d4b\u8bd5\u548c\u5f00\u53d1\u3002"}}
{"id": "2511.13204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13204", "abs": "https://arxiv.org/abs/2511.13204", "authors": ["Junhee Lee", "ChaeBeen Bang", "MyoungChul Kim", "MyeongAh Cho"], "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection", "comment": "Accepted to AAAI 2026", "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.", "AI": {"tldr": "RefineVAD \u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u4fe1\u606f\u548c\u8bed\u4e49\u7c7b\u522b\u6765\u6539\u8fdb\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5c06\u6240\u6709\u5f02\u5e38\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u5f02\u5e38\u7684\u591a\u6837\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u6a21\u62df\u4eba\u7c7b\u7684\u5206\u6790\u65b9\u5f0f\uff0c\u901a\u8fc7\u8054\u5408\u5206\u6790\u8fd0\u52a8\u6a21\u5f0f\u548c\u8bed\u4e49\u7ed3\u6784\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a RefineVAD \u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1. \u8fd0\u52a8\u611f\u77e5\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u518d\u6821\u51c6 (MoTAR)\uff1a\u4f30\u8ba1\u8fd0\u52a8\u663e\u8457\u6027\u5e76\u901a\u8fc7\u57fa\u4e8e\u79fb\u4f4d\u7684\u6ce8\u610f\u529b\u548c\u57fa\u4e8e Transformer \u7684\u5168\u5c40\u6a21\u578b\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u7126\u70b9\u3002 2. \u7c7b\u522b\u5bfc\u5411\u4f18\u5316 (CORE)\uff1a\u901a\u8fc7\u5c06\u7247\u6bb5\u7ea7\u7279\u5f81\u4e0e\u53ef\u5b66\u4e60\u7684\u7c7b\u522b\u539f\u578b\u5bf9\u9f50\uff0c\u5c06\u8f6f\u5f02\u5e38\u7c7b\u522b\u5148\u9a8c\u6ce8\u5165\u5230\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728 WVAD \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86 RefineVAD \u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6574\u5408\u8bed\u4e49\u4e0a\u4e0b\u6587\u6765\u6307\u5bfc\u7279\u5f81\u4f18\u5316\u4ee5\u83b7\u5f97\u4e0e\u5f02\u5e38\u76f8\u5173\u7684\u6a21\u5f0f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "RefineVAD \u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u6f14\u53d8\u65b9\u5f0f\u548c\u5176\u6240\u5c5e\u7684\u8bed\u4e49\u7c7b\u522b\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6574\u5408\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u7279\u5f81\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.13208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13208", "abs": "https://arxiv.org/abs/2511.13208", "authors": ["Yonghui Yu", "Jiahang Cai", "Xun Wang", "Wenwu Yang"], "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer", "comment": null, "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet", "AI": {"tldr": "PAVE-Net\u662f\u4e00\u4e2a\u5168\u7aef\u5230\u7aef\u7684\u89c6\u9891\u591a\u89c6\u89d22D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u59ff\u6001\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4e86\u8de8\u5e27\u4e2a\u4f53\u5173\u8054\u95ee\u9898\uff0c\u5e76\u5728PoseTrack2017\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u591a\u89c6\u89d22D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u68c0\u6d4b\u3001RoI\u88c1\u526a\u548cNMS\u7b49\u542f\u53d1\u5f0f\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5168\u7aef\u5230\u7aef\u7684\u65b9\u6cd5\uff0c\u6d88\u9664\u8fd9\u4e9b\u64cd\u4f5c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PAVE-Net\uff0c\u4e00\u4e2a\u5305\u542b\u7a7a\u95f4\u7f16\u7801\u5668\uff08\u7528\u4e8e\u5efa\u6a21\u5e27\u5185\u5173\u7cfb\uff09\u548c\u65f6\u7a7a\u59ff\u6001\u89e3\u7801\u5668\uff08\u7528\u4e8e\u6355\u6349\u8de8\u5e27\u5168\u5c40\u4f9d\u8d56\uff09\u7684\u6846\u67b6\u3002\u901a\u8fc7\u59ff\u6001\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u5e27\u4e2a\u4f53\u5173\u8054\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u59ff\u6001\u5173\u952e\u70b9\u4e4b\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002", "result": "PAVE-Net\u5728PoseTrack2017\u4e0a\u6bd4\u4e4b\u524d\u7684\u7aef\u5230\u7aef\u56fe\u50cf\u65b9\u6cd5\u63d0\u9ad8\u4e866.0 mAP\uff0c\u51c6\u786e\u6027\u4e0e\u6700\u5148\u8fdb\u7684\u4e24\u9636\u6bb5\u89c6\u9891\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "PAVE-Net\u662f\u9996\u4e2a\u7528\u4e8e\u591a\u89c6\u89d22D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u5168\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u8de8\u5e27\u4e2a\u4f53\u5173\u8054\u95ee\u9898\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.13211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13211", "abs": "https://arxiv.org/abs/2511.13211", "authors": ["Yijia Fan", "Jusheng Zhang", "Kaitong Cai", "Jing Yang", "Jian Wang", "Keze Wang"], "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale", "comment": null, "summary": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.", "AI": {"tldr": "3DAlign-DAER\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u7b56\u7565\u548c\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\u6765\u5bf9\u9f50\u6587\u672c\u548c3D\u51e0\u4f55\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u5927\u89c4\u6a21\u6570\u636e\u5e93\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728Align3D-2M\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u6587\u672c-3D\u51e0\u4f55\u5bf9\u9f50\u548c\u5927\u89c4\u6a21\u6570\u636e\u5e93\u68c0\u7d22\u65b9\u9762\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u7b56\u7565\uff08DAP\uff09\uff0c\u5305\u62ec\u5206\u5c42\u6ce8\u610f\u529b\u878d\u5408\uff08HAF\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u4ee5\u5b66\u4e60\u7ec6\u7c92\u5ea6\u6ce8\u610f\u529b\u6743\u91cd\u3002\u5f15\u5165\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\uff08ERS\uff09\u4ee5\u5728\u5927\u89c4\u6a21\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u6548\u68c0\u7d22\u3002", "result": "\u5728Align3D-2M\uff08200\u4e07\u6587\u672c-3D\u5bf9\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e863DAlign-DAER\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "3DAlign-DAER\u5728\u7ec6\u7c92\u5ea6\u6587\u672c-3D\u5bf9\u9f50\u548c\u5927\u89c4\u6a21\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u7b56\u7565\u548c\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13222", "abs": "https://arxiv.org/abs/2511.13222", "authors": ["Qida Tan", "Hongyu Yang", "Wenchao Du"], "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation", "comment": "AAAI2026", "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.", "AI": {"tldr": "HARL\u6846\u67b6\u901a\u8fc7\u591a\u6e90\u6df7\u5408\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u6ce8\u89c6\u8868\u5f81\uff0c\u89e3\u51b3\u4e86\u8de8\u57df\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5916\u89c2\u7684\u6ce8\u89c6\u4f30\u8ba1\u65b9\u6cd5\u5728\u8de8\u57df\u8bc4\u4f30\u65f6\uff0c\u4f1a\u53d7\u5230\u8868\u60c5\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u56fe\u50cf\u8d28\u91cf\u7b49\u65e0\u5173\u56e0\u7d20\u7684\u5e72\u6270\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faHARL\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5bf9\u9f50\u9ad8\u8d28\u91cf\u8fd1\u773c\u56fe\u50cf\u548c\u4f4e\u8d28\u91cf\u9762\u90e8\u56fe\u50cf\u7684\u7279\u5f81\uff0c\u89e3\u8026\u6ce8\u89c6\u76f8\u5173\u8868\u5f81\u3002\u8bbe\u8ba1\u7a00\u758f\u56fe\u878d\u5408\u6a21\u5757\uff0c\u5229\u7528\u5934\u90e8\u59ff\u6001\u4e0e\u6ce8\u89c6\u65b9\u5411\u4e4b\u95f4\u7684\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u5728EyeDiap\u3001MPIIFaceGaze\u548cGaze360\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52305.02\u00b0\u30013.36\u00b0\u548c9.26\u00b0\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "HARL\u6846\u67b6\u80fd\u591f\u5b66\u4e60\u9c81\u68d2\u7684\u6ce8\u89c6\u8868\u5f81\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8de8\u57df\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2511.13232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13232", "abs": "https://arxiv.org/abs/2511.13232", "authors": ["Malek Al Abed", "Sebiha Demir", "Anne Groteklaes", "Elodie Germani", "Shahrooz Faghihroohi", "Hemmen Sabir", "Shadi Albarqouni"], "title": "MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI", "comment": "5 pages, 4 figures", "summary": "Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.", "AI": {"tldr": "MRIQT\u662f\u4e00\u4e2a3D\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u8d85\u4f4e\u573aMRI\uff08uLF-MRI\uff09\u56fe\u50cf\u63d0\u5347\u81f3\u9ad8\u573a\uff08HF\uff09MRI\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86uLF-MRI\u4fe1\u566a\u6bd4\u4f4e\u548c\u8bca\u65ad\u8d28\u91cf\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4fbf\u643a\u5f0f\u8d85\u4f4e\u573aMRI\uff08uLF-MRI\uff09\u867d\u7136\u80fd\u7528\u4e8e\u65b0\u751f\u513f\u62a4\u7406\uff0c\u4f46\u5176\u4fe1\u53f7\u566a\u58f0\u6bd4\u4f4e\uff0c\u8bca\u65ad\u8d28\u91cf\u4e0d\u5982\u9ad8\u573a\uff08HF\uff09MRI\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "MRIQT\u662f\u4e00\u4e2a3D\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7528\u4e8e\u7269\u7406\u4e00\u81f4\u6027uLF\u6a21\u62df\u7684k\u7a7a\u95f4\u9000\u5316\uff0c\u7528\u4e8e\u7a33\u5b9a\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u7684v-\u9884\u6d4b\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u4ee5\u53ca\u7528\u4e8e\u89e3\u5256\u4fdd\u771f\u5ea6\u7684\u4fe1\u566a\u6bd4\u52a0\u67433D\u611f\u77e5\u635f\u5931\u3002\u8be5\u6a21\u578b\u5229\u7528\u57fa\u4e8e\u4f53\u6ce8\u610f\u529b\u7684UNet\u67b6\u6784\uff0c\u4ece\u5e26\u566a\u58f0\u7684uLF\u8f93\u5165\u4e2d\u53bb\u566a\uff0c\u540c\u65f6\u4fdd\u7559\u7ed3\u6784\u3002", "result": "MRIQT\u5728PSNR\u4e0a\u6bd4\u6700\u65b0\u7684GAN\u548cCNN\u57fa\u7ebf\u63d0\u9ad8\u4e8615.3%\uff0c\u5e76\u4e14\u572830.7%\u7684\u57fa\u7ebf\u4e0a\u63d0\u9ad8\u4e861.78%\u3002\u533b\u751f\u8bc4\u4f30\u517685%\u7684\u8f93\u51fa\u5177\u6709\u826f\u597d\u7684\u8d28\u91cf\uff0c\u5e76\u4e14\u75c5\u7406\u6e05\u6670\u53ef\u89c1\u3002", "conclusion": "MRIQT\u80fd\u591f\u57fa\u4e8e\u6269\u6563\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u4fbf\u643a\u5f0f\u8d85\u4f4e\u573a\uff08uLF\uff09MRI\u589e\u5f3a\uff0c\u7528\u4e8e\u53ef\u9760\u7684\u65b0\u751f\u513f\u8111\u8bc4\u4f30\u3002"}}
{"id": "2511.13242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13242", "abs": "https://arxiv.org/abs/2511.13242", "authors": ["Junjie Wu", "Guohong Fu"], "title": "MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection", "comment": null, "summary": "Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.", "AI": {"tldr": "MMD-Thinker\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u7ef4\u5ea6\u601d\u8003\u6765\u68c0\u6d4b\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5b9a\u5236\u601d\u7ef4\u6a21\u5f0f\u3001\u6307\u4ee4\u8c03\u4f18\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u589e\u5f3a\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728MMR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u65f6\u5b58\u5728\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u548c\u63a8\u7406\u504f\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u4f4e\u6210\u672c\u3001\u9ad8\u6b3a\u9a97\u6027\u7684AIGC\u65f6\u4ee3\u7684\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u548c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "MMD-Thinker\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u5f00\u53d1\u9488\u5bf9\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7684\u5b9a\u5236\u601d\u7ef4\u6a21\u5f0f\u30022. \u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u6307\u4ee4\u8c03\u4f18\uff0c\u5c06\u5b9a\u5236\u601d\u7ef4\u6a21\u5f0f\u6ce8\u5165\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u30023. \u5229\u7528\u5177\u6709\u6df7\u5408\u4f18\u52bf\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6fc0\u52b1\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u5305\u542b8K+\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u63a8\u7406\uff08MMR\uff09\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMD-Thinker\u5728\u540c\u57df\u548c\u8de8\u57df\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7075\u6d3b\u7684\u63a8\u7406\u548c\u4ee3\u5e01\u4f7f\u7528\u3002", "conclusion": "MMD-Thinker\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u7ef4\u5ea6\u601d\u8003\u6709\u6548\u5730\u89e3\u51b3\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u63a8\u7406\u80fd\u529b\u548c\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.13249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13249", "abs": "https://arxiv.org/abs/2511.13249", "authors": ["Yu Wen", "Shuyong Gao", "Shuping Zhang", "Miao Huang", "Lili Tao", "Han Yang", "Haozhe Xing", "Lihe Zhang", "Boxue Hou"], "title": "Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention", "comment": "12 pages, 7figures, This work is supported by National Nature Science Foundation of China (Grant No. 62203291)", "summary": "Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.13259", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13259", "abs": "https://arxiv.org/abs/2511.13259", "authors": ["Yushuo Zheng", "Jiangyong Ying", "Huiyu Duan", "Chunyi Li", "Zicheng Zhang", "Jing Liu", "Xiaohong Liu", "Guangtao Zhai"], "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models", "comment": null, "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.", "AI": {"tldr": "GeoX-Bench\u662f\u4e00\u4e2a\u5305\u542b\u5168\u666f-\u536b\u661f\u56fe\u50cf\u5bf9\u548c\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b10,859\u5f20\u5168\u666f-\u536b\u661f\u56fe\u50cf\u5bf9\uff0c\u8986\u76d649\u4e2a\u56fd\u5bb6\u7684128\u4e2a\u57ce\u5e02\uff0c\u4ee5\u53ca755,976\u4e2a\u95ee\u7b54\u5bf9\u3002\u73b0\u6709LMM\u5728\u5730\u7406\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u8de8\u89c6\u89d2\u5730\u7406\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u80fd\u529b\u5728\u5bfc\u822a\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u6237\u5916\u673a\u5668\u4eba\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51faGeoX-Bench\u57fa\u51c6\uff0c\u5305\u542b10,859\u5f20\u5168\u666f-\u536b\u661f\u56fe\u50cf\u5bf9\uff08\u8986\u76d6128\u4e2a\u57ce\u5e02\uff0c49\u4e2a\u56fd\u5bb6\uff09\u548c755,976\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5176\u4e2d42,900\u4e2a\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4f59\u7528\u4e8e\u589e\u5f3aLMM\u80fd\u529b\u3002\u5728GeoX-Bench\u4e0a\u8bc4\u4f3025\u4e2a\u6700\u5148\u8fdb\u7684LMM\uff0c\u5e76\u63a2\u7d22\u6307\u4ee4\u5fae\u8c03\u7684\u589e\u5f3a\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709LMM\u5728\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u66f4\u590d\u6742\u7684\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u6307\u4ee4\u5fae\u8c03LMM\u5728GeoX-Bench\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8de8\u89c6\u89d2\u5730\u7406\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "GeoX-Bench\u4e3a\u8bc4\u4f30LMM\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6307\u4ee4\u5fae\u8c03\u5bf9\u63d0\u5347LMM\u5730\u7406\u611f\u77e5\u80fd\u529b\u7684\u6548\u679c\u3002"}}
{"id": "2511.13261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13261", "abs": "https://arxiv.org/abs/2511.13261", "authors": ["Junlong Li", "Huaiyuan Xu", "Sijie Cheng", "Kejun Wu", "Kim-Hui Yap", "Lap-Pui Chau", "Yi Wang"], "title": "Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges", "comment": "26 pages, 8 figures, 8 tables, Under peer-review", "summary": "Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a EgoProceAssist \u7684\u6982\u5ff5\uff0c\u65e8\u5728\u5229\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3a\u65e5\u5e38\u7a0b\u5e8f\u6027\u4efb\u52a1\u63d0\u4f9b\u5206\u6b65\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u4e3a\u65e5\u5e38\u7a0b\u5e8f\u6027\u4efb\u52a1\u63d0\u4f9b\u5206\u6b65\u652f\u6301\uff0c\u53d7 VLM \u548c\u81ea\u6211\u611f\u77e5\u7814\u7a76\u7684\u63a8\u52a8\u3002", "method": "\u5bf9\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\u8fdb\u884c\u4e86\u5206\u7c7b\uff1a\u81ea\u6211\u611f\u77e5\u7a0b\u5e8f\u6027\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u611f\u77e5\u7a0b\u5e8f\u6027\u5b66\u4e60\u548c\u81ea\u6211\u611f\u77e5\u7a0b\u5e8f\u6027\u95ee\u7b54\u3002\u5bf9\u73b0\u6709\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4e86\u5168\u9762\u5ba1\u67e5\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u5b9e\u9a8c\u548c\u5bf9\u4ee3\u8868\u6027 VLM \u65b9\u6cd5\u7684\u7efc\u5408\u8bc4\u4f30\uff0c\u9610\u8ff0\u4e86 EgoProceAssist \u4e0e\u73b0\u6709 VLM AI \u52a9\u624b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u8bc4\u4f30\u4e86\u4ee3\u8868\u6027\u7684 VLM \u65b9\u6cd5\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u6700\u65b0\u7814\u7a76\u7684\u516c\u5171\u5b58\u50a8\u5e93\u3002"}}
{"id": "2511.13269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13269", "abs": "https://arxiv.org/abs/2511.13269", "authors": ["Lingfeng Zhang", "Yuchen Zhang", "Hongsheng Li", "Haoxiang Fu", "Yingbo Tang", "Hangjun Ye", "Long Chen", "Xiaojun Liang", "Xiaoshuai Hao", "Wenbo Ding"], "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation", "comment": null, "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSpatialSky-Bench\u57fa\u51c6\u548cSky-VLM\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u65e0\u4eba\u673a\uff08UAV\uff09\u5bfc\u822a\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLM\u5728UAV\u5bfc\u822a\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u548c\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faSpatialSky-Bench\u57fa\u51c6\uff0c\u5305\u542b\u73af\u5883\u611f\u77e5\u548c\u573a\u666f\u7406\u89e3\u4e24\u7c7b13\u4e2a\u5b50\u4efb\u52a1\uff08\u5982\u8fb9\u754c\u6846\u3001\u989c\u8272\u3001\u8ddd\u79bb\u3001\u9ad8\u5ea6\u3001\u7740\u9646\u5b89\u5168\u5206\u6790\u7b49\uff09\u3002\u6784\u5efa\u4e86\u5305\u542b1M\u4e2a\u6837\u672c\u7684SpatialSky-Dataset\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u7528\u4e8eUAV\u7a7a\u95f4\u63a8\u7406\u7684Sky-VLM\u6a21\u578b\u3002", "result": "\u5bf9\u4e3b\u6d41VLM\u5728SpatialSky-Bench\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5b83\u4eec\u5728\u590d\u6742\u7684UAV\u5bfc\u822a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002Sky-VLM\u5728\u6240\u6709\u57fa\u51c6\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SpatialSky-Bench\u548cSky-VLM\u7684\u63d0\u51fa\u586b\u8865\u4e86UAV\u9886\u57dfVLM\u7a7a\u95f4\u667a\u80fd\u8bc4\u4f30\u548c\u5e94\u7528\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u9002\u7528\u4e8eUAV\u573a\u666f\u7684VLM\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.13276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13276", "abs": "https://arxiv.org/abs/2511.13276", "authors": ["Noam Tsfaty", "Avishai Weizman", "Liav Cohen", "Moshe Tshuva", "Yehudit Aperstein"], "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models", "comment": "1 figure, 1 table", "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.", "AI": {"tldr": "We propose a dual-backbone framework using convolutional and transformer representations with top-k pooling for detecting rare and diverse anomalies in surveillance videos, achieving 90.7% AUC on the UCF-Crime dataset using only video-level supervision.", "motivation": "The challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision.", "method": "A dual-backbone framework that combines convolutional and transformer representations through top-k pooling.", "result": "Achieved 90.7% AUC on the UCF-Crime dataset.", "conclusion": "The proposed dual-backbone framework is effective for detecting rare and diverse anomalies in surveillance videos with video-level supervision."}}
{"id": "2511.13278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13278", "abs": "https://arxiv.org/abs/2511.13278", "authors": ["Zihan Li", "Tengfei Wang", "Wentian Gan", "Hao Zhan", "Xin Wang", "Zongqian Zhan"], "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting", "comment": null, "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/", "AI": {"tldr": "SF-Recon\u53ef\u4ee5\u76f4\u63a5\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u6a21\u578b\uff0c\u65e0\u9700\u540e\u671f\u7f51\u683c\u7b80\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u8868\u9762\u6a21\u578b\u65f6\u5b58\u5728\u7e41\u7410\u3001\u8d28\u91cf\u654f\u611f\u7b49\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faSF-Recon\u76f4\u63a5\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u8868\u9762\u3002", "method": "\u9996\u5148\u8bad\u7ec33D\u9ad8\u65af\u6cfc\u6e85\u573a\u83b7\u5f97\u89c6\u56fe\u4e00\u81f4\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u6cd5\u7ebf-\u68af\u5ea6\u5f15\u5bfc\u7684\u9ad8\u65af\u4f18\u5316\u63d0\u53d6\u5efa\u7b51\u7ed3\u6784\uff0c\u5e76\u8fdb\u884c\u591a\u89c6\u56fe\u8fb9\u7f18\u4e00\u81f4\u6027\u526a\u679d\uff0c\u6700\u540e\u901a\u8fc7\u591a\u89c6\u56fe\u6df1\u5ea6\u7ea6\u675fDelaunay\u4e09\u89d2\u5316\u751f\u6210\u8f7b\u91cf\u7ea7\u3001\u7ed3\u6784\u5fe0\u5b9e\u7684\u5efa\u7b51\u7f51\u683c\u3002", "result": "SF-Recon\u53ef\u4ee5\u76f4\u63a5\u4ece\u591a\u89c6\u56fe\u5f71\u50cf\u4e2d\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u6a21\u578b\uff0c\u9762\u6570\u548c\u9876\u70b9\u6570\u5927\u5927\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SF-Recon\u662f\u4e00\u79cd\u76f4\u63a5\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u8868\u9762\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u540e\u671f\u7f51\u683c\u7b80\u5316\uff0c\u80fd\u591f\u751f\u6210\u9762\u6570\u548c\u9876\u70b9\u6570\u66f4\u5c11\u4f46\u7ed3\u6784\u5fe0\u5b9e\u7684\u5efa\u7b51\u7f51\u683c\u3002"}}
{"id": "2511.13282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13282", "abs": "https://arxiv.org/abs/2511.13282", "authors": ["Kaiwen Wang", "Kaili Zheng", "Yiming Shi", "Chenyi Guo", "Ji Wu"], "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space", "comment": null, "summary": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDTO\uff08Depth-conditioned Translation Optimization\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u5f20\u56fe\u50cf\u4e2d\u591a\u4eba\u7f51\u683c\u6062\u590d\u7684\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u573a\u666f\u4e2d\u6240\u6709\u4e2a\u4f53\u7684\u76f8\u673a\u7a7a\u95f4\u5e73\u79fb\uff0c\u5229\u7528\u4eba\u4f53\u6d4b\u91cf\u5b66\u5148\u9a8c\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u63d0\u4f9b\u7684\u6df1\u5ea6\u7ebf\u7d22\uff0c\u5728\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u6846\u67b6\u4e0b\u6c42\u89e3\u573a\u666f\u4e00\u81f4\u7684\u7269\u4f53\u653e\u7f6e\u3002\u57fa\u4e8eDTO\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDTO-Humans\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMetric-Aware HMR\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u80fd\u591f\u76f4\u63a5\u4ee5\u5ea6\u91cf\u5c3a\u5ea6\u4f30\u8ba1\u4eba\u4f53\u7f51\u683c\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u5728\u76f8\u5bf9\u6df1\u5ea6\u63a8\u7406\u548c\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u751f\u6210\u91ce\u5916\u591a\u4eba\u7f51\u683c\u4f2a\u5730\u9762\u771f\u5b9e\uff08pGT\uff09\u65f6\uff0c\u901a\u5e38\u5c06\u6bcf\u4e2a\u4eba\u5355\u72ec\u5904\u7406\uff0c\u7f3a\u4e4f\u573a\u666f\u7ea7\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u540c\u4e00\u56fe\u50cf\u4e2d\u7684\u4e2a\u4f53\u5728\u6df1\u5ea6\u548c\u5c3a\u5ea6\u4e0a\u5b58\u5728\u51b2\u7a81\u3002", "method": "DTO\uff08Depth-conditioned Translation Optimization\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4eba\u4f53\u6d4b\u91cf\u5b66\u5148\u9a8c\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u63d0\u4f9b\u7684\u6df1\u5ea6\u7ebf\u7d22\uff0c\u5728\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u6846\u67b6\u4e0b\uff0c\u8054\u5408\u4f18\u5316\u573a\u666f\u4e2d\u6240\u6709\u4e2a\u4f53\u7684\u76f8\u673a\u7a7a\u95f4\u5e73\u79fb\uff0c\u4ee5\u5b9e\u73b0\u573a\u666f\u4e00\u81f4\u7684\u7269\u4f53\u653e\u7f6e\u3002Metric-Aware HMR\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5305\u542b\u4e00\u4e2a\u76f8\u673a\u5206\u652f\u548c\u4e00\u4e2a\u65b0\u9896\u7684\u76f8\u5bf9\u5ea6\u91cf\u635f\u5931\uff0c\u80fd\u591f\u76f4\u63a5\u4ee5\u5ea6\u91cf\u5c3a\u5ea6\u4f30\u8ba1\u4eba\u4f53\u7f51\u683c\u548c\u76f8\u673a\u53c2\u6570\u3002", "result": "\u4f5c\u8005\u57fa\u4e8eDTO\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDTO-Humans\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b56\u4e07\u5f20\u9ad8\u8d28\u91cf\u3001\u573a\u666f\u4e00\u81f4\u7684\u591a\u4eba\u56fe\u50cf\u3002\u5728\u6d88\u878d\u5b9e\u9a8c\u4e2d\uff0cMetric-Aware HMR\u5728\u76f8\u5bf9\u6df1\u5ea6\u63a8\u7406\u548c\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DTO\u65b9\u6cd5\u548cMetric-Aware HMR\u7f51\u7edc\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5355\u5f20\u56fe\u50cf\u4e2d\u591a\u4eba\u7f51\u683c\u6062\u590d\u7684\u6311\u6218\uff0c\u751f\u6210\u573a\u666f\u4e00\u81f4\u4e14\u5ea6\u91cf\u7cbe\u786e\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2511.13283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13283", "abs": "https://arxiv.org/abs/2511.13283", "authors": ["Jongha Kim", "Minseong Bae", "Sanghyeok Lee", "Jinsung Yoon", "Hyunwoo J. Kim"], "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing", "comment": "AAAI 2026 (Main Technical Track)", "summary": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.", "AI": {"tldr": "TabFlash\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684MLLM\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\u3001\u526a\u679d\u7b56\u7565\u548c\u6807\u8bb0\u5173\u6ce8\u6765\u4f18\u5316\u8868\u683c\u7406\u89e3\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u5728\u5904\u7406\u8868\u683c\u56fe\u50cf\u65f6\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u6709\u6548\u5173\u6ce8\u95ee\u9898\u76f8\u5173\u4fe1\u606f\u5e76\u5904\u7406\u5197\u4f59\u80cc\u666f\uff0c\u5bfc\u81f4\u89c6\u89c9\u8868\u793a\u4fe1\u606f\u4e0d\u8db3\u4e14\u5197\u4f59\u3002\u672c\u7814\u7a76\u65e8\u5728\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u4e14\u7d27\u51d1\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ee5\u6539\u8fdb\u8868\u683c\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\uff08progressive question conditioning\uff09\uff0c\u5c06\u95ee\u9898\u9010\u6e10\u6ce8\u5165Vision Transformer\u5c42\uff1b\u63d0\u51fa\u526a\u679d\u7b56\u7565\uff08pruning strategy\uff09\uff0c\u79fb\u9664\u80cc\u666f\u6807\u8bb0\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b\u63d0\u51fa\u6807\u8bb0\u5173\u6ce8\uff08token focusing\uff09\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u5c06\u5173\u952e\u4fe1\u606f\u96c6\u4e2d\u5728\u4fdd\u7559\u7684\u6807\u8bb0\u4e0a\u3002\u7ed3\u5408\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86TabFlash\u6a21\u578b\u3002", "result": "TabFlash\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u548c\u95ed\u6e90MLLM\u3002\u4e0e\u7b2c\u4e8c\u4f18\u7684MLLM\u76f8\u6bd4\uff0cTabFlash\u7684\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u51cf\u5c11\u4e8627%\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8630%\u3002", "conclusion": "TabFlash\u901a\u8fc7\u7ed3\u5408\u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\u3001\u526a\u679d\u7b56\u7565\u548c\u6807\u8bb0\u5173\u6ce8\uff0c\u80fd\u591f\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u4e14\u7d27\u51d1\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2511.13285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13285", "abs": "https://arxiv.org/abs/2511.13285", "authors": ["Yunjie Yu", "Jingchen Wu", "Junchen Zhu", "Chunze Lin", "Guibin Chen"], "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design", "comment": null, "summary": "Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.", "AI": {"tldr": "SkyReels-Text\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u5b57\u4f53\u53ef\u63a7\u7684\u7cbe\u786e\u6d77\u62a5\u6587\u672c\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u7f16\u8f91\u5177\u6709\u4e0d\u540c\u5b57\u4f53\u6837\u5f0f\u7684\u591a\u4e2a\u6587\u672c\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u7f16\u8f91\u533a\u57df\u7684\u89c6\u89c9\u5916\u89c2\u3002", "motivation": "\u73b0\u4ee3\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7684\u3001\u5b57\u4f53\u611f\u77e5\u7684\u6587\u672c\u64cd\u7eb5\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6d77\u62a5\u7f16\u8f91\u7b49\u4e13\u4e1a\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "method": "SkyReels-Text\u6846\u67b6\uff0c\u7528\u6237\u53ea\u9700\u63d0\u4f9b\u6240\u9700\u5b57\u4f53\u5bf9\u5e94\u7684\u88c1\u526a\u5b57\u5f62\u56fe\u5757\uff0c\u5373\u53ef\u5b9e\u73b0\u591a\u6587\u672c\u533a\u57df\u7684\u540c\u65f6\u7f16\u8f91\uff0c\u4e14\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700\u5b57\u4f53\u6807\u7b7e\u6216\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u624b\u5199\u6587\u672c\u57fa\u51c6\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSkyReels-Text\u5728\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u5ea6\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5b57\u4f53\u7cfb\u5217\u548c\u6837\u5f0f\u7ec6\u5fae\u5dee\u522b\u7684\u7a7a\u524d\u63a7\u5236\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u901a\u7528\u56fe\u50cf\u7f16\u8f91\u4e0e\u4e13\u4e1a\u7ea7\u5b57\u4f53\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.13297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13297", "abs": "https://arxiv.org/abs/2511.13297", "authors": ["Enhui Ma", "Lijun Zhou", "Tao Tang", "Jiahuan Zhang", "Junpeng Jiang", "Zhan Zhang", "Dong Han", "Kun Zhan", "Xueyang Zhang", "XianPeng Lang", "Haiyang Sun", "Xia Zhou", "Di Lin", "Kaicheng Yu"], "title": "CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving", "comment": null, "summary": "End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCorrectAD\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u548c3D\u5e03\u5c40\u6765\u89e3\u51b3\u957f\u5c3e\u95ee\u9898\uff0c\u901a\u8fc7PM-Agent\u6a21\u62df\u4ea7\u54c1\u7ecf\u7406\u6536\u96c6\u6570\u636e\uff0c\u5e76\u4f7f\u7528DriveSora\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u3001\u65f6\u7a7a\u4e00\u81f4\u7684\u89c6\u9891\u3002\u8be5\u7cfb\u7edf\u53ef\u4ee5\u6539\u8fdb\u4efb\u4f55\u7aef\u5230\u7aef\u89c4\u5212\u5668\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u78b0\u649e\u7387\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5728\u5904\u7406\u7f55\u89c1\u4f46\u5173\u952e\u7684\u5931\u8d25\u6848\u4f8b\uff08\u957f\u5c3e\u95ee\u9898\uff09\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPM-Agent\u6765\u8bc6\u522b\u6570\u636e\u9700\u6c42\uff0c\u5e76\u4f7f\u7528DriveSora\u751f\u6210\u4e0e3D\u5e03\u5c40\u5339\u914d\u7684\u9ad8\u4fdd\u771f\u5ea6\u3001\u65f6\u7a7a\u4e00\u81f4\u7684\u89c6\u9891\u3002\u5c06\u8fd9\u4e9b\u7ec4\u4ef6\u96c6\u6210\u5230\u540d\u4e3aCorrectAD\u7684\u81ea\u6821\u6b63\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u6539\u8fdb\u7aef\u5230\u7aef\u89c4\u5212\u5668\u3002", "result": "CorrectAD\u80fd\u591f\u7ea0\u6b6362.5%\uff08nuScenes\uff09\u548c49.8%\uff08\u5185\u90e8\u6570\u636e\u96c6\uff09\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u5e76\u5c06\u78b0\u649e\u7387\u5206\u522b\u964d\u4f4e39%\u548c27%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u548c3D\u5e03\u5c40\u7684\u81ea\u6821\u6b63\u7cfb\u7edfCorrectAD\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u957f\u5c3e\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u7aef\u5230\u7aef\u89c4\u5212\u5668\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2511.13309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13309", "abs": "https://arxiv.org/abs/2511.13309", "authors": ["Kaiwen Cai", "Xinze Liu", "Xia Zhou", "Hengtong Hu", "Jie Xiang", "Luyao Zhang", "Xueyang Zhang", "Kun Zhan", "Yifei Zhan", "Xianpeng Lang"], "title": "DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving", "comment": "AAAI2026", "summary": "The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.", "AI": {"tldr": "DriveLiDAR4D\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u6d41\u7a0b\uff0c\u5b83\u4f7f\u7528\u591a\u6a21\u6001\u6761\u4ef6\u548c\u65b0\u9896\u7684\u5e8f\u5217\u566a\u58f0\u9884\u6d4b\u6a21\u578bLiDAR4DNet\uff0c\u80fd\u591f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u524d\u666f\u7269\u4f53\u53ef\u63a7\u3001\u80cc\u666f\u771f\u5b9e\u7684\u6fc0\u5149\u96f7\u8fbe\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u76843D\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u5728\u5e8f\u5217\u751f\u6210\u3001\u524d\u666f\u7269\u4f53\u7cbe\u786e\u5b9a\u4f4d\u548c\u80cc\u666f\u771f\u5b9e\u611f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDriveLiDAR4D\u7684\u65b0\u578b\u6fc0\u5149\u96f7\u8fbe\u751f\u6210\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5305\u542b\u591a\u6a21\u6001\u6761\u4ef6\u548c\u540d\u4e3aLiDAR4DNet\u7684\u65b0\u578b\u5e8f\u5217\u566a\u58f0\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728nuScenes\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86743.13\u7684FRD\u5f97\u5206\u548c16.96\u7684FVD\u5f97\u5206\uff0c\u4f18\u4e8e\u5f53\u524dSOTA\u65b9\u6cd5UniScene\uff0cFRD\u548cFVD\u5206\u522b\u63d0\u5347\u4e8637.2%\u548c24.1%\u3002", "conclusion": "DriveLiDAR4D\u662f\u7b2c\u4e00\u4e2a\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\u573a\u666f\u5e8f\u5217\u751f\u6210\u548c\u5168\u573a\u666f\u64cd\u63a7\u95ee\u9898\u7684\u7814\u7a76\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13315", "abs": "https://arxiv.org/abs/2511.13315", "authors": ["Narthana Sivalingam", "Santhirarajah Sivasthigan", "Thamayanthi Mahendranathan", "G. M. R. I. Godaliyadda", "M. P. B. Ekanayake", "H. M. V. R. Herath"], "title": "Computer Vision based group activity detection and action spotting", "comment": null, "summary": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u4eba\u573a\u666f\u4e2d\u8fdb\u884c\u7fa4\u4f53\u6d3b\u52a8\u8bc6\u522b\u548c\u52a8\u4f5c\u68c0\u6d4b\u3002", "motivation": "\u8bc6\u522b\u591a\u4eba\u573a\u666f\u4e2d\u7684\u7fa4\u4f53\u6d3b\u52a8\u9762\u4e34\u7740\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\u3001\u906e\u6321\u548c\u5916\u89c2\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u8be5\u7cfb\u7edf\u9996\u5148\u4f7f\u7528 Mask R-CNN \u8fdb\u884c\u6f14\u5458\u5b9a\u4f4d\uff0c\u7136\u540e\u5229\u7528\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\uff08\u5982 Inception V3\u3001MobileNet\u3001VGG16\uff09\u63d0\u53d6\u7279\u5f81\u56fe\u3002\u901a\u8fc7 RoIAlign \u548c\u63a9\u6a21\u4fe1\u606f\u878d\u5408\u6765\u83b7\u5f97\u6bcf\u4e2a\u6f14\u5458\u7684\u7cbe\u70bc\u63a9\u6a21\u7279\u5f81\u8868\u793a\u3002\u63a5\u7740\uff0c\u6784\u5efa\u6f14\u5458\u5173\u7cfb\u56fe\u6765\u7f16\u7801\u4e2a\u4f53\u95f4\u7684\u76f8\u4f3c\u6027\u548c\u4f4d\u7f6e\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u5728\u8fd9\u4e9b\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u9884\u6d4b\u4e2a\u4f53\u52a8\u4f5c\u548c\u7fa4\u4f53\u6d3b\u52a8\u3002", "result": "\u5728 Collective Activity \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u62e5\u6324\u548c\u975e\u62e5\u6324\u573a\u666f\u4e0b\u5747\u80fd\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u5c06\u5206\u5272\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5173\u7cfb\u56fe\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2511.13344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13344", "abs": "https://arxiv.org/abs/2511.13344", "authors": ["Ori Meiraz", "Sharon Shalev", "Avishai Weizman"], "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection", "comment": "1 figure, 1 table", "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5728\u591a\u4e2a YOLOv9-T \u4e13\u5bb6\u4e4b\u95f4\u8fdb\u884c\u81ea\u9002\u5e94\u8def\u7531\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u7279\u5f81\u4e13\u5316\uff0c\u4ece\u800c\u5728 mAP \u548c AR \u65b9\u9762\u4f18\u4e8e\u5355\u4e00 YOLOv9-T \u6a21\u578b\u3002", "motivation": "\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u52a8\u6001\u5730\u5728\u591a\u4e2a YOLOv9-T \u4e13\u5bb6\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u548c\u5206\u914d\u3002", "result": "\u4e0e\u5355\u4e00 YOLOv9-T \u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u5728 mAP \u548c AR \u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u5b9e\u73b0\u4e86\u52a8\u6001\u7279\u5f81\u4e13\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13353", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13353", "abs": "https://arxiv.org/abs/2511.13353", "authors": ["Lucas Gabriel Telesco", "Danila Nejamkin", "Estefan\u00eda Mata", "Francisco Filizzola", "Kevin Wignall", "Luc\u00eda Franco Troilo", "Mar\u00eda de los Angeles Cenoz", "Melissa Thompson", "Mercedes Legu\u00eda", "Ignacio Larrabide", "Jos\u00e9 Ignacio Orlando"], "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images", "comment": null, "summary": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08RIQA\uff09\uff0c\u65e8\u5728\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u8d28\u91cf\u53cd\u9988\uff0c\u6307\u5bfc\u56fe\u50cf\u91cd\u65b0\u62cd\u6444\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684\u8be6\u7ec6\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u7684RIQA\u5de5\u5177\u4e3b\u8981\u8bc4\u4f30\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\uff0c\u7f3a\u4e4f\u5bf9\u5177\u4f53\u6210\u50cf\u7f3a\u9677\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u8be6\u7ec6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684RIQA\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u591a\u4efb\u52a1\u6846\u67b6\u4e0b\u7ed3\u5408\u4e86\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u7684\u624b\u52a8\u6807\u7b7e\u548c\u56fe\u50cf\u8d28\u91cf\u7ec6\u8282\u7684\u4f2a\u6807\u7b7e\u3002\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7136\u540e\u7528\u4e8e\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u4f7f\u7528ResNet-18\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728EyeQ\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u4ece0.863\u63d0\u9ad8\u52300.875\uff1b\u5728DeepDRiD\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u4ece0.763\u63d0\u9ad8\u52300.778\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u591a\u4efb\u52a1\u6a21\u578b\u5728\u5927\u591a\u6570\u7ec6\u8282\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u5f53\uff08p > 0.05\uff09\u3002\u5728\u65b0\u6807\u6ce8\u7684EyeQ\u5b50\u96c6\u4e0a\uff0c\u6a21\u578b\u8868\u73b0\u4e0e\u4e13\u5bb6\u6c34\u5e73\u76f8\u5f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u63d0\u4f9b\u5173\u4e8e\u6210\u50cf\u6761\u4ef6\uff08\u5982\u5149\u7167\u3001\u6e05\u6670\u5ea6\u3001\u5bf9\u6bd4\u5ea6\uff09\u7684\u53ef\u89e3\u91ca\u53cd\u9988\uff0c\u4ece\u800c\u65e0\u9700\u989d\u5916\u7684\u6807\u6ce8\u6210\u672c\u5373\u53ef\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u4f9b\u4e34\u5e8a\u4e0a\u53ef\u64cd\u4f5c\u7684\u8f93\u51fa\u4ee5\u6307\u5bfc\u56fe\u50cf\u91cd\u65b0\u62cd\u6444\u3002"}}
{"id": "2511.13387", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13387", "abs": "https://arxiv.org/abs/2511.13387", "authors": ["Fei Kong"], "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model", "comment": "in Chinese language", "summary": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3agDDCM\u7684\u901a\u7528\u6269\u6563\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u73b0\u6709\u7684DDCM\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u5305\u62ecDDPM\u3001Score-Based Models\u3001Consistency Models\u548cRectified Flow\u5728\u5185\u7684\u591a\u79cd\u4e3b\u6d41\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728CIFAR-10\u548cLSUN Bedroom\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DDCM\u65b9\u6cd5\u4ec5\u9650\u4e8eDDPM\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6269\u6563\u6a21\u578b\u538b\u7f29\uff08gDDCM\uff09\uff0c\u5c06DDCM\u6269\u5c55\u5230\u5305\u62ecDDPM\u3001Score-Based Models\u3001Consistency Models\u548cRectified Flow\u5728\u5185\u7684\u591a\u79cd\u4e3b\u6d41\u6269\u6563\u6a21\u578b\u53ca\u5176\u53d8\u4f53\u3002", "result": "\u5728CIFAR-10\u548cLSUN Bedroom\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cgDDCM\u6210\u529f\u5730\u5c06DDCM\u63a8\u5e7f\u5230\u4e0a\u8ff0\u6a21\u578b\uff0c\u5e76\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "gDDCM\u80fd\u591f\u6709\u6548\u5730\u5c06DDCM\u6269\u5c55\u5230\u591a\u79cd\u4e3b\u6d41\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6027\u80fd\u7684\u63d0\u5347\u3002"}}
{"id": "2511.13397", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13397", "abs": "https://arxiv.org/abs/2511.13397", "authors": ["Nikos Theodoridis", "Tim Brophy", "Reenu Mohandas", "Ganesh Sistu", "Fiachra Collins", "Anthony Scanlan", "Ciaran Eising"], "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)", "comment": null, "summary": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDTPQA\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u8fdc\u8ddd\u79bb\u7269\u4f53\u7684\u8bc6\u522b\u548c\u7406\u89e3\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981VLMs\u5177\u5907\u9c81\u68d2\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u4ea4\u901a\u573a\u666f\u548c\u8bc6\u522b\u8fdc\u8ddd\u79bb\u7269\u4f53\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5728\u9694\u79bb\u5176\u4ed6\u6280\u80fd\uff08\u5982\u63a8\u7406\u6216\u4e16\u754c\u77e5\u8bc6\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5355\u72ec\u8bc4\u4f30VLMs\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "DTPQA\u57fa\u51c6\u5305\u542b\u4e24\u90e8\u5206\uff1a\u4e00\u4e2a\u4f7f\u7528\u6a21\u62df\u5668\u521b\u5efa\u7684\u5408\u6210\u57fa\u51c6\uff08DTP-Synthetic\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u56fe\u50cf\u6784\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\uff08DTP-Real\uff09\u3002\u8be5\u57fa\u51c6\u8fd8\u5305\u542b\u7269\u4f53\u8ddd\u79bb\u7684\u6807\u6ce8\uff0c\u5141\u8bb8\u5206\u6790\u6a21\u578b\u6027\u80fd\u968f\u8ddd\u79bb\u7684\u9000\u5316\u60c5\u51b5\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u7528\u4e8e\u751f\u6210\u6570\u636e\u7684Python\u811a\u672c\u3002", "result": "DTPQA\u57fa\u51c6\u5305\u542b\u56fe\u50cf\u3001\u95ee\u9898\u3001\u57fa\u672c\u7b54\u6848\u548c\u7269\u4f53\u8ddd\u79bb\u6807\u6ce8\uff0c\u80fd\u591f\u8bc4\u4f30VLMs\u5728\u4e0d\u540c\u8ddd\u79bb\u4e0b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u6027\u80fd\u968f\u8ddd\u79bb\u7684\u53d8\u5316\u3002", "conclusion": "DTPQA\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbVLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u8fdc\u8ddd\u79bb\u7269\u4f53\u8bc6\u522b\u65b9\u9762\u3002"}}
{"id": "2511.13399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13399", "abs": "https://arxiv.org/abs/2511.13399", "authors": ["Yuchen Bao", "Yiting Wang", "Wenjian Huang", "Haowei Wang", "Shen Chen", "Taiping Yao", "Shouhong Ding", "Jianguo Zhang"], "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing", "comment": "Accepted by AAAI2026", "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS", "AI": {"tldr": "TripleFDS\u662f\u4e00\u4e2a\u7528\u4e8e\u573a\u666f\u6587\u672c\u7f16\u8f91\uff08STE\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6587\u672c\u98ce\u683c\u3001\u5185\u5bb9\u548c\u80cc\u666f\u4e09\u4e2a\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002\u5b83\u5f15\u5165\u4e86SCB Synthesis\u6570\u636e\u96c6\u548c\u201cSCB Group\u201d\u6982\u5ff5\uff0c\u5e76\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6b63\u4ea4\u6027\u7ea6\u675f\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTripleFDS\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u51c6\u786e\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u652f\u6301\u98ce\u683c\u66ff\u6362\u548c\u80cc\u666f\u8fc1\u79fb\u7b49\u65b0\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u6587\u672c\u7f16\u8f91\u65b9\u6cd5\u5728\u89e3\u8026\u53ef\u7f16\u8f91\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f80\u5f80\u53ea\u80fd\u5904\u7406\u5355\u4e00\u5c5e\u6027\uff08\u5982\u6587\u672c\u5185\u5bb9\uff09\uff0c\u5bfc\u81f4\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faTripleFDS\u6846\u67b6\uff0c\u5b83\u5305\u542b\u89e3\u8026\u7684\u6a21\u5757\u5316\u5c5e\u6027\uff0c\u5e76\u5229\u7528SCB Synthesis\u6570\u636e\u96c6\u3002SCB Synthesis\u6570\u636e\u96c6\u901a\u8fc7\u201cSCB Group\u201d\u5c06\u4e09\u4e2a\u5c5e\u6027\uff08\u6587\u672c\u98ce\u683c\u3001\u5185\u5bb9\u3001\u80cc\u666f\uff09\u7ec4\u5408\uff0c\u7528\u4e8e\u8bad\u7ec3\u3002TripleFDS\u9996\u5148\u8fdb\u884c\u4e09\u5143\u7279\u5f81\u89e3\u8026\uff0c\u5229\u7528\u7ec4\u95f4\u5bf9\u6bd4\u6b63\u5219\u5316\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6837\u672c\u5185\u591a\u7279\u5f81\u6b63\u4ea4\u6027\u51cf\u5c11\u5197\u4f59\u3002\u5728\u5408\u6210\u9636\u6bb5\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u6620\u5c04\u9632\u6b62\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u7684\u201c\u6377\u5f84\u201d\u73b0\u8c61\u548c\u7f13\u89e3\u7279\u5f81\u6cc4\u6f0f\u3002", "result": "\u5728\u4e3b\u6d41STE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTripleFDS\u5728125,000\u4e2aSCB Group\u4e0a\u8bad\u7ec3\u540e\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\uff08SSIM\u4e3a44.54\uff09\u548c\u6587\u672c\u51c6\u786e\u6027\uff08ACC\u4e3a93.58%\uff09\u3002", "conclusion": "TripleFDS\u6846\u67b6\u5728\u573a\u666f\u6587\u672c\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u5176\u89e3\u8026\u7684\u5c5e\u6027\u7f16\u8f91\u80fd\u529b\uff0c\u652f\u6301\u4e86\u98ce\u683c\u66ff\u6362\u548c\u80cc\u666f\u8fc1\u79fb\u7b49\u65b0\u7684\u7f16\u8f91\u529f\u80fd\u3002"}}
{"id": "2511.13400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13400", "abs": "https://arxiv.org/abs/2511.13400", "authors": ["Jinkun Zhao", "Lei Huang", "Wenjun Wu"], "title": "What Color Is It? A Text-Interference Multimodal Hallucination Benchmark", "comment": null, "summary": "With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the \"What Color Is It\" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.", "AI": {"tldr": "MLMs\u5bb9\u6613\u53d7\u5230\u89c6\u89c9\u611f\u77e5\u4fe1\u606f\u5e72\u6270\uff0c\u5c24\u5176\u662f\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\uff0c\u8fd9\u4f1a\u589e\u52a0\u5e7b\u89c9\u7684\u98ce\u9669\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u201cWhat Color Is It\u201d\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u7b80\u5355\u65b9\u6cd5\u89e6\u53d1MLM\u5355\u6a21\u6001\u89c6\u89c9\u5e7b\u89c9\u7684\u65b0\u578b\u57fa\u51c6\u3002\u5728\u6b64\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86MLM\u89c6\u89c9\u6a21\u6001\u4e2d\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLMs\uff09\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4fe1\u606f\u5e72\u6270\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\uff0c\u8fd9\u589e\u52a0\u4e86\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u98ce\u9669\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201cWhat Color Is It\u201d\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e6\u53d1\u548c\u7814\u7a76MLM\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\u7684\u5355\u6a21\u6001\u89c6\u89c9\u5e7b\u89c9\u3002", "result": "\u901a\u8fc7\u201cWhat Color Is It\u201d\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e86MLM\u89c6\u89c9\u6a21\u6001\u4e2d\u4ea7\u751f\u5e7b\u89c9\u7684\u539f\u56e0\u3002", "conclusion": "\u63d0\u51fa\u4e86\u589e\u5f3aMLM\u89c6\u89c9\u6a21\u6001\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13417", "abs": "https://arxiv.org/abs/2511.13417", "authors": ["Mykola Lavreniuk", "Nataliia Kussul", "Andrii Shelestov", "Yevhenii Salii", "Volodymyr Kuzin", "Sergii Skakun", "Zoltan Szantoi"], "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source", "comment": null, "summary": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.", "AI": {"tldr": "DelAnyFlow\u662f\u4e00\u79cd\u5206\u8fa8\u7387\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408DelAny\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u548c\u7ed3\u6784\u5316\u540e\u5904\u7406\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u3001\u9ad8\u7cbe\u5ea6\u5730\u7ed8\u5236\u519c\u7530\u8fb9\u754c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u4e4c\u514b\u5170\u3002", "motivation": "\u73b0\u6709\u7684\u519c\u7530\u8fb9\u754c\u7ed8\u5236\u65b9\u6cd5\u5728\u5b8c\u6574\u6027\u3001\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u652f\u6301\u571f\u5730\u7ba1\u7406\u548c\u4f5c\u7269\u76d1\u6d4b\u3002", "method": "\u63d0\u51faDelAnyFlow\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8eYOLOv11\u9aa8\u5e72\u7684DelAny\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\uff08\u5728FBIS 22M\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff09\u4ee5\u53ca\u7ed3\u6784\u5316\u540e\u5904\u7406\u3001\u5408\u5e76\u548c\u77e2\u91cf\u5316\u6d41\u7a0b\uff0c\u4ee5\u751f\u6210\u62d3\u6251\u4e00\u81f4\u7684\u77e2\u91cf\u8fb9\u754c\u3002FBIS 22M\u662f\u6700\u5927\u7684\u540c\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u542b\u5927\u91cf\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u5df2\u9a8c\u8bc1\u7684\u7530\u91ce\u5b9e\u4f8b\u3002", "result": "DelAny\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0cmAP\u6bd4SAM2\u9ad8100%\u4ee5\u4e0a\uff0c\u63a8\u7406\u901f\u5ea6\u5feb400\u500d\u3002DelAnyFlow\u6210\u529f\u751f\u6210\u4e86\u4e4c\u514b\u5170\uff08603,000\u5e73\u65b9\u516c\u91cc\uff09\u7684\u5b8c\u6574\u519c\u7530\u8fb9\u754c\u5c42\uff0c\u8017\u65f6\u4e0d\u5230\u516d\u5c0f\u65f6\u3002\u4e0eSinergise Solutions\u548cNASA Harvest\u7684\u8fd0\u8425\u4ea7\u54c1\u76f8\u6bd4\uff0cDelAnyFlow\u7684\u8fb9\u754c\u5b8c\u6574\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5c0f\u519c\u548c\u788e\u7247\u5316\u7cfb\u7edf\u4e2d\u3002\u5728\u4e4c\u514b\u5170\uff0cDelAnyFlow\u57285\u7c73\u548c2.5\u7c73\u5206\u8fa8\u7387\u4e0b\u5206\u522b\u7ed8\u5236\u4e863.75M\u548c5.15M\u4e2a\u7530\u91ce\uff0c\u800cSinergise Solutions\u548cNASA Harvest\u5206\u522b\u68c0\u6d4b\u52302.66M\u548c1.69M\u4e2a\u7530\u91ce\u3002", "conclusion": "DelAnyFlow\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7f3a\u4e4f\u6570\u5b57\u5730\u7c4d\u6570\u636e\u7684\u5730\u533a\u8fdb\u884c\u519c\u7530\u8fb9\u754c\u7ed8\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13420", "abs": "https://arxiv.org/abs/2511.13420", "authors": ["Xingming Long", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task", "comment": "8 pages", "summary": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.", "AI": {"tldr": "\u5927\u591a\u6570\u5173\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5e7b\u89c9\u7684\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u4e0d\u5141\u8bb8\u56fe\u50cf\u4e2d\u4efb\u4f55\u7f3a\u5931\u7684\u8f93\u51fa\u7684\u4e8b\u5b9e\u63cf\u8ff0\u4efb\u52a1\u4e0a\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6545\u4e8b\u5199\u4f5c\u7b49\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\uff0c\u4eba\u4eec\u5f88\u5c11\u5173\u6ce8\uff0c\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5e94\u8be5\u751f\u6210\u56fe\u50cf\u4e4b\u5916\u7684\u65b0\u5185\u5bb9\u3002\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0c\u7b80\u5355\u5730\u5c06\u8fd9\u79cd\u60f3\u8c61\u51fa\u7684\u65b0\u5185\u5bb9\u89c6\u4e3a\u5e7b\u89c9\u662f\u4e0d\u6070\u5f53\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u613f\u60f3\u8c61\u5bf9\u8c61\u5b58\u5728\u8bc4\u4f30\uff08VOPE\uff09\u2014\u2014\u4e00\u79cd\u901a\u8fc7\u5b58\u5728\u8bc4\u4f30\u6765\u8bc4\u4f30\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d LVLM \u5e7b\u89c9\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0cVOPE \u901a\u8fc7\u91cd\u65b0\u68c0\u67e5\u95ee\u9898\u6765\u8bc4\u4f30 LVLM \u5982\u4f55\u89e3\u91ca\u5176\u81ea\u8eab\u54cd\u5e94\u4e2d\u60f3\u8c61\u5bf9\u8c61\u7684\u5b58\u5728\u3002\u7136\u540e\uff0c\u6a21\u578b\u89e3\u91ca\u4e0e\u56fe\u50cf\u4e2d\u5bf9\u8c61\u5b58\u5728\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u88ab\u7528\u6765\u786e\u5b9a\u6a21\u578b\u5728\u751f\u6210\u54cd\u5e94\u65f6\u662f\u5426\u4ea7\u751f\u5e7b\u89c9\u3002\u6211\u4eec\u5c06 VOPE \u5e94\u7528\u4e8e\u51e0\u79cd\u4e3b\u6d41\u7684 LVLM \u548c\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4e24\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u5927\u591a\u6570 LVLM \u5728\u81ea\u613f\u60f3\u8c61\u671f\u95f4\u4f1a\u4ea7\u751f\u5927\u91cf\u5e7b\u89c9\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u60f3\u8c61\u5bf9\u8c61\u4e0a\u7684\u5b58\u5728\u8bc4\u4f30\u8868\u73b0\u660e\u663e\u8f83\u5dee\uff1b(2) \u73b0\u6709\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u6709\u9650\uff0c\u8fd9\u4f7f\u5f97\u8be5\u9886\u57df\u6210\u4e3a\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002", "motivation": "\u5927\u591a\u6570\u5173\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5e7b\u89c9\u7684\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u4e0d\u5141\u8bb8\u56fe\u50cf\u4e2d\u4efb\u4f55\u7f3a\u5931\u7684\u8f93\u51fa\u7684\u4e8b\u5b9e\u63cf\u8ff0\u4efb\u52a1\u4e0a\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6545\u4e8b\u5199\u4f5c\u7b49\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\uff0c\u4eba\u4eec\u5f88\u5c11\u5173\u6ce8\uff0c\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5e94\u8be5\u751f\u6210\u56fe\u50cf\u4e4b\u5916\u7684\u65b0\u5185\u5bb9\u3002\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0c\u7b80\u5355\u5730\u5c06\u8fd9\u79cd\u60f3\u8c61\u51fa\u7684\u65b0\u5185\u5bb9\u89c6\u4e3a\u5e7b\u89c9\u662f\u4e0d\u6070\u5f53\u7684\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LVLM\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u613f\u60f3\u8c61\u5bf9\u8c61\u5b58\u5728\u8bc4\u4f30\uff08VOPE\uff09\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b58\u5728\u8bc4\u4f30\u6765\u8bc4\u4f30\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684 LVLM \u5e7b\u89c9\u3002VOPE \u901a\u8fc7\u91cd\u65b0\u68c0\u67e5\u95ee\u9898\u6765\u8bc4\u4f30 LVLM \u5982\u4f55\u89e3\u91ca\u5176\u81ea\u8eab\u54cd\u5e94\u4e2d\u60f3\u8c61\u5bf9\u8c61\u7684\u5b58\u5728\u3002\u7136\u540e\uff0c\u6a21\u578b\u89e3\u91ca\u4e0e\u56fe\u50cf\u4e2d\u5bf9\u8c61\u5b58\u5728\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u88ab\u7528\u6765\u786e\u5b9a\u6a21\u578b\u5728\u751f\u6210\u54cd\u5e94\u65f6\u662f\u5426\u4ea7\u751f\u5e7b\u89c9\u3002", "result": "\u5c06 VOPE \u5e94\u7528\u4e8e\u51e0\u79cd\u4e3b\u6d41\u7684 LVLM \u548c\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4e24\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u5927\u591a\u6570 LVLM \u5728\u81ea\u613f\u60f3\u8c61\u671f\u95f4\u4f1a\u4ea7\u751f\u5927\u91cf\u5e7b\u89c9\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u60f3\u8c61\u5bf9\u8c61\u4e0a\u7684\u5b58\u5728\u8bc4\u4f30\u8868\u73b0\u660e\u663e\u8f83\u5dee\uff1b(2) \u73b0\u6709\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5927\u591a\u6570 LVLM \u5728\u81ea\u613f\u60f3\u8c61\u671f\u95f4\u4f1a\u4ea7\u751f\u5927\u91cf\u5e7b\u89c9\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u60f3\u8c61\u5bf9\u8c61\u4e0a\u7684\u5b58\u5728\u8bc4\u4f30\u8868\u73b0\u660e\u663e\u8f83\u5dee\u3002\u73b0\u6709\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u6709\u9650\uff0c\u8fd9\u4f7f\u5f97\u8be5\u9886\u57df\u6210\u4e3a\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.13431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13431", "abs": "https://arxiv.org/abs/2511.13431", "authors": ["Lorenzo Olearo", "Giulio Vigan\u00f2", "Daniele Baieri", "Filippo Maggioli", "Simone Melzi"], "title": "FUSE: A Flow-based Mapping Between Shapes", "comment": "11 pages, 9 figures", "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.", "AI": {"tldr": "\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u57283D\u5f62\u72b6\u4e4b\u95f4\u8fdb\u884c\u65b0\u9896\u7684\u795e\u7ecf\u8868\u793a\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u5373\u53ef\u8fdb\u884c\u8de8\u8868\u793a\u5f62\u72b6\u5339\u914d\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u57283D\u5f62\u72b6\u4e4b\u95f4\u5efa\u7acb\u6620\u5c04\uff0c\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5e76\u652f\u6301\u8de8\u8868\u793a\u7684\u5f62\u72b6\u5339\u914d\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u6570\u636e\u9a71\u52a8\u3002", "method": "\u5c063D\u5f62\u72b6\u8868\u793a\u4e3a\u7531\u4ece\u56fa\u5b9a\u951a\u70b9\u5206\u5e03\u5230\u8fde\u7eed\u4e14\u53ef\u9006\u7684\u6d41\u6620\u5c04\u5f15\u8d77\u7684\u6982\u7387\u5206\u5e03\u3002\u901a\u8fc7\u5c06\u6e90\u7684\u9006\u6d41\uff08\u6e90\u5230\u951a\u70b9\uff09\u4e0e\u6b63\u5411\u6d41\uff08\u951a\u70b9\u5230\u76ee\u6807\uff09\u7ec4\u5408\uff0c\u53ef\u4ee5\u5728\u4e24\u4e2a\u8868\u9762\u4e4b\u95f4\u8fde\u7eed\u5730\u6620\u5c04\u70b9\u3002\u901a\u8fc7\u5bf9\u5f62\u72b6\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u9010\u70b9\u5d4c\u5165\u7f16\u7801\uff0c\u6b64\u6784\u9020\u63d0\u4f9b\u4e86\u5f62\u72b6\u4e4b\u95f4\u6620\u5c04\u7684\u53ef\u9006\u4e14\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u8868\u793a\uff0c\u9002\u7528\u4e8e\u70b9\u4e91\u3001\u7f51\u683c\u3001\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u548c\u4f53\u79ef\u6570\u636e\u3002", "result": "\u8be5\u8868\u793a\u5728\u5f62\u72b6\u5339\u914d\u65b9\u9762\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\uff0c\u6301\u7eed\u5b9e\u73b0\u9ad8\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5728UV\u6620\u5c04\u548c\u4eba\u4f53\u539f\u59cb\u70b9\u4e91\u626b\u63cf\u914d\u51c6\u7b49\u4efb\u52a1\u4e2d\u4e5f\u663e\u793a\u51fa\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6d41\u5339\u914d\u76843D\u5f62\u72b6\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u5730\u8fdb\u884c\u8de8\u8868\u793a\u7684\u5f62\u72b6\u5339\u914d\uff0c\u5e76\u5728UV\u6620\u5c04\u548c\u70b9\u4e91\u914d\u51c6\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2511.13442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13442", "abs": "https://arxiv.org/abs/2511.13442", "authors": ["Rui Zuo", "Qinyue Tong", "Zhe-Ming Lu", "Ziqian Lu"], "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline", "comment": null, "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.", "AI": {"tldr": "Foresee\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff08IFDL\uff09\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5404\u79cd\u7be1\u6539\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4e30\u5bcc\u7684\u6587\u672c\u89e3\u91ca\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u800c\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u672a\u80fd\u5145\u5206\u53d1\u6398\u5176\u6f5c\u529b\u3002", "method": "Foresee\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u57fa\u4e8eMLLM\u7684\u56fe\u50cf\u4f2a\u9020\u5206\u6790\u6d41\u7a0b\uff0c\u91c7\u7528\u7c7b\u578b\u5148\u9a8c\u9a71\u52a8\u7b56\u7565\u548c\u7075\u6d3b\u7279\u5f81\u68c0\u6d4b\uff08FFD\uff09\u6a21\u5757\u6765\u5904\u7406\u590d\u5236-\u79fb\u52a8\u7be1\u6539\uff0c\u4ee5\u53d1\u6325\u539f\u59cbMLLM\u5728\u53d6\u8bc1\u9886\u57df\u7684\u6f5c\u529b\u3002", "result": "Foresee\u5728\u7be1\u6539\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6587\u672c\u89e3\u91ca\u4e30\u5bcc\u5ea6\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5305\u62ec\u590d\u5236-\u79fb\u52a8\u3001\u62fc\u63a5\u3001\u79fb\u9664\u3001\u5c40\u90e8\u589e\u5f3a\u3001\u6df1\u5ea6\u4f2a\u9020\u548cAIGC\u7f16\u8f91\u5728\u5185\u7684\u5404\u79cd\u7be1\u6539\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u6bd4\u73b0\u6709IFDL\u65b9\u6cd5\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Foresee\u6210\u529f\u5730\u5229\u7528\u4e86\u539f\u59cbMLLM\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.13478", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13478", "abs": "https://arxiv.org/abs/2511.13478", "authors": ["Adam Hazimeh", "Ke Wang", "Mark Collier", "Gilles Baechler", "Efi Kokiopoulou", "Pascal Frossard"], "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling", "comment": null, "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.", "AI": {"tldr": "SliDer\u6846\u67b6\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u5e7b\u706f\u7247\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684SVG\u683c\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u9ad8\u5c42\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u51e0\u4f55\u6805\u683c\u77e2\u91cf\u5316\u65b9\u6cd5\u5728\u5904\u7406\u5e7b\u706f\u7247\u7b49\u590d\u6742\u6587\u6863\u65f6\uff0c\u65e0\u6cd5\u4fdd\u7559\u9ad8\u5c42\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u6062\u590d\u6587\u6863\u7684\u53ef\u7f16\u8f91\u6027\u3002", "method": "SliDer\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u8bc6\u522b\u548c\u63d0\u53d6\u6805\u683c\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u548c\u56fe\u50cf\u5143\u7d20\u53ca\u5176\u5c5e\u6027\uff0c\u5e76\u5c06\u5b83\u4eec\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u7684SVG\u683c\u5f0f\u3002\u8be5\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u8fed\u4ee3\u5730\u4f18\u5316\u9884\u6d4b\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u91cd\u5efa\u539f\u59cb\u6805\u683c\u56fe\u50cf\u3002", "result": "SliDer\u5728\u91cd\u5efaLPIPS\u65b9\u9762\u8fbe\u5230\u4e860.069\uff0c\u5e76\u4e14\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\uff0c82.9%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6700\u5f3a\u7684\u96f6\u6837\u672cVLM\u57fa\u7ebf\u3002", "conclusion": "SliDer\u6846\u67b6\u901a\u8fc7\u5f15\u5165VLMs\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5e7b\u706f\u7247\u7b49\u6587\u6863\u7684\u8bed\u4e49\u6587\u6863\u89e3\u6e32\u67d3\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7cbe\u786e\u3001\u53ef\u7f16\u8f91\u7684SVG\u8868\u793a\u3002\u540c\u65f6\uff0cSlide2SVG\u6570\u636e\u96c6\u7684\u53d1\u5e03\u5c06\u6709\u52a9\u4e8e\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.13488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13488", "abs": "https://arxiv.org/abs/2511.13488", "authors": ["Lipeng Wang", "Hongxing Fan", "Haohua Chen", "Zehuan Huang", "Lu Sheng"], "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE", "comment": "Accepted to AAAI-26. Codes: https://github.com/Lighten001/InterMoE", "summary": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.", "AI": {"tldr": "InterMoE\u662f\u4e00\u4e2a\u5229\u7528\u52a8\u6001\u65f6\u95f4\u9009\u62e9\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u4eba\u9645\u4ea4\u4e92\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u7559\u4e2a\u4f53\u7279\u5f81\u5e76\u9075\u5faa\u6587\u672c\u63cf\u8ff0\uff0c\u5728InterHuman\u548cInterX\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u4eba\u9645\u4ea4\u4e92\u65f6\uff0c\u96be\u4ee5\u4fdd\u7559\u72ec\u7279\u7684\u4e2a\u4f53\u7279\u5f81\u5e76\u5b8c\u5168\u9075\u5faa\u6587\u672c\u63cf\u8ff0\u3002", "method": "\u63d0\u51faInterMoE\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5229\u7528\u9ad8\u5c42\u6587\u672c\u8bed\u4e49\u548c\u4f4e\u5c42\u8fd0\u52a8\u4e0a\u4e0b\u6587\u7684\u8def\u7531\u673a\u5236\uff0c\u5c06\u65f6\u6001\u8fd0\u52a8\u7279\u5f81\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u4e13\u5bb6\uff0c\u4f7f\u4e13\u5bb6\u80fd\u52a8\u6001\u786e\u5b9a\u9009\u62e9\u80fd\u529b\u5e76\u4e13\u6ce8\u4e8e\u5173\u952e\u65f6\u6001\u7279\u5f81\u3002", "result": "InterMoE\u5728InterHuman\u6570\u636e\u96c6\u4e0aFID\u5206\u6570\u964d\u4f4e\u4e869%\uff0c\u5728InterX\u6570\u636e\u96c6\u4e0a\u964d\u4f4e\u4e8622%\uff0c\u8fbe\u5230\u4e86\u5728\u4e2a\u4f53\u7279\u5b9a\u9ad8\u4fdd\u771f3D\u4eba\u9645\u4ea4\u4e92\u751f\u6210\u65b9\u9762\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "InterMoE\u80fd\u591f\u4fdd\u7559\u4e2a\u4f53\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u8bc1\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5728\u4eba\u9645\u4ea4\u4e92\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.13494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13494", "abs": "https://arxiv.org/abs/2511.13494", "authors": ["Jae Joong Lee"], "title": "Language-Guided Invariance Probing of Vision-Language Models", "comment": null, "summary": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.\n  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.", "AI": {"tldr": "LGIP\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5904\u7406\u8bed\u8a00\u6270\u52a8\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4fdd\u6301\u542b\u4e49\u7684\u6539\u5199\u548c\u6539\u53d8\u542b\u4e49\u7684\u8bed\u4e49\u7ffb\u8f6c\u7684\u53cd\u5e94\u3002", "motivation": "\u8bc4\u4f30\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u9762\u5bf9\u6709\u63a7\u5236\u7684\u8bed\u8a00\u6270\u52a8\uff08\u5982\u6539\u5199\u548c\u8bed\u4e49\u7ffb\u8f6c\uff09\u65f6\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528MS COCO\u6570\u636e\u96c6\u548c\u4eba\u7c7b\u7f16\u5199\u7684\u6807\u9898\uff0c\u81ea\u52a8\u751f\u6210\u4fdd\u6301\u542b\u4e49\u7684\u6539\u5199\u548c\u6539\u53d8\u542b\u4e49\u7684\u8bed\u4e49\u7ffb\u8f6c\uff08\u6d89\u53ca\u5bf9\u8c61\u7c7b\u522b\u3001\u989c\u8272\u6216\u6570\u91cf\uff09\uff0c\u5e76\u5f15\u5165LGIP\u57fa\u51c6\u6d4b\u8bd5\u6765\u8861\u91cf\u6a21\u578b\u7684\u884c\u4e3a\u3002", "result": "EVA02-CLIP\u548c\u5927\u578bOpenCLIP\u53d8\u4f53\u5728\u4e0d\u53d8\u6027-\u654f\u611f\u6027\u6743\u8861\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002SigLIP\u548cSigLIP2\u5219\u8868\u73b0\u51fa\u8f83\u5927\u7684\u4e0d\u53d8\u6027\u8bef\u5dee\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f18\u5148\u9009\u62e9\u88ab\u7ffb\u8f6c\u7684\u6807\u9898\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8c61\u548c\u989c\u8272\u7f16\u8f91\u65b9\u9762\u3002", "conclusion": "LGIP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u53ef\u4ee5\u8bc4\u4f30VLM\u7684\u8bed\u8a00\u9c81\u68d2\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u51c6\u786e\u6027\u6307\u6807\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u68c0\u7d22\u6307\u6807\u53ef\u80fd\u5ffd\u89c6\u7684\u6a21\u578b\u5f31\u70b9\u3002"}}
{"id": "2511.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13507", "abs": "https://arxiv.org/abs/2511.13507", "authors": ["Wenyu Zhang", "Yao Tong", "Yiqiu Liu", "Rui Cao"], "title": "Mapping the Vanishing and Transformation of Urban Villages in China", "comment": "Appendix A. Supplementary data at https://ars.els-cdn.com/content/image/1-s2.0-S2210670725008418-mmc1.docx", "summary": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.", "AI": {"tldr": "\u4e2d\u56fd\u57ce\u4e2d\u6751\u62c6\u9664\u91cd\u5efa\u540e\u7684\u571f\u5730\u518d\u5229\u7528\u8bc4\u4f30\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u9065\u611f\u5f71\u50cf\u8bed\u4e49\u5206\u5272\u6280\u672f\uff0c\u76d1\u6d4b\u57ce\u4e2d\u6751\u7684\u65f6\u7a7a\u53d8\u5316\u53ca\u5176\u62c6\u9664\u540e\u7684\u571f\u5730\u5229\u7528\u3002\u7814\u7a76\u9009\u53d6\u4e86\u4e2d\u56fd\u56db\u4e2a\u7ecf\u6d4e\u533a\u57df\u7684\u56db\u4e2a\u4ee3\u8868\u6027\u57ce\u5e02\uff08\u5e7f\u5dde\u3001\u90d1\u5dde\u3001\u897f\u5b89\u3001\u54c8\u5c14\u6ee8\uff09\u8fdb\u884c\u5206\u6790\uff0c\u7ed3\u679c\u663e\u793a\u57ce\u4e2d\u6751\u7684\u91cd\u5efa\u8fc7\u7a0b\u666e\u904d\u8017\u65f6\u8f83\u957f\uff0c\u91cd\u5efa\u4e3b\u8981\u53d1\u751f\u5728\u57ce\u5e02\u5916\u56f4\u533a\u57df\uff0c\u5e76\u63ed\u793a\u4e86\u540c\u6b65\u91cd\u5efa\u3001\u5ef6\u8fdf\u91cd\u5efa\u548c\u6e10\u8fdb\u4f18\u5316\u4e09\u79cd\u65f6\u7a7a\u8f6c\u578b\u8def\u5f84\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u57ce\u4e2d\u6751\u91cd\u5efa\u7684\u788e\u7247\u5316\u3001\u590d\u6742\u6027\u548c\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u547c\u5401\u5236\u5b9a\u5206\u5c42\u548c\u56e0\u5730\u5236\u5b9c\u7684\u89c4\u5212\u7b56\u7565\uff0c\u4e3a\u66f4\u5177\u5305\u5bb9\u6027\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u57ce\u5e02\u66f4\u65b0\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301\uff0c\u5e76\u4e3a\u5168\u7403\u8303\u56f4\u5185\u975e\u6b63\u89c4\u4f4f\u533a\u8f6c\u578b\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u4e2d\u56fd\u57ce\u4e2d\u6751\uff08UVs\uff09\u5728\u57ce\u5e02\u5316\u8fdb\u7a0b\u4e2d\u88ab\u5927\u89c4\u6a21\u62c6\u9664\u548c\u91cd\u5efa\uff0c\u4f46\u5176\u62c6\u9664\u540e\u7684\u571f\u5730\u662f\u5426\u5f97\u5230\u6709\u6548\u518d\u5229\u7528\uff0c\u4ee5\u53ca\u91cd\u5efa\u5b9e\u8df5\u7684\u53ef\u6301\u7eed\u6027\uff0c\u4e00\u76f4\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e2d\u56fd\u57ce\u4e2d\u6751\u62c6\u9664\u540e\u7684\u571f\u5730\u518d\u5229\u7528\u60c5\u51b5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u9996\u5148\u5229\u7528\u591a\u65f6\u76f8\u9065\u611f\u5f71\u50cf\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u7ed8\u5236\u57ce\u4e2d\u6751\u4e0d\u65ad\u53d8\u5316\u7684\u8fb9\u754c\u3002\u7136\u540e\uff0c\u6839\u636e\u201c\u4fdd\u7559-\u62c6\u9664-\u518d\u5f00\u53d1\u201d\u7684\u9636\u6bb5\uff0c\u5c06\u62c6\u9664\u540e\u7684\u571f\u5730\u5229\u7528\u5206\u4e3a\u516d\u7c7b\uff1a\u4e0d\u5b8c\u5168\u62c6\u9664\u3001\u7a7a\u5730\u3001\u5efa\u7b51\u5de5\u5730\u3001\u5efa\u7b51\u7269\u3001\u7eff\u5730\u548c\u5176\u4ed6\u3002\u7814\u7a76\u9009\u53d6\u4e86\u4e2d\u56fd\u56db\u4e2a\u7ecf\u6d4e\u533a\u57df\u7684\u56db\u4e2a\u4ee3\u8868\u6027\u57ce\u5e02\uff08\u5e7f\u5dde\u3001\u90d1\u5dde\u3001\u897f\u5b89\u3001\u54c8\u5c14\u6ee8\uff09\u4f5c\u4e3a\u7814\u7a76\u533a\u57df\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a1) \u57ce\u4e2d\u6751\u7684\u91cd\u5efa\u8fc7\u7a0b\u666e\u904d\u8017\u65f6\u8f83\u957f\uff1b2) \u91cd\u5efa\u8f6c\u578b\u4e3b\u8981\u53d1\u751f\u5728\u57ce\u5e02\u5916\u56f4\u533a\u57df\uff0c\u800c\u57ce\u5e02\u6838\u5fc3\u533a\u76f8\u5bf9\u7a33\u5b9a\uff1b3) \u63ed\u793a\u4e86\u540c\u6b65\u91cd\u5efa\u3001\u5ef6\u8fdf\u91cd\u5efa\u548c\u6e10\u8fdb\u4f18\u5316\u4e09\u79cd\u65f6\u7a7a\u8f6c\u578b\u8def\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u57ce\u4e2d\u6751\u91cd\u5efa\u7684\u788e\u7247\u5316\u3001\u590d\u6742\u548c\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u5f3a\u8c03\u4e86\u5236\u5b9a\u5206\u5c42\u548c\u56e0\u5730\u5236\u5b9c\u7684\u89c4\u5212\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002\u901a\u8fc7\u5c06\u7a7a\u95f4\u52a8\u6001\u4e0e\u91cd\u5efa\u653f\u7b56\u80cc\u666f\u76f8\u7ed3\u5408\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u66f4\u5177\u5305\u5bb9\u6027\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u57ce\u5e02\u66f4\u65b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8bc1\u89c1\u89e3\uff0c\u5e76\u4e3a\u5168\u7403\u975e\u6b63\u89c4\u4f4f\u533a\u8f6c\u578b\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u7406\u89e3\u3002"}}
{"id": "2511.13533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13533", "abs": "https://arxiv.org/abs/2511.13533", "authors": ["Jeffrey Wen", "Rizwan Ahmad", "Philip Schniter"], "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems", "comment": null, "summary": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.", "AI": {"tldr": "Conformal prediction is extended for uncertainty quantification in ill-posed imaging inverse problems with multiple estimation targets, providing tight prediction intervals and joint marginal coverage.", "motivation": "Uncertainty quantification is a significant challenge in ill-posed imaging inverse problems, particularly for safety-critical applications. Existing conformal prediction methods only handle scalar estimation targets, while practical applications often require multiple targets.", "method": "The paper proposes an asymptotically minimax approach to multi-target conformal prediction that ensures joint marginal coverage and provides tight prediction intervals. This approach is then applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition.", "result": "The proposed minimax method demonstrates numerical benefits over existing multi-target conformal prediction methods using both synthetic and MRI data.", "conclusion": "The developed asymptotically minimax approach effectively addresses multi-target uncertainty quantification in inverse imaging problems, offering tighter prediction intervals and joint marginal coverage, with demonstrated advantages in practical applications."}}
{"id": "2511.13535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13535", "abs": "https://arxiv.org/abs/2511.13535", "authors": ["Farhin Farhad Riya", "Shahinul Hoque", "Jinyuan Stella Sun", "Olivera Kotevska"], "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew", "comment": null, "summary": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c0f\u7684\u989c\u8272\u6270\u52a8\u6765\u653b\u51fb\u6a21\u578b\u7684\u89e3\u91ca\u6027\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u89c6\u89c9\u89e3\u91ca\u6280\u672f\u5bf9\u4e8e\u652f\u6301\u900f\u660e\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u7c7b\u65b0\u7684\u653b\u51fb\uff0c\u5b83\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u7834\u574f\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8272\u5ea6\u6270\u52a8\u6a21\u5757\u201d\u7684\u3001\u7528\u4e8e\u751f\u6210\u5bf9\u6297\u6027\u6837\u672c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u53d8\u524d\u666f\u548c\u80cc\u666f\u4e4b\u95f4\u7684\u989c\u8272\u5bf9\u6bd4\u5ea6\u6765\u7834\u574f\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\u3002", "result": "\u8be5\u653b\u51fb\u5728\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u6027\u9ad8\u4e8e 96% \u7684\u540c\u65f6\uff0c\u5c06 Grad-CAM \u89e3\u91ca\u4e2d\u7684\u5cf0\u503c\u6fc0\u6d3b\u91cd\u53e0\u51cf\u5c11\u4e86\u9ad8\u8fbe 35%\u3002", "conclusion": "\u89e3\u91ca\u6027\u672c\u8eab\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u7684\u9014\u5f84\uff0c\u5e76\u4e14\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u89e3\u91ca\u6027\u7684\u9000\u5316\u66f4\u96be\u88ab\u5bdf\u89c9\u548c\u9632\u5fa1\u3002"}}
{"id": "2511.13539", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13539", "abs": "https://arxiv.org/abs/2511.13539", "authors": ["Yuanchao Wang", "Tian Qin", "Eduardo Valle", "Bruno Abrahao"], "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse", "comment": "8 pages", "summary": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.", "AI": {"tldr": "BootOOD\u662f\u4e00\u4e2a\u5168\u81ea\u76d1\u7763\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u4ec5\u5229\u7528ID\u6570\u636e\u8fdb\u884c\u5f15\u5bfc\uff0c\u65e8\u5728\u89e3\u51b3\u8bed\u4e49\u4e0a\u76f8\u4f3c\u7684OOD\u6837\u672c\u68c0\u6d4b\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u4e0eID\u7c7b\u522b\u8bed\u4e49\u76f8\u4f3c\u7684OOD\u6837\u672c\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800cBootOOD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "BootOOD\u901a\u8fc7\u5bf9ID\u8868\u793a\u8fdb\u884c\u7b80\u5355\u53d8\u6362\u6765\u5408\u6210\u4f2aOOD\u7279\u5f81\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u574d\u7f29\uff08NC\uff09\u7279\u6027\u3002\u5b83\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8f85\u52a9\u5934\uff0c\u901a\u8fc7\u57fa\u4e8e\u534a\u5f84\u7684\u7279\u5f81\u8303\u6570\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06OOD\u68c0\u6d4b\u4e0e\u4e3b\u5206\u7c7b\u5668\u89e3\u8026\uff0c\u5e76\u653e\u5bbd\u8981\u6c42\uff1aOOD\u6837\u672c\u7684\u5b66\u4e60\u76ee\u6807\u662f\u5177\u6709\u6bd4ID\u6837\u672c\u66f4\u5c0f\u7684\u7279\u5f81\u8303\u6570\u3002", "result": "BootOOD\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-200\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5176OOD\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u4e8b\u540e\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e0d\u4f7f\u7528\u5f02\u5e38\u503c\u66b4\u9732\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5728ID\u51c6\u786e\u7387\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u503c\u66b4\u9732\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "BootOOD\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u7279\u5f81\u8303\u6570\u534a\u5f84\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u5bf9\u8bed\u4e49\u4e0a\u76f8\u4f3c\u7684OOD\u6837\u672c\u7684\u68c0\u6d4b\uff0c\u5e76\u5728\u4fdd\u6301ID\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13545", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13545", "abs": "https://arxiv.org/abs/2511.13545", "authors": ["Md. Iqbal Hossain", "Afia Sajeeda", "Neeresh Kumar Perla", "Ming Shao"], "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks", "comment": null, "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u62b5\u5fa1\u540e\u95e8\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u8bc6\u522b\u540e\u95e8\u89e6\u53d1\u5668\u3001\u53d7\u5bb3\u8005\u6837\u672c\u548c\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u4fee\u6b63\u53d7\u6c61\u67d3\u7684\u6a21\u578b\u6765\u6d88\u9664\u540e\u95e8\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CLIP\uff09\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6216\u4f7f\u7528\u5927\u578b\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u4e14\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u53d7\u5f71\u54cd\u7684\u6807\u7b7e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u7b56\u7565\uff0c\u5f15\u5165\u56fe\u50cf\u5206\u5272\u201coracle\u201d\u4f5c\u4e3a\u53d7\u6c61\u67d3CLIP\u6a21\u578b\u8f93\u51fa\u7684\u76d1\u7763\u5668\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a1\uff09\u533a\u5206CLIP\u548cOracle\u7684\u77e5\u8bc6\u4ee5\u8bc6\u522b\u6f5c\u5728\u7684\u89e6\u53d1\u5668\uff1b2\uff09\u7cbe\u786e\u5b9a\u4f4d\u53d7\u5f71\u54cd\u7684\u6807\u7b7e\u548c\u53d7\u5bb3\u8005\u6837\u672c\uff0c\u5e76\u7b56\u5212\u4e00\u4e2a\u7cbe Compact \u7684\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u4ee5\u4fee\u6b63\u53d7\u6c61\u67d3\u7684CLIP\u6a21\u578b\uff0c\u6d88\u9664\u540e\u95e8\u6548\u5e94\u3002", "result": "\u901a\u8fc7\u5728\u89c6\u89c9\u8bc6\u522b\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7b56\u7565\u5728\u57fa\u4e8eCLIP\u7684\u540e\u95e8\u9632\u5fa1\u4e2d\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u6709\u6548\u8bc6\u522b\u540e\u95e8\u89e6\u53d1\u5668\u3001\u53d7\u5bb3\u8005\u6837\u672c\u548c\u6807\u7b7e\uff0c\u5e76\u6210\u529f\u4fee\u6b63\u53d7\u6c61\u67d3\u7684CLIP\u6a21\u578b\uff0c\u4ece\u800c\u6d88\u9664\u540e\u95e8\u6548\u5e94\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13552", "abs": "https://arxiv.org/abs/2511.13552", "authors": ["Sining Chen", "Xiao Xiang Zhu"], "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images", "comment": null, "summary": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TSE-Net \u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u5728\u9065\u611f\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6602\u8d35\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u83b7\u53d6\u7684\u6807\u6ce8\u6570\u636e\u3002", "method": "TSE-Net \u91c7\u7528\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u542b\u6559\u5e08\u7f51\u7edc\u3001\u5b66\u751f\u7f51\u7edc\u548c\u8003\u8bd5\u7f51\u7edc\u3002\u5b66\u751f\u7f51\u7edc\u5229\u7528\u6559\u5e08\u7f51\u7edc\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u6559\u5e08\u7f51\u7edc\u7ed3\u5408\u4e86\u56de\u5f52\u548c\u5206\u7c7b\u6a21\u578b\uff0c\u56de\u5f52\u5206\u652f\u9884\u6d4b\u9ad8\u5ea6\u503c\u4f5c\u4e3a\u4f2a\u6807\u7b7e\uff0c\u5206\u7c7b\u5206\u652f\u9884\u6d4b\u9ad8\u5ea6\u503c\u7c7b\u522b\u53ca\u7c7b\u522b\u6982\u7387\uff0c\u7528\u4e8e\u8fc7\u6ee4\u4f2a\u6807\u7b7e\u3002\u4e3a\u5904\u7406\u9ad8\u5ea6\u5206\u5e03\u7684\u957f\u5c3e\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u5206\u5c42\u53cc\u526a\u7b56\u7565\u5b9a\u4e49\u9ad8\u5ea6\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7 Plackett-Luce \u6a21\u578b\u6821\u51c6\u7c7b\u522b\u6982\u7387\u4ee5\u53cd\u6620\u4f2a\u6807\u7b7e\u7684\u9884\u671f\u51c6\u786e\u6027\u3002\u8003\u8bd5\u7f51\u7edc\u4f5c\u4e3a\u5b66\u751f\u7f51\u7edc\u7684\u65f6\u5e8f\u96c6\u6210\u4ee5\u7a33\u5b9a\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u6210\u50cf\u6a21\u5f0f\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "TSE-Net \u901a\u8fc7\u5229\u7528\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u6709\u6548\u514b\u670d\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13571", "abs": "https://arxiv.org/abs/2511.13571", "authors": ["Ziyang Huang", "Jiagang Chen", "Jin Liu", "Shunping Ji"], "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation", "comment": "Accepted at AAAI 2026 as a Conference Paper", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.", "AI": {"tldr": "3DGS\u4f18\u5316\u5b58\u5728\u9677\u5165\u5c40\u90e8\u6700\u4f18\u548c\u6536\u655b\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faOpt3DGS\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u81ea\u9002\u5e94\u63a2\u7d22\uff08SGLD\uff09\u548c\u66f2\u7387\u5f15\u5bfc\u5229\u7528\uff08L-BFGS-Adam\uff09\uff0c\u63d0\u53473DGS\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3DGS\u4f18\u5316\u8fc7\u7a0b\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4e14\u6536\u655b\u8d28\u91cf\u6709\u5f85\u63d0\u9ad8\u3002", "method": "\u63d0\u51faOpt3DGS\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u81ea\u9002\u5e94\u52a0\u6743\u968f\u673a\u68af\u5ea6 Langevin \u52a8\u529b\u5b66\uff08SGLD\uff09\u589e\u5f3a\u5168\u5c40\u641c\u7d22\u4ee5\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5c40\u90e8\u62df\u725b\u987f\u65b9\u5411\u5f15\u5bfc\u7684Adam\u4f18\u5316\u5668\uff0c\u5229\u7528\u66f2\u7387\u4fe1\u606f\u8fdb\u884c\u7cbe\u786e\u9ad8\u6548\u6536\u655b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOpt3DGS\u901a\u8fc7\u6539\u8fdb3DGS\u4f18\u5316\u8fc7\u7a0b\uff0c\u5728\u4e0d\u4fee\u6539\u5176\u5e95\u5c42\u8868\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "Opt3DGS\u901a\u8fc7\u81ea\u9002\u5e94\u63a2\u7d22\u548c\u66f2\u7387\u5f15\u5bfc\u5229\u7528\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u4f18\u5316\u4e2d\u7684\u5c40\u90e8\u6700\u4f18\u548c\u6536\u655b\u8d28\u91cf\u95ee\u9898\uff0c\u8fbe\u5230\u4e86\u9886\u5148\u7684\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2511.13575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13575", "abs": "https://arxiv.org/abs/2511.13575", "authors": ["Linhan Zhou", "Shuang Li", "Neng Dong", "Yonghang Tai", "Yafei Zhang", "Huafeng Li"], "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification", "comment": "9 pages, 4 figures, accepted by AAAI 2026", "summary": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HPL \u7684\u5206\u5c42\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u5904\u7406\u56fe\u50cf\u68c0\u7d22\uff08I2I\uff09\u548c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff08T2I\uff09\u4e24\u79cd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5c06 I2I \u548c T2I \u4efb\u52a1\u5206\u5f00\u5904\u7406\uff0c\u53ef\u80fd\u5bfc\u81f4\u8868\u793a\u7684\u7ea0\u7f20\u548c\u6b21\u4f18\u6027\u80fd\u3002", "method": "HPL \u6846\u67b6\u5229\u7528\u4efb\u52a1\u611f\u77e5\u7684\u63d0\u793a\u5efa\u6a21\u6765\u8054\u5408\u4f18\u5316\u8fd9\u4e24\u4e2a\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u4efb\u52a1\u8def\u7531 Transformer\uff0c\u5c06\u53cc\u5206\u7c7b\u4ee4\u724c\u7eb3\u5165\u5171\u4eab\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5206\u522b\u8def\u7531 I2I \u548c T2I \u5206\u652f\u7684\u7279\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u5c42\u63d0\u793a\u751f\u6210\u65b9\u6848\uff0c\u5c06\u8eab\u4efd\u7ea7\u522b\u7684\u53ef\u5b66\u4e60\u4ee4\u724c\u4e0e\u5b9e\u4f8b\u7ea7\u522b\u7684\u4f2a\u6587\u672c\u4ee4\u724c\u76f8\u7ed3\u5408\u3002\u8fd9\u4e9b\u4f2a\u4ee4\u724c\u901a\u8fc7\u7279\u5b9a\u6a21\u6001\u7684\u9006\u8f6c\u6362\u7f51\u7edc\u4ece\u56fe\u50cf\u6216\u6587\u672c\u7279\u5f81\u4e2d\u63d0\u53d6\uff0c\u5c06\u7ec6\u7c92\u5ea6\u7684\u3001\u5b9e\u4f8b\u7279\u5b9a\u7684\u8bed\u4e49\u6ce8\u5165\u5230\u63d0\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u63d0\u793a\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4ee5\u5728\u63d0\u793a\u4ee4\u724c\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u8bed\u4e49\u5bf9\u9f50\uff0c\u786e\u4fdd\u4f2a\u63d0\u793a\u4fdd\u7559\u6e90\u6a21\u6001\u7684\u7279\u5f81\uff0c\u540c\u65f6\u589e\u5f3a\u8de8\u6a21\u6001\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u5728\u591a\u4e2a ReID \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728 I2I \u548c T2I \u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HPL \u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316 I2I \u548c T2I \u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u521b\u65b0\u7684\u63d0\u793a\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728 ReID \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2511.13586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13586", "abs": "https://arxiv.org/abs/2511.13586", "authors": ["Yinuo Xu", "Yan Cui", "Mingyao Li", "Zhi Huang"], "title": "Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images", "comment": null, "summary": "Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.\n  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.\n  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.", "AI": {"tldr": "NuClass\u662f\u4e00\u4e2a\u7ec6\u80de\u7ea7\u591a\u5c3a\u5ea6\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6838\u5f62\u6001\u548c\u5fae\u73af\u5883\u80cc\u666f\uff0c\u4ee5\u63d0\u9ad8\u7ec6\u80de\u7c7b\u578b\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u5757\u7684\u6a21\u578b\u5728\u6574\u5408\u7ec6\u80de\u529f\u80fd\u548c\u8eab\u4efd\u8bc6\u522b\u7684\u5173\u952e\u7ec4\u7ec7\u80cc\u666f\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u540c\u65f6\uff0c\u53ef\u7528\u7684\u4eba\u7c7b\u6ce8\u91ca\u7c92\u5ea6\u7c97\u7cd9\u4e14\u5206\u5e03\u4e0d\u5747\uff0c\u96be\u4ee5\u83b7\u5f97\u7ec6\u7c92\u5ea6\u7684\u4e9a\u578b\u76d1\u7763\u3002NuClass\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "NuClass\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aPath local\uff08\u5173\u6ce8224x224\u50cf\u7d20\u88c1\u526a\u7684\u6838\u5f62\u6001\uff09\u548cPath global\uff08\u5efa\u6a211024x1024\u50cf\u7d20\u7684\u90bb\u57df\uff09\u3002\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u6a21\u5757\u81ea\u9002\u5e94\u5730\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002\u4e3a\u4e86\u9f13\u52b1\u4e92\u8865\u5b66\u4e60\uff0c\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u76ee\u6807\uff0c\u5f15\u5bfc\u5168\u5c40\u8def\u5f84\u4f18\u5148\u5904\u7406\u5c40\u90e8\u8def\u5f84\u4e0d\u786e\u5b9a\u7684\u533a\u57df\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6821\u51c6\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548cGrad-CAM\u53ef\u89c6\u5316\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u9ad8\u8d28\u91cf\u6ce8\u91ca\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eXenium\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6d4b\u5b9a\u7684\u6807\u8bb0\u5f15\u5bfc\u6570\u636e\u96c6\uff0c\u4e3a\u516b\u4e2a\u5668\u5b98\u548c16\u4e2a\u7c7b\u522b\u4e2d\u7684\u4e24\u767e\u591a\u4e07\u4e2a\u7ec6\u80de\u63d0\u4f9b\u4e86\u5355\u7ec6\u80de\u5206\u8fa8\u7387\u7684\u6807\u7b7e\u3002", "result": "\u5728\u4e09\u4e2a\u5b8c\u5168\u4fdd\u7559\u7684\u961f\u5217\u4e0a\u8bc4\u4f30\uff0cNuClass\u5728\u5176\u8868\u73b0\u6700\u4f73\u7684\u7c7b\u522b\u4e2d\u8fbe\u5230\u4e8696%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u5c3a\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u80fd\u591f\u5f25\u5408\u5e7b\u706f\u7247\u7ea7\u75c5\u7406\u57fa\u7840\u6a21\u578b\u4e0e\u53ef\u9760\u7684\u7ec6\u80de\u7ea7\u8868\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "NuClass\u901a\u8fc7\u591a\u5c3a\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6574\u5408\u7ec4\u7ec7\u80cc\u666f\u4fe1\u606f\u548c\u5904\u7406\u6807\u6ce8\u4e0d\u8db3\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ec6\u80de\u7c7b\u578b\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.13587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13587", "abs": "https://arxiv.org/abs/2511.13587", "authors": ["Haotian Dong", "Ye Li", "Rongwei Lu", "Chen Tang", "Shu-Tao Xia", "Zhi Wang"], "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping", "comment": null, "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u9a8c\u8bc1\u8df3\u8fc7\u673a\u5236\uff0cVVS\u6846\u67b6\u5728\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u4e2d\u5b9e\u73b0\u4e862.8\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u52a0\u901f\u89c6\u89c9\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u800c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u5982\u63a8\u6d4b\u6027\u89e3\u7801\uff09\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9a8c\u8bc1\u8df3\u8fc7\u673a\u5236\u6765\u51cf\u5c11\u524d\u5411\u4f20\u64ad\u6b21\u6570\uff0c\u4ee5\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVVS\u7684\u65b0\u578b\u63a8\u6d4b\u6027\u89e3\u7801\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\uff081\uff09\u5177\u6709\u52a8\u6001\u622a\u65ad\u7684\u65e0\u9a8c\u8bc1\u6807\u8bb0\u9009\u62e9\u5668\u3001\uff082\uff09\u6807\u8bb0\u7ea7\u7279\u5f81\u7f13\u5b58\u548c\u91cd\u7528\u3001\uff083\uff09\u7ec6\u7c92\u5ea6\u8df3\u8fc7\u6b65\u9aa4\u8c03\u5ea6\u8fd9\u4e09\u4e2a\u6a21\u5757\u6765\u5b9e\u73b0\u90e8\u5206\u9a8c\u8bc1\u8df3\u8fc7\u3002", "result": "VVS\u5c06\u76ee\u6807\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u6b21\u6570\u51cf\u5c11\u4e862.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u751f\u6210\u8d28\u91cf\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4f20\u7edf\u63a8\u6d4b\u6027\u89e3\u7801\u6846\u67b6\u7684\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u3002", "conclusion": "VVS\u901a\u8fc7\u5f15\u5165\u9a8c\u8bc1\u8df3\u8fc7\u673a\u5236\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u5176\u91cd\u5851\u63a8\u6d4b\u6027\u89e3\u7801\u8303\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13607", "abs": "https://arxiv.org/abs/2511.13607", "authors": ["Xin Xu", "Hao Liu", "Wei Liu", "Wei Wang", "Jiayi Wu", "Kui Jiang"], "title": "ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement", "comment": "Accepted by AAAI-26", "summary": "Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICLR\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7DIEM\u6a21\u5757\u589e\u5f3a\u4eae\u5ea6\u548c\u8272\u5ea6\u7279\u5f81\u7684\u4e92\u8865\u6027\uff0c\u5e76\u5229\u7528CCL\u635f\u5931\u6765\u89e3\u51b3\u4eae\u5ea6\u548c\u8272\u5ea6\u5206\u652f\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728HVI\u989c\u8272\u7a7a\u95f4\u4e0b\uff0c\u4eae\u5ea6\u548c\u8272\u5ea6\u5206\u652f\u7684\u5206\u5e03\u5dee\u5f02\u548c\u4eae\u5ea6\u8bef\u5dee\u4f20\u9012\u9650\u5236\u4e86\u4e92\u8865\u7279\u5f81\u63d0\u53d6\uff1b\u8272\u5ea6\u5206\u652f\u4e4b\u95f4\u5728\u540c\u8272\u533a\u57df\u5f31\u76f8\u5173\u5bfc\u81f4\u4f20\u7edf\u9010\u50cf\u7d20\u635f\u5931\u4ea7\u751f\u68af\u5ea6\u51b2\u7a81\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faICLR\u6846\u67b6\uff0c\u5305\u542bDIEM\u6a21\u5757\uff08\u901a\u8fc7\u878d\u5408\u548c\u589e\u5f3a\u4e24\u4e2a\u7ef4\u5ea6\u6765\u63d0\u53d6\u4e92\u8865\u4fe1\u606f\uff09\u548cCCL\u635f\u5931\uff08\u5229\u7528\u4eae\u5ea6\u6b8b\u5dee\u7edf\u8ba1\u60e9\u7f5a\u8272\u5ea6\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u8272\u5ea6\u5206\u652f\u534f\u65b9\u5dee\u6765\u5e73\u8861\u68af\u5ea6\u51b2\u7a81\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684ICLR\u6846\u67b6\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ICLR\u6846\u67b6\u901a\u8fc7DIEM\u548cCCL\u7684\u6709\u6548\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u4eae\u5ea6\u548c\u8272\u5ea6\u7279\u5f81\u63d0\u53d6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13609", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13609", "abs": "https://arxiv.org/abs/2511.13609", "authors": ["Marianne Rakic", "Andrew Hoopes", "S. Mazdak Abulnaga", "Mert R. Sabuncu", "John V. Guttag", "Adrian V. Dalca"], "title": "AtlasMorph: Learning conditional deformable templates for brain MRI", "comment": null, "summary": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.", "AI": {"tldr": "We developed a machine learning framework using convolutional neural networks to create adaptive anatomical templates conditioned on subject attributes (e.g., age, sex). These templates improve registration accuracy and are more representative of specific populations compared to traditional static templates.", "motivation": "The creation of deformable templates (atlases) for medical image analysis is computationally expensive, leading to the use of sub-optimal, non-representative templates, especially for populations with significant variations. This hinders accurate analysis.", "method": "A machine learning framework employing convolutional registration neural networks was used to learn a function that generates templates conditioned on subject-specific attributes (age, sex). The framework also leverages segmentations to create probabilistic anatomical label maps for the generated templates and can be used for registering subject images to these templates.", "result": "The framework was demonstrated on 3D brain MRI datasets, successfully learning high-quality, representative templates. Conditional templates, especially when annotated, showed superior registration performance compared to unlabeled unconditional templates and other template construction methods.", "conclusion": "The proposed machine learning framework efficiently learns conditional anatomical templates that are representative of specific populations, leading to improved registration accuracy and outperforming existing methods."}}
{"id": "2511.13615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13615", "abs": "https://arxiv.org/abs/2511.13615", "authors": ["Kesi Xu", "Eleni Chiou", "Ali Varamesh", "Laura Acqualagna", "Nasir Rajpoot"], "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images", "comment": "5 pages, 3 figures. Under review", "summary": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.13618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13618", "abs": "https://arxiv.org/abs/2511.13618", "authors": ["Ashlesha G. Sawant", "Shreyash S. Kamble", "Raj S. Kanade", "Raunak N. Kanugo", "Tanishq A. Kapse", "Karan A. Bhapse"], "title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio", "comment": "6 pages, 8 referenced papers", "summary": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u6444\u50cf\u5934\u548c\u9762\u90e8\u5730\u6807\u68c0\u6d4b\u7684\u9a7e\u9a76\u5458\u55dc\u7761\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u6d4b\u773c\u775b\u7684\u7eb5\u6a2a\u6bd4\uff08EAR\uff09\u6765\u8bc6\u522b\u75b2\u52b3\u8ff9\u8c61\uff0c\u5e76\u53d1\u51fa\u58f0\u97f3\u8b66\u62a5\uff0c\u4ee5\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3002", "motivation": "\u7531\u4e8e\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u5bfc\u81f4\u9053\u8def\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u6bcf\u5e74\u9020\u6210\u6570\u5343\u4eba\u4f24\u4ea1\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u4e2a\u7cfb\u7edf\u6765\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3002", "method": "\u8be5\u7cfb\u7edf\u4f7f\u7528\u6807\u51c6\u7f51\u7edc\u6444\u50cf\u5934\u8ddf\u8e2a\u9a7e\u9a76\u5458\u7684\u9762\u90e8\u7279\u5f81\uff0c\u91cd\u70b9\u662f\u4f7f\u7528\u773c\u775b\u7eb5\u6a2a\u6bd4\uff08EAR\uff09\u65b9\u6cd5\u68c0\u67e5\u773c\u7403\u8fd0\u52a8\u3002\u5b83\u5229\u7528 MediaPipe \u7684 Face Mesh \u6846\u67b6\u6765\u51c6\u786e\u9ad8\u6548\u5730\u8bc6\u522b\u9762\u90e8\u5730\u6807\uff0c\u5e76\u4f7f\u7528 OpenCV \u6765\u5904\u7406\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u975e\u5e38\u51c6\u786e\u4e14\u54cd\u5e94\u8fc5\u901f\uff0c\u80fd\u591f\u68c0\u6d4b\u5230\u957f\u65f6\u95f4\u95ed\u773c\u6216\u7728\u773c\u9891\u7387\u4f4e\u7684\u8ff9\u8c61\uff0c\u5e76\u901a\u8fc7\u58f0\u97f3\u8b66\u62a5\u63d0\u9192\u9a7e\u9a76\u5458\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684\u9a7e\u9a76\u5458\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u7684\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2511.13621", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13621", "abs": "https://arxiv.org/abs/2511.13621", "authors": ["Dimitrios Koutsianos", "Ladislav Mosner", "Yannis Panagakis", "Themos Stafylakis"], "title": "Alpha Divergence Losses for Biometric Verification", "comment": null, "summary": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $\u03b1$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $\u03b1>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $\u03b1$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u57fa\u4e8e \u03b1-\u6563\u5ea6\u4e14\u5305\u542b\u89d2\u5ea6\u8fb9\u754c\u7684\u635f\u5931\u51fd\u6570 Q-Margin \u548c A3M\uff0c\u7528\u4e8e\u4eba\u8138\u548c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4f59\u5f26\u7684\u635f\u5931\u51fd\u6570\uff08\u5982 CosFace \u548c ArcFace\uff09\u5728\u4eba\u8138\u548c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46 \u03b1-\u6563\u5ea6\u635f\u5931\u51fd\u6570\u5728\u5f15\u5165\u89d2\u5ea6\u8fb9\u754c\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5728\u53c2\u8003\u6d4b\u5ea6\u6216\u5bf9\u6570\u4f3c\u7136\u5206\u6570\u4e0a\u5f15\u5165\u89d2\u5ea6\u8fb9\u754c\uff0c\u63a8\u5bfc\u4e86 Q-Margin \u548c A3M \u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\u3002\u9488\u5bf9 A3M \u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u578b\u91cd\u65b0\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "Q-Margin \u548c A3M \u5728 IJB-B \u548c IJB-C \u4eba\u8138\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca VoxCeleb \u8bf4\u8bdd\u4eba\u8bc6\u522b\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8bef\u8bc6\u7387\uff08FAR\uff09\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684 Q-Margin \u548c A3M \u635f\u5931\u51fd\u6570\u5728\u4eba\u8138\u548c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5b89\u5168\u6027\u5e94\u7528\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u964d\u4f4e\u8bef\u8bc6\u7387\u3002"}}
{"id": "2511.13644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13644", "abs": "https://arxiv.org/abs/2511.13644", "authors": ["Shrenik Patel", "Daivik Patel"], "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding", "comment": null, "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.", "AI": {"tldr": "CacheFlow\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u4e22\u5f03\uff08DTD\uff09\u548c\u538b\u7f29\u7684\u957f\u671f\u8bb0\u5fc6\u6765\u89e3\u51b3\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u957f\u89c6\u9891\u95ee\u7b54\uff08VQA\uff09\u5bf9\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u9020\u6210\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u538b\u529b\uff0c\u56e0\u4e3a\u6ce8\u610f\u529b\u548c\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u4f1a\u968f\u7740\u8fd0\u884c\u65f6\u95f4\u800c\u589e\u957f\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6602\u8d35\u7684\u63a8\u7406\u6210\u672c\u6216\u8fd1\u89c6\u7684\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u957f\u89c6\u9891\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u9ad8\u6548 VLM \u65b9\u6cd5\u3002", "method": "CacheFlow \u5f15\u5165\u4e86\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684\u6d41\u6c34\u7ebf\uff0c\u8be5\u6d41\u6c34\u7ebf\u7ed3\u5408\u4e86\u52a8\u6001\u4ee4\u724c\u4e22\u5f03\uff08DTD\uff09\u548c\u538b\u7f29\u7684\u957f\u671f\u8bb0\u5fc6\u3002DTD \u901a\u8fc7\u8ba1\u7b97\u5f53\u524d\u5e27\u4e0e\u524d\u4e00\u5e27\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u5728\u7ebf\u4fee\u526a\u6bcf\u4e2a\u56fe\u50cf\u5757\u7684\u4ee4\u724c\uff0c\u5e76\u5c06\u5b58\u6d3b\u4e0b\u6765\u7684\u4ee4\u724c\u6253\u5305\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u3002\u5728\u5904\u7406\u5757\u65f6\uff0c\u6bcf\u4e2a\u5757\u7684\u952e\u7531\u4e00\u4e2a\u5c0f\u578b\u5faa\u73af\u7f16\u7801\u5668\u8fdb\u884c\u6458\u8981\uff0c\u5f62\u6210\u4e00\u4e2a\u68c0\u7d22\u7d22\u5f15\uff0c\u540c\u65f6\u5757\u7684\u5b8c\u6574 KV \u5bf9\u88ab\u5378\u8f7d\u5e76\u7a0d\u540e\u91cd\u65b0\u6c34\u5408\u4ee5\u8fdb\u884c\u751f\u6210\uff0c\u4ece\u800c\u4fdd\u6301\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002\u5728\u63a8\u7406\u65f6\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5171\u8bc6\u7684\u68c0\u7d22\u673a\u5236\u4ec5\u68c0\u7d22 Top-K \u4e2a\u6700\u76f8\u5173\u7684\u5757\uff0c\u5e76\u540c\u65f6\u5173\u6ce8\u68c0\u7d22\u5230\u7684\u548c\u672c\u5730\u7684\u4e0a\u4e0b\u6587\u4ee5\u8fdb\u884c\u7cbe\u786e\u7684\u957f\u8ddd\u79bb\u63a8\u7406\u3002", "result": "CacheFlow \u5728\u79bb\u7ebf\u548c\u6d41\u5f0f VQA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u5f53\u524d\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5904\u7406\u7684\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u4e86 87%\u3002", "conclusion": "CacheFlow \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u4e0e\u67b6\u6784\u65e0\u5173\u4e14\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u4f7f VLM \u80fd\u591f\u517c\u987e\u6548\u7387\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u4e3a\u5b9e\u9645\u7684\u957f\u89c6\u9891\u7406\u89e3\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2511.13647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13647", "abs": "https://arxiv.org/abs/2511.13647", "authors": ["Chunshi Wang", "Junliang Ye", "Yunhan Yang", "Yang Li", "Zizhuo Lin", "Jun Zhu", "Zhuo Chen", "Yawei Luo", "Chunchao Guo"], "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model", "comment": null, "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/", "AI": {"tldr": "Part-X-MLLM\u662f\u4e00\u4e2a\u7edf\u4e003D\u4efb\u52a1\u7684\u539f\u751f3D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5176\u5236\u5b9a\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u7684\u8bed\u6cd5\u7a0b\u5e8f\u6765\u5b9e\u73b0\u3002", "motivation": "\u7edf\u4e00\u5404\u79cd3D\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u5305\u542b\u90e8\u4ef6\u7ea7\u8fb9\u754c\u6846\u3001\u8bed\u4e49\u63cf\u8ff0\u548c\u7f16\u8f91\u547d\u4ee4\u7684\u5355\u4e00\u3001\u8fde\u8d2f\u7684\u4ee4\u724c\u5e8f\u5217\uff0c\u4f5c\u4e3a\u9a71\u52a8\u4e0b\u6e38\u51e0\u4f55\u611f\u77e5\u6a21\u5757\u7684\u63a5\u53e3\u3002", "method": "\u9884\u8bad\u7ec3\u4e00\u4e2a\u53cc\u7f16\u7801\u5668\u67b6\u6784\u6765\u5206\u79bb\u7ed3\u6784\u548c\u8bed\u4e49\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u3001\u4ee5\u90e8\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6307\u4ee4\u8c03\u6574\u3002", "result": "\u5728\u57fa\u7840\u95ee\u7b54\u3001\u7ec4\u5408\u751f\u6210\u548c\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u53e3\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5c06\u7b26\u53f7\u89c4\u5212\u4e0e\u51e0\u4f55\u5408\u6210\u5206\u79bb\uff0c\u5141\u8bb8\u901a\u8fc7\u5355\u4e00\u7684\u3001\u539f\u751f\u8bed\u8a00\u7684\u524d\u7aef\u6765\u63a7\u5236\u4efb\u4f55\u517c\u5bb9\u7684\u51e0\u4f55\u5f15\u64ce\u3002"}}
{"id": "2511.13649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13649", "abs": "https://arxiv.org/abs/2511.13649", "authors": ["Dengyang Jiang", "Dongyang Liu", "Zanyi Wang", "Qilong Wu", "Xin Jin", "David Liu", "Zhen Li", "Mengmeng Wang", "Peng Gao", "Harry Yang"], "title": "Distribution Matching Distillation Meets Reinforcement Learning", "comment": "The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr", "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMDR\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3\u7684\u591a\u6b65\u6269\u6563\u6a21\u578b\u84b8\u998f\u5230\u5c11\u6b65\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u514b\u670d\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5c11\u6b65\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u514b\u670d\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u591a\u6b65\u6559\u5e08\u6a21\u578b\u7684\u74f6\u9888\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6280\u672f\u878d\u5165\u84b8\u998f\u8fc7\u7a0b\uff0c\u5229\u7528DMD\u635f\u5931\u4f5c\u4e3aRL\u7684\u6b63\u5219\u5316\u9879\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u5206\u5e03\u6307\u5bfc\u548c\u52a8\u6001\u91cd\u566a\u91c7\u6837\u8bad\u7ec3\u7b56\u7565\u3002", "result": "DMDR\u5728\u5c11\u6b65\u6269\u6563\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u9886\u5148\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u63d0\u793a\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u6027\u80fd\u8d85\u8d8a\u4e86\u591a\u6b65\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "DMDR\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u8fdb\u884c\u84b8\u998f\u548cRL\uff0c\u6709\u6548\u91ca\u653e\u4e86\u5c11\u6b65\u751f\u6210\u5668\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2511.13655", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13655", "abs": "https://arxiv.org/abs/2511.13655", "authors": ["Henry Herzog", "Favyen Bastani", "Yawen Zhang", "Gabriel Tseng", "Joseph Redmon", "Hadrien Sablon", "Ryan Park", "Jacob Morrison", "Alexandra Buraczynski", "Karen Farley", "Joshua Hansen", "Andrew Howe", "Patrick Alan Johnson", "Mark Otterlee", "Ted Schmitt", "Hunter Pitelka", "Stephen Daspit", "Rachel Ratner", "Christopher Wilhelm", "Sebastian Wood", "Mike Jacobi", "Hannah Kerner", "Evan Shelhamer", "Ali Farhadi", "Ranjay Krishna", "Patrick Beukema"], "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation", "comment": null, "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.", "AI": {"tldr": "OlmoEarth\u662f\u4e00\u4e2a\u7528\u4e8e\u5730\u7403\u89c2\u6d4b\u7684\u591a\u6a21\u6001\u3001\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u9879\u57fa\u51c6\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u88ab\u6574\u5408\u5230\u4e00\u4e2a\u5e73\u53f0\u4e2d\uff0c\u4ee5\u652f\u6301\u975e\u8425\u5229\u7ec4\u7ec7\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5177\u6709\u7a7a\u95f4\u6027\u3001\u5e8f\u5217\u6027\u548c\u591a\u6a21\u6001\u6027\uff0c\u8fd9\u7ed9\u6570\u636e\u5904\u7406\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u9488\u5bf9\u5730\u7403\u89c2\u6d4b\u9886\u57df\u8bbe\u8ba1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u7aef\u5230\u7aef\u5e73\u53f0\u3002", "result": "OlmoEarth\u572824\u9879\u8bc4\u4f30\u4efb\u52a1\u4e2d\u768415\u9879\u4ee5\u53ca29\u9879\u5b8c\u5168\u5fae\u8c03\u4efb\u52a1\u4e2d\u768419\u9879\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u4f18\u4e8e12\u79cd\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "OlmoEarth\u4e3a\u5730\u7403\u89c2\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u548c\u5e73\u53f0\uff0c\u80fd\u591f\u652f\u6301\u975e\u8425\u5229\u7ec4\u7ec7\u5e94\u5bf9\u5168\u7403\u6027\u6311\u6218\u3002"}}
{"id": "2511.13684", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13684", "abs": "https://arxiv.org/abs/2511.13684", "authors": ["Jiangnan Ye", "Jiedong Zhuang", "Lianrui Mu", "Wenjie Zheng", "Jiaqi Hu", "Xingze Zou", "Jing Wang", "Haoji Hu"], "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting", "comment": "Submitting for Neurocomputing", "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.", "AI": {"tldr": "GS-Light\u662f\u4e00\u4e2a\u7528\u4e8e3D\u9ad8\u65af\u55b7\u6e85\u573a\u666f\u7684\u6587\u672c\u5f15\u5bfc\u5149\u7167\u8c03\u6574\u7ba1\u7ebf\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u5149\u7167\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u74063D\u573a\u666f\u7684\u6587\u672c\u5f15\u5bfc\u5149\u7167\u8c03\u6574\u65f6\uff0c\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u5149\u7167\u65b9\u5411\u548c\u7ec6\u8282\uff0c\u4e14\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u9ad8\u6548\u4e14\u80fd\u7cbe\u786e\u63a7\u5236\u5149\u7167\u6548\u679c\u7684\u65b9\u6cd5\u3002", "method": "GS-Light\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u5149\u7167\u8c03\u6574\uff1a1. \u4f7f\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u89e3\u6790\u7528\u6237\u8f93\u5165\u7684\u6587\u672c\u63d0\u793a\uff0c\u63d0\u53d6\u5149\u7167\u5148\u9a8c\u4fe1\u606f\u30022. \u5229\u7528\u73b0\u6210\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u4f30\u8ba1\u5668\uff08\u6df1\u5ea6\u3001\u6cd5\u7ebf\u3001\u8bed\u4e49\u5206\u5272\uff09\u83b7\u53d6\u573a\u666f\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u30023. \u878d\u5408\u5149\u7167\u5148\u9a8c\u548c\u89c6\u56fe-\u51e0\u4f55\u7ea6\u675f\uff0c\u8ba1\u7b97\u5149\u7167\u56fe\u5e76\u751f\u6210\u591a\u89c6\u56fe\u521d\u59cb\u6f5c\u5728\u7f16\u7801\u30024. \u5c06\u521d\u59cb\u6f5c\u5728\u7f16\u7801\u8f93\u5165\u5230\u591a\u89c6\u56fe\u5149\u7167\u8c03\u6574\u6a21\u578b\u4e2d\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u5149\u7167\u8c03\u6574\u56fe\u50cf\u30025. \u901a\u8fc7\u8c03\u6574\u9ad8\u65af\u55b7\u6e85\u573a\u666f\u7684\u5916\u89c2\uff0c\u83b7\u5f97\u5b8c\u5168\u5149\u7167\u8c03\u6574\u540e\u76843D\u573a\u666f\u3002", "result": "GS-Light\u5728\u5ba4\u5185\u5916\u573a\u666f\u7684\u8bc4\u4f30\u4e2d\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u9010\u89c6\u56fe\u5149\u7167\u8c03\u6574\u3001\u89c6\u9891\u5149\u7167\u8c03\u6574\u548c\u573a\u666f\u7f16\u8f91\u65b9\u6cd5\uff09\u76f8\u6bd4\uff0c\u5728\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u3001\u7f8e\u5b66\u8bc4\u5206\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u7b49\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002\u7528\u6237\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "GS-Light\u662f\u4e00\u79cd\u6709\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc3D\u573a\u666f\u5149\u7167\u8c03\u6574\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u63d0\u793a\u751f\u6210\u7cbe\u786e\u3001\u9ad8\u8d28\u91cf\u7684\u5149\u7167\u6548\u679c\uff0c\u5e76\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2511.13704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13704", "abs": "https://arxiv.org/abs/2511.13704", "authors": ["Harold Haodong Chen", "Disen Lan", "Wen-Jie Shu", "Qingyang Liu", "Zihan Wang", "Sirui Chen", "Wenkai Cheng", "Kanghao Chen", "Hongfei Zhang", "Zixin Zhang", "Rongjin Guo", "Yu Cheng", "Ying-Cong Chen"], "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models", "comment": "Project: https://haroldchen19.github.io/TiViBench-Page/", "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.", "AI": {"tldr": "TiViBench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86VideoTPO\u6d4b\u8bd5\u65f6\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u672a\u80fd\u6355\u6349\u66f4\u9ad8\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "TiViBench\u662f\u4e00\u4e2a\u5206\u5c42\u57fa\u51c6\uff0c\u8bc4\u4f30\u7ed3\u6784\u63a8\u7406\u4e0e\u641c\u7d22\u3001\u7a7a\u95f4\u4e0e\u89c6\u89c9\u6a21\u5f0f\u63a8\u7406\u3001\u7b26\u53f7\u4e0e\u903b\u8f91\u63a8\u7406\u3001\u52a8\u4f5c\u89c4\u5212\u4e0e\u4efb\u52a1\u6267\u884c\u56db\u4e2a\u7ef4\u5ea6\u3002VideoTPO\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u7b56\u7565\uff0c\u901a\u8fc7LLM\u81ea\u6211\u5206\u6790\u6765\u8bc6\u522b\u751f\u6210\u89c6\u9891\u7684\u4f18\u7f3a\u70b9\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5546\u4e1a\u6a21\u578b\uff08\u5982Sora 2\uff0cVeo 3.1\uff09\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u6f5c\u529b\u3002\u5f00\u6e90\u6a21\u578b\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u8bad\u7ec3\u89c4\u6a21\u548c\u6570\u636e\u591a\u6837\u6027\u7684\u9650\u5236\u3002VideoTPO\u7b56\u7565\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TiViBench\u548cVideoTPO\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u8be5\u65b0\u5174\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13713", "abs": "https://arxiv.org/abs/2511.13713", "authors": ["Xincheng Shuai", "Zhenyuan Qin", "Henghui Ding", "Dacheng Tao"], "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine", "comment": "AAAI 2026, Project Page: https://henghuiding.com/FFSE/", "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.", "AI": {"tldr": "FFSE\u662f\u4e00\u4e2a3D\u611f\u77e5\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u8fdb\u884c\u5bf9\u8c61\u7f16\u8f91\uff0c\u5b9e\u73b0\u5e73\u79fb\u3001\u7f29\u653e\u548c\u65cb\u8f6c\u7b49\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u903c\u771f\u7684\u80cc\u666f\u6548\u679c\u548c\u5168\u5c40\u573a\u666f\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3a3DObjectEditor\u7684\u65b0\u6df7\u5408\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5355\u8f6e\u548c\u591a\u8f6e3D\u611f\u77e5\u7f16\u8f91\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u57283D\u611f\u77e5\u5bf9\u8c61\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u76f4\u89c2\u3001\u7269\u7406\u4e00\u81f4\u7684\u5bf9\u8c61\u7f16\u8f91\u7684\u6846\u67b6\u3002", "method": "FFSE\u5c06\u7f16\u8f91\u5efa\u6a21\u4e3a\u4e00\u7cfb\u5217\u5b66\u4e60\u5230\u76843D\u53d8\u6362\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a3DObjectEditor\u7684\u6df7\u5408\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u591a\u8f6e3D\u611f\u77e5\u5bf9\u8c61\u64cd\u4f5c\u3002", "result": "FFSE\u5728\u5355\u8f6e\u548c\u591a\u8f6e3D\u611f\u77e5\u7f16\u8f91\u573a\u666f\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FFSE\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c3D\u611f\u77e5\u5bf9\u8c61\u7f16\u8f91\uff0c\u5e76\u80fd\u4fdd\u6301\u903c\u771f\u7684\u80cc\u666f\u6548\u679c\u548c\u5168\u5c40\u573a\u666f\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.13714", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13714", "abs": "https://arxiv.org/abs/2511.13714", "authors": ["Junwei Yu", "Trevor Darrell", "XuDong Wang"], "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity", "comment": null, "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.", "AI": {"tldr": "UnSAMv2\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5206\u5272\u7c92\u5ea6\u7684\u4efb\u610f\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM-2\u5728\u591a\u79cd\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u5728\u63a7\u5236\u5206\u5272\u7c92\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7528\u6237\u9700\u8981\u624b\u52a8\u8c03\u6574\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u679c\u4e0d\u786e\u5b9a\u3002", "method": "UnSAMv2\u5728UnSAM\u7684\u57fa\u7840\u4e0a\uff0c\u6269\u5c55\u4e86\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\uff0c\u53d1\u73b0\u4e86\u66f4\u591a\u7684\u63a9\u7801-\u7c92\u5ea6\u5bf9\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u7c92\u5ea6\u63a7\u5236\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5206\u5272\u5c3a\u5ea6\u7684\u7cbe\u786e\u3001\u8fde\u7eed\u63a7\u5236\u3002", "result": "\u5728\u4ec5\u4f7f\u75286K\u65e0\u6807\u7b7e\u56fe\u50cf\u548c0.02%\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0cUnSAMv2\u663e\u8457\u589e\u5f3a\u4e86SAM-2\uff0c\u5728\u4ea4\u4e92\u5f0f\u3001\u5168\u56fe\u548c\u89c6\u9891\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4efb\u610f\u7c92\u5ea6\u7684\u5206\u5272\u3002\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUnSAMv2\u5728NoC90\uff085.69->4.75\uff09\u30011-IoU\uff0858.0->73.1\uff09\u548cAR1000\uff0849.6->68.3\uff09\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u5c11\u91cf\u7684\u65e0\u6807\u7b7e\u6570\u636e\u914d\u5408\u611f\u77e5\u7c92\u5ea6\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5145\u5206\u6316\u6398\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13715", "abs": "https://arxiv.org/abs/2511.13715", "authors": ["Hengrui Hu", "Kaining Ying", "Henghui Ding"], "title": "Segment Anything Across Shots: A Method and Benchmark", "comment": "AAAI 2026, Project Page: https://henghuiding.com/SAAS/", "summary": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAAS\u7684\u591a\u955c\u5934\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aCut-VOS\u7684\u65b0\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u955c\u5934\u5207\u6362\u65f6\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08VOS\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u955c\u5934\u89c6\u9891\uff0c\u5728\u5904\u7406\u591a\u955c\u5934\u89c6\u9891\u65f6\uff0c\u5c24\u5176\u662f\u5728\u955c\u5934\u5207\u6362\u5904\uff0c\u5206\u5272\u6548\u679c\u4f1a\u53d7\u5230\u5f88\u5927\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u955c\u5934\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08MVOS\uff09\u4e2d\u7684\u955c\u5934\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u6e21\u6a21\u4eff\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff08TMA\uff09\uff0c\u4ec5\u4f7f\u7528\u5355\u955c\u5934\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8de8\u955c\u5934\u6cdb\u5316\uff0c\u4ee5\u7f13\u89e3\u591a\u955c\u5934\u6807\u6ce8\u6570\u636e\u7a00\u758f\u7684\u95ee\u9898\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86Segment Anything Across Shots (SAAS) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u7406\u89e3\u955c\u5934\u8f6c\u6362\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aCut-VOS\u7684\u65b0MVOS\u57fa\u51c6\uff0c\u5305\u542b\u5bc6\u96c6\u63a9\u7801\u6807\u6ce8\u3001\u591a\u6837\u7684\u5bf9\u8c61\u7c7b\u522b\u548c\u9ad8\u9891\u8fc7\u6e21\uff0c\u4ee5\u652f\u6301MVOS\u7684\u8bc4\u4f30\u548c\u672a\u6765\u7814\u7a76\u3002", "result": "\u5728YouMVOS\u548cCut-VOS\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SAAS\u6a21\u578b\u901a\u8fc7\u6709\u6548\u6a21\u4eff\u3001\u7406\u89e3\u548c\u5206\u5272\u590d\u6742\u7684\u8fc7\u6e21\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SAAS\u6a21\u578b\u548cTMA\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u955c\u5934\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u7684\u955c\u5934\u5207\u6362\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u5de5\u5177\u3002"}}
{"id": "2511.13720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13720", "abs": "https://arxiv.org/abs/2511.13720", "authors": ["Tianhong Li", "Kaiming He"], "title": "Back to Basics: Let Denoising Generative Models Denoise", "comment": "Tech report. Code at https://github.com/LTH14/JiT", "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684JiT\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u6e05\u6670\u56fe\u50cf\uff0c\u800c\u975e\u566a\u58f0\uff0c\u5728ImageNet\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u4e0d\u76f4\u63a5\u9884\u6d4b\u6e05\u6670\u56fe\u50cf\uff0c\u800c\u662f\u9884\u6d4b\u566a\u58f0\u6216\u542b\u566a\u58f0\u91cf\uff0c\u8fd9\u4e0e\u81ea\u7136\u6570\u636e\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u7684\u5047\u8bbe\u76f8\u6096\u3002", "method": "\u63d0\u51faJiT\uff08Just image Transformers\uff09\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u50cf\u7d20\u4e0a\u64cd\u4f5c\uff0c\u4f7f\u7528\u5927\u5757Transformer\uff0c\u65e0\u9700\u5206\u8bcd\u5668\u3001\u9884\u8bad\u7ec3\u6216\u989d\u5916\u635f\u5931\u3002", "result": "\u5728ImageNet\u4e0a\uff0c\u4f7f\u752816\u548c32\u7684\u5757\u5927\u5c0f\uff0c\u5728256\u548c512\u5206\u8fa8\u7387\u4e0b\uff0cJiT\u6a21\u578b\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u8868\u660e\u76f4\u63a5\u9884\u6d4b\u542b\u566a\u58f0\u91cf\u53ef\u80fd\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u5931\u8d25\u3002", "conclusion": "JiT\u6a21\u578b\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u6e05\u6670\u6570\u636e\uff0c\u56de\u5f52\u5230\u57fa\u4e8e\u6d41\u5f62\u5047\u8bbe\u7684Transformer\u6269\u6563\u57fa\u672c\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u6a21\u578b\u5728\u5904\u7406\u9ad8\u7ef4\u539f\u59cb\u6570\u636e\u65f6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11676", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11676", "abs": "https://arxiv.org/abs/2511.11676", "authors": ["Hanchen David Wang", "Siwoo Bae", "Zirong Chen", "Meiyi Ma"], "title": "Learning with Preserving for Continual Multitask Learning", "comment": "25 pages, 16 figures, accepted at AAAI-2026", "summary": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.", "AI": {"tldr": "LwP\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u89e3\u51b3\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\uff08CMTL\uff09\u8bbe\u7f6e\u4e2d\u5e38\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b66\u4e60\u7684\u7279\u5b9a\u4efb\u52a1\u7279\u5f81\u4f1a\u76f8\u4e92\u5e72\u6270\uff0c\u5bfc\u81f4\u9057\u5fd8\u5148\u524d\u5b66\u5230\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLwP\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u8ddd\u79bb\u4fdd\u6301\uff08DWDP\uff09\u635f\u5931\u6765\u4fdd\u6301\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8be5\u635f\u5931\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6570\u636e\u8868\u793a\u4e4b\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\u6765\u9632\u6b62\u8868\u793a\u6f02\u79fb\uff0c\u800c\u65e0\u9700\u91cd\u653e\u7f13\u51b2\u533a\u3002", "result": "LwP\u4e0d\u4ec5\u51cf\u8f7b\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u4e14\u5728CMTL\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "LwP\u901a\u8fc7\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u6709\u6548\u89e3\u51b3CMTL\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5355\u4efb\u52a1\u5b66\u4e60\u7684\u6027\u80fd\u548c\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.11680", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11680", "abs": "https://arxiv.org/abs/2511.11680", "authors": ["Udaya Bhasker Cheerala", "Varun Teja Chirukuri", "Venkata Akhil Kumar Gummadi", "Jintu Moni Bhuyan", "Praveen Damacharla"], "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP", "comment": "7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)", "summary": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\uff08SHAP\uff09\u4e3a\u52a0\u5229\u798f\u5c3c\u4e9a\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u91ce\u706b\u98ce\u9669\u5730\u56fe\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\u548c\u9ad8\u98ce\u9669\u533a\u57df\uff0c\u4e3a\u91ce\u706b\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "motivation": "\u52a0\u5229\u798f\u5c3c\u4e9a\u91ce\u706b\u9891\u53d1\uff0c\u5bf9\u751f\u6001\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5168\u9762\u7684\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5e94\u7528\u968f\u673a\u68ee\u6797\uff08RF\uff09\u7b97\u6cd5\u6784\u5efa\u91ce\u706b\u98ce\u9669\u6a21\u578b\uff0c\u5e76\u7ed3\u5408Shapley Additive exPlanations\uff08SHAP\uff09\u8fdb\u884c\u6a21\u578b\u89e3\u91ca\uff0c\u6700\u540e\u8fdb\u884c\u7a7a\u95f4\u548c\u65f6\u95f4\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u751f\u6001\u7cfb\u7edf\uff08\u68ee\u6797\u548c\u8349\u539f\uff09\u7684\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\u548c\u533a\u57df\u5206\u5e03\u3002", "result": "RF\u6a21\u578b\u5728\u9884\u6d4b\u8349\u539f\uff08AUC=0.996\uff09\u548c\u68ee\u6797\uff08AUC=0.997\uff09\u91ce\u706b\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u663e\u793a\u6a21\u578b\u5177\u6709\u4e2d\u7b49\u7684\u8fc1\u79fb\u80fd\u529b\uff08\u68ee\u6797ROC-AUC=0.6155\uff0c\u8349\u539fROC-AUC=0.5416\uff09\uff0c\u800c\u65f6\u95f4\u5206\u5272\u9a8c\u8bc1\u663e\u793a\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff08\u68ee\u6797ROC-AUC=0.6615\uff0cPR-AUC=0.8423\uff09\u3002SHAP\u5206\u6790\u786e\u5b9a\u4e86\u68ee\u6797\u548c\u8349\u539f\u7684\u5173\u952e\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\uff0c\u5982\u571f\u58e4\u6709\u673a\u78b3\u3001\u6811\u6728\u8986\u76d6\u7387\u3001NDVI\u3001\u5730\u8868\u6e29\u5ea6\u548c\u6d77\u62d4\u3002\u98ce\u9669\u5730\u56fe\u663e\u793a\uff0cCentral Valley\u548cNorthern Buttes\u5730\u533a\u8349\u539f\u706b\u707e\u98ce\u9669\u6700\u9ad8\uff0cNorthern Buttes\u548cNorth Coast Redwoods\u5730\u533a\u68ee\u6797\u706b\u707e\u98ce\u9669\u6700\u9ad8\u3002", "conclusion": "RF-SHAP\u6846\u67b6\u4e3a\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u51cf\u707e\u7b56\u7565\u3002"}}
{"id": "2511.11681", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11681", "abs": "https://arxiv.org/abs/2511.11681", "authors": ["Penghui Niu", "Jiashuai She", "Taotao Cai", "Yajuan Zhang", "Ping Zhang", "Junhua Gu", "Jianxin Li"], "title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation", "comment": null, "summary": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.", "AI": {"tldr": "MPCM-Net\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u548cMamba\u67b6\u6784\u6765\u63d0\u9ad8\u4e91\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5f15\u5165\u65b0\u7684CSRC\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4e91\u56fe\u50cf\u5206\u5272\u65b9\u9762\u5b58\u5728\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u4e0d\u8db3\u3001\u7279\u5f81\u589e\u5f3a\u6548\u7387\u4e0d\u9ad8\u4ee5\u53ca\u89e3\u7801\u5668\u65e0\u6cd5\u5efa\u7acb\u5168\u5c40\u4f9d\u8d56\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faMPCM-Net\uff0c\u7f16\u7801\u5668\u4f7f\u7528\u5305\u542bParCM\u548cParSM\u7684MPAC\u5757\u8fdb\u884c\u7a7a\u95f4\u4ea4\u4e92\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u89e3\u7801\u5668\u4f7f\u7528M2B\u6a21\u5757\u548cSSHD\u8fdb\u884c\u4e0a\u4e0b\u6587\u635f\u5931\u7684\u7f13\u89e3\u548c\u6df1\u5c42\u7279\u5f81\u805a\u5408\u3002", "result": "MPCM-Net\u5728CSRC\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "MPCM-Net\u5728\u4e91\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684CSRC\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2511.11683", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11683", "abs": "https://arxiv.org/abs/2511.11683", "authors": ["Longhua Li", "Lei Qi", "Xin Geng"], "title": "Stratified Knowledge-Density Super-Network for Scalable Vision Transformers", "comment": "Accepted by AAAI 2026", "summary": "Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \\textbf{W}eighted \\textbf{P}CA for \\textbf{A}ttention \\textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \\textbf{P}rogressive \\textbf{I}mportance-\\textbf{A}ware \\textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3ViT\u6a21\u578b\u8f6c\u6362\u4e3a\u5206\u5c42\u77e5\u8bc6\u5bc6\u5ea6\u8d85\u7f51\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7WPAC\u548cPIAD\u6280\u672f\u5b9e\u73b0\u77e5\u8bc6\u7684\u538b\u7f29\u548c\u5206\u5c42\u7ec4\u7ec7\uff0c\u4ece\u800c\u80fd\u591f\u7075\u6d3b\u63d0\u53d6\u9002\u5e94\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u7684\u5b50\u7f51\u7edc\uff0c\u5e76\u80fd\u5728\u6a21\u578b\u538b\u7f29\u548c\u6269\u5c55\u65b9\u9762\u63d0\u4f9b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u89e3\u51b3\u8bad\u7ec3\u548c\u90e8\u7f72\u591a\u79cdViT\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u6240\u5e26\u6765\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3ViT\u6a21\u578b\u8f6c\u6362\u4e3a\u5206\u5c42\u77e5\u8bc6\u5bc6\u5ea6\u8d85\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u5f15\u5165\u4e86\u52a0\u6743PCA\u6ce8\u610f\u529b\u6536\u7f29\uff08WPAC\uff09\u6280\u672f\uff0c\u5c06\u77e5\u8bc6\u96c6\u4e2d\u5230\u5173\u952e\u6743\u91cd\u4e2d\uff0c\u5e76\u901a\u8fc7\u6ce8\u5165\u53d8\u6362\u548c\u9006\u53d8\u6362\u77e9\u9635\u6765\u4fdd\u7559\u539f\u59cb\u7f51\u7edc\u529f\u80fd\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u6e10\u8fdb\u91cd\u8981\u6027\u611f\u77e5\u4e22\u5f03\uff08PIAD\uff09\u6280\u672f\uff0c\u901a\u8fc7\u8bc4\u4f30\u6743\u91cd\u7ec4\u7684\u91cd\u8981\u6027\u5e76\u8fdb\u884c\u76f8\u5e94\u7684\u4e22\u5f03\uff0c\u6765\u4fc3\u8fdb\u77e5\u8bc6\u7684\u5206\u5c42\u7ec4\u7ec7\u3002", "result": "WPAC\u5728\u77e5\u8bc6\u96c6\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u526a\u679d\u6807\u51c6\u3002WPAC\u4e0ePIAD\u7ed3\u5408\u540e\uff0c\u5728\u6a21\u578b\u538b\u7f29\u548c\u6a21\u578b\u6269\u5c55\u65b9\u9762\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "WPAC\u548cPIAD\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u5c06ViT\u6a21\u578b\u538b\u7f29\u5e76\u7ec4\u7ec7\u6210\u4e00\u4e2a\u5206\u5c42\u77e5\u8bc6\u5bc6\u5ea6\u7684\u8d85\u7f51\u7edc\uff0c\u4ece\u800c\u80fd\u591f\u7075\u6d3b\u5730\u63d0\u53d6\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u5c3a\u5bf8\u7684\u5b50\u7f51\u7edc\uff0c\u5e76\u5728\u6a21\u578b\u538b\u7f29\u548c\u6269\u5c55\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11688", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11688", "abs": "https://arxiv.org/abs/2511.11688", "authors": ["Aihua Zhu", "Rui Su", "Qinglin Zhao", "Li Feng", "Meng Shen", "Shibo He"], "title": "Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling", "comment": null, "summary": "Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.", "AI": {"tldr": "\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u6846\u67b6HSO\uff0c\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u5728\u6781\u4f4e\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff08NFE\uff09\u4e0b\u7684\u91c7\u6837\u52a0\u901f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u8d28\u91cf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u81ea\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u6838\u5fc3\u539f\u5219\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSO\uff08Hierarchical-Schedule-Optimizer\uff09\u7684\u65b0\u578b\u5206\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u66ff\u7684\u4e0a\u4e0b\u4e24\u5c42\u4f18\u5316\u6765\u5bfb\u627e\u5168\u5c40\u6700\u4f18\u91c7\u6837\u65f6\u95f4\u8868\uff1a\u4e0a\u5c42\u8fdb\u884c\u5168\u5c40\u641c\u7d22\u4ee5\u786e\u5b9a\u6700\u4f18\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4e0b\u5c42\u8fdb\u884c\u5c40\u90e8\u4f18\u5316\u4ee5\u7cbe\u70bc\u65f6\u95f4\u8868\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e2d\u671f\u8bef\u5dee\u4ee3\u7406\uff08MEP\uff09\u548c\u95f4\u9694\u60e9\u7f5a\u9002\u5e94\u5ea6\uff08SPF\uff09\u51fd\u6570\uff0c\u5206\u522b\u7528\u4e8e\u6709\u6548\u4f18\u5316\u548c\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "HSO\u5728\u6781\u4f4eNFE\u7684\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u4ec5\u4f7f\u75285\u6b21\u51fd\u6570\u8bc4\u4f30\uff08NFE=5\uff09\u7684\u60c5\u51b5\u4e0b\uff0cHSO\u5728LAION-Aesthetics\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Stable Diffusion v2.1\u6a21\u578b\u8fbe\u5230\u4e8611.94\u7684FID\u5206\u6570\u3002", "conclusion": "HSO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u4e00\u6b21\u6027\u4f18\u5316\uff08\u6210\u672c\u4f4e\u4e8e8\u79d2\uff09\u5373\u53ef\u5728\u6781\u4f4eNFE\u4e0b\u5b9e\u73b0\u9876\u5c16\u7684\u91c7\u6837\u6027\u80fd\uff0c\u65e0\u9700\u8fdb\u884c\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2511.11690", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11690", "abs": "https://arxiv.org/abs/2511.11690", "authors": ["Fei Song", "Yi Li", "Rui Wang", "Jiahuan Zhou", "Changwen Zheng", "Jiangmeng Li"], "title": "Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models", "comment": "Accepted by AAAI2026", "summary": "Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u7684\u8c03\u5236\u6a21\u5757\u548c\u53ef\u9760\u6027\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u6a21\u5757\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u53bb\u504f\u7684\u6d4b\u8bd5\u65f6\u95f4\u63d0\u793a\u8c03\u6574\u65b9\u6cd5\uff08DB-TTPT\uff09\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6d4b\u8bd5\u65f6\u95f4\u63d0\u793a\u8c03\u6574\u4e2d\u7684\u4f18\u5316\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u95f4\u63d0\u793a\u8c03\u6574\uff08TTPT\uff09\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ec5\u57fa\u4e8e\u65e0\u6807\u7b7e\u6d4b\u8bd5\u6570\u636e\u8c03\u6574\u53ef\u5b66\u4e60\u63d0\u793a\u4f1a\u5bfc\u81f4\u4f18\u5316\u504f\u89c1\uff0c\u4ece\u800c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u5206\u6790\u5e76\u89e3\u51b3\u8fd9\u4e00\u4f18\u5316\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u53bb\u504f\u7684\u6d4b\u8bd5\u65f6\u95f4\u63d0\u793a\u8c03\u6574\uff08DB-TTPT\uff09\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u7684\u8c03\u5236\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4f7f\u7528\u6d4b\u8bd5\u56fe\u50cf\u7279\u5f81\u4f5c\u4e3a\u67e5\u8be2\uff0c\u4ece\u52a8\u6001\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u9ad8\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\uff0c\u5e76\u7528\u4ee5\u8c03\u6574\u6a21\u578b\u9884\u6d4b\u3002\u7136\u540e\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u9760\u6027\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u6a21\u5757\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a0\u6743\u96c6\u6210\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u84b8\u998f\uff0c\u5728\u63d0\u793a\u8c03\u6574\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u6b63\u5219\u5316\u7ea6\u675f\u3002", "result": "\u5728\u6d89\u53ca\u81ea\u7136\u5206\u5e03\u53d8\u5316\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u768415\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DB-TTPT\u65b9\u6cd5\u5728\u51cf\u8f7b\u63d0\u793a\u4f18\u5316\u504f\u89c1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DB-TTPT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u51cf\u8f7b\u6d4b\u8bd5\u65f6\u95f4\u63d0\u793a\u8c03\u6574\u4e2d\u7684\u4f18\u5316\u504f\u89c1\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u5206\u5e03\u53d8\u5316\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11692", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11692", "abs": "https://arxiv.org/abs/2511.11692", "authors": ["Jiayin Zhu", "Linlin Yang", "Yicong Li", "Angela Yao"], "title": "AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation", "comment": "Accepted by AAAI 2026. Project page: https://jyzhu.top/AnchorDS_Webpage/", "summary": "Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to \"semantic over-smoothing\" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.", "AI": {"tldr": "SDS\u5ffd\u7565\u4e86\u6e90\u52a8\u6001\uff0c\u5bfc\u81f4\u4e86\u201c\u8bed\u4e49\u8fc7\u5e73\u6ed1\u201d\u7684\u4f2a\u5f71\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u52303D\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u52a8\u6001\u6f14\u5316\u7684\u6e90\u5206\u5e03\u6620\u5c04\u5230\u56fa\u5b9a\u76ee\u6807\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5f15\u5165AnchorDS\u6765\u6539\u8fdbSDS\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u3001\u66f4\u81ea\u7136\u7684\u989c\u8272\u548c\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "SDS\u65b9\u6cd5\u5728\u6587\u672c\u52303D\u4f18\u5316\u4e2d\u5ffd\u7565\u4e86\u6e90\u52a8\u6001\uff0c\u5bfc\u81f4\u4e86\u201c\u8bed\u4e49\u8fc7\u5e73\u6ed1\u201d\u7b49\u4f2a\u5f71\uff0c\u6291\u5236\u6216\u5408\u5e76\u4e86\u8bed\u4e49\u7ebf\u7d22\uff0c\u4ea7\u751f\u4e86\u4e0d\u4e00\u81f4\u7684\u8f68\u8ff9\u3002", "method": "\u5c06\u6587\u672c\u52303D\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a8\u6001\u6f14\u5316\u7684\u6e90\u5206\u5e03\u5230\u56fa\u5b9a\u76ee\u6807\u5206\u5e03\u7684\u6620\u5c04\u3002\u5728\u6587\u672c\u63d0\u793a\u548c\u4e2d\u95f4\u6e32\u67d3\u56fe\u50cf\u7684\u8054\u5408\u6761\u4ef6\u4e0b\uff0c\u5c06\u95ee\u9898\u7eb3\u5165\u4e00\u4e2a\u5bf9\u5076\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u3002\u5f15\u5165AnchorDS\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u5f97\u5206\u84b8\u998f\u673a\u5236\uff0c\u63d0\u4f9b\u5177\u6709\u56fe\u50cf\u6761\u4ef6\u7684\u3001\u72b6\u6001\u951a\u5b9a\u7684\u5f15\u5bfc\uff0c\u5e76\u7a33\u5b9a\u751f\u6210\u3002\u901a\u8fc7\u60e9\u7f5a\u9519\u8bef\u7684\u6e90\u4f30\u8ba1\u548c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7684\u8fc7\u6ee4\u548c\u5fae\u8c03\u7b56\u7565\u6765\u4f18\u5316\u951a\u70b9\u3002", "result": "AnchorDS\u5728\u7ec6\u8282\u3001\u989c\u8272\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnchorDS\u901a\u8fc7\u5f15\u5165\u72b6\u6001\u951a\u5b9a\u7684\u5f15\u5bfc\u548c\u5bf9\u5076\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\uff0c\u6210\u529f\u89e3\u51b3\u4e86SDS\u5728\u6587\u672c\u52303D\u4f18\u5316\u4e2d\u5b58\u5728\u7684\u201c\u8bed\u4e49\u8fc7\u5e73\u6ed1\u201d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2511.11693", "categories": ["cs.AI", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11693", "abs": "https://arxiv.org/abs/2511.11693", "authors": ["Xin Zhao", "Xiaojun Chen", "Bingshan Liu", "Zeyao Liu", "Zhendong Zhao", "Xiaoyan Gu"], "title": "Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation", "comment": null, "summary": "Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.", "AI": {"tldr": "VALOR\u662f\u4e00\u4e2a\u7528\u4e8e\u5b89\u5168\u3001\u6709\u5e2e\u52a9\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u591a\u5c42\u63d0\u793a\u5206\u6790\u548c\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u63a8\u7406\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5e76\u91cd\u5199\u4e0d\u5b89\u5168\u6216\u4e0d\u6070\u5f53\u7684\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u548c\u7528\u6237\u610f\u56fe\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u751f\u6210\u4e0d\u5b89\u5168\u3001\u5192\u72af\u6027\u6216\u6587\u5316\u4e0d\u5f53\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u98ce\u9669\uff0c\u800c\u73b0\u6709\u7684\u9632\u5fa1\u63aa\u65bd\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u3002", "method": "VALOR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u3001\u96f6\u6837\u672c\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\u5206\u5c42\u63d0\u793a\u5206\u6790\u548c\u4eba\u7c7b\u4ef7\u503c\u89c2\u63a8\u7406\uff1a\u4e00\u4e2a\u591a\u5c42NSFW\u68c0\u6d4b\u5668\u8fc7\u6ee4\u8bcd\u6c47\u548c\u8bed\u4e49\u98ce\u9669\uff1b\u4e00\u4e2a\u6587\u5316\u4ef7\u503c\u89c2\u5bf9\u9f50\u6a21\u5757\u8bc6\u522b\u793e\u4f1a\u89c4\u8303\u3001\u5408\u6cd5\u6027\u548c\u4ee3\u8868\u6027\u4f26\u7406\u7684\u8fdd\u89c4\u884c\u4e3a\uff1b\u4e00\u4e2a\u610f\u56fe\u6b67\u4e49\u6d88\u9664\u5668\u68c0\u6d4b\u7ec6\u5fae\u6216\u95f4\u63a5\u7684\u4e0d\u5b89\u5168\u542b\u4e49\u3002\u5f53\u68c0\u6d4b\u5230\u4e0d\u5b89\u5168\u5185\u5bb9\u65f6\uff0c\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6839\u636e\u52a8\u6001\u7684\u3001\u7279\u5b9a\u89d2\u8272\u7684\u6307\u4ee4\u91cd\u5199\u63d0\u793a\uff0c\u4ee5\u5728\u5f3a\u5236\u5bf9\u9f50\u7684\u540c\u65f6\u4fdd\u7559\u7528\u6237\u610f\u56fe\u3002\u5982\u679c\u751f\u6210\u7684\u56fe\u50cf\u4ecd\u672a\u901a\u8fc7\u5b89\u5168\u68c0\u67e5\uff0cVALOR\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6267\u884c\u98ce\u683c\u5316\u518d\u751f\uff0c\u4ee5\u5f15\u5bfc\u8f93\u51fa\u671d\u7740\u66f4\u5b89\u5168\u7684\u89c6\u89c9\u9886\u57df\u53d1\u5c55\uff0c\u800c\u4e0d\u6539\u53d8\u6838\u5fc3\u8bed\u4e49\u3002", "result": "\u5728\u5bf9\u6297\u6027\u3001\u6b67\u4e49\u6027\u548c\u4ef7\u503c\u654f\u611f\u6027\u63d0\u793a\u7684\u5b9e\u9a8c\u4e2d\uff0cVALOR\u80fd\u591f\u5c06\u4e0d\u5b89\u5168\u8f93\u51fa\u663e\u8457\u51cf\u5c11\u9ad8\u8fbe100.00%\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u7684\u6709\u7528\u6027\u548c\u521b\u9020\u6027\u3002", "conclusion": "VALOR\u4e3a\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u5bf9\u9f50\u4e14\u6709\u7528\u7684\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11696", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11696", "abs": "https://arxiv.org/abs/2511.11696", "authors": ["Xun Shao", "Aoba Otani", "Yuto Hirasuka", "Runji Cai", "Seng W. Loke"], "title": "Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL", "comment": "This is the author's preprint version of a paper accepted for presentation at EAI MONAMI 2025 (to appear in Springer LNICST). The final authenticated version will be available online at Springer Link upon publication", "summary": "This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u672a\u6765\u7684\u3001\u8d85\u8d8a\u4f20\u7edf\u7684\u8dcc\u5012\u68c0\u6d4b\u3001\u65e8\u5728\u5b9e\u73b0\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff08ADL\uff09\u8bc6\u522b\u7684\u8001\u5e74\u4eba\u76d1\u6d4b\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5c06\u5229\u7528\u9690\u79c1\u4fdd\u62a4\u3001\u8fb9\u7f18\u90e8\u7f72\u548c\u8054\u90a6\u5f0f\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u4ee5\u9c81\u68d2\u5730\u8bc6\u522b\u548c\u7406\u89e3\u8001\u5e74\u4eba\u7684\u65e5\u5e38\u6d3b\u52a8\uff0c\u4ece\u800c\u5728\u8001\u9f84\u5316\u793e\u4f1a\u4e2d\u7ef4\u62a4\u5176\u72ec\u7acb\u6027\u548c\u5c0a\u4e25\u3002", "motivation": "\u5f53\u524d\uff0c\u4e13\u95e8\u7528\u4e8eADL\u8bc6\u522b\u7684\u6570\u636e\u96c6\u4ecd\u5728\u6536\u96c6\u9636\u6bb5\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f7f\u7528SISFall\u6570\u636e\u96c6\u53ca\u5176GAN\u589e\u5f3a\u53d8\u4f53\u8fdb\u884c\u8dcc\u5012\u68c0\u6d4b\u7684\u5b9e\u9a8c\u6765\u521d\u6b65\u8bba\u8bc1\u5176\u53ef\u884c\u6027\uff0c\u5e76\u5c06\u8dcc\u5012\u68c0\u6d4b\u4f5c\u4e3a\u4e00\u9879\u4ee3\u7406\u4efb\u52a1\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u540c\u65f6\u8fd8\u5728Jetson Orin Nano\u8bbe\u5907\u4e0a\u8fdb\u884c\u4e86\u5d4c\u5165\u5f0f\u90e8\u7f72\u5b9e\u9a8c\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6761\u4ef6\u4e0b\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5728Jetson Orin Nano\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5d4c\u5165\u5f0f\u90e8\u7f72\u3002", "conclusion": "\u672c\u6587\u6307\u51fa\u4e86\u5728\u5b9e\u73b0\u5168\u9762\u7684ADL\u76d1\u6d4b\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u57df\u6f02\u79fb\u3001\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u667a\u80fd\u623f\u95f4\u73af\u5883\u4e2d\u5b9e\u73b0\u5168\u9762ADL\u76d1\u6d4b\u7684\u65b9\u5411\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4ece\u5355\u4e00\u4efb\u52a1\u68c0\u6d4b\u5411\u7efc\u5408\u6027\u65e5\u5e38\u6d3b\u52a8\u8bc6\u522b\u7684\u8f6c\u53d8\uff0c\u4e3a\u53ef\u6301\u7eed\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u8001\u5e74\u62a4\u7406\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65e9\u671f\u8bc1\u636e\u548c\u53d1\u5c55\u84dd\u56fe\u3002"}}
{"id": "2511.11704", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11704", "abs": "https://arxiv.org/abs/2511.11704", "authors": ["Matvey Skripkin", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Simple Vision-Language Math Reasoning via Rendered Text", "comment": null, "summary": "We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.", "AI": {"tldr": "\u901a\u8fc7\u5c06LaTeX\u7f16\u7801\u7684\u65b9\u7a0b\u6e32\u67d3\u6210\u56fe\u50cf\u5e76\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u727a\u7272\u901a\u7528\u9886\u57df\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u9886\u57df\u80fd\u529b\u3002", "method": "\u5c06LaTeX\u7f16\u7801\u7684\u65b9\u7a0b\u6e32\u67d3\u6210\u56fe\u50cf\uff0c\u5e76\u4e0e\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u914d\u5bf9\uff0c\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5728\u6570\u5b66\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u5728MMM, ChartQA\u548cDocVQA\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u8fbe20%\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e7f\u6cdb\u7684\u901a\u7528\u9886\u57df\u80fd\u529b\u3002", "conclusion": "\u6e32\u67d3\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u8bbe\u8ba1\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6570\u5b66\u95ee\u9898\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.11705", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11705", "abs": "https://arxiv.org/abs/2511.11705", "authors": ["Arya Narang"], "title": "Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs", "comment": null, "summary": "This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.", "AI": {"tldr": "\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u56fe\u50cf\u548c\u6587\u672c\uff08\u83dc\u80b4\u540d\u79f0\uff09\u8fdb\u884c\u5361\u8def\u91cc\u4f30\u7b97\uff0c\u5728Nutrition5k\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4ece84.76 kcal\u964d\u4f4e\u523083.70 kcal\uff0c\u6539\u8fdb\u7387\u4e3a1.25%\u3002", "motivation": "\u786e\u5b9a\u4e86\u77ed\u6587\u672c\u8f93\u5165\uff08\u5728\u6b64\u6848\u4f8b\u4e2d\u662f\u83dc\u80b4\u540d\u79f0\uff09\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u6539\u5584\u4e0e\u4ec5\u56fe\u50cf\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\u7684\u5361\u8def\u91cc\u4f30\u7b97\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6539\u8fdb\u662f\u5426\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "method": "\u5229\u7528TensorFlow\u5e93\u548cNutrition5k\u6570\u636e\u96c6\uff08\u7531Google\u7b56\u5212\uff09\u6765\u8bad\u7ec3\u4ec5\u56fe\u50cf\u7684CNN\u548c\u63a5\u53d7\u6587\u672c\u548c\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u7684\u3001\u591a\u6a21\u6001\u7684CNN\u3002", "result": "\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6a21\u578b\u5c06\u4f30\u7b97\u5361\u8def\u91cc\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4ece84.76 kcal\u964d\u4f4e\u4e861.06 kcal\uff0c\u964d\u81f383.70 kcal\uff08\u6539\u8fdb\u7387\u4e3a1.25%\uff09\u3002", "conclusion": "\u4e0e\u4ec5\u56fe\u50cf\u6a21\u578b\u76f8\u6bd4\uff0c\u7ed3\u5408\u83dc\u80b4\u540d\u79f0\u7684\u6587\u672c\u4fe1\u606f\u53ef\u4ee5\u8f7b\u5fae\u4f46\u6709\u610f\u4e49\u5730\u63d0\u9ad8\u5361\u8def\u91cc\u4f30\u7b97\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.11706", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11706", "abs": "https://arxiv.org/abs/2511.11706", "authors": ["Julia Peters", "Karin Mora", "Miguel D. Mahecha", "Chaonan Ji", "David Montero", "Clemens Mosig", "Guido Kraemer"], "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling", "comment": "10 pages (incliding 2 pages of references), 7 figures", "summary": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u540c\u4f20\u611f\u5668\u7684\u9065\u611f\u6570\u636e\uff0c\u751f\u6210\u53ef\u7528\u4e8e\u751f\u6001\u7cfb\u7edf\u52a8\u529b\u5b66\u5206\u6790\u7684\u901a\u7528\u5d4c\u5165\u5f0f\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u56fa\u5b9a\u7a7a\u95f4\u6216\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u9ad8\u65f6\u95f4\u4fdd\u771f\u5ea6\u7684\u751f\u6001\u5b66\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u4e0d\u540c\u5730\u7403\u89c2\u6d4b\u6a21\u6001\u5230\u7edf\u4e00\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u72ec\u7acb\u5efa\u6a21Sentinel-1\u548cSentinel-2\u6570\u636e\u4ee5\u6355\u6349\u5176\u7279\u5b9a\u4f20\u611f\u5668\u7279\u5f81\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8868\u793a\u7ec4\u5408\u5230\u4e00\u4e2a\u5171\u4eab\u6a21\u578b\u4e2d\u3002\u8fd9\u79cd\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5141\u8bb8\u5bf9\u7279\u5b9a\u6a21\u6001\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u80fd\u8f7b\u677e\u6269\u5c55\u5230\u65b0\u4f20\u611f\u5668\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4ec5\u91cd\u65b0\u8bad\u7ec3\u878d\u5408\u5c42\u3002", "result": "\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5f0f\u6570\u636e\u5728\u5f02\u6784\u666f\u89c2\u4e2d\u8868\u73b0\u51fa\u9ad8\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u5728\u4f30\u7b97\u521d\u7ea7\u751f\u4ea7\u529b\uff08Gross Primary Production\uff09\u7684\u5b9a\u91cf\u8bc4\u4f30\u4e2d\uff0c\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u6570\u636e\u80fd\u591f\u7f16\u7801\u5177\u6709\u751f\u6001\u5b66\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u5e76\u4fdd\u7559\u8db3\u591f\u7684\u65f6\u95f4\u4fdd\u771f\u5ea6\u4ee5\u652f\u6301\u7cbe\u7ec6\u5c3a\u5ea6\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u5206\u6790\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e0d\u540c\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u73af\u5883\u5e94\u7528\u3002"}}
{"id": "2511.11727", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11727", "abs": "https://arxiv.org/abs/2511.11727", "authors": ["Tongda Xu"], "title": "Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm", "comment": "NIPS 25 Workshop: Frontiers in Probabilistic Inference: Sampling Meets Learning", "summary": "Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.", "AI": {"tldr": "\u5229\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6269\u6563\u6a21\u578b\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u5176\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff0c\u5bfc\u81f4\u5206\u6570\u8303\u6570\u589e\u5927\uff0c\u5e76\u5f71\u54cd\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u8bb8\u591a\u8fd1\u671f\u5de5\u4f5c\u5229\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4f46\u8fd9\u79cd\u4f18\u5316\u65b9\u5f0f\u5b58\u5728\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u5229\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6269\u6563\u6a21\u578b\u6761\u4ef6\u8f93\u5165\u65f6\uff0c\u5176\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\u88ab\u7834\u574f\u7684\u73b0\u8c61\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u79cd\u504f\u5dee\u4f1a\u5bfc\u81f4\u5206\u6570\u8303\u6570\u589e\u5927\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f18\u5316\u6570\u636e\u5206\u5e03\u65f6\u51fa\u73b0\u7684\u7c7b\u4f3c\u504f\u5dee\u3002", "result": "\u8bc1\u660e\u4e86\u4f18\u5316\u6269\u6563\u6a21\u578b\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u5176\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff1b\u8bc1\u660e\u4e86\u8fd9\u79cd\u504f\u5dee\u4f1a\u5bfc\u81f4\u5206\u6570\u8303\u6570\u589e\u5927\uff1b\u89c2\u5bdf\u5230\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f18\u5316\u6570\u636e\u5206\u5e03\u65f6\u5b58\u5728\u7c7b\u4f3c\u504f\u5dee\u3002", "conclusion": "\u4f18\u5316\u6269\u6563\u6a21\u578b\u6761\u4ef6\u8f93\u5165\u7684\u884c\u4e3a\u4f1a\u5f15\u5165\u504f\u5dee\uff0c\u589e\u52a0\u5206\u6570\u8303\u6570\uff0c\u5e76\u5f71\u54cd\u5305\u62ecMAR\u3001PerCo\u548cDreamFusion\u5728\u5185\u7684\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2511.11753", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11753", "abs": "https://arxiv.org/abs/2511.11753", "authors": ["Mehdi Khaleghi", "Nastaran Khaleghi", "Sobhan Sheykhivand", "Sebelan Danishvar"], "title": "Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain", "comment": null, "summary": "Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08H-GSN\uff09\u6765\u4f18\u5316\u4f9b\u5e94\u94fe\u7269\u6d41\u7ba1\u7406\uff0c\u901a\u8fc7\u9884\u6d4b\u8d27\u8fd0\u7c7b\u578b\u3001\u5ef6\u8fdf\u548c\u4ea4\u901a\u72b6\u51b5\uff0c\u63d0\u9ad8\u4e86\u4f9b\u5e94\u94fe\u7684\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u63d0\u9ad8\u4f9b\u5e94\u94fe\u7684\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u7269\u6d41\u3001\u8fd0\u8f93\u548c\u4ed3\u50a8\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08H-GSN\uff09\u7528\u4e8e\u591a\u4efb\u52a1\u7269\u6d41\u7ba1\u7406\uff0c\u4ee5\u9884\u6d4b\u8d27\u8fd0\u7c7b\u578b\u3001\u8d27\u8fd0\u72b6\u6001\u3001\u4ea4\u901a\u72b6\u51b5\u3001\u7269\u6d41ID\u548c\u7269\u6d41\u5ef6\u8fdf\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684Kaggle\u6570\u636e\u5e93\uff08DataCo\u3001Shipping\u548cSmart Logistics\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002Smart Logistics\u6570\u636e\u96c6\u4e0a\u7684\u7269\u6d41ID\u548c\u4ea4\u901a\u72b6\u51b5\u9884\u6d4b\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u4e3a97.8%\u548c100%\u3002DataCo\u6570\u636e\u96c6\u4e0a\u7684\u8d27\u8fd0\u7c7b\u578b\u9884\u6d4b\u5e73\u5747\u51c6\u786e\u7387\u4e3a98.7%\uff0cShipping\u6570\u636e\u96c6\u4e0a\u7684\u7269\u6d41\u5ef6\u8fdf\u9884\u6d4b\u5e73\u5747\u51c6\u786e\u7387\u4e3a99.4%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684H-GSN\u65b9\u6cd5\u5728\u63d0\u9ad8\u4f9b\u5e94\u94fe\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5176\u5728\u4e0d\u540c\u7269\u6d41\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u6307\u6807\u8bc1\u5b9e\u4e86\u5176\u6548\u7387\u3002"}}
{"id": "2511.11781", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11781", "abs": "https://arxiv.org/abs/2511.11781", "authors": ["Vlad Rakhlin", "Amir Jevnisek", "Shai Avidan"], "title": "Coordinate Descent for Network Linearization", "comment": null, "summary": "ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.", "AI": {"tldr": "ReLU \u6fc0\u6d3b\u51fd\u6570\u662f\u57fa\u4e8e ResNet \u7f51\u7edc\u7684\u79c1\u6709\u63a8\u7406\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u56e0\u4e3a\u5b83\u4f1a\u5e26\u6765\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002ReLU \u6570\u91cf\u7684\u51cf\u5c11\u662f\u4e00\u4e2a\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u6709\u4e24\u79cd\u5e38\u89c1\u7684\u89e3\u51b3\u65b9\u6cd5\u3002\u5927\u591a\u6570\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u90fd\u57fa\u4e8e\u5e73\u6ed1\u8fd1\u4f3c\uff0c\u540c\u65f6\u4f18\u5316\u7f51\u7edc\u7cbe\u5ea6\u548c ReLU \u9884\u7b97\u3002\u7136\u800c\uff0c\u4f18\u5316\u8fc7\u7a0b\u7684\u6700\u540e\u4e00\u4e2a\u786c\u9608\u503c\u6b65\u9aa4\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f83\u5927\u7684\u6027\u80fd\u635f\u5931\u3002\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5750\u6807\u4e0b\u964d\u4f5c\u4e3a\u6211\u4eec\u7684\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u79bb\u6563\u57df\u4e2d\u8fdb\u884c\u64cd\u4f5c\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bbe\u8ba1\u4e0a\u4ea7\u751f\u4e86\u7a00\u758f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "ReLU \u6fc0\u6d3b\u51fd\u6570\u5728\u57fa\u4e8e ResNet \u7684\u79c1\u6709\u63a8\u7406\u4e2d\u9020\u6210\u4e86\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u662f\u4e3b\u8981\u7684\u6027\u80fd\u74f6\u9888\u3002\u56e0\u6b64\uff0c\u51cf\u5c11 ReLU \u7684\u6570\u91cf\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u5728\u79bb\u6563\u57df\u4e2d\u8fdb\u884c\u64cd\u4f5c\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5750\u6807\u4e0b\u964d\u4f5c\u4e3a\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3 ReLU \u6570\u91cf\u7684\u79bb\u6563\u4f18\u5316\u95ee\u9898\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bbe\u8ba1\u4e0a\u4ea7\u751f\u4e86\u7a00\u758f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5750\u6807\u4e0b\u964d\u7684\u79bb\u6563\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11 ReLU \u7684\u6570\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u79c1\u6709\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.11831", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11831", "abs": "https://arxiv.org/abs/2511.11831", "authors": ["Wenhao Zhou", "Hao Zheng", "Rong Zhao"], "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", "AI": {"tldr": "TopoPerception\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u5728\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5373\u4f7f\u662f\u6700\u5f3a\u5927\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u6216\u67b6\u6784\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u7684\u5c40\u90e8\u6377\u5f84\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u5f53\u524dLVLM\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51faTopoPerception\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5229\u7528\u62d3\u6251\u5c5e\u6027\u8bc4\u4f30LVLM\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "result": "\u5728TopoPerception\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u5728\u6700\u7c97\u7565\u7684\u611f\u77e5\u7c92\u5ea6\u4e0a\u8868\u73b0\u5747\u4e0d\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u8868\u660e\u5176\u7f3a\u4e4f\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u611f\u77e5\u80fd\u529b\u3002\u6a21\u578b\u80fd\u529b\u8d8a\u5f3a\uff0c\u51c6\u786e\u7387\u8d8a\u4f4e\u3002", "conclusion": "\u5f53\u524dLVLM\u5728\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4ec5\u4ec5\u6269\u5927\u6a21\u578b\u89c4\u6a21\u662f\u4e0d\u8db3\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\uff0c\u53cd\u800c\u53ef\u80fd\u52a0\u5267\u95ee\u9898\u3002\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u6216\u67b6\u6784\u6765\u6539\u8fdbLVLM\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002TopoPerception\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLVLM\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11880", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11880", "abs": "https://arxiv.org/abs/2511.11880", "authors": ["David Montero", "Miguel D. Mahecha", "Francesco Martinuzzi", "C\u00e9sar Aybar", "Anne Klosterhalfen", "Alexander Knohl", "Jes\u00fas Anaya", "Clemens Mosig", "Sebastian Wieneke"], "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production", "comment": null, "summary": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.", "AI": {"tldr": "EC\u5854\u5728\u4f30\u7b97\u68ee\u6797\u603b\u521d\u7ea7\u751f\u4ea7\u529b\uff08GPP\uff09\u65b9\u9762\u5b58\u5728\u7a7a\u95f4\u8986\u76d6\u6709\u9650\u7684\u6311\u6218\uff0c\u800c\u4f20\u7edf\u9065\u611f\u65b9\u6cd5\u96be\u4ee5\u6355\u6349GPP\u590d\u6742\u7684\u65f6\u95f4\u52a8\u6001\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08GPT-2\u548cLSTM\uff09\u5728\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u9884\u6d4bGPP\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u8f93\u5165\u7279\u5f81\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u5229\u7528\u9065\u611f\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21GPP\u4f30\u7b97\u65f6\uff0c\u5728\u6355\u6349GPP\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u636e\u878d\u5408\u6280\u672f\u4e3a\u6539\u8fdbGPP\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u9047\uff0c\u4f46\u5173\u4e8e\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u6a21\u6001GPP\u9884\u6d4b\u65b9\u9762\u7684\u6bd4\u8f83\u8bc4\u4f30\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u591a\u53d8\u91cf\u8f93\u5165\uff08\u5305\u62ec\u6765\u81eaEC\u5854\u7684\u6570\u636e\u4ee5\u53caSentinel-1\u3001Sentinel-2\u3001MODIS\u7b49\u4f20\u611f\u5668\u7684\u6570\u636e\uff09\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u4ee3\u8868\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aGPT-2\uff08Transformer\u67b6\u6784\uff09\u548cLSTM\uff08\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u9884\u6d4bGPP\u65b9\u9762\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u65f6\u95f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u4e86\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u3002", "result": "GPT-2\u548cLSTM\u5728GPP\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u76f8\u4f3c\u7684\u51c6\u786e\u5ea6\u3002LSTM\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u7565\u4f18\uff0c\u800cGPT-2\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002LSTM\u5728\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u6240\u9700\u7684\u8f93\u5165\u65f6\u95f4\u7a97\u53e3\u6bd4GPT-2\u77ed\uff0c\u8868\u660e\u4e24\u8005\u5728\u7cbe\u5ea6-\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u8868\u660e\uff0c\u8f90\u5c04\u662f\u4e3b\u8981\u7684\u9884\u6d4b\u56e0\u5b50\uff0c\u5176\u6b21\u662fSentinel-2\u3001MODIS\u9646\u8868\u6e29\u5ea6\u548cSentinel-1\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u591a\u6a21\u6001\u8f93\u5165\u5171\u540c\u51b3\u5b9a\u4e86GPP\u9884\u6d4b\u7684\u6027\u80fd\u3002GPT-2\u548cLSTM\u5728GPP\u9884\u6d4b\u65b9\u9762\u5404\u6709\u4f18\u52a3\uff0cLSTM\u5728\u6574\u4f53\u6548\u7387\u4e0a\u66f4\u4f18\uff0c\u800cGPT-2\u5728\u5904\u7406\u6781\u7aef\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5f00\u53d1\u7528\u4e8e\u76d1\u6d4b\u9646\u5730\u78b3\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.11899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11899", "abs": "https://arxiv.org/abs/2511.11899", "authors": ["Xi Li", "Nicholas Matsumoto", "Ujjwal Pasupulety", "Atharva Deo", "Cherine Yang", "Jay Moran", "Miguel E. Hernandez", "Peter Wager", "Jasmine Lin", "Jeanine Kim", "Alvin C. Goh", "Christian Wagner", "Geoffrey A. Sonn", "Andrew J. Hung"], "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction", "comment": null, "summary": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.", "AI": {"tldr": "F2O\u7cfb\u7edf\u5c06\u624b\u672f\u89c6\u9891\u8f6c\u5316\u4e3a\u624b\u52bf\u5e8f\u5217\uff0c\u8bc6\u522b\u4e0e\u672f\u540e\u7ed3\u679c\u76f8\u5173\u7684\u6a21\u5f0f\uff0c\u53ef\u7528\u4e8e\u624b\u672f\u53cd\u9988\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5206\u6790\u672f\u4e2d\u884c\u4e3a\u53ca\u5176\u5bf9\u60a3\u8005\u7ed3\u5c40\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faF2O\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5c06\u7ec4\u7ec7\u89e3\u5256\u89c6\u9891\u8f6c\u5316\u4e3a\u624b\u52bf\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u57fa\u4e8eTransformer\u7684\u65f6\u7a7a\u5efa\u6a21\u548c\u9010\u5e27\u5206\u7c7b\uff0c\u68c0\u6d4b\u673a\u5668\u4eba\u8f85\u52a9\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u4e2d\u795e\u7ecf\u4fdd\u7559\u6b65\u9aa4\u7684\u624b\u52bf\u3002", "result": "F2O\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u624b\u52bf\uff08\u5e27\u7ea7AUC\uff1a0.80\uff1b\u89c6\u9891\u7ea7AUC\uff1a0.81\uff09\uff0c\u5176\u63d0\u53d6\u7684\u7279\u5f81\uff08\u624b\u52bf\u9891\u7387\u3001\u6301\u7eed\u65f6\u95f4\u548c\u8f6c\u6362\uff09\u5728\u9884\u6d4b\u672f\u540e\u7ed3\u5c40\u65b9\u9762\u7684\u51c6\u786e\u6027\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u5f53\uff080.79 vs. 0.75\uff09\u300225\u4e2a\u5171\u4eab\u7279\u5f81\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6548\u5e94\u5927\u5c0f\u65b9\u5411\uff08\u5dee\u5f02\u7ea60.07\uff09\u548c\u5f3a\u76f8\u5173\u6027\uff08r=0.96, p<1e-14\uff09\u3002F2O\u8fd8\u8bc6\u522b\u51fa\u4e0e\u52c3\u8d77\u529f\u80fd\u6062\u590d\u76f8\u5173\u7684\u5173\u952e\u6a21\u5f0f\uff0c\u5982\u957f\u65f6\u95f4\u7ec4\u7ec7\u5265\u79bb\u548c\u51cf\u5c11\u80fd\u91cf\u4f7f\u7528\u3002", "conclusion": "F2O\u7cfb\u7edf\u901a\u8fc7\u5b9e\u73b0\u81ea\u52a8\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u524d\u77bb\u6027\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11934", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11934", "abs": "https://arxiv.org/abs/2511.11934", "authors": ["C. C\u00e9sar Claros Olivares", "Austin J. Brockmeier"], "title": "A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts", "comment": null, "summary": "We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540cOOD\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e0d\u540cCLIP\u5206\u5c42\u6a21\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528AURC\u548cAUGRC\u4f5c\u4e3a\u4e3b\u8981\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5728\u5206\u5e03\u5916(OOD)\u68c0\u6d4b\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u8868\u5f81\u5b66\u4e60\u8303\u5f0f\u4e0b\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u7279\u522b\u662f\u4f7f\u7528\u7edf\u4e00\u7684\u8bc4\u4f30\u6307\u6807\u548c\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u591a\u91cd\u6bd4\u8f83\u63a7\u5236\u7684\u3001\u57fa\u4e8e\u6392\u540d\u7684\u8bc4\u4f30\u6d41\u7a0b\uff08Friedman\u68c0\u9a8c\u4e0eConover-Holm\u4e8b\u540e\u68c0\u9a8c\uff09\u4ee5\u53caBron-Kerbosch\u7b97\u6cd5\u6765\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e0d\u540cOOD\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u8bc4\u4f30\u5bf9\u8c61\u5305\u62ec\u4ece\u5934\u8bad\u7ec3\u7684CNN\u548c\u5fae\u8c03\u7684Vision Transformer (ViT)\uff0c\u5728CIFAR-10/100\u3001SuperCIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7a7a\u95f4\u5bf9OOD\u68c0\u6d4b\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002\u5bf9\u4e8eCNN\u548cViT\uff0c\u6982\u7387\u5206\u6570\uff08\u5982MSR, GEN\uff09\u5728\u68c0\u6d4b\u8bef\u5206\u7c7b\uff08ID\uff09\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5728\u66f4\u5f3a\u7684\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u51e0\u4f55\u611f\u77e5\u5206\u6570\uff08\u5982NNGuide, fDBD, CTM\uff09\u5728CNN\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u800c\u5728ViT\u4e0a\uff0cGradNorm\u548cKPCA\u91cd\u6784\u8bef\u5dee\u5219\u6301\u7eed\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0Monte-Carlo Dropout\uff08MCD\uff09\u5b58\u5728\u7c7b\u522b\u6570\u91cf\u4f9d\u8d56\u7684\u6743\u8861\uff0c\u5e76\u4e14\u7b80\u5355\u7684PCA\u6295\u5f71\u80fd\u63d0\u5347\u591a\u79cd\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "OOD\u68c0\u6d4b\u7684\u6548\u679c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u8ba1\u5b66\u4f9d\u636e\uff0c\u5e76\u5f3a\u8c03\u4e86\u4ee5\u8868\u5f81\u4e3a\u4e2d\u5fc3\u7684OOD\u68c0\u6d4b\u89c2\u70b9\u3002"}}
{"id": "2511.12002", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12002", "abs": "https://arxiv.org/abs/2511.12002", "authors": ["Tenghao Ji", "Eytan Adar"], "title": "Selecting Fine-Tuning Examples by Quizzing VLMs", "comment": null, "summary": "A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \\textit{do} exemplify the target concept (e.g., a \\textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.", "AI": {"tldr": "QZLoRA\u4f7f\u7528QuizRank\u65b9\u6cd5\u6765\u81ea\u52a8\u9009\u62e9\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5fae\u8c03\u7684\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u4ee3\u8868\u6027\u3002", "motivation": "\u4ece\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u56fe\u50cf\u96c6\uff08\u5982\u7ef4\u57fa\u5171\u4eab\u8d44\u6e90\uff09\u8fdb\u884c\u5fae\u8c03\u901a\u5e38\u4f1a\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u6548\u679c\u4e0d\u4f73\u3002\u7136\u800c\uff0c\u4f7f\u7528\u80fd\u591f\u5145\u5206\u4f53\u73b0\u76ee\u6807\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u96cc\u6027\u5c71\u84dd\u9e1f\uff09\u7684\u8bad\u7ec3\u56fe\u50cf\uff0c\u53ef\u4ee5\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u5177\u6709\u4ee3\u8868\u6027\uff08\u4f8b\u5982\uff0c\u5177\u6709\u5178\u578b\u7684\u84dd\u8272\u7fc5\u8180\u548c\u7070\u8272\u80f8\u90e8\uff09\u3002", "method": "\u63d0\u51faQZLoRA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528QuizRank\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u89c6\u4e3a\u201c\u6559\u80b2\u5e72\u9884\u201d\u5e76\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u201c\u6d4b\u9a8c\u201d\u6765\u81ea\u52a8\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u540d\uff0c\u4ece\u800c\u9009\u62e9\u7528\u4e8e\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u7684\u56fe\u50cf\u3002", "result": "QZLoRA\u80fd\u591f\u7528\u66f4\u5c11\u7684\u6837\u672c\u751f\u6210\u4e0e\u4e3b\u9898\u66f4\u5951\u5408\u3001\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u540c\u6837\u5177\u6709\u4ee3\u8868\u6027\u7684\u98ce\u683c\u5316\u56fe\u50cf\uff08\u4f8b\u5982\uff0c\u63d2\u56fe\uff09\u3002", "conclusion": "\u5c06\u81ea\u52a8\u89c6\u89c9\u63a8\u7406\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u5728\u4e3b\u9898\u81ea\u9002\u5e94\u751f\u6210\u6a21\u578b\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12008", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12008", "abs": "https://arxiv.org/abs/2511.12008", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "comment": null, "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "AI": {"tldr": "RECAP-PATH\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8ba9AI\u5728\u4e0d\u8fdb\u884c\u767d\u76d2\u8bbf\u95ee\u6216\u6743\u91cd\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u5c31\u80fd\u5bf9\u764c\u75c7\u8fdb\u884c\u8bca\u65ad\uff0c\u5e76\u63d0\u4f9b\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u4ece\u800c\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "AI\u5728\u75c5\u7406\u5b66\u9886\u57df\u7684\u5e94\u7528\u867d\u7136\u63d0\u9ad8\u4e86\u7b5b\u67e5\u901a\u91cf\u3001\u6807\u51c6\u5316\u4e86\u91cf\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u9884\u540e\u6a21\u5f0f\uff0c\u4f46\u7531\u4e8e\u5927\u591a\u6570\u7cfb\u7edf\u7f3a\u4e4f\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u80fd\u529b\u6765\u5ba1\u8ba1\u51b3\u7b56\u548c\u9632\u6b62\u9519\u8bef\uff0c\u56e0\u6b64\u63a8\u5e7f\u5e94\u7528\u4ecd\u7136\u53d7\u9650\u3002", "method": "RECAP-PATH\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u591a\u6837\u5316\u6269\u5c55\u75c5\u7406\u5b66\u98ce\u683c\u7684\u89e3\u91ca\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u6765\u63d0\u9ad8\u89e3\u91ca\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u81ea\u4e3b\u63a8\u5bfc\u51fa\u8bca\u65ad\u6807\u51c6\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u8981\u5c11\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u65e0\u9700\u767d\u76d2\u8bbf\u95ee\u6216\u6743\u91cd\u66f4\u65b0\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u524d\u5217\u817a\u764c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cRECAP-PATH\u751f\u6210\u7684\u89e3\u91ca\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u5e76\u5728\u8bca\u65ad\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RECAP-PATH\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e34\u5e8a\u4e0a\u503c\u5f97\u4fe1\u8d56\u7684AI\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u6761\u901a\u5f80\u57fa\u4e8e\u8bc1\u636e\u7684\u89e3\u91ca\u7684\u901a\u7528\u8def\u5f84\u3002"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "VBackChecker\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u65e0\u53c2\u8003\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u63a5\u5730\u8bed\u8a00\u6a21\u578b\u6765\u9a8c\u8bc1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5R\u00b2-HalBench\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\uff0c\u51c6\u786e\u68c0\u6d4b\u5e7b\u89c9\u5bf9\u4e8e\u5176\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVBackChecker\u7684\u65e0\u53c2\u8003\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u50cf\u7d20\u7ea7\u63a5\u5730\u8bed\u8a00\u6a21\u578b\uff08\u5177\u5907\u63a8\u7406\u548c\u6307\u4ee3\u5206\u5272\u80fd\u529b\uff09\u6765\u9a8c\u8bc1MLLM\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002\u8bbe\u8ba1\u4e86R-Instruct\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5305\u542b\u5bcc\u6587\u672c\u63cf\u8ff0\u3001\u63a5\u5730\u63a9\u7801\u548c\u8d1f\u6837\u672c\u3002\u6784\u5efa\u4e86R\u00b2-HalBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "VBackChecker\u8d85\u8d8a\u4e86\u5148\u524d\u590d\u6742\u7684\u6846\u67b6\uff0c\u5728R\u00b2-HalBench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u5e7b\u89c9\u68c0\u6d4b\u65b9\u9762\u4e0eGPT-4o\u76f8\u5f53\u3002\u5728\u50cf\u7d20\u7ea7\u63a5\u5730\u4efb\u52a1\u4e0a\u4e5f\u53d6\u5f97\u4e86\u8d85\u8fc710%\u7684\u63d0\u5347\u3002", "conclusion": "VBackChecker\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65e0\u53c2\u8003\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u50cf\u7d20\u7ea7\u63a5\u5730\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u51b3MLLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12143", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12143", "abs": "https://arxiv.org/abs/2511.12143", "authors": ["Jialiang Wang", "Xiong Zhou", "Xianming Liu", "Gangfeng Hu", "Deming Zhai", "Junjun Jiang", "Haoliang Li"], "title": "Variation-Bounded Loss for Noise-Tolerant Learning", "comment": "Accepted by AAAI2026", "summary": "Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u53d8\u5dee\u6709\u754c\u635f\u5931\uff08VBL\uff09\u7684\u65b0\u578b\u9c81\u68d2\u635f\u5931\u51fd\u6570\u65cf\uff0c\u901a\u8fc7\u754c\u5b9a\u53d8\u5dee\u6bd4\u6765\u63d0\u9ad8\u635f\u5931\u51fd\u6570\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u76d1\u7763\u5b66\u4e60\u4e2d\u6709\u566a\u58f0\u6807\u7b7e\u7684\u8d1f\u9762\u5f71\u54cd\u95ee\u9898\uff0c\u63d0\u51fa\u9c81\u68d2\u635f\u5931\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u53d8\u5dee\u6bd4\uff08Variation Ratio\uff09\u4f5c\u4e3a\u8861\u91cf\u635f\u5931\u51fd\u6570\u9c81\u68d2\u6027\u7684\u65b0\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u53d8\u5dee\u6709\u754c\u635f\u5931\uff08VBL\uff09\u3002\u5bf9\u53d8\u5dee\u6bd4\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u66f4\u5c0f\u7684\u53d8\u5dee\u6bd4\u80fd\u5e26\u6765\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5c55\u793a\u53d8\u5dee\u6bd4\u5982\u4f55\u653e\u5bbd\u5bf9\u79f0\u6761\u4ef6\u548c\u5b9e\u73b0\u975e\u5bf9\u79f0\u6761\u4ef6\u3002\u5c06\u5e38\u7528\u635f\u5931\u51fd\u6570\u6539\u5199\u4e3a\u53d8\u5dee\u6709\u754c\u5f62\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u53d8\u5dee\u6709\u754c\u635f\u5931\uff08VBL\uff09\u53ca\u5176\u53d8\u5dee\u6bd4\u6307\u6807\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u635f\u5931\u51fd\u6570\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12241", "abs": "https://arxiv.org/abs/2511.12241", "authors": ["Junhyuk Seo", "Hyeyoon Moon", "Kyu-Hwan Jung", "Namkee Oh", "Taerim Kim"], "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos", "comment": "12 pages, 5 figures", "summary": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u89c6\u9891\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a AURA \u7684\u89c6\u89c9\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4bICU\u4e2d\u7684\u610f\u5916\u62d4\u7ba1\u4e8b\u4ef6\u3002", "motivation": "\u610f\u5916\u62d4\u7ba1\uff08UE\uff09\u662fICU\u60a3\u8005\u5b89\u5168\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5b9e\u65f6\u68c0\u6d4b\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a AURA \u7684\u89c6\u89c9\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u59ff\u6001\u4f30\u8ba1\u6765\u8bc6\u522b\u4e24\u79cd\u9ad8\u98ce\u9669\u8fd0\u52a8\u6a21\u5f0f\uff1a\u78b0\u649e\uff08\u624b\u8fdb\u5165\u6c14\u7ba1\u9644\u8fd1\u7684\u7a7a\u95f4\u533a\u57df\uff09\u548c\u6fc0\u52a8\uff08\u901a\u8fc7\u8ddf\u8e2a\u7684\u89e3\u5256\u5173\u952e\u70b9\u7684\u901f\u5ea6\u91cf\u5316\uff09\u3002\u8be5\u7cfb\u7edf\u5b8c\u5168\u5728\u5b8c\u5168\u5408\u6210\u7684\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5f00\u53d1\u548c\u9a8c\u8bc1\uff0c\u8be5\u6570\u636e\u96c6\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\uff0c\u6027\u80fd\u8bc4\u4f30\u663e\u793a\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u6fc0\u52a8\u8bc6\u522b\u6027\u80fd\u4e2d\u7b49\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u5f00\u53d1\u53ef\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u91cd\u590d\u7684\u60a3\u8005\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\u7684\u65b0\u9014\u5f84\uff0c\u6709\u53ef\u80fd\u5728\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2511.12265", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12265", "abs": "https://arxiv.org/abs/2511.12265", "authors": ["Rui Wang", "Zeming Wei", "Xiyue Zhang", "Meng Sun"], "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks", "comment": null, "summary": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.", "AI": {"tldr": "\u5bf9\u6297\u6027\u8bad\u7ec3\uff08AT\uff09\u6846\u67b6\u65e0\u6cd5\u5904\u7406\u6240\u6709\u5b9e\u9645\u653b\u51fb\u7c7b\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CAS\u6765\u63d0\u9ad8DNN\u7684\u6574\u4f53\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709AT\u6846\u67b6\u901a\u5e38\u53ea\u5173\u6ce8\u6709\u9650\u7684\u653b\u51fb\u7c7b\u578b\uff0c\u5bfc\u81f4DNN\u5bb9\u6613\u53d7\u5230\u672a\u8003\u8651\u5230\u7684\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAS\u7684\u6709\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u7684\u4f18\u5316\u89d2\u5ea6\u51fa\u53d1\uff0c\u8003\u8651\u4e86\u591a\u91cd\u9c81\u68d2\u6027\u7ef4\u5ea6\u7684\u52a8\u6001\u548c\u76f8\u4e92\u4f9d\u8d56\u7279\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u8bbe\u8ba1\u5956\u52b1\u548c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u6765\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "CAS\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e05\u6d01\u51c6\u786e\u7387\u3002", "conclusion": "CAS\u4e3aDNN\u7684\u9c81\u68d2\u6cdb\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.12502", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12502", "abs": "https://arxiv.org/abs/2511.12502", "authors": ["Yu Liang", "Yu Yang", "Wenjie Wei", "Ammar Belatreche", "Shuai Wang", "Malu Zhang", "Yang Yang"], "title": "BSO: Binary Spiking Online Optimization Algorithm", "comment": null, "summary": "Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.", "AI": {"tldr": "BSO\u662f\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u8bad\u7ec3\u7b97\u6cd5\uff0c\u53ef\u663e\u8457\u51cf\u5c11BSNN\u7684\u8bad\u7ec3\u5185\u5b58\uff0c\u800cT-BSO\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u52a8\u6001\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "BSNNs\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5176\u8bad\u7ec3\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\u6765\u5b58\u50a8\u6f5c\u5728\u6743\u91cd\u548c\u5904\u7406\u65f6\u95f4\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBSO\u7684\u65b0\u578b\u5728\u7ebf\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5b83\u5728\u5728\u7ebf\u8bad\u7ec3\u6846\u67b6\u4e0b\u76f4\u63a5\u901a\u8fc7\u7ffb\u8f6c\u4fe1\u53f7\u66f4\u65b0\u6743\u91cd\uff0c\u5f53\u68af\u5ea6\u52a8\u91cf\u548c\u6743\u91cd\u7684\u4e58\u79ef\u8d85\u8fc7\u9608\u503c\u65f6\u89e6\u53d1\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u4ece\u800c\u65e0\u9700\u5728\u8bad\u7ec3\u671f\u95f4\u5b58\u50a8\u6f5c\u5728\u6743\u91cd\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-BSO\u7684\u65f6\u95f4\u611f\u77e5\u53d8\u4f53\uff0c\u5b83\u901a\u8fc7\u8de8\u65f6\u95f4\u6b65\u6355\u83b7\u68af\u5ea6\u4fe1\u606f\u6765\u8c03\u6574\u9608\u503c\uff0c\u4ee5\u5229\u7528BSNN\u7684\u5185\u5728\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u4e86BSO\u548cT-BSO\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u63cf\u8ff0\u5176\u6536\u655b\u901f\u5ea6\u7684\u6b63\u5f0f\u9057\u61be\u754c\u9650\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u73b0\u6709\u7684BSNN\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0cBSO\u548cT-BSO\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "BSO\u548cT-BSO\u7b97\u6cd5\u5728\u51cf\u5c11BSNN\u8bad\u7ec3\u5185\u5b58\u548c\u63d0\u9ad8\u4f18\u5316\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3BSNN\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12564", "categories": ["cs.LG", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12564", "abs": "https://arxiv.org/abs/2511.12564", "authors": ["David Denisov", "Shlomi Dolev", "Dan Felmdan", "Michael Segal"], "title": "Linear time small coresets for k-mean clustering of segments with applications", "comment": "First published in WALCOM 2026 by Springer Nature", "summary": "We study the $k$-means problem for a set $\\mathcal{S} \\subseteq \\mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \\subseteq \\mathbb{R}^d$ that minimize\n  $D(\\mathcal{S},X) := \\sum_{S \\in \\mathcal{S}} \\min_{x \\in X} D(S,x)$, where $D(S,x) := \\int_{p \\in S} |p - x| dp$\n  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\\varepsilon > 0$, an $\\varepsilon$-coreset is a weighted subset $C \\subseteq \\mathbb{R}^d$ that approximates $D(\\mathcal{S},X)$ within a factor of $1 \\pm \\varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\\varepsilon$, it produces a coreset of size $O(\\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8ek-means\u805a\u7c7b\u95ee\u9898\u7684coreset\u6784\u9020\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u7684\u8f93\u5165\u7ebf\u6bb5\uff0c\u5e76\u5728\u5e38\u6570k\u548c\u03b5\u4e0b\uff0c\u6784\u9020\u51fa\u5927\u5c0f\u4e3aO(log^2 n)\u4e14\u53ef\u5728O(nd)\u65f6\u95f4\u5185\u8ba1\u7b97\u7684coreset\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8ddf\u8e2a\u7b49\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "k-means\u805a\u7c7b\u95ee\u9898\u901a\u5e38\u662f\u9488\u5bf9\u70b9\u96c6\u8fdb\u884c\u7684\uff0c\u672c\u7814\u7a76\u5c06\u95ee\u9898\u6269\u5c55\u5230\u7ebf\u6bb5\u96c6\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u7684coreset\u6784\u9020\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdcoreset\u6784\u9020\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u7684\u8f93\u5165\u7ebf\u6bb5\uff0c\u5e76\u5728\u5e38\u6570k\u548c\u03b5\u4e0b\uff0c\u6784\u9020\u51fa\u5927\u5c0f\u4e3aO(log^2 n)\u4e14\u53ef\u5728O(nd)\u65f6\u95f4\u5185\u8ba1\u7b97\u7684coreset\u3002", "result": "\u6240\u63d0\u51fa\u7684coreset\u6784\u9020\u65b9\u6cd5\u80fd\u591f\u5728O(nd)\u65f6\u95f4\u5185\u751f\u6210\u5927\u5c0f\u4e3aO(log^2 n)\u7684coreset\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982\u89c6\u9891\u8ddf\u8e2a\uff09\u4e2d\u80fd\u591f\u5b9e\u73b0\u663e\u8457\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5c0f\u7684\u805a\u7c7b\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684coreset\u6784\u9020\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u8df5\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u89e3\u51b3\u7ebf\u6bb5\u96c6\u5408\u7684k-means\u805a\u7c7b\u95ee\u9898\u3002"}}
{"id": "2511.12609", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12609", "abs": "https://arxiv.org/abs/2511.12609", "authors": ["Yunxin Li", "Xinyu Chen", "Shenyuan Jiang", "Haoyuan Shi", "Zhenyu Liu", "Xuanyu Zhang", "Nanhao Deng", "Zhenran Xu", "Yicheng Ma", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data", "comment": "47 pages,10 Figures, Project Website: https://idealistxy.github.io/Uni-MoE-v2.github.io/; Codes: https://github.com/HITsz-TMG/Uni-MoE", "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.", "AI": {"tldr": "Uni-MoE 2.0 \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u5927\u6a21\u578b\uff0c\u5728\u8bed\u8a00\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5728 85 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u89c6\u9891\u7406\u89e3\u3001\u5168\u6a21\u6001\u7406\u89e3\u548c\u89c6\u542c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u663e\u8457\u63d0\u5347\u5168\u6a21\u6001\u5927\u6a21\u578b\u5728\u8bed\u8a00\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u52a8\u6001\u5bb9\u91cf\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u3001\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff08\u7ed3\u5408\u8fed\u4ee3\u5f3a\u5316\u7b56\u7565\uff09\u4ee5\u53ca\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6a21\u6001\u6570\u636e\u5339\u914d\u6280\u672f\uff0c\u4ece Qwen2.5-7B \u57fa\u7840\u67b6\u6784\u4e0a\u6784\u5efa Uni-MoE 2.0\u3002", "result": "Uni-MoE 2.0 \u5728 85 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728 76 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 50 \u591a\u4e2a\u8d85\u8fc7\u4e86 Qwen2.5-Omni\uff0c\u5728\u89c6\u9891\u7406\u89e3\u3001\u5168\u6a21\u6001\u7406\u89e3\u548c\u89c6\u542c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u8bed\u97f3\u5904\u7406\u548c\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Uni-MoE 2.0 \u901a\u8fc7\u5176\u521b\u65b0\u7684 MoE \u67b6\u6784\u3001\u5148\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5168\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u7684\u98de\u8dc3\uff0c\u5e76\u5728\u591a\u9879\u5173\u952e\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2511.12861", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12861", "abs": "https://arxiv.org/abs/2511.12861", "authors": ["Wenxin Zhu", "Andong Chen", "Yuchen Song", "Kehai Chen", "Conghui Zhu", "Ziyan Chen", "Tiejun Zhao"], "title": "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models", "comment": "Survey; 7 figures, 3 tables, 44 pages", "summary": "With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on \"Multimodal Chain-of-Thought\" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MCoT\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709MLLMs\u63a8\u7406\u8def\u5f84\u4e0d\u900f\u660e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u501f\u9274\u8bed\u8a00\u6a21\u578b\u4e2dCoT\u7684\u6210\u529f\u7ecf\u9a8c\uff0c\u5c06\u5176\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\u4ee5\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4eceCoT\u8303\u5f0f\u3001\u8bad\u7ec3\u540e\u9636\u6bb5\u548c\u63a8\u7406\u9636\u6bb5\u4e09\u4e2a\u65b9\u9762\u4ecb\u7ecd\u4e3b\u6d41MCoT\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u673a\u5236\u3002\u540c\u65f6\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u57fa\u51c6\u3001\u6307\u6807\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "MCoT\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "MCoT\u662f\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u65b9\u5411\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u514b\u670d\u73b0\u6709\u6311\u6218\u5e76\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.12898", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12898", "abs": "https://arxiv.org/abs/2511.12898", "authors": ["Zhiqi Li", "Yuchen Sun", "Greg Turk", "Bo Zhu"], "title": "Functional Mean Flow in Hilbert Space", "comment": "29 pages, 13 figures", "summary": "We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.", "AI": {"tldr": "Functional Mean Flow (FMF)\u662f\u4e00\u4e2a\u65b0\u7684\u5355\u6b65\u751f\u6210\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u51fd\u6570\u57df\uff0c\u80fd\u591f\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u50cf\u3001\u504f\u5fae\u5206\u65b9\u7a0b\u548c3D\u51e0\u4f55\u7b49\u591a\u79cd\u6570\u636e\u3002\u5b83\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8df5\u7684\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u6539\u8fdb\u7a33\u5b9a\u6027\u7684x1-\u9884\u6d4b\u53d8\u4f53\u3002", "motivation": "\u5c06\u5355\u6b65\u751f\u6210\u6a21\u578b\uff08Mean Flow\uff09\u6269\u5c55\u5230\u51fd\u6570\u57df\uff0c\u4ee5\u89e3\u51b3\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51faFunctional Flow Matching\u7406\u8bba\u548cFMF\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0x1-\u9884\u6d4b\u53d8\u4f53\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "FMF\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u50cf\u3001\u504f\u5fae\u5206\u65b9\u7a0b\u548c3D\u51e0\u4f55\u7b49\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "FMF\u4e3a\u51fd\u6570\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u5355\u6b65\u6d41\u5339\u914d\u65b9\u6cd5\u3002"}}
{"id": "2511.12937", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12937", "abs": "https://arxiv.org/abs/2511.12937", "authors": ["Guoyan Wang", "Yanyan Huang", "Chunlin Chen", "Lifeng Wang", "Yuxiang Sun"], "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models", "comment": "32 pages, 13 figures", "summary": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.", "AI": {"tldr": "Yanyun-3\u662f\u4e00\u4e2a\u901a\u7528\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b83\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u7b56\u7565\u6e38\u620f\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u6574\u5408\u4e86Qwen2.5-VL\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u76ee\u6807\u5b9a\u4f4d\u3001\u6218\u6597\u8d44\u6e90\u5206\u914d\u548c\u533a\u57df\u63a7\u5236\u7b49\u6838\u5fc3\u4efb\u52a1\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u73b0\u878d\u5408\u591a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u5e76\u6df7\u5408\u9759\u6001\u56fe\u50cf\u7684\u6df7\u5408\u7b56\u7565\uff08MV+S\uff09\u663e\u8457\u4f18\u4e8e\u5b8c\u5168\u878d\u5408\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8663%\uff0cBLEU-4\u5206\u6570\u63d0\u9ad8\u4e8612\u500d\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u95ed\u73af\u6d41\u7a0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u589e\u5f3aVLM\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002", "motivation": "\u81ea\u52a8\u5316\u8de8\u5e73\u53f0\u7b56\u7565\u6e38\u620f\u9700\u8981\u667a\u80fd\u4f53\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u7684\u7528\u6237\u754c\u9762\u548c\u52a8\u6001\u6218\u573a\u6761\u4ef6\u3002\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u7b56\u7565\u6e38\u620f\u7b49\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aYanyun-3\u7684\u901a\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u96c6\u6210\u4e86Qwen2.5-VL\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e09\u4e2a\u5f02\u6784\u7b56\u7565\u6e38\u620f\u73af\u5883\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0d\u540c\u591a\u6a21\u6001\u6570\u636e\u7ec4\u5408\uff08\u9759\u6001\u56fe\u50cf\u3001\u591a\u56fe\u50cf\u5e8f\u5217\u3001\u89c6\u9891\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u7ec4\u5408\u7c92\u5ea6\u6982\u5ff5\u6765\u533a\u5206\u6837\u672c\u5185\u878d\u5408\u548c\u6837\u672c\u95f4\u6df7\u5408\u7b56\u7565\u3002", "result": "\u6df7\u5408\u7b56\u7565\uff08MV+S\uff09\u663e\u8457\u4f18\u4e8e\u5b8c\u5168\u878d\u5408\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8663%\uff0cBLEU-4\u5206\u6570\u4ece4.81%\u63d0\u9ad8\u523062.41%\uff08\u7ea612.98\u500d\uff09\u3002\u8be5\u667a\u80fd\u4f53\u901a\u8fc7\u95ed\u73af\u6d41\u7a0b\uff08\u5c4f\u5e55\u6355\u83b7\u3001\u6a21\u578b\u63a8\u7406\u3001\u52a8\u4f5c\u6267\u884c\uff09\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Yanyun-3\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u7ec4\u7ec7\uff0c\u6709\u6548\u63d0\u5347\u4e86VLM\u5728\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u9759\u6001\u611f\u77e5\u4e0e\u52a8\u6001\u63a8\u7406\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002\u8be5\u5de5\u4f5c\u4e3a\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u589e\u5f3aVLM\u6027\u80fd\u7684\u901a\u7528\u8303\u5f0f\u3002"}}
{"id": "2511.12985", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12985", "abs": "https://arxiv.org/abs/2511.12985", "authors": ["Minsoo Jo", "Dongyoon Yang", "Taesup Kim"], "title": "Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks", "comment": "Accepted by AAAI 2026", "summary": "Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \\textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6b3a\u9a97\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53cc\u66f2\u7f51\u7edc\u4e2d\u6ca1\u6709\u8003\u8651\u5230\u53cc\u66f2\u7ed3\u6784\uff0c\u53ef\u80fd\u5bfc\u81f4\u653b\u51fb\u6548\u7387\u4f4e\u4e0b\u6216\u51e0\u4f55\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u975e\u6b27\u51e0\u91cc\u51e0\u4f55\u4e2d\u7684\u653b\u51fb\u7b56\u7565\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u53cc\u66f2\u7a7a\u95f4\u7684\u5207\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u5f84\u5411\uff08\u6df1\u5ea6\uff09\u548c\u89d2\u5411\uff08\u8bed\u4e49\uff09\u5206\u91cf\uff0c\u4ec5\u4ece\u89d2\u5411\u5206\u91cf\u63a8\u5bfc\u51fa\u6270\u52a8\uff0c\u4ece\u800c\u5728\u8bed\u4e49\u654f\u611f\u7684\u65b9\u5411\u4e0a\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "result": "\u4e0e\u4f20\u7edf\u5bf9\u6297\u6027\u653b\u51fb\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u548c\u7f51\u7edc\u67b6\u6784\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6b3a\u9a97\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f3a\u8c03\u4e86\u51e0\u4f55\u611f\u77e5\u5bf9\u6297\u7b56\u7565\u5728\u5f2f\u66f2\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u653b\u51fb\u5c42\u6b21\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2511.13082", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13082", "abs": "https://arxiv.org/abs/2511.13082", "authors": ["Kyunghyun Lee", "Yong-Min Shin", "Minwoo Shin", "Jihun Kim", "Sunghwan Lim", "Won-Yong Shin", "Kyungho Yoon"], "title": "Real-time prediction of breast cancer sites using deformation-aware graph neural network", "comment": null, "summary": "Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u5f15\u5bfc\u7684\u6d3b\u68c0\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u9884\u6d4b\u4e73\u817a\u764c\u75c5\u7076\u7684\u53d8\u5f62\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e9\u671f\u8bca\u65ad\u4e73\u817a\u764c\u5bf9\u4e8e\u5236\u5b9a\u6cbb\u7597\u8ba1\u5212\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1MRI\u5f15\u5bfc\u7684\u6d3b\u68c0\u6548\u679c\u826f\u597d\uff0c\u4f46\u5176\u8fc7\u7a0b\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u3002\u95f4\u63a5MRI\u5f15\u5bfc\u7684\u6d3b\u68c0\u867d\u7136\u53ef\u4ee5\u5728MRI\u5ba4\u5916\u8fdb\u884c\uff0c\u4f46\u5728\u521b\u5efa\u51c6\u786e\u7684\u5b9e\u65f6\u53ef\u53d8\u5f62\u4e73\u817a\u6a21\u578b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u5730\u5b9e\u65f6\u9884\u6d4b\u6d3b\u68c0\u8fc7\u7a0b\u4e2d\u53d8\u5f62\u7684\u4e73\u817a\u764c\u75c5\u7076\u4f4d\u7f6e\u3002\u7814\u7a76\u4eba\u5458\u9996\u5148\u6784\u5efa\u4e86\u4e2a\u4f53\u5316\u7684\u6709\u9650\u5143\uff08FE\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4e73\u817a\u548c\u80bf\u7624\u7684MR\u56fe\u50cf\u7ed3\u6784\u4fe1\u606f\u6765\u6a21\u62df\u53d8\u5f62\u884c\u4e3a\u3002\u7136\u540e\uff0c\u5229\u7528GNN\u6a21\u578b\u5904\u7406\u8868\u9762\u4f4d\u79fb\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u56fe\u6570\u636e\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u5305\u62ec\u80bf\u7624\u533a\u57df\u53d8\u5f62\u5728\u5185\u7684\u6574\u4f53\u7ec4\u7ec7\u4f4d\u79fb\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5e7b\u5f71\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u80bf\u7624\u8282\u70b9\u4f4d\u79fb\u65b9\u9762\u8fbe\u5230\u4e860.2\u6beb\u7c73\uff08mm\uff09\u7684\u51c6\u786e\u7387\uff08RMSE\uff09\uff0c\u5728\u4e0e\u5b9e\u9645\u764c\u53d8\u533a\u57df\u7684\u7a7a\u95f4\u91cd\u53e0\u65b9\u9762\u53d6\u5f97\u4e860.977\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\uff0c\u4e0e\u4f20\u7edf\u7684FE\u6a21\u62df\u76f8\u6bd4\uff0c\u8ba1\u7b97\u6210\u672c\u63d0\u9ad8\u4e864000\u591a\u500d\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684\u53d8\u5f62\u611f\u77e5GNN\u6a21\u578b\u4e3a\u4e73\u817a\u764c\u6d3b\u68c0\u4e2d\u7684\u5b9e\u65f6\u80bf\u7624\u4f4d\u79fb\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u80fd\u529b\u3002\u5c06\u5176\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u4e73\u817a\u764c\u8bca\u65ad\u7684\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2511.13087", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13087", "abs": "https://arxiv.org/abs/2511.13087", "authors": ["SeokJoo Kwak", "Jihoon Kim", "Boyoun Kim", "Jung Jae Yoon", "Wooseok Jang", "Jeonghoon Hong", "Jaeho Yang", "Yeong-Dae Kwon"], "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements", "comment": "26 pages, 7 figures. Code available at https://github.com/samsungsds-research-papers/mega-gui", "summary": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.", "AI": {"tldr": "MEGA-GUI\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u9009\u62e9\u548c\u5143\u7d20\u5b9a\u4f4d\u6765\u6539\u8fdbGUI\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684GUI\u57fa\u7840\u7cfb\u7edf\u7f3a\u4e4f\u6a21\u5757\u5316\uff0c\u5728\u89c6\u89c9\u6df7\u4e71\u548c\u6307\u4ee4\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "MEGA-GUI\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u9996\u5148\u9009\u62e9\u5927\u7684\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\uff0c\u7136\u540e\u8fdb\u884c\u7cbe\u7ec6\u7684\u5143\u7d20\u5b9a\u4f4d\u3002\u5b83\u4f7f\u7528\u53cc\u5411ROI\u7f29\u653e\u7b97\u6cd5\u6765\u51cf\u5c11\u7a7a\u95f4\u7a00\u91ca\uff0c\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u5199\u4ee3\u7406\u6765\u51cf\u5c11\u8bed\u4e49\u6b67\u4e49\u3002", "result": "MEGA-GUI\u5728ScreenSpot-Pro\u57fa\u51c6\u4e0a\u8fbe\u523073.18%\u7684\u51c6\u786e\u7387\uff0c\u5728OSWorld-G\u57fa\u51c6\u4e0a\u8fbe\u523068.63%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MEGA-GUI\u901a\u8fc7\u5176\u6a21\u5757\u5316\u7ed3\u6784\u548c\u4e13\u95e8\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u5728GUI\u57fa\u7840\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u89c6\u89c9\u5bc6\u96c6\u548c\u8bed\u4e49\u590d\u6742\u7684\u573a\u666f\u65f6\u3002"}}
{"id": "2511.13243", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13243", "abs": "https://arxiv.org/abs/2511.13243", "authors": ["Xiaoqi Han", "Ru Li", "Ran Yi", "Hongye Tan", "Zhuomin Liang", "V\u00edctor Guti\u00e9rrez-Basulto", "Jeff Z. Pan"], "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing", "comment": "Accepted at AAAI'26", "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.", "AI": {"tldr": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u4f1a\u56e0\u4e3a\u4f7f\u7528\u4f4e\u76f8\u4f3c\u5ea6\u6216\u968f\u673a\u8f93\u5165\u800c\u9ad8\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63a9\u76d6\u8fc7\u62df\u5408\u73b0\u8c61\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5c40\u90e8\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u968f\u673a\u56fe\u50cf\u3001\u65e0\u56fe\u50cf\u548c\u4e00\u81f4\u56fe\u50cf\u4e09\u79cd\u5c40\u90e8\u6027\uff0c\u5e76\u901a\u8fc7\u4e03\u79cd\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u64cd\u4f5c\u5316\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u591a\u6a21\u6001\u7f16\u8f91\u7684\u8be6\u7ec6\u3001\u7ed3\u6784\u5316\u5206\u6790\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f15\u5165\u4e86\u52a8\u6001\u8bc4\u4f30\u65b9\u6cd5De-VQA\uff0c\u53d1\u73b0\u4e86\u201c\u77ac\u65f6\u5931\u660e\u201d\u73b0\u8c61\u2014\u2014\u6a21\u578b\u4f1a\u8fc7\u5206\u62df\u5408\u7f16\u8f91\u540e\u7684\u6587\u672c\u800c\u5ffd\u7565\u56fe\u50cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u611f\u77e5\u5c40\u90e8\u6027\u7684\u5bf9\u6297\u6027\u635f\u5931\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u201c\u77ac\u65f6\u5931\u660e\u201d\uff0c\u5e73\u5747\u63d0\u9ad817%\u7684\u5c40\u90e8\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5305\u542b\u968f\u673a\u56fe\u50cf\u3001\u65e0\u56fe\u50cf\u548c\u4e00\u81f4\u56fe\u50cf\u4e09\u79cd\u5c40\u90e8\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f15\u5165De-VQA\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u611f\u77e5\u5c40\u90e8\u6027\u7684\u5bf9\u6297\u6027\u635f\u5931\u3002", "result": "\u65b0\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0\u5e76\u7f13\u89e3\u201c\u77ac\u65f6\u5931\u660e\u201d\u73b0\u8c61\uff0c\u5e73\u5747\u63d0\u9ad817%\u7684\u5c40\u90e8\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u5c40\u90e8\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13306", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13306", "abs": "https://arxiv.org/abs/2511.13306", "authors": ["Bowen Ye", "Bin Zhang", "Hang Zhao"], "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving", "comment": null, "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.", "AI": {"tldr": "DAP\u662f\u4e00\u4e2a\u79bb\u6563\u6807\u8bb0\u7684\u81ea\u56de\u5f52\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548cego\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u5f0f\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u5c01\u95ed\u5f0f\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a1.6\u4ebf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u5728\u6269\u5c55\u6570\u636e\u548c\u6a21\u578b\u9884\u7b97\u4ee5\u83b7\u5f97\u53ef\u6301\u7eed\u6027\u80fd\u63d0\u5347\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5355\u72ec\u9884\u6d4bego\u8f68\u8ff9\u5b58\u5728\u76d1\u7763\u7a00\u758f\u548c\u573a\u666f\u7ea6\u675f\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAP\u7684\u79bb\u6563\u6807\u8bb0\u81ea\u56de\u5f52\u89c4\u5212\u5668\uff0c\u8be5\u89c4\u5212\u5668\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548cego\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u3002", "result": "DAP\u5728\u5f00\u653e\u5f0f\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5c01\u95ed\u5f0f\u7ed3\u679c\uff0c\u540c\u65f6\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3a1.6\u4ebf\u3002", "conclusion": "DAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u8303\u5f0f\uff0c\u5176\u5b8c\u5168\u79bb\u6563\u6807\u8bb0\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u53ef\u540c\u65f6\u5904\u7406\u6805\u683c\u5316BEV\u548cego\u52a8\u4f5c\u3002"}}
{"id": "2511.13654", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13654", "abs": "https://arxiv.org/abs/2511.13654", "authors": ["Pascal Zimmer", "Ghassan Karame"], "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning", "comment": "To appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) 2026", "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u8be6\u7ec6\u5206\u6790\u4e86\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\u3001\u52a8\u91cf\u548c\u6279\u5927\u5c0f\uff09\u5982\u4f55\u5f71\u54cd\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u964d\u4f4e\u5b66\u4e60\u7387\u53ef\u663e\u8457\u63d0\u9ad8\u5bf9\u8fc1\u79fb\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff08\u6700\u9ad8\u8fbe64%\uff09\uff0c\u800c\u63d0\u9ad8\u5b66\u4e60\u7387\u5219\u53ef\u63d0\u9ad8\u5bf9\u67e5\u8be2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff08\u6700\u9ad8\u8fbe28%\uff09\u3002\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u53ef\u4ee5\u540c\u65f6\u7f13\u89e3\u8fd9\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\uff0c\u5176\u4e2d\u5206\u5e03\u5f0f\u6a21\u578b\u5728\u8fd9\u79cd\u8d85\u53c2\u6570\u8c03\u6574\u4e2d\u83b7\u76ca\u6700\u5927\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5206\u6790\u4f18\u5316\u8d85\u53c2\u6570\u5bf9\u6a21\u578b\u5728\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u8fd9\u4e9b\u8d85\u53c2\u6570\u6765\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u5bf9\u4e24\u79cd\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u5728\u4e2d\u5fc3\u5316\u8bad\u7ec3\u3001\u96c6\u6210\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b49\u591a\u79cd\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e0b\uff0c\u5206\u6790\u4e86\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\u3001\u52a8\u91cf\u548c\u6279\u5927\u5c0f\u7b49\u4f18\u5316\u8d85\u53c2\u6570\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u964d\u4f4e\u5b66\u4e60\u7387\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5bf9\u8fc1\u79fb\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff08\u6700\u9ad8\u63d0\u534764%\uff09\uff0c\u800c\u63d0\u9ad8\u5b66\u4e60\u7387\u5219\u80fd\u63d0\u9ad8\u6a21\u578b\u5bf9\u67e5\u8be2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff08\u6700\u9ad8\u63d0\u534728%\uff09\u3002\u5206\u5e03\u5f0f\u6a21\u578b\u5728\u8d85\u53c2\u6570\u8c03\u6574\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u540c\u65f6\u6709\u6548\u5730\u51cf\u8f7b\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\u3002", "conclusion": "\u4f18\u5316\u8d85\u53c2\u6570\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u5b58\u5728\u76f8\u4e92\u5236\u7ea6\u7684\u5173\u7cfb\u3002\u901a\u8fc7\u7cbe\u7ec6\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4e24\u79cd\u653b\u51fb\u7684\u6709\u6548\u9632\u5fa1\uff0c\u53d6\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.13689", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13689", "abs": "https://arxiv.org/abs/2511.13689", "authors": ["Sofia Jamil", "Kotla Sai Charan", "Sriparna Saha", "Koustava Goswami", "Joseph K J"], "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation", "comment": null, "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTAI\u7684\u6846\u67b6\uff0c\u5229\u7528LLM\u548cLDM\u6765\u7ffb\u8bd1\u548c\u751f\u6210\u5370\u5ea6\u8bd7\u6b4c\u7684\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u5176\u53ef\u53ca\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b21\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u8bd7\u6b4c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u8bd7\u6b4c\u4f5c\u54c1\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u89c6\u4e86\u5370\u5ea6\u8bed\u8a00\u7684\u8bd7\u6b4c\uff0c\u800c\u5370\u5ea6\u8bd7\u6b4c\u56e0\u5176\u8bed\u8a00\u590d\u6742\u6027\u548c\u6df1\u539a\u7684\u6587\u5316\u5e95\u8574\u800c\u5177\u6709\u4e30\u5bcc\u7684\u6587\u5316\u610f\u4e49\uff0c\u8fd9\u7ed9\u7406\u89e3\u5e26\u6765\u4e86\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u975e\u6bcd\u8bed\u8005\u6216\u4e0d\u719f\u6089\u5176\u80cc\u666f\u548c\u8bed\u8a00\u7684\u8bfb\u8005\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAI\uff08Translation and Image Generation\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u4ee5\u53ca\u9002\u5f53\u7684\u63d0\u793a\u8c03\u6574\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u4f7f\u7528\u4f18\u52bf\u6bd4\u504f\u597d\u5bf9\u9f50\u7b97\u6cd5\uff08Odds Ratio Preference Alignment Algorithm\uff09\u5c06\u5f62\u6001\u4e30\u5bcc\u7684\u8bd7\u6b4c\u51c6\u786e\u7ffb\u8bd1\u6210\u82f1\u8bed\u7684\u7ffb\u8bd1\u6a21\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4f7f\u7528\u8bed\u4e49\u56fe\u6765\u6355\u6349\u8bd7\u6b4c\u4e2d\u6bd4\u55bb\u53ca\u5176\u542b\u4e49\u4e4b\u95f4\u7684\u4ee3\u5e01\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u8bed\u4e49\u5173\u7cfb\uff0c\u4ece\u800c\u751f\u6210\u6709\u610f\u4e49\u7684\u89c6\u89c9\u8868\u5f81\u7684\u56fe\u50cf\u751f\u6210\u6a21\u5757\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aMorphoVerse\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b21\u79cd\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u76841570\u9996\u8bd7\u6b4c\u3002", "result": "TAI Diffusion\u5728\u8bd7\u6b4c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd9\u5728\u5305\u62ec\u4eba\u7c7b\u548c\u5b9a\u91cf\u8bc4\u4f30\u5728\u5185\u7684\u5168\u9762\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\u5f97\u5230\u4e86\u8bc1\u660e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7TAI\u6846\u67b6\u89e3\u51b3\u4e86\u8bd7\u6b4c\u7ffb\u8bd1\u548c\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u65e8\u5728\u6269\u5927\u5370\u5ea6\u8bd7\u6b4c\u7684\u53ef\u53ca\u6027\u5e76\u4e30\u5bcc\u8bfb\u8005\u7684\u4f53\u9a8c\u3002\u8be5\u6846\u67b6\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u5373\u4f18\u8d28\u6559\u80b2\uff08SDG 4\uff09\u548c\u51cf\u5c11\u4e0d\u5e73\u7b49\uff08SDG 10\uff09\u3002"}}
